1
00:00:01,280 --> 00:00:05,600
Thank you so much. Well, first of all, I

2
00:00:03,120 --> 00:00:09,160
hope you had a great break and that you

3
00:00:05,600 --> 00:00:11,440
are ready to engage again on the

4
00:00:09,160 --> 00:00:13,920
conversation. I first of all would like

5
00:00:11,440 --> 00:00:16,240
to thank CIRC for giving us the

6
00:00:13,920 --> 00:00:19,760
opportunity to be providing a Latin

7
00:00:16,240 --> 00:00:22,560
American overview of um what is the

8
00:00:19,760 --> 00:00:24,720
landscape of AI in a region that data

9
00:00:22,560 --> 00:00:26,880
and research shows that is under

10
00:00:24,720 --> 00:00:28,640
reppresented. So it's quite important

11
00:00:26,880 --> 00:00:31,599
for us and it's personally very

12
00:00:28,640 --> 00:00:34,480
important for me coming from Costa Rica

13
00:00:31,599 --> 00:00:36,960
uh part of Latin America. So we started

14
00:00:34,480 --> 00:00:40,719
thinking about what are the impacts of

15
00:00:36,960 --> 00:00:44,000
AI that actually is being produced the

16
00:00:40,719 --> 00:00:45,920
models let's say are dominated to

17
00:00:44,000 --> 00:00:48,079
different geographies right now

18
00:00:45,920 --> 00:00:50,640
currently the domination is the United

19
00:00:48,079 --> 00:00:54,320
States you have China and you have

20
00:00:50,640 --> 00:00:57,600
Europe but the deployment of AI it's

21
00:00:54,320 --> 00:00:59,800
been global so we started to think and

22
00:00:57,600 --> 00:01:02,239
reflect what are those

23
00:00:59,800 --> 00:01:04,559
deployments are going to be the same are

24
00:01:02,239 --> 00:01:09,040
going to be perceived the same in the

25
00:01:04,559 --> 00:01:11,360
US, in China, in the global south? And

26
00:01:09,040 --> 00:01:13,200
if so, do we need to have a better

27
00:01:11,360 --> 00:01:16,400
context and a better understanding of

28
00:01:13,200 --> 00:01:20,080
the nuances of the social infrastructure

29
00:01:16,400 --> 00:01:23,280
of these places to really engage in an

30
00:01:20,080 --> 00:01:26,240
ethical way of AI in the in its whole

31
00:01:23,280 --> 00:01:28,960
life cycle. So we believe that cultural,

32
00:01:26,240 --> 00:01:32,159
political and economic factors within a

33
00:01:28,960 --> 00:01:35,680
given region, country or community will

34
00:01:32,159 --> 00:01:38,240
shape how we engage with technology. How

35
00:01:35,680 --> 00:01:40,880
we shape social technical imaginaries

36
00:01:38,240 --> 00:01:44,200
could influence positively or negatively

37
00:01:40,880 --> 00:01:47,040
the impacts of each

38
00:01:44,200 --> 00:01:49,360
technology. So what is it that we did? I

39
00:01:47,040 --> 00:01:51,600
will walk you through what we did a

40
00:01:49,360 --> 00:01:53,600
little bit of how we did it. So the

41
00:01:51,600 --> 00:01:57,119
methodology is quite important but I

42
00:01:53,600 --> 00:01:59,360
will try to go to the conclusions um the

43
00:01:57,119 --> 00:02:01,799
vulnerabilities that we have found and

44
00:01:59,360 --> 00:02:05,439
then a little bit of what needs to be

45
00:02:01,799 --> 00:02:07,439
done. So what we did is to create an

46
00:02:05,439 --> 00:02:10,479
ethical assessment of the

47
00:02:07,439 --> 00:02:13,040
vulnerabilities of AI in Latin America.

48
00:02:10,479 --> 00:02:15,040
So how can the region sees the

49
00:02:13,040 --> 00:02:18,480
opportunities? There is a lot of

50
00:02:15,040 --> 00:02:22,160
promises regarding AI being able to

51
00:02:18,480 --> 00:02:24,879
close gaps to help us with our um

52
00:02:22,160 --> 00:02:27,360
structural and historical inequalities.

53
00:02:24,879 --> 00:02:29,920
But at the same time that is possible if

54
00:02:27,360 --> 00:02:32,560
we can also mitigate the risk and we

55
00:02:29,920 --> 00:02:34,959
cannot understand how to leverage the

56
00:02:32,560 --> 00:02:37,920
opportunities and mitigate the risk if

57
00:02:34,959 --> 00:02:40,640
we don't understand our own status our

58
00:02:37,920 --> 00:02:44,480
own landscape and where is Latin America

59
00:02:40,640 --> 00:02:47,040
standing in the global landscape of AI.

60
00:02:44,480 --> 00:02:49,519
So very very briefly, Latin America it's

61
00:02:47,040 --> 00:02:51,920
very diverse. It's very vast. Very hard

62
00:02:49,519 --> 00:02:54,400
for me to let you know exactly what

63
00:02:51,920 --> 00:02:57,560
Latin America is. Just give you two

64
00:02:54,400 --> 00:02:59,760
facts. First of all is one of the most

65
00:02:57,560 --> 00:03:02,159
biodiverse regions in the whole world

66
00:02:59,760 --> 00:03:04,400
and we produce a lot of data. Some of

67
00:03:02,159 --> 00:03:06,360
you may be working in that area. We

68
00:03:04,400 --> 00:03:09,120
produce a lot of data regarding

69
00:03:06,360 --> 00:03:12,159
biodiversity. At the same time, Latin

70
00:03:09,120 --> 00:03:15,840
America is the most not one of the most

71
00:03:12,159 --> 00:03:18,319
the most unequal region in the world. So

72
00:03:15,840 --> 00:03:20,480
this is very important to keep in mind

73
00:03:18,319 --> 00:03:24,239
because we are going to put on top of

74
00:03:20,480 --> 00:03:27,280
that the layer of technology. So how is

75
00:03:24,239 --> 00:03:29,920
that Latin America can leverage the

76
00:03:27,280 --> 00:03:32,519
opportunities while we have still to

77
00:03:29,920 --> 00:03:35,519
catch up with these

78
00:03:32,519 --> 00:03:37,599
challenges? Why this is important? Well,

79
00:03:35,519 --> 00:03:39,920
for me it is important because AI it's

80
00:03:37,599 --> 00:03:43,280
already being deployed and we are

81
00:03:39,920 --> 00:03:45,760
actually feeling the positives uh

82
00:03:43,280 --> 00:03:47,840
impacts of AI but Latin America it's

83
00:03:45,760 --> 00:03:51,200
also already experiencing the negative

84
00:03:47,840 --> 00:03:54,400
impacts of AI in regards of are we ready

85
00:03:51,200 --> 00:03:57,280
for this? Do we have the provisions to

86
00:03:54,400 --> 00:04:01,360
mitigate the risk and the reality is our

87
00:03:57,280 --> 00:04:04,640
case studies shows that not not too much

88
00:04:01,360 --> 00:04:07,120
ready in certain areas. So for example

89
00:04:04,640 --> 00:04:09,439
there is biometric information that has

90
00:04:07,120 --> 00:04:12,400
been collected in Argentina during the

91
00:04:09,439 --> 00:04:15,400
economic crisis. So people was selling

92
00:04:12,400 --> 00:04:18,079
let's say selling their their biometric

93
00:04:15,400 --> 00:04:22,520
information to worldcoin which is a

94
00:04:18,079 --> 00:04:24,720
US-based company without entirely

95
00:04:22,520 --> 00:04:26,880
understanding what is it that they are

96
00:04:24,720 --> 00:04:28,199
providing what the company was going to

97
00:04:26,880 --> 00:04:30,800
do with this

98
00:04:28,199 --> 00:04:34,160
information because Latin America

99
00:04:30,800 --> 00:04:36,880
provides a lot of data work and

100
00:04:34,160 --> 00:04:39,360
microwork let's say um for data

101
00:04:36,880 --> 00:04:40,720
collection and tagging and content

102
00:04:39,360 --> 00:04:42,880
moderation.

103
00:04:40,720 --> 00:04:45,360
Most of the times because of instruct

104
00:04:42,880 --> 00:04:47,759
because of structural informality in the

105
00:04:45,360 --> 00:04:51,120
labor market in Latin America. Most of

106
00:04:47,759 --> 00:04:54,000
that work is also informal sometimes

107
00:04:51,120 --> 00:04:56,080
very very poorly paid in very precarious

108
00:04:54,000 --> 00:04:59,120
conditions but they are part of the

109
00:04:56,080 --> 00:05:01,120
supply chain of the AI ecosystem. And I

110
00:04:59,120 --> 00:05:04,320
will go back to this because I think

111
00:05:01,120 --> 00:05:06,080
when we are talking about ethics in AI,

112
00:05:04,320 --> 00:05:08,800
should we think about the bias of the

113
00:05:06,080 --> 00:05:11,840
model and the deployment or we actually

114
00:05:08,800 --> 00:05:13,560
think about AI in the live in the in the

115
00:05:11,840 --> 00:05:16,800
live uh

116
00:05:13,560 --> 00:05:20,680
cycle. It is important because Venezuela

117
00:05:16,800 --> 00:05:23,479
it's already using Gen AI to provide

118
00:05:20,680 --> 00:05:26,000
propaganda, misinformation,

119
00:05:23,479 --> 00:05:29,120
disinformation from the government. But

120
00:05:26,000 --> 00:05:31,880
also it's an interesting case because

121
00:05:29,120 --> 00:05:34,160
journalism in Venezuela that has been

122
00:05:31,880 --> 00:05:37,039
repressed by the authoritarian

123
00:05:34,160 --> 00:05:39,080
government of uh Nicolas Maduro they

124
00:05:37,039 --> 00:05:43,199
have been also using

125
00:05:39,080 --> 00:05:45,039
Genai to create journalism without the

126
00:05:43,199 --> 00:05:46,639
journalists to be threatened by the

127
00:05:45,039 --> 00:05:48,479
government. So it is very important.

128
00:05:46,639 --> 00:05:51,360
Geni was started to produce propaganda

129
00:05:48,479 --> 00:05:54,400
for the government but now also is an

130
00:05:51,360 --> 00:05:56,960
innovative way for the opposition to

131
00:05:54,400 --> 00:05:59,199
also provide information. It is

132
00:05:56,960 --> 00:06:02,080
important because we are very very

133
00:05:59,199 --> 00:06:04,639
vulnerable to cyber attacks. fraud and

134
00:06:02,080 --> 00:06:06,800
scams and cyber attacks are on the surge

135
00:06:04,639 --> 00:06:09,440
in Latin America. And the reality is

136
00:06:06,800 --> 00:06:12,400
that we are somehow prepared for some of

137
00:06:09,440 --> 00:06:14,720
those, but mostly if they are targeted

138
00:06:12,400 --> 00:06:16,960
into our critical infrastructure, which

139
00:06:14,720 --> 00:06:19,520
was the case for Costa Rica, they

140
00:06:16,960 --> 00:06:22,639
targeted our health care system. So

141
00:06:19,520 --> 00:06:24,800
sensitive um we are we need to be

142
00:06:22,639 --> 00:06:27,840
prepared for this. And also, and I

143
00:06:24,800 --> 00:06:30,639
wanted to point this out um in a very

144
00:06:27,840 --> 00:06:32,440
nuanced and sensible way, Latin America

145
00:06:30,639 --> 00:06:35,520
needs to catch up with a lot of

146
00:06:32,440 --> 00:06:37,600
investment. AI is not possible if we

147
00:06:35,520 --> 00:06:39,919
don't have the infrastructure that make

148
00:06:37,600 --> 00:06:42,120
that possible. That means that Latin

149
00:06:39,919 --> 00:06:45,840
America needs to catch up with

150
00:06:42,120 --> 00:06:48,160
5G. Currently, there is telecom

151
00:06:45,840 --> 00:06:51,280
operators in Latin America, but there is

152
00:06:48,160 --> 00:06:54,400
no deployer of 5G in Latin America. That

153
00:06:51,280 --> 00:06:57,199
will mean that Latin America needs to go

154
00:06:54,400 --> 00:06:59,680
to do an international beating and that

155
00:06:57,199 --> 00:07:01,960
beating usually if you want competition

156
00:06:59,680 --> 00:07:05,080
will mean that you will have US

157
00:07:01,960 --> 00:07:07,759
providers, Europe providers and China

158
00:07:05,080 --> 00:07:11,280
providers. Needless to say that China

159
00:07:07,759 --> 00:07:14,000
currently is the global leader in 5G

160
00:07:11,280 --> 00:07:16,720
deployment very competitive. So if Latin

161
00:07:14,000 --> 00:07:20,000
America is trapped in a geopolitical

162
00:07:16,720 --> 00:07:22,599
battle between the US and China, the

163
00:07:20,000 --> 00:07:25,120
reality is that it could be very very

164
00:07:22,599 --> 00:07:27,599
detrimental in how we can catch up with

165
00:07:25,120 --> 00:07:30,120
our deployment of 5G. So it is important

166
00:07:27,599 --> 00:07:33,280
to to provide that

167
00:07:30,120 --> 00:07:37,280
perspective. So going back to the ethics

168
00:07:33,280 --> 00:07:39,759
and the AI AI system life cycle. Um

169
00:07:37,280 --> 00:07:42,160
where is Latin America currently in the

170
00:07:39,759 --> 00:07:44,000
life cycle? This is the OECD life cycle

171
00:07:42,160 --> 00:07:47,440
just to provide a reference. Latin

172
00:07:44,000 --> 00:07:50,639
America right now is engaged in the AI

173
00:07:47,440 --> 00:07:53,919
systems providing low added value

174
00:07:50,639 --> 00:07:56,080
inputs. So we provide data collection

175
00:07:53,919 --> 00:07:58,319
and processing content moderation that

176
00:07:56,080 --> 00:08:00,400
could be in the loop. uh going back to

177
00:07:58,319 --> 00:08:03,599
data collection and processing and

178
00:08:00,400 --> 00:08:05,680
obviously we are full on on deployment

179
00:08:03,599 --> 00:08:08,400
but when we are talking about ethics

180
00:08:05,680 --> 00:08:11,520
going back to my point are we just

181
00:08:08,400 --> 00:08:12,280
talking about biased in the systems in

182
00:08:11,520 --> 00:08:15,199
the

183
00:08:12,280 --> 00:08:17,840
models should we think about the ethics

184
00:08:15,199 --> 00:08:21,319
of AI thinking about the whole supply

185
00:08:17,840 --> 00:08:24,319
chain now that it is a hot topic in the

186
00:08:21,319 --> 00:08:27,039
conversation why because maybe we could

187
00:08:24,319 --> 00:08:29,840
also think about that Latin America is

188
00:08:27,039 --> 00:08:31,840
engaged in the AI system life cycle

189
00:08:29,840 --> 00:08:34,640
through critical minerals providing the

190
00:08:31,840 --> 00:08:37,479
critical minerals that makes AI

191
00:08:34,640 --> 00:08:40,519
possible. Should we think that these

192
00:08:37,479 --> 00:08:43,279
inputs should be ethical

193
00:08:40,519 --> 00:08:46,120
sourced? Do we think that the data

194
00:08:43,279 --> 00:08:51,120
workers that are part of the supply

195
00:08:46,120 --> 00:08:53,600
chain should be ethically um should be

196
00:08:51,120 --> 00:08:56,040
ethically be be within ethical dynamics

197
00:08:53,600 --> 00:08:59,600
of labor. If they are

198
00:08:56,040 --> 00:09:02,920
not, can we say that our model, our

199
00:08:59,600 --> 00:09:06,560
system is an ethical

200
00:09:02,920 --> 00:09:08,640
AI? So this is just um it is important

201
00:09:06,560 --> 00:09:12,560
just to to say that Latin America right

202
00:09:08,640 --> 00:09:16,399
now has very uh little participation in

203
00:09:12,560 --> 00:09:18,880
the design, planning and um and

204
00:09:16,399 --> 00:09:21,519
development of the models. We are very

205
00:09:18,880 --> 00:09:23,360
engaged on deployment providing feedback

206
00:09:21,519 --> 00:09:26,800
and there's obviously Latin American

207
00:09:23,360 --> 00:09:29,360
talent in all the phases but we should

208
00:09:26,800 --> 00:09:31,920
be um we should be more engaged in

209
00:09:29,360 --> 00:09:34,240
decision making and how we design and

210
00:09:31,920 --> 00:09:37,600
shape these models to capture the

211
00:09:34,240 --> 00:09:40,160
cultural nuances of Latin America of our

212
00:09:37,600 --> 00:09:43,519
own language of our own culture and of

213
00:09:40,160 --> 00:09:47,839
our own social infrastructure.

214
00:09:43,519 --> 00:09:49,760
So how we did this ethical analysis to

215
00:09:47,839 --> 00:09:52,080
understand where are our critical

216
00:09:49,760 --> 00:09:56,080
vulnerabilities. So Latin America is

217
00:09:52,080 --> 00:09:58,760
very broad. We took as a reference the

218
00:09:56,080 --> 00:10:01,600
Latin American artificial intelligence

219
00:09:58,760 --> 00:10:04,000
index as a reference. What they do is

220
00:10:01,600 --> 00:10:06,560
that they just measure the readiness for

221
00:10:04,000 --> 00:10:10,160
each country regarding AI. They don't

222
00:10:06,560 --> 00:10:13,120
get into AI um ethical dynamics or

223
00:10:10,160 --> 00:10:16,399
compliances. they just measure where the

224
00:10:13,120 --> 00:10:20,800
countries are in regards of readiness.

225
00:10:16,399 --> 00:10:22,880
So for our research we took um two

226
00:10:20,800 --> 00:10:25,839
pioneers the leading countries in Latin

227
00:10:22,880 --> 00:10:29,760
America which are Chile and Brazil. They

228
00:10:25,839 --> 00:10:34,320
are leading the way very much. We have

229
00:10:29,760 --> 00:10:37,440
the standards let's say the the adopters

230
00:10:34,320 --> 00:10:40,079
which are the standard of Latin America.

231
00:10:37,440 --> 00:10:42,720
We took Mexico and Costa Rica. And for

232
00:10:40,079 --> 00:10:44,800
the explorers, let's another word to say

233
00:10:42,720 --> 00:10:48,160
the the countries that needs to catch

234
00:10:44,800 --> 00:10:50,000
up. Um we took El Salvador and Honduras.

235
00:10:48,160 --> 00:10:53,360
This is important not only to understand

236
00:10:50,000 --> 00:10:56,079
a little bit of the the differences

237
00:10:53,360 --> 00:10:59,040
between the region but also to have

238
00:10:56,079 --> 00:11:01,600
representation from um North Latin

239
00:10:59,040 --> 00:11:04,240
America, Central Latin America and South

240
00:11:01,600 --> 00:11:08,160
Latin America.

241
00:11:04,240 --> 00:11:10,160
We took um we needed to define which

242
00:11:08,160 --> 00:11:12,560
principles which ethical principles are

243
00:11:10,160 --> 00:11:15,560
we willing to work with to understand

244
00:11:12,560 --> 00:11:18,640
the ethical type of dynamics in Latin

245
00:11:15,560 --> 00:11:21,720
America. Our countries and these six

246
00:11:18,640 --> 00:11:23,839
countries have signed several

247
00:11:21,720 --> 00:11:26,880
declarations, several international

248
00:11:23,839 --> 00:11:30,079
agreements. So what we did is that we um

249
00:11:26,880 --> 00:11:33,440
we did a mapping of which agreements are

250
00:11:30,079 --> 00:11:36,079
shared but all of by all of these six

251
00:11:33,440 --> 00:11:40,240
countries and basically these six

252
00:11:36,079 --> 00:11:42,560
countries Chile, Brazil, Costa Rica,

253
00:11:40,240 --> 00:11:45,519
Mexico, Salvador and Honduras. The the

254
00:11:42,560 --> 00:11:47,519
three of them have committed to the

255
00:11:45,519 --> 00:11:50,519
UNESCO's recommendation on the ethics of

256
00:11:47,519 --> 00:11:53,200
AI. This is probably the most uh

257
00:11:50,519 --> 00:11:55,200
ratified agreement in terms of ethics

258
00:11:53,200 --> 00:11:57,519
globally. So many countries have signed

259
00:11:55,200 --> 00:12:00,000
it. Uh we also took the Santiago

260
00:11:57,519 --> 00:12:02,800
declaration that is based on UNESCO and

261
00:12:00,000 --> 00:12:06,000
the Interameric framework on data

262
00:12:02,800 --> 00:12:09,519
governance and AI. Sorry the typo. Um

263
00:12:06,000 --> 00:12:11,839
those two are based on the UNESCO. So

264
00:12:09,519 --> 00:12:14,399
based on these three agreements that

265
00:12:11,839 --> 00:12:17,839
these six countries have signed, we

266
00:12:14,399 --> 00:12:19,920
created what are the shared principal

267
00:12:17,839 --> 00:12:22,720
principles that we are going to work

268
00:12:19,920 --> 00:12:26,959
with. So we came up and this is quite

269
00:12:22,720 --> 00:12:28,480
important. with 15 ethical principles

270
00:12:26,959 --> 00:12:30,639
uh because they were very different. So

271
00:12:28,480 --> 00:12:33,120
we came up with the 15 ethical

272
00:12:30,639 --> 00:12:35,760
principles that you can um share with

273
00:12:33,120 --> 00:12:39,040
all those declarations and after

274
00:12:35,760 --> 00:12:42,480
understanding these principles we went

275
00:12:39,040 --> 00:12:45,959
country by country understanding how

276
00:12:42,480 --> 00:12:49,200
each country um creates provisions for

277
00:12:45,959 --> 00:12:53,399
proportionality. Is it rightsbased type

278
00:12:49,200 --> 00:12:56,000
of uh proportionality? Is it um

279
00:12:53,399 --> 00:12:57,600
riskbased? How is the regional status

280
00:12:56,000 --> 00:12:59,240
around proportionality? How is the

281
00:12:57,600 --> 00:13:01,760
regional status around

282
00:12:59,240 --> 00:13:05,120
security? And for each one of those

283
00:13:01,760 --> 00:13:07,760
principles, we provided if you if you

284
00:13:05,120 --> 00:13:09,680
want to have security, what are the

285
00:13:07,760 --> 00:13:11,600
indicators that will guarantee to your

286
00:13:09,680 --> 00:13:14,320
population that you will have security

287
00:13:11,600 --> 00:13:16,320
in AI? Well, maybe cyber security is one

288
00:13:14,320 --> 00:13:18,680
of them. So, that is one indicator. And

289
00:13:16,320 --> 00:13:22,000
then we anal we analyzed all the

290
00:13:18,680 --> 00:13:24,720
provisions, metrics, mechanisms that

291
00:13:22,000 --> 00:13:27,360
each one of these countries have in

292
00:13:24,720 --> 00:13:29,279
order to guarantee these commitments

293
00:13:27,360 --> 00:13:31,760
that they have ratified. Because the

294
00:13:29,279 --> 00:13:35,200
reality is that ethical commitments

295
00:13:31,760 --> 00:13:37,560
sometimes can be very very vague and

296
00:13:35,200 --> 00:13:41,200
sometimes unfortunately very

297
00:13:37,560 --> 00:13:43,600
aspirational and there is um little

298
00:13:41,200 --> 00:13:45,839
monitoring or enforcement control

299
00:13:43,600 --> 00:13:48,639
afterwards.

300
00:13:45,839 --> 00:13:51,760
So we took three benchmarks to provide

301
00:13:48,639 --> 00:13:54,480
also inputs to our analysis. The first

302
00:13:51,760 --> 00:13:58,000
one was um a global benchmarks. We took

303
00:13:54,480 --> 00:14:01,120
the Stamford AI index 2024, a regional

304
00:13:58,000 --> 00:14:04,079
benchmark um which is the Latin American

305
00:14:01,120 --> 00:14:07,360
AI index that provides regional metrics

306
00:14:04,079 --> 00:14:11,199
around AI and then uh in order to

307
00:14:07,360 --> 00:14:14,800
provide the ethical base the UNESCO um

308
00:14:11,199 --> 00:14:17,680
we took all um reports paper literature

309
00:14:14,800 --> 00:14:20,000
that we had and also everything that has

310
00:14:17,680 --> 00:14:21,800
to do with regulatory frameworks and any

311
00:14:20,000 --> 00:14:24,839
type of

312
00:14:21,800 --> 00:14:27,680
provisions and what we did is a cross

313
00:14:24,839 --> 00:14:30,440
analysis. A cross analysis for each one

314
00:14:27,680 --> 00:14:33,279
of the principles, each one of the

315
00:14:30,440 --> 00:14:36,560
indicators, each one of the countries in

316
00:14:33,279 --> 00:14:39,600
a regional status in general. This is uh

317
00:14:36,560 --> 00:14:41,920
through two um cross analysis methods.

318
00:14:39,600 --> 00:14:44,160
The first one is the cross tabulation.

319
00:14:41,920 --> 00:14:46,959
So just to give you an example here, uh

320
00:14:44,160 --> 00:14:49,040
we're talking about data protection. The

321
00:14:46,959 --> 00:14:51,199
indic the sub indicator will be cyber

322
00:14:49,040 --> 00:14:53,199
security and we went through understand

323
00:14:51,199 --> 00:14:55,040
what is the regional status what is the

324
00:14:53,199 --> 00:14:57,279
status for each country what are the

325
00:14:55,040 --> 00:14:58,800
metrics that we have for any type of

326
00:14:57,279 --> 00:15:01,440
indicator the other indicator will be

327
00:14:58,800 --> 00:15:04,160
automation exposed jobs etc etc etc this

328
00:15:01,440 --> 00:15:07,199
is for gender equality this is just a an

329
00:15:04,160 --> 00:15:09,360
example um the research is uh is based

330
00:15:07,199 --> 00:15:11,760
on numbers and numbers of tables because

331
00:15:09,360 --> 00:15:13,920
we did that for each one of the 15

332
00:15:11,760 --> 00:15:16,480
principles for each one of the countries

333
00:15:13,920 --> 00:15:19,120
and for a regional overview And after

334
00:15:16,480 --> 00:15:21,519
that we said well if we think about AI

335
00:15:19,120 --> 00:15:24,800
as a system we should also think about

336
00:15:21,519 --> 00:15:26,880
the principles of ethics of AI also as a

337
00:15:24,800 --> 00:15:29,120
system because they actually are

338
00:15:26,880 --> 00:15:31,680
intertwined they relate to each other.

339
00:15:29,120 --> 00:15:33,440
So for example um we needed to

340
00:15:31,680 --> 00:15:36,079
understand what is the relationship

341
00:15:33,440 --> 00:15:38,639
between the principles and what is the

342
00:15:36,079 --> 00:15:41,199
impact that each principle has in any

343
00:15:38,639 --> 00:15:43,760
other principle also as part of our

344
00:15:41,199 --> 00:15:45,440
cross um analysis. So just to give you

345
00:15:43,760 --> 00:15:48,880
an example, there are some principles

346
00:15:45,440 --> 00:15:51,600
that they be in tension, meaning that if

347
00:15:48,880 --> 00:15:54,120
you want to go high with security, you

348
00:15:51,600 --> 00:15:56,320
may go a little bit lower in

349
00:15:54,120 --> 00:15:59,759
accessibility. So they could get into

350
00:15:56,320 --> 00:16:02,320
tension or some others that when we have

351
00:15:59,759 --> 00:16:04,560
more non-discrimination probably you

352
00:16:02,320 --> 00:16:06,320
will have better gender equality or if

353
00:16:04,560 --> 00:16:09,040
you have better gender equality, your

354
00:16:06,320 --> 00:16:10,519
non-discrimination will al will actually

355
00:16:09,040 --> 00:16:14,880
also go

356
00:16:10,519 --> 00:16:19,360
high. and how they um they uh impact

357
00:16:14,880 --> 00:16:20,959
each other. So um again security and

358
00:16:19,360 --> 00:16:23,279
accessibility will have a three because

359
00:16:20,959 --> 00:16:26,959
they are strongly impacting each other.

360
00:16:23,279 --> 00:16:29,759
What we found in the the relationship

361
00:16:26,959 --> 00:16:31,120
and the cross impact analysis is that

362
00:16:29,759 --> 00:16:33,480
when we are talking about these

363
00:16:31,120 --> 00:16:36,720
principles, what it's at the core of the

364
00:16:33,480 --> 00:16:38,000
conversation, it's human rights and also

365
00:16:36,720 --> 00:16:40,560
what it's at the core of the

366
00:16:38,000 --> 00:16:43,040
conversation that we need to take into

367
00:16:40,560 --> 00:16:45,759
consideration is participation.

368
00:16:43,040 --> 00:16:48,240
participation has a strong impact on how

369
00:16:45,759 --> 00:16:50,639
we manage gender equality because if

370
00:16:48,240 --> 00:16:53,440
women are participating then they will

371
00:16:50,639 --> 00:16:55,759
provide a better strong and different

372
00:16:53,440 --> 00:16:58,240
points of view. If you want to make sure

373
00:16:55,759 --> 00:17:01,199
that you have non-discrimination well

374
00:16:58,240 --> 00:17:04,160
depending on who is at the table you may

375
00:17:01,199 --> 00:17:06,720
improve those numbers. So human rights

376
00:17:04,160 --> 00:17:08,319
at the core any principle is going down

377
00:17:06,720 --> 00:17:10,799
human rights are going down. It seems

378
00:17:08,319 --> 00:17:15,000
obvious but it's important to measure it

379
00:17:10,799 --> 00:17:19,640
and also participation multistakeholder

380
00:17:15,000 --> 00:17:23,520
engagement. Um so based on those um

381
00:17:19,640 --> 00:17:25,919
methodologies we got to 10 conclusions

382
00:17:23,520 --> 00:17:28,079
10 critical 10 critical vulnerabilities

383
00:17:25,919 --> 00:17:29,679
and some recommendations. I will go

384
00:17:28,079 --> 00:17:31,840
through because I think I'm short of

385
00:17:29,679 --> 00:17:34,320
time. I will go through the critical

386
00:17:31,840 --> 00:17:37,120
vulnerabilities very very briefly. This

387
00:17:34,320 --> 00:17:39,120
is what we found. Um some of them are

388
00:17:37,120 --> 00:17:41,679
not necessarily just for Latin American

389
00:17:39,120 --> 00:17:44,559
region but they are especially critical

390
00:17:41,679 --> 00:17:46,480
for the Latin American region. Um we may

391
00:17:44,559 --> 00:17:47,760
have expansion of surveillance and

392
00:17:46,480 --> 00:17:49,840
social control. Just to give you an

393
00:17:47,760 --> 00:17:52,480
example of this, more than 30

394
00:17:49,840 --> 00:17:54,480
initiatives for facial technology for

395
00:17:52,480 --> 00:17:56,400
facial recognition technologies are in

396
00:17:54,480 --> 00:17:59,520
Latin America without necessarily the

397
00:17:56,400 --> 00:18:02,320
proper regulation. Um proliferation of

398
00:17:59,520 --> 00:18:04,400
misinformation, escalation of AI enabled

399
00:18:02,320 --> 00:18:06,080
organized crime. Organized crime is

400
00:18:04,400 --> 00:18:07,679
already used in AI in Latin America.

401
00:18:06,080 --> 00:18:10,559
They have the resources. They are

402
00:18:07,679 --> 00:18:12,799
flexible enough to navigate uh the use

403
00:18:10,559 --> 00:18:14,559
of AI. While the government and the

404
00:18:12,799 --> 00:18:17,520
police officers, well, they have other

405
00:18:14,559 --> 00:18:19,840
type of limitations. So we have these 10

406
00:18:17,520 --> 00:18:23,520
critical vulnerabilities

407
00:18:19,840 --> 00:18:25,280
um and then we provided five action five

408
00:18:23,520 --> 00:18:26,919
sectors where we think are the

409
00:18:25,280 --> 00:18:29,440
priorities of the actions and

410
00:18:26,919 --> 00:18:32,960
recommendations. First of all we need to

411
00:18:29,440 --> 00:18:35,840
collaborate and work as a region. If we

412
00:18:32,960 --> 00:18:38,960
want to negotiate we need scale. If we

413
00:18:35,840 --> 00:18:41,760
want to enhance and leverage on data we

414
00:18:38,960 --> 00:18:43,760
need to create synergy. If we want to

415
00:18:41,760 --> 00:18:46,480
fight organized crime that actually has

416
00:18:43,760 --> 00:18:48,240
no borders, well, we need to create a

417
00:18:46,480 --> 00:18:50,080
regional synergy around that

418
00:18:48,240 --> 00:18:53,760
institutional transformation is not

419
00:18:50,080 --> 00:18:56,240
enough to do national AI strategies. We

420
00:18:53,760 --> 00:18:58,559
need a more comprehensive development

421
00:18:56,240 --> 00:19:01,440
vision that it's um that it's

422
00:18:58,559 --> 00:19:04,240
transversal to AI prioritize education

423
00:19:01,440 --> 00:19:06,160
mostly gender equality and urban rural

424
00:19:04,240 --> 00:19:08,160
this will be more important uh very

425
00:19:06,160 --> 00:19:11,360
important for Latin America industrial

426
00:19:08,160 --> 00:19:13,360
policies and investment 5G and other

427
00:19:11,360 --> 00:19:16,760
type of investments. Thank you. Thank

428
00:19:13,360 --> 00:19:16,760
you so much.

