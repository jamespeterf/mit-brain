1
00:00:02,639 --> 00:00:06,520
today I'm going to talk about some

2
00:00:04,640 --> 00:00:08,559
thoughts and considerations that I use

3
00:00:06,520 --> 00:00:10,840
when doing binary classification for

4
00:00:08,559 --> 00:00:14,280
disease outcomes using human biobank

5
00:00:10,840 --> 00:00:16,199
data so I'm going to particularly focus

6
00:00:14,280 --> 00:00:19,640
on

7
00:00:16,199 --> 00:00:24,519
dementia and I have done some work using

8
00:00:19,640 --> 00:00:27,960
the UK biobank which is one of many

9
00:00:24,519 --> 00:00:30,880
biobanks and UK biobank has about half a

10
00:00:27,960 --> 00:00:32,759
million people in it and what kinds of

11
00:00:30,880 --> 00:00:34,600
data sources might you want to use to

12
00:00:32,759 --> 00:00:36,640
try and predict who will be diagnosed

13
00:00:34,600 --> 00:00:39,440
with dementia so there are three

14
00:00:36,640 --> 00:00:44,160
specific data sets that I have used one

15
00:00:39,440 --> 00:00:47,079
is a proteomic data set which comes from

16
00:00:44,160 --> 00:00:47,079
blood

17
00:00:47,120 --> 00:00:53,640
plasma another is a neuroimaging data

18
00:00:51,160 --> 00:00:56,800
set which comes from various brain

19
00:00:53,640 --> 00:01:00,239
Imaging modalities like uh MRI and

20
00:00:56,800 --> 00:01:03,199
diffusion tensor Imaging and others

21
00:01:00,239 --> 00:01:06,920
finally um there is a cognitive test

22
00:01:03,199 --> 00:01:09,240
data set which looks at uh measures of

23
00:01:06,920 --> 00:01:12,280
aect as well as things like numerical

24
00:01:09,240 --> 00:01:14,840
and spatial reasoning so when you look

25
00:01:12,280 --> 00:01:17,200
at complex data sources like these to

26
00:01:14,840 --> 00:01:19,799
try and predict disease you also want to

27
00:01:17,200 --> 00:01:22,000
use good baselines and when I say

28
00:01:19,799 --> 00:01:24,040
Baseline I don't mean using logistic

29
00:01:22,000 --> 00:01:26,640
regression and comparing that to a deep

30
00:01:24,040 --> 00:01:29,600
learning classifier I mean Baseline sets

31
00:01:26,640 --> 00:01:31,680
of features so very basic clinical and

32
00:01:29,600 --> 00:01:33,399
demog graphic characteristics that you

33
00:01:31,680 --> 00:01:36,840
would want to check against your more

34
00:01:33,399 --> 00:01:39,520
complex harder to obtain data sources so

35
00:01:36,840 --> 00:01:39,520
when we talk about

36
00:01:40,399 --> 00:01:45,960
baselines that can include things like

37
00:01:46,560 --> 00:01:50,799
demographics and that can include things

38
00:01:48,920 --> 00:01:52,439
like

39
00:01:50,799 --> 00:01:54,560
age

40
00:01:52,439 --> 00:02:00,079
sex

41
00:01:54,560 --> 00:02:02,560
education uh maybe ethnicity and um

42
00:02:00,079 --> 00:02:04,479
apoe

43
00:02:02,560 --> 00:02:06,200
genotype which is the most

44
00:02:04,479 --> 00:02:08,679
well-characterized uh genetic

45
00:02:06,200 --> 00:02:10,840
predisposition to Alzheimer's disease

46
00:02:08,679 --> 00:02:13,040
now in addition to these very basic uh

47
00:02:10,840 --> 00:02:17,319
demographic factors you might also want

48
00:02:13,040 --> 00:02:17,319
to look at clinical risk factors

49
00:02:18,239 --> 00:02:24,640
so that includes things like

50
00:02:21,800 --> 00:02:30,040
smoking or alcohol

51
00:02:24,640 --> 00:02:32,440
consumption or history of head injury

52
00:02:30,040 --> 00:02:34,040
and it's good to be very comprehensive

53
00:02:32,440 --> 00:02:36,720
with baselines because you want to be

54
00:02:34,040 --> 00:02:40,480
very rigorous when justifying the use of

55
00:02:36,720 --> 00:02:41,680
more complex and costly data sources so

56
00:02:40,480 --> 00:02:43,000
these are the different features that

57
00:02:41,680 --> 00:02:44,840
you might want to use to predict

58
00:02:43,000 --> 00:02:47,080
dementia now let's talk about how we

59
00:02:44,840 --> 00:02:47,879
actually use them so let's talk about

60
00:02:47,080 --> 00:02:50,920
data

61
00:02:47,879 --> 00:02:53,000
splitting so when you split a data set

62
00:02:50,920 --> 00:02:55,280
to run a machine learning experiment

63
00:02:53,000 --> 00:02:57,519
some people they might do something like

64
00:02:55,280 --> 00:03:00,080
a random 8020 split for training and

65
00:02:57,519 --> 00:03:02,360
tests or maybe a stratified 8020 split

66
00:03:00,080 --> 00:03:04,599
or you might do something like 60220 for

67
00:03:02,360 --> 00:03:07,120
training validation and test here is

68
00:03:04,599 --> 00:03:10,440
something that I did so this is my very

69
00:03:07,120 --> 00:03:12,599
bad drawing of the UK and in the UK

70
00:03:10,440 --> 00:03:15,120
biobank there are all these different

71
00:03:12,599 --> 00:03:18,640
assessment centers where the data in the

72
00:03:15,120 --> 00:03:22,440
UK biob bank uh are collected so what I

73
00:03:18,640 --> 00:03:25,440
did is I grouped them into 10

74
00:03:22,440 --> 00:03:25,440
geographical

75
00:03:25,640 --> 00:03:33,239
regions and I used a geographical

76
00:03:30,120 --> 00:03:33,239
cross validation

77
00:03:33,360 --> 00:03:40,080
procedure where one of these regions is

78
00:03:36,959 --> 00:03:42,720
held out and then you train on the other

79
00:03:40,080 --> 00:03:45,040
nine and then you iterate overall 10 of

80
00:03:42,720 --> 00:03:47,319
these regions uh performing the same

81
00:03:45,040 --> 00:03:49,519
procedure so that's how data splitting

82
00:03:47,319 --> 00:03:52,319
was done and now let's talk about uh the

83
00:03:49,519 --> 00:03:55,680
model briefly so I use an algorithm

84
00:03:52,319 --> 00:03:58,920
called light gradient boosted machine or

85
00:03:55,680 --> 00:04:00,920
light GBM and uh what's really nice

86
00:03:58,920 --> 00:04:03,120
about light GBM is that it scales really

87
00:04:00,920 --> 00:04:05,760
well to high-dimensional data sets with

88
00:04:03,120 --> 00:04:08,159
lots of samples and lots of features and

89
00:04:05,760 --> 00:04:09,959
um it's very fast in comparison to other

90
00:04:08,159 --> 00:04:12,519
kinds of models like random forests or

91
00:04:09,959 --> 00:04:15,560
XG boost or um even his histogram

92
00:04:12,519 --> 00:04:17,359
gradient boosting classifiers and um

93
00:04:15,560 --> 00:04:19,239
whenever you have a model like this you

94
00:04:17,359 --> 00:04:21,160
have to optimize the hyperparameters and

95
00:04:19,239 --> 00:04:24,120
so for that I used an auto machine

96
00:04:21,160 --> 00:04:26,360
learning framework called

97
00:04:24,120 --> 00:04:28,800
Flamel which stands for fast and

98
00:04:26,360 --> 00:04:30,479
lightweight Auto Machine learning and

99
00:04:28,800 --> 00:04:32,560
the the thing that Flamel takes

100
00:04:30,479 --> 00:04:35,320
advantage of is that during your

101
00:04:32,560 --> 00:04:37,400
training procedure it's very easy to

102
00:04:35,320 --> 00:04:39,600
reduce your error when you're early in

103
00:04:37,400 --> 00:04:41,520
the training procedure it becomes more

104
00:04:39,600 --> 00:04:43,479
costly and timec consuming to reduce

105
00:04:41,520 --> 00:04:45,840
your error when you're later in the

106
00:04:43,479 --> 00:04:49,400
training procedure and so Flamel takes

107
00:04:45,840 --> 00:04:52,840
advantage of this by exploiting lower

108
00:04:49,400 --> 00:04:55,039
cost sets of hyperparameters earlier in

109
00:04:52,840 --> 00:04:57,639
the training procedure before moving on

110
00:04:55,039 --> 00:04:59,759
to more complex and costly sets of

111
00:04:57,639 --> 00:05:02,000
hyperparameters in the search space

112
00:04:59,759 --> 00:05:04,199
later in the training

113
00:05:02,000 --> 00:05:05,880
procedure okay so we have this cross

114
00:05:04,199 --> 00:05:08,520
validation procedure and we have this

115
00:05:05,880 --> 00:05:12,639
modeling approach now let's talk about

116
00:05:08,520 --> 00:05:12,639
metrics how do we evaluate these

117
00:05:14,039 --> 00:05:20,639
models now what too many papers do is

118
00:05:18,440 --> 00:05:23,199
they draw a rock

119
00:05:20,639 --> 00:05:26,800
curve with the true positive rate on the

120
00:05:23,199 --> 00:05:28,960
y- axis false positive rate on the xaxis

121
00:05:26,800 --> 00:05:32,360
you have your chance along the diagonal

122
00:05:28,960 --> 00:05:35,800
you draw a cur Cur and maybe your au

123
00:05:32,360 --> 00:05:37,360
is8 to 0.9 and then you declare Victory

124
00:05:35,800 --> 00:05:39,280
and you go home and one of the

125
00:05:37,360 --> 00:05:42,840
take-homes of this talk is that this is

126
00:05:39,280 --> 00:05:44,800
bad do not do this for multiple reasons

127
00:05:42,840 --> 00:05:47,000
one reason is that when you calculate

128
00:05:44,800 --> 00:05:49,160
the Au of a rock curve you are

129
00:05:47,000 --> 00:05:52,880
considering decision thresholds that are

130
00:05:49,160 --> 00:05:55,160
absolutely nonsensical so for example

131
00:05:52,880 --> 00:05:57,560
the last point of a rock curve

132
00:05:55,160 --> 00:06:00,080
corresponds to a decision threshold of

133
00:05:57,560 --> 00:06:02,960
zero in this case meaning that it

134
00:06:00,080 --> 00:06:05,160
predicts that everybody has dementia so

135
00:06:02,960 --> 00:06:07,400
your true positive rate is going to be

136
00:06:05,160 --> 00:06:10,319
one which is great but your false

137
00:06:07,400 --> 00:06:14,240
positive rate is also going to be one

138
00:06:10,319 --> 00:06:16,720
not great so do you really want to

139
00:06:14,240 --> 00:06:19,680
consider decision thresholds that are

140
00:06:16,720 --> 00:06:22,319
very far on the x-axis corresponding to

141
00:06:19,680 --> 00:06:24,400
high false positive rates I wouldn't I

142
00:06:22,319 --> 00:06:27,000
don't think you would either so one

143
00:06:24,400 --> 00:06:30,280
thing that some that you can do is to do

144
00:06:27,000 --> 00:06:31,960
a partial Au course responding to the

145
00:06:30,280 --> 00:06:35,039
part of the curve that encompasses

146
00:06:31,960 --> 00:06:36,960
decision thresholds that are clinically

147
00:06:35,039 --> 00:06:39,120
or biologically

148
00:06:36,960 --> 00:06:40,800
reasonable that's one thing you can do

149
00:06:39,120 --> 00:06:43,160
that's good another thing that you can

150
00:06:40,800 --> 00:06:45,400
do that's good is to actually pick a

151
00:06:43,160 --> 00:06:47,560
decision threshold so when people use

152
00:06:45,400 --> 00:06:49,120
packages like s kit learn the default

153
00:06:47,560 --> 00:06:52,280
decision threshold is

154
00:06:49,120 --> 00:06:53,880
0.5 why should we use 0.5 the answer is

155
00:06:52,280 --> 00:06:56,440
you shouldn't and you should have some

156
00:06:53,880 --> 00:06:58,479
kind of rational procedure for choosing

157
00:06:56,440 --> 00:07:00,919
a decision threshold and there are many

158
00:06:58,479 --> 00:07:04,840
different procedures that people use one

159
00:07:00,919 --> 00:07:07,720
is called um the uden's J statistic

160
00:07:04,840 --> 00:07:10,680
which is the binary case of a more

161
00:07:07,720 --> 00:07:13,280
General metric uh called the bookmaker

162
00:07:10,680 --> 00:07:14,560
informedness and it's pretty simple it's

163
00:07:13,280 --> 00:07:17,080
uh the

164
00:07:14,560 --> 00:07:20,360
sensitivity plus the

165
00:07:17,080 --> 00:07:22,680
specificity minus one and so you ride

166
00:07:20,360 --> 00:07:24,840
your rock curve and you see which

167
00:07:22,680 --> 00:07:27,400
threshold correspond to the highest uden

168
00:07:24,840 --> 00:07:29,639
J statistic and now you've chosen a

169
00:07:27,400 --> 00:07:31,520
specific decision threshold and once you

170
00:07:29,639 --> 00:07:35,560
have a specific decision threshold well

171
00:07:31,520 --> 00:07:35,560
now you can draw out your confusion

172
00:07:36,160 --> 00:07:43,919
Matrix with your true positives true

173
00:07:40,080 --> 00:07:48,240
negatives false positives and false

174
00:07:43,919 --> 00:07:50,319
negatives you have your actual cases and

175
00:07:48,240 --> 00:07:54,400
controls your

176
00:07:50,319 --> 00:07:58,440
predicted cases and controls positive

177
00:07:54,400 --> 00:08:00,000
negative positive negative now there are

178
00:07:58,440 --> 00:08:01,840
many many metrics

179
00:08:00,000 --> 00:08:03,680
that are threshold based that you can

180
00:08:01,840 --> 00:08:05,680
then calculate from your confusion

181
00:08:03,680 --> 00:08:09,000
Matrix and one that I draw I want to

182
00:08:05,680 --> 00:08:12,240
draw your attention to is

183
00:08:09,000 --> 00:08:15,639
precision and precision is also known as

184
00:08:12,240 --> 00:08:19,039
the positive predictive value and this

185
00:08:15,639 --> 00:08:22,520
is corresponding to the true positives

186
00:08:19,039 --> 00:08:25,080
divided by the predicted positives and

187
00:08:22,520 --> 00:08:27,800
the reason why Precision is so important

188
00:08:25,080 --> 00:08:30,039
is that in medical contexts we are

189
00:08:27,800 --> 00:08:32,880
almost always dealing with highly

190
00:08:30,039 --> 00:08:34,880
imbalanced data it's usually a small

191
00:08:32,880 --> 00:08:37,560
minority of your data set that is part

192
00:08:34,880 --> 00:08:38,880
of the positive class such you know EG

193
00:08:37,560 --> 00:08:40,919
having the condition that you're trying

194
00:08:38,880 --> 00:08:43,719
to predict and when you have a high

195
00:08:40,919 --> 00:08:46,279
class imbalance this rock curve can be

196
00:08:43,719 --> 00:08:49,120
very very misleading so I'll give you a

197
00:08:46,279 --> 00:08:51,120
little worked out example let's say your

198
00:08:49,120 --> 00:08:54,959
sensitivity which is also your true

199
00:08:51,120 --> 00:08:57,760
positive rate is 0.9 and your

200
00:08:54,959 --> 00:09:00,760
specificity is also 0. n and these are

201
00:08:57,760 --> 00:09:03,920
characteristics of your model or your

202
00:09:00,760 --> 00:09:07,600
test or your predictor but Precision is

203
00:09:03,920 --> 00:09:10,519
also dependent on prevalence so let's

204
00:09:07,600 --> 00:09:12,640
say only 5% of the people are in the

205
00:09:10,519 --> 00:09:14,800
positive class so you can rewrite

206
00:09:12,640 --> 00:09:17,200
precision as a function of these three

207
00:09:14,800 --> 00:09:18,399
quantities and you can do it like this

208
00:09:17,200 --> 00:09:21,079
so

209
00:09:18,399 --> 00:09:23,720
ppv is your

210
00:09:21,079 --> 00:09:26,560
sensitivity times your

211
00:09:23,720 --> 00:09:30,200
prevalence divided by the same thing

212
00:09:26,560 --> 00:09:33,360
sensitivity times prevalence

213
00:09:30,200 --> 00:09:37,519
plus 1 minus your

214
00:09:33,360 --> 00:09:40,399
specificity times 1 minus your

215
00:09:37,519 --> 00:09:45,320
prevalence so if you work this out what

216
00:09:40,399 --> 00:09:49,240
you end up with is uh

217
00:09:45,320 --> 00:09:52,320
045 divided by

218
00:09:49,240 --> 00:09:55,120
045 uh plus

219
00:09:52,320 --> 00:10:01,640
0 uh

220
00:09:55,120 --> 00:10:03,560
95 which is about 0.3 to so again

221
00:10:01,640 --> 00:10:06,880
Precision answers the question out of

222
00:10:03,560 --> 00:10:09,760
who you predict is positive who's

223
00:10:06,880 --> 00:10:11,640
positive and the answer is 32% in this

224
00:10:09,760 --> 00:10:15,360
particular context because of the low

225
00:10:11,640 --> 00:10:16,920
prevalence which means 68% of the time

226
00:10:15,360 --> 00:10:20,079
when the model predicts that somebody

227
00:10:16,920 --> 00:10:22,079
has dementia they don't which is why

228
00:10:20,079 --> 00:10:25,240
prevalence is so important now the last

229
00:10:22,079 --> 00:10:27,680
thing I want to talk about is um one

230
00:10:25,240 --> 00:10:29,600
particular summary metric that very is

231
00:10:27,680 --> 00:10:32,399
very rarely reported but is a really

232
00:10:29,600 --> 00:10:34,399
convenient summary of a confusion Matrix

233
00:10:32,399 --> 00:10:36,200
so you can use a confusion mat Matrix to

234
00:10:34,399 --> 00:10:38,000
calculate uh rates that we're all

235
00:10:36,200 --> 00:10:40,720
probably pretty familiar with so when

236
00:10:38,000 --> 00:10:42,760
you divide um by the actual number of

237
00:10:40,720 --> 00:10:44,440
positives uh you get the true positive

238
00:10:42,760 --> 00:10:46,440
rate divide by the actual number of

239
00:10:44,440 --> 00:10:49,279
negatives the true negative rate so on

240
00:10:46,440 --> 00:10:51,279
and so forth now when you divide by the

241
00:10:49,279 --> 00:10:53,480
predicted number of positives and

242
00:10:51,279 --> 00:10:55,680
negatives here again you get the

243
00:10:53,480 --> 00:10:57,519
positive predicted value when you divide

244
00:10:55,680 --> 00:10:59,680
the true negatives by the predicted

245
00:10:57,519 --> 00:11:01,560
negatives you get the negative prediced

246
00:10:59,680 --> 00:11:03,240
value when you divide the false

247
00:11:01,560 --> 00:11:06,920
positives by the predicted positives you

248
00:11:03,240 --> 00:11:08,920
get the false Discovery rate and when

249
00:11:06,920 --> 00:11:11,399
you divide the false negatives by the

250
00:11:08,920 --> 00:11:13,120
predicted negatives you get the false

251
00:11:11,399 --> 00:11:15,760
Omission

252
00:11:13,120 --> 00:11:17,800
rate and now there's one metric that

253
00:11:15,760 --> 00:11:19,639
actually synthesizes all eight of these

254
00:11:17,800 --> 00:11:22,760
metrics into one number and it's called

255
00:11:19,639 --> 00:11:26,440
the Matthews correlation

256
00:11:22,760 --> 00:11:29,360
coefficient and uh it's essentially the

257
00:11:26,440 --> 00:11:30,600
uh positive predictive value times the

258
00:11:29,360 --> 00:11:33,600
true positive

259
00:11:30,600 --> 00:11:37,920
rate times the true negative

260
00:11:33,600 --> 00:11:41,839
rate times the negative predictive

261
00:11:37,920 --> 00:11:46,480
value square root then you subtract the

262
00:11:41,839 --> 00:11:49,279
um false Discovery rate times the false

263
00:11:46,480 --> 00:11:53,720
positive rate times the false negative

264
00:11:49,279 --> 00:11:56,839
rate finally times the false Omission

265
00:11:53,720 --> 00:12:00,519
rate and so this is just one very

266
00:11:56,839 --> 00:12:02,519
convenient metric that um can and enable

267
00:12:00,519 --> 00:12:03,920
researchers to more transparently report

268
00:12:02,519 --> 00:12:06,399
their results in clinical machine

269
00:12:03,920 --> 00:12:08,399
learning in which in those cases where

270
00:12:06,399 --> 00:12:10,680
you often have a highly imbalanced data

271
00:12:08,399 --> 00:12:14,360
set in which case the rock curve can be

272
00:12:10,680 --> 00:12:16,480
very misleading so uh in some um use

273
00:12:14,360 --> 00:12:18,800
rigorous Baseline sets of features to

274
00:12:16,480 --> 00:12:21,440
try and justify the use of complex data

275
00:12:18,800 --> 00:12:25,000
sources which might be very costly to um

276
00:12:21,440 --> 00:12:27,000
obtain um like GBM is a scalable very

277
00:12:25,000 --> 00:12:28,680
fast algorithm Auto auto machine

278
00:12:27,000 --> 00:12:30,360
learning packages like Flamel can um

279
00:12:28,680 --> 00:12:33,800
take the work out of optimizing

280
00:12:30,360 --> 00:12:36,360
hyperparameters and um transparent

281
00:12:33,800 --> 00:12:39,320
comprehensive metrics reporting is

282
00:12:36,360 --> 00:12:41,480
extremely important and uh should be

283
00:12:39,320 --> 00:12:43,480
done regularly often I see Precision

284
00:12:41,480 --> 00:12:45,000
values that are either never reported or

285
00:12:43,480 --> 00:12:46,760
they're relegated to the supplements of

286
00:12:45,000 --> 00:12:49,320
papers and it's a very serious problem

287
00:12:46,760 --> 00:12:49,320
and uh that's

288
00:12:49,760 --> 00:12:56,440
it so ban optimization is a paradigm for

289
00:12:53,839 --> 00:12:59,320
optimizing blackbox

290
00:12:56,440 --> 00:13:02,639
functions so maximizing or minimizing

291
00:12:59,320 --> 00:13:05,639
ing a blackbox function f now the thing

292
00:13:02,639 --> 00:13:06,800
with f is U it's hidden it's latent and

293
00:13:05,639 --> 00:13:09,440
we don't know the ground trth we don't

294
00:13:06,800 --> 00:13:11,320
know it's analytical form and neither so

295
00:13:09,440 --> 00:13:13,399
so we don't also have access to its

296
00:13:11,320 --> 00:13:15,839
gradients uh the only thing we have

297
00:13:13,399 --> 00:13:18,199
access to is a few very expensive

298
00:13:15,839 --> 00:13:20,519
evaluations of f so another thing about

299
00:13:18,199 --> 00:13:23,240
f is not only is it hidden or latent but

300
00:13:20,519 --> 00:13:25,800
it's very expensive to evaluate so the

301
00:13:23,240 --> 00:13:29,920
big caveat in Bay opt is that you have a

302
00:13:25,800 --> 00:13:33,079
budget that you can evaluate f

303
00:13:29,920 --> 00:13:36,079
only M

304
00:13:33,079 --> 00:13:38,639
times and M is potentially a very small

305
00:13:36,079 --> 00:13:40,519
number so uh you want to you want to you

306
00:13:38,639 --> 00:13:43,240
want to optimize this blackbox function

307
00:13:40,519 --> 00:13:46,519
f and the only thing you have access to

308
00:13:43,240 --> 00:13:48,959
is maybe a few evaluations of f so you

309
00:13:46,519 --> 00:13:50,959
have where you evaluated F and the

310
00:13:48,959 --> 00:13:53,959
evaluation and this is

311
00:13:50,959 --> 00:13:57,800
expensive right so one can think of this

312
00:13:53,959 --> 00:13:59,600
as sort of some setting where you know f

313
00:13:57,800 --> 00:14:01,720
ofx is like the free energy perturbation

314
00:13:59,600 --> 00:14:03,440
of a molecule or some kind of invivo

315
00:14:01,720 --> 00:14:06,360
trial outcome so something that's really

316
00:14:03,440 --> 00:14:09,199
expensive and you want to get you want

317
00:14:06,360 --> 00:14:11,800
to get an idea of where f is maximized

318
00:14:09,199 --> 00:14:14,480
or minimized only by using this tool of

319
00:14:11,800 --> 00:14:17,680
like you can only evaluate f m times and

320
00:14:14,480 --> 00:14:19,079
so Bean optimization is is a is sort of

321
00:14:17,680 --> 00:14:21,199
a methodology a general purpose

322
00:14:19,079 --> 00:14:26,000
methodology that you can kind of use to

323
00:14:21,199 --> 00:14:29,680
sort of um optimize functions F uh which

324
00:14:26,000 --> 00:14:32,199
are sort of a blackbox so uh the analogy

325
00:14:29,680 --> 00:14:33,920
that's actually used is um so imagine

326
00:14:32,199 --> 00:14:37,079
you know this is 1800s and you're on

327
00:14:33,920 --> 00:14:39,279
like a patch of finite land in Australia

328
00:14:37,079 --> 00:14:42,120
and you know that there's gold in two

329
00:14:39,279 --> 00:14:44,399
spots and you need to you can only like

330
00:14:42,120 --> 00:14:47,120
dig M times and you need to basically

331
00:14:44,399 --> 00:14:49,800
find gold or increase your probabilistic

332
00:14:47,120 --> 00:14:50,920
sort of probability of finding gold so

333
00:14:49,800 --> 00:14:52,639
how you're going to go about doing it

334
00:14:50,920 --> 00:14:54,839
obviously randomly digging in spots not

335
00:14:52,639 --> 00:14:56,680
going to be a good idea so I'm going to

336
00:14:54,839 --> 00:14:58,680
just like basically cast this into like

337
00:14:56,680 --> 00:15:03,839
a ond problem because I think that's the

338
00:14:58,680 --> 00:15:03,839
easiest to understand um

339
00:15:15,959 --> 00:15:20,920
so so imagine that this is my latent

340
00:15:18,880 --> 00:15:24,160
ground

341
00:15:20,920 --> 00:15:26,279
truth and I obviously don't know this

342
00:15:24,160 --> 00:15:28,639
but for the purposes of

343
00:15:26,279 --> 00:15:30,880
illustration and my points the two

344
00:15:28,639 --> 00:15:34,240
points whose expensive evaluations have

345
00:15:30,880 --> 00:15:38,040
access to kind of live here and

346
00:15:34,240 --> 00:15:41,199
here so um the way the algorithm works

347
00:15:38,040 --> 00:15:43,920
is that as a first step I would fit a

348
00:15:41,199 --> 00:15:48,000
surrogate model a probabilistic

349
00:15:43,920 --> 00:15:49,519
surrogate model to my two points and the

350
00:15:48,000 --> 00:15:52,199
Paradigm that I'm going to use is a

351
00:15:49,519 --> 00:15:54,240
gaussian process uh you can use other

352
00:15:52,199 --> 00:15:56,240
surrogate models to kind of fit these

353
00:15:54,240 --> 00:15:57,759
two points but uh I think it's going to

354
00:15:56,240 --> 00:16:00,480
become clear why you actually need a

355
00:15:57,759 --> 00:16:03,360
probabilistic model to fit uh these two

356
00:16:00,480 --> 00:16:05,600
points so this is basically just simple

357
00:16:03,360 --> 00:16:07,720
nonlinear regression setting you have

358
00:16:05,600 --> 00:16:10,079
two points you have X and F ofx and just

359
00:16:07,720 --> 00:16:11,480
fit a smooth function through it right

360
00:16:10,079 --> 00:16:13,560
so I'm going to use a gausin process and

361
00:16:11,480 --> 00:16:15,800
I'm going to fit a smooth sort of you

362
00:16:13,560 --> 00:16:17,360
know curve through these two points so

363
00:16:15,800 --> 00:16:19,920
I'm going to get

364
00:16:17,360 --> 00:16:21,560
something like this potentially right

365
00:16:19,920 --> 00:16:24,240
and I'm assuming that they're noisess

366
00:16:21,560 --> 00:16:25,800
but of course these points might have

367
00:16:24,240 --> 00:16:31,279
noise

368
00:16:25,800 --> 00:16:34,279
so the setting is Epsilon is IID noise

369
00:16:31,279 --> 00:16:38,720
and the thing with the gaussian process

370
00:16:34,279 --> 00:16:40,560
so f is modeled as a gaussian process

371
00:16:38,720 --> 00:16:42,680
and I'm not gonna I'm not going to talk

372
00:16:40,560 --> 00:16:44,480
a lot about how to fit a gaussian

373
00:16:42,680 --> 00:16:46,240
process uh but I'm going to say a little

374
00:16:44,480 --> 00:16:48,560
bit about it I mean you don't actually

375
00:16:46,240 --> 00:16:50,040
need to understand how aian process fit

376
00:16:48,560 --> 00:16:52,360
to data to kind of understand Bean

377
00:16:50,040 --> 00:16:55,839
optimization but you have great software

378
00:16:52,360 --> 00:16:57,959
to fit aian process and uh the thing

379
00:16:55,839 --> 00:16:59,880
about giian process say versus new

380
00:16:57,959 --> 00:17:02,920
networks is that a gin process is a

381
00:16:59,880 --> 00:17:04,760
probabilistic nonlinear way of fitting a

382
00:17:02,920 --> 00:17:06,280
data right in a regression setting so

383
00:17:04,760 --> 00:17:09,360
what you get when you fit a gin process

384
00:17:06,280 --> 00:17:11,160
is not just a single function estimate

385
00:17:09,360 --> 00:17:13,480
but what you get access to is actually a

386
00:17:11,160 --> 00:17:14,880
whole Space of functions compatible with

387
00:17:13,480 --> 00:17:17,799
the data so you actually get a

388
00:17:14,880 --> 00:17:19,880
probability distribution of curves or

389
00:17:17,799 --> 00:17:23,079
functions that are compatible with the

390
00:17:19,880 --> 00:17:26,079
data so I'm going to just draw this band

391
00:17:23,079 --> 00:17:26,079
here

392
00:17:30,679 --> 00:17:35,280
which really symbolizes that there are

393
00:17:32,520 --> 00:17:37,160
actually loads of functions in this in

394
00:17:35,280 --> 00:17:39,160
this space that are going to be

395
00:17:37,160 --> 00:17:41,840
compatible with the data and these are

396
00:17:39,160 --> 00:17:44,559
you can think of these as like maybe 95%

397
00:17:41,840 --> 00:17:47,640
confidence intervals right so in a gan

398
00:17:44,559 --> 00:17:49,960
process just a detour uh you have a

399
00:17:47,640 --> 00:17:51,559
prior which is a prior over a space of

400
00:17:49,960 --> 00:17:52,760
smooth functions and now the smoothness

401
00:17:51,559 --> 00:17:54,440
of the functions is controlled by the

402
00:17:52,760 --> 00:17:55,919
choice of the kernel function and then

403
00:17:54,440 --> 00:17:58,520
when you fit it to data so if You

404
00:17:55,919 --> 00:18:02,000
observe two data points here the space

405
00:17:58,520 --> 00:18:04,280
of functions collapses around the data

406
00:18:02,000 --> 00:18:07,400
so then you get

407
00:18:04,280 --> 00:18:09,919
functions that look like

408
00:18:07,400 --> 00:18:12,159
this so they pass through the data but

409
00:18:09,919 --> 00:18:14,280
they vary a lot where you don't see any

410
00:18:12,159 --> 00:18:18,080
data so you see data here and you don't

411
00:18:14,280 --> 00:18:21,120
see data there so the uncertainty bands

412
00:18:18,080 --> 00:18:23,280
around your data obviously collapse and

413
00:18:21,120 --> 00:18:24,960
then they kind of go the the uncertainty

414
00:18:23,280 --> 00:18:27,039
bands kind of widen out where you don't

415
00:18:24,960 --> 00:18:29,039
have data so this is a very very

416
00:18:27,039 --> 00:18:30,960
actually compelling property of using

417
00:18:29,039 --> 00:18:32,600
gausin processes is that they give you

418
00:18:30,960 --> 00:18:35,200
this uncertainty quantification by

419
00:18:32,600 --> 00:18:38,280
Design you get that for free when you

420
00:18:35,200 --> 00:18:40,640
fit a gaussin process to data right so

421
00:18:38,280 --> 00:18:42,480
uh so gaussin process is the surrogate

422
00:18:40,640 --> 00:18:44,760
that we're going to use to model our

423
00:18:42,480 --> 00:18:46,000
kind of expensive valuations but then

424
00:18:44,760 --> 00:18:47,120
why do we need this probabilistic

425
00:18:46,000 --> 00:18:50,039
Paradigm like what are we going to do

426
00:18:47,120 --> 00:18:52,480
with the uncertainty there so that's so

427
00:18:50,039 --> 00:18:54,600
now comes like the key part uh we're

428
00:18:52,480 --> 00:18:58,080
going to actually use the uncertainty to

429
00:18:54,600 --> 00:18:59,840
decide where to sample next right so the

430
00:18:58,080 --> 00:19:01,960
next thing that we do in Bas off so the

431
00:18:59,840 --> 00:19:04,320
first step is we fit a surrogate

432
00:19:01,960 --> 00:19:06,440
problemistic model to your expensive

433
00:19:04,320 --> 00:19:08,320
data points like here I just have two

434
00:19:06,440 --> 00:19:11,600
but you can have like you know maybe

435
00:19:08,320 --> 00:19:13,280
more in a real setting the next step is

436
00:19:11,600 --> 00:19:15,880
you compute something called an

437
00:19:13,280 --> 00:19:17,440
acquisition function right which is also

438
00:19:15,880 --> 00:19:20,840
something that you compute on your

439
00:19:17,440 --> 00:19:23,080
entire sort of input Space X and the

440
00:19:20,840 --> 00:19:25,039
acquisition function is something that

441
00:19:23,080 --> 00:19:27,280
is derived from the probability

442
00:19:25,039 --> 00:19:29,320
distribution that you fit to your data

443
00:19:27,280 --> 00:19:31,600
so in the most simplest setting one can

444
00:19:29,320 --> 00:19:33,200
just look at the uncertainty right so

445
00:19:31,600 --> 00:19:34,799
what happens to the uncertainty in the

446
00:19:33,200 --> 00:19:36,919
input region after you fit your

447
00:19:34,799 --> 00:19:40,000
surrogate problemistic model you can

448
00:19:36,919 --> 00:19:42,280
just plot it right so uh this would look

449
00:19:40,000 --> 00:19:44,720
you can just compute it and plot

450
00:19:42,280 --> 00:19:46,400
it just kind of goes up here so I have

451
00:19:44,720 --> 00:19:50,520
no data so the uncertainty potentially

452
00:19:46,400 --> 00:19:52,080
just is very high in this region and um

453
00:19:50,520 --> 00:19:53,640
the important thing is fitting a

454
00:19:52,080 --> 00:19:55,760
gaussian process is actually quite

455
00:19:53,640 --> 00:19:57,760
expensive so you can only do it to very

456
00:19:55,760 --> 00:19:59,799
few you can only fit a GP to like very

457
00:19:57,760 --> 00:20:02,240
few data points when I say few I mean

458
00:19:59,799 --> 00:20:04,360
you can go up like few thousands uh

459
00:20:02,240 --> 00:20:06,720
that's because the computational

460
00:20:04,360 --> 00:20:10,159
complexity of fitting a function f using

461
00:20:06,720 --> 00:20:12,960
a gaussian process is actually in

462
00:20:10,159 --> 00:20:16,840
Cube and the memory and it's quadratic

463
00:20:12,960 --> 00:20:18,799
in in the memory complexity so uh

464
00:20:16,840 --> 00:20:20,760
Computing the acquisition function is

465
00:20:18,799 --> 00:20:22,400
cheap fitting a gaussin process is

466
00:20:20,760 --> 00:20:24,720
non-trivial that's expensive but fitting

467
00:20:22,400 --> 00:20:26,520
an acquisition like I mean deriving like

468
00:20:24,720 --> 00:20:29,360
writing out the uh acquisition for

469
00:20:26,520 --> 00:20:31,600
computing the acquisition value across

470
00:20:29,360 --> 00:20:33,080
your input spaces is is is very cheap so

471
00:20:31,600 --> 00:20:35,120
that you should be able to do cheaply

472
00:20:33,080 --> 00:20:37,640
and you can do it across the whole sort

473
00:20:35,120 --> 00:20:39,240
of input domain so you you you kind of

474
00:20:37,640 --> 00:20:41,240
derive this acquisition function so

475
00:20:39,240 --> 00:20:43,320
which is like a scoring function it just

476
00:20:41,240 --> 00:20:45,640
indicates to you where you're going to

477
00:20:43,320 --> 00:20:47,200
sample next right so you just look at

478
00:20:45,640 --> 00:20:50,039
the acquisition function this 1D case

479
00:20:47,200 --> 00:20:51,679
and be like okay I mean it seems like

480
00:20:50,039 --> 00:20:55,039
this is a good point because that's

481
00:20:51,679 --> 00:20:57,480
where my score is maximized uh and here

482
00:20:55,039 --> 00:21:00,440
the acquisition function is just a proxy

483
00:20:57,480 --> 00:21:03,880
for the variance right right so assuming

484
00:21:00,440 --> 00:21:05,280
like this is basically you know your the

485
00:21:03,880 --> 00:21:07,080
width of your uncert it's it's appr

486
00:21:05,280 --> 00:21:09,760
proxy for the width of your uncertainty

487
00:21:07,080 --> 00:21:11,400
interal and then you can just say okay

488
00:21:09,760 --> 00:21:13,120
this is where I'm going to like place my

489
00:21:11,400 --> 00:21:17,760
next sample so this is where I'm going

490
00:21:13,120 --> 00:21:20,120
to evaluate X3 right and then the

491
00:21:17,760 --> 00:21:22,960
algorithm sort of proceeds so this is

492
00:21:20,120 --> 00:21:26,240
basically one Loop of your Bas op

493
00:21:22,960 --> 00:21:29,840
procedure and now your data set has

494
00:21:26,240 --> 00:21:29,840
three evaluations

495
00:21:38,799 --> 00:21:43,240
okay and as one can

496
00:21:44,440 --> 00:21:51,679
imagine so I need to just replicate the

497
00:21:47,679 --> 00:21:51,679
same function

498
00:22:00,840 --> 00:22:06,559
okay Point here I have a point here and

499
00:22:03,720 --> 00:22:09,000
then I evaluate somewhere here and then

500
00:22:06,559 --> 00:22:10,720
again I sort of fit my surrogate gausian

501
00:22:09,000 --> 00:22:14,320
process do my three expensive

502
00:22:10,720 --> 00:22:14,320
evaluations and I get something like

503
00:22:14,440 --> 00:22:20,960
this starting to get a lot better right

504
00:22:17,679 --> 00:22:25,400
so this is basically a primer like a

505
00:22:20,960 --> 00:22:27,640
really sort of um kind of vanilla

506
00:22:25,400 --> 00:22:29,159
Bop uh application in the

507
00:22:27,640 --> 00:22:31,679
one-dimensional case so you kind of do

508
00:22:29,159 --> 00:22:33,039
the same thing you fit your sarate GP to

509
00:22:31,679 --> 00:22:35,240
your very expensive valuations you

510
00:22:33,039 --> 00:22:36,640
compute your acquisition function and

511
00:22:35,240 --> 00:22:38,159
then you find out where you're going to

512
00:22:36,640 --> 00:22:39,440
sample next and then you add that to

513
00:22:38,159 --> 00:22:40,960
your data point and then that's how the

514
00:22:39,440 --> 00:22:44,720
algorithm proceeds it kind of alternates

515
00:22:40,960 --> 00:22:47,679
between these two steps um so um there's

516
00:22:44,720 --> 00:22:50,200
a lot of Rich theory behind acquisition

517
00:22:47,679 --> 00:22:52,039
functions right and I'm not going to

518
00:22:50,200 --> 00:22:54,240
have time to kind of go into the details

519
00:22:52,039 --> 00:22:56,919
of you know several of them but like one

520
00:22:54,240 --> 00:22:59,120
very important Concept in choosing good

521
00:22:56,919 --> 00:23:03,080
acquisition functions is this concept

522
00:22:59,120 --> 00:23:03,080
called exploration exploitation

523
00:23:14,240 --> 00:23:18,840
tradeoff so if you didn't have the

524
00:23:16,960 --> 00:23:20,960
acquisition function and you just had

525
00:23:18,840 --> 00:23:23,720
access to the surrogate model you can

526
00:23:20,960 --> 00:23:26,760
say okay you know I I think one strategy

527
00:23:23,720 --> 00:23:29,080
is exploit which means just sample

528
00:23:26,760 --> 00:23:31,720
aggressively over the points where you

529
00:23:29,080 --> 00:23:34,200
know the ground Truth uh hoping that

530
00:23:31,720 --> 00:23:35,200
maybe you know you're going to uh you're

531
00:23:34,200 --> 00:23:37,480
going to you're going to you're going to

532
00:23:35,200 --> 00:23:40,240
uncover something or or the other

533
00:23:37,480 --> 00:23:43,159
strategy just exploration is a just

534
00:23:40,240 --> 00:23:44,960
sample where the uncertainty is like the

535
00:23:43,159 --> 00:23:46,480
you know maximized so that's what we're

536
00:23:44,960 --> 00:23:49,080
doing in this acquisition function here

537
00:23:46,480 --> 00:23:50,840
which is just a proxy for the variance

538
00:23:49,080 --> 00:23:53,159
around the predictions right and the

539
00:23:50,840 --> 00:23:55,520
variance is basically how much the the

540
00:23:53,159 --> 00:23:57,360
space of functions actually vary in that

541
00:23:55,520 --> 00:23:59,880
region where you don't have the data but

542
00:23:57,360 --> 00:24:02,400
a really good AC position function will

543
00:23:59,880 --> 00:24:04,440
have that tradeoff so neither will it

544
00:24:02,400 --> 00:24:07,640
just look at NE neither would it tell

545
00:24:04,440 --> 00:24:08,960
you to just aggressively sample around a

546
00:24:07,640 --> 00:24:10,480
point that you already know the ground

547
00:24:08,960 --> 00:24:12,480
tooth off NE and neither would it just

548
00:24:10,480 --> 00:24:15,360
tell you to just go for explore which is

549
00:24:12,480 --> 00:24:17,000
just basically uh you know just sample

550
00:24:15,360 --> 00:24:19,279
somewhere where you you know which is

551
00:24:17,000 --> 00:24:21,120
like farthest like away from from from

552
00:24:19,279 --> 00:24:24,320
any of the points that you've uncovered

553
00:24:21,120 --> 00:24:26,120
so an example of an acquisition function

554
00:24:24,320 --> 00:24:28,799
that's

555
00:24:26,120 --> 00:24:32,120
used um looks something like this so

556
00:24:28,799 --> 00:24:34,919
it's it's got this Lambda parameter that

557
00:24:32,120 --> 00:24:36,919
controls this tradeoff and so all of the

558
00:24:34,919 --> 00:24:38,679
acquisition functions in literature that

559
00:24:36,919 --> 00:24:42,200
are actually popular have this kind of

560
00:24:38,679 --> 00:24:44,360
trade-off built into them um so I'm

561
00:24:42,200 --> 00:24:46,480
going to kind of like switch tones a

562
00:24:44,360 --> 00:24:49,200
little bit for the last few minutes to

563
00:24:46,480 --> 00:24:51,399
talk about how beijan optimization can

564
00:24:49,200 --> 00:24:53,880
be used in a generative model oh two

565
00:24:51,399 --> 00:24:55,840
minutes right um so so this going to be

566
00:24:53,880 --> 00:24:58,679
really quick um so of course this is

567
00:24:55,840 --> 00:25:00,520
just a contrived example like this is 1D

568
00:24:58,679 --> 00:25:01,960
uh you can also do beian optimization in

569
00:25:00,520 --> 00:25:03,520
the batch case where instead of just

570
00:25:01,960 --> 00:25:05,919
sampling one point one point you're

571
00:25:03,520 --> 00:25:07,559
doing like a batch of points uh but in

572
00:25:05,919 --> 00:25:10,440
the case of Genera models assume you

573
00:25:07,559 --> 00:25:12,600
have a generative a model for for for

574
00:25:10,440 --> 00:25:15,360
for small molecules right so you have

575
00:25:12,600 --> 00:25:16,760
like a vae like setup and you have a

576
00:25:15,360 --> 00:25:19,159
latent

577
00:25:16,760 --> 00:25:20,600
space which itself I mean it's it's

578
00:25:19,159 --> 00:25:23,440
supposed to be low dimensional but let's

579
00:25:20,600 --> 00:25:24,240
say it has about 10 or 15 dimensions and

580
00:25:23,440 --> 00:25:26,039
you

581
00:25:24,240 --> 00:25:28,799
have a

582
00:25:26,039 --> 00:25:31,159
decoder and say you know what you're

583
00:25:28,799 --> 00:25:31,159
actually

584
00:25:32,840 --> 00:25:39,559
encoding so this is the vae sort of

585
00:25:35,520 --> 00:25:39,559
autoencoder setup this is the latent

586
00:25:39,960 --> 00:25:45,200
space and so each molecule that you

587
00:25:42,679 --> 00:25:47,360
train with this vae setup is represented

588
00:25:45,200 --> 00:25:49,399
as a compressed point in this low

589
00:25:47,360 --> 00:25:51,480
dimensional space uh this low

590
00:25:49,399 --> 00:25:53,159
dimensional latent space right so each

591
00:25:51,480 --> 00:25:55,080
each point here represents a point in

592
00:25:53,159 --> 00:25:56,679
your training data and and assume that

593
00:25:55,080 --> 00:25:58,559
these are like you know small molecules

594
00:25:56,679 --> 00:26:01,039
so drug modality and you only have

595
00:25:58,559 --> 00:26:03,080
access to some really expensive property

596
00:26:01,039 --> 00:26:05,360
like some kind of maybe a sort of free

597
00:26:03,080 --> 00:26:05,360
energy

598
00:26:06,080 --> 00:26:12,159
peration space using base

599
00:26:09,600 --> 00:26:14,640
opt uh such that maybe you want to sort

600
00:26:12,159 --> 00:26:17,399
of maximize or minimize this property

601
00:26:14,640 --> 00:26:18,880
right um so you only have access these

602
00:26:17,399 --> 00:26:20,279
two points you what you can do is

603
00:26:18,880 --> 00:26:21,760
essentially fit a gausian process to

604
00:26:20,279 --> 00:26:23,559
this High dimensional setup so this is a

605
00:26:21,760 --> 00:26:27,200
1D example but you can fit a gin process

606
00:26:23,559 --> 00:26:28,960
to any dimensional data and uh

607
00:26:27,200 --> 00:26:30,559
essentially do the same thing so instead

608
00:26:28,960 --> 00:26:32,200
of like fitting a gausin this this would

609
00:26:30,559 --> 00:26:33,760
be like this would be essentially

610
00:26:32,200 --> 00:26:36,039
fitting a gausin process to a high

611
00:26:33,760 --> 00:26:38,640
dimensional sort of surface and then

612
00:26:36,039 --> 00:26:40,760
using kind of the same Machinery here to

613
00:26:38,640 --> 00:26:43,840
navigate the latent space of this

614
00:26:40,760 --> 00:26:46,360
generative model to keep then coming up

615
00:26:43,840 --> 00:26:49,000
with proposals that you can then decode

616
00:26:46,360 --> 00:26:52,399
to get your small molecule of Interest

617
00:26:49,000 --> 00:26:54,520
right so this is how um essentially Bas

618
00:26:52,399 --> 00:26:55,960
opt is actually used in modern day

619
00:26:54,520 --> 00:26:57,919
science a lot of the times they not it's

620
00:26:55,960 --> 00:27:00,720
not just used to optimize some blackbox

621
00:26:57,919 --> 00:27:02,399
function like in a in a vanilla setting

622
00:27:00,720 --> 00:27:05,559
but a lot of times it's actually used

623
00:27:02,399 --> 00:27:07,559
it's embedded as an auxiliary algorithm

624
00:27:05,559 --> 00:27:09,679
in the latent space of generative models

625
00:27:07,559 --> 00:27:10,840
to actually come up with interesting

626
00:27:09,679 --> 00:27:12,720
proposals where you're kind of

627
00:27:10,840 --> 00:27:15,159
maximizing or minimizing a certain

628
00:27:12,720 --> 00:27:15,159
property of

629
00:27:16,840 --> 00:27:21,039
Interest the the problem that we're

630
00:27:18,720 --> 00:27:23,039
interested in is how do cells

631
00:27:21,039 --> 00:27:24,720
compartmentalize proteins and by

632
00:27:23,039 --> 00:27:26,360
compartmentalizing proteins into

633
00:27:24,720 --> 00:27:28,159
different compartments and there is a

634
00:27:26,360 --> 00:27:30,080
variety of these some of these are are

635
00:27:28,159 --> 00:27:32,039
membrane bound like the nucleus some of

636
00:27:30,080 --> 00:27:33,440
them are contained within that

637
00:27:32,039 --> 00:27:36,320
compartment and and I'm going to

638
00:27:33,440 --> 00:27:38,720
highlight two here this is the nucleis

639
00:27:36,320 --> 00:27:40,679
I'm drawing it quite large here and then

640
00:27:38,720 --> 00:27:42,960
uh you can also have compartments called

641
00:27:40,679 --> 00:27:44,559
uh transcriptional condensates and both

642
00:27:42,960 --> 00:27:46,240
these function and different

643
00:27:44,559 --> 00:27:48,399
physiological processes so they need to

644
00:27:46,240 --> 00:27:52,279
concentrate different types of molecules

645
00:27:48,399 --> 00:27:55,440
um to do their function um and what we

646
00:27:52,279 --> 00:27:57,600
know is that um these

647
00:27:55,440 --> 00:27:59,679
compartments have different proteins

648
00:27:57,600 --> 00:28:01,200
that can be concentrated in them um

649
00:27:59,679 --> 00:28:04,200
there's different types of scaffolding

650
00:28:01,200 --> 00:28:07,200
molecules that accumulate in there and

651
00:28:04,200 --> 00:28:08,960
they're not all different so some of

652
00:28:07,200 --> 00:28:11,880
them there can be an overlap between

653
00:28:08,960 --> 00:28:14,159
these proteins um but we want to

654
00:28:11,880 --> 00:28:16,240
understand what is the chemistry that

655
00:28:14,159 --> 00:28:18,000
makes them distinct and and protein

656
00:28:16,240 --> 00:28:20,600
language models have provided us the

657
00:28:18,000 --> 00:28:22,279
ability to now go ahead and do this um

658
00:28:20,600 --> 00:28:24,720
and the work that I'm going to describe

659
00:28:22,279 --> 00:28:26,640
to you as an example of how one might so

660
00:28:24,720 --> 00:28:28,679
uh we took 12

661
00:28:26,640 --> 00:28:30,960
compartments um throughout the

662
00:28:28,679 --> 00:28:32,919
throughout the cell uh this includes

663
00:28:30,960 --> 00:28:35,440
diverse physiological processes such as

664
00:28:32,919 --> 00:28:38,760
gene expression the compartmentalization

665
00:28:35,440 --> 00:28:40,360
of stress uh cognition all of these

666
00:28:38,760 --> 00:28:43,159
processes transcend through these

667
00:28:40,360 --> 00:28:46,440
different bodies we have about 5,000 or

668
00:28:43,159 --> 00:28:46,440
so protein

669
00:28:47,120 --> 00:28:52,399
sequences um and we can use this

670
00:28:49,399 --> 00:28:53,919
information as input um so we can we

671
00:28:52,399 --> 00:28:57,120
actually what we did specifically is we

672
00:28:53,919 --> 00:28:58,919
took esm2 and with esm2 we have a

673
00:28:57,120 --> 00:29:00,399
representation of prot sequences that we

674
00:28:58,919 --> 00:29:04,960
can generate for each and every one of

675
00:29:00,399 --> 00:29:07,880
these take esm2 U this model we feed it

676
00:29:04,960 --> 00:29:12,440
in our protein sequences and we get out

677
00:29:07,880 --> 00:29:14,200
a a big Matrix n by 320 in this case so

678
00:29:12,440 --> 00:29:16,240
n is the length of that protein sequence

679
00:29:14,200 --> 00:29:19,320
and we can then sum this up and get an

680
00:29:16,240 --> 00:29:22,600
embedding and this will give us a vector

681
00:29:19,320 --> 00:29:25,919
that we can then go and use to uh

682
00:29:22,600 --> 00:29:27,600
fine-tuned esm to to understand what is

683
00:29:25,919 --> 00:29:29,200
different between all of these different

684
00:29:27,600 --> 00:29:32,679
compartments so we add a couple of

685
00:29:29,200 --> 00:29:35,760
additional layers onto um our

686
00:29:32,679 --> 00:29:38,480
model onto this large language model and

687
00:29:35,760 --> 00:29:40,679
now when we feed it an unknown protein

688
00:29:38,480 --> 00:29:42,480
sequence um it will spit out the

689
00:29:40,679 --> 00:29:44,640
probability of a protein being

690
00:29:42,480 --> 00:29:47,519
concentrated in one of the 12

691
00:29:44,640 --> 00:29:50,080
compartments that we looked at and what

692
00:29:47,519 --> 00:29:53,840
we found is that uh this actually works

693
00:29:50,080 --> 00:29:56,559
pretty dang well so uh for our test set

694
00:29:53,840 --> 00:29:59,279
um the prediction

695
00:29:56,559 --> 00:30:03,919
probabilities um

696
00:29:59,279 --> 00:30:03,919
were uh we pretty accurate

697
00:30:04,240 --> 00:30:09,399
um great so U that that to us is

698
00:30:07,640 --> 00:30:10,960
something of uh at least we're getting

699
00:30:09,399 --> 00:30:13,240
at some of the chemical information

700
00:30:10,960 --> 00:30:14,960
inside of protein sequences that that

701
00:30:13,240 --> 00:30:16,840
lead them to be compartmentalize in

702
00:30:14,960 --> 00:30:18,600
different parts of the cell and what we

703
00:30:16,840 --> 00:30:20,120
wanted to do was then go and demonstrate

704
00:30:18,600 --> 00:30:22,640
this so we want to take this this

705
00:30:20,120 --> 00:30:24,600
ability to uh figure out whether or not

706
00:30:22,640 --> 00:30:26,600
a particular peptide is associated with

707
00:30:24,600 --> 00:30:28,559
one compartment or another compartment

708
00:30:26,600 --> 00:30:30,240
we wanted to use this as a tool to

709
00:30:28,559 --> 00:30:32,320
actually make something that we can go

710
00:30:30,240 --> 00:30:34,000
put into a cell and look at using a

711
00:30:32,320 --> 00:30:35,480
fancy microscope and see if it's going

712
00:30:34,000 --> 00:30:37,840
into the right compartment or the wrong

713
00:30:35,480 --> 00:30:40,039
compartment or what's happening so what

714
00:30:37,840 --> 00:30:43,039
we did was we

715
00:30:40,039 --> 00:30:44,480
uh we took this model and by the way I I

716
00:30:43,039 --> 00:30:47,080
should have said this earlier but we

717
00:30:44,480 --> 00:30:51,200
decided to call it pro GPS uh sort of a

718
00:30:47,080 --> 00:30:54,440
GPS for your protein um and the idea was

719
00:30:51,200 --> 00:30:56,760
to take this uh Pro GPS and and think

720
00:30:54,440 --> 00:30:59,399
about ways that we could take a protein

721
00:30:56,760 --> 00:31:02,039
so proteins can have different sorts of

722
00:30:59,399 --> 00:31:04,720
properties to it um you know one one

723
00:31:02,039 --> 00:31:07,960
type of domain on a protein is a folded

724
00:31:04,720 --> 00:31:10,399
domain and another is a disordered

725
00:31:07,960 --> 00:31:12,399
domain or an intrinsically disordered

726
00:31:10,399 --> 00:31:16,519
region and this is a structure that

727
00:31:12,399 --> 00:31:19,600
doesn't really um adopt a uh a certain

728
00:31:16,519 --> 00:31:20,919
one really uh highly occupied structure

729
00:31:19,600 --> 00:31:22,559
so it'll sample a lot of different

730
00:31:20,919 --> 00:31:24,679
states so it's very Dynamic it's

731
00:31:22,559 --> 00:31:26,720
changing they're actually incredibly

732
00:31:24,679 --> 00:31:29,639
useful in biology for doing an array of

733
00:31:26,720 --> 00:31:32,080
different functions and um what we

734
00:31:29,639 --> 00:31:34,159
wanted to do was add on a region onto a

735
00:31:32,080 --> 00:31:35,639
protein sequence that then would uh

736
00:31:34,159 --> 00:31:38,159
cause it to go into a specific

737
00:31:35,639 --> 00:31:40,080
compartment so our first approach was uh

738
00:31:38,159 --> 00:31:43,519
to use a greedy search based algorithm

739
00:31:40,080 --> 00:31:45,799
to try and model this process so we uh

740
00:31:43,519 --> 00:31:50,080
actually failed spectacularly with this

741
00:31:45,799 --> 00:31:52,600
one um but it worked in the sense that

742
00:31:50,080 --> 00:31:54,919
when we we looked at the data um the

743
00:31:52,600 --> 00:31:56,799
experimental data we could actually see

744
00:31:54,919 --> 00:31:58,000
the proteins were moving around so we we

745
00:31:56,799 --> 00:32:01,200
tapped into some of this information

746
00:31:58,000 --> 00:32:02,639
inform and uh but there there is

747
00:32:01,200 --> 00:32:04,799
problems and some of the problems were

748
00:32:02,639 --> 00:32:07,919
things like 13 tryptophan residues in a

749
00:32:04,799 --> 00:32:09,480
row that that's bad um you you will

750
00:32:07,919 --> 00:32:11,399
cause aggregation all kinds of things

751
00:32:09,480 --> 00:32:13,639
like this and so we we knew we had some

752
00:32:11,399 --> 00:32:16,840
of the right sauce but we weren't quite

753
00:32:13,639 --> 00:32:19,559
there yet um so in our second approach

754
00:32:16,840 --> 00:32:21,840
um and these are all sort of generative

755
00:32:19,559 --> 00:32:26,039
strategies uh for

756
00:32:21,840 --> 00:32:29,080
testing um protein codes

757
00:32:26,039 --> 00:32:32,039
really uh so in our second strategy uh

758
00:32:29,080 --> 00:32:34,320
we did a marov chain Monte Carlo blocked

759
00:32:32,039 --> 00:32:35,440
Gibbs sampling procedure and so I'm tell

760
00:32:34,320 --> 00:32:37,360
you a little bit about the energy

761
00:32:35,440 --> 00:32:39,840
function that we used and and I think

762
00:32:37,360 --> 00:32:42,120
that this uh this approach really

763
00:32:39,840 --> 00:32:44,760
encapsulates u a variety of different

764
00:32:42,120 --> 00:32:46,600
approaches for sampling chemical space

765
00:32:44,760 --> 00:32:48,720
and protein sequences that isn't

766
00:32:46,600 --> 00:32:51,760
critical uh for getting something to the

767
00:32:48,720 --> 00:32:54,039
right place so um as a whole this looks

768
00:32:51,760 --> 00:32:57,080
something like um and and this is

769
00:32:54,039 --> 00:33:00,519
building really upon um the work that

770
00:32:57,080 --> 00:33:02,840
Reeves and co-work workers put out um so

771
00:33:00,519 --> 00:33:05,919
there's a projection term and so this

772
00:33:02,840 --> 00:33:08,240
refers to the uh the ability of a

773
00:33:05,919 --> 00:33:10,639
protein amino acid sequence to match a

774
00:33:08,240 --> 00:33:13,600
particular backbone we have a a language

775
00:33:10,639 --> 00:33:15,159
model term so what that really is is you

776
00:33:13,600 --> 00:33:17,639
know is this protein sequence that we're

777
00:33:15,159 --> 00:33:20,519
sampling found in nature and esm2 if

778
00:33:17,639 --> 00:33:22,320
you're not familiar is uh trained on 60

779
00:33:20,519 --> 00:33:25,480
million protein sequences so it learns

780
00:33:22,320 --> 00:33:28,559
something about that um the other terms

781
00:33:25,480 --> 00:33:30,799
in here uh convert is sort of a prior so

782
00:33:28,559 --> 00:33:34,639
this is an engram version where we have

783
00:33:30,799 --> 00:33:37,480
different small um small sets of amino

784
00:33:34,639 --> 00:33:41,399
acids and asking how does our protein SE

785
00:33:37,480 --> 00:33:44,960
sampling match that um and the and the

786
00:33:41,399 --> 00:33:47,799
The Innovation here that we added in was

787
00:33:44,960 --> 00:33:53,760
was two different things so um we have

788
00:33:47,799 --> 00:33:56,679
Pro GPS and um this component of model

789
00:33:53,760 --> 00:33:59,399
um tells us to well what we're going to

790
00:33:56,679 --> 00:34:01,240
do is sample subsequences of proteins or

791
00:33:59,399 --> 00:34:03,440
or sequences of proteins that are are

792
00:34:01,240 --> 00:34:05,360
found in the Target compartment so we

793
00:34:03,440 --> 00:34:07,200
we'll sample chemistry that you might

794
00:34:05,360 --> 00:34:09,879
actually find in that specific

795
00:34:07,200 --> 00:34:12,960
condensate and uh the other term that we

796
00:34:09,879 --> 00:34:16,440
have here is one that enforces

797
00:34:12,960 --> 00:34:17,679
disorder and the uh disorder term so

798
00:34:16,440 --> 00:34:20,320
going back to this right we're we're

799
00:34:17,679 --> 00:34:22,599
adding on a new domain and the sequences

800
00:34:20,320 --> 00:34:25,839
we want to sample are going to be you

801
00:34:22,599 --> 00:34:28,079
know disordered fractions of this or uh

802
00:34:25,839 --> 00:34:30,520
dis or are these components and we want

803
00:34:28,079 --> 00:34:32,720
to enforce a disorder condition so that

804
00:34:30,520 --> 00:34:34,520
this this fraction of the protein has a

805
00:34:32,720 --> 00:34:37,320
higher propensity to be disordered and

806
00:34:34,520 --> 00:34:39,440
therefore less likely to influence the

807
00:34:37,320 --> 00:34:41,440
structure of the folded domain or

808
00:34:39,440 --> 00:34:45,159
influence the other properties of the

809
00:34:41,440 --> 00:34:48,440
protein it's more of a sort of a tuning

810
00:34:45,159 --> 00:34:51,079
of that of that protein um so when we

811
00:34:48,440 --> 00:34:53,599
did this what we found was that um

812
00:34:51,079 --> 00:34:55,839
depending on on how litigious you'd like

813
00:34:53,599 --> 00:34:59,000
to be about this we could get between 10

814
00:34:55,839 --> 00:35:01,359
and and 100% performance out a the small

815
00:34:59,000 --> 00:35:03,119
subset of of sequences that we looked at

816
00:35:01,359 --> 00:35:05,800
so you know we we went back to the

817
00:35:03,119 --> 00:35:08,880
nucleis and and this is a a great

818
00:35:05,800 --> 00:35:11,839
example it's a very large condensate and

819
00:35:08,880 --> 00:35:14,400
so it it assembles material inside of it

820
00:35:11,839 --> 00:35:16,960
um and it's very easy to observe so

821
00:35:14,400 --> 00:35:19,720
experimentally you know most of the

822
00:35:16,960 --> 00:35:22,599
sequences uh that we got into this one

823
00:35:19,720 --> 00:35:24,280
um 10 out of 10 were a technical win and

824
00:35:22,599 --> 00:35:25,839
that when we compared it to a control

825
00:35:24,280 --> 00:35:28,480
protein all of them were doing better

826
00:35:25,839 --> 00:35:29,960
than that control uh and and then we

827
00:35:28,480 --> 00:35:32,240
started looking at these other smaller

828
00:35:29,960 --> 00:35:34,560
bodies and what we found was actually

829
00:35:32,240 --> 00:35:36,200
quite interesting um we were able to get

830
00:35:34,560 --> 00:35:37,400
partitioning into these but they're not

831
00:35:36,200 --> 00:35:39,079
all the same so there's something

832
00:35:37,400 --> 00:35:41,280
stochastic about the way these things

833
00:35:39,079 --> 00:35:42,800
assemble into different condensates and

834
00:35:41,280 --> 00:35:44,200
and the material that accumulates in

835
00:35:42,800 --> 00:35:46,640
them is not always going to be the same

836
00:35:44,200 --> 00:35:48,839
between one compartment or another so

837
00:35:46,640 --> 00:35:50,319
when we went and trained this model on

838
00:35:48,839 --> 00:35:52,040
all of this data we were getting

839
00:35:50,319 --> 00:35:54,680
different flavors of different condens

840
00:35:52,040 --> 00:35:56,319
so we understand that uh the the

841
00:35:54,680 --> 00:35:58,359
imperfections in this model in our

842
00:35:56,319 --> 00:36:00,240
approach is now kind of of a

843
00:35:58,359 --> 00:36:02,240
relationship about well how good is that

844
00:36:00,240 --> 00:36:05,280
data how good is that the approach that

845
00:36:02,240 --> 00:36:06,880
we took to experimentally build um a

846
00:36:05,280 --> 00:36:09,280
model of each of these

847
00:36:06,880 --> 00:36:10,920
compartments okay so you know we could

848
00:36:09,280 --> 00:36:13,119
we could test these things out

849
00:36:10,920 --> 00:36:15,000
experimentally um I think we learned

850
00:36:13,119 --> 00:36:17,400
quite a bit about how to generate

851
00:36:15,000 --> 00:36:19,760
proteins using a fine-tune model like

852
00:36:17,400 --> 00:36:21,599
this and and things to consider you know

853
00:36:19,760 --> 00:36:23,720
the the chemical space of proteins was

854
00:36:21,599 --> 00:36:25,880
really bottlenecked in this strategy

855
00:36:23,720 --> 00:36:28,160
like we were thinking about protein

856
00:36:25,880 --> 00:36:30,440
sequences that are found in nature we

857
00:36:28,160 --> 00:36:31,960
were thinking about um the types of

858
00:36:30,440 --> 00:36:33,560
sequences that are specific to that

859
00:36:31,960 --> 00:36:35,560
compartment and then we were also

860
00:36:33,560 --> 00:36:38,000
thinking about protein sequences and

861
00:36:35,560 --> 00:36:40,000
features that are associated with a

862
00:36:38,000 --> 00:36:42,160
specific type of domain that wouldn't

863
00:36:40,000 --> 00:36:45,280
alter the overall structure of that of

864
00:36:42,160 --> 00:36:47,319
that protein um and if I have time the

865
00:36:45,280 --> 00:36:49,960
the last thing I will I'll tell you

866
00:36:47,319 --> 00:36:50,839
about is how we then took this Tool uh

867
00:36:49,960 --> 00:36:53,599
Pro

868
00:36:50,839 --> 00:36:55,319
GPS and and started asking questions

869
00:36:53,599 --> 00:36:57,760
about well you know if you have a

870
00:36:55,319 --> 00:36:59,720
protein sequence and you have its mut

871
00:36:57,760 --> 00:37:01,359
form that's found in disease how are

872
00:36:59,720 --> 00:37:03,480
those two things different and how is

873
00:37:01,359 --> 00:37:04,640
the information content and the wild

874
00:37:03,480 --> 00:37:06,200
type protein different from the

875
00:37:04,640 --> 00:37:08,280
information content in the mutant

876
00:37:06,200 --> 00:37:11,160
protein and can we use that to learn

877
00:37:08,280 --> 00:37:13,040
something about how how nature uh

878
00:37:11,160 --> 00:37:14,880
manifests information that drives the

879
00:37:13,040 --> 00:37:17,520
distribution of proteins and so this is

880
00:37:14,880 --> 00:37:18,480
what we did we took something like um I

881
00:37:17,520 --> 00:37:19,760
I don't know we've actually ran

882
00:37:18,480 --> 00:37:22,520
something like 300,000 different

883
00:37:19,760 --> 00:37:25,319
proteins that are found in disease and

884
00:37:22,520 --> 00:37:27,880
and or have an uncertain prediction or a

885
00:37:25,319 --> 00:37:30,000
u a benign characteristic but they're

886
00:37:27,880 --> 00:37:33,400
all mutated and we asked you know we

887
00:37:30,000 --> 00:37:35,480
took the wild type and the mutant and we

888
00:37:33,400 --> 00:37:38,480
pushed this through this algorithm um

889
00:37:35,480 --> 00:37:40,400
what is the what is the distance of the

890
00:37:38,480 --> 00:37:44,599
you know distribution of probabilities

891
00:37:40,400 --> 00:37:47,119
for the wild type and mutant um so to

892
00:37:44,599 --> 00:37:48,960
the washer distance of this and what we

893
00:37:47,119 --> 00:37:50,720
found is that you know if you look at

894
00:37:48,960 --> 00:37:53,800
the data for this there's there's

895
00:37:50,720 --> 00:37:55,680
something of a you know a a histogram

896
00:37:53,800 --> 00:37:58,040
that you could draw where there are some

897
00:37:55,680 --> 00:37:59,400
that you know have a very have no effect

898
00:37:58,040 --> 00:38:00,760
essentially so you're mutating some part

899
00:37:59,400 --> 00:38:03,119
of the protein that doesn't change its

900
00:38:00,760 --> 00:38:05,200
distribution okay I I completely buy

901
00:38:03,119 --> 00:38:06,960
that this that's actually an expectation

902
00:38:05,200 --> 00:38:08,880
right if you you mutate a catalytic

903
00:38:06,960 --> 00:38:10,400
residue then perhaps you shouldn't alter

904
00:38:08,880 --> 00:38:12,599
the distribution of protein maybe you

905
00:38:10,400 --> 00:38:14,000
should but if you mutate something on

906
00:38:12,599 --> 00:38:16,040
the surface or something that's

907
00:38:14,000 --> 00:38:18,200
important for the the fold of a protein

908
00:38:16,040 --> 00:38:20,960
and now that protein is opened up like a

909
00:38:18,200 --> 00:38:22,319
a flower in the H Springtime then it's

910
00:38:20,960 --> 00:38:24,920
going to be able to interact with many

911
00:38:22,319 --> 00:38:27,480
different parts of the cell and so that

912
00:38:24,920 --> 00:38:29,680
surface area is exposed and changes a

913
00:38:27,480 --> 00:38:31,319
dist of the protein so absolutely you

914
00:38:29,680 --> 00:38:32,000
know a lot of these are going to alter

915
00:38:31,319 --> 00:38:34,560
the

916
00:38:32,000 --> 00:38:36,480
distribution and what we found is that

917
00:38:34,560 --> 00:38:38,640
uh for pathogenic mutants we could then

918
00:38:36,480 --> 00:38:41,119
go and experimentally investigate these

919
00:38:38,640 --> 00:38:43,560
guys and we would see really dramatic

920
00:38:41,119 --> 00:38:45,480
differences between what we would find

921
00:38:43,560 --> 00:38:48,240
for the wild type protein and the mutant

922
00:38:45,480 --> 00:38:49,880
protein so they really are moving around

923
00:38:48,240 --> 00:38:52,240
and what are the things that are

924
00:38:49,880 --> 00:38:54,839
important for this um it's actually

925
00:38:52,240 --> 00:38:56,839
quite incredible that you have the same

926
00:38:54,839 --> 00:39:00,079
magnitude of effect coming from the

927
00:38:56,839 --> 00:39:01,880
folded domain from the IDR domain so you

928
00:39:00,079 --> 00:39:05,400
could have something that is just a you

929
00:39:01,880 --> 00:39:07,079
know an sort of a uh a disordered um

930
00:39:05,400 --> 00:39:09,720
confirmational Ensemble of states that

931
00:39:07,079 --> 00:39:11,359
is a similar degree of effect that

932
00:39:09,720 --> 00:39:14,440
something on the folded domain could

933
00:39:11,359 --> 00:39:16,280
have um this uh this is really possible

934
00:39:14,440 --> 00:39:17,720
because we were able to or all of these

935
00:39:16,280 --> 00:39:19,720
you know findings were really possible

936
00:39:17,720 --> 00:39:22,200
because we were able to take the data

937
00:39:19,720 --> 00:39:23,880
from um many different protein sequences

938
00:39:22,200 --> 00:39:25,680
and learn all of the connections between

939
00:39:23,880 --> 00:39:27,560
the folded Andor disordered domains

940
00:39:25,680 --> 00:39:29,760
simultaneously we weren't you know

941
00:39:27,560 --> 00:39:32,079
pigeon hold into what biochemists have

942
00:39:29,760 --> 00:39:34,040
been doing for uh the last 100 years or

943
00:39:32,079 --> 00:39:36,119
so which is looking at individual motifs

944
00:39:34,040 --> 00:39:38,560
and and pieces of proteins we can learn

945
00:39:36,119 --> 00:39:40,960
all the relationships at once uh using

946
00:39:38,560 --> 00:39:43,160
these Transformer models and uh I think

947
00:39:40,960 --> 00:39:44,800
it's a powerful tool for now moving

948
00:39:43,160 --> 00:39:47,720
forward to understand you know how can

949
00:39:44,800 --> 00:39:49,000
we build small molecules and proteins um

950
00:39:47,720 --> 00:39:50,040
that are are going to go to the right

951
00:39:49,000 --> 00:39:52,839
place in the

952
00:39:50,040 --> 00:39:56,400
cell and and help them optimize their

953
00:39:52,839 --> 00:39:56,400
therapeutic activity

954
00:39:58,000 --> 00:40:01,400
today I'd like to tell you a little bit

955
00:40:00,040 --> 00:40:04,760
about Alpha

956
00:40:01,400 --> 00:40:06,760
flow which is a deep learning method

957
00:40:04,760 --> 00:40:09,200
that we've developed to try to model

958
00:40:06,760 --> 00:40:12,119
protein confirmational ensembles given a

959
00:40:09,200 --> 00:40:13,920
protein sequence so to set the context

960
00:40:12,119 --> 00:40:15,359
you've likely all heard of alpha fold

961
00:40:13,920 --> 00:40:17,400
which addresses the protein structure

962
00:40:15,359 --> 00:40:19,240
prediction problem given an input

963
00:40:17,400 --> 00:40:20,920
sequence it produces the most

964
00:40:19,240 --> 00:40:23,680
representative output structure for that

965
00:40:20,920 --> 00:40:25,800
sequence so pictorially we can think of

966
00:40:23,680 --> 00:40:28,560
proteins as living in these uh in this

967
00:40:25,800 --> 00:40:30,920
dual space sequence space and a

968
00:40:28,560 --> 00:40:34,760
structure space and um throughout please

969
00:40:30,920 --> 00:40:34,760
forgive my terrible protein

970
00:40:35,040 --> 00:40:40,760
drawings and alphafold has learned a

971
00:40:37,480 --> 00:40:42,599
mapping from this space to that now of

972
00:40:40,760 --> 00:40:45,079
course this is a slight simplification

973
00:40:42,599 --> 00:40:47,800
of reality as Henry already alluded to

974
00:40:45,079 --> 00:40:49,880
in his last talk proteins do not adopt a

975
00:40:47,800 --> 00:40:51,480
single structure rather an ensemble

976
00:40:49,880 --> 00:40:53,800
structures depending on the binding

977
00:40:51,480 --> 00:40:56,280
Partners the molecular environment the

978
00:40:53,800 --> 00:40:58,599
pH the ionic strength and of course just

979
00:40:56,280 --> 00:41:00,480
regular thermal fluctu that cause atoms

980
00:40:58,599 --> 00:41:02,119
to constantly move and jiggle around as

981
00:41:00,480 --> 00:41:04,520
an intrinsically disordered regions or

982
00:41:02,119 --> 00:41:06,359
proteins so instead of a single

983
00:41:04,520 --> 00:41:08,400
welldefined you know value for this

984
00:41:06,359 --> 00:41:10,440
function we actually have u a

985
00:41:08,400 --> 00:41:11,520
distribution of structures possibly with

986
00:41:10,440 --> 00:41:14,200
multiple

987
00:41:11,520 --> 00:41:16,560
modes and Alpha fold has done an

988
00:41:14,200 --> 00:41:18,599
excellent job of learning a

989
00:41:16,560 --> 00:41:20,119
representative structure among this

990
00:41:18,599 --> 00:41:22,480
distribution so you might think of

991
00:41:20,119 --> 00:41:25,200
alphafold as learning something like a

992
00:41:22,480 --> 00:41:27,760
mapping from the sequence to the

993
00:41:25,200 --> 00:41:30,440
expectation of the structure

994
00:41:27,760 --> 00:41:30,440
conditioned on this

995
00:41:31,160 --> 00:41:36,520
sequence now what we would like to do is

996
00:41:33,319 --> 00:41:38,800
to learn a mapping from the sequence to

997
00:41:36,520 --> 00:41:41,720
the entire distribution of structure

998
00:41:38,800 --> 00:41:43,800
given sequence in a way that we can then

999
00:41:41,720 --> 00:41:46,760
sample for new sequences at inference

1000
00:41:43,800 --> 00:41:48,960
time so this is what we want and

1001
00:41:46,760 --> 00:41:51,640
alphafold has not been trained or

1002
00:41:48,960 --> 00:41:54,000
formulated for this task so we we do in

1003
00:41:51,640 --> 00:41:55,839
this work is we extend Alpha fold under

1004
00:41:54,000 --> 00:41:58,119
a generative modeling framework called

1005
00:41:55,839 --> 00:42:00,960
flow matching to model these these

1006
00:41:58,119 --> 00:42:03,560
ensembles the flow matching is the

1007
00:42:00,960 --> 00:42:07,000
algorithmic framework that makes this

1008
00:42:03,560 --> 00:42:10,079
possible and in flow matching we have

1009
00:42:07,000 --> 00:42:12,119
two distributions P0 which is going to

1010
00:42:10,079 --> 00:42:14,560
be some noisy prior distribution for

1011
00:42:12,119 --> 00:42:17,760
example an easy to sample gaan in our

1012
00:42:14,560 --> 00:42:19,800
case it might be some you know arbitrary

1013
00:42:17,760 --> 00:42:22,040
defined uh distribution of unfolded

1014
00:42:19,800 --> 00:42:25,520
structures and then P1 is going to be

1015
00:42:22,040 --> 00:42:28,119
the data distribution uh that we want to

1016
00:42:25,520 --> 00:42:29,359
sample and um for now we're just going

1017
00:42:28,119 --> 00:42:32,160
to think of the space that these

1018
00:42:29,359 --> 00:42:34,240
distributions are over to be some ukian

1019
00:42:32,160 --> 00:42:36,040
space r3n where n is the number of atoms

1020
00:42:34,240 --> 00:42:37,599
in the protein now this is a slight

1021
00:42:36,040 --> 00:42:38,920
simplification of what's actually

1022
00:42:37,599 --> 00:42:40,200
presented in the paper but it gets the

1023
00:42:38,920 --> 00:42:42,359
main point

1024
00:42:40,200 --> 00:42:44,839
across uh and what flow matching

1025
00:42:42,359 --> 00:42:46,400
provides is a means to learn a Time

1026
00:42:44,839 --> 00:42:49,440
dependent Vector field which we're going

1027
00:42:46,400 --> 00:42:51,400
to call V it's a function of XT which is

1028
00:42:49,440 --> 00:42:52,800
going to be like intermediate values

1029
00:42:51,400 --> 00:42:55,000
that interpolate between the prior and

1030
00:42:52,800 --> 00:42:57,240
the data it's going to learn a flow over

1031
00:42:55,000 --> 00:42:59,079
those values that pushes the

1032
00:42:57,240 --> 00:43:01,440
distribution p 0 to

1033
00:42:59,079 --> 00:43:03,440
P1 and this gives us a j of model

1034
00:43:01,440 --> 00:43:05,040
because by construction we can easily

1035
00:43:03,440 --> 00:43:06,960
sample points from the prior and then we

1036
00:43:05,040 --> 00:43:09,160
learn the flow that allows us to evolve

1037
00:43:06,960 --> 00:43:12,680
those points to

1038
00:43:09,160 --> 00:43:14,599
P1 so pictorially we can draw this as

1039
00:43:12,680 --> 00:43:16,920
having the prior on left hand side for

1040
00:43:14,599 --> 00:43:19,880
example and let's say it's just some

1041
00:43:16,920 --> 00:43:22,520
gaan and then on the right hand side we

1042
00:43:19,880 --> 00:43:24,079
have P1 which is some more complicated

1043
00:43:22,520 --> 00:43:26,280
data

1044
00:43:24,079 --> 00:43:27,880
distribution um and and again this will

1045
00:43:26,280 --> 00:43:29,000
be our distribution of like unfolded

1046
00:43:27,880 --> 00:43:30,359
protein structures and this is our

1047
00:43:29,000 --> 00:43:33,319
distribution of folded protein

1048
00:43:30,359 --> 00:43:35,839
structures we want to learn a flow that

1049
00:43:33,319 --> 00:43:38,319
pushes uh this distribution to to that

1050
00:43:35,839 --> 00:43:40,079
one and in doing so it creates an

1051
00:43:38,319 --> 00:43:43,920
evolving set of intermediate

1052
00:43:40,079 --> 00:43:46,640
distributions which we'll call DT of

1053
00:43:43,920 --> 00:43:49,319
XT and flow matching enables us to learn

1054
00:43:46,640 --> 00:43:51,079
this flow uh in three steps so in the

1055
00:43:49,319 --> 00:43:54,480
first step we

1056
00:43:51,079 --> 00:43:57,640
Define a noising distribution or we

1057
00:43:54,480 --> 00:44:00,440
Define a way of adding noise to some

1058
00:43:57,640 --> 00:44:02,559
given data point X1 to get intermediate

1059
00:44:00,440 --> 00:44:04,800
distributions XT right so this is a

1060
00:44:02,559 --> 00:44:07,880
family of distributions index by time

1061
00:44:04,800 --> 00:44:10,599
and index by X1 such that at time equals

1062
00:44:07,880 --> 00:44:12,480
one this distribution is just saying

1063
00:44:10,599 --> 00:44:15,359
don't add any noise to X1 so it's over

1064
00:44:12,480 --> 00:44:16,880
here and at time T equals zero um it

1065
00:44:15,359 --> 00:44:20,200
adds so much noise that it's equivalent

1066
00:44:16,880 --> 00:44:21,960
to the prior so pictorially we can maybe

1067
00:44:20,200 --> 00:44:24,119
take as an example just some like linear

1068
00:44:21,960 --> 00:44:27,599
interpolation between a given data point

1069
00:44:24,119 --> 00:44:30,000
X1 and the prior p 0

1070
00:44:27,599 --> 00:44:32,680
and in this interpolation then the way

1071
00:44:30,000 --> 00:44:34,720
that you would sample XT given X1 is by

1072
00:44:32,680 --> 00:44:36,599
sampling the noisy point x0 and then

1073
00:44:34,720 --> 00:44:40,079
just linearly

1074
00:44:36,599 --> 00:44:40,079
interpolating uh between the

1075
00:44:41,160 --> 00:44:47,119
two so for example we sample some point

1076
00:44:44,119 --> 00:44:48,920
x0 then we interpolate to X1 and then we

1077
00:44:47,119 --> 00:44:51,440
choose whatever time T we're interested

1078
00:44:48,920 --> 00:44:53,240
in and we get XT so this is how we add

1079
00:44:51,440 --> 00:44:56,680
noise to the data that's what we

1080
00:44:53,240 --> 00:44:59,040
defined second we Define a denoising

1081
00:44:56,680 --> 00:44:59,040
vector

1082
00:44:59,520 --> 00:45:05,280
field that actually undo this process so

1083
00:45:03,520 --> 00:45:08,040
this denoising Vector field is a vector

1084
00:45:05,280 --> 00:45:09,440
field that for a given point X1 so right

1085
00:45:08,040 --> 00:45:12,400
now we assume we know what X1 we're

1086
00:45:09,440 --> 00:45:15,960
interested in it pushes

1087
00:45:12,400 --> 00:45:19,040
P0 to X1 and we're going to call this

1088
00:45:15,960 --> 00:45:21,720
denoting Vector field U of

1089
00:45:19,040 --> 00:45:23,960
XT given

1090
00:45:21,720 --> 00:45:26,520
X1 now in this linear interpolation

1091
00:45:23,960 --> 00:45:28,400
example um this is also very easy to

1092
00:45:26,520 --> 00:45:30,720
Define this is just the vector field

1093
00:45:28,400 --> 00:45:32,680
that says um you know whatever point I'm

1094
00:45:30,720 --> 00:45:35,160
currently at the direction that I should

1095
00:45:32,680 --> 00:45:38,480
go is in the direction of

1096
00:45:35,160 --> 00:45:41,160
X1 and I'm going to do so at a rate that

1097
00:45:38,480 --> 00:45:44,319
allows me to get to X1 in the remaining

1098
00:45:41,160 --> 00:45:48,599
time which is one minus

1099
00:45:44,319 --> 00:45:50,640
t now this is what we' defined so far

1100
00:45:48,599 --> 00:45:52,400
and you can sort of see maybe where this

1101
00:45:50,640 --> 00:45:54,000
is going we have these Vector fields

1102
00:45:52,400 --> 00:45:56,240
that push the prior p 0 to any

1103
00:45:54,000 --> 00:45:57,800
individual point xon and what flow

1104
00:45:56,240 --> 00:46:00,559
matching tells us is that by

1105
00:45:57,800 --> 00:46:02,640
marginalizing over X1 in the appropriate

1106
00:46:00,559 --> 00:46:05,800
fashion we can learn a vector field that

1107
00:46:02,640 --> 00:46:06,800
pushes p 0 to the marginal of X1 which

1108
00:46:05,800 --> 00:46:09,720
is

1109
00:46:06,800 --> 00:46:12,480
P1 and so that's the third step we're

1110
00:46:09,720 --> 00:46:15,119
going to learn typically and in this

1111
00:46:12,480 --> 00:46:16,400
case with the neural network some V of

1112
00:46:15,119 --> 00:46:21,359
XT given

1113
00:46:16,400 --> 00:46:24,960
T um and Theta this now neural network

1114
00:46:21,359 --> 00:46:24,960
and this is going to be the

1115
00:46:25,680 --> 00:46:29,800
expectation of this D noising Vector

1116
00:46:30,359 --> 00:46:35,240
field where this expectation is

1117
00:46:32,599 --> 00:46:36,599
conditioned on the clean data point

1118
00:46:35,240 --> 00:46:38,760
given that we are currently at the noisy

1119
00:46:36,599 --> 00:46:41,760
data point so uh let me just unpack this

1120
00:46:38,760 --> 00:46:44,040
a little bit more uh operationally so

1121
00:46:41,760 --> 00:46:46,000
what we do is we sample points X1 from

1122
00:46:44,040 --> 00:46:47,960
our data and then we add noise to those

1123
00:46:46,000 --> 00:46:50,119
data points by doing this interpolation

1124
00:46:47,960 --> 00:46:51,880
and in doing so we get to some point XT

1125
00:46:50,119 --> 00:46:53,640
but when we're at a particular XT there

1126
00:46:51,880 --> 00:46:55,160
were actually many possible clean data

1127
00:46:53,640 --> 00:46:56,280
points that could have gotten us there

1128
00:46:55,160 --> 00:46:58,559
right so here we could have done this

1129
00:46:56,280 --> 00:47:01,200
interpolation we could have also sampled

1130
00:46:58,559 --> 00:47:02,720
x0 here and X1 here and done this

1131
00:47:01,200 --> 00:47:05,760
interpolation and arrived at the same

1132
00:47:02,720 --> 00:47:07,559
point XT so in these two cases we have

1133
00:47:05,760 --> 00:47:10,119
two different values for this Den

1134
00:47:07,559 --> 00:47:12,000
noising Vector field U one pointing in

1135
00:47:10,119 --> 00:47:14,200
this way and one pointing this

1136
00:47:12,000 --> 00:47:16,800
way and now what flow matching tells us

1137
00:47:14,200 --> 00:47:18,400
to do is to learn the expectation over

1138
00:47:16,800 --> 00:47:20,640
all of these possible interpolations

1139
00:47:18,400 --> 00:47:21,920
that went through XT of this ding Vector

1140
00:47:20,640 --> 00:47:23,119
field which will generally Point

1141
00:47:21,920 --> 00:47:26,319
somewhere in the

1142
00:47:23,119 --> 00:47:28,079
middle and you know just to hopefully

1143
00:47:26,319 --> 00:47:30,079
make things a little bit clearer later

1144
00:47:28,079 --> 00:47:31,480
uh we can actually write this through a

1145
00:47:30,079 --> 00:47:33,160
little bit and we find that this Den

1146
00:47:31,480 --> 00:47:36,200
noising Vector field can be

1147
00:47:33,160 --> 00:47:37,160
parameterized by a den noisy model which

1148
00:47:36,200 --> 00:47:39,440
is just

1149
00:47:37,160 --> 00:47:41,359
predicting what the most likely data

1150
00:47:39,440 --> 00:47:44,040
point is given that we're currently at

1151
00:47:41,359 --> 00:47:44,040
the noisy data

1152
00:47:44,160 --> 00:47:48,079
point um and then we do some appropriate

1153
00:47:46,520 --> 00:47:51,040
transformation to incrementally step

1154
00:47:48,079 --> 00:47:53,280
towards that so in our protein setting

1155
00:47:51,040 --> 00:47:55,559
um like I said this is going to be our

1156
00:47:53,280 --> 00:47:56,839
um uh our distribution of unfolded

1157
00:47:55,559 --> 00:47:58,319
protein structures this is the

1158
00:47:56,839 --> 00:48:00,760
distribution folded protein structures

1159
00:47:58,319 --> 00:48:02,520
for some sequence of interest and what

1160
00:48:00,760 --> 00:48:04,359
we're what we've now defined is a way of

1161
00:48:02,520 --> 00:48:06,640
adding noise to these protein structures

1162
00:48:04,359 --> 00:48:08,319
and we want to learn given a noisy

1163
00:48:06,640 --> 00:48:10,359
protein structure what was the most

1164
00:48:08,319 --> 00:48:11,839
likely noiseless protein structure that

1165
00:48:10,359 --> 00:48:13,720
gave rise to that noisy protein

1166
00:48:11,839 --> 00:48:16,280
structure so that's where Alpha fold

1167
00:48:13,720 --> 00:48:16,280
comes into the

1168
00:48:16,520 --> 00:48:22,880
picture we're going to use Alpha fold to

1169
00:48:20,240 --> 00:48:24,760
parameterize this Den noising model here

1170
00:48:22,880 --> 00:48:26,920
and I apologize probably should use

1171
00:48:24,760 --> 00:48:30,079
different colored markers for this sort

1172
00:48:26,920 --> 00:48:31,640
thing um but flow matching tells us that

1173
00:48:30,079 --> 00:48:34,440
we need a neural network that predicts

1174
00:48:31,640 --> 00:48:36,280
the expected D noise protein structure

1175
00:48:34,440 --> 00:48:38,559
um and has this noisy protein structure

1176
00:48:36,280 --> 00:48:41,119
as input so how do we do that with a

1177
00:48:38,559 --> 00:48:44,119
model like Alpha fold so Alpha fold

1178
00:48:41,119 --> 00:48:45,760
right now is taking in the sequence as

1179
00:48:44,119 --> 00:48:49,160
input and the

1180
00:48:45,760 --> 00:48:51,559
MSA and it's roughly predicting as we

1181
00:48:49,160 --> 00:48:54,079
said earlier the expectation of the

1182
00:48:51,559 --> 00:48:56,240
structure given that

1183
00:48:54,079 --> 00:49:01,280
sequence and now we just need to modify

1184
00:48:56,240 --> 00:49:01,280
alha to also take us input the noisy

1185
00:49:02,920 --> 00:49:07,240
structure so that the um prediction is

1186
00:49:05,680 --> 00:49:11,119
now the expectation of the structure

1187
00:49:07,240 --> 00:49:11,119
given the sequence and the noisy

1188
00:49:11,880 --> 00:49:18,240
structure now how do we do this so

1189
00:49:14,880 --> 00:49:20,599
recall that Alpha fold has what's called

1190
00:49:18,240 --> 00:49:23,400
a a template track as input so you know

1191
00:49:20,599 --> 00:49:26,720
in addition to the sequence and the

1192
00:49:23,400 --> 00:49:29,079
MSA Alpha fold also accept this input um

1193
00:49:26,720 --> 00:49:31,640
a template protein structure which can

1194
00:49:29,079 --> 00:49:33,839
help it inform um its output protein

1195
00:49:31,640 --> 00:49:35,720
structure now typically the template

1196
00:49:33,839 --> 00:49:37,520
input is it's not always used because it

1197
00:49:35,720 --> 00:49:39,720
doesn't turn out to affect the accuracy

1198
00:49:37,520 --> 00:49:41,480
of the final prediction that much but in

1199
00:49:39,720 --> 00:49:43,160
this work we actually use this fact to

1200
00:49:41,480 --> 00:49:45,720
our advantage and leverage the template

1201
00:49:43,160 --> 00:49:47,920
input not to provide a template um but

1202
00:49:45,720 --> 00:49:49,720
to provide a noisy structure the noisy

1203
00:49:47,920 --> 00:49:52,480
structure

1204
00:49:49,720 --> 00:49:54,480
XT um and this provides us with a way of

1205
00:49:52,480 --> 00:49:56,040
training Alpha fold under this flow

1206
00:49:54,480 --> 00:49:59,440
matching framework so that it actually

1207
00:49:56,040 --> 00:50:02,200
learns to push you know a distribution

1208
00:49:59,440 --> 00:50:03,760
of unfolded structures to an ensemble of

1209
00:50:02,200 --> 00:50:06,440
folded structures rather than just a

1210
00:50:03,760 --> 00:50:08,760
single one and a conceptual way of

1211
00:50:06,440 --> 00:50:11,240
understanding why this works is that you

1212
00:50:08,760 --> 00:50:13,640
know in the absence of any noisy input

1213
00:50:11,240 --> 00:50:16,160
the the the sequence input is completely

1214
00:50:13,640 --> 00:50:18,440
deterministic so the output must also be

1215
00:50:16,160 --> 00:50:20,559
deterministic output can only ever

1216
00:50:18,440 --> 00:50:22,280
produce a single structure again a bit

1217
00:50:20,559 --> 00:50:24,079
of a simplification here but um it

1218
00:50:22,280 --> 00:50:25,520
produces a single structure and and so

1219
00:50:24,079 --> 00:50:27,559
it has to pick one that's representative

1220
00:50:25,520 --> 00:50:29,319
of this entire distribution

1221
00:50:27,559 --> 00:50:31,040
but now by having the noisy structure we

1222
00:50:29,319 --> 00:50:33,119
inject some stochasticity into the

1223
00:50:31,040 --> 00:50:35,480
system so alphafold can look at what the

1224
00:50:33,119 --> 00:50:37,960
current noisy structure is and observe

1225
00:50:35,480 --> 00:50:40,400
that it contains hints about which of

1226
00:50:37,960 --> 00:50:42,200
the many possible ground truth

1227
00:50:40,400 --> 00:50:45,359
structures this noisy structure came

1228
00:50:42,200 --> 00:50:47,520
from and so alfold can use that to uh

1229
00:50:45,359 --> 00:50:48,960
affect what the output structure is um

1230
00:50:47,520 --> 00:50:51,200
so it's no longer just a terministic

1231
00:50:48,960 --> 00:50:54,400
function of the

1232
00:50:51,200 --> 00:50:57,359
input um so uh in summary what we've

1233
00:50:54,400 --> 00:50:58,880
done is we've provided a way of making a

1234
00:50:57,359 --> 00:51:00,040
relatively minor change to the alfold

1235
00:50:58,880 --> 00:51:01,440
architecture right because template

1236
00:51:00,040 --> 00:51:03,119
Stacks already there the entire

1237
00:51:01,440 --> 00:51:05,240
architecture is unchanged the training

1238
00:51:03,119 --> 00:51:07,720
laws is unchanged because you know we're

1239
00:51:05,240 --> 00:51:09,079
still learning an expectation so um it's

1240
00:51:07,720 --> 00:51:11,640
still the same regression based loss

1241
00:51:09,079 --> 00:51:13,680
frame align Point error loss but now we

1242
00:51:11,640 --> 00:51:17,440
have a principal method for learning

1243
00:51:13,680 --> 00:51:19,319
from um protein Ensemble data right so

1244
00:51:17,440 --> 00:51:21,559
right now that might mean data from MD

1245
00:51:19,319 --> 00:51:23,559
simulations of course relative to static

1246
00:51:21,559 --> 00:51:25,799
protein structures we still have a

1247
00:51:23,559 --> 00:51:27,200
relative dirth of dynamic ensembles in

1248
00:51:25,799 --> 00:51:29,400
the public data set

1249
00:51:27,200 --> 00:51:32,400
um but hopefully as that increases in

1250
00:51:29,400 --> 00:51:34,359
the coming years methods like this can

1251
00:51:32,400 --> 00:51:36,480
help us you know better take advantage

1252
00:51:34,359 --> 00:51:38,440
of those data and use existing

1253
00:51:36,480 --> 00:51:41,680
Foundation models like alphal um to

1254
00:51:38,440 --> 00:51:43,319
model these more richer um output labels

1255
00:51:41,680 --> 00:51:45,680
um that more accurately describe the

1256
00:51:43,319 --> 00:51:45,680
protein

1257
00:51:47,920 --> 00:51:53,280
in so what we want to do here now is no

1258
00:51:51,280 --> 00:51:56,599
longer generate protein structures but

1259
00:51:53,280 --> 00:52:01,880
now we want to generate DNA sequences so

1260
00:51:56,599 --> 00:52:04,359
I'll just write down one um little a few

1261
00:52:01,880 --> 00:52:06,319
letters and we want to generate

1262
00:52:04,359 --> 00:52:08,720
sequences like this so now we have these

1263
00:52:06,319 --> 00:52:11,000
discrete objects right no longer our

1264
00:52:08,720 --> 00:52:13,680
continuous protein structures and we

1265
00:52:11,000 --> 00:52:15,680
have like a sequence of these discret

1266
00:52:13,680 --> 00:52:19,000
categories and the way that we'll

1267
00:52:15,680 --> 00:52:21,839
represent the sequence is just via um

1268
00:52:19,000 --> 00:52:25,200
like a sequence of simplies or What's

1269
00:52:21,839 --> 00:52:27,440
syesis um if we now consider or let's

1270
00:52:25,200 --> 00:52:30,319
assume we have only three three letters

1271
00:52:27,440 --> 00:52:33,119
in our DNA sequences of course we will

1272
00:52:30,319 --> 00:52:35,040
have four letters later on but in all my

1273
00:52:33,119 --> 00:52:38,599
drawings we will consider having three

1274
00:52:35,040 --> 00:52:41,839
letters then our sequence of simplies

1275
00:52:38,599 --> 00:52:44,680
would we could draw it down like this

1276
00:52:41,839 --> 00:52:46,640
and then say oh yeah our letter A here

1277
00:52:44,680 --> 00:52:49,400
is represented as this corner of the

1278
00:52:46,640 --> 00:52:52,119
Simplex our letter G as this corner of

1279
00:52:49,400 --> 00:52:54,599
the simplex and C as this corner of the

1280
00:52:52,119 --> 00:52:57,760
simplex and then what we want to do is

1281
00:52:54,599 --> 00:52:59,960
to generate like this the sequence and

1282
00:52:57,760 --> 00:53:03,880
then at the end we can decode this

1283
00:52:59,960 --> 00:53:06,599
sequence of syes into a sequence of um

1284
00:53:03,880 --> 00:53:09,880
nucleotides and a sequence a DNA

1285
00:53:06,599 --> 00:53:12,359
sequence yeah and then for this flow

1286
00:53:09,880 --> 00:53:16,359
matching process right Bowen told us we

1287
00:53:12,359 --> 00:53:17,760
need uh this prior here where was it his

1288
00:53:16,359 --> 00:53:21,559
P

1289
00:53:17,760 --> 00:53:25,640
P0 and we have our data distribution the

1290
00:53:21,559 --> 00:53:28,760
P1 and now I'll always only talk about a

1291
00:53:25,640 --> 00:53:31,839
single simple Le because then it's

1292
00:53:28,760 --> 00:53:35,359
easier and I need to draw this and um if

1293
00:53:31,839 --> 00:53:38,960
we now look at our single

1294
00:53:35,359 --> 00:53:41,160
Simplex then what can could our like

1295
00:53:38,960 --> 00:53:45,240
it's clear what our data is right for

1296
00:53:41,160 --> 00:53:48,720
example if single Simplex the data point

1297
00:53:45,240 --> 00:53:52,400
is this it's it's an a for the Simplex

1298
00:53:48,720 --> 00:53:56,040
then we would have this corner as our

1299
00:53:52,400 --> 00:53:59,599
data point on it and our uh what we

1300
00:53:56,040 --> 00:54:02,200
could choose as prior oh yeah as prior

1301
00:53:59,599 --> 00:54:05,400
is for example a uniform distribution

1302
00:54:02,200 --> 00:54:07,559
over all the simplex and yeah now we

1303
00:54:05,400 --> 00:54:10,559
want to Define our flow matching

1304
00:54:07,559 --> 00:54:12,079
procedure for that to generate data like

1305
00:54:10,559 --> 00:54:17,079
these

1306
00:54:12,079 --> 00:54:19,799
um this this a or these corners and of

1307
00:54:17,079 --> 00:54:22,160
course later on we'll Stitch all of this

1308
00:54:19,799 --> 00:54:24,319
together right our all of our simplies

1309
00:54:22,160 --> 00:54:26,440
and our neural network it will always be

1310
00:54:24,319 --> 00:54:28,920
one that takes all these simplies as

1311
00:54:26,440 --> 00:54:32,839
input then predicts an output for all

1312
00:54:28,920 --> 00:54:35,760
the simplexes jointly and uh it won't be

1313
00:54:32,839 --> 00:54:39,960
as primitive as just orienting where you

1314
00:54:35,760 --> 00:54:43,079
want to flow Mass on a single Simplex

1315
00:54:39,960 --> 00:54:45,200
but now to to see how we do this on

1316
00:54:43,079 --> 00:54:47,640
Simplex or how we Define our flow

1317
00:54:45,200 --> 00:54:50,000
matching procedure on the Simplex let's

1318
00:54:47,640 --> 00:54:53,200
consider again these three main

1319
00:54:50,000 --> 00:54:57,359
quantities that Bor wrote down here for

1320
00:54:53,200 --> 00:55:00,720
this conditional probability path then

1321
00:54:57,359 --> 00:55:03,359
this conditional flow map or this

1322
00:55:00,720 --> 00:55:06,880
interpolant and then this conditional

1323
00:55:03,359 --> 00:55:08,839
Vector field here and these three

1324
00:55:06,880 --> 00:55:10,920
objects are connected with each other

1325
00:55:08,839 --> 00:55:14,880
right you can go from the conditional

1326
00:55:10,920 --> 00:55:18,520
flow map to the vector field by

1327
00:55:14,880 --> 00:55:22,880
differentiating it and for example the

1328
00:55:18,520 --> 00:55:25,039
this Vector field is connected to the um

1329
00:55:22,880 --> 00:55:27,599
to the conditional probability path by

1330
00:55:25,039 --> 00:55:29,680
the continuity equation by saying that

1331
00:55:27,599 --> 00:55:31,880
if we have this or what is the vector

1332
00:55:29,680 --> 00:55:36,280
field that gives rise to this

1333
00:55:31,880 --> 00:55:39,640
conditional probability path yeah and

1334
00:55:36,280 --> 00:55:42,640
now let's just do what Bowen did here

1335
00:55:39,640 --> 00:55:45,960
again just with a different P1 and a

1336
00:55:42,640 --> 00:55:48,119
different P0 and do this what we call

1337
00:55:45,960 --> 00:55:51,520
the interpolant perspective where we

1338
00:55:48,119 --> 00:55:54,000
first Define this interpolant here and

1339
00:55:51,520 --> 00:55:56,799
then trivially just via

1340
00:55:54,000 --> 00:55:59,440
differentiation like we different iated

1341
00:55:56,799 --> 00:56:01,720
it and got this guy here our conditional

1342
00:55:59,440 --> 00:56:03,880
Vector field arises we train our neural

1343
00:56:01,720 --> 00:56:08,160
network to just match this conditional

1344
00:56:03,880 --> 00:56:11,240
Vector field and then we end up in

1345
00:56:08,160 --> 00:56:13,440
because we um train with many of these

1346
00:56:11,240 --> 00:56:16,200
x1s with many of these data points we

1347
00:56:13,440 --> 00:56:18,280
end up with a vector field that

1348
00:56:16,200 --> 00:56:20,440
approximates the

1349
00:56:18,280 --> 00:56:24,520
non-conditional vector field that

1350
00:56:20,440 --> 00:56:27,160
approximates the guy and yeah if we now

1351
00:56:24,520 --> 00:56:29,559
do this for simplex

1352
00:56:27,160 --> 00:56:33,720
let's think about what happens and I

1353
00:56:29,559 --> 00:56:39,440
mean it won't be the final solution

1354
00:56:33,720 --> 00:56:41,480
and final solution and um now we have

1355
00:56:39,440 --> 00:56:45,079
our data point here right let's say we

1356
00:56:41,480 --> 00:56:47,640
sampled a as our X1 and then we sample

1357
00:56:45,079 --> 00:56:50,760
somewhere on here from our uniform prior

1358
00:56:47,640 --> 00:56:55,440
right our

1359
00:56:50,760 --> 00:56:58,160
x0 and then our X1 with this linear

1360
00:56:55,440 --> 00:57:00,799
interpolant here here with the T going

1361
00:56:58,160 --> 00:57:04,039
from 0 to one it it'll be somewhere on

1362
00:57:00,799 --> 00:57:07,359
this line so maybe if our T is

1363
00:57:04,039 --> 00:57:11,760
0.5 then it will be here in the middle

1364
00:57:07,359 --> 00:57:14,039
XT then Bowen told us we take the

1365
00:57:11,760 --> 00:57:16,079
derivative of this interpolant we get

1366
00:57:14,039 --> 00:57:17,720
this little little Vector conditional

1367
00:57:16,079 --> 00:57:21,160
Vector field here we match our NE

1368
00:57:17,720 --> 00:57:23,240
Network against okay then this the thing

1369
00:57:21,160 --> 00:57:27,480
this works and this is something that we

1370
00:57:23,240 --> 00:57:30,960
can do but let us look at what is the

1371
00:57:27,480 --> 00:57:32,520
conditional probability path that arises

1372
00:57:30,960 --> 00:57:36,880
if we do this

1373
00:57:32,520 --> 00:57:39,039
procedure okay um for that we will

1374
00:57:36,880 --> 00:57:42,720
probably draw a few more

1375
00:57:39,039 --> 00:57:46,160
triangles and so let's look at the

1376
00:57:42,720 --> 00:57:50,480
conditional probability path at time

1377
00:57:46,160 --> 00:57:54,240
0.5 so conditioned on this X1

1378
00:57:50,480 --> 00:57:56,720
here which are the possible points from

1379
00:57:54,240 --> 00:57:59,720
this probability distributions condition

1380
00:57:56,720 --> 00:58:02,559
on this X1 what are the possible XTS

1381
00:57:59,720 --> 00:58:06,079
that we could have reached well and it

1382
00:58:02,559 --> 00:58:07,960
turns out that the possible XTS are in

1383
00:58:06,079 --> 00:58:10,640
this triangle

1384
00:58:07,960 --> 00:58:13,640
here and it's a uniform distribution

1385
00:58:10,640 --> 00:58:16,680
over this triangle why is that because

1386
00:58:13,640 --> 00:58:18,839
consider what is the furthest away point

1387
00:58:16,680 --> 00:58:21,760
that you could sample from your prior

1388
00:58:18,839 --> 00:58:24,640
your uniform prior it is somewhere down

1389
00:58:21,760 --> 00:58:28,960
here at this face of our simplex and

1390
00:58:24,640 --> 00:58:31,880
then at time 0.5 five this point would

1391
00:58:28,960 --> 00:58:34,039
go here and you could also have

1392
00:58:31,880 --> 00:58:36,440
uniformly sampled another point from the

1393
00:58:34,039 --> 00:58:39,559
prior right this one right

1394
00:58:36,440 --> 00:58:41,880
here then the point would end up here

1395
00:58:39,559 --> 00:58:45,960
and so on and so forth but all your

1396
00:58:41,880 --> 00:58:50,640
points they will end up in

1397
00:58:45,960 --> 00:58:53,960
here okay then why why do I

1398
00:58:50,640 --> 00:58:56,319
think um I didn't say it yet but this is

1399
00:58:53,960 --> 00:58:58,760
a problem that we have this condition

1400
00:58:56,319 --> 00:59:01,920
probability path that does not at all

1401
00:58:58,760 --> 00:59:02,640
times T like here for example the one

1402
00:59:01,920 --> 00:59:06,240
time

1403
00:59:02,640 --> 00:59:09,240
0.5 that does not at all Point times T

1404
00:59:06,240 --> 00:59:11,079
have full support over the the space

1405
00:59:09,240 --> 00:59:13,319
that we're interested in and not full

1406
00:59:11,079 --> 00:59:16,039
support over the entire sylex and why

1407
00:59:13,319 --> 00:59:19,200
does this happen here and not for bones

1408
00:59:16,039 --> 00:59:22,760
continuous stuff well because here we

1409
00:59:19,200 --> 00:59:25,000
have a compact space a bounded space and

1410
00:59:22,760 --> 00:59:28,559
in in Bowen scenario right you could

1411
00:59:25,000 --> 00:59:30,359
have always had a point that's like much

1412
00:59:28,559 --> 00:59:33,240
arbitrarily far away because you have

1413
00:59:30,359 --> 00:59:36,039
this caution and here you have a

1414
00:59:33,240 --> 00:59:39,799
boundary and you can't get arbit far

1415
00:59:36,039 --> 00:59:42,160
away so to say okay now why do I think

1416
00:59:39,799 --> 00:59:45,799
this this might be a problem well let's

1417
00:59:42,160 --> 00:59:47,680
draw another triangle we have this a lot

1418
00:59:45,799 --> 00:59:51,760
here today

1419
00:59:47,680 --> 00:59:54,640
and then um let's consider the

1420
00:59:51,760 --> 00:59:56,559
conditional Vector field of this

1421
00:59:54,640 --> 01:00:00,319
point and

1422
00:59:56,559 --> 01:00:03,559
this point at time

1423
01:00:00,319 --> 01:00:05,599
0.8 right at time 0.8 this guy's

1424
01:00:03,559 --> 01:00:08,280
conditional probability path will look

1425
01:00:05,599 --> 01:00:08,280
something like

1426
01:00:08,480 --> 01:00:13,640
this and this guy's conditional

1427
01:00:12,000 --> 01:00:15,920
probability path will look something

1428
01:00:13,640 --> 01:00:15,920
like

1429
01:00:17,599 --> 01:00:22,920
this okay and then let's draw the

1430
01:00:20,440 --> 01:00:27,319
conditional Vector fi that b was drawing

1431
01:00:22,920 --> 01:00:30,440
here right we have all these um or this

1432
01:00:27,319 --> 01:00:34,880
guy so we have all these interpolant

1433
01:00:30,440 --> 01:00:37,880
going from data to um to

1434
01:00:34,880 --> 01:00:40,799
our point that we condition on sorry

1435
01:00:37,880 --> 01:00:44,599
going from x0 to X1 so from noise to

1436
01:00:40,799 --> 01:00:46,599
data and then let's say this is our our

1437
01:00:44,599 --> 01:00:49,359
XT

1438
01:00:46,599 --> 01:00:51,880
here then our Vector field will be

1439
01:00:49,359 --> 01:00:53,880
pointing to this corner and the vector

1440
01:00:51,880 --> 01:00:56,720
field will always be pointing to this

1441
01:00:53,880 --> 01:00:59,119
corner

1442
01:00:56,720 --> 01:01:01,839
and of course this conditional Vector

1443
01:00:59,119 --> 01:01:03,280
field over here will also be pointing to

1444
01:01:01,839 --> 01:01:06,400
the

1445
01:01:03,280 --> 01:01:09,760
corner but now if your data could have

1446
01:01:06,400 --> 01:01:13,200
come from here or from

1447
01:01:09,760 --> 01:01:16,880
here then the the vector field

1448
01:01:13,200 --> 01:01:19,559
marginalized over these two here um or

1449
01:01:16,880 --> 01:01:22,000
the the change of the vector field

1450
01:01:19,559 --> 01:01:24,920
that's in which it's in which direction

1451
01:01:22,000 --> 01:01:26,960
it is pointing here it changes

1452
01:01:24,920 --> 01:01:31,079
discontinuously once you cross this

1453
01:01:26,960 --> 01:01:34,079
border here right and also in in terms

1454
01:01:31,079 --> 01:01:36,000
of so this Vector field is this

1455
01:01:34,079 --> 01:01:40,119
continuous in Space the

1456
01:01:36,000 --> 01:01:42,200
marginality and presumably if we want to

1457
01:01:40,119 --> 01:01:45,480
approximate something with new networks

1458
01:01:42,200 --> 01:01:48,160
then um having something like

1459
01:01:45,480 --> 01:01:51,000
discontinous um this probably not so

1460
01:01:48,160 --> 01:01:54,720
nice and we also in our experiments show

1461
01:01:51,000 --> 01:01:59,480
that um the where we have like a smooth

1462
01:01:54,720 --> 01:02:02,720
smoothly changing V field in space um

1463
01:01:59,480 --> 01:02:05,960
this seems to work better but now what

1464
01:02:02,720 --> 01:02:07,680
do we actually do and what do we do

1465
01:02:05,960 --> 01:02:10,000
instead of this what we call the

1466
01:02:07,680 --> 01:02:12,839
interpolant perspective where we first

1467
01:02:10,000 --> 01:02:15,240
Define an interpolant this thing here

1468
01:02:12,839 --> 01:02:18,720
then take the derivative and very easily

1469
01:02:15,240 --> 01:02:21,720
get our training part what we say is

1470
01:02:18,720 --> 01:02:24,000
right with this we come or we arise with

1471
01:02:21,720 --> 01:02:26,520
these or we obtain these conditional

1472
01:02:24,000 --> 01:02:30,720
probability paths that have the

1473
01:02:26,520 --> 01:02:32,599
nesir desirable property that it does

1474
01:02:30,720 --> 01:02:37,279
not have full support over the entire

1475
01:02:32,599 --> 01:02:39,760
Simplex so instead we say let's start by

1476
01:02:37,279 --> 01:02:43,440
the our flow matching procedure or flow

1477
01:02:39,760 --> 01:02:46,039
matching design by defining a

1478
01:02:43,440 --> 01:02:48,599
conditional probability path so this is

1479
01:02:46,039 --> 01:02:50,839
now what we want to design first and

1480
01:02:48,599 --> 01:02:54,440
then all the other stuff follows from

1481
01:02:50,839 --> 01:02:58,480
that and uh then okay what choice could

1482
01:02:54,440 --> 01:03:01,359
we make the choice that we make

1483
01:02:58,480 --> 01:03:04,720
is if this is the again the point that

1484
01:03:01,359 --> 01:03:09,200
we condition on then we want a sequence

1485
01:03:04,720 --> 01:03:11,720
of conditional probability paths that in

1486
01:03:09,200 --> 01:03:15,440
the beginning right it is a it should be

1487
01:03:11,720 --> 01:03:19,440
a uniform over the entire simplex and at

1488
01:03:15,440 --> 01:03:22,480
the end we want it to be a d at the um

1489
01:03:19,440 --> 01:03:25,039
at our data point okay and then what we

1490
01:03:22,480 --> 01:03:27,760
choose for this is is just a deay

1491
01:03:25,039 --> 01:03:27,760
distribution

1492
01:03:28,160 --> 01:03:37,200
where this Alpha parameter

1493
01:03:31,079 --> 01:03:40,200
here it's um increases or s s time in

1494
01:03:37,200 --> 01:03:44,319
our from goes from zero to one so s t

1495
01:03:40,200 --> 01:03:48,559
goes from zero to to one um or this

1496
01:03:44,319 --> 01:03:51,760
Alpha parameter will go from one

1497
01:03:48,559 --> 01:03:54,760
to essentially Infinity where Infinity

1498
01:03:51,760 --> 01:03:58,200
is eight because that's far enough and

1499
01:03:54,760 --> 01:04:00,640
then the um what the what this these

1500
01:03:58,200 --> 01:04:04,640
three parameters will correspond to if

1501
01:04:00,640 --> 01:04:08,279
up here is our data point is that here

1502
01:04:04,640 --> 01:04:12,880
will be um the ones for these two

1503
01:04:08,279 --> 01:04:15,799
corners and up here will be this Alpha

1504
01:04:12,880 --> 01:04:18,520
parameter and then the way this looks

1505
01:04:15,799 --> 01:04:21,559
like when maybe we don't all know how

1506
01:04:18,520 --> 01:04:25,440
this distributions look like it is just

1507
01:04:21,559 --> 01:04:27,039
that um if this is maybe one if this is

1508
01:04:25,440 --> 01:04:30,000
one then this will be the uniform

1509
01:04:27,039 --> 01:04:33,400
distribution and if this is two then

1510
01:04:30,000 --> 01:04:37,160
like a lot more mass is contracted up

1511
01:04:33,400 --> 01:04:39,240
here then um a lot more masses up here

1512
01:04:37,160 --> 01:04:41,400
than down here and it just goes on and

1513
01:04:39,240 --> 01:04:43,520
on like this and if you have like eight

1514
01:04:41,400 --> 01:04:46,920
for example is Alpha then almost all

1515
01:04:43,520 --> 01:04:51,520
mass is up here and almost no Mass on

1516
01:04:46,920 --> 01:04:53,920
here okay so now we have just written

1517
01:04:51,520 --> 01:04:56,920
down oh we would like a conditional

1518
01:04:53,920 --> 01:04:59,119
probability path like this instead of

1519
01:04:56,920 --> 01:05:02,559
conditional probability paths like the

1520
01:04:59,119 --> 01:05:05,119
red one and then we still don't have a

1521
01:05:02,559 --> 01:05:09,799
training procedure so to obtain this

1522
01:05:05,119 --> 01:05:12,680
training procedure we then now need to

1523
01:05:09,799 --> 01:05:17,200
instead of just taking the derivative as

1524
01:05:12,680 --> 01:05:18,880
we did here we need to so right we we

1525
01:05:17,200 --> 01:05:21,319
have a conditional Vector field defined

1526
01:05:18,880 --> 01:05:23,400
take the derivative sorry we have our

1527
01:05:21,319 --> 01:05:25,359
interpolant defined take the derivative

1528
01:05:23,400 --> 01:05:26,960
and get the conditional Vector field and

1529
01:05:25,359 --> 01:05:29,359
then we can match against this

1530
01:05:26,960 --> 01:05:32,400
conditional Vector field here we have

1531
01:05:29,359 --> 01:05:36,599
the conditional probability path and we

1532
01:05:32,400 --> 01:05:38,760
said it is uh connected to the

1533
01:05:36,599 --> 01:05:42,559
conditional Vector field via the

1534
01:05:38,760 --> 01:05:45,799
continuity equation so now we defined

1535
01:05:42,559 --> 01:05:49,680
this um this conditional probability

1536
01:05:45,799 --> 01:05:52,400
path PT let's write it down PT and then

1537
01:05:49,680 --> 01:05:55,200
the continuity equation just tells us

1538
01:05:52,400 --> 01:05:58,400
that it's the change in density with

1539
01:05:55,200 --> 01:06:04,480
respect to to time this should be the

1540
01:05:58,400 --> 01:06:04,480
same as the Divergence of the vector

1541
01:06:05,119 --> 01:06:09,920
field so it it just tells us that if we

1542
01:06:08,440 --> 01:06:14,559
have a point

1543
01:06:09,920 --> 01:06:17,839
here then the amount or the the change

1544
01:06:14,559 --> 01:06:20,119
of density in time should be the same as

1545
01:06:17,839 --> 01:06:22,960
the amount of density that basically

1546
01:06:20,119 --> 01:06:27,200
flows into this point and out of this

1547
01:06:22,960 --> 01:06:30,799
point um or the sum of that um at that

1548
01:06:27,200 --> 01:06:34,119
time point and at that point in space

1549
01:06:30,799 --> 01:06:37,240
yeah and then we go ahead solve this

1550
01:06:34,119 --> 01:06:39,279
continuity equation uh which there's an

1551
01:06:37,240 --> 01:06:42,160
interesting trick to do this where we

1552
01:06:39,279 --> 01:06:45,039
just look at the flux across a plane

1553
01:06:42,160 --> 01:06:48,839
that goes through this point because all

1554
01:06:45,039 --> 01:06:52,520
the um like all the density along this

1555
01:06:48,839 --> 01:06:55,279
plane will be is the same in this D in

1556
01:06:52,520 --> 01:06:56,039
this D path and we obtain this Vector

1557
01:06:55,279 --> 01:06:59,640
field

1558
01:06:56,039 --> 01:07:04,559
and that is what we match against yeah

1559
01:06:59,640 --> 01:07:06,480
and then to summarize we tried doing

1560
01:07:04,559 --> 01:07:08,440
this interpolant perspective for

1561
01:07:06,480 --> 01:07:11,359
defining a flow matching process on the

1562
01:07:08,440 --> 01:07:14,279
Simplex had some or found some issues

1563
01:07:11,359 --> 01:07:17,440
with doing that and then instead decided

1564
01:07:14,279 --> 01:07:21,200
to take this uh probability path

1565
01:07:17,440 --> 01:07:23,240
perspective and this then gives rise to

1566
01:07:21,200 --> 01:07:26,920
some complications for finding your

1567
01:07:23,240 --> 01:07:31,359
training Target but uh in in the case of

1568
01:07:26,920 --> 01:07:34,200
this simplex and theay probability path

1569
01:07:31,359 --> 01:07:37,359
This was um or this is possible to find

1570
01:07:34,200 --> 01:07:39,400
this Vector Feld and I think maybe the

1571
01:07:37,359 --> 01:07:43,760
this is the interesting takeaway if we

1572
01:07:39,400 --> 01:07:47,520
have like a um a space that for example

1573
01:07:43,760 --> 01:07:51,960
it's a manifold with um like a cut Locus

1574
01:07:47,520 --> 01:07:54,640
or some compact um some compact space

1575
01:07:51,960 --> 01:07:57,640
like our Simplex here for example then

1576
01:07:54,640 --> 01:07:59,680
it's might be interesting to take a

1577
01:07:57,640 --> 01:08:02,640
different perspective on Flow matching

1578
01:07:59,680 --> 01:08:06,119
this path probability path perspective

1579
01:08:02,640 --> 01:08:06,119
instead of this

1580
01:08:07,359 --> 01:08:12,920
interent uh thanks to the organization

1581
01:08:09,680 --> 01:08:16,839
for the invitation uh for me to present

1582
01:08:12,920 --> 01:08:18,839
a very light uh talk on how we have used

1583
01:08:16,839 --> 01:08:22,000
deep learning to navigate vast chemical

1584
01:08:18,839 --> 01:08:26,560
spaces to find novel uh Therapeutics for

1585
01:08:22,000 --> 01:08:28,279
a wide variety of pathologies so uh with

1586
01:08:26,560 --> 01:08:31,480
this I would want to

1587
01:08:28,279 --> 01:08:35,400
introduce um how traditional methods

1588
01:08:31,480 --> 01:08:38,159
have been doing Lian Discovery

1589
01:08:35,400 --> 01:08:41,600
campaigns and an

1590
01:08:38,159 --> 01:08:43,560
example um that's still widely used

1591
01:08:41,600 --> 01:08:47,040
certainly here at the Bro Institute

1592
01:08:43,560 --> 01:08:50,520
would be uh hrut screening campaigns so

1593
01:08:47,040 --> 01:08:53,359
this would be U System where you have a

1594
01:08:50,520 --> 01:08:56,279
very large chemical Library which is

1595
01:08:53,359 --> 01:08:58,400
stored in plates

1596
01:08:56,279 --> 01:09:01,279
and at the broad it's around

1597
01:08:58,400 --> 01:09:03,440
800,000 individual unique molecules but

1598
01:09:01,279 --> 01:09:05,480
has now been expounded recently to

1599
01:09:03,440 --> 01:09:07,520
contain over a million

1600
01:09:05,480 --> 01:09:09,960
molecules so what you would do is that

1601
01:09:07,520 --> 01:09:12,719
you would for instance in the context of

1602
01:09:09,960 --> 01:09:15,600
um an anti bacterial Discovery campaign

1603
01:09:12,719 --> 01:09:19,799
you would have in your

1604
01:09:15,600 --> 01:09:21,239
experiment um a vile or an in the well

1605
01:09:19,799 --> 01:09:23,759
you would have your bacteria here

1606
01:09:21,239 --> 01:09:25,440
floating around and then you're

1607
01:09:23,759 --> 01:09:27,400
basically going to test individually

1608
01:09:25,440 --> 01:09:30,080
each molecule in your hydrop screening

1609
01:09:27,400 --> 01:09:31,640
library for its antibacterial properties

1610
01:09:30,080 --> 01:09:34,440
so you're checking with the phenotypic

1611
01:09:31,640 --> 01:09:38,120
readout whether the

1612
01:09:34,440 --> 01:09:40,319
bacteria um dies or doesn't die upon

1613
01:09:38,120 --> 01:09:44,040
exposure of your small molecule so what

1614
01:09:40,319 --> 01:09:46,120
you're then trying to find is basically

1615
01:09:44,040 --> 01:09:47,880
if you if you would have done it in

1616
01:09:46,120 --> 01:09:49,759
multiple

1617
01:09:47,880 --> 01:09:53,799
concentrations you're checking for

1618
01:09:49,759 --> 01:09:56,480
molecules that are active or inactive

1619
01:09:53,799 --> 01:09:59,560
and um using this

1620
01:09:56,480 --> 01:10:02,800
procedure and now more recently in a

1621
01:09:59,560 --> 01:10:06,000
more automated or um using robots

1622
01:10:02,800 --> 01:10:08,520
version um you can actually uh

1623
01:10:06,000 --> 01:10:10,120
experimentally validate a large junk of

1624
01:10:08,520 --> 01:10:12,199
molecules but it's still limited to what

1625
01:10:10,120 --> 01:10:14,400
you actually have uh physically

1626
01:10:12,199 --> 01:10:17,440
accessible in your library which is

1627
01:10:14,400 --> 01:10:21,040
around the best case several hundred,

1628
01:10:17,440 --> 01:10:22,679
molecules so the chemical space is quite

1629
01:10:21,040 --> 01:10:24,159
narrow if you consider that the total

1630
01:10:22,679 --> 01:10:26,199
drug like chemical space has been

1631
01:10:24,159 --> 01:10:29,159
estimated to be 10 to to the Power 60

1632
01:10:26,199 --> 01:10:31,360
molecules so um another way of doing

1633
01:10:29,159 --> 01:10:34,040
things is going computational rather

1634
01:10:31,360 --> 01:10:37,159
than doing an experimental hyro screen

1635
01:10:34,040 --> 01:10:41,040
so what you can do um and this is with

1636
01:10:37,159 --> 01:10:44,159
the recent uh advancements in uh

1637
01:10:41,040 --> 01:10:47,000
combinatorial chemistry U mainly driven

1638
01:10:44,159 --> 01:10:50,000
by the Ukrainian chemical vendor enamine

1639
01:10:47,000 --> 01:10:53,360
is if you would have on the Shelf a

1640
01:10:50,000 --> 01:10:58,440
bunch of building blocks

1641
01:10:53,360 --> 01:11:02,640
oops building block a building block B

1642
01:10:58,440 --> 01:11:05,000
Building Block C and you would do an in

1643
01:11:02,640 --> 01:11:07,560
silico enumeration of these building

1644
01:11:05,000 --> 01:11:10,520
blocks to afford virtual molecules you

1645
01:11:07,560 --> 01:11:14,040
could end up

1646
01:11:10,520 --> 01:11:14,040
with in this

1647
01:11:15,560 --> 01:11:21,920
case most recent Library sizes are now

1648
01:11:18,920 --> 01:11:23,440
up to I think it's 50 billion molecules

1649
01:11:21,920 --> 01:11:24,880
that are so-called make on demand

1650
01:11:23,440 --> 01:11:27,800
molecules what's special about these

1651
01:11:24,880 --> 01:11:30,760
molecules is that they can be readily

1652
01:11:27,800 --> 01:11:32,920
synthesized in four weeks and the vendor

1653
01:11:30,760 --> 01:11:35,480
has a synthesis success rate of up to

1654
01:11:32,920 --> 01:11:37,239
80% so these molecules don't exist but

1655
01:11:35,480 --> 01:11:41,679
they are in cataloges that you can

1656
01:11:37,239 --> 01:11:43,880
access um so 50 billion

1657
01:11:41,679 --> 01:11:46,080
molecules acquiring all of these and

1658
01:11:43,880 --> 01:11:47,840
testing them individually is completely

1659
01:11:46,080 --> 01:11:50,719
intractable so now we need to rely on

1660
01:11:47,840 --> 01:11:52,679
computational models to help us um

1661
01:11:50,719 --> 01:11:56,440
prioritize specific molecules for

1662
01:11:52,679 --> 01:11:57,880
synthesis and um experimental evaluation

1663
01:11:56,440 --> 01:12:00,320
so

1664
01:11:57,880 --> 01:12:02,120
um what can we do now we can actually

1665
01:12:00,320 --> 01:12:03,880
use the data that you have gotten

1666
01:12:02,120 --> 01:12:05,400
through a high throughput screen so

1667
01:12:03,880 --> 01:12:08,639
again you have your

1668
01:12:05,400 --> 01:12:11,920
library with all your molecules and then

1669
01:12:08,639 --> 01:12:14,480
basically based on the experimental

1670
01:12:11,920 --> 01:12:17,080
phenotypic readout you can classify the

1671
01:12:14,480 --> 01:12:19,639
molecules or you can binarize them into

1672
01:12:17,080 --> 01:12:22,080
active molecules or inactive molecules

1673
01:12:19,639 --> 01:12:26,360
you have a very large data set like in

1674
01:12:22,080 --> 01:12:29,239
our case to 40,000 molecules

1675
01:12:26,360 --> 01:12:32,000
of actives and inactive molecules and

1676
01:12:29,239 --> 01:12:35,520
now you can train neural network

1677
01:12:32,000 --> 01:12:38,480
here which in our case was U message

1678
01:12:35,520 --> 01:12:40,920
passing message passing neural network

1679
01:12:38,480 --> 01:12:42,880
and then you can now look in this very

1680
01:12:40,920 --> 01:12:44,760
large chemical space of billions of

1681
01:12:42,880 --> 01:12:47,000
molecules using your train neural

1682
01:12:44,760 --> 01:12:49,159
network to find the compounds that have

1683
01:12:47,000 --> 01:12:51,520
the highest predicted antibacterial

1684
01:12:49,159 --> 01:12:55,320
score and then you could have molecules

1685
01:12:51,520 --> 01:12:57,760
that have a um like score of

1686
01:12:55,320 --> 01:12:59,120
antibacterial which can be uh low or

1687
01:12:57,760 --> 01:13:02,800
high and then you prioritize the

1688
01:12:59,120 --> 01:13:05,400
molecules that are top scoring you um

1689
01:13:02,800 --> 01:13:08,400
contact the exper

1690
01:13:05,400 --> 01:13:10,159
chemical synthesis Fender and you

1691
01:13:08,400 --> 01:13:13,320
prioritize those molecules for synthesis

1692
01:13:10,159 --> 01:13:14,239
and then you explicitly evaluate them um

1693
01:13:13,320 --> 01:13:19,159
in the

1694
01:13:14,239 --> 01:13:20,920
lab um and if you have lots of data um

1695
01:13:19,159 --> 01:13:23,280
and lots of molecules you can actually

1696
01:13:20,920 --> 01:13:27,360
deconvolute the molecules that you have

1697
01:13:23,280 --> 01:13:29,840
for instance in this case um you could

1698
01:13:27,360 --> 01:13:32,120
using a Monte Carlo research basically

1699
01:13:29,840 --> 01:13:34,440
find a region of the molecule that's

1700
01:13:32,120 --> 01:13:36,320
most contributing to the antibacterial

1701
01:13:34,440 --> 01:13:40,760
score and this could then be considered

1702
01:13:36,320 --> 01:13:46,639
something um like the explainable AI

1703
01:13:40,760 --> 01:13:48,760
realm so um then in this case um the

1704
01:13:46,639 --> 01:13:51,440
annoying thing is that even if you do

1705
01:13:48,760 --> 01:13:53,320
find molecules that are antibacterial in

1706
01:13:51,440 --> 01:13:55,920
the experiment you still don't really

1707
01:13:53,320 --> 01:13:58,280
know what their actual mechanism of

1708
01:13:55,920 --> 01:14:00,080
action is or what they're doing um

1709
01:13:58,280 --> 01:14:03,280
inside the cell or what protein they're

1710
01:14:00,080 --> 01:14:05,360
modulating so if you find a molecule

1711
01:14:03,280 --> 01:14:07,600
which would then be denoted as as a hit

1712
01:14:05,360 --> 01:14:09,320
molecule and you want to optimize the

1713
01:14:07,600 --> 01:14:12,159
physical chemical properties to make it

1714
01:14:09,320 --> 01:14:14,800
more like a preclinical drug candidate

1715
01:14:12,159 --> 01:14:16,800
if you do not know what protein Target

1716
01:14:14,800 --> 01:14:18,800
your molecule is modulating you're

1717
01:14:16,800 --> 01:14:21,280
basically optimizing in the dark and you

1718
01:14:18,800 --> 01:14:24,159
need to rely on Brute Force Medicinal

1719
01:14:21,280 --> 01:14:25,639
Chemistry efforts to find um a better

1720
01:14:24,159 --> 01:14:30,199
molecule

1721
01:14:25,639 --> 01:14:32,280
so um and with this I want to introduce

1722
01:14:30,199 --> 01:14:34,719
uh another biological system that we

1723
01:14:32,280 --> 01:14:37,400
have been working on in the lab recently

1724
01:14:34,719 --> 01:14:43,000
which would be um

1725
01:14:37,400 --> 01:14:46,480
so-called uh dof flatulates which are

1726
01:14:43,000 --> 01:14:46,480
um Marine

1727
01:14:51,880 --> 01:14:59,159
Plankton um so these molecules they

1728
01:14:55,880 --> 01:15:03,719
biosynthesize a specific

1729
01:14:59,159 --> 01:15:06,120
molecule um called braev Toxin and brav

1730
01:15:03,719 --> 01:15:08,040
toxin for the sake of time I'm not going

1731
01:15:06,120 --> 01:15:10,719
to explicitly draw out here it's a very

1732
01:15:08,040 --> 01:15:15,159
large molecule basically what it does is

1733
01:15:10,719 --> 01:15:15,159
that um in our neuronal

1734
01:15:15,239 --> 01:15:20,760
cells we

1735
01:15:17,120 --> 01:15:24,199
have um specific this would be the

1736
01:15:20,760 --> 01:15:27,520
phospho the B layer you have specific

1737
01:15:24,199 --> 01:15:31,120
proteins that are involved with the

1738
01:15:27,520 --> 01:15:37,920
management of ion flux one of these

1739
01:15:31,120 --> 01:15:40,480
would be the uh voltage voltage gated um

1740
01:15:37,920 --> 01:15:44,080
sodium Channel and braev toxin if I

1741
01:15:40,480 --> 01:15:47,239
didn't noce as B lies in on the inside

1742
01:15:44,080 --> 01:15:50,440
of this um ION channel and keeps it open

1743
01:15:47,239 --> 01:15:54,560
so you would basically have a net influx

1744
01:15:50,440 --> 01:15:57,199
of a lot of these um sodium ions inside

1745
01:15:54,560 --> 01:15:58,840
which would mess up the cell signaling

1746
01:15:57,199 --> 01:16:01,960
in the neuronal cells and can lead to

1747
01:15:58,840 --> 01:16:04,400
all kinds of pathologies so um what we

1748
01:16:01,960 --> 01:16:07,120
then did is the same thing is here we

1749
01:16:04,400 --> 01:16:12,600
got a library of

1750
01:16:07,120 --> 01:16:14,760
2.5 th000 um molecules and we have uh

1751
01:16:12,600 --> 01:16:19,040
individually uh in each of these Wells

1752
01:16:14,760 --> 01:16:21,199
we had um the neuronal cell and small

1753
01:16:19,040 --> 01:16:25,000
molecule and the braev Toxin and our

1754
01:16:21,199 --> 01:16:28,480
readout is basically um which of these

1755
01:16:25,000 --> 01:16:31,080
molecules rescue the um neuronal cells

1756
01:16:28,480 --> 01:16:33,760
from from from dying and then based on

1757
01:16:31,080 --> 01:16:36,280
that we can again train a neural network

1758
01:16:33,760 --> 01:16:37,600
and we have used the Bro 800k library in

1759
01:16:36,280 --> 01:16:40,880
this

1760
01:16:37,600 --> 01:16:43,000
case and found uh 100 molecules that had

1761
01:16:40,880 --> 01:16:45,040
good scores and then we've tested them

1762
01:16:43,000 --> 01:16:47,159
experimentally in the lab to see which

1763
01:16:45,040 --> 01:16:50,000
one of these um actually rescued the

1764
01:16:47,159 --> 01:16:52,880
cells and a nice molecule that we found

1765
01:16:50,000 --> 01:16:52,880
here

1766
01:16:53,679 --> 01:16:57,520
was if if I draw this

1767
01:17:08,080 --> 01:17:13,120
correctly this

1768
01:17:10,239 --> 01:17:15,400
molecule looks

1769
01:17:13,120 --> 01:17:17,159
like and what's so interesting about

1770
01:17:15,400 --> 01:17:19,840
this molecule is that it actually

1771
01:17:17,159 --> 01:17:24,320
contains this Motif

1772
01:17:19,840 --> 01:17:24,320
here which is nicotine

1773
01:17:25,080 --> 01:17:31,440
so again in the lines of explainable AI

1774
01:17:28,760 --> 01:17:35,040
if you think about this substructure um

1775
01:17:31,440 --> 01:17:37,719
and about nicotine itself that nicotine

1776
01:17:35,040 --> 01:17:39,840
is also

1777
01:17:37,719 --> 01:17:44,280
involved in

1778
01:17:39,840 --> 01:17:47,080
the um sodium uh ion

1779
01:17:44,280 --> 01:17:49,840
management also in neuronal cells and

1780
01:17:47,080 --> 01:17:51,719
then nicotin acetel Coolin

1781
01:17:49,840 --> 01:17:54,360
receptors they look like this they're

1782
01:17:51,719 --> 01:17:56,600
pentomic structures and basically you

1783
01:17:54,360 --> 01:17:58,520
have Alpha and beta subunits and

1784
01:17:56,600 --> 01:18:02,960
depending on the on the sto geometry you

1785
01:17:58,520 --> 01:18:06,679
have a different uh ISO form of the um

1786
01:18:02,960 --> 01:18:09,520
nicotine acetyl choline receptor and

1787
01:18:06,679 --> 01:18:12,920
nicotine itself lies in between an alpha

1788
01:18:09,520 --> 01:18:15,199
and beta subunit and then basically uh

1789
01:18:12,920 --> 01:18:17,679
either through agonism or antagonism you

1790
01:18:15,199 --> 01:18:21,239
also modulate iron flux so this could be

1791
01:18:17,679 --> 01:18:24,239
a potential mechanism of action um to

1792
01:18:21,239 --> 01:18:25,800
rescue um neuronal cells so then what

1793
01:18:24,239 --> 01:18:28,040
we've done is that we looked again in

1794
01:18:25,800 --> 01:18:29,840
the broad 800k library for molecules

1795
01:18:28,040 --> 01:18:32,760
that contain a nicotine substructure and

1796
01:18:29,840 --> 01:18:36,440
that had a high score from our neural

1797
01:18:32,760 --> 01:18:37,400
network and 11 of the tested molecules

1798
01:18:36,440 --> 01:18:41,760
all had

1799
01:18:37,400 --> 01:18:44,480
nanomolar potencies in rescuing um the

1800
01:18:41,760 --> 01:18:47,000
neuronal cells from the Marine toxin so

1801
01:18:44,480 --> 01:18:51,440
that doesn't mean that if you like are

1802
01:18:47,000 --> 01:18:53,679
exposed to shelfish sell shellfish tox

1803
01:18:51,440 --> 01:18:57,480
poisoning that you need to start smoking

1804
01:18:53,679 --> 01:19:01,239
but um it is a nice indication that this

1805
01:18:57,480 --> 01:19:04,080
chemotype um might be like leading to

1806
01:19:01,239 --> 01:19:05,400
specific mechanism of action so now that

1807
01:19:04,080 --> 01:19:08,480
we have

1808
01:19:05,400 --> 01:19:10,199
specific um protein structure in mind

1809
01:19:08,480 --> 01:19:12,920
the pentomic structure like there are

1810
01:19:10,199 --> 01:19:15,280
experimental X-ray and crym structures

1811
01:19:12,920 --> 01:19:17,400
of the nicotin actil cooline receptor

1812
01:19:15,280 --> 01:19:21,920
what we can do now is use these very

1813
01:19:17,400 --> 01:19:24,719
large chemical spaces and rely on a um

1814
01:19:21,920 --> 01:19:25,679
instead of from a Lian based discovery

1815
01:19:24,719 --> 01:19:28,120
campaign we can move to a

1816
01:19:25,679 --> 01:19:30,639
structure-based drug Discovery campaign

1817
01:19:28,120 --> 01:19:34,560
and basically build uh molecular docking

1818
01:19:30,639 --> 01:19:36,760
models of these subunits and then uh use

1819
01:19:34,560 --> 01:19:39,120
the uh scoring function of this

1820
01:19:36,760 --> 01:19:40,840
molecular docking model to navigate this

1821
01:19:39,120 --> 01:19:42,840
very large chemical space of billions of

1822
01:19:40,840 --> 01:19:45,320
molecules and in conjunction with their

1823
01:19:42,840 --> 01:19:47,880
neural network look for compounds that

1824
01:19:45,320 --> 01:19:52,760
have a good interaction energy and a

1825
01:19:47,880 --> 01:19:56,360
good um phenotypic learned score for

1826
01:19:52,760 --> 01:19:58,000
um inhibiting this toxin so what we have

1827
01:19:56,360 --> 01:20:00,199
done in the first step is that we looked

1828
01:19:58,000 --> 01:20:02,320
for molecules in this billion space that

1829
01:20:00,199 --> 01:20:04,280
contain the nicotine substructure and

1830
01:20:02,320 --> 01:20:06,199
now we've docked them against this

1831
01:20:04,280 --> 01:20:08,239
against this system but what we're also

1832
01:20:06,199 --> 01:20:10,320
trying to do is find molecules that do

1833
01:20:08,239 --> 01:20:12,679
not have a nicotine substructure and see

1834
01:20:10,320 --> 01:20:15,320
if we can find an entire new scaffold

1835
01:20:12,679 --> 01:20:19,760
that would also exhibit um the cell

1836
01:20:15,320 --> 01:20:21,480
rescue phenotype so with this I hope I

1837
01:20:19,760 --> 01:20:24,120
was able to convince you that there's

1838
01:20:21,480 --> 01:20:26,440
many ways of using machine learning to

1839
01:20:24,120 --> 01:20:29,239
um navigate very large chemical spaces

1840
01:20:26,440 --> 01:20:30,760
and um that there's multiple layers

1841
01:20:29,239 --> 01:20:34,040
where you can use machine learning so we

1842
01:20:30,760 --> 01:20:36,880
can also use Alpha folds um or the alpha

1843
01:20:34,040 --> 01:20:39,239
flow as was mentioned earlier to create

1844
01:20:36,880 --> 01:20:43,239
ensembles of these structures we can we

1845
01:20:39,239 --> 01:20:45,480
have also um buil neural networks that

1846
01:20:43,239 --> 01:20:48,360
were trained on cytotoxicity of of

1847
01:20:45,480 --> 01:20:50,080
normal cells um that could be an

1848
01:20:48,360 --> 01:20:52,159
additional filter for these billions of

1849
01:20:50,080 --> 01:20:53,840
molecules to enrich for molecules that

1850
01:20:52,159 --> 01:20:58,080
would not be cytotoxic but would still

1851
01:20:53,840 --> 01:21:01,159
rescue the the neuronal cells from the

1852
01:20:58,080 --> 01:21:03,400
Marine toxin so yeah this is a very much

1853
01:21:01,159 --> 01:21:08,679
ongoing project and we're very curious

1854
01:21:03,400 --> 01:21:08,679
to see where we end up with it thank you

