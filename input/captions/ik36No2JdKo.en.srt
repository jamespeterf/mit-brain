1
00:00:00,000 --> 00:00:10,928

2
00:00:10,928 --> 00:00:12,470
CAROLYN TIERNAN:
Without further ado,

3
00:00:12,470 --> 00:00:15,410
I will hand the mic over
to Bill Bonvillian, who

4
00:00:15,410 --> 00:00:17,830
will be the moderator
of this session today.

5
00:00:17,830 --> 00:00:19,640
Many of you will know Bill.

6
00:00:19,640 --> 00:00:22,370
He's a lecturer at MIT
and senior director

7
00:00:22,370 --> 00:00:24,920
for special projects
at Open Learning,

8
00:00:24,920 --> 00:00:27,858
and he will introduce our
guests, Eric and Kate.

9
00:00:27,858 --> 00:00:29,150
I'll hand it over to you, Bill.

10
00:00:29,150 --> 00:00:31,350
WILLIAM BONVILLIAN: Thanks,
Carolyn, appreciate it.

11
00:00:31,350 --> 00:00:36,500
And let me just introduce
briefly Kate and Eric.

12
00:00:36,500 --> 00:00:41,000
Eric is a professor at MIT and a
director of the Scheller Teacher

13
00:00:41,000 --> 00:00:44,510
Education Program and the
Education Arcade at MIT.

14
00:00:44,510 --> 00:00:48,080
He's head of MIT'S Department
of Comparative Media Studies

15
00:00:48,080 --> 00:00:49,190
and Writing.

16
00:00:49,190 --> 00:00:52,310
He is an expert on the
application of technology

17
00:00:52,310 --> 00:00:55,490
to education, and his
research has really

18
00:00:55,490 --> 00:00:58,340
focused on computer
gaming and simulations

19
00:00:58,340 --> 00:01:01,110
for building understanding
of science, technology,

20
00:01:01,110 --> 00:01:02,210
engineering.

21
00:01:02,210 --> 00:01:05,090
He's the author of four
books on these subjects.

22
00:01:05,090 --> 00:01:08,390
And Kate Moore is a
research scientist

23
00:01:08,390 --> 00:01:11,180
who studies how to teach
middle and high school

24
00:01:11,180 --> 00:01:13,790
students about
systems and ethics

25
00:01:13,790 --> 00:01:15,960
behind artificial intelligence.

26
00:01:15,960 --> 00:01:18,980
She earned her doctoral degree
at Columbia University's

27
00:01:18,980 --> 00:01:22,650
Teachers College, and before
entering the field of education

28
00:01:22,650 --> 00:01:27,050
and research, Kate had
on-the-ground experience serving

29
00:01:27,050 --> 00:01:30,560
as a middle school science
and special education

30
00:01:30,560 --> 00:01:32,150
teacher for 10 years.

31
00:01:32,150 --> 00:01:38,040
So we've got two very
talented discussants today.

32
00:01:38,040 --> 00:01:39,700
Let me give a
brief introduction.

33
00:01:39,700 --> 00:01:42,300
We've discussed many topics,
as Carolyn suggested,

34
00:01:42,300 --> 00:01:46,210
in our J-WEL series
on AI and education,

35
00:01:46,210 --> 00:01:49,080
and now we turn to
a crucial one, which

36
00:01:49,080 --> 00:01:52,170
is how teachers in
classrooms are using it

37
00:01:52,170 --> 00:01:53,830
and the challenges they face.

38
00:01:53,830 --> 00:01:59,490
So the availability of
generative AI tools like ChatGPT

39
00:01:59,490 --> 00:02:03,480
is, as we all know, a new
disruptive force in education.

40
00:02:03,480 --> 00:02:07,870
And MIT's Scheller
Education Program,

41
00:02:07,870 --> 00:02:12,660
which Eric directs and where
Kate is a research scientist,

42
00:02:12,660 --> 00:02:16,110
this program is engaged in
activities to both understand

43
00:02:16,110 --> 00:02:20,190
how teachers are impacted by
the arrival of AI and help

44
00:02:20,190 --> 00:02:24,240
prepare them and their
students to use AI tools

45
00:02:24,240 --> 00:02:26,620
both ethically and effectively.

46
00:02:26,620 --> 00:02:30,150
And they see generative AI as
offering real opportunities

47
00:02:30,150 --> 00:02:32,730
in education, yet
many first attempts

48
00:02:32,730 --> 00:02:35,790
will miss opportunities
for connection as well.

49
00:02:35,790 --> 00:02:41,250
How do teachers and school
districts use generative AI?

50
00:02:41,250 --> 00:02:43,230
Well, the way they
use it, it's going

51
00:02:43,230 --> 00:02:45,083
to affect gaps in education.

52
00:02:45,083 --> 00:02:46,500
Those who know how
to use the tool

53
00:02:46,500 --> 00:02:50,830
effectively could show
significant, remarkable gains

54
00:02:50,830 --> 00:02:53,070
even in learning.

55
00:02:53,070 --> 00:02:55,810
Those who don't may miss
out on these opportunities.

56
00:02:55,810 --> 00:02:58,920
So that's why teacher
and student AI literacy

57
00:02:58,920 --> 00:03:00,780
is so important.

58
00:03:00,780 --> 00:03:03,570
And Kate and Eric are
both deeply involved

59
00:03:03,570 --> 00:03:05,790
in a series of AI
fluency projects

60
00:03:05,790 --> 00:03:11,070
that support students and
teachers in their creative and--

61
00:03:11,070 --> 00:03:14,050
then the connected use
of AI for learning.

62
00:03:14,050 --> 00:03:19,175
So let me turn first to Eric,
and then we'll turn to Kate.

63
00:03:19,175 --> 00:03:20,300
ERIC KLOPFER: Thanks, Bill.

64
00:03:20,300 --> 00:03:23,200
I'm going to share
some slides here.

65
00:03:23,200 --> 00:03:23,740
Great.

66
00:03:23,740 --> 00:03:27,100
So thanks for having us here.

67
00:03:27,100 --> 00:03:29,980
I'm going to give a
quick overview of some

68
00:03:29,980 --> 00:03:34,330
of the frameworks we use to
think about AI education,

69
00:03:34,330 --> 00:03:35,920
and then I'm going
to turn it over

70
00:03:35,920 --> 00:03:38,770
to Kate for some more deep
dives into some case studies

71
00:03:38,770 --> 00:03:41,570
that she's been working
on as part of her work,

72
00:03:41,570 --> 00:03:43,790
working deeply with
teachers in schools,

73
00:03:43,790 --> 00:03:45,477
thinking about both
those opportunities,

74
00:03:45,477 --> 00:03:47,560
as Bill was just talking
about, as-- opportunities

75
00:03:47,560 --> 00:03:51,100
that they have as well as the
challenges that they face.

76
00:03:51,100 --> 00:03:54,937
And this slide here, I think,
really summarizes also--

77
00:03:54,937 --> 00:03:57,520
and where Bill was going, it's
sort of about the opportunities

78
00:03:57,520 --> 00:03:59,230
and the potential costs.

79
00:03:59,230 --> 00:04:01,512
The bottom line here is that
we work from the premise

80
00:04:01,512 --> 00:04:03,220
that we don't think
that people are going

81
00:04:03,220 --> 00:04:07,040
to be replaced by AI
in many professions,

82
00:04:07,040 --> 00:04:10,150
but we do think that people who
know AI will replace people who

83
00:04:10,150 --> 00:04:12,370
don't know AI in many places.

84
00:04:12,370 --> 00:04:14,260
And so our goal is
to prepare everybody

85
00:04:14,260 --> 00:04:16,000
to really be able to
use AI effectively

86
00:04:16,000 --> 00:04:18,550
and creatively and ethically
as part of their work

87
00:04:18,550 --> 00:04:21,829
and as part of their lives.

88
00:04:21,829 --> 00:04:23,630
My work is part of
a larger initiative

89
00:04:23,630 --> 00:04:27,140
at MIT called RAISE, Responsible
AI for Social Empowerment

90
00:04:27,140 --> 00:04:28,580
and Education.

91
00:04:28,580 --> 00:04:31,640
I'm one of the co-PIs
along with Hal Abelson,

92
00:04:31,640 --> 00:04:36,570
and Cynthia Breazeal is
the PI of this project.

93
00:04:36,570 --> 00:04:38,240
And this is a
comprehensive project

94
00:04:38,240 --> 00:04:41,000
across many
different subprojects

95
00:04:41,000 --> 00:04:43,920
where we're working
on both AI education,

96
00:04:43,920 --> 00:04:46,640
teaching people about
AI, as well as thinking

97
00:04:46,640 --> 00:04:48,350
about ways that we
can use AI to help

98
00:04:48,350 --> 00:04:52,040
people learn more effectively.

99
00:04:52,040 --> 00:04:54,450
The goals of RAISE,
just real quickly,

100
00:04:54,450 --> 00:04:56,960
are to advance equity
in learning, education,

101
00:04:56,960 --> 00:04:59,000
and empowerment through AI.

102
00:04:59,000 --> 00:05:01,850
We do this by rethinking and
innovating how to holistically

103
00:05:01,850 --> 00:05:04,640
and equitably prepare
diverse K-12 students

104
00:05:04,640 --> 00:05:06,662
and an inclusive workforce.

105
00:05:06,662 --> 00:05:08,870
And we want them to be happy,
engaged, and successful

106
00:05:08,870 --> 00:05:10,740
in an increasingly
AI-powered society.

107
00:05:10,740 --> 00:05:13,697
So this isn't just
about career readiness.

108
00:05:13,697 --> 00:05:15,530
That is one component
of it, because we also

109
00:05:15,530 --> 00:05:19,160
want them to be sort of happy
and engaged citizens and people

110
00:05:19,160 --> 00:05:21,370
in the world.

111
00:05:21,370 --> 00:05:23,120
I'm going to lay out
some principles here,

112
00:05:23,120 --> 00:05:25,245
which I call the Roadmap
for Transforming Education

113
00:05:25,245 --> 00:05:28,330
with AI, which is just
some principles about how

114
00:05:28,330 --> 00:05:31,760
we think about using AI in
schools and in education.

115
00:05:31,760 --> 00:05:34,570
And I think this will help
establish some baseline stuff

116
00:05:34,570 --> 00:05:37,210
when I turn it over to Kate.

117
00:05:37,210 --> 00:05:40,150
And many of you may know
Bloom's taxonomy, the idea

118
00:05:40,150 --> 00:05:41,230
that we have--

119
00:05:41,230 --> 00:05:42,860
at the baseline,
we have remember,

120
00:05:42,860 --> 00:05:44,470
and we push this
all up to create.

121
00:05:44,470 --> 00:05:48,460
And we really want to be pushing
people up along this taxonomy

122
00:05:48,460 --> 00:05:51,010
in many activities that
they do, including the ways

123
00:05:51,010 --> 00:05:52,820
that we use AI in schools.

124
00:05:52,820 --> 00:05:55,100
So we want people to
know and understand AI.

125
00:05:55,100 --> 00:05:57,010
We want people to
use and apply AI,

126
00:05:57,010 --> 00:06:00,340
and we really want them
to evaluate and create AI.

127
00:06:00,340 --> 00:06:04,760
And in talking with Kate about
this diagram just yesterday

128
00:06:04,760 --> 00:06:08,680
or the day before, she said,
the interesting thing about AI

129
00:06:08,680 --> 00:06:11,428
is that creating
and evaluating is--

130
00:06:11,428 --> 00:06:13,720
we should put it at the top,
but it doesn't necessarily

131
00:06:13,720 --> 00:06:17,560
require people to analyze
and apply and understand it

132
00:06:17,560 --> 00:06:18,560
as they're doing that.

133
00:06:18,560 --> 00:06:20,518
So we really need to
think about taxonomy maybe

134
00:06:20,518 --> 00:06:22,240
more as like a pillar,
that they really

135
00:06:22,240 --> 00:06:23,990
need to be able to do
all of these things.

136
00:06:23,990 --> 00:06:26,240
They need to be able to
create, evaluate, and analyze,

137
00:06:26,240 --> 00:06:29,090
but they also need to be able to
apply and understand it as well.

138
00:06:29,090 --> 00:06:31,007
And I think that's a
really important comment.

139
00:06:31,007 --> 00:06:33,430

140
00:06:33,430 --> 00:06:35,815
As Bill mentioned
our word fluency,

141
00:06:35,815 --> 00:06:37,690
we're trying to move
away from "AI literacy,"

142
00:06:37,690 --> 00:06:40,232
even though people use that word
a little bit more frequently

143
00:06:40,232 --> 00:06:43,140
than "fluency," but we really
want people to be much more--

144
00:06:43,140 --> 00:06:46,080
have a much deeper understanding
than just literacy, which

145
00:06:46,080 --> 00:06:48,258
is maybe a superficial
understanding, where

146
00:06:48,258 --> 00:06:49,300
they'd be fluent with it.

147
00:06:49,300 --> 00:06:51,538
So they're using it
readily in activities

148
00:06:51,538 --> 00:06:54,080
when it's appropriate, figuring
out when it's not appropriate

149
00:06:54,080 --> 00:06:56,420
and not using it
in those places.

150
00:06:56,420 --> 00:06:58,320
And we think about these
different components

151
00:06:58,320 --> 00:07:02,205
about ethics and AI, core
concepts, creativity,

152
00:07:02,205 --> 00:07:03,720
career readiness.

153
00:07:03,720 --> 00:07:08,780
All of these are foundations for
what we think about as fluency.

154
00:07:08,780 --> 00:07:10,390
I do want to--

155
00:07:10,390 --> 00:07:12,605
this is a common metaphor
that we see a lot now,

156
00:07:12,605 --> 00:07:14,730
a professor in every pocket
or something like this,

157
00:07:14,730 --> 00:07:17,490
a tutor on everybody's screen.

158
00:07:17,490 --> 00:07:19,490
Everybody's going to be
able to get much smarter

159
00:07:19,490 --> 00:07:20,960
and do better because we
have tutors that are going

160
00:07:20,960 --> 00:07:22,500
to be available to everybody.

161
00:07:22,500 --> 00:07:23,480
And this is something
that's pushed

162
00:07:23,480 --> 00:07:25,280
a lot in a lot of
different-- from a lot

163
00:07:25,280 --> 00:07:27,867
of different companies and
a lot of different sources.

164
00:07:27,867 --> 00:07:30,200
I think we need to think
about, in this model that we're

165
00:07:30,200 --> 00:07:32,700
going to have a professor who's
going to be able to teach us

166
00:07:32,700 --> 00:07:34,430
everything in our
pockets, we need

167
00:07:34,430 --> 00:07:37,850
to ask fundamental questions
about, does AI improve learning?

168
00:07:37,850 --> 00:07:39,575
Can we define learning
in that context?

169
00:07:39,575 --> 00:07:40,700
What does it mean to learn?

170
00:07:40,700 --> 00:07:42,380
And how do we think
about these tools being

171
00:07:42,380 --> 00:07:44,570
used in association with
those different definitions

172
00:07:44,570 --> 00:07:45,443
of learning?

173
00:07:45,443 --> 00:07:47,360
One important thing--
it's maybe a little hard

174
00:07:47,360 --> 00:07:48,702
to see on your screens here.

175
00:07:48,702 --> 00:07:50,660
At the beginning of the
semester, my colleague,

176
00:07:50,660 --> 00:07:52,827
Justin Reich, who runs the
Teaching Systems Lab here

177
00:07:52,827 --> 00:07:57,200
at MIT, posted some
information about a new poster

178
00:07:57,200 --> 00:07:59,850
that they developed, and I'll
have a link to this later on.

179
00:07:59,850 --> 00:08:02,450
It was some guidelines around
AI for schools at the beginning

180
00:08:02,450 --> 00:08:04,970
of this current year.

181
00:08:04,970 --> 00:08:08,310
The very first one of those
things was that AI will not

182
00:08:08,310 --> 00:08:11,400
replace teachers, and right
above that, in my Twitter feed,

183
00:08:11,400 --> 00:08:14,580
where I saw this, was this
commercial for a tool that says,

184
00:08:14,580 --> 00:08:18,000
"One app replaces teachers,
tutors and textbooks."

185
00:08:18,000 --> 00:08:20,172
And this is really not
what we're thinking about.

186
00:08:20,172 --> 00:08:21,130
This is not our design.

187
00:08:21,130 --> 00:08:23,005
We're not looking to
replace teachers or even

188
00:08:23,005 --> 00:08:23,960
textbooks perhaps.

189
00:08:23,960 --> 00:08:25,710
We're thinking about,
how does these tools

190
00:08:25,710 --> 00:08:28,920
get integrated into an ecosystem
that involves teachers, that

191
00:08:28,920 --> 00:08:32,100
involves peers, that
involves traditional media

192
00:08:32,100 --> 00:08:34,520
but also involves AI
along with those things?

193
00:08:34,520 --> 00:08:37,409

194
00:08:37,409 --> 00:08:39,450
One of the reasons I
think that people are so

195
00:08:39,450 --> 00:08:41,653
attracted to this
idea of the AI tutor

196
00:08:41,653 --> 00:08:43,320
that can teach you
everything is that we

197
00:08:43,320 --> 00:08:45,760
need to move beyond retention
to much deeper learning.

198
00:08:45,760 --> 00:08:48,540
Yes, I think you can-- if you
want to think about quick recall

199
00:08:48,540 --> 00:08:50,280
and understanding
of some math facts,

200
00:08:50,280 --> 00:08:53,110
probably some of these AI tutors
can get you there very quickly.

201
00:08:53,110 --> 00:08:55,193
But we need to be
thinking about learning

202
00:08:55,193 --> 00:08:56,610
that's much more
durable than that

203
00:08:56,610 --> 00:08:58,485
and that's much more
comprehensive than that.

204
00:08:58,485 --> 00:09:03,090
Again, with AI tools readily
available and computational

205
00:09:03,090 --> 00:09:06,263
tools more generally available,
we need to be thinking about,

206
00:09:06,263 --> 00:09:07,680
what's the value
added that people

207
00:09:07,680 --> 00:09:10,150
will have inside lots of
different kinds of ecosystems?

208
00:09:10,150 --> 00:09:12,442
And those are not going to
be to do the kinds of things

209
00:09:12,442 --> 00:09:15,940
that tutors can teach
you really quickly.

210
00:09:15,940 --> 00:09:18,760
We need to be able to design
things for the whole student.

211
00:09:18,760 --> 00:09:21,230
As we think about designing
educational technologies,

212
00:09:21,230 --> 00:09:22,813
we often think about,
OK, this student

213
00:09:22,813 --> 00:09:25,395
needs to learn this thing to
get past this benchmark here,

214
00:09:25,395 --> 00:09:26,770
but we really need
to be thinking

215
00:09:26,770 --> 00:09:28,780
about who that student
is, what position do they

216
00:09:28,780 --> 00:09:30,460
have in their
community, in the world,

217
00:09:30,460 --> 00:09:32,890
with their friends and peers.

218
00:09:32,890 --> 00:09:34,845
What kind of identity
do they have?

219
00:09:34,845 --> 00:09:36,220
Thinking about
that whole student

220
00:09:36,220 --> 00:09:37,595
can really help
transform the way

221
00:09:37,595 --> 00:09:40,660
we think about these tools that
are not just delivering facts

222
00:09:40,660 --> 00:09:44,470
to them but helping them
understand deeper concepts

223
00:09:44,470 --> 00:09:47,103
to develop identity about who
they are and what they're doing.

224
00:09:47,103 --> 00:09:48,520
Those kinds of
things are actually

225
00:09:48,520 --> 00:09:49,603
much more difficult to do.

226
00:09:49,603 --> 00:09:51,728
I think we can get there
with a lot of these tools,

227
00:09:51,728 --> 00:09:53,950
but it's going to require
a much deeper thought

228
00:09:53,950 --> 00:09:58,810
than just getting people to
respond to math questions.

229
00:09:58,810 --> 00:10:00,340
We do need to do research.

230
00:10:00,340 --> 00:10:03,290
I run a reading group here on
campus around AI education.

231
00:10:03,290 --> 00:10:04,940
Kate is a part of this as well.

232
00:10:04,940 --> 00:10:08,820
And one week we read two papers.

233
00:10:08,820 --> 00:10:11,000
These are paraphrases
of those two papers.

234
00:10:11,000 --> 00:10:14,470
One was called "Large Language
Models," or things like ChatGPT,

235
00:10:14,470 --> 00:10:17,590
"May Harm Learning," and the
other one was called "LLMs

236
00:10:17,590 --> 00:10:19,810
Perform as Well
as Human Tutors."

237
00:10:19,810 --> 00:10:22,570
And these were both papers
that were peer-reviewed

238
00:10:22,570 --> 00:10:25,640
and published, and they both
have some merit to them.

239
00:10:25,640 --> 00:10:28,010
There's qualifications
around a lot of the things.

240
00:10:28,010 --> 00:10:29,710
As you can imagine
in research, there's

241
00:10:29,710 --> 00:10:30,940
a lot of qualifications
of the things

242
00:10:30,940 --> 00:10:32,190
we might think about in these.

243
00:10:32,190 --> 00:10:34,940
But the fact is that we don't
know a lot about where and when

244
00:10:34,940 --> 00:10:37,570
large language models and
AI can work effectively

245
00:10:37,570 --> 00:10:39,430
and where it works
counterproductively.

246
00:10:39,430 --> 00:10:41,680
And we need to be thinking
about doing research

247
00:10:41,680 --> 00:10:44,428
that helps us understand
that space about who, when,

248
00:10:44,428 --> 00:10:46,720
and where these different
kinds of things can be useful

249
00:10:46,720 --> 00:10:48,178
and when they can be harmful.

250
00:10:48,178 --> 00:10:49,720
The ways they're
harmful is sometimes

251
00:10:49,720 --> 00:10:52,600
it gets kids to think that
they are good at something

252
00:10:52,600 --> 00:10:54,110
because they have this AI help.

253
00:10:54,110 --> 00:10:56,590
When the help is
withdrawn, they actually

254
00:10:56,590 --> 00:10:58,010
don't understand it very well.

255
00:10:58,010 --> 00:10:59,600
So sometimes it
provides a false sense

256
00:10:59,600 --> 00:11:03,490
of understanding to them,
which can be harmful.

257
00:11:03,490 --> 00:11:05,530
A quick example
from my own class--

258
00:11:05,530 --> 00:11:07,870
we did a little
activity in my class

259
00:11:07,870 --> 00:11:10,460
where we gave a programming
assignment to students,

260
00:11:10,460 --> 00:11:13,540
and we gave three different
groups-- a group with ChatGPT,

261
00:11:13,540 --> 00:11:17,080
a group with Code Llama, which
is an open-source large language

262
00:11:17,080 --> 00:11:19,490
model, and a group that
could only use Google,

263
00:11:19,490 --> 00:11:20,980
I first gave them
a task where they

264
00:11:20,980 --> 00:11:22,750
needed to do a
programming assignment

265
00:11:22,750 --> 00:11:24,260
and they could use these tools.

266
00:11:24,260 --> 00:11:25,180
And then I told them I
was going to give them

267
00:11:25,180 --> 00:11:27,650
a test afterwards where
they couldn't use the tools.

268
00:11:27,650 --> 00:11:29,350
And sure enough,
the ChatGPT group

269
00:11:29,350 --> 00:11:31,450
got it really quickly
on the first activity

270
00:11:31,450 --> 00:11:33,050
where they could use the tools.

271
00:11:33,050 --> 00:11:35,743
The Code Llama group struggled,
took a little bit longer,

272
00:11:35,743 --> 00:11:37,660
and finally, the Google
group took the longest

273
00:11:37,660 --> 00:11:38,930
to solve the problem.

274
00:11:38,930 --> 00:11:42,280
But then when I took the tools
away and I gave them a test,

275
00:11:42,280 --> 00:11:43,840
the ChatGPT group all failed.

276
00:11:43,840 --> 00:11:46,090
They could not understand--
they could not repeat what

277
00:11:46,090 --> 00:11:47,530
they had done with ChatGPT.

278
00:11:47,530 --> 00:11:48,465
The Code Llama group--

279
00:11:48,465 --> 00:11:50,840
about half of them got it
right, and half of them didn't.

280
00:11:50,840 --> 00:11:52,190
And the Google group
all got it right.

281
00:11:52,190 --> 00:11:54,040
So there's something
about struggling with this

282
00:11:54,040 --> 00:11:56,110
and taking a little bit
longer that can actually

283
00:11:56,110 --> 00:11:57,730
be really helpful
and effective when

284
00:11:57,730 --> 00:12:02,218
you think about where we use
those AI tools to help people.

285
00:12:02,218 --> 00:12:04,260
AI must support educators
in supporting students.

286
00:12:04,260 --> 00:12:05,935
Teachers are such an important
part of the ecosystem.

287
00:12:05,935 --> 00:12:07,935
We need to think about
how we integrate teachers

288
00:12:07,935 --> 00:12:10,430
into the process and not
remove them from the process

289
00:12:10,430 --> 00:12:12,740
by replacing them with tutors.

290
00:12:12,740 --> 00:12:15,068
Learning is so much more
than just learning facts.

291
00:12:15,068 --> 00:12:16,610
It's about those
social relationships

292
00:12:16,610 --> 00:12:19,190
and about caring
mentors and teachers.

293
00:12:19,190 --> 00:12:22,160
We need to think about how
we make those people more

294
00:12:22,160 --> 00:12:24,380
empowered to do their
job and not remove them

295
00:12:24,380 --> 00:12:26,830
more from the process.

296
00:12:26,830 --> 00:12:29,290
That means educators need to
be part of the AI conversation

297
00:12:29,290 --> 00:12:30,843
as we're designing tools.

298
00:12:30,843 --> 00:12:33,010
Sometimes tools are designed
by technology companies

299
00:12:33,010 --> 00:12:35,730
without really thinking about
who the educators really are,

300
00:12:35,730 --> 00:12:37,272
and those educators
really need to be

301
00:12:37,272 --> 00:12:39,442
a part of the conversation.

302
00:12:39,442 --> 00:12:40,900
They need to stay
up to date on AI.

303
00:12:40,900 --> 00:12:44,190
This is a course we developed
with Google, Generative AI

304
00:12:44,190 --> 00:12:45,070
for Educators.

305
00:12:45,070 --> 00:12:49,320
It's a two-hour course open
to anybody who can just

306
00:12:49,320 --> 00:12:52,400
learn from the fundamentals
of learning about AI,

307
00:12:52,400 --> 00:12:53,650
and this was really effective.

308
00:12:53,650 --> 00:12:56,150
We've had many, many people
from around the world take this.

309
00:12:56,150 --> 00:12:58,820
It's now available in several
other languages as well.

310
00:12:58,820 --> 00:13:00,570
But this has been
really a nice experience

311
00:13:00,570 --> 00:13:01,960
to help people stay up to date.

312
00:13:01,960 --> 00:13:03,120
And it's going to be
something that people

313
00:13:03,120 --> 00:13:04,210
need to do regularly.

314
00:13:04,210 --> 00:13:05,835
AI is not going to
be the kind of thing

315
00:13:05,835 --> 00:13:09,773
that you learn once and then
can sit on for five or 10 years.

316
00:13:09,773 --> 00:13:11,190
It's going to be
the kind of thing

317
00:13:11,190 --> 00:13:15,078
that teachers need to stay
up to date on a yearly basis.

318
00:13:15,078 --> 00:13:17,370
I think one of the most
important things about learning

319
00:13:17,370 --> 00:13:19,287
about AI is to create
communities, experiment,

320
00:13:19,287 --> 00:13:20,050
and share.

321
00:13:20,050 --> 00:13:21,610
The best way that
people can learn,

322
00:13:21,610 --> 00:13:24,120
educators can learn is
from other educators,

323
00:13:24,120 --> 00:13:26,370
so really thinking about
ways that teachers can create

324
00:13:26,370 --> 00:13:29,170
communities, share their
experiences with each other,

325
00:13:29,170 --> 00:13:31,380
learn from each other
what worked, what didn't.

326
00:13:31,380 --> 00:13:32,963
It's going to be
really important part

327
00:13:32,963 --> 00:13:35,850
of advancing our understanding.

328
00:13:35,850 --> 00:13:38,760
And policy, educators being
an important part of policy

329
00:13:38,760 --> 00:13:41,980
as well, we're going to see
policies at the school level,

330
00:13:41,980 --> 00:13:45,000
district level, state
level, federal level

331
00:13:45,000 --> 00:13:47,863
emerging over the coming
years, and really, teachers

332
00:13:47,863 --> 00:13:49,530
need to be a part of
those conversations

333
00:13:49,530 --> 00:13:51,613
because they're the ones
who really understand how

334
00:13:51,613 --> 00:13:54,050
this is working in a classroom.

335
00:13:54,050 --> 00:13:56,690
I'll just quickly mention
a project here just

336
00:13:56,690 --> 00:13:59,270
to give you the kinds of
things that we're working on.

337
00:13:59,270 --> 00:14:01,413
CAIL is a project we're
working on in our lab.

338
00:14:01,413 --> 00:14:03,080
It's really thinking
about, what happens

339
00:14:03,080 --> 00:14:06,410
when we move from an AI
tutor on a one-to-one basis

340
00:14:06,410 --> 00:14:09,350
where an AI tutor is
actually now part of a group?

341
00:14:09,350 --> 00:14:11,600
As I mentioned, I think
that collaborative learning

342
00:14:11,600 --> 00:14:13,610
and learning with
peers is so important

343
00:14:13,610 --> 00:14:16,700
a part of the process
for so many students.

344
00:14:16,700 --> 00:14:20,120
And CAIL is a way
of thinking about,

345
00:14:20,120 --> 00:14:22,550
how does an AI agent become
a member of that community?

346
00:14:22,550 --> 00:14:26,310
How does an AI agent actually
help the group work together

347
00:14:26,310 --> 00:14:28,470
so it's not just about
people learning from AI

348
00:14:28,470 --> 00:14:31,880
but people learning
from each other?

349
00:14:31,880 --> 00:14:33,990
I do think-- I won't spend
a lot of time on this.

350
00:14:33,990 --> 00:14:35,970
I do think the future is
bright if we think about this.

351
00:14:35,970 --> 00:14:37,970
I don't want to sound
too negative about AI

352
00:14:37,970 --> 00:14:39,300
in the future of education.

353
00:14:39,300 --> 00:14:41,190
I'm very optimistic about it.

354
00:14:41,190 --> 00:14:43,370
But we need to be thinking
about it carefully,

355
00:14:43,370 --> 00:14:45,710
with research behind
it, and thinking

356
00:14:45,710 --> 00:14:52,820
about ways that we can advance
this in an evidence-based way.

357
00:14:52,820 --> 00:14:55,863
This is, just real quickly-- and
I can share these slides later

358
00:14:55,863 --> 00:14:57,030
so you can see these things.

359
00:14:57,030 --> 00:15:00,650
This is Justin Reich's
poster on "Teachers on AI."

360
00:15:00,650 --> 00:15:03,950
This is a report that we
released on "Generative AI

361
00:15:03,950 --> 00:15:05,040
and K-12 Education--

362
00:15:05,040 --> 00:15:08,720
An MIT Perspective," which
was released this past spring

363
00:15:08,720 --> 00:15:10,010
or summer.

364
00:15:10,010 --> 00:15:14,120
And we also have a policy paper
that we released this past fall,

365
00:15:14,120 --> 00:15:16,850
"How Policy Can Help Ensure
the Proper Use of AI in K-12

366
00:15:16,850 --> 00:15:17,720
Education."

367
00:15:17,720 --> 00:15:18,720
These are all available.

368
00:15:18,720 --> 00:15:20,303
I can share the
slides and the QR code

369
00:15:20,303 --> 00:15:23,090
so that people can
download those later.

370
00:15:23,090 --> 00:15:25,017
With that, I will
turn it over to Kate.

371
00:15:25,017 --> 00:15:26,100
KATE MOORE: Hi, everybody.

372
00:15:26,100 --> 00:15:27,350
I'm Kate Moore.

373
00:15:27,350 --> 00:15:30,050
Thank you so much for
the introduction, Bill.

374
00:15:30,050 --> 00:15:34,790
I'm a research scientist
focusing on AI in education.

375
00:15:34,790 --> 00:15:39,770
I work with Eric and many other
researchers at the STEP Lab,

376
00:15:39,770 --> 00:15:42,890
and I've been invited to speak
a little bit about the work I've

377
00:15:42,890 --> 00:15:47,510
been doing with Irene Lee,
who is at the Education

378
00:15:47,510 --> 00:15:49,320
Arcade, the MIT
STEP Lab with us,

379
00:15:49,320 --> 00:15:51,830
and also at New Mexico
State University,

380
00:15:51,830 --> 00:15:54,260
and also with Dr.
Helen Zhang, who's

381
00:15:54,260 --> 00:15:58,250
at the Lynch School of
Education at Boston College.

382
00:15:58,250 --> 00:16:00,440
What I wanted to speak
about was a project

383
00:16:00,440 --> 00:16:03,860
called Everyday AI,
which I've sent a link to

384
00:16:03,860 --> 00:16:05,600
in the chat for you.

385
00:16:05,600 --> 00:16:09,950
Everyday AI is a middle
school curriculum

386
00:16:09,950 --> 00:16:16,710
that focuses on developing AI
literacy for middle school,

387
00:16:16,710 --> 00:16:18,660
and we've also seen it
applied in high school.

388
00:16:18,660 --> 00:16:21,470
But we're very open to receiving
the term "fluency," Eric.

389
00:16:21,470 --> 00:16:23,477
It's just that we've been
working with the word

390
00:16:23,477 --> 00:16:25,060
"literacy," and it's
in the title now.

391
00:16:25,060 --> 00:16:26,760
So we're going to stick with it.

392
00:16:26,760 --> 00:16:30,360
And on top of that
curriculum, we

393
00:16:30,360 --> 00:16:34,050
have been working
since 2021 on a teacher

394
00:16:34,050 --> 00:16:36,420
professional
development project.

395
00:16:36,420 --> 00:16:38,460
What that means is
we've really tackled

396
00:16:38,460 --> 00:16:46,140
this problem in education
where teachers are putting out

397
00:16:46,140 --> 00:16:47,410
dumpster fires every day.

398
00:16:47,410 --> 00:16:48,520
They're working.

399
00:16:48,520 --> 00:16:49,192
They're fixing.

400
00:16:49,192 --> 00:16:49,900
They're building.

401
00:16:49,900 --> 00:16:50,608
They're creating.

402
00:16:50,608 --> 00:16:52,020
They're innovating.

403
00:16:52,020 --> 00:16:56,470
And they do not have time
to learn how AI works,

404
00:16:56,470 --> 00:16:59,130
how to teach their
students about AI.

405
00:16:59,130 --> 00:17:04,380
And what we've done is
tackled that problem

406
00:17:04,380 --> 00:17:08,339
by creating a professional
development program that

407
00:17:08,339 --> 00:17:13,680
connects teachers who are
often isolated with each other

408
00:17:13,680 --> 00:17:17,212
through a book club and
then through a opportunity

409
00:17:17,212 --> 00:17:19,920
to practice during a summer camp
where they can practice teaching

410
00:17:19,920 --> 00:17:21,099
this curriculum.

411
00:17:21,099 --> 00:17:25,260
And we've been working on
this for three years, four

412
00:17:25,260 --> 00:17:28,530
before I joined the program,
Irene and Helen working on this.

413
00:17:28,530 --> 00:17:32,110
And since then, we've
reached almost 70 teachers,

414
00:17:32,110 --> 00:17:35,080
2,000 students all
across the country.

415
00:17:35,080 --> 00:17:38,790
And what we're learning is
that teachers, once they

416
00:17:38,790 --> 00:17:41,820
are connected to a community
of people who are all chatting

417
00:17:41,820 --> 00:17:44,430
and learning about
education and once they

418
00:17:44,430 --> 00:17:48,880
practice teaching AI concepts or
AI fluency in their classrooms,

419
00:17:48,880 --> 00:17:54,970
they outperform expert
curriculum developers

420
00:17:54,970 --> 00:18:00,430
and experts in machine learning
when these experts taught

421
00:18:00,430 --> 00:18:02,310
the same curriculum.

422
00:18:02,310 --> 00:18:06,840
So what I mean is teachers
have the pedagogical wisdom

423
00:18:06,840 --> 00:18:10,980
and the insights into things
like culturally responsive

424
00:18:10,980 --> 00:18:14,070
pedagogy, what their students
love and are interested in that

425
00:18:14,070 --> 00:18:17,850
makes these challenging topics
really accessible and meaningful

426
00:18:17,850 --> 00:18:19,500
for their middle
school students.

427
00:18:19,500 --> 00:18:22,960
So what we're finding, just
to put a fine point on it,

428
00:18:22,960 --> 00:18:27,540
is that teachers are great at
advancing middle and high school

429
00:18:27,540 --> 00:18:28,810
AI literacy.

430
00:18:28,810 --> 00:18:30,930
What they need is proper
professional development

431
00:18:30,930 --> 00:18:33,550
to help them get there.

432
00:18:33,550 --> 00:18:36,400
I've had the privilege of
interviewing about 65 teachers

433
00:18:36,400 --> 00:18:39,310
across the country to
ask them about their work

434
00:18:39,310 --> 00:18:44,470
teaching AI concepts to
their middle school students.

435
00:18:44,470 --> 00:18:47,210
And talking to them
about this work,

436
00:18:47,210 --> 00:18:49,330
I've learned that
some of them are also

437
00:18:49,330 --> 00:18:51,470
trying to work with
large language models.

438
00:18:51,470 --> 00:18:54,380
Working with the PD has made
them feel comfortable with AI.

439
00:18:54,380 --> 00:18:57,370
So some of these teachers are
using large language models

440
00:18:57,370 --> 00:19:00,580
today for their own
work, such as ideation,

441
00:19:00,580 --> 00:19:03,140
developing ideas as if they're
working with a colleague,

442
00:19:03,140 --> 00:19:05,710
developing lesson
plans, rapid creation

443
00:19:05,710 --> 00:19:07,840
of differentiated materials.

444
00:19:07,840 --> 00:19:09,700
And all of this is--

445
00:19:09,700 --> 00:19:11,260
I'm just hearing
from some teachers

446
00:19:11,260 --> 00:19:13,450
that this is helping
them feel like they

447
00:19:13,450 --> 00:19:17,408
have an extra ear that they
can bounce ideas off of.

448
00:19:17,408 --> 00:19:19,700
And maybe, although I don't
have any evidence of this--

449
00:19:19,700 --> 00:19:21,617
maybe this is tackling
one of the great issues

450
00:19:21,617 --> 00:19:24,317
in teachers' work,
which is isolation.

451
00:19:24,317 --> 00:19:25,900
Particularly teachers
who are teaching

452
00:19:25,900 --> 00:19:28,750
about artificial intelligence
as part of their class,

453
00:19:28,750 --> 00:19:31,000
they're probably the only
one in the building teaching

454
00:19:31,000 --> 00:19:31,790
this content.

455
00:19:31,790 --> 00:19:35,510
So it's hard to find somebody
they can connect with to feel

456
00:19:35,510 --> 00:19:38,910
that they're not alone in this.

457
00:19:38,910 --> 00:19:40,150
I'm at my five minutes.

458
00:19:40,150 --> 00:19:42,930
I'm just going to take two
more to emphasize a few things

459
00:19:42,930 --> 00:19:45,660
that we've also found
from the research

460
00:19:45,660 --> 00:19:50,190
by interviewing and measuring
teacher and student learning

461
00:19:50,190 --> 00:19:51,660
from this project.

462
00:19:51,660 --> 00:19:57,210
One is that we've learned
that as teachers participate

463
00:19:57,210 --> 00:20:01,120
in this program, they start
to see artificial intelligence

464
00:20:01,120 --> 00:20:04,950
education or understanding
how AI works as not just

465
00:20:04,950 --> 00:20:06,963
about learning the
technical content

466
00:20:06,963 --> 00:20:08,880
but as something that's
important for a person

467
00:20:08,880 --> 00:20:11,910
to learn as part of
their being a citizen

468
00:20:11,910 --> 00:20:14,990
and part of their
engaging in society.

469
00:20:14,990 --> 00:20:18,710
And what that's shown
us is that AI literacy

470
00:20:18,710 --> 00:20:20,370
is interdepartmental.

471
00:20:20,370 --> 00:20:22,500
We started thinking
about this as something

472
00:20:22,500 --> 00:20:24,750
that would be championed by
computer science teachers,

473
00:20:24,750 --> 00:20:27,125
but what we're seeing is that
teachers in the Everyday AI

474
00:20:27,125 --> 00:20:30,540
project, they're librarians,
they're English teachers,

475
00:20:30,540 --> 00:20:33,530
they're social studies teachers
and math teachers and science

476
00:20:33,530 --> 00:20:34,740
teachers.

477
00:20:34,740 --> 00:20:37,760
So teachers of any discipline
can introduce their students

478
00:20:37,760 --> 00:20:41,540
to AI concepts with
great efficacy,

479
00:20:41,540 --> 00:20:47,000
and their students report having
positive experiences with it.

480
00:20:47,000 --> 00:20:48,950
One thing I'm also
hearing from teachers

481
00:20:48,950 --> 00:20:53,750
is that there are ways
that they have learned

482
00:20:53,750 --> 00:20:58,310
to identify bias in AI that have
been really helpful for them.

483
00:20:58,310 --> 00:21:01,700
For example--
actually, I'm going

484
00:21:01,700 --> 00:21:03,680
to give a personal example.

485
00:21:03,680 --> 00:21:05,660
I was teaching a class--

486
00:21:05,660 --> 00:21:07,310
this was last year--

487
00:21:07,310 --> 00:21:08,840
to graduate
students, but we were

488
00:21:08,840 --> 00:21:14,180
doing an activity where
you generate a movie

489
00:21:14,180 --> 00:21:19,940
poster with image generator,
generative AI tool.

490
00:21:19,940 --> 00:21:22,130
And one of my
students was trying

491
00:21:22,130 --> 00:21:25,460
to create the movie poster for
the movie Barbie, which had come

492
00:21:25,460 --> 00:21:27,230
out then and was very popular.

493
00:21:27,230 --> 00:21:31,710
And when we created
the movie poster,

494
00:21:31,710 --> 00:21:35,990
we noticed that Ken
was driving the car.

495
00:21:35,990 --> 00:21:38,010
And I don't know if you
guys have seen Barbie,

496
00:21:38,010 --> 00:21:40,220
but one of the main
things in this movie

497
00:21:40,220 --> 00:21:42,780
is that Ken does
not drive the car.

498
00:21:42,780 --> 00:21:44,480
Barbie does.

499
00:21:44,480 --> 00:21:47,000
But because the
model was trained

500
00:21:47,000 --> 00:21:50,000
on this overrepresented--
this dataset that

501
00:21:50,000 --> 00:21:53,250
had an overrepresentation
of men driving cars,

502
00:21:53,250 --> 00:21:55,850
the movie, one of the
main points of the movie,

503
00:21:55,850 --> 00:21:57,300
was misrepresented.

504
00:21:57,300 --> 00:22:00,560
And teachers who go through
professional development that

505
00:22:00,560 --> 00:22:03,440
helps them be prepared
to speak about these ways

506
00:22:03,440 --> 00:22:07,040
that bias can emerge
in generative AI tools

507
00:22:07,040 --> 00:22:09,380
are ready to jump
on those moments

508
00:22:09,380 --> 00:22:11,790
and speak to their students
about some of these ways

509
00:22:11,790 --> 00:22:13,350
that bias can emerge.

510
00:22:13,350 --> 00:22:15,640
And teachers without that
training might miss it,

511
00:22:15,640 --> 00:22:18,480
and that's where the harms might
come in, where students start

512
00:22:18,480 --> 00:22:24,250
noticing that information
that's generated by these tools,

513
00:22:24,250 --> 00:22:26,910
they might think
that this is normal.

514
00:22:26,910 --> 00:22:28,410
One of the things
that we've noticed

515
00:22:28,410 --> 00:22:31,710
is that there's an
overrepresentation

516
00:22:31,710 --> 00:22:34,930
of some groups over others
in the training dataset,

517
00:22:34,930 --> 00:22:40,140
and the risk is that students
get the idea that the people who

518
00:22:40,140 --> 00:22:42,030
are marginalized in
the dataset are also

519
00:22:42,030 --> 00:22:43,180
marginalized in society.

520
00:22:43,180 --> 00:22:44,973
And that can give really
difficult messages

521
00:22:44,973 --> 00:22:47,640
for young people who are trying
to find their place in the world

522
00:22:47,640 --> 00:22:49,210
and see themselves as leaders.

523
00:22:49,210 --> 00:22:51,360
So I'm being a little
bit vague because there's

524
00:22:51,360 --> 00:22:55,140
so many different ways that
bias can creep in and give

525
00:22:55,140 --> 00:22:57,930
confusing messages to
our youth, but that's

526
00:22:57,930 --> 00:23:00,037
one of the reasons AI
literacy is so important,

527
00:23:00,037 --> 00:23:02,370
to help teachers be prepared
to have these conversations

528
00:23:02,370 --> 00:23:04,490
about bias and I.

529
00:23:04,490 --> 00:23:06,980
So I think I'll
stop there because I

530
00:23:06,980 --> 00:23:11,600
did want to just emphasize
the fact that we wanted

531
00:23:11,600 --> 00:23:13,340
to have teachers
come to this talk

532
00:23:13,340 --> 00:23:16,640
to share their experiences
introducing concepts

533
00:23:16,640 --> 00:23:17,880
to their students.

534
00:23:17,880 --> 00:23:20,120
And just because of
the time of this talk,

535
00:23:20,120 --> 00:23:22,640
they're all in the classroom
right now, so what I did

536
00:23:22,640 --> 00:23:25,540
is I went in and I interviewed
three of our teachers.

537
00:23:25,540 --> 00:23:28,940
These are teachers who have been
participating in our project

538
00:23:28,940 --> 00:23:31,250
since the first or second
year of its beginning,

539
00:23:31,250 --> 00:23:32,750
and I'll ask Eric
if I could trouble

540
00:23:32,750 --> 00:23:33,805
you to share your screen.

541
00:23:33,805 --> 00:23:37,190

542
00:23:37,190 --> 00:23:38,720
Yeah, so these
are three teachers

543
00:23:38,720 --> 00:23:42,050
who kindly let me chat
with them after school.

544
00:23:42,050 --> 00:23:43,550
You'll see some of
them in the video

545
00:23:43,550 --> 00:23:46,610
I'm about to play are
actually in their classroom.

546
00:23:46,610 --> 00:23:49,640
And I want to introduce them
because each of these teachers

547
00:23:49,640 --> 00:23:52,590
not only went through the
PD that I described earlier,

548
00:23:52,590 --> 00:23:55,340
but then they returned
in subsequent years

549
00:23:55,340 --> 00:23:57,870
and led the PD themselves.

550
00:23:57,870 --> 00:24:01,680
And what we've found is that
when teachers teach teachers,

551
00:24:01,680 --> 00:24:06,320
we have the greatest impact on
student learning of AI concepts,

552
00:24:06,320 --> 00:24:08,750
and we have the greatest
growth in AI literacy,

553
00:24:08,750 --> 00:24:09,968
according to our measures.

554
00:24:09,968 --> 00:24:11,510
So it's these teachers
who are really

555
00:24:11,510 --> 00:24:14,660
showing that it's through
educators that we might

556
00:24:14,660 --> 00:24:17,510
be able to advance some of
the difficult conversations

557
00:24:17,510 --> 00:24:18,710
about AI.

558
00:24:18,710 --> 00:24:20,870
So let me give each teacher
a quick introduction,

559
00:24:20,870 --> 00:24:23,150
and then I'll ask Eric
to play the video.

560
00:24:23,150 --> 00:24:26,260
So on the left here
is Phylis Wilson.

561
00:24:26,260 --> 00:24:31,658
Phylis is a veteran teacher of
over 10 years with experience

562
00:24:31,658 --> 00:24:33,950
teaching in middle and high
school science and computer

563
00:24:33,950 --> 00:24:34,800
science.

564
00:24:34,800 --> 00:24:38,430
She served, as I said, as
what we call a facilitator

565
00:24:38,430 --> 00:24:41,130
during the Everyday AI project,
where she introduced teachers

566
00:24:41,130 --> 00:24:42,380
to AI concepts.

567
00:24:42,380 --> 00:24:45,840
Phylis is a master of strategies
for integrating AI education

568
00:24:45,840 --> 00:24:47,740
into STEM education.

569
00:24:47,740 --> 00:24:50,640
And she's presented her
work at several conferences

570
00:24:50,640 --> 00:24:52,900
as part of our
Everyday AI project,

571
00:24:52,900 --> 00:24:55,590
and she's presented
her work on AI,

572
00:24:55,590 --> 00:24:58,200
for example, at the National
Association for Research

573
00:24:58,200 --> 00:25:00,360
on Science Teaching, or NARST.

574
00:25:00,360 --> 00:25:04,110
Jesse is also an experienced
classroom teacher,

575
00:25:04,110 --> 00:25:06,960
with nine years of
experience teaching in middle

576
00:25:06,960 --> 00:25:09,490
and high school media arts.

577
00:25:09,490 --> 00:25:13,950
He teaches graphic design,
yearbook, film, esports.

578
00:25:13,950 --> 00:25:19,350
His school, with some
of his assistance,

579
00:25:19,350 --> 00:25:22,920
was awarded the state
Innovation Zone grant in 2023

580
00:25:22,920 --> 00:25:25,950
for exceptional
work, in part because

581
00:25:25,950 --> 00:25:28,180
of the work he's doing in
his film and media classes.

582
00:25:28,180 --> 00:25:31,950
Jesse served as a facilitator as
well in the Everyday AI project,

583
00:25:31,950 --> 00:25:34,410
and I consider him a
master of strategies

584
00:25:34,410 --> 00:25:37,410
in integrating AI education
into digital media education.

585
00:25:37,410 --> 00:25:40,330
And lastly, Kelly Powers--

586
00:25:40,330 --> 00:25:41,820
but certainly not least--

587
00:25:41,820 --> 00:25:44,550
Kelly is a veteran
teacher of 25 years

588
00:25:44,550 --> 00:25:46,920
of experience teaching middle
and high school computer

589
00:25:46,920 --> 00:25:47,620
science.

590
00:25:47,620 --> 00:25:49,320
She is a leader
in the development

591
00:25:49,320 --> 00:25:51,550
of strategic pathways
in computer science.

592
00:25:51,550 --> 00:25:54,330
She was formerly working
at IBM, and she's

593
00:25:54,330 --> 00:25:56,010
left industry to
come to the classroom

594
00:25:56,010 --> 00:25:58,450
and served as a facilitator
in the Everyday AI project.

595
00:25:58,450 --> 00:26:00,870
And fun fact about
Kelly is that she

596
00:26:00,870 --> 00:26:02,760
was one of the
teachers who initially

597
00:26:02,760 --> 00:26:09,000
advised the AI4K12 working group
in 2018, which has become--

598
00:26:09,000 --> 00:26:12,930
will inform the AI4K12
frameworks and learning

599
00:26:12,930 --> 00:26:14,460
standards worldwide.

600
00:26:14,460 --> 00:26:17,380
So with that, I'm going
to ask Eric to play--

601
00:26:17,380 --> 00:26:21,300
it's just a seven-minute video
of these teachers sharing

602
00:26:21,300 --> 00:26:26,490
their insights on, first, how
they are using large language

603
00:26:26,490 --> 00:26:28,570
models just for their
own personal use

604
00:26:28,570 --> 00:26:32,040
and then reflecting on some
of the challenges and benefits

605
00:26:32,040 --> 00:26:35,523
and future of large language
models in the classroom.

606
00:26:35,523 --> 00:26:36,190
Thank you, Eric.

607
00:26:36,190 --> 00:26:37,432
[VIDEO PLAYBACK]

608
00:26:37,432 --> 00:26:44,240

609
00:26:44,240 --> 00:26:47,490
- So I use it in work.

610
00:26:47,490 --> 00:26:52,710
What I've been doing is any
time I have to write an email--

611
00:26:52,710 --> 00:26:54,750
I think I do pretty well
with writing anyway,

612
00:26:54,750 --> 00:26:57,960
but when I'm doing
a parent email,

613
00:26:57,960 --> 00:26:59,760
sometimes I'll send
it out directly.

614
00:26:59,760 --> 00:27:02,330
But other times I'll put
it into Chat and say, edit

615
00:27:02,330 --> 00:27:08,740
this draft to a parent
regarding the student's warning,

616
00:27:08,740 --> 00:27:10,490
just to make sure my
tone is coming across

617
00:27:10,490 --> 00:27:12,687
and my message, intent is there.

618
00:27:12,687 --> 00:27:14,270
And so I'll feed it
through there just

619
00:27:14,270 --> 00:27:15,478
to see if it makes it better.

620
00:27:15,478 --> 00:27:17,070
And sometimes I
keep the original,

621
00:27:17,070 --> 00:27:19,220
but normally at least
I change a few things

622
00:27:19,220 --> 00:27:24,120
because l like having some of
the verbiage the Chat uses.

623
00:27:24,120 --> 00:27:26,070
It's like, OK,
that sounds nicer.

624
00:27:26,070 --> 00:27:27,120
That sounds better.

625
00:27:27,120 --> 00:27:28,350
So I use it for that.

626
00:27:28,350 --> 00:27:32,310
And I did that to create
a template, basically,

627
00:27:32,310 --> 00:27:33,950
and I shared it
with my department

628
00:27:33,950 --> 00:27:36,710
to contact parents about
violations of the cell phone

629
00:27:36,710 --> 00:27:37,640
policy.

630
00:27:37,640 --> 00:27:41,760
- So for my educational
practice and how I'm using LLMs,

631
00:27:41,760 --> 00:27:43,960
I'm using it a lot
for my prep work,

632
00:27:43,960 --> 00:27:48,510
creating assignments, not
necessarily analyzing work just

633
00:27:48,510 --> 00:27:54,510
yet but mostly just getting
assignments to a point

634
00:27:54,510 --> 00:27:57,690
where I can come up with an
idea for a creative assignment,

635
00:27:57,690 --> 00:28:01,000
and I have a master prompt.

636
00:28:01,000 --> 00:28:04,072
- I use it for lesson ideas.

637
00:28:04,072 --> 00:28:05,530
If I have an idea
of what I want it

638
00:28:05,530 --> 00:28:12,400
to look like but I don't really
have it spelled all the way out,

639
00:28:12,400 --> 00:28:14,387
I'll ask Chat for--

640
00:28:14,387 --> 00:28:15,970
just to come up with
something and see

641
00:28:15,970 --> 00:28:17,780
if it's in line with
what I was thinking.

642
00:28:17,780 --> 00:28:21,880
- So professionally, I must say
I haven't really used it a ton

643
00:28:21,880 --> 00:28:23,320
because the lessons--

644
00:28:23,320 --> 00:28:25,220
and this is where
I get confused.

645
00:28:25,220 --> 00:28:28,760
A lot of these tools create
amazing lesson plans,

646
00:28:28,760 --> 00:28:31,960
amazing activities, but if
you think of the public school

647
00:28:31,960 --> 00:28:34,803
teacher, not me but the
public school teacher,

648
00:28:34,803 --> 00:28:36,220
they're usually
given a curriculum

649
00:28:36,220 --> 00:28:37,870
that they have to follow.

650
00:28:37,870 --> 00:28:39,980
So do they really--

651
00:28:39,980 --> 00:28:42,260
if anything, maybe
differentiation activities,

652
00:28:42,260 --> 00:28:44,210
that would be a use case.

653
00:28:44,210 --> 00:28:46,935
But we're not really
writing curriculum so much.

654
00:28:46,935 --> 00:28:49,828

655
00:28:49,828 --> 00:28:51,870
- But I know that students
right now are using it

656
00:28:51,870 --> 00:28:55,570
as a big crutch for just
turning in projects,

657
00:28:55,570 --> 00:28:57,720
and it's not necessarily
learning because that's

658
00:28:57,720 --> 00:29:02,140
the issue is that schools right
now are focused on, do the task,

659
00:29:02,140 --> 00:29:05,960
get the grade, not find--

660
00:29:05,960 --> 00:29:08,460
I'm telling kids, you
can find the answer,

661
00:29:08,460 --> 00:29:10,710
but are you understanding
the solution?

662
00:29:10,710 --> 00:29:14,420
And that's the biggest
part of learning.

663
00:29:14,420 --> 00:29:16,410
And what made it
worse was Google.

664
00:29:16,410 --> 00:29:18,380
You can Google a question
now, and now Google

665
00:29:18,380 --> 00:29:20,540
has an AI response.

666
00:29:20,540 --> 00:29:22,805
But now no one's actually
clicking on those links

667
00:29:22,805 --> 00:29:24,690
of where these are coming from.

668
00:29:24,690 --> 00:29:27,930
- I definitely see the
middle school brain--

669
00:29:27,930 --> 00:29:31,640
I see them using these tools
to get their homework done

670
00:29:31,640 --> 00:29:35,090
and not really thinking like we
would want them to think, think

671
00:29:35,090 --> 00:29:38,270
thoughtfully and thoroughly and
spend some time with the model

672
00:29:38,270 --> 00:29:40,430
to engage in back-and-forth.

673
00:29:40,430 --> 00:29:42,860
- Because I don't
think everyone has

674
00:29:42,860 --> 00:29:47,810
the discretion needed to go
back and evaluate the product.

675
00:29:47,810 --> 00:29:50,640
You have to be able to
say, OK, this was my idea.

676
00:29:50,640 --> 00:29:51,830
This is what prompted.

677
00:29:51,830 --> 00:29:54,710
But is it actually doing
what it was asked to do?

678
00:29:54,710 --> 00:29:55,950
And is it my work?

679
00:29:55,950 --> 00:29:57,750
Is it speaking the way
I want it to speak,

680
00:29:57,750 --> 00:30:00,350
or is it just saying
something random?

681
00:30:00,350 --> 00:30:03,870
I think that part
takes time as well.

682
00:30:03,870 --> 00:30:07,880
You have to evaluate
what's there.

683
00:30:07,880 --> 00:30:09,020
Very rarely.

684
00:30:09,020 --> 00:30:13,230
I haven't had-- we haven't
done that because even

685
00:30:13,230 --> 00:30:17,910
at the school that I'm at
it's still kind of in between.

686
00:30:17,910 --> 00:30:19,800
And I don't want to--

687
00:30:19,800 --> 00:30:23,100
I don't want to step on other
teachers toes who are reaching

688
00:30:23,100 --> 00:30:24,870
against it because we have--

689
00:30:24,870 --> 00:30:26,430
I really, really--

690
00:30:26,430 --> 00:30:29,740
I am super, super on board
with classical learning,

691
00:30:29,740 --> 00:30:32,420
especially with reading,
writing, especially writing.

692
00:30:32,420 --> 00:30:34,170
Learning how to write
and synthesizing it,

693
00:30:34,170 --> 00:30:36,030
that's how your brain functions.

694
00:30:36,030 --> 00:30:40,780
- We're still limited by
lots of tools being blocked,

695
00:30:40,780 --> 00:30:44,075
so they're not in
schools lacking policy.

696
00:30:44,075 --> 00:30:47,760
How are we going to integrate
AI into the curriculum

697
00:30:47,760 --> 00:30:50,920
so it's thoughtful and it's
just right for the kids--

698
00:30:50,920 --> 00:30:52,350
it's age-appropriate?

699
00:30:52,350 --> 00:30:55,530
And a lot of schools are
still struggling with what

700
00:30:55,530 --> 00:30:56,970
that should look like.

701
00:30:56,970 --> 00:31:01,050
- So I think it's kind of crazy
that some teachers are going

702
00:31:01,050 --> 00:31:02,610
to be able to
develop their skill

703
00:31:02,610 --> 00:31:07,110
set because of the district they
work in and benefit the kids

704
00:31:07,110 --> 00:31:10,240
and make their workload easier
and all this kind of stuff.

705
00:31:10,240 --> 00:31:14,174
And then you're going
to have, once again--

706
00:31:14,174 --> 00:31:16,482
not a technology
divide, but it's

707
00:31:16,482 --> 00:31:18,190
like some people are
getting left behind,

708
00:31:18,190 --> 00:31:20,270
and they have to do
with the regular way.

709
00:31:20,270 --> 00:31:22,347
And I just don't think--

710
00:31:22,347 --> 00:31:23,930
I'm happy that the
district I'm in now

711
00:31:23,930 --> 00:31:27,380
is not on the downside of that.

712
00:31:27,380 --> 00:31:29,930
But I do think about
what that means

713
00:31:29,930 --> 00:31:32,450
for other teachers in
other school systems

714
00:31:32,450 --> 00:31:35,690
who are not being taught
anything about AI, who are not

715
00:31:35,690 --> 00:31:38,990
allowed to use it at all
for their personal work

716
00:31:38,990 --> 00:31:42,830
or with students, and what
that will mean for them.

717
00:31:42,830 --> 00:31:47,010
The whole point is for
everyone to get better.

718
00:31:47,010 --> 00:31:49,140
- So that's where
my jam is, really,

719
00:31:49,140 --> 00:31:51,000
is like, how can
I expose students

720
00:31:51,000 --> 00:31:56,940
to how AI works, how
AI presents some harms

721
00:31:56,940 --> 00:32:01,860
and benefits to society, and
as we prepare our students who

722
00:32:01,860 --> 00:32:06,202
are in my classroom to become
creators of anything, how might

723
00:32:06,202 --> 00:32:08,160
what are some of the
questions they're thinking

724
00:32:08,160 --> 00:32:09,660
about when designing tech?

725
00:32:09,660 --> 00:32:11,520
What are the harms
their tech might

726
00:32:11,520 --> 00:32:14,350
have on society or
people in our society,

727
00:32:14,350 --> 00:32:15,820
and what are the benefits?

728
00:32:15,820 --> 00:32:19,170
So they are getting a real great
sense of how these technologies

729
00:32:19,170 --> 00:32:23,070
work and doing a lot of--

730
00:32:23,070 --> 00:32:25,050
getting feedback
from their models

731
00:32:25,050 --> 00:32:26,940
and then making
adjustments, going

732
00:32:26,940 --> 00:32:29,470
through the problem-solving
and design cycle.

733
00:32:29,470 --> 00:32:34,540
So we are doing a lot of AI,
just not prompt engineering

734
00:32:34,540 --> 00:32:35,915
so much with my students.

735
00:32:35,915 --> 00:32:40,150

736
00:32:40,150 --> 00:32:42,130
- AI at this point
is going to make sure

737
00:32:42,130 --> 00:32:44,980
that everyone has their
own production company,

738
00:32:44,980 --> 00:32:48,320
everyone is able to
make all these things.

739
00:32:48,320 --> 00:32:51,070
I'm watching these interesting
videos of-- there's one

740
00:32:51,070 --> 00:32:53,382
good writer, and
you want to say LLMs

741
00:32:53,382 --> 00:32:54,590
are going to replace writers.

742
00:32:54,590 --> 00:32:56,320
And I'm like, no,
LLMs are just going

743
00:32:56,320 --> 00:32:58,660
to be a catapult for
writers because because--

744
00:32:58,660 --> 00:33:03,970
- But some of the tools are
having AI buddies inside

745
00:33:03,970 --> 00:33:08,050
of them, and in
that case, then you

746
00:33:08,050 --> 00:33:10,000
will be creating certain
prompts to interact

747
00:33:10,000 --> 00:33:12,910
with the tool embedded in
the curriculum resource

748
00:33:12,910 --> 00:33:14,050
that you're using.

749
00:33:14,050 --> 00:33:18,070
I can see that blowing up
beautifully in a great way.

750
00:33:18,070 --> 00:33:21,540
- I feel like a lot of--

751
00:33:21,540 --> 00:33:22,040
not a lot.

752
00:33:22,040 --> 00:33:24,980
A lot of people want to be
on the PD train, I would say,

753
00:33:24,980 --> 00:33:27,970
when it comes to AI,
but everybody is not

754
00:33:27,970 --> 00:33:35,290
teaching teachers how to help
students use AI or teaching

755
00:33:35,290 --> 00:33:39,720
teachers how to integrate
AI into their curriculum,

756
00:33:39,720 --> 00:33:42,630
even more importantly.

757
00:33:42,630 --> 00:33:45,240
That's not what they're doing.

758
00:33:45,240 --> 00:33:49,140
It's more about, this is
a tool for teachers only.

759
00:33:49,140 --> 00:33:50,330
That's kind of what you get.

760
00:33:50,330 --> 00:33:57,412
This is a tool for teachers
to write learning objectives.

761
00:33:57,412 --> 00:33:58,890
You know what I mean?

762
00:33:58,890 --> 00:34:01,980
It's not necessarily
helping me to deliver

763
00:34:01,980 --> 00:34:05,440
the content any better or
to teach children content.

764
00:34:05,440 --> 00:34:11,011
So I feel like that part
is not being addressed.

765
00:34:11,011 --> 00:34:12,260
[END PLAYBACK]

766
00:34:12,260 --> 00:34:14,929
So I'll close by saying
these teachers highlighted

767
00:34:14,929 --> 00:34:16,920
some areas of concern.

768
00:34:16,920 --> 00:34:19,429
One is that they're concerned
about a digital divide

769
00:34:19,429 --> 00:34:21,650
between teachers who
have training on AI

770
00:34:21,650 --> 00:34:25,010
and don't, and they're not
yet seeing the connection

771
00:34:25,010 --> 00:34:28,280
between the LLM tools of
today being useful immediately

772
00:34:28,280 --> 00:34:29,520
for them in their classroom.

773
00:34:29,520 --> 00:34:31,003
Happy to take questions.

774
00:34:31,003 --> 00:34:32,670
I'm sorry I went a
little bit over time.

775
00:34:32,670 --> 00:34:35,173
I'll pass the
microphone back to Bill.

776
00:34:35,173 --> 00:34:37,340
WILLIAM BONVILLIAN: Kate
and Eric, thanks very much.

777
00:34:37,340 --> 00:34:40,639
This is really a
real helpful session.

778
00:34:40,639 --> 00:34:42,750
We will have a few
minutes for questions.

779
00:34:42,750 --> 00:34:45,260
Let me just tell the group, if
you could post your questions

780
00:34:45,260 --> 00:34:49,429
in chat, then I'll
try and field them

781
00:34:49,429 --> 00:34:51,800
so we stay within
our time limit.

782
00:34:51,800 --> 00:34:54,110
Let me start with a
question I've got,

783
00:34:54,110 --> 00:34:57,170
and I'll ask it
really of each of you.

784
00:34:57,170 --> 00:35:01,580
Eric, what are-- you've
given us a lot of insights

785
00:35:01,580 --> 00:35:02,550
in your presentation.

786
00:35:02,550 --> 00:35:05,810
What are the best
practices that schools

787
00:35:05,810 --> 00:35:08,960
ought to be thinking
about for AI

788
00:35:08,960 --> 00:35:10,620
and its application
to education?

789
00:35:10,620 --> 00:35:13,950
And then, Kate, let me ask
you, with all your dealings

790
00:35:13,950 --> 00:35:16,500
with teachers, what do you
think the best practices are

791
00:35:16,500 --> 00:35:19,510
that teachers ought to
start thinking about?

792
00:35:19,510 --> 00:35:21,040
ERIC KLOPFER:
Yeah, it comes back

793
00:35:21,040 --> 00:35:23,620
to that comment I made earlier
about creating communities

794
00:35:23,620 --> 00:35:24,470
of practice.

795
00:35:24,470 --> 00:35:27,280
I think the best thing
to do is to have people

796
00:35:27,280 --> 00:35:32,260
who are willing to experiment
and share with other teachers

797
00:35:32,260 --> 00:35:34,760
to figure out what's
working, what's not working,

798
00:35:34,760 --> 00:35:36,520
what's working for whom.

799
00:35:36,520 --> 00:35:41,380
And it would be a waste of
time to think about everybody

800
00:35:41,380 --> 00:35:44,120
trying everything and not
sharing it with each other.

801
00:35:44,120 --> 00:35:45,833
And I also think
that there's going

802
00:35:45,833 --> 00:35:47,750
to be people at different
levels of readiness,

803
00:35:47,750 --> 00:35:50,410
and so the people who are
ready to test things now

804
00:35:50,410 --> 00:35:52,840
can learn and share that
with the next wave of people

805
00:35:52,840 --> 00:35:55,507
who are maybe a little bit more
reticent to use the technologies

806
00:35:55,507 --> 00:35:57,040
in their classrooms.

807
00:35:57,040 --> 00:36:02,510
I do think that one piece of
advice is to not ban things.

808
00:36:02,510 --> 00:36:04,760
There's a sense that, well,
if we just ban everything,

809
00:36:04,760 --> 00:36:06,480
we just put up a big
gate at the door,

810
00:36:06,480 --> 00:36:07,730
we will not have any problems.

811
00:36:07,730 --> 00:36:09,230
We won't have to
think about this.

812
00:36:09,230 --> 00:36:11,630
We can just ignore this
problem, and we'll just say,

813
00:36:11,630 --> 00:36:13,450
you can't use AI.

814
00:36:13,450 --> 00:36:16,570
I think really that's the
"sticking your head in the sand"

815
00:36:16,570 --> 00:36:20,570
solution, and I don't think
that will be effective.

816
00:36:20,570 --> 00:36:22,260
What that does is
create disparities.

817
00:36:22,260 --> 00:36:25,370
It means that people who
have more access to tools

818
00:36:25,370 --> 00:36:28,230
outside of school will use
those and benefit from those,

819
00:36:28,230 --> 00:36:30,110
and the people who don't
have access to those

820
00:36:30,110 --> 00:36:34,130
will be hurt by not
having access to those.

821
00:36:34,130 --> 00:36:37,310
It's really important to have
people learning how to use them

822
00:36:37,310 --> 00:36:38,960
effectively and
ethically, and I think

823
00:36:38,960 --> 00:36:41,390
that that's only going to
happen through practice.

824
00:36:41,390 --> 00:36:43,453
But I think teachers
need to have policies

825
00:36:43,453 --> 00:36:45,870
that they feel like that they're
comfortable with as well,

826
00:36:45,870 --> 00:36:48,410
and they need to be a part of
that process, which says, hey,

827
00:36:48,410 --> 00:36:51,890
if we say you can't use the
AI on certain assignments,

828
00:36:51,890 --> 00:36:53,727
we need to assume
that that's the case.

829
00:36:53,727 --> 00:36:56,060
And if we find out that you
are-- and I don't think that

830
00:36:56,060 --> 00:36:57,930
should be done through
AI detection tools.

831
00:36:57,930 --> 00:36:59,707
I think through
conversation with students

832
00:36:59,707 --> 00:37:01,790
that you find out whether
that's happened or not--

833
00:37:01,790 --> 00:37:04,635
then there should be
consequences to those things.

834
00:37:04,635 --> 00:37:06,010
WILLIAM BONVILLIAN:
Thanks, Eric.

835
00:37:06,010 --> 00:37:08,010
Kate, how about teachers
and best practices?

836
00:37:08,010 --> 00:37:09,760
KATE MOORE: I just
agree with everything--

837
00:37:09,760 --> 00:37:12,120
I agree with everything
that Eric just said,

838
00:37:12,120 --> 00:37:13,870
and I think it's all
relevant to teachers,

839
00:37:13,870 --> 00:37:16,450
particularly staying
connected in a community.

840
00:37:16,450 --> 00:37:20,640
We found that teachers who are
connected do the work longer

841
00:37:20,640 --> 00:37:23,880
and feel more
refreshed and involved.

842
00:37:23,880 --> 00:37:25,620
Staying isolated,
not connected, just

843
00:37:25,620 --> 00:37:29,340
trying to bulldoze through it
on your own leads to burnout.

844
00:37:29,340 --> 00:37:31,650
I also really
appreciate Eric's point

845
00:37:31,650 --> 00:37:37,050
about the disparity, the risk
of disparity if we just ban.

846
00:37:37,050 --> 00:37:43,830
And the one thing I'll add is
that the research is still not

847
00:37:43,830 --> 00:37:45,390
out yet on this, so
I'm just speaking

848
00:37:45,390 --> 00:37:49,500
from my own personal work
interviewing teachers.

849
00:37:49,500 --> 00:37:53,580
But it seems that
giving students AI tools

850
00:37:53,580 --> 00:37:56,970
and treating that as the
educative experience leads

851
00:37:56,970 --> 00:37:59,940
to confusion, misunderstanding.

852
00:37:59,940 --> 00:38:03,210
Perhaps start by talking
about how AI works first

853
00:38:03,210 --> 00:38:05,650
with your students, giving
them a sense of what's

854
00:38:05,650 --> 00:38:08,230
happening under the hood,
using some of the curricula

855
00:38:08,230 --> 00:38:11,030
that Eric shared, that I've
shared that's already out there.

856
00:38:11,030 --> 00:38:14,565
And then give them the tools.

857
00:38:14,565 --> 00:38:15,940
WILLIAM BONVILLIAN:
Great points.

858
00:38:15,940 --> 00:38:18,200
And again, if folks
have got questions,

859
00:38:18,200 --> 00:38:22,780
just please post them
in the chat feature.

860
00:38:22,780 --> 00:38:24,710
There's been a lot
of talk, obviously--

861
00:38:24,710 --> 00:38:27,310
and you all have both
raised it at some length--

862
00:38:27,310 --> 00:38:32,620
about the application of
AI-based tutors in education.

863
00:38:32,620 --> 00:38:35,330
Where are we in that process?

864
00:38:35,330 --> 00:38:36,907
How are these
tutors coming along?

865
00:38:36,907 --> 00:38:38,990
What are some of the
problems we've got with them?

866
00:38:38,990 --> 00:38:41,470
And what do you think
their potential role is?

867
00:38:41,470 --> 00:38:44,440
And let me turn to you,
Kate, and then to Eric.

868
00:38:44,440 --> 00:38:46,690
KATE MOORE: Well, Eric's
certainly the expert on this,

869
00:38:46,690 --> 00:38:49,810
but I did make a few--

870
00:38:49,810 --> 00:38:53,010
I do have a few comments on
this, and I'll pass to Eric.

871
00:38:53,010 --> 00:38:57,210
One is, as I said before,
teachers are having a hard time

872
00:38:57,210 --> 00:38:58,770
connecting the pipes.

873
00:38:58,770 --> 00:39:05,520
They're using tools like ChatGPT
but also Khan Academy's Khanmigo

874
00:39:05,520 --> 00:39:07,540
to create materials
for their students

875
00:39:07,540 --> 00:39:09,960
if they have a membership,
and the materials just

876
00:39:09,960 --> 00:39:12,280
aren't quite landing.

877
00:39:12,280 --> 00:39:13,900
They're great at
differentiating.

878
00:39:13,900 --> 00:39:17,320
There's opportunities to
really make materials quickly,

879
00:39:17,320 --> 00:39:23,340
but the opportunity to
put in meaningful images,

880
00:39:23,340 --> 00:39:27,360
relevant content, and have
tutors that are actually

881
00:39:27,360 --> 00:39:30,420
responding to where the students
are in their needs as learners

882
00:39:30,420 --> 00:39:32,070
isn't quite there.

883
00:39:32,070 --> 00:39:34,060
And there's a few concerns--

884
00:39:34,060 --> 00:39:35,880
I don't think they made
it into the video--

885
00:39:35,880 --> 00:39:38,310
that students might
feel if they're just

886
00:39:38,310 --> 00:39:41,370
working with a tutor
or just working

887
00:39:41,370 --> 00:39:45,550
with AI-produced materials
that they might feel isolated

888
00:39:45,550 --> 00:39:48,400
or like they're not connecting
with-- the social learning

889
00:39:48,400 --> 00:39:49,863
that's so critical in education.

890
00:39:49,863 --> 00:39:51,280
And I know you
were probably going

891
00:39:51,280 --> 00:39:53,210
to say the same thing, Eric, so
I'll pass the microphone to you.

892
00:39:53,210 --> 00:39:55,640
ERIC KLOPFER: Yeah, I'll
definitely agree with that.

893
00:39:55,640 --> 00:39:57,940
I think that's the
key is that tutors

894
00:39:57,940 --> 00:40:02,380
need to be part of
an ecosystem of tools

895
00:40:02,380 --> 00:40:04,600
and used at the
appropriate times

896
00:40:04,600 --> 00:40:07,360
for the appropriate activities.

897
00:40:07,360 --> 00:40:09,170
So much of learning is social.

898
00:40:09,170 --> 00:40:11,060
It's about connecting
with other people.

899
00:40:11,060 --> 00:40:13,760
It's about developing
my identity.

900
00:40:13,760 --> 00:40:16,570
It's about having relationships
with a teacher who

901
00:40:16,570 --> 00:40:18,080
I feel like cares about me.

902
00:40:18,080 --> 00:40:21,820
A lot of this stuff from tutors
comes from the 2 sigma result

903
00:40:21,820 --> 00:40:24,220
of many years ago where
if you give a kid a tutor,

904
00:40:24,220 --> 00:40:25,780
you can make a 2
standard deviation

905
00:40:25,780 --> 00:40:27,980
difference in their learning.

906
00:40:27,980 --> 00:40:30,380
And I think there's evidence
that some of that is true,

907
00:40:30,380 --> 00:40:32,590
but that's not all
about the tutor can just

908
00:40:32,590 --> 00:40:33,680
deliver them facts.

909
00:40:33,680 --> 00:40:35,830
It's about a real
tutor, a live tutor

910
00:40:35,830 --> 00:40:39,290
who can understand the student,
think about their relationship,

911
00:40:39,290 --> 00:40:41,990
think about who how they
develop as a person.

912
00:40:41,990 --> 00:40:44,770
And that's what a tutor
does, and AI tutors

913
00:40:44,770 --> 00:40:47,630
don't do that even if we try
to pretend that they do that.

914
00:40:47,630 --> 00:40:50,440
But I do think that they
can play an important role.

915
00:40:50,440 --> 00:40:52,840
You just have to think about
how the other people are

916
00:40:52,840 --> 00:40:55,580
using that tool as part of
that ecosystem of things.

917
00:40:55,580 --> 00:40:57,700
So as I said, in
our examples, we

918
00:40:57,700 --> 00:41:00,940
use it as a member of a group,
so as a way to connect people

919
00:41:00,940 --> 00:41:01,755
within a group.

920
00:41:01,755 --> 00:41:03,130
We have a project
going on in MIT

921
00:41:03,130 --> 00:41:06,908
where it's helping students
learn software development.

922
00:41:06,908 --> 00:41:08,950
But it's integrated into
the software development

923
00:41:08,950 --> 00:41:11,230
environment, and they
can ask questions.

924
00:41:11,230 --> 00:41:13,910
It's things like
professional programmers use,

925
00:41:13,910 --> 00:41:16,490
except it's so much more
Socratic method of doing that.

926
00:41:16,490 --> 00:41:19,780
So I think there's really much
more better contextual ways

927
00:41:19,780 --> 00:41:21,002
of doing things like this.

928
00:41:21,002 --> 00:41:23,210
There's better social ways
of doing things like this.

929
00:41:23,210 --> 00:41:25,480
We're using it in
games, for example,

930
00:41:25,480 --> 00:41:28,790
where it's supporting some of
the gameplay that they're doing.

931
00:41:28,790 --> 00:41:31,480
So I think it's about thinking
about where to apply it

932
00:41:31,480 --> 00:41:33,580
effectively, but I think
the idea is this is

933
00:41:33,580 --> 00:41:34,870
a tool that's just going to--

934
00:41:34,870 --> 00:41:36,412
if we give this to
everybody, they'll

935
00:41:36,412 --> 00:41:38,380
make 2 standard
deviations of differences.

936
00:41:38,380 --> 00:41:40,485
It's a little bit shortsighted.

937
00:41:40,485 --> 00:41:41,860
WILLIAM BONVILLIAN:
Great, thanks

938
00:41:41,860 --> 00:41:44,030
for those thoughtful responses.

939
00:41:44,030 --> 00:41:48,340
Eric, let me ask you a question
and then Kate chime in as well.

940
00:41:48,340 --> 00:41:50,470
MIT RAISE has been--

941
00:41:50,470 --> 00:41:55,190
the effort you mentioned at MIT
to confront AI in education,

942
00:41:55,190 --> 00:41:57,610
MIT RAISE has been
collaborating with Google

943
00:41:57,610 --> 00:42:00,020
for teacher professional
development,

944
00:42:00,020 --> 00:42:02,600
and where does that stand?

945
00:42:02,600 --> 00:42:04,850
What's going to be
the results of that?

946
00:42:04,850 --> 00:42:07,193
What can we hope to see
from that collaboration?

947
00:42:07,193 --> 00:42:09,610
ERIC KLOPFER: Yeah, so one of
the courses I mentioned is--

948
00:42:09,610 --> 00:42:11,193
there's a two-hour
course that we have

949
00:42:11,193 --> 00:42:15,670
that people can take that's
an intro to AI for teachers.

950
00:42:15,670 --> 00:42:18,580
The rationale behind
that one was we

951
00:42:18,580 --> 00:42:21,125
want people to
just understand AI.

952
00:42:21,125 --> 00:42:22,750
Don't immediately
understand how you're

953
00:42:22,750 --> 00:42:24,370
going to apply this to
students and apply this

954
00:42:24,370 --> 00:42:25,720
in your professional practice.

955
00:42:25,720 --> 00:42:27,970
Just understand what
it is and how it's used

956
00:42:27,970 --> 00:42:30,490
and how it can help you.

957
00:42:30,490 --> 00:42:32,807
We think that's a really
important step because I think

958
00:42:32,807 --> 00:42:34,390
teachers often jump
too quickly about,

959
00:42:34,390 --> 00:42:36,310
how am I going to use
this with my students?

960
00:42:36,310 --> 00:42:39,620
And so that's our first step
is that we've done that.

961
00:42:39,620 --> 00:42:41,570
I think now we're
thinking about expanding

962
00:42:41,570 --> 00:42:43,370
that kind of
professional development

963
00:42:43,370 --> 00:42:46,790
to provide other kinds
of tools, to provide

964
00:42:46,790 --> 00:42:49,530
more in-depth learning
experiences for students.

965
00:42:49,530 --> 00:42:52,910
Google themselves has developed
a special large language

966
00:42:52,910 --> 00:42:54,930
model for education--

967
00:42:54,930 --> 00:42:56,780
I think it's called LearnLM--

968
00:42:56,780 --> 00:42:59,068
that they announced
recently, and I

969
00:42:59,068 --> 00:43:01,610
think we'll start to see some
interesting results from things

970
00:43:01,610 --> 00:43:04,370
like that where people can apply
that in different contexts.

971
00:43:04,370 --> 00:43:07,910
I think it's going to be
really important to then do

972
00:43:07,910 --> 00:43:10,550
some research on that and
understand where it's succeeding

973
00:43:10,550 --> 00:43:13,430
and where it's not succeeding.

974
00:43:13,430 --> 00:43:16,530
WILLIAM BONVILLIAN: Let me just
ask one closing point here.

975
00:43:16,530 --> 00:43:20,360
Could you give us maybe a couple
of sentences from each of you

976
00:43:20,360 --> 00:43:23,480
on what you think the most
important kind of applications

977
00:43:23,480 --> 00:43:28,040
of AI are going to be
in education in this K

978
00:43:28,040 --> 00:43:29,430
through 12 kind of realm?

979
00:43:29,430 --> 00:43:32,630
What do you think the
really significant things

980
00:43:32,630 --> 00:43:35,240
are going to-- contributions
that AI could make?

981
00:43:35,240 --> 00:43:36,930
What might they be?

982
00:43:36,930 --> 00:43:38,310
Kate, can I start with you?

983
00:43:38,310 --> 00:43:40,268
KATE MOORE: Yeah, yeah,
and I'm excited to hear

984
00:43:40,268 --> 00:43:42,530
what Eric's going to say.

985
00:43:42,530 --> 00:43:45,800
So first, I'll just
resonate with what

986
00:43:45,800 --> 00:43:47,900
Kelly Powers said in
the video, that she

987
00:43:47,900 --> 00:43:49,440
sees this opportunity with--

988
00:43:49,440 --> 00:43:52,550
she calls them "buddies," but
it's actually kind of like what

989
00:43:52,550 --> 00:43:55,490
Eric was saying, too, where you
have an AI agent who's inserted

990
00:43:55,490 --> 00:43:58,700
into an ecosystem that's
designed for a collaborative

991
00:43:58,700 --> 00:44:02,000
learning experience or it's
part of this model that

992
00:44:02,000 --> 00:44:06,560
you're exploring and you have
a buddy or an agent jump in.

993
00:44:06,560 --> 00:44:09,590
I do think there are
opportunities for personalized

994
00:44:09,590 --> 00:44:12,710
learning, but I'm hesitant to
say we're anywhere close to that

995
00:44:12,710 --> 00:44:15,600
now because we have to think
very seriously about what it

996
00:44:15,600 --> 00:44:17,910
means to personalize
what kind of data

997
00:44:17,910 --> 00:44:19,930
security risks are
involved in that

998
00:44:19,930 --> 00:44:23,580
and what does it mean to have
a personal tutor know you

999
00:44:23,580 --> 00:44:27,250
very well, for example,
like surveillance issues.

1000
00:44:27,250 --> 00:44:30,360
So I do have
hesitancy about that.

1001
00:44:30,360 --> 00:44:32,320
But I do think those
are two opportunities.

1002
00:44:32,320 --> 00:44:33,940
I hope I didn't take
too much time from Eric

1003
00:44:33,940 --> 00:44:35,760
because I'm very excited
what you're going to say.

1004
00:44:35,760 --> 00:44:37,385
ERIC KLOPFER: No, so
I'm going to agree

1005
00:44:37,385 --> 00:44:40,350
that collaborative learning
is a really important place

1006
00:44:40,350 --> 00:44:41,940
that we'll see AI.

1007
00:44:41,940 --> 00:44:45,060
I think ways that can help
teachers support teachers

1008
00:44:45,060 --> 00:44:49,350
in making sense of student data,
those kinds of things as well.

1009
00:44:49,350 --> 00:44:52,260
But I also think that we'll
see it integrated more

1010
00:44:52,260 --> 00:44:56,310
into other tools, so rather
than seeing the standalone tutor

1011
00:44:56,310 --> 00:44:58,600
where I come in and
I ask questions,

1012
00:44:58,600 --> 00:45:01,140
I think the Python
Tutor that I mentioned

1013
00:45:01,140 --> 00:45:02,500
is a good example of this.

1014
00:45:02,500 --> 00:45:04,420
We'll see it integrated
into other tools.

1015
00:45:04,420 --> 00:45:06,240
So things that people
are already using

1016
00:45:06,240 --> 00:45:08,090
as part of their practice--

1017
00:45:08,090 --> 00:45:09,138
it could be a game.

1018
00:45:09,138 --> 00:45:10,180
It could be a simulation.

1019
00:45:10,180 --> 00:45:11,860
It could be a development tool.

1020
00:45:11,860 --> 00:45:14,220
It could be a creative tool.

1021
00:45:14,220 --> 00:45:16,890
Things like that where you have
AI integrated into those to help

1022
00:45:16,890 --> 00:45:18,960
provide students with
feedback, to help provide them

1023
00:45:18,960 --> 00:45:20,730
with guidance, those
are the places where

1024
00:45:20,730 --> 00:45:23,600
I think we'll see a lot
of interesting results.

1025
00:45:23,600 --> 00:45:25,350
WILLIAM BONVILLIAN:
Thank you both so much

1026
00:45:25,350 --> 00:45:27,160
for your really
terrific insights.

1027
00:45:27,160 --> 00:45:31,080
Obviously, the role for
AI for teachers in schools

1028
00:45:31,080 --> 00:45:34,860
is a crucial question for
everybody participating

1029
00:45:34,860 --> 00:45:36,660
in this event, and
you've really given us

1030
00:45:36,660 --> 00:45:40,500
some great thoughts about
how that might work for us.

1031
00:45:40,500 --> 00:45:46,000

