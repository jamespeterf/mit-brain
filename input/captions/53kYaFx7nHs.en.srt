1
00:00:01,423 --> 00:00:05,370
(swooshing tune)
(electric tune)

2
00:00:05,370 --> 00:00:06,990
- Hi, I'm Phillip Isola

3
00:00:06,990 --> 00:00:10,530
and I'm a faculty member at MIT in EECS.

4
00:00:10,530 --> 00:00:12,180
And I study computer vision

5
00:00:12,180 --> 00:00:14,340
and machine learning in robotics,

6
00:00:14,340 --> 00:00:15,810
and I'm happy to answer some questions

7
00:00:15,810 --> 00:00:17,430
about those topics today.

8
00:00:17,430 --> 00:00:19,950
So let's go to the first question.

9
00:00:19,950 --> 00:00:23,100
Okay. What is embodied intelligence?

10
00:00:23,100 --> 00:00:26,580
Embodied intelligence is
the study of AI systems

11
00:00:26,580 --> 00:00:28,980
that have to interact in a physical world,

12
00:00:28,980 --> 00:00:30,300
in the real world.

13
00:00:30,300 --> 00:00:31,680
So think robotics.

14
00:00:31,680 --> 00:00:35,010
But the study of embodied
intelligence is really,

15
00:00:35,010 --> 00:00:36,540
encompasses a lot of different fields.

16
00:00:36,540 --> 00:00:39,630
So we need to be able to
perceive the 3D world around us.

17
00:00:39,630 --> 00:00:40,650
That's a vision problem.

18
00:00:40,650 --> 00:00:44,670
We need to be able to take
actions and make decisions.

19
00:00:44,670 --> 00:00:48,750
So these are problems in
decision making and robotics.

20
00:00:48,750 --> 00:00:51,900
So embodied intelligence is
a very kind of holistic take

21
00:00:51,900 --> 00:00:53,370
on the AI problem.

22
00:00:53,370 --> 00:00:55,500
Let's go to the next question.

23
00:00:55,500 --> 00:00:57,210
Can you tell me about your contributions

24
00:00:57,210 --> 00:00:58,950
to the field of computer vision?

25
00:00:58,950 --> 00:01:00,570
Specifically, could you detail

26
00:01:00,570 --> 00:01:02,580
some of your key work in
applying machine learning

27
00:01:02,580 --> 00:01:04,470
to image processing?

28
00:01:04,470 --> 00:01:06,960
My work has been on a
few different topics,

29
00:01:06,960 --> 00:01:08,970
but one of the central themes has been

30
00:01:08,970 --> 00:01:12,090
on trying to understand
how should we represent

31
00:01:12,090 --> 00:01:13,470
the world around us,

32
00:01:13,470 --> 00:01:16,080
specifically how should we
represent the visual world

33
00:01:16,080 --> 00:01:17,160
around us.

34
00:01:17,160 --> 00:01:19,200
There's a lot of different
answers to that question.

35
00:01:19,200 --> 00:01:20,910
And you know, one answer might be

36
00:01:20,910 --> 00:01:23,460
you could have 3D models of objects,

37
00:01:23,460 --> 00:01:25,500
and you could have the
geometry represented

38
00:01:25,500 --> 00:01:28,140
and you could have like
the semantics represented.

39
00:01:28,140 --> 00:01:30,630
For example, I would know
there's a cat in the photo

40
00:01:30,630 --> 00:01:32,040
or a dog in the photo.

41
00:01:32,040 --> 00:01:34,710
My work has been on a
slightly different direction,

42
00:01:34,710 --> 00:01:37,890
which is, let's not kind of pre-specify

43
00:01:37,890 --> 00:01:40,980
or hand engineer the way of
representing the visual world,

44
00:01:40,980 --> 00:01:43,800
but let's let it emerge
from a learning algorithm.

45
00:01:43,800 --> 00:01:46,770
And I've specifically worked
on using deep neural networks

46
00:01:46,770 --> 00:01:49,980
to learn visual representations from data,

47
00:01:49,980 --> 00:01:51,930
where the representation that's learned

48
00:01:51,930 --> 00:01:55,230
is what's called a neural embedding.

49
00:01:55,230 --> 00:01:58,080
It's what the neural net has come up with

50
00:01:58,080 --> 00:02:01,710
as a way of organizing
the data that it sees.

51
00:02:01,710 --> 00:02:03,900
And in particular, I've worked a lot,

52
00:02:03,900 --> 00:02:06,750
my group has worked a lot on
an approach which is called

53
00:02:06,750 --> 00:02:09,240
self supervised learning.

54
00:02:09,240 --> 00:02:12,120
So in this approach, you
don't teach the neural network

55
00:02:12,120 --> 00:02:15,150
from kind of human specified labels,

56
00:02:15,150 --> 00:02:17,970
but instead you train the network

57
00:02:17,970 --> 00:02:21,660
to try to predict properties
of the raw data itself.

58
00:02:21,660 --> 00:02:23,880
So for example, you could take a image

59
00:02:23,880 --> 00:02:27,030
and you could mask out half
of the pixels in the image

60
00:02:27,030 --> 00:02:29,550
and try to predict the missing pixels

61
00:02:29,550 --> 00:02:31,770
from the pixels that are observed.

62
00:02:31,770 --> 00:02:34,080
So this is something that you can train

63
00:02:34,080 --> 00:02:35,190
on just raw data

64
00:02:35,190 --> 00:02:37,710
without having any labels
or humans specifying

65
00:02:37,710 --> 00:02:40,140
what you know, the
representation should be.

66
00:02:40,140 --> 00:02:43,260
But the neural net learns how
to make these predictions.

67
00:02:43,260 --> 00:02:45,660
And by doing so, it comes
up with a representation

68
00:02:45,660 --> 00:02:48,990
of the world, which knows
something about objects

69
00:02:48,990 --> 00:02:50,040
and continuity.

70
00:02:50,040 --> 00:02:52,050
Like it knows that if I have, you know,

71
00:02:52,050 --> 00:02:53,280
the left half of the table here,

72
00:02:53,280 --> 00:02:54,720
there's probably a right half of the table

73
00:02:54,720 --> 00:02:55,650
on the other side.

74
00:02:55,650 --> 00:02:58,170
This idea that just by
predicting missing data,

75
00:02:58,170 --> 00:02:59,820
missing visual information,

76
00:02:59,820 --> 00:03:01,800
you will get a representation

77
00:03:01,800 --> 00:03:03,690
which knows how the world works.

78
00:03:03,690 --> 00:03:05,370
And that's a representation
that can be used

79
00:03:05,370 --> 00:03:07,890
to make meaningful
predictions about the future.

80
00:03:07,890 --> 00:03:10,470
For example, what will
happen if I open this door?

81
00:03:10,470 --> 00:03:11,303
What will I see?

82
00:03:11,303 --> 00:03:13,890
Will I, what will happen
if I drive my car forward?

83
00:03:13,890 --> 00:03:16,170
Will I go through a red light?

84
00:03:16,170 --> 00:03:18,735
If I can make these predictions,
I can make good decisions.

85
00:03:18,735 --> 00:03:20,640
And that's the type of
visual representation

86
00:03:20,640 --> 00:03:21,640
that I've worked on.

87
00:03:23,100 --> 00:03:25,410
Any advice on how to tell the difference

88
00:03:25,410 --> 00:03:29,430
between AI generated
images and natural images?

89
00:03:29,430 --> 00:03:33,240
Right, this is a hard one
because I can't tell anymore.

90
00:03:33,240 --> 00:03:35,070
We're just at a point in history

91
00:03:35,070 --> 00:03:37,470
where if I see a photo online,

92
00:03:37,470 --> 00:03:40,320
I don't know if it's real or if it's fake.

93
00:03:40,320 --> 00:03:41,700
And that wasn't true a few years ago.

94
00:03:41,700 --> 00:03:43,530
This is very much a new thing.

95
00:03:43,530 --> 00:03:45,780
The main way that I would try to assess

96
00:03:45,780 --> 00:03:48,630
if an image or media
online is real or fake

97
00:03:48,630 --> 00:03:51,330
is not gonna be by looking at artifacts

98
00:03:51,330 --> 00:03:52,650
or the visual content.

99
00:03:52,650 --> 00:03:54,360
It'll be, you know,
looking at the context.

100
00:03:54,360 --> 00:03:57,960
If there is some kind of
crazy implausible photo

101
00:03:57,960 --> 00:03:59,550
of some celebrity on Mars,

102
00:03:59,550 --> 00:04:01,320
I'm gonna think it's probably fake.

103
00:04:01,320 --> 00:04:04,290
If I don't trust this source,
if it's an anonymous source,

104
00:04:04,290 --> 00:04:06,000
I might not trust it.

105
00:04:06,000 --> 00:04:07,890
That's the state that we're in right now.

106
00:04:07,890 --> 00:04:10,170
Luckily, there are some
technical tools that can help.

107
00:04:10,170 --> 00:04:13,050
So there are AI systems
that have been trained

108
00:04:13,050 --> 00:04:16,320
to detect if a photo
or some kind of content

109
00:04:16,320 --> 00:04:20,430
is made by another AI or if
it's real authentic content.

110
00:04:20,430 --> 00:04:22,170
And those sort of work.

111
00:04:22,170 --> 00:04:25,320
We're in an arms race where
the generative AI methods

112
00:04:25,320 --> 00:04:29,430
get better and better at making
realistic synthetic content.

113
00:04:29,430 --> 00:04:31,500
And the detectors get better and better

114
00:04:31,500 --> 00:04:33,510
at detecting any little artifact

115
00:04:33,510 --> 00:04:36,750
that gives away that the image
or the content is not real

116
00:04:36,750 --> 00:04:38,850
and it's gonna go back and forth.

117
00:04:38,850 --> 00:04:41,490
That arms race is not gonna
have necessarily a clear winner,

118
00:04:41,490 --> 00:04:44,280
but we'll see what happens
over the next few years.

119
00:04:44,280 --> 00:04:47,070
Ultimately, I would say I trust media

120
00:04:47,070 --> 00:04:48,720
when I trust its source.

121
00:04:48,720 --> 00:04:51,420
I can't verify just with my eyes anymore

122
00:04:51,420 --> 00:04:53,280
whether or not an image is real or fake.

123
00:04:53,280 --> 00:04:55,230
Okay. Let's go to the next question.

124
00:04:55,230 --> 00:04:57,870
Could you explain to me what deep nets are

125
00:04:57,870 --> 00:04:59,910
and the kinds of
representations that they learn?

126
00:04:59,910 --> 00:05:02,160
So deep nets are, you know, super cool.

127
00:05:02,160 --> 00:05:05,400
They're the big thing in
AI that everyone uses now.

128
00:05:05,400 --> 00:05:09,660
Deep nets are basically
just a class of functions

129
00:05:09,660 --> 00:05:12,270
that can be used to make decisions

130
00:05:12,270 --> 00:05:15,120
or learn representations of the world.

131
00:05:15,120 --> 00:05:17,910
And there are class of
functions which are modeled

132
00:05:17,910 --> 00:05:20,790
off of how the brain works to some degree.

133
00:05:20,790 --> 00:05:23,220
So in our brain we have a neural network,

134
00:05:23,220 --> 00:05:25,050
a real neural network, and a deep net

135
00:05:25,050 --> 00:05:27,000
is kind of an approximation to that.

136
00:05:27,000 --> 00:05:28,050
It turns out that deep nets

137
00:05:28,050 --> 00:05:31,740
are really a simple class of
function, in a certain sense.

138
00:05:31,740 --> 00:05:34,560
They're just a stack of layers

139
00:05:34,560 --> 00:05:36,870
and every layer takes the input data

140
00:05:36,870 --> 00:05:39,690
at the bottom of the stack
and it kind of transforms it.

141
00:05:39,690 --> 00:05:41,610
And layer by layer the data gets processed

142
00:05:41,610 --> 00:05:44,010
and transformed until at
the top of the network

143
00:05:44,010 --> 00:05:46,770
you get a better kind of
representation of the data

144
00:05:46,770 --> 00:05:49,860
that can be used to make
predictions or decisions.

145
00:05:49,860 --> 00:05:53,040
And these layers are all
doing very simple operations.

146
00:05:53,040 --> 00:05:55,050
They're just doing what's
called a linear operation.

147
00:05:55,050 --> 00:05:57,960
And then a simple thing
called a point non-linearity.

148
00:05:57,960 --> 00:06:00,300
It's just this linear,
point wise non-linear,

149
00:06:00,300 --> 00:06:01,860
linear, point wise non-linear.

150
00:06:01,860 --> 00:06:03,810
So these are very simple
mathematical objects,

151
00:06:03,810 --> 00:06:07,650
but they can do amazing things
when they learn from data.

152
00:06:07,650 --> 00:06:11,160
And what kinds of representations
do deep nets learn?

153
00:06:11,160 --> 00:06:15,570
Well, they learn to
somehow organize the data

154
00:06:15,570 --> 00:06:18,900
in a way that makes it easy
to solve the task of interest,

155
00:06:18,900 --> 00:06:21,150
which might be to make a
prediction about the data,

156
00:06:21,150 --> 00:06:22,530
to classify the data

157
00:06:22,530 --> 00:06:24,660
or to make a decision on the basis of

158
00:06:24,660 --> 00:06:26,730
what the network is seeing.

159
00:06:26,730 --> 00:06:28,590
Okay. So let's go to the next question.

160
00:06:28,590 --> 00:06:31,530
How did your research in
image to image translation

161
00:06:31,530 --> 00:06:32,970
shape your current work?

162
00:06:32,970 --> 00:06:35,760
Okay, this is taking
me back a little ways.

163
00:06:35,760 --> 00:06:37,500
But back during my postdoc

164
00:06:37,500 --> 00:06:39,330
about seven or eight years ago,

165
00:06:39,330 --> 00:06:42,060
I worked on this problem called
image to image translation,

166
00:06:42,060 --> 00:06:46,590
which is to try to take
an image that is presented

167
00:06:46,590 --> 00:06:51,060
in one kind of style or
modality, like a color image,

168
00:06:51,060 --> 00:06:54,720
and predict what it would look
like viewed a different way.

169
00:06:54,720 --> 00:06:58,050
For example, translate
a photo into a sketch

170
00:06:58,050 --> 00:07:00,150
or translate a sketch into a photo,

171
00:07:00,150 --> 00:07:02,520
or maybe take a photo and try to say,

172
00:07:02,520 --> 00:07:05,130
what would it look like
if some artist, Cison,

173
00:07:05,130 --> 00:07:06,420
had painted that scene?

174
00:07:06,420 --> 00:07:08,640
So you can kind of visualize
the world in different ways

175
00:07:08,640 --> 00:07:10,440
and we called this a translation problem,

176
00:07:10,440 --> 00:07:13,680
translating from one visual
format to another one.

177
00:07:13,680 --> 00:07:16,650
So this project was really fun,

178
00:07:16,650 --> 00:07:20,820
it was popular, and it
definitely affected my thinking

179
00:07:20,820 --> 00:07:23,250
and my ongoing thinking about AI.

180
00:07:23,250 --> 00:07:26,580
To take you back, in the
era before we did that work,

181
00:07:26,580 --> 00:07:28,140
there were a lot of special purpose

182
00:07:28,140 --> 00:07:31,530
ways of solving problems
in computer vision.

183
00:07:31,530 --> 00:07:35,520
So if you wanted to solve a
problem, like object detection,

184
00:07:35,520 --> 00:07:37,470
try to label all the objects in a photo,

185
00:07:37,470 --> 00:07:39,210
you would use a fairly different method

186
00:07:39,210 --> 00:07:40,710
than if you wanted to solve a problem,

187
00:07:40,710 --> 00:07:43,770
like estimating the depth of
all the objects in the photo,

188
00:07:43,770 --> 00:07:46,560
like predicting what's called a depth map.

189
00:07:46,560 --> 00:07:48,930
So that didn't really satisfy me.

190
00:07:48,930 --> 00:07:51,720
I really wanted something
that would be a more uniform,

191
00:07:51,720 --> 00:07:53,670
unified solution to all those problems

192
00:07:53,670 --> 00:07:56,310
because what we kind of realized is that

193
00:07:56,310 --> 00:07:58,470
all of these different
computer vision problems

194
00:07:58,470 --> 00:08:01,860
could be thought of as taking
some set of pixels as input

195
00:08:01,860 --> 00:08:04,980
and predicting some other
set of pixels as output.

196
00:08:04,980 --> 00:08:08,130
So the output pixels could
represent where the objects are,

197
00:08:08,130 --> 00:08:11,280
or they could visualize
the depth map of the scene.

198
00:08:11,280 --> 00:08:13,440
So these were all pixel to pixel problems

199
00:08:13,440 --> 00:08:16,020
and that led to us calling
this project Pix to Pix.

200
00:08:16,020 --> 00:08:19,350
That was the name we gave
it, Pixels to Pixels.

201
00:08:19,350 --> 00:08:23,220
Essentially, we had a more
general purpose framework

202
00:08:23,220 --> 00:08:24,780
for solving all of those problems,

203
00:08:24,780 --> 00:08:27,480
rather than having specialized
solutions for each one.

204
00:08:28,350 --> 00:08:30,930
And that made me realize essentially

205
00:08:30,930 --> 00:08:33,570
how powerful a more general purpose

206
00:08:33,570 --> 00:08:35,700
and unified solution can be.

207
00:08:35,700 --> 00:08:37,890
And really the big trend
over the last, you know,

208
00:08:37,890 --> 00:08:40,950
5 or 10 years of AI, has been toward these

209
00:08:40,950 --> 00:08:42,990
general purpose solutions.

210
00:08:42,990 --> 00:08:44,940
The biggest phase of that now is

211
00:08:44,940 --> 00:08:46,680
with these large language models

212
00:08:46,680 --> 00:08:49,320
where one big model is trained to do

213
00:08:49,320 --> 00:08:50,880
a lot of different tasks.

214
00:08:50,880 --> 00:08:52,770
So when you interact with a chat bot

215
00:08:52,770 --> 00:08:56,010
or a language model, you can
ask it all kinds of questions.

216
00:08:56,010 --> 00:08:57,660
You can even ask it to make an image

217
00:08:57,660 --> 00:09:00,360
or to solve a math problem.

218
00:09:00,360 --> 00:09:05,360
And it's one model that with
a kind of generic framework

219
00:09:05,910 --> 00:09:08,250
is able to solve all those problems.

220
00:09:08,250 --> 00:09:09,570
So I think that's an interesting trend

221
00:09:09,570 --> 00:09:11,100
and that's what I'm always
kind of on the outlook

222
00:09:11,100 --> 00:09:12,060
for right now.

223
00:09:12,060 --> 00:09:14,550
Simple, unifying principles and methods

224
00:09:14,550 --> 00:09:17,850
that can be applied to a
wide variety of problems.

225
00:09:17,850 --> 00:09:22,290
Any recommendations for the
best AI image generation models?

226
00:09:22,290 --> 00:09:23,250
Hmm, okay.

227
00:09:23,250 --> 00:09:26,220
One I really like right now
is called Stable Diffusion.

228
00:09:26,220 --> 00:09:28,620
I like this model
because it's open source,

229
00:09:28,620 --> 00:09:30,630
so we get to use it and intervene on it

230
00:09:30,630 --> 00:09:32,880
and really study it as scientist.

231
00:09:32,880 --> 00:09:36,390
And it's one of the image
generation models that works well.

232
00:09:36,390 --> 00:09:39,120
So my lab mostly uses Stable Diffusion

233
00:09:39,120 --> 00:09:41,670
in our research as the current model.

234
00:09:41,670 --> 00:09:44,550
Another one that I like a
lot is called Control Net.

235
00:09:44,550 --> 00:09:49,110
Control Net is kind of the
current most popular iteration

236
00:09:49,110 --> 00:09:51,330
of image to image translation.

237
00:09:51,330 --> 00:09:54,720
We did, you know, old work like X Depicts

238
00:09:54,720 --> 00:09:56,490
and now people have gone way beyond that

239
00:09:56,490 --> 00:09:58,590
and Control Net is the latest.

240
00:09:58,590 --> 00:10:00,300
So what it does is it allows you

241
00:10:00,300 --> 00:10:04,470
to condition your
generative model on an image

242
00:10:04,470 --> 00:10:05,970
and produce another image.

243
00:10:05,970 --> 00:10:07,560
For example, you can take a sketch

244
00:10:07,560 --> 00:10:10,366
and you can turn that sketch into a photo.

245
00:10:10,366 --> 00:10:12,600
So Control Net, Stable Diffusion,

246
00:10:12,600 --> 00:10:13,890
these are some of the fun ones

247
00:10:13,890 --> 00:10:15,190
that I'm playing with now.

