1
00:00:01,280 --> 00:00:06,319
Okay, I'll go ahead and get started.

2
00:00:03,679 --> 00:00:08,000
Good afternoon. My name is Aspen Hopkins

3
00:00:06,319 --> 00:00:10,000
and I am going to be presenting joint

4
00:00:08,000 --> 00:00:12,400
work with my collaborators Isabella

5
00:00:10,000 --> 00:00:15,440
Struckman, Kevin Kleimman, and of

6
00:00:12,400 --> 00:00:17,920
course, Susan Sylvie. Um, and today

7
00:00:15,440 --> 00:00:20,400
we'll be talking about AI supply chains,

8
00:00:17,920 --> 00:00:23,920
how stakeholders, markets, and the

9
00:00:20,400 --> 00:00:26,080
complexities of AI shape harm and inform

10
00:00:23,920 --> 00:00:27,960
redress.

11
00:00:26,080 --> 00:00:30,800
I want to start with a fairly direct

12
00:00:27,960 --> 00:00:33,200
provocation which is that AI supply um

13
00:00:30,800 --> 00:00:35,120
AI systems are increasingly the result

14
00:00:33,200 --> 00:00:37,840
of fragmented development across

15
00:00:35,120 --> 00:00:40,920
multiple organizations contributing

16
00:00:37,840 --> 00:00:43,760
models, data and AI

17
00:00:40,920 --> 00:00:45,920
services. And these kinds of composed

18
00:00:43,760 --> 00:00:48,239
systems are embedded throughout our

19
00:00:45,920 --> 00:00:51,360
daily lives. Consider an example of a

20
00:00:48,239 --> 00:00:53,280
patient going to an ER complaining of a

21
00:00:51,360 --> 00:00:55,440
stomach ache.

22
00:00:53,280 --> 00:00:57,440
In the ER, they tell their provider some

23
00:00:55,440 --> 00:00:59,760
details. The doctor discusses the

24
00:00:57,440 --> 00:01:02,719
symptoms and makes some observations to

25
00:00:59,760 --> 00:01:04,479
direct next steps of their care. This

26
00:01:02,719 --> 00:01:07,040
conversation is transcribed and

27
00:01:04,479 --> 00:01:09,040
summarized by a voice enabled AI

28
00:01:07,040 --> 00:01:11,920
assistant which updates the electronic

29
00:01:09,040 --> 00:01:14,400
health record of the patient. This might

30
00:01:11,920 --> 00:01:16,960
sound familiar.

31
00:01:14,400 --> 00:01:19,200
These notes are fed into an upstream or

32
00:01:16,960 --> 00:01:22,080
downstream sepsis model which flags

33
00:01:19,200 --> 00:01:24,640
high-risk patients for escalation to

34
00:01:22,080 --> 00:01:26,799
support triage. This information is then

35
00:01:24,640 --> 00:01:29,479
fed back into an EHR system with

36
00:01:26,799 --> 00:01:32,640
information about the patients

37
00:01:29,479 --> 00:01:34,720
risks. This interdepend interdependent

38
00:01:32,640 --> 00:01:37,280
system I just described is an example of

39
00:01:34,720 --> 00:01:39,840
an emerging phenomena we call AI supply

40
00:01:37,280 --> 00:01:42,240
chains.

41
00:01:39,840 --> 00:01:44,079
We can redraw this system view to

42
00:01:42,240 --> 00:01:46,759
emphasize which components are doing

43
00:01:44,079 --> 00:01:49,280
what at any given

44
00:01:46,759 --> 00:01:50,960
time. Going back to our example, it

45
00:01:49,280 --> 00:01:54,000
turns out that the patient displayed

46
00:01:50,960 --> 00:01:56,320
symptoms of sepsis, but was not flagged

47
00:01:54,000 --> 00:01:58,240
in a timely manner. If I were to ask you

48
00:01:56,320 --> 00:01:59,920
which component in this AI supply chain

49
00:01:58,240 --> 00:02:02,000
is most likely responsible for this

50
00:01:59,920 --> 00:02:04,479
failure, you'll probably say the sepsis

51
00:02:02,000 --> 00:02:06,320
model. After all, this is the AI

52
00:02:04,479 --> 00:02:08,560
component that is actually monitoring

53
00:02:06,320 --> 00:02:11,200
for sepsis.

54
00:02:08,560 --> 00:02:13,760
and you would be sort of correct, but

55
00:02:11,200 --> 00:02:16,480
also incorrect. I'm here today because

56
00:02:13,760 --> 00:02:19,120
this problem is not as clearcut as we

57
00:02:16,480 --> 00:02:21,280
might think initially. It turns out that

58
00:02:19,120 --> 00:02:24,239
a few days ago in our example, the

59
00:02:21,280 --> 00:02:26,879
voicetoext app had a system update. The

60
00:02:24,239 --> 00:02:29,360
update overall improved quality and

61
00:02:26,879 --> 00:02:31,360
accuracy of patient summarizations.

62
00:02:29,360 --> 00:02:33,360
But for a subset of doctors with

63
00:02:31,360 --> 00:02:35,840
specific accents, it actually

64
00:02:33,360 --> 00:02:38,480
experienced a regression, a distribution

65
00:02:35,840 --> 00:02:40,959
shift from its expected behavior. And

66
00:02:38,480 --> 00:02:43,959
this introduced unwanted distortions in

67
00:02:40,959 --> 00:02:46,319
the EHR record like mistransating

68
00:02:43,959 --> 00:02:47,840
keywords. Information about this update

69
00:02:46,319 --> 00:02:50,879
wasn't necessarily shared with the

70
00:02:47,840 --> 00:02:52,400
sepsis model developer and in fact that

71
00:02:50,879 --> 00:02:56,480
developer might not be aware of the

72
00:02:52,400 --> 00:02:58,959
voicetoext app in the first place.

73
00:02:56,480 --> 00:03:01,040
The challenge is that the crisis that I

74
00:02:58,959 --> 00:03:02,959
just described isn't necessarily a

75
00:03:01,040 --> 00:03:04,599
technical failure on the part of just

76
00:03:02,959 --> 00:03:07,239
the sepsis model

77
00:03:04,599 --> 00:03:10,120
provider. This is an AI supply chain

78
00:03:07,239 --> 00:03:12,480
failure. Harms like poor patient

79
00:03:10,120 --> 00:03:14,519
outcomes, the sepsis model provider

80
00:03:12,480 --> 00:03:17,840
losing customers over ineffective

81
00:03:14,519 --> 00:03:21,000
products, loss of hospital reputation

82
00:03:17,840 --> 00:03:24,200
are all the results. But who is actually

83
00:03:21,000 --> 00:03:27,599
responsible? And how can AI supply chain

84
00:03:24,200 --> 00:03:29,360
participants fix it? Can they even do

85
00:03:27,599 --> 00:03:31,640
this without the help of the upstream

86
00:03:29,360 --> 00:03:34,720
voice to tax

87
00:03:31,640 --> 00:03:37,519
app? I'm here today because AI supply

88
00:03:34,720 --> 00:03:40,239
chains are humanmade. That means that we

89
00:03:37,519 --> 00:03:43,440
can design them. But to do so, we must

90
00:03:40,239 --> 00:03:46,000
ask who participates in these systems?

91
00:03:43,440 --> 00:03:47,760
What happens when harms occur? And how

92
00:03:46,000 --> 00:03:52,360
do power differentials and market

93
00:03:47,760 --> 00:03:55,040
structures shape our ability to respond?

94
00:03:52,360 --> 00:03:56,840
Poignantly, there is no one-sizefits-all

95
00:03:55,040 --> 00:03:58,920
in AI supply chain

96
00:03:56,840 --> 00:04:01,640
configurations. This makes generalizing

97
00:03:58,920 --> 00:04:03,680
interventions exceptionally

98
00:04:01,640 --> 00:04:05,519
difficult. There are also many

99
00:04:03,680 --> 00:04:07,799
stakeholder roles. There are

100
00:04:05,519 --> 00:04:10,319
infrastructure, data and model

101
00:04:07,799 --> 00:04:12,560
providers. There are user and consumerf

102
00:04:10,319 --> 00:04:15,599
facing developers. There are

103
00:04:12,560 --> 00:04:18,959
intermediary AI services. And of course,

104
00:04:15,599 --> 00:04:21,359
there are users and consumers.

105
00:04:18,959 --> 00:04:23,680
AI supply chains are not just technical.

106
00:04:21,359 --> 00:04:25,840
They are institutional arrangements with

107
00:04:23,680 --> 00:04:28,080
many stakeholders that have incentives

108
00:04:25,840 --> 00:04:31,120
shaped by contracts, limited

109
00:04:28,080 --> 00:04:33,680
coordination or accountability, and that

110
00:04:31,120 --> 00:04:35,919
often play multiple roles. What this

111
00:04:33,680 --> 00:04:37,919
means is that even those who cause harm,

112
00:04:35,919 --> 00:04:39,919
failing to capture the census, for

113
00:04:37,919 --> 00:04:42,240
example, may be constrained in

114
00:04:39,919 --> 00:04:45,240
responding due to position or to market

115
00:04:42,240 --> 00:04:45,240
incentives.

116
00:04:45,440 --> 00:04:49,919
To restate this, stakeholders in AI

117
00:04:47,919 --> 00:04:52,720
supply chains operate with unequal

118
00:04:49,919 --> 00:04:55,600
bargaining power. Variations in power

119
00:04:52,720 --> 00:04:57,919
and influence shape outcomes. And this

120
00:04:55,600 --> 00:05:00,240
becomes most evident when we examine not

121
00:04:57,919 --> 00:05:03,639
the harm itself, but how a responsible

122
00:05:00,240 --> 00:05:06,560
party responds to a

123
00:05:03,639 --> 00:05:09,280
harm. In our work, we contribute a

124
00:05:06,560 --> 00:05:11,759
typology of redress. We ask, how do we

125
00:05:09,280 --> 00:05:13,280
respond to harms? Now that we've seen

126
00:05:11,759 --> 00:05:15,759
some examples of harms that can emerge

127
00:05:13,280 --> 00:05:17,320
in an AI supply chain, we need to think

128
00:05:15,759 --> 00:05:20,160
about how do we fix

129
00:05:17,320 --> 00:05:22,639
them. We describe five different cases

130
00:05:20,160 --> 00:05:24,560
of responses. Recognition in which you

131
00:05:22,639 --> 00:05:27,360
acknowledge that a harm occurred.

132
00:05:24,560 --> 00:05:30,800
Recourse in which you stop the harm.

133
00:05:27,360 --> 00:05:33,199
Repair in which the harm is corrected.

134
00:05:30,800 --> 00:05:35,440
Reparation or compensation. And

135
00:05:33,199 --> 00:05:36,960
prevention in which we learn from and

136
00:05:35,440 --> 00:05:38,960
then ensure that future harms are

137
00:05:36,960 --> 00:05:41,039
mitigated by adopting new policies or

138
00:05:38,960 --> 00:05:42,800
processes.

139
00:05:41,039 --> 00:05:44,639
These forms of response are common

140
00:05:42,800 --> 00:05:46,639
across domains but they operate

141
00:05:44,639 --> 00:05:47,639
differently depending on who you are in

142
00:05:46,639 --> 00:05:51,479
the

143
00:05:47,639 --> 00:05:54,000
system. So when I ask will redress

144
00:05:51,479 --> 00:05:56,080
happen we have to note that it's not

145
00:05:54,000 --> 00:05:58,160
just about issuing a fix or a refund.

146
00:05:56,080 --> 00:06:00,639
It's more complex. It involves

147
00:05:58,160 --> 00:06:02,160
determining who can act what's

148
00:06:00,639 --> 00:06:04,160
technically and organizationally

149
00:06:02,160 --> 00:06:06,000
possible and whether there is enough

150
00:06:04,160 --> 00:06:08,400
agreement among stakeholders to make

151
00:06:06,000 --> 00:06:10,560
something happen. In other words, we can

152
00:06:08,400 --> 00:06:12,800
re define redress as possible when two

153
00:06:10,560 --> 00:06:15,080
things hold. That the redress is

154
00:06:12,800 --> 00:06:17,120
achievable physically, technically,

155
00:06:15,080 --> 00:06:19,120
legally, and that there's minimal

156
00:06:17,120 --> 00:06:21,759
consensus, meaning the relevant parties

157
00:06:19,120 --> 00:06:23,199
agree to act or can be compelled to.

158
00:06:21,759 --> 00:06:26,240
That's where things get tricky in AI

159
00:06:23,199 --> 00:06:28,720
supply chains. There's not systems where

160
00:06:26,240 --> 00:06:30,800
any one company owns and operates

161
00:06:28,720 --> 00:06:32,319
everything. They're fragmented, built

162
00:06:30,800 --> 00:06:34,319
from models, services, and

163
00:06:32,319 --> 00:06:36,319
infrastructure spread across films with

164
00:06:34,319 --> 00:06:38,720
firms with different incentives and

165
00:06:36,319 --> 00:06:42,240
levels of control. So when a harm

166
00:06:38,720 --> 00:06:44,880
occurs, say a patient is misprioritized

167
00:06:42,240 --> 00:06:46,960
or a hospital loses trust in a system,

168
00:06:44,880 --> 00:06:50,199
the path to fixing it is often blocked

169
00:06:46,960 --> 00:06:52,319
not by malice, but by

170
00:06:50,199 --> 00:06:53,919
structure. The question then becomes,

171
00:06:52,319 --> 00:06:56,240
what shapes what is achievable? What

172
00:06:53,919 --> 00:06:58,000
makes consensus possible or impossible?

173
00:06:56,240 --> 00:06:59,880
and supply chains. The answer is market

174
00:06:58,000 --> 00:07:02,319
structure and institutional

175
00:06:59,880 --> 00:07:04,639
arrangements. The ways that AI services

176
00:07:02,319 --> 00:07:05,840
are sourced, built, delivered, whether

177
00:07:04,639 --> 00:07:07,840
they're vertically integrated,

178
00:07:05,840 --> 00:07:08,919
horizontally, modular, or fragmented

179
00:07:07,840 --> 00:07:11,840
into

180
00:07:08,919 --> 00:07:14,240
microervices, directly shapes what kinds

181
00:07:11,840 --> 00:07:17,039
of redress are even on the table. And

182
00:07:14,240 --> 00:07:19,520
often the same features that make AI

183
00:07:17,039 --> 00:07:23,039
scalable or cost-effective also make

184
00:07:19,520 --> 00:07:24,720
harm harder to trace and fix. In our

185
00:07:23,039 --> 00:07:25,560
work, we examine this across three

186
00:07:24,720 --> 00:07:27,919
common

187
00:07:25,560 --> 00:07:31,039
configurations. Um, I'm going to focus

188
00:07:27,919 --> 00:07:33,199
on the vertical integration because of

189
00:07:31,039 --> 00:07:35,440
time constraints. So, let's look a

190
00:07:33,199 --> 00:07:38,160
little bit more closely at how a harm

191
00:07:35,440 --> 00:07:40,160
and redress plays out in this setting.

192
00:07:38,160 --> 00:07:42,479
In a vertically integrated market, one

193
00:07:40,160 --> 00:07:44,560
firm owns the full stack. In our

194
00:07:42,479 --> 00:07:46,800
example, this might mean that the voice

195
00:07:44,560 --> 00:07:49,520
to text app, the sepsis model, and the

196
00:07:46,800 --> 00:07:53,039
EHR system is all owned by one company.

197
00:07:49,520 --> 00:07:55,360
Perhaps you can think of examples

198
00:07:53,039 --> 00:07:57,120
In vertically integrated markets, that

199
00:07:55,360 --> 00:07:59,520
firm gets to decide whether the harm is

200
00:07:57,120 --> 00:08:02,720
even recognized. Recourse, the ability

201
00:07:59,520 --> 00:08:04,720
to avoid or disable a harmful component,

202
00:08:02,720 --> 00:08:06,919
is limited because you can't remove one

203
00:08:04,720 --> 00:08:09,280
piece without necessarily leaving the

204
00:08:06,919 --> 00:08:10,879
ecosystem. Repair is sometimes easier

205
00:08:09,280 --> 00:08:13,360
because the firm controls the whole

206
00:08:10,879 --> 00:08:15,039
stack. So, you can choose what to fix

207
00:08:13,360 --> 00:08:17,360
and you can choose to holistically fix

208
00:08:15,039 --> 00:08:18,720
it. But fixes are prioritized based off

209
00:08:17,360 --> 00:08:21,440
of business interests and not

210
00:08:18,720 --> 00:08:23,599
stakeholder needs.

211
00:08:21,440 --> 00:08:25,280
When it comes to rep uh reparation or

212
00:08:23,599 --> 00:08:27,319
compensating people who are harmed,

213
00:08:25,280 --> 00:08:29,440
things get murkier. Without

214
00:08:27,319 --> 00:08:31,120
transparency, firms can minimize or

215
00:08:29,440 --> 00:08:32,800
obscure their role in the harm, making

216
00:08:31,120 --> 00:08:35,120
it difficult for affected parties to

217
00:08:32,800 --> 00:08:38,240
even ask for compensation, let alone

218
00:08:35,120 --> 00:08:41,120
receive it. And prevention, making

219
00:08:38,240 --> 00:08:42,719
changes to avoid future harms, is often

220
00:08:41,120 --> 00:08:44,880
only prioritized when there's external

221
00:08:42,719 --> 00:08:48,000
pressure, say from regulation, public

222
00:08:44,880 --> 00:08:49,519
outcry, or reputational risk.

223
00:08:48,000 --> 00:08:51,120
Across the board, market structure

224
00:08:49,519 --> 00:08:53,959
doesn't just determine who builds what,

225
00:08:51,120 --> 00:08:56,480
it constrains how and whether harm is

226
00:08:53,959 --> 00:08:58,880
addressed. So, if these supply chain

227
00:08:56,480 --> 00:09:01,120
specific harms aren't caused by one

228
00:08:58,880 --> 00:09:03,279
actor or one broken model, but by the

229
00:09:01,120 --> 00:09:05,839
way the system is organized, then the

230
00:09:03,279 --> 00:09:07,839
fix itself can't just be technical. It

231
00:09:05,839 --> 00:09:09,600
has to be structural. And that brings us

232
00:09:07,839 --> 00:09:11,839
to our final point, which is

233
00:09:09,600 --> 00:09:13,640
intervention is possible, but it has to

234
00:09:11,839 --> 00:09:16,160
be targeted and it has to be

235
00:09:13,640 --> 00:09:18,320
intentional. When we say designing AI

236
00:09:16,160 --> 00:09:20,399
supply chains, we don't just mean

237
00:09:18,320 --> 00:09:22,560
choosing what APIs to use or how to

238
00:09:20,399 --> 00:09:24,160
fine-tune a model. We mean shaping the

239
00:09:22,560 --> 00:09:27,360
institutional context in which these

240
00:09:24,160 --> 00:09:29,680
systems operate through law, governance

241
00:09:27,360 --> 00:09:31,519
and market mechanisms. This can look

242
00:09:29,680 --> 00:09:33,200
like regulation which forces

243
00:09:31,519 --> 00:09:34,880
transparency, traceability or

244
00:09:33,200 --> 00:09:37,040
auditability.

245
00:09:34,880 --> 00:09:39,440
um licensing standards, procurement

246
00:09:37,040 --> 00:09:41,120
criteria, especially in public sector AI

247
00:09:39,440 --> 00:09:43,279
that demands evidence of recourse or

248
00:09:41,120 --> 00:09:45,839
repair mechanisms, contractual

249
00:09:43,279 --> 00:09:48,720
frameworks that clarify accountability

250
00:09:45,839 --> 00:09:50,560
between firms, and yes, tort law, where

251
00:09:48,720 --> 00:09:52,519
harm can't be avoided, there must be

252
00:09:50,560 --> 00:09:54,800
pathways to

253
00:09:52,519 --> 00:09:57,519
compensation. The point here isn't that

254
00:09:54,800 --> 00:09:59,440
one solution can work everywhere. Harms

255
00:09:57,519 --> 00:10:01,519
are fundamental to risky business and to

256
00:09:59,440 --> 00:10:03,200
new technology. Each of these

257
00:10:01,519 --> 00:10:05,040
interventions can target a specific

258
00:10:03,200 --> 00:10:07,360
structural barrier to redress for all

259
00:10:05,040 --> 00:10:09,440
stakeholders and together they can help

260
00:10:07,360 --> 00:10:11,080
build systems where it's possible to act

261
00:10:09,440 --> 00:10:13,519
before harms

262
00:10:11,080 --> 00:10:15,360
cascade. The final point I want to leave

263
00:10:13,519 --> 00:10:18,000
you with is that AI supply chains are

264
00:10:15,360 --> 00:10:20,320
not natural. They are made. And that

265
00:10:18,000 --> 00:10:21,959
means we can design them differently. We

266
00:10:20,320 --> 00:10:23,920
can design for transparency, for

267
00:10:21,959 --> 00:10:25,680
interoperability, for feedback channels

268
00:10:23,920 --> 00:10:29,839
that let stakeholders identify harm

269
00:10:25,680 --> 00:10:31,680
early and to do something about it.

270
00:10:29,839 --> 00:10:33,839
We have to look at the entire system,

271
00:10:31,680 --> 00:10:36,000
the actors, the markets, the

272
00:10:33,839 --> 00:10:38,800
institutions, and ask who has the power

273
00:10:36,000 --> 00:10:41,519
to respond when things go wrong, who's

274
00:10:38,800 --> 00:10:44,000
left without a voice. Because in an AI

275
00:10:41,519 --> 00:10:47,480
today, those answers are often, frankly,

276
00:10:44,000 --> 00:10:51,720
structured long before the harm

277
00:10:47,480 --> 00:10:51,720
occurs. Thank you.

278
00:10:54,640 --> 00:10:59,000
Thank you, Aston. questions for us.

279
00:11:00,640 --> 00:11:04,760
Uh sorry I missed. Yeah,

280
00:11:08,920 --> 00:11:14,160
thanks. Um there is significant body of

281
00:11:11,600 --> 00:11:16,480
work on digital supply chains. Uh what's

282
00:11:14,160 --> 00:11:19,120
different? What's what does AI bring

283
00:11:16,480 --> 00:11:21,920
into into the into the mix which is

284
00:11:19,120 --> 00:11:23,360
different or substantially new? Yeah,

285
00:11:21,920 --> 00:11:24,800
that's a wonderful question. And I think

286
00:11:23,360 --> 00:11:28,959
this is a question on many people's

287
00:11:24,800 --> 00:11:31,200
minds. Um so we have a bunch of work

288
00:11:28,959 --> 00:11:33,600
that we actually didn't include in this

289
00:11:31,200 --> 00:11:35,760
presentation. Um some of it talks about

290
00:11:33,600 --> 00:11:37,600
what's new to AI from the supply chain

291
00:11:35,760 --> 00:11:40,000
perspective. The other side is what's

292
00:11:37,600 --> 00:11:42,000
new to supply chains from the AI

293
00:11:40,000 --> 00:11:44,560
perspective. I think one of the

294
00:11:42,000 --> 00:11:47,680
challenges with AI specifically is that

295
00:11:44,560 --> 00:11:49,920
you are operating off of um

296
00:11:47,680 --> 00:11:52,399
distributions. So you're learning and

297
00:11:49,920 --> 00:11:54,399
producing distributions rather than

298
00:11:52,399 --> 00:11:56,240
modular components that have inherent

299
00:11:54,399 --> 00:11:57,839
traceability which you do have in in

300
00:11:56,240 --> 00:12:00,640
digital supply chains like software.

301
00:11:57,839 --> 00:12:03,040
Software is traceable. Um granted that

302
00:12:00,640 --> 00:12:05,680
was designed for but in many cases it is

303
00:12:03,040 --> 00:12:07,839
hard to take two different probabilities

304
00:12:05,680 --> 00:12:10,000
and understand where the probability

305
00:12:07,839 --> 00:12:11,839
it's like the the output that you get is

306
00:12:10,000 --> 00:12:14,720
itself comes from. you have issues of

307
00:12:11,839 --> 00:12:17,120
like no onetoone mapping or you have

308
00:12:14,720 --> 00:12:19,120
things like collapsing distributions and

309
00:12:17,120 --> 00:12:21,120
and it becomes exceptionally challenging

310
00:12:19,120 --> 00:12:23,200
to do this traceability that we care

311
00:12:21,120 --> 00:12:25,279
about or it becomes computationally

312
00:12:23,200 --> 00:12:26,880
expensive um and therefore you have to

313
00:12:25,279 --> 00:12:29,600
have ownership over the model or

314
00:12:26,880 --> 00:12:32,880
whatever it is.

315
00:12:29,600 --> 00:12:34,480
Great. Other questions?

316
00:12:32,880 --> 00:12:36,560
Yeah,

317
00:12:34,480 --> 00:12:39,120
I should also say that Susan's in the in

318
00:12:36,560 --> 00:12:41,519
the audience and is very respected when

319
00:12:39,120 --> 00:12:44,000
it comes to regulations. So ask your

320
00:12:41,519 --> 00:12:46,160
questions as well. Yeah. So uh the

321
00:12:44,000 --> 00:12:48,720
question that I have is regarding

322
00:12:46,160 --> 00:12:51,760
thinking about this uh supply chain that

323
00:12:48,720 --> 00:12:55,440
you outline like uh there's a different

324
00:12:51,760 --> 00:12:58,639
uh stakeholder uh along this uh supply

325
00:12:55,440 --> 00:13:02,240
chain and how can you bring when you

326
00:12:58,639 --> 00:13:06,720
talking about design it seems uh to be

327
00:13:02,240 --> 00:13:08,720
that design in a relative uh isolate

328
00:13:06,720 --> 00:13:11,920
environment when you talking about

329
00:13:08,720 --> 00:13:14,880
design but then the impact is actually

330
00:13:11,920 --> 00:13:17,560
uh there's multiple stakeholder multiple

331
00:13:14,880 --> 00:13:20,320
like party involved and how can you

332
00:13:17,560 --> 00:13:22,959
ensure that their incentive their

333
00:13:20,320 --> 00:13:26,160
interest uh kind of align with uh this

334
00:13:22,959 --> 00:13:28,000
kind of uh design process I'm

335
00:13:26,160 --> 00:13:31,519
particularly thinking about in this kind

336
00:13:28,000 --> 00:13:33,920
of uh environment of the hospital uh

337
00:13:31,519 --> 00:13:36,240
healthcare related that probably a

338
00:13:33,920 --> 00:13:39,040
strong interest uh from insurance

339
00:13:36,240 --> 00:13:42,560
standpoint so for example hospital want

340
00:13:39,040 --> 00:13:45,920
to make sure that uh the kind of uh case

341
00:13:42,560 --> 00:13:48,880
happen early introduced is very little.

342
00:13:45,920 --> 00:13:51,040
So that the in premium won't be

343
00:13:48,880 --> 00:13:53,839
increased uh uh next year something like

344
00:13:51,040 --> 00:13:57,040
that and uh that kind of incentive

345
00:13:53,839 --> 00:14:00,639
probably would uh help you uh kind of

346
00:13:57,040 --> 00:14:03,680
try to make uh optimize of uh design. I

347
00:14:00,639 --> 00:14:05,519
I see um to interpret your question I

348
00:14:03,680 --> 00:14:07,920
feel like there are two or three parts.

349
00:14:05,519 --> 00:14:11,040
Um the first was that it's hard to

350
00:14:07,920 --> 00:14:13,040
design for systems that have um very

351
00:14:11,040 --> 00:14:17,120
complex incentives that may not

352
00:14:13,040 --> 00:14:20,959
necessarily align right. Um Susan has a

353
00:14:17,120 --> 00:14:23,440
body of work on regulation and on um the

354
00:14:20,959 --> 00:14:25,399
common place of law, how you know when

355
00:14:23,440 --> 00:14:28,399
law is written, it is actually

356
00:14:25,399 --> 00:14:30,639
interpreted. Um often there has to be an

357
00:14:28,399 --> 00:14:33,120
adjudication process. The point of our

358
00:14:30,639 --> 00:14:35,760
work is actually to highlight that these

359
00:14:33,120 --> 00:14:39,199
um misalignments and incentives and

360
00:14:35,760 --> 00:14:40,959
perhaps more critically um the power the

361
00:14:39,199 --> 00:14:43,480
relationships and powers across AI

362
00:14:40,959 --> 00:14:47,959
supply chains can

363
00:14:43,480 --> 00:14:50,720
push away some of the necessary

364
00:14:47,959 --> 00:14:53,440
uh pieces of the conversation. And so

365
00:14:50,720 --> 00:14:56,079
and so like I think your comment around

366
00:14:53,440 --> 00:14:58,959
insurance being a narrative that we can

367
00:14:56,079 --> 00:15:01,199
align around I'm I'm not sure that I

368
00:14:58,959 --> 00:15:04,079
necessarily agree per se. I think that

369
00:15:01,199 --> 00:15:06,000
it can surface some of the challenges

370
00:15:04,079 --> 00:15:08,240
and the question that I would have then

371
00:15:06,000 --> 00:15:09,800
is the people who are designing these

372
00:15:08,240 --> 00:15:12,079
mechanisms for

373
00:15:09,800 --> 00:15:14,480
ensuring more transparency and

374
00:15:12,079 --> 00:15:18,000
communication around updates. how they

375
00:15:14,480 --> 00:15:20,639
do so in a way that does not center only

376
00:15:18,000 --> 00:15:22,600
the insurance company but also looks at

377
00:15:20,639 --> 00:15:24,880
these

378
00:15:22,600 --> 00:15:30,199
um more modular components. I think

379
00:15:24,880 --> 00:15:30,199
Susan has an answer though. Thank you.

380
00:15:30,639 --> 00:15:36,959
It's on. Hello. Oh, sorry. Uh thank you

381
00:15:34,399 --> 00:15:37,800
for that very fine question and it is a

382
00:15:36,959 --> 00:15:40,639
very

383
00:15:37,800 --> 00:15:43,519
complicated question and situation which

384
00:15:40,639 --> 00:15:47,519
you by your your question illustrate

385
00:15:43,519 --> 00:15:51,000
that it is very rare to be able to

386
00:15:47,519 --> 00:15:54,000
create a system of traceability and

387
00:15:51,000 --> 00:15:56,880
accountability with so many stakeholders

388
00:15:54,000 --> 00:15:58,959
with divergent interests and that is

389
00:15:56,880 --> 00:16:00,480
basically the history of government

390
00:15:58,959 --> 00:16:03,240
regulation.

391
00:16:00,480 --> 00:16:07,320
There is not a way for the market by

392
00:16:03,240 --> 00:16:11,360
itself to be able to do that. The market

393
00:16:07,320 --> 00:16:14,560
requires minimally a system of contracts

394
00:16:11,360 --> 00:16:17,440
and tors to enforce it. But that relies

395
00:16:14,560 --> 00:16:20,800
on the resources of the individual

396
00:16:17,440 --> 00:16:23,199
actors in a complex supply chain.

397
00:16:20,800 --> 00:16:25,519
They're not going to be aligned. So the

398
00:16:23,199 --> 00:16:29,199
comments that Marise made at the end of

399
00:16:25,519 --> 00:16:32,240
her presentation just before on almost

400
00:16:29,199 --> 00:16:35,600
the same example of a problem in data in

401
00:16:32,240 --> 00:16:37,959
the hospital. She made the point that it

402
00:16:35,600 --> 00:16:41,600
is only through the

403
00:16:37,959 --> 00:16:44,959
collective resources and information

404
00:16:41,600 --> 00:16:48,480
collect collected by government agencies

405
00:16:44,959 --> 00:16:50,199
that one can see across the competing

406
00:16:48,480 --> 00:16:53,040
interests and

407
00:16:50,199 --> 00:16:55,720
compromise to some sort of set of

408
00:16:53,040 --> 00:17:00,399
boundaries and responsibilities. Though

409
00:16:55,720 --> 00:17:03,559
as Aspen puts up, short of government in

410
00:17:00,399 --> 00:17:06,839
the past industries have

411
00:17:03,559 --> 00:17:09,679
self-regulated. This one has

412
00:17:06,839 --> 00:17:12,240
not. There there are some attempts to um

413
00:17:09,679 --> 00:17:14,799
particularly in the agentic space. Um

414
00:17:12,240 --> 00:17:16,799
there's something that the Silicon Bros

415
00:17:14,799 --> 00:17:19,839
are excited about called model context

416
00:17:16,799 --> 00:17:23,600
protocol. It is an example where a

417
00:17:19,839 --> 00:17:26,240
system is being designed already with

418
00:17:23,600 --> 00:17:28,319
that comment that I had earlier. Um that

419
00:17:26,240 --> 00:17:30,960
the structure is in place before the

420
00:17:28,319 --> 00:17:33,919
harm necessarily incur occurs and so you

421
00:17:30,960 --> 00:17:37,280
know participatory design maybe is one

422
00:17:33,919 --> 00:17:41,720
solution to this.

423
00:17:37,280 --> 00:17:41,720
Thank you. Any further questions?

424
00:17:47,280 --> 00:17:51,440
There's also a lot of very interesting

425
00:17:49,039 --> 00:17:53,039
technical problems. Um, if you're

426
00:17:51,440 --> 00:17:56,039
interested in those, please chat with me

427
00:17:53,039 --> 00:17:56,039
later.

