1
00:00:05,000 --> 00:00:10,559
okay so maybe we can get started hello

2
00:00:08,120 --> 00:00:13,759
everyone and thank you for joining us

3
00:00:10,559 --> 00:00:16,080
this uh late afternoon on behalf of the

4
00:00:13,759 --> 00:00:18,600
MIT swartzman College of computing I

5
00:00:16,080 --> 00:00:21,519
would like to welcome you to this

6
00:00:18,600 --> 00:00:24,920
event um which is part of the Circ

7
00:00:21,519 --> 00:00:27,199
Workshop series now dealing with cir

8
00:00:24,920 --> 00:00:29,560
social and ethical responsibilities of

9
00:00:27,199 --> 00:00:32,520
computing uh is a core mission of the

10
00:00:29,560 --> 00:00:35,239
College of Computing here at MIT and

11
00:00:32,520 --> 00:00:37,480
with these events our goal is to bring

12
00:00:35,239 --> 00:00:40,719
the broader MIT and scientific

13
00:00:37,480 --> 00:00:44,079
communities together to engage in

14
00:00:40,719 --> 00:00:48,160
discussions uh on the responsible safe

15
00:00:44,079 --> 00:00:50,879
and ethical use of AI and Computing now

16
00:00:48,160 --> 00:00:53,640
with that in mind uh we are delighted to

17
00:00:50,879 --> 00:00:56,280
have with us today uh an industry and

18
00:00:53,640 --> 00:00:59,559
thought leader in this area Eric

19
00:00:56,280 --> 00:01:03,239
horvit and among the very long list of

20
00:00:59,559 --> 00:01:06,799
things that um Eric does he serves as

21
00:01:03,239 --> 00:01:08,759
Chief scientific officer at Microsoft

22
00:01:06,799 --> 00:01:10,840
and at the president's Council of

23
00:01:08,759 --> 00:01:13,360
advisors on Science and

24
00:01:10,840 --> 00:01:15,600
Technology Eric is also a distinguished

25
00:01:13,360 --> 00:01:18,000
scientist he has been at the Helm of the

26
00:01:15,600 --> 00:01:20,720
research Enterprise at Microsoft for a

27
00:01:18,000 --> 00:01:24,799
few years now has served as director of

28
00:01:20,720 --> 00:01:28,000
Microsoft research Labs Eric also leads

29
00:01:24,799 --> 00:01:30,000
efforts on the responsible use of AI not

30
00:01:28,000 --> 00:01:32,759
just within Microsoft through the The

31
00:01:30,000 --> 00:01:35,560
Ether committee uh but also industrywide

32
00:01:32,759 --> 00:01:38,520
through his involvement with various

33
00:01:35,560 --> 00:01:41,079
organizations including partnership on

34
00:01:38,520 --> 00:01:44,200
AI we're also very fortunate to have

35
00:01:41,079 --> 00:01:46,479
with us today the inaugural dean of MIT

36
00:01:44,200 --> 00:01:49,680
swartzman College of computing Dan

37
00:01:46,479 --> 00:01:52,960
Hatten Locker Dan is going to moderate

38
00:01:49,680 --> 00:01:56,360
and engage um in today's fire side chat

39
00:01:52,960 --> 00:02:00,439
with uh with Eric now before I pass on

40
00:01:56,360 --> 00:02:03,159
the Baton to to Dan um I want to extend

41
00:02:00,439 --> 00:02:05,960
a very big thank you to Corey Harris for

42
00:02:03,159 --> 00:02:08,560
helping organizing this event and Terry

43
00:02:05,960 --> 00:02:09,840
and Eric and everyone else who helped

44
00:02:08,560 --> 00:02:11,840
couple of

45
00:02:09,840 --> 00:02:16,160
logistics uh we're going to have the

46
00:02:11,840 --> 00:02:19,800
fireside chat uh followed by a

47
00:02:16,160 --> 00:02:22,080
Q&A uh please use the QR code and we are

48
00:02:19,800 --> 00:02:24,959
a little bit uh trying to fix the

49
00:02:22,080 --> 00:02:27,000
transitions here but if you uh follow

50
00:02:24,959 --> 00:02:28,879
this QR code you should be able to

51
00:02:27,000 --> 00:02:30,440
submit your questions throughout and

52
00:02:28,879 --> 00:02:33,400
then Dan is going to move

53
00:02:30,440 --> 00:02:35,560
moderate uh and second after the event

54
00:02:33,400 --> 00:02:37,120
we're going to have a reception at stata

55
00:02:35,560 --> 00:02:40,080
Center across the

56
00:02:37,120 --> 00:02:42,239
street um so please join us for that as

57
00:02:40,080 --> 00:02:44,800
well okay so without further delay

58
00:02:42,239 --> 00:02:46,280
please join me in welcoming Eric horvit

59
00:02:44,800 --> 00:02:49,480
and Dan hutten

60
00:02:46,280 --> 00:02:49,480
loer thank

61
00:02:51,200 --> 00:02:56,159
you ER Eric it's great to have you here

62
00:02:54,400 --> 00:02:58,560
I should say that some of the questions

63
00:02:56,159 --> 00:03:00,959
that I have here are also audience

64
00:02:58,560 --> 00:03:03,200
questions because many of you submitted

65
00:03:00,959 --> 00:03:04,519
questions online and Eric and I were

66
00:03:03,200 --> 00:03:07,400
sort of going through some of those

67
00:03:04,519 --> 00:03:09,080
questions uh beforehand and we decided

68
00:03:07,400 --> 00:03:11,480
that we might start calling on people in

69
00:03:09,080 --> 00:03:14,680
the audience also so we might run this a

70
00:03:11,480 --> 00:03:17,000
little bit like a like a MBA class

71
00:03:14,680 --> 00:03:18,560
where where everybody participates in

72
00:03:17,000 --> 00:03:21,879
the answers and not just the questions

73
00:03:18,560 --> 00:03:24,879
but but Eric has uh you know tremendous

74
00:03:21,879 --> 00:03:27,920
uh experience and leadership as Nico

75
00:03:24,879 --> 00:03:30,599
said but but um but really beyond that

76
00:03:27,920 --> 00:03:33,840
he's one of the few people I think of

77
00:03:30,599 --> 00:03:37,200
who has combined his own deep

78
00:03:33,840 --> 00:03:39,760
fundamental research in this area uh his

79
00:03:37,200 --> 00:03:43,159
research leadership leading research

80
00:03:39,760 --> 00:03:46,120
organizations his leadership of one of

81
00:03:43,159 --> 00:03:48,319
the Great American companies Microsoft

82
00:03:46,120 --> 00:03:51,200
and his incredible government service

83
00:03:48,319 --> 00:03:54,280
and in fact um Maria Zuber is is on

84
00:03:51,200 --> 00:03:57,720
pcast with him and she keeps me on my

85
00:03:54,280 --> 00:03:59,360
toes yeah and she she but she also says

86
00:03:57,720 --> 00:04:01,720
I can't believe how much work he does on

87
00:03:59,360 --> 00:04:01,720
P

88
00:04:02,360 --> 00:04:05,879
she says she says it wouldn't be

89
00:04:03,760 --> 00:04:08,879
anything without you Eric

90
00:04:05,879 --> 00:04:10,640
so um so so let me let me start with a

91
00:04:08,879 --> 00:04:13,000
couple High Lev things and we'll go on

92
00:04:10,640 --> 00:04:15,680
and uh uh and feel free to turn

93
00:04:13,000 --> 00:04:18,479
questions back at the rest of us but uh

94
00:04:15,680 --> 00:04:20,000
but in what ways do you think Ai and you

95
00:04:18,479 --> 00:04:22,400
know particularly this sort of machine

96
00:04:20,000 --> 00:04:23,880
learning uh instantiation of AI since

97
00:04:22,400 --> 00:04:26,400
you and I have both at this been at this

98
00:04:23,880 --> 00:04:29,880
game long enough to remember various

99
00:04:26,400 --> 00:04:32,120
other iterations of AI uh is likely to

100
00:04:29,880 --> 00:04:34,160
change our society and and and in

101
00:04:32,120 --> 00:04:36,320
particular what are impacts you think

102
00:04:34,160 --> 00:04:38,120
that are not appreciated enough right

103
00:04:36,320 --> 00:04:40,160
now or that might be being

104
00:04:38,120 --> 00:04:42,080
overlooked um and you know what should

105
00:04:40,160 --> 00:04:43,600
we be excited about and what should be

106
00:04:42,080 --> 00:04:45,360
should we be worried about in those

107
00:04:43,600 --> 00:04:48,919
things that might be being overlooked

108
00:04:45,360 --> 00:04:52,960
well what a big question

109
00:04:48,919 --> 00:04:55,080
so I think we're in truly uh a

110
00:04:52,960 --> 00:04:59,440
remarkable time I think we all know that

111
00:04:55,080 --> 00:05:00,840
deep down uh one of my Reflections is

112
00:04:59,440 --> 00:05:05,520
that

113
00:05:00,840 --> 00:05:08,360
500 years from now people will look back

114
00:05:05,520 --> 00:05:10,680
and the next 25 years from now will be a

115
00:05:08,360 --> 00:05:11,720
recognized period of time that will

116
00:05:10,680 --> 00:05:14,320
probably have a

117
00:05:11,720 --> 00:05:16,800
name uh because of the transformation

118
00:05:14,320 --> 00:05:20,680
that will be coming from computation in

119
00:05:16,800 --> 00:05:24,319
particular um technologies that uh read

120
00:05:20,680 --> 00:05:27,240
on uh Reflections actions inventions

121
00:05:24,319 --> 00:05:30,199
that have been monopolized to date by

122
00:05:27,240 --> 00:05:32,720
human cognition as as machine start

123
00:05:30,199 --> 00:05:34,600
playing a a significant role uh

124
00:05:32,720 --> 00:05:38,000
collaboratively and

125
00:05:34,600 --> 00:05:39,479
autonomously uh what does it all mean uh

126
00:05:38,000 --> 00:05:41,280
What will what will the history books

127
00:05:39,479 --> 00:05:44,720
show when when you open up that chapter

128
00:05:41,280 --> 00:05:48,880
and look at what happened

129
00:05:44,720 --> 00:05:51,120
um first uh I can guarantee to this

130
00:05:48,880 --> 00:05:55,199
audience that there will be surprises we

131
00:05:51,120 --> 00:05:58,759
can't predict uh and that to me is

132
00:05:55,199 --> 00:06:00,520
mostly positively exciting uh of course

133
00:05:58,759 --> 00:06:03,520
we have to be concerned about rough

134
00:06:00,520 --> 00:06:06,520
edges um I think some of the surprises

135
00:06:03,520 --> 00:06:10,880
will come in areas we now recognize as

136
00:06:06,520 --> 00:06:13,039
being likely areas for big impact like

137
00:06:10,880 --> 00:06:16,199
like in the biological sciences in

138
00:06:13,039 --> 00:06:18,360
healthc Care Medicine Material Science

139
00:06:16,199 --> 00:06:20,400
the the shape of skyline is changing

140
00:06:18,360 --> 00:06:23,479
because of how we do design architecture

141
00:06:20,400 --> 00:06:25,680
materials uh looking at it how New York

142
00:06:23,479 --> 00:06:30,520
City skyline or Boston will change over

143
00:06:25,680 --> 00:06:32,919
35 years 40 years um even though we know

144
00:06:30,520 --> 00:06:35,080
this these technologies will have great

145
00:06:32,919 --> 00:06:38,000
influence in those spaces we will be

146
00:06:35,080 --> 00:06:40,160
surprised by the by the by the the way

147
00:06:38,000 --> 00:06:43,000
and the extent of the

148
00:06:40,160 --> 00:06:45,080
influences in areas that I think we

149
00:06:43,000 --> 00:06:47,160
don't think deeply enough about or

150
00:06:45,080 --> 00:06:50,120
expect because it's hard I

151
00:06:47,160 --> 00:06:52,080
think uh it's an unfair question what

152
00:06:50,120 --> 00:06:54,080
overlooked what yeah what comes to mind

153
00:06:52,080 --> 00:06:57,039
for me is what I call the category in my

154
00:06:54,080 --> 00:06:59,680
mind that I the word I use is deeper

155
00:06:57,039 --> 00:07:03,160
currents like deeper currents of the

156
00:06:59,680 --> 00:07:05,319
influence of Technology on society um

157
00:07:03,160 --> 00:07:07,680
what comes of human

158
00:07:05,319 --> 00:07:11,160
dignity human

159
00:07:07,680 --> 00:07:12,000
agency uh how it feels for a human being

160
00:07:11,160 --> 00:07:16,039
to

161
00:07:12,000 --> 00:07:19,400
create um to celebrate their own

162
00:07:16,039 --> 00:07:20,680
intellect uh when machines um are doing

163
00:07:19,400 --> 00:07:22,400
this I don't know in the last couple

164
00:07:20,680 --> 00:07:24,160
weeks how many people played with that

165
00:07:22,400 --> 00:07:25,879
uh I know what the application was

166
00:07:24,160 --> 00:07:29,080
called that M new music app that went

167
00:07:25,879 --> 00:07:31,080
viral on on the web you know people were

168
00:07:29,080 --> 00:07:33,240
writing uh you know show tunes to their

169
00:07:31,080 --> 00:07:36,560
spouses complete orchestration and

170
00:07:33,240 --> 00:07:39,520
chorus with a with a simple prompt you

171
00:07:36,560 --> 00:07:42,800
know it made me think and it brought to

172
00:07:39,520 --> 00:07:45,400
light a this a prediction I made in

173
00:07:42,800 --> 00:07:48,159
2018 uh that was an AI magazine AI

174
00:07:45,400 --> 00:07:49,560
magazine ran a prediction Market article

175
00:07:48,159 --> 00:07:52,840
and I was asked for some predictions and

176
00:07:49,560 --> 00:07:54,520
I made a bunch uh most of which people

177
00:07:52,840 --> 00:07:56,039
said oh that's not going to happen you

178
00:07:54,520 --> 00:08:00,280
had to give a prediction by a certain

179
00:07:56,039 --> 00:08:03,680
date and I said oh by by 20

180
00:08:00,280 --> 00:08:06,759
35 AI will be so ubiquitous in how it

181
00:08:03,680 --> 00:08:09,360
does Artistry art music literature that

182
00:08:06,759 --> 00:08:13,800
human beings will certify that their

183
00:08:09,360 --> 00:08:15,319
creation was human soul authored and

184
00:08:13,800 --> 00:08:17,240
they'll get more money and payment for

185
00:08:15,319 --> 00:08:19,039
that like on the wall in a home people

186
00:08:17,240 --> 00:08:21,720
will

187
00:08:19,039 --> 00:08:25,159
say that was done by a human being this

188
00:08:21,720 --> 00:08:27,919
is an original horbal no AI right even

189
00:08:25,159 --> 00:08:29,840
even a label and a stamp now what but

190
00:08:27,919 --> 00:08:31,800
again I mean what what does it all mean

191
00:08:29,840 --> 00:08:33,200
for how we feel about ourselves as

192
00:08:31,800 --> 00:08:37,719
people now there was an article that

193
00:08:33,200 --> 00:08:40,440
came out um maybe three years ago now

194
00:08:37,719 --> 00:08:44,519
that said it was in in the Journal of

195
00:08:40,440 --> 00:08:46,720
radiology that said uh fears about the

196
00:08:44,519 --> 00:08:48,399
this was before the current ERA you know

197
00:08:46,720 --> 00:08:51,240
like different different realm three

198
00:08:48,399 --> 00:08:53,839
years ago it's amazing to say that right

199
00:08:51,240 --> 00:08:55,480
um that in the realm of radiology

200
00:08:53,839 --> 00:08:58,600
there's so much fear among medical

201
00:08:55,480 --> 00:09:01,680
students that are graduating um about

202
00:08:58,600 --> 00:09:04,040
about Radiology being overtaken by AI

203
00:09:01,680 --> 00:09:06,640
systems and therefore being devalued as

204
00:09:04,040 --> 00:09:09,040
a specialty that it's changing decisions

205
00:09:06,640 --> 00:09:11,839
about residency programs as's a study of

206
00:09:09,040 --> 00:09:16,079
this what does it mean as AI becomes

207
00:09:11,839 --> 00:09:18,040
more obviously um Central in taking on

208
00:09:16,079 --> 00:09:20,360
key tasks that were celebrated or

209
00:09:18,040 --> 00:09:22,200
enjoyed in other in other areas so I

210
00:09:20,360 --> 00:09:26,040
think it's that's one area I like to

211
00:09:22,200 --> 00:09:28,440
keep my my my eyes on and then other

212
00:09:26,040 --> 00:09:30,600
areas come to mind like how education

213
00:09:28,440 --> 00:09:31,920
might change um what does it all mean

214
00:09:30,600 --> 00:09:32,959
and I'll mention one one little story

215
00:09:31,920 --> 00:09:35,000
here which I thought was very

216
00:09:32,959 --> 00:09:37,640
interesting which gives you a sense for

217
00:09:35,000 --> 00:09:40,200
like maybe the tangent about what we are

218
00:09:37,640 --> 00:09:43,360
right now I was at a a yoga retreat with

219
00:09:40,200 --> 00:09:46,680
my wife she's a yogini uh and uh at

220
00:09:43,360 --> 00:09:48,440
esselon and at the at the uh at the at

221
00:09:46,680 --> 00:09:51,279
the one of the breakfasts I was talking

222
00:09:48,440 --> 00:09:52,360
to a to a a um a middle school teacher

223
00:09:51,279 --> 00:09:55,480
from San

224
00:09:52,360 --> 00:09:57,240
Francisco and AI came up and I said so

225
00:09:55,480 --> 00:09:58,959
aren't you can I mean what's going on in

226
00:09:57,240 --> 00:10:01,279
your classrooms and so on she goes oh no

227
00:09:58,959 --> 00:10:02,760
it's not a big deal she's a literature

228
00:10:01,279 --> 00:10:05,440
literary Arts which I guess is the new

229
00:10:02,760 --> 00:10:06,560
phrase for English an English teacher

230
00:10:05,440 --> 00:10:08,720
and she said well you know it's okay I

231
00:10:06,560 --> 00:10:11,000
just tell the kids in the class like for

232
00:10:08,720 --> 00:10:13,040
this what assignment you could Google it

233
00:10:11,000 --> 00:10:15,279
for that one you can AI it and they and

234
00:10:13,040 --> 00:10:17,240
they know what this means so so so for

235
00:10:15,279 --> 00:10:19,079
now maybe there are some packaging and

236
00:10:17,240 --> 00:10:20,920
tools but what does it all mean in the

237
00:10:19,079 --> 00:10:22,440
future is going to be a big question I

238
00:10:20,920 --> 00:10:26,440
think

239
00:10:22,440 --> 00:10:28,720
so the words agency and dignity I think

240
00:10:26,440 --> 00:10:29,880
are which are almost the first words out

241
00:10:28,720 --> 00:10:32,040
of your mouth

242
00:10:29,880 --> 00:10:34,880
uh or at least to the first one they

243
00:10:32,040 --> 00:10:37,320
were early I'd love love to just dig in

244
00:10:34,880 --> 00:10:38,920
a little further there because I think

245
00:10:37,320 --> 00:10:41,600
you know

246
00:10:38,920 --> 00:10:44,320
we're we think about sort of you know

247
00:10:41,600 --> 00:10:46,519
kind of immediate possible risks and

248
00:10:44,320 --> 00:10:49,760
potential harms of AI and we focus on

249
00:10:46,519 --> 00:10:51,440
things like uh you know bias and

250
00:10:49,760 --> 00:10:54,839
algorithms in an AI which are very

251
00:10:51,440 --> 00:10:56,519
important but but they're they're quite

252
00:10:54,839 --> 00:10:58,720
localized and when you think about

253
00:10:56,519 --> 00:11:00,560
something like a term like dignity or

254
00:10:58,720 --> 00:11:02,200
human age

255
00:11:00,560 --> 00:11:04,399
it's such a

256
00:11:02,200 --> 00:11:05,959
general PHR like it's hard to even

257
00:11:04,399 --> 00:11:07,639
characterize what like we all have some

258
00:11:05,959 --> 00:11:09,600
sense of what it means to have human

259
00:11:07,639 --> 00:11:12,800
dignity or human agency but we can't

260
00:11:09,600 --> 00:11:15,360
Define it like you know this algorithm

261
00:11:12,800 --> 00:11:17,720
isn't you know exhibiting racial bias in

262
00:11:15,360 --> 00:11:19,079
hiring or in recommending Medical

263
00:11:17,720 --> 00:11:23,639
Treatments how do you think we're going

264
00:11:19,079 --> 00:11:25,880
to start to develop these more precise

265
00:11:23,639 --> 00:11:28,760
characterizations of what

266
00:11:25,880 --> 00:11:31,600
constitutes the things that AI should be

267
00:11:28,760 --> 00:11:33,320
enhancing in in agency and dignity

268
00:11:31,600 --> 00:11:35,959
instead of depriving us

269
00:11:33,320 --> 00:11:37,560
of let me get to that in my next couple

270
00:11:35,959 --> 00:11:42,760
sentences but let me first say given

271
00:11:37,560 --> 00:11:45,240
your use of those terms just now um my

272
00:11:42,760 --> 00:11:48,720
my sense that

273
00:11:45,240 --> 00:11:52,240
um we have to really think deeply about

274
00:11:48,720 --> 00:11:55,680
about how important it is to people

275
00:11:52,240 --> 00:11:58,240
today uh because we don't really

276
00:11:55,680 --> 00:12:00,240
understand what it's like to not be at

277
00:11:58,240 --> 00:12:02,200
the top of the pyramid in when it comes

278
00:12:00,240 --> 00:12:05,240
to intellect we celebrate that about

279
00:12:02,200 --> 00:12:08,800
Humanity about ourselves it's unclear

280
00:12:05,240 --> 00:12:12,040
how much we need to have that be true to

281
00:12:08,800 --> 00:12:14,440
have a sense of wonder and um to Value

282
00:12:12,040 --> 00:12:17,600
human life way we do I hope it's

283
00:12:14,440 --> 00:12:19,360
irrelevant but it could become an

284
00:12:17,600 --> 00:12:22,360
influence

285
00:12:19,360 --> 00:12:26,079
um

286
00:12:22,360 --> 00:12:27,360
uh and I'll say one more thing and in

287
00:12:26,079 --> 00:12:32,040
the rise of

288
00:12:27,360 --> 00:12:34,560
automation my optimistic side says is it

289
00:12:32,040 --> 00:12:37,399
possible in a sea of

290
00:12:34,560 --> 00:12:40,519
AI that will'll come to celebrate and

291
00:12:37,399 --> 00:12:43,480
value even more deeply human touch human

292
00:12:40,519 --> 00:12:46,680
caring you know I was talking to a uh to

293
00:12:43,480 --> 00:12:52,079
a actually a well-known um psychologist

294
00:12:46,680 --> 00:12:55,000
in the Cambridge area uh about the um

295
00:12:52,079 --> 00:12:56,800
excitement about automated therapists of

296
00:12:55,000 --> 00:12:58,720
the future oh they'll be less expensive

297
00:12:56,800 --> 00:13:00,560
and they'll be available you can kick

298
00:12:58,720 --> 00:13:02,839
back and talk to a therapist whenever

299
00:13:00,560 --> 00:13:05,760
it's uh you know be automated you know

300
00:13:02,839 --> 00:13:07,639
how much let's say you have a and this

301
00:13:05,760 --> 00:13:11,040
is a kind of an interesting assessment

302
00:13:07,639 --> 00:13:13,279
you have access to a fabulous proven

303
00:13:11,040 --> 00:13:15,880
automated therapist who's like the best

304
00:13:13,279 --> 00:13:18,040
in the world the system at handling

305
00:13:15,880 --> 00:13:20,519
depression how important is it that

306
00:13:18,040 --> 00:13:23,320
there's a human being at the other end

307
00:13:20,519 --> 00:13:25,240
that's actually there to listen that's

308
00:13:23,320 --> 00:13:27,880
the question to ask ourselves right now

309
00:13:25,240 --> 00:13:30,199
goes beyond efficacy go the fact that

310
00:13:27,880 --> 00:13:33,199
there's a human being there that will

311
00:13:30,199 --> 00:13:37,399
never go away I think um back to your

312
00:13:33,199 --> 00:13:37,399
question now um agency

313
00:13:38,440 --> 00:13:44,240
um you know a few years ago actually

314
00:13:42,000 --> 00:13:45,440
1999 I wrote a paper some of you may be

315
00:13:44,240 --> 00:13:47,199
familiar with it's one of my higher

316
00:13:45,440 --> 00:13:49,199
cited papers a mixed initiative

317
00:13:47,199 --> 00:13:50,920
interaction and we had principles for

318
00:13:49,199 --> 00:13:54,320
how a human being works with a comp with

319
00:13:50,920 --> 00:13:56,519
an AI system um and they're all very

320
00:13:54,320 --> 00:13:57,880
balanced about when the AI should should

321
00:13:56,519 --> 00:13:59,480
take an action and when the human should

322
00:13:57,880 --> 00:14:01,560
take an action given competence icies

323
00:13:59,480 --> 00:14:03,240
and given the the pace and the flow of

324
00:14:01,560 --> 00:14:05,079
the conversation where it's

325
00:14:03,240 --> 00:14:08,079
conversational around a pro conversation

326
00:14:05,079 --> 00:14:10,759
around a problem solving task and steps

327
00:14:08,079 --> 00:14:12,920
not language but a a problem solving

328
00:14:10,759 --> 00:14:15,800
session and it was all very balanced

329
00:14:12,920 --> 00:14:17,600
back then made a lot of sense I just

330
00:14:15,800 --> 00:14:21,519
wrote a paper with Abby selin who's

331
00:14:17,600 --> 00:14:23,800
director of our Cambridge lab um that's

332
00:14:21,519 --> 00:14:26,000
that's all about celebrating the Primacy

333
00:14:23,800 --> 00:14:30,079
of human agency in our interaction in

334
00:14:26,000 --> 00:14:32,160
our interaction model and some of the

335
00:14:30,079 --> 00:14:34,360
reviewers wouldn't say complain but they

336
00:14:32,160 --> 00:14:37,800
said well why are you changing the tune

337
00:14:34,360 --> 00:14:39,480
from this equal mixed initiative human

338
00:14:37,800 --> 00:14:42,079
machine depending on who can do what

339
00:14:39,480 --> 00:14:44,959
better and so on and we sort of changed

340
00:14:42,079 --> 00:14:47,079
we make wein this is would be in cacm

341
00:14:44,959 --> 00:14:49,320
the summer this piece we refined the

342
00:14:47,079 --> 00:14:52,000
article to call out that with the

343
00:14:49,320 --> 00:14:54,560
ubiquity of AI and the role that it's

344
00:14:52,000 --> 00:14:58,600
playing we need to start thinking deeply

345
00:14:54,560 --> 00:14:59,759
about a decision and a value a value

346
00:14:58,600 --> 00:15:02,519
assertion

347
00:14:59,759 --> 00:15:05,959
that humans are basically on top they're

348
00:15:02,519 --> 00:15:10,240
making the decisions they do the guiding

349
00:15:05,959 --> 00:15:11,680
today in uh I I see David Sant here uh

350
00:15:10,240 --> 00:15:14,920
one of his fabulous students we just did

351
00:15:11,680 --> 00:15:17,279
a PhD thesis I'm here primarily for a um

352
00:15:14,920 --> 00:15:19,279
PhD committee today and celebrating

353
00:15:17,279 --> 00:15:23,839
Hussein's passing and he did he did a

354
00:15:19,279 --> 00:15:25,800
fabulous job uh but um we talked in the

355
00:15:23,839 --> 00:15:29,399
in the in one of the sessions after the

356
00:15:25,800 --> 00:15:30,480
after the after the uh defense you know

357
00:15:29,399 --> 00:15:33,560
about

358
00:15:30,480 --> 00:15:36,800
clarifying the different areas of study

359
00:15:33,560 --> 00:15:39,600
there's AI assisted human decision

360
00:15:36,800 --> 00:15:40,800
making and human assisted AI

361
00:15:39,600 --> 00:15:42,160
decision-making and there's very

362
00:15:40,800 --> 00:15:44,079
different perspective you can have on

363
00:15:42,160 --> 00:15:45,360
those two things they both would

364
00:15:44,079 --> 00:15:46,560
probably work well and you might even

365
00:15:45,360 --> 00:15:49,160
say in certain

366
00:15:46,560 --> 00:15:51,160
designs for fail safe and so on maybe

367
00:15:49,160 --> 00:15:52,759
the human gives an input to a automated

368
00:15:51,160 --> 00:15:54,720
AI system and it does that's the best

369
00:15:52,759 --> 00:15:56,759
thing to do but I think more generally

370
00:15:54,720 --> 00:15:59,000
the design principle for the first time

371
00:15:56,759 --> 00:16:01,560
in my life I started thinking about we

372
00:15:59,000 --> 00:16:03,560
needed to sort of think about the

373
00:16:01,560 --> 00:16:05,759
principal agent and I'm using the phrase

374
00:16:03,560 --> 00:16:08,199
in a decision analytic way the principal

375
00:16:05,759 --> 00:16:10,399
agent of decisions decision should be

376
00:16:08,199 --> 00:16:13,639
humans it should be the humans that are

377
00:16:10,399 --> 00:16:15,360
accountable some human if it's a if

378
00:16:13,639 --> 00:16:18,120
people are are worried about lethal

379
00:16:15,360 --> 00:16:20,360
autonomous weapons there's no such thing

380
00:16:18,120 --> 00:16:22,240
as not having a human accountable for

381
00:16:20,360 --> 00:16:24,040
every single thing those weapons do so

382
00:16:22,240 --> 00:16:25,120
that human who pushes that button and

383
00:16:24,040 --> 00:16:27,839
guides those

384
00:16:25,120 --> 00:16:28,959
weapons better know that he or she is

385
00:16:27,839 --> 00:16:31,880
accountable and better understand

386
00:16:28,959 --> 00:16:35,360
understand how that weapon works for

387
00:16:31,880 --> 00:16:37,120
example super it actually reminds me I'm

388
00:16:35,360 --> 00:16:39,920
completely not following my script here

389
00:16:37,120 --> 00:16:41,000
because keeps reminding reminds me of

390
00:16:39,920 --> 00:16:42,040
something we were just talking about

391
00:16:41,000 --> 00:16:43,079
right before this and then I'm going to

392
00:16:42,040 --> 00:16:45,720
go back because there were some

393
00:16:43,079 --> 00:16:48,199
interesting things on the script but uh

394
00:16:45,720 --> 00:16:50,440
just before when we were talking about

395
00:16:48,199 --> 00:16:53,240
uh you know

396
00:16:50,440 --> 00:16:56,399
whether sort of metrics for evaluating

397
00:16:53,240 --> 00:16:59,600
performance and looking at whether the

398
00:16:56,399 --> 00:17:02,480
on a given task an AI system

399
00:16:59,600 --> 00:17:04,760
or a human or a human and an AI system

400
00:17:02,480 --> 00:17:08,480
together what their relative performance

401
00:17:04,760 --> 00:17:12,039
are and that we're sort of in this state

402
00:17:08,480 --> 00:17:13,959
at the moment where because in general

403
00:17:12,039 --> 00:17:17,959
machine learning is optimizing for the

404
00:17:13,959 --> 00:17:19,319
AI alone it's not surprising AI alone

405
00:17:17,959 --> 00:17:22,959
usually does the

406
00:17:19,319 --> 00:17:24,799
best uh how do we move beyond that sort

407
00:17:22,959 --> 00:17:26,600
of world the the example I just gave we

408
00:17:24,799 --> 00:17:29,679
were talking up in in Dan's conference

409
00:17:26,600 --> 00:17:32,559
room was I was just helping with a study

410
00:17:29,679 --> 00:17:35,000
at Stanford of clinicians they did a

411
00:17:32,559 --> 00:17:39,640
very intensive study I mean involving

412
00:17:35,000 --> 00:17:41,240
like you know tens of Physicians uh a

413
00:17:39,640 --> 00:17:43,160
the the control group were just doing

414
00:17:41,240 --> 00:17:45,039
their thing with some case diagnostic

415
00:17:43,160 --> 00:17:47,160
case studies challenging hard problems

416
00:17:45,039 --> 00:17:50,480
in clinical medicine and the the

417
00:17:47,160 --> 00:17:52,120
intervention group were given GPT for to

418
00:17:50,480 --> 00:17:54,720
you know ask questions and to help with

419
00:17:52,120 --> 00:17:59,600
the diagnosis and the bar the bar graph

420
00:17:54,720 --> 00:18:01,799
was like gp4 alone clinicians alone

421
00:17:59,600 --> 00:18:03,760
together and so we all looked at this

422
00:18:01,799 --> 00:18:07,320
and and it was done really well very

423
00:18:03,760 --> 00:18:10,880
careful carefully um and it just

424
00:18:07,320 --> 00:18:12,600
suggests that there's a huge Headroom in

425
00:18:10,880 --> 00:18:15,280
I think Headroom in like maybe three

426
00:18:12,600 --> 00:18:18,039
three three triple on Tandra but there's

427
00:18:15,280 --> 00:18:20,440
huge Headroom in figuring out like how

428
00:18:18,039 --> 00:18:23,799
to design these systems to work well

429
00:18:20,440 --> 00:18:25,039
with people and because you know uh and

430
00:18:23,799 --> 00:18:27,000
I think Usain had a slide about this

431
00:18:25,039 --> 00:18:30,960
today and he's done work in this space

432
00:18:27,000 --> 00:18:32,520
uh our new our almost new PhD going off

433
00:18:30,960 --> 00:18:35,120
into the world that you can you should

434
00:18:32,520 --> 00:18:36,880
get a bigger bar uh out of the

435
00:18:35,120 --> 00:18:38,720
combination and we just haven't been

436
00:18:36,880 --> 00:18:41,720
investing enough in that

437
00:18:38,720 --> 00:18:45,200
yet and how do you think we get focused

438
00:18:41,720 --> 00:18:46,880
more on that investment either wearing a

439
00:18:45,200 --> 00:18:48,840
uh you know an academic hat or a

440
00:18:46,880 --> 00:18:51,960
corporate hat or or you know does

441
00:18:48,840 --> 00:18:54,200
government have a role here or well I

442
00:18:51,960 --> 00:18:56,080
think I think human AI collaboration if

443
00:18:54,200 --> 00:18:59,320
you will call that that call that out of

444
00:18:56,080 --> 00:19:01,480
the field is especially given the

445
00:18:59,320 --> 00:19:05,679
advances in AI is a severely

446
00:19:01,480 --> 00:19:07,919
underinvested area and um look I mean I

447
00:19:05,679 --> 00:19:10,480
I I'm on The Advisory Board also at the

448
00:19:07,919 --> 00:19:13,440
the Hai program Sanford's human centered

449
00:19:10,480 --> 00:19:14,919
Ai and I I've called the map by saying

450
00:19:13,440 --> 00:19:16,559
you know you're human centered AI but I

451
00:19:14,919 --> 00:19:19,200
don't see very many people in your

452
00:19:16,559 --> 00:19:21,840
department working on the technology and

453
00:19:19,200 --> 00:19:22,919
the psychology and the in depth on this

454
00:19:21,840 --> 00:19:25,120
this actual

455
00:19:22,919 --> 00:19:26,799
interaction uh and and so I'd like to

456
00:19:25,120 --> 00:19:28,720
see more of that not just there but here

457
00:19:26,799 --> 00:19:31,640
and everywhere um it's Challen

458
00:19:28,720 --> 00:19:34,440
challenging work and I think in part

459
00:19:31,640 --> 00:19:37,799
because you need to involve in a deep

460
00:19:34,440 --> 00:19:40,960
way cognitive psychologists to know what

461
00:19:37,799 --> 00:19:43,559
the gaps and the capabilities are you

462
00:19:40,960 --> 00:19:46,640
know the biases and blind spots in human

463
00:19:43,559 --> 00:19:48,960
cognition and if you look at the 100

464
00:19:46,640 --> 00:19:50,520
Years of cognitive psychology that's

465
00:19:48,960 --> 00:19:53,360
exactly what what that those groups have

466
00:19:50,520 --> 00:19:55,240
studied um that could be very helpful

467
00:19:53,360 --> 00:19:57,000
then there's the whole design you know

468
00:19:55,240 --> 00:19:59,200
like the the interaction the

469
00:19:57,000 --> 00:20:04,880
conversation with an AI system

470
00:19:59,200 --> 00:20:11,159
um the idea of of a general purpose uh

471
00:20:04,880 --> 00:20:13,280
oneline input uh as the you know choice

472
00:20:11,159 --> 00:20:15,200
or the uh the standard of practice for

473
00:20:13,280 --> 00:20:17,880
talking to large scale language models

474
00:20:15,200 --> 00:20:19,880
well I'm sure I hope a decade from now

475
00:20:17,880 --> 00:20:22,480
seem like an old Franklin stove approach

476
00:20:19,880 --> 00:20:24,600
in the old days we have new kinds of

477
00:20:22,480 --> 00:20:25,880
innovative ideas in fact be to think

478
00:20:24,600 --> 00:20:28,840
about with this group to check back with

479
00:20:25,880 --> 00:20:30,720
this group in 15 years like what's the

480
00:20:28,840 --> 00:20:34,080
human AI

481
00:20:30,720 --> 00:20:35,840
collaborative uh state-ofthe-art and

482
00:20:34,080 --> 00:20:37,200
best practices across fields to see

483
00:20:35,840 --> 00:20:40,120
people doing all sorts of interesting

484
00:20:37,200 --> 00:20:41,120
things with gesture and and engagement

485
00:20:40,120 --> 00:20:43,360
with different different kinds of

486
00:20:41,120 --> 00:20:45,240
modalities would be very interesting we

487
00:20:43,360 --> 00:20:47,520
have to get you to MIT for a slightly

488
00:20:45,240 --> 00:20:50,480
longer time period than this visit

489
00:20:47,520 --> 00:20:55,200
because uh

490
00:20:50,480 --> 00:20:58,760
um I I'll I'll publicly go on record and

491
00:20:55,200 --> 00:21:00,480
say that uh we will be working a lot lot

492
00:20:58,760 --> 00:21:04,400
on human AI collaboration here and we

493
00:21:00,480 --> 00:21:06,360
already are um partly with faculty that

494
00:21:04,400 --> 00:21:08,039
we already have partly with a number of

495
00:21:06,360 --> 00:21:09,159
Faculty we've been hiring including a

496
00:21:08,039 --> 00:21:13,240
co-author of

497
00:21:09,159 --> 00:21:16,039
yours and uh uh and uh I couldn't agree

498
00:21:13,240 --> 00:21:18,919
with you more it's incredibly under and

499
00:21:16,039 --> 00:21:20,440
just to call this out people are hearing

500
00:21:18,919 --> 00:21:22,200
different things when they hear human

501
00:21:20,440 --> 00:21:25,120
collaboration some people are thinking

502
00:21:22,200 --> 00:21:29,400
design some people are thinking you know

503
00:21:25,120 --> 00:21:33,640
uh you know uh psych psychological

504
00:21:29,400 --> 00:21:36,200
advances it's really it's the whole menu

505
00:21:33,640 --> 00:21:39,240
going to core computer science up up to

506
00:21:36,200 --> 00:21:41,679
cognitive psychology uh to human factors

507
00:21:39,240 --> 00:21:43,159
so it's it there's a soft design aspect

508
00:21:41,679 --> 00:21:44,440
to it that takes a lot of interesting

509
00:21:43,159 --> 00:21:47,039
effort and then there's also getting

510
00:21:44,440 --> 00:21:50,000
down pedal to the metal you know methods

511
00:21:47,039 --> 00:21:52,840
for grounding and confirming and and for

512
00:21:50,000 --> 00:21:57,919
the Baton passing which get into really

513
00:21:52,840 --> 00:21:59,360
um you know in some applications um your

514
00:21:57,919 --> 00:22:02,799
millisecond

515
00:21:59,360 --> 00:22:04,600
controls I can give examples of those

516
00:22:02,799 --> 00:22:07,400
things

517
00:22:04,600 --> 00:22:10,600
so you have this very broad view from

518
00:22:07,400 --> 00:22:12,679
the multiple organizations that you're

519
00:22:10,600 --> 00:22:15,720
involved with in you know in government

520
00:22:12,679 --> 00:22:17,240
and Industry consortia at Microsoft um

521
00:22:15,720 --> 00:22:19,760
and I guess in

522
00:22:17,240 --> 00:22:22,840
particular um you know you've been

523
00:22:19,760 --> 00:22:24,880
involved in the responsible AI

524
00:22:22,840 --> 00:22:27,000
initiatives in that office at Microsoft

525
00:22:24,880 --> 00:22:28,760
and in the the safety board the

526
00:22:27,000 --> 00:22:32,120
deployment safety board at Microsoft and

527
00:22:28,760 --> 00:22:35,240
I just you know are there examples of

528
00:22:32,120 --> 00:22:38,240
how uh you know either at Microsoft or

529
00:22:35,240 --> 00:22:41,279
in other organizations you're working uh

530
00:22:38,240 --> 00:22:44,960
uh in these kinds of questions on

531
00:22:41,279 --> 00:22:48,919
about how you frame these considerations

532
00:22:44,960 --> 00:22:50,279
and maybe trying to tie broader safety

533
00:22:48,919 --> 00:22:51,640
considerations because I mean I know

534
00:22:50,279 --> 00:22:54,159
when you look at safety it's not like

535
00:22:51,640 --> 00:22:56,279
just turn it off it's how do we balance

536
00:22:54,159 --> 00:22:58,520
the like so how do you frame these

537
00:22:56,279 --> 00:22:59,960
things and then maybe a specific case or

538
00:22:58,520 --> 00:23:01,360
too so there's kind of the broader

539
00:22:59,960 --> 00:23:03,880
Framing and the

540
00:23:01,360 --> 00:23:08,840
specificity we I ask easy short

541
00:23:03,880 --> 00:23:12,400
questions yeah so in 2016

542
00:23:08,840 --> 00:23:13,960
um just had the sense that we needed to

543
00:23:12,400 --> 00:23:17,480
get going on at

544
00:23:13,960 --> 00:23:19,520
Microsoft uh on stating a set of

545
00:23:17,480 --> 00:23:22,200
principles when it came to systems that

546
00:23:19,520 --> 00:23:24,880
were starting to read in the open world

547
00:23:22,200 --> 00:23:28,039
on in the realm of human intellect human

548
00:23:24,880 --> 00:23:29,640
decision- making um and Brad Smith and I

549
00:23:28,039 --> 00:23:31,919
uh went together formed what was called

550
00:23:29,640 --> 00:23:33,960
The Ether committee um we got

551
00:23:31,919 --> 00:23:36,080
representatives from across Microsoft as

552
00:23:33,960 --> 00:23:39,720
well as experts from Microsoft research

553
00:23:36,080 --> 00:23:41,240
and legal team policy people um and one

554
00:23:39,720 --> 00:23:44,000
of the early things we did was we

555
00:23:41,240 --> 00:23:46,080
defined what what are now called U

556
00:23:44,000 --> 00:23:47,919
Microsoft's AI principles and there are

557
00:23:46,080 --> 00:23:50,039
six of them and they they they they've

558
00:23:47,919 --> 00:23:52,000
been standing the test of time but maybe

559
00:23:50,039 --> 00:23:53,360
even more interesting because there are

560
00:23:52,000 --> 00:23:55,720
principles everywhere there's like you

561
00:23:53,360 --> 00:23:58,159
know 10 principles oecd four principles

562
00:23:55,720 --> 00:24:02,200
U you know and so on and they offer

563
00:23:58,159 --> 00:24:05,360
quite equivalent these all these

564
00:24:02,200 --> 00:24:06,720
embracing uh value statements but one

565
00:24:05,360 --> 00:24:09,159
thing we found interesting we said we

566
00:24:06,720 --> 00:24:11,000
defined we wanted to Define what a

567
00:24:09,159 --> 00:24:13,559
sensitive use of AI was we called it a

568
00:24:11,000 --> 00:24:15,400
sensitive use and we came up with a

569
00:24:13,559 --> 00:24:18,840
definition or a set of attributes that

570
00:24:15,400 --> 00:24:20,400
have also stood the test of time and

571
00:24:18,840 --> 00:24:22,679
I'll just say what they are I mean and

572
00:24:20,400 --> 00:24:25,440
so at Microsoft any AI application has

573
00:24:22,679 --> 00:24:27,120
to be reviewed per policy and compliance

574
00:24:25,440 --> 00:24:28,960
with whether or not it pings on one of

575
00:24:27,120 --> 00:24:32,760
these attributes and you can look at it

576
00:24:28,960 --> 00:24:36,159
as Rings starting with a human being

577
00:24:32,760 --> 00:24:38,399
first Circle does this application that

578
00:24:36,159 --> 00:24:41,799
we're going to ship or create and then

579
00:24:38,399 --> 00:24:43,640
ship uh somehow um put someone at risk

580
00:24:41,799 --> 00:24:45,799
for physical or emotional

581
00:24:43,640 --> 00:24:49,320
harm and I can give you examples of

582
00:24:45,799 --> 00:24:51,120
those including apps that we haven't we

583
00:24:49,320 --> 00:24:54,279
we we we've stopped we halted at

584
00:24:51,120 --> 00:24:56,799
development the second ring is um does

585
00:24:54,279 --> 00:24:58,880
this application put someone at risk for

586
00:24:56,799 --> 00:25:02,799
losing access to a consequ quential

587
00:24:58,880 --> 00:25:04,640
service like health care loans education

588
00:25:02,799 --> 00:25:07,080
and the third ring is does this

589
00:25:04,640 --> 00:25:08,399
application put Society at risk does put

590
00:25:07,080 --> 00:25:13,159
Microsoft at risk for its commitment to

591
00:25:08,399 --> 00:25:14,159
Human Rights civil liberties privacy um

592
00:25:13,159 --> 00:25:18,399
uh and

593
00:25:14,159 --> 00:25:20,000
democracy uh and it's an outer ring um

594
00:25:18,399 --> 00:25:21,760
and we have impact statements that

595
00:25:20,000 --> 00:25:24,640
require an analysis of all our

596
00:25:21,760 --> 00:25:26,320
applications and so on now we first

597
00:25:24,640 --> 00:25:27,679
rolled out this program and said you

598
00:25:26,320 --> 00:25:29,039
know and Sai actually was I always

599
00:25:27,679 --> 00:25:31,480
people say how' you get how' you get it

600
00:25:29,039 --> 00:25:34,440
to work at Microsoft well Sacha nadela

601
00:25:31,480 --> 00:25:35,760
sent the whole company a note about this

602
00:25:34,440 --> 00:25:37,679
we have The Ether committee here's what

603
00:25:35,760 --> 00:25:41,520
we're doing here's the link and so all

604
00:25:37,679 --> 00:25:43,240
of a sudden we we had incoming cases um

605
00:25:41,520 --> 00:25:45,840
I my my first reaction was you know what

606
00:25:43,240 --> 00:25:47,960
we're going to have maybe I don't know

607
00:25:45,840 --> 00:25:50,919
seven to 12 cases and we'll have

608
00:25:47,960 --> 00:25:53,440
precedent and we'll automate well here

609
00:25:50,919 --> 00:25:55,880
we are you know like nine years later

610
00:25:53,440 --> 00:25:59,159
eight years later or so and we're over

611
00:25:55,880 --> 00:26:01,640
600 sensitive uses reviews all very all

612
00:25:59,159 --> 00:26:03,399
different and nuanced and precedent is

613
00:26:01,640 --> 00:26:06,600
still there we can point at other cases

614
00:26:03,399 --> 00:26:09,159
and so on uh and uh and it doesn't seem

615
00:26:06,600 --> 00:26:10,600
to be any any simple way to summarize or

616
00:26:09,159 --> 00:26:13,320
generalize on these

617
00:26:10,600 --> 00:26:15,919
cases U one of the first cases that came

618
00:26:13,320 --> 00:26:18,399
to that I I I have I give a recent case

619
00:26:15,919 --> 00:26:21,360
and an older case one of the first cases

620
00:26:18,399 --> 00:26:25,559
that came to us was uh there was a

621
00:26:21,360 --> 00:26:27,520
project called project aware and this it

622
00:26:25,559 --> 00:26:29,840
was built by our Consulting teams that

623
00:26:27,520 --> 00:26:31,960
build custom tailored apps that run on

624
00:26:29,840 --> 00:26:33,200
Microsoft services and I think the first

625
00:26:31,960 --> 00:26:34,960
one was done for New York City and it's

626
00:26:33,200 --> 00:26:38,600
always kind of a cool system it helps

627
00:26:34,960 --> 00:26:40,080
triage 311 calls and it it it it does

628
00:26:38,600 --> 00:26:42,520
some proactive work about where police

629
00:26:40,080 --> 00:26:45,799
should go you know it helps helps manage

630
00:26:42,520 --> 00:26:49,159
Emergency Services all sounds really

631
00:26:45,799 --> 00:26:50,720
great well it was you can imagine sales

632
00:26:49,159 --> 00:26:52,520
and marketing teams around the world say

633
00:26:50,720 --> 00:26:53,799
we have uh on the menu we also have

634
00:26:52,520 --> 00:26:55,799
project to wear you want to buy this

635
00:26:53,799 --> 00:26:57,559
thing and I'll just won't mention a

636
00:26:55,799 --> 00:26:59,399
government but but but a military

637
00:26:57,559 --> 00:27:02,799
government in the Middle East

638
00:26:59,399 --> 00:27:04,880
said we'll take that uh and it came

639
00:27:02,799 --> 00:27:06,200
through sensitive uses and that was the

640
00:27:04,880 --> 00:27:08,679
first time we had to Think Through okay

641
00:27:06,200 --> 00:27:11,000
what does it mean to take a Microsoft

642
00:27:08,679 --> 00:27:13,200
product and and and think deeply about

643
00:27:11,000 --> 00:27:17,720
who the user is and and who and what

644
00:27:13,200 --> 00:27:19,279
does this mean for the for um the first

645
00:27:17,720 --> 00:27:23,240
second and third Loops of that sensitive

646
00:27:19,279 --> 00:27:26,600
uses uh a definition um and that went

647
00:27:23,240 --> 00:27:28,440
back and forth and in the end uh um we

648
00:27:26,600 --> 00:27:33,200
ended up defining our policies when it

649
00:27:28,440 --> 00:27:35,520
comes to um um who gets access to these

650
00:27:33,200 --> 00:27:38,320
Technologies AI Technologies in

651
00:27:35,520 --> 00:27:40,279
particular um and so we told the

652
00:27:38,320 --> 00:27:43,799
disappointed salesperson who had a huge

653
00:27:40,279 --> 00:27:45,200
contract on his hands no uh they came

654
00:27:43,799 --> 00:27:48,399
back and they said well well how about

655
00:27:45,200 --> 00:27:50,880
for this Museum of Antiquities they just

656
00:27:48,399 --> 00:27:52,120
for the museum and so they we got back

657
00:27:50,880 --> 00:27:54,360
together again and thought about what is

658
00:27:52,120 --> 00:27:57,360
this mean a museum can we keep the

659
00:27:54,360 --> 00:27:58,679
object code in in just one place can we

660
00:27:57,360 --> 00:28:00,399
uh and so we have this beautiful

661
00:27:58,679 --> 00:28:02,480
statement that we've never had before at

662
00:28:00,399 --> 00:28:05,080
the time which said yeah you can have

663
00:28:02,480 --> 00:28:06,919
this system for the museum but and it

664
00:28:05,080 --> 00:28:08,600
was a statement about you cannot it had

665
00:28:06,919 --> 00:28:11,720
statements it had assertions like you

666
00:28:08,600 --> 00:28:14,480
cannot do facial recognition you cannot

667
00:28:11,720 --> 00:28:16,440
detect gender or race you cannot predict

668
00:28:14,480 --> 00:28:19,519
future events with with any kind of

669
00:28:16,440 --> 00:28:21,360
add-on so had a an AI statement about

670
00:28:19,519 --> 00:28:22,640
things you could not do with the system

671
00:28:21,360 --> 00:28:24,840
that was an example of like trying to

672
00:28:22,640 --> 00:28:26,440
Define borders and things have gotten

673
00:28:24,840 --> 00:28:27,760
way sophisticated since then in terms of

674
00:28:26,440 --> 00:28:29,760
how we deal with countries and different

675
00:28:27,760 --> 00:28:33,760
governments and

676
00:28:29,760 --> 00:28:36,120
Technologies another um uh example that

677
00:28:33,760 --> 00:28:40,120
came a little bit later was um when a

678
00:28:36,120 --> 00:28:42,279
large uh Sheriff Department um in

679
00:28:40,120 --> 00:28:45,120
California got excited about facial

680
00:28:42,279 --> 00:28:47,000
recognition this was before the recent

681
00:28:45,120 --> 00:28:50,440
police actions where Microsoft now says

682
00:28:47,000 --> 00:28:53,519
will not even even uh um distribute

683
00:28:50,440 --> 00:28:55,000
facial recognition to to police agencies

684
00:28:53,519 --> 00:28:56,399
and it was really excited the police

685
00:28:55,000 --> 00:28:58,640
department they said we have three use

686
00:28:56,399 --> 00:29:00,799
cases uh stopping some somebody in the

687
00:28:58,640 --> 00:29:05,080
wild you know like a camera on on the

688
00:29:00,799 --> 00:29:07,200
vest uh a traffic pullover we also want

689
00:29:05,080 --> 00:29:08,440
to have it on the way into into into

690
00:29:07,200 --> 00:29:09,919
jail because we don't know who these

691
00:29:08,440 --> 00:29:12,440
people are this would help us ID

692
00:29:09,919 --> 00:29:15,000
somebody who's done who's been observed

693
00:29:12,440 --> 00:29:16,440
performing a crime and the third was in

694
00:29:15,000 --> 00:29:18,399
inside the jails apparently people pass

695
00:29:16,440 --> 00:29:19,640
around these cards for the you know when

696
00:29:18,399 --> 00:29:21,000
they when they go when they when they

697
00:29:19,640 --> 00:29:22,440
buy and sell and they want to you know

698
00:29:21,000 --> 00:29:24,600
people want they want to certify who's

699
00:29:22,440 --> 00:29:26,720
who when they see people using resources

700
00:29:24,600 --> 00:29:29,399
and cards and whatever they do at the

701
00:29:26,720 --> 00:29:30,880
canteen for example and so each case was

702
00:29:29,399 --> 00:29:35,039
examined very carefully by the by The

703
00:29:30,880 --> 00:29:37,960
Ether committee sensitive uses review um

704
00:29:35,039 --> 00:29:40,399
uh very clear set of requirements were

705
00:29:37,960 --> 00:29:41,399
were made including education to the

706
00:29:40,399 --> 00:29:42,760
police department this stuff doesn't

707
00:29:41,399 --> 00:29:45,360
work very well there's a false positive

708
00:29:42,760 --> 00:29:48,200
false negative rate we will work with

709
00:29:45,360 --> 00:29:49,760
you in fact we will be willing to first

710
00:29:48,200 --> 00:29:52,279
of all it was absolutely no for open

711
00:29:49,760 --> 00:29:54,039
world case for the second case in the

712
00:29:52,279 --> 00:29:55,480
third case it was like okay listen

713
00:29:54,039 --> 00:29:58,799
here's what we have to do community

714
00:29:55,480 --> 00:30:00,480
engagement uh reporting every six months

715
00:29:58,799 --> 00:30:05,240
uh impact of false positives and false

716
00:30:00,480 --> 00:30:07,679
negatives tracking um uh Community uh

717
00:30:05,240 --> 00:30:09,760
you know hotline for complaints that we

718
00:30:07,679 --> 00:30:10,840
requireed before we chip the technology

719
00:30:09,760 --> 00:30:11,880
and the police department is very

720
00:30:10,840 --> 00:30:14,080
interesting the police the sheriff's

721
00:30:11,880 --> 00:30:16,200
department said we've reviewed the

722
00:30:14,080 --> 00:30:18,120
technology we look at what you're saying

723
00:30:16,200 --> 00:30:21,200
and what you've asked us to do we've

724
00:30:18,120 --> 00:30:23,880
decided this technology is not for us

725
00:30:21,200 --> 00:30:25,440
it's not ready to be used uh just by

726
00:30:23,880 --> 00:30:28,320
educating them by the process we went

727
00:30:25,440 --> 00:30:31,440
through as an example

728
00:30:28,320 --> 00:30:33,080
uh um just as you were saying that I was

729
00:30:31,440 --> 00:30:36,159
thinking about you know the widespread

730
00:30:33,080 --> 00:30:38,159
use of fingerprinting technology in the

731
00:30:36,159 --> 00:30:40,440
criminal justice system and it has all

732
00:30:38,159 --> 00:30:42,480
of those same problems even though it

733
00:30:40,440 --> 00:30:44,200
you know it's not an AI technology so

734
00:30:42,480 --> 00:30:45,840
it's not really getting reviewed and if

735
00:30:44,200 --> 00:30:47,919
you and if you look at the what and we

736
00:30:45,840 --> 00:30:50,399
can get to this later but the European

737
00:30:47,919 --> 00:30:52,200
the EU AI act now will forbid certain

738
00:30:50,399 --> 00:30:56,919
biometric uses of

739
00:30:52,200 --> 00:30:59,880
AI so one of the you know uh when people

740
00:30:56,919 --> 00:31:01,399
signed up uh they many of them there was

741
00:30:59,880 --> 00:31:03,840
a spot for them to put a little question

742
00:31:01,399 --> 00:31:06,279
in one of those questions so this is

743
00:31:03,840 --> 00:31:08,080
pre- audience questions uh one of those

744
00:31:06,279 --> 00:31:10,200
questions was very relevant to this

745
00:31:08,080 --> 00:31:13,120
because it said you know Microsoft seems

746
00:31:10,200 --> 00:31:17,120
to be taking a thoughtful approach to

747
00:31:13,120 --> 00:31:18,000
responsible uh uses of AI uh compared to

748
00:31:17,120 --> 00:31:21,320
many

749
00:31:18,000 --> 00:31:23,519
other uh companies out there um many

750
00:31:21,320 --> 00:31:28,039
other actors

751
00:31:23,519 --> 00:31:30,880
uh how do we go about trying to we know

752
00:31:28,039 --> 00:31:33,279
more broadly socialize these kinds of

753
00:31:30,880 --> 00:31:36,360
approaches you know is it something that

754
00:31:33,279 --> 00:31:38,399
you know you're seeing being able to

755
00:31:36,360 --> 00:31:40,679
happen just with sort of you know

756
00:31:38,399 --> 00:31:43,279
industry consortia and collaborations

757
00:31:40,679 --> 00:31:46,039
and Industry groups is it a place for

758
00:31:43,279 --> 00:31:48,720
governments to be uh getting involved

759
00:31:46,039 --> 00:31:54,159
how do you how do you think about how we

760
00:31:48,720 --> 00:31:56,440
you know take best practices um and uh

761
00:31:54,159 --> 00:31:58,639
you know yours or others and and and

762
00:31:56,440 --> 00:32:01,200
really make sure that they're getting as

763
00:31:58,639 --> 00:32:02,960
broadly applied as they can sometimes I

764
00:32:01,200 --> 00:32:06,279
think about where we are today as kind

765
00:32:02,960 --> 00:32:10,000
of being akin to you know if anybody's

766
00:32:06,279 --> 00:32:12,120
ever visited the the original Con Edison

767
00:32:10,000 --> 00:32:15,919
Consolidated Edison electric power plant

768
00:32:12,120 --> 00:32:17,120
in Manhattan that Tom Edison built to to

769
00:32:15,919 --> 00:32:18,399
basically light up the light bulbs he

770
00:32:17,120 --> 00:32:19,600
was selling he like well I have

771
00:32:18,399 --> 00:32:21,320
generators here I'm going to sell you

772
00:32:19,600 --> 00:32:23,840
light bulbs and it's going to be a

773
00:32:21,320 --> 00:32:26,200
service and I often think well was there

774
00:32:23,840 --> 00:32:28,399
a government regulatory body dealing

775
00:32:26,200 --> 00:32:31,039
with like you know ground

776
00:32:28,399 --> 00:32:33,960
hazards and electrocution and there was

777
00:32:31,039 --> 00:32:38,919
no UL laborat there was no Laboratories

778
00:32:33,960 --> 00:32:41,559
to do standards and so I'm hoping that

779
00:32:38,919 --> 00:32:43,880
the need for companies and I can talk

780
00:32:41,559 --> 00:32:47,480
about our motivations like Microsoft to

781
00:32:43,880 --> 00:32:50,279
do this intensive work and reflection

782
00:32:47,480 --> 00:32:52,840
about what do we build and what what

783
00:32:50,279 --> 00:32:55,720
what are the benefits and harms will

784
00:32:52,840 --> 00:32:57,600
pass to society more generally and we

785
00:32:55,720 --> 00:33:00,279
won't rely on companies to do this kind

786
00:32:57,600 --> 00:33:03,320
of thing is I think it's transitional I

787
00:33:00,279 --> 00:33:05,519
think we're we have we don't really have

788
00:33:03,320 --> 00:33:07,639
regulations just yet we have this

789
00:33:05,519 --> 00:33:09,840
incredible executive order from

790
00:33:07,639 --> 00:33:12,080
President Biden and the administration

791
00:33:09,840 --> 00:33:14,200
and ostp that and other agencies that

792
00:33:12,080 --> 00:33:15,480
came together with report out starting

793
00:33:14,200 --> 00:33:18,519
very soon I think on Monday we'll be

794
00:33:15,480 --> 00:33:22,480
hearing a few uh 120 days I guess that

795
00:33:18,519 --> 00:33:26,159
was um and um but I think we'll be in a

796
00:33:22,480 --> 00:33:30,039
world almost by necessity within a few

797
00:33:26,159 --> 00:33:32,799
years where societies who have come to

798
00:33:30,039 --> 00:33:35,320
especially democracies where that people

799
00:33:32,799 --> 00:33:38,080
have spoken and leaders will take take a

800
00:33:35,320 --> 00:33:40,039
take a a point of view where regulations

801
00:33:38,080 --> 00:33:42,240
will come to hold in best practices and

802
00:33:40,039 --> 00:33:44,919
it won't just be you know we won't rely

803
00:33:42,240 --> 00:33:46,880
on industry but instead will enforce and

804
00:33:44,919 --> 00:33:49,720
require industry to do certain

805
00:33:46,880 --> 00:33:51,320
things when you talked about Edison and

806
00:33:49,720 --> 00:33:52,639
light bulbs it reminds me of an example

807
00:33:51,320 --> 00:33:55,440
that some of us have been using who are

808
00:33:52,639 --> 00:33:56,880
working on AI policy issues which we've

809
00:33:55,440 --> 00:33:59,039
come to refer to as the fork and a

810
00:33:56,880 --> 00:34:01,440
toaster problem

811
00:33:59,039 --> 00:34:04,440
uh which is you know today we have very

812
00:34:01,440 --> 00:34:06,559
good Norms about uh you know when you're

813
00:34:04,440 --> 00:34:08,079
misusing a device like a toaster and if

814
00:34:06,559 --> 00:34:11,399
you get electrocuted it's probably your

815
00:34:08,079 --> 00:34:14,040
fault versus uh but if if you go back

816
00:34:11,399 --> 00:34:15,879
and look at in the 1920s the ear the

817
00:34:14,040 --> 00:34:18,520
first electric

818
00:34:15,879 --> 00:34:20,119
toasters they they look like they were

819
00:34:18,520 --> 00:34:23,240
designed to electrocute

820
00:34:20,119 --> 00:34:26,639
you well it it's interesting and and so

821
00:34:23,240 --> 00:34:28,280
as the the the EU AI act so the Norms

822
00:34:26,639 --> 00:34:30,320
you know the industry sort of develop

823
00:34:28,280 --> 00:34:32,240
Norms then regul regulation came in

824
00:34:30,320 --> 00:34:35,240
exactly following but if you look at the

825
00:34:32,240 --> 00:34:37,839
a AI act it was interesting to see um

826
00:34:35,240 --> 00:34:39,240
Europeans who's responsible comes up and

827
00:34:37,839 --> 00:34:41,440
and that created lots of questions as it

828
00:34:39,240 --> 00:34:43,159
was coming to as it was yelling there

829
00:34:41,440 --> 00:34:45,079
was like you know the there was they

830
00:34:43,159 --> 00:34:46,919
called out roles of the general provider

831
00:34:45,079 --> 00:34:49,079
of the services of the general AI

832
00:34:46,919 --> 00:34:51,240
Services then the application Builder on

833
00:34:49,079 --> 00:34:53,240
top then the consumer and the industry

834
00:34:51,240 --> 00:34:55,079
shipping these things and like they they

835
00:34:53,240 --> 00:34:57,079
kind of call out like who's in you know

836
00:34:55,079 --> 00:35:00,240
who's on the hot seat for something that

837
00:34:57,079 --> 00:35:00,240
goes wrong at any of those

838
00:35:16,359 --> 00:35:20,480
levels I agree

839
00:35:36,359 --> 00:35:44,200
so so this is whyu or should I try to

840
00:35:40,119 --> 00:35:47,000
okay good yeah no so so just to react

841
00:35:44,200 --> 00:35:49,400
um this is one reason I feel

842
00:35:47,000 --> 00:35:51,680
particularly passionate about the

843
00:35:49,400 --> 00:35:54,280
responsibility of people in this room

844
00:35:51,680 --> 00:35:57,960
technologists academics people who know

845
00:35:54,280 --> 00:35:59,560
the technology IND desists who are have

846
00:35:57,960 --> 00:36:03,359
their fingers in the technology who get

847
00:35:59,560 --> 00:36:05,079
a sense for where things are going for

848
00:36:03,359 --> 00:36:07,599
their role both to Think Through

849
00:36:05,079 --> 00:36:10,440
Solutions technically soci technically

850
00:36:07,599 --> 00:36:14,079
and even policy and communicate them out

851
00:36:10,440 --> 00:36:17,160
to others uh and this is why i' I've

852
00:36:14,079 --> 00:36:19,440
been excited about Microsoft's um policy

853
00:36:17,160 --> 00:36:22,240
teams in DC for example that have really

854
00:36:19,440 --> 00:36:25,520
weighed in with with with uh

855
00:36:22,240 --> 00:36:28,720
Congressional staffers and um other

856
00:36:25,520 --> 00:36:30,720
agencies you know in ining waston State

857
00:36:28,720 --> 00:36:33,680
um when it came to facial

858
00:36:30,720 --> 00:36:35,560
recognition uh we didn't want to compete

859
00:36:33,680 --> 00:36:37,440
to the bottom with our competitors

860
00:36:35,560 --> 00:36:40,760
wanted to sort of raise the bar so we

861
00:36:37,440 --> 00:36:42,480
worked directly with uh staffers to help

862
00:36:40,760 --> 00:36:44,400
with Washington State facial recognition

863
00:36:42,480 --> 00:36:46,440
legislation we're very proud of that

864
00:36:44,400 --> 00:36:48,760
that we got you know sort of it's

865
00:36:46,440 --> 00:36:50,480
communicated out and then and so on in

866
00:36:48,760 --> 00:36:52,560
other areas things are so are moving so

867
00:36:50,480 --> 00:36:54,520
fast so we all know and we're all

868
00:36:52,560 --> 00:36:56,400
worried I think in this room what's

869
00:36:54,520 --> 00:36:58,440
going to happen with democracy and

870
00:36:56,400 --> 00:37:00,920
elections even coming in the next 18

871
00:36:58,440 --> 00:37:02,800
months I think a a billion people are

872
00:37:00,920 --> 00:37:04,760
voting in the next 18 months on the

873
00:37:02,800 --> 00:37:06,839
planet and just look at what look at

874
00:37:04,760 --> 00:37:09,240
what's going on I get reports daily from

875
00:37:06,839 --> 00:37:13,839
our internal uh security analyses around

876
00:37:09,240 --> 00:37:16,839
the world and it's amazing how fast the

877
00:37:13,839 --> 00:37:19,240
latest uh synthetic uh media

878
00:37:16,839 --> 00:37:23,200
Technologies are being used by you in

879
00:37:19,240 --> 00:37:26,240
adversarial ways to to impersonate uh

880
00:37:23,200 --> 00:37:27,960
and to persuade um if anybody saw the

881
00:37:26,240 --> 00:37:30,200
latest coming out of mic Microsoft

882
00:37:27,960 --> 00:37:31,960
research you know it just put a chill up

883
00:37:30,200 --> 00:37:35,000
my down my from our from our from our

884
00:37:31,960 --> 00:37:37,880
from our China lab is incredible I mean

885
00:37:35,000 --> 00:37:41,839
to take a a still picture uh and to

886
00:37:37,880 --> 00:37:45,359
completely synthesize a just a beautiful

887
00:37:41,839 --> 00:37:47,119
uh um rendition of a fluid conversation

888
00:37:45,359 --> 00:37:49,599
a person talking where the voice is

889
00:37:47,119 --> 00:37:52,560
synthetic the face is synthetic and even

890
00:37:49,599 --> 00:37:55,440
even the content is synthetic uh means

891
00:37:52,560 --> 00:37:58,079
we have to do something so folks have

892
00:37:55,440 --> 00:38:00,119
taken dramatic efforts our team

893
00:37:58,079 --> 00:38:02,240
sat together and said we're creating

894
00:38:00,119 --> 00:38:04,319
something called media Providence and

895
00:38:02,240 --> 00:38:06,240
we're going to use cryptography and show

896
00:38:04,319 --> 00:38:07,920
how that works with manifests we're

897
00:38:06,240 --> 00:38:09,480
going to go to the BBC and talk to them

898
00:38:07,920 --> 00:38:11,359
and get them started this thing called

899
00:38:09,480 --> 00:38:14,480
project origin with the New York Times

900
00:38:11,359 --> 00:38:16,680
we helped to co-found ctpa the Coalition

901
00:38:14,480 --> 00:38:18,440
for Content Providence and authenticity

902
00:38:16,680 --> 00:38:20,240
and then thinking through what are the

903
00:38:18,440 --> 00:38:22,800
soot technical issues with having this

904
00:38:20,240 --> 00:38:25,400
stamp of of authenticity will people

905
00:38:22,800 --> 00:38:27,079
trust it will they distrust it and

906
00:38:25,400 --> 00:38:28,800
understanding all that comes along with

907
00:38:27,079 --> 00:38:31,000
it so this is these are initiatives

908
00:38:28,800 --> 00:38:34,319
going on by people who just happen to be

909
00:38:31,000 --> 00:38:36,599
in the know and see what's coming down

910
00:38:34,319 --> 00:38:40,240
the bike I mean let's say synthetic

911
00:38:36,599 --> 00:38:41,720
media deep fakes I saw my first demo at

912
00:38:40,240 --> 00:38:45,160
Stanford in

913
00:38:41,720 --> 00:38:46,640
2016 it was a cvpr paper people were

914
00:38:45,160 --> 00:38:48,920
really proud they said watch I I can

915
00:38:46,640 --> 00:38:51,040
make Donald Trump he was he was just a

916
00:38:48,920 --> 00:38:53,079
candidate back then make his mouth move

917
00:38:51,040 --> 00:38:55,240
in certain ways and make him say things

918
00:38:53,079 --> 00:38:57,440
and the same with Obama at the time and

919
00:38:55,240 --> 00:39:00,280
it was it was fun and it was a demo and

920
00:38:57,440 --> 00:39:01,960
it was a it got into cvpr uh great it

921
00:39:00,280 --> 00:39:03,440
was a science fair project I gave a

922
00:39:01,960 --> 00:39:07,040
South by Southwest talk and I showed

923
00:39:03,440 --> 00:39:10,440
this and I said remember this moment as

924
00:39:07,040 --> 00:39:12,359
2017 as South by Southwest here we are

925
00:39:10,440 --> 00:39:14,960
today it's it's common parlance you know

926
00:39:12,359 --> 00:39:16,880
a deep fake and it's they're persuasive

927
00:39:14,960 --> 00:39:20,440
and it's it's a problem we need to get

928
00:39:16,880 --> 00:39:22,920
ahead of it somehow and by the way

929
00:39:20,440 --> 00:39:25,839
staffers in Congress are talking about

930
00:39:22,920 --> 00:39:27,800
legislation now not just requiring

931
00:39:25,839 --> 00:39:30,119
certain kinds of stamps of auth

932
00:39:27,800 --> 00:39:33,000
cryptographically signed but making it a

933
00:39:30,119 --> 00:39:37,280
penalty to remove them a penalize

934
00:39:33,000 --> 00:39:39,000
offense so we probably need all of that

935
00:39:37,280 --> 00:39:41,880
while respecting civil

936
00:39:39,000 --> 00:39:45,240
liberties an interesting

937
00:39:41,880 --> 00:39:46,839
balance I wanted to come back to this um

938
00:39:45,240 --> 00:39:49,079
point you were making

939
00:39:46,839 --> 00:39:51,000
about sort of the different levels of

940
00:39:49,079 --> 00:39:53,280
responsibility kind of you know a

941
00:39:51,000 --> 00:39:54,440
foundational model provider application

942
00:39:53,280 --> 00:39:57,520
provider

943
00:39:54,440 --> 00:39:59,200
Etc um

944
00:39:57,520 --> 00:40:03,960
I was

945
00:39:59,200 --> 00:40:07,119
uh POS positively surprised when uh

946
00:40:03,960 --> 00:40:09,560
pretty much all of the uh large

947
00:40:07,119 --> 00:40:11,520
providers of llms decided that they

948
00:40:09,560 --> 00:40:16,119
needed to

949
00:40:11,520 --> 00:40:18,319
indemnify their paying customers

950
00:40:16,119 --> 00:40:21,560
against copyright

951
00:40:18,319 --> 00:40:24,240
infringement uh both claims and

952
00:40:21,560 --> 00:40:25,760
judgments uh positively surprised in the

953
00:40:24,240 --> 00:40:27,680
sense that just you know when we think

954
00:40:25,760 --> 00:40:30,640
about the software industry and its

955
00:40:27,680 --> 00:40:33,480
history it tends to be caveat empor

956
00:40:30,640 --> 00:40:36,040
right you know there's a long end user

957
00:40:33,480 --> 00:40:37,720
license agreement that pretty much says

958
00:40:36,040 --> 00:40:40,000
you're the human you're responsible

959
00:40:37,720 --> 00:40:42,720
essentially back to some earlier

960
00:40:40,000 --> 00:40:45,839
conversation uh so it was you know I

961
00:40:42,720 --> 00:40:48,680
think different do you

962
00:40:45,839 --> 00:40:50,599
think as we start to think about sort of

963
00:40:48,680 --> 00:40:52,280
responsibilities of layers that's

964
00:40:50,599 --> 00:40:54,560
essentially a layer dayi thing right I

965
00:40:52,280 --> 00:40:56,240
mean it's it's like there's a provider

966
00:40:54,560 --> 00:40:58,240
but the end user is going to use that

967
00:40:56,240 --> 00:41:00,480
work product and some way and they don't

968
00:40:58,240 --> 00:41:01,880
really know the provence and so if you

969
00:41:00,480 --> 00:41:03,760
didn't take responsibility you probably

970
00:41:01,880 --> 00:41:05,560
would have any customers it's clear the

971
00:41:03,760 --> 00:41:07,400
Dynamics are different in some of those

972
00:41:05,560 --> 00:41:09,880
things than conventional

973
00:41:07,400 --> 00:41:13,680
software how do you think about how

974
00:41:09,880 --> 00:41:18,280
those things might uh might pan out over

975
00:41:13,680 --> 00:41:19,720
time it's interesting Dan I I have um a

976
00:41:18,280 --> 00:41:22,040
general response to your general

977
00:41:19,720 --> 00:41:26,839
question but also on the specific case

978
00:41:22,040 --> 00:41:28,960
this this whole area of attribution and

979
00:41:26,839 --> 00:41:31,760
uh generation of people who provide

980
00:41:28,960 --> 00:41:35,280
content or artists who have a specific

981
00:41:31,760 --> 00:41:37,960
style that you know worked most of their

982
00:41:35,280 --> 00:41:39,839
life in a world where they didn't uh

983
00:41:37,960 --> 00:41:42,520
have a fear that their style would be

984
00:41:39,839 --> 00:41:44,800
scooped up and then regenerated uh

985
00:41:42,520 --> 00:41:48,720
without their name on it um these are

986
00:41:44,800 --> 00:41:50,440
really interesting questions I just um

987
00:41:48,720 --> 00:41:53,599
spent time on a National Academy of

988
00:41:50,440 --> 00:41:55,480
Science committee uh that met at

989
00:41:53,599 --> 00:41:58,280
sunnylands uh and there'll be an article

990
00:41:55,480 --> 00:42:01,319
coming out in pnas

991
00:41:58,280 --> 00:42:04,560
entitled uh protecting scientific

992
00:42:01,319 --> 00:42:06,480
Integrity in the era of generative Ai

993
00:42:04,560 --> 00:42:09,280
and part of the what's called for is

994
00:42:06,480 --> 00:42:10,920
like how do we deal with attribution a

995
00:42:09,280 --> 00:42:12,480
scientist is using a system doesn't know

996
00:42:10,920 --> 00:42:14,400
where ideas are even coming from as

997
00:42:12,480 --> 00:42:15,960
they're being mixed for example so it's

998
00:42:14,400 --> 00:42:17,720
even more General than copyright it's

999
00:42:15,960 --> 00:42:20,200
like you know how do we back to human

1000
00:42:17,720 --> 00:42:22,480
agency and human dignity if you know how

1001
00:42:20,200 --> 00:42:24,599
do we we reward folks for their

1002
00:42:22,480 --> 00:42:27,240
creativity if we even if if the in the

1003
00:42:24,599 --> 00:42:30,480
mill of an llm

1004
00:42:27,240 --> 00:42:35,280
all identity is lost as things are

1005
00:42:30,480 --> 00:42:38,119
created and provided to somebody else

1006
00:42:35,280 --> 00:42:42,880
um and it's been interesting to see

1007
00:42:38,119 --> 00:42:46,559
companies uh buying up now in in in the

1008
00:42:42,880 --> 00:42:50,640
frame of lawsuits digital rights for

1009
00:42:46,559 --> 00:42:52,640
corporate x for use in model y uh and

1010
00:42:50,640 --> 00:42:55,400
price is going up all of a sudden

1011
00:42:52,640 --> 00:42:58,240
because oh I didn't realize that I could

1012
00:42:55,400 --> 00:43:00,240
charge so much for my big medical

1013
00:42:58,240 --> 00:43:03,119
journal over the last 30 years of

1014
00:43:00,240 --> 00:43:04,839
content that's great let's negotiate now

1015
00:43:03,119 --> 00:43:06,720
so that's happening what does it mean

1016
00:43:04,839 --> 00:43:09,440
more generally

1017
00:43:06,720 --> 00:43:12,599
um you know I think because of the way

1018
00:43:09,440 --> 00:43:15,839
current models work um where we haven't

1019
00:43:12,599 --> 00:43:17,839
really cracked attribution yet besides

1020
00:43:15,839 --> 00:43:20,000
kind of back tracing with reference

1021
00:43:17,839 --> 00:43:23,119
creation and trying our best to put

1022
00:43:20,000 --> 00:43:25,240
links and so on that's one solution sort

1023
00:43:23,119 --> 00:43:27,280
of retrieval augmentation and those

1024
00:43:25,240 --> 00:43:30,240
kinds of those kinds of approaches yep

1025
00:43:27,280 --> 00:43:31,720
and matching uh hashing and so on I I

1026
00:43:30,240 --> 00:43:34,680
think that we're going to be in a world

1027
00:43:31,720 --> 00:43:37,359
where companies who want to sell the

1028
00:43:34,680 --> 00:43:41,839
service will have to be indentifying end

1029
00:43:37,359 --> 00:43:43,480
users and I like the idea of that even

1030
00:43:41,839 --> 00:43:45,960
becoming a costly thing to do because

1031
00:43:43,480 --> 00:43:48,240
it'll Force Innovation and even Force

1032
00:43:45,960 --> 00:43:50,440
policy

1033
00:43:48,240 --> 00:43:53,000
eventually like I said it was a positive

1034
00:43:50,440 --> 00:43:54,520
surprise to me so it sounds like maybe

1035
00:43:53,000 --> 00:43:57,200
not a surprise to you because you were

1036
00:43:54,520 --> 00:44:00,880
closer to it but uh but but a similar

1037
00:43:57,200 --> 00:44:04,160
kind of kind of view there um all right

1038
00:44:00,880 --> 00:44:08,160
so now uh

1039
00:44:04,160 --> 00:44:10,440
um we covered a lot of things here uh

1040
00:44:08,160 --> 00:44:12,160
kind of out of order um all right so I'm

1041
00:44:10,440 --> 00:44:17,680
going to go to some of the things that

1042
00:44:12,160 --> 00:44:21,800
kind of came in in advance um and uh so

1043
00:44:17,680 --> 00:44:24,400
uh um as the chief scientific officer at

1044
00:44:21,800 --> 00:44:27,440
Microsoft how do you find a balance

1045
00:44:24,400 --> 00:44:30,079
between sort of constant Improvement the

1046
00:44:27,440 --> 00:44:32,240
industry is sort of rushing forward at a

1047
00:44:30,079 --> 00:44:36,640
at a rapid P Pace you a lot of

1048
00:44:32,240 --> 00:44:39,400
competitors out there um or or Coop

1049
00:44:36,640 --> 00:44:42,440
petitions with you you have both

1050
00:44:39,400 --> 00:44:44,359
competitions and Coop petitions uh you

1051
00:44:42,440 --> 00:44:47,800
know how how do you find that balance

1052
00:44:44,359 --> 00:44:50,119
between an industry rushing forward and

1053
00:44:47,800 --> 00:44:52,480
maintaining the highest possible social

1054
00:44:50,119 --> 00:44:54,960
and ethical standards in your own

1055
00:44:52,480 --> 00:44:58,520
company but also encouraging that as

1056
00:44:54,960 --> 00:45:00,559
part of how the industry does things

1057
00:44:58,520 --> 00:45:03,440
well there's definitely a tension there

1058
00:45:00,559 --> 00:45:05,839
um some people might know notice that we

1059
00:45:03,440 --> 00:45:09,760
shipped something called open model

1060
00:45:05,839 --> 00:45:13,400
called 53 mini this week um that was

1061
00:45:09,760 --> 00:45:14,960
held up because of safety concerns uh

1062
00:45:13,400 --> 00:45:17,200
and it was sent back by the deployment

1063
00:45:14,960 --> 00:45:20,760
safety board like back to the lab you

1064
00:45:17,200 --> 00:45:22,040
can't ship uh on the earlier date um

1065
00:45:20,760 --> 00:45:23,599
even though it was considered a

1066
00:45:22,040 --> 00:45:25,559
competitive thing want to get it into

1067
00:45:23,599 --> 00:45:27,640
hugging face get it into the hands of of

1068
00:45:25,559 --> 00:45:29,480
end users

1069
00:45:27,640 --> 00:45:31,200
because it hadn't passed some metrics on

1070
00:45:29,480 --> 00:45:33,440
safety we have a certain set of

1071
00:45:31,200 --> 00:45:36,240
requirements and a bar to be reached

1072
00:45:33,440 --> 00:45:38,599
happy to say that it did but but it was

1073
00:45:36,240 --> 00:45:40,319
wasn't going to go forward until it did

1074
00:45:38,599 --> 00:45:43,720
the the there's

1075
00:45:40,319 --> 00:45:45,800
um there's a commitment uh called the

1076
00:45:43,720 --> 00:45:48,440
responsible AI standard of

1077
00:45:45,800 --> 00:45:52,880
Microsoft that

1078
00:45:48,440 --> 00:45:55,359
um is is uh in some ways was overseen by

1079
00:45:52,880 --> 00:45:57,119
the board of directors of the company uh

1080
00:45:55,359 --> 00:46:00,920
for the an audit committee that reported

1081
00:45:57,119 --> 00:46:02,480
to them so in some ways

1082
00:46:00,920 --> 00:46:04,960
uh

1083
00:46:02,480 --> 00:46:06,760
the the the promise and commitment is

1084
00:46:04,960 --> 00:46:10,000
firm about what Microsoft does when it

1085
00:46:06,760 --> 00:46:11,800
comes to its standards that said you can

1086
00:46:10,000 --> 00:46:14,119
imagine there's lots of gray areas and

1087
00:46:11,800 --> 00:46:15,640
there's pushing and tugging going on and

1088
00:46:14,119 --> 00:46:17,480
one reaction to this is Can Can

1089
00:46:15,640 --> 00:46:20,119
Microsoft stay ahead on

1090
00:46:17,480 --> 00:46:22,040
the where it's advancing the technology

1091
00:46:20,119 --> 00:46:24,240
as fast as anybody else but also is

1092
00:46:22,040 --> 00:46:26,400
putting enough investments into the

1093
00:46:24,240 --> 00:46:28,319
responsible AI aspects so that it can

1094
00:46:26,400 --> 00:46:30,599
either

1095
00:46:28,319 --> 00:46:33,079
uh uh learn something and warn the

1096
00:46:30,599 --> 00:46:35,640
planet warn governments warn uh

1097
00:46:33,079 --> 00:46:38,200
colleagues or come up with the right

1098
00:46:35,640 --> 00:46:40,480
kind of constraints uh guard rails

1099
00:46:38,200 --> 00:46:42,920
filters and so

1100
00:46:40,480 --> 00:46:45,280
on yeah well it's

1101
00:46:42,920 --> 00:46:47,640
amazing the scale at which you all have

1102
00:46:45,280 --> 00:46:51,280
been able to do that so far I think that

1103
00:46:47,640 --> 00:46:52,520
uh um so there are a number of questions

1104
00:46:51,280 --> 00:46:53,960
that came in beforehand and then I'll

1105
00:46:52,520 --> 00:46:57,040
start going to some of the ones that are

1106
00:46:53,960 --> 00:46:59,520
coming in uh now uh there are number

1107
00:46:57,040 --> 00:47:02,680
questions before that were uh something

1108
00:46:59,520 --> 00:47:04,200
along the lines of you know what's the

1109
00:47:02,680 --> 00:47:08,319
path to

1110
00:47:04,200 --> 00:47:12,520
AGI uh you know are llms the path to AGI

1111
00:47:08,319 --> 00:47:14,720
or um what do you think about first of

1112
00:47:12,520 --> 00:47:18,559
all what AGI

1113
00:47:14,720 --> 00:47:21,920
is and is is it something that you know

1114
00:47:18,559 --> 00:47:24,680
is a little bit like uh you know

1115
00:47:21,920 --> 00:47:27,559
um in our lifetimes Quantum Computing

1116
00:47:24,680 --> 00:47:29,000
has always been somewhere between the

1117
00:47:27,559 --> 00:47:30,319
the the time period is getting shorter

1118
00:47:29,000 --> 00:47:32,839
I'll give it that but it's always

1119
00:47:30,319 --> 00:47:35,359
somewhere on the horizon right uh well

1120
00:47:32,839 --> 00:47:38,280
the the the old joke uh in AI used to be

1121
00:47:35,359 --> 00:47:41,839
through most of my career was AI stands

1122
00:47:38,280 --> 00:47:43,520
for almost implemented uh and things are

1123
00:47:41,839 --> 00:47:46,040
changing now and that on that front I

1124
00:47:43,520 --> 00:47:47,400
think now with AI is an interesting I've

1125
00:47:46,040 --> 00:47:48,800
had an interesting relationship to that

1126
00:47:47,400 --> 00:47:51,280
concept over the years I think probably

1127
00:47:48,800 --> 00:47:53,400
we all have you know mainstream AI

1128
00:47:51,280 --> 00:47:54,720
researchers always looked a scance at

1129
00:47:53,400 --> 00:47:57,680
that term and go what does that mean I

1130
00:47:54,720 --> 00:48:00,400
mean AI for us was AGI we were looking

1131
00:47:57,680 --> 00:48:03,240
for general principles I think that the

1132
00:48:00,400 --> 00:48:05,720
terminology came from largely at least

1133
00:48:03,240 --> 00:48:08,800
as amplification from outside mainstream

1134
00:48:05,720 --> 00:48:11,640
AI research in that people were

1135
00:48:08,800 --> 00:48:13,880
seeing um actual great advances that

1136
00:48:11,640 --> 00:48:15,680
were in narrow domains you know it's

1137
00:48:13,880 --> 00:48:17,240
like you know like from the point of

1138
00:48:15,680 --> 00:48:19,960
view of AI it's like first things first

1139
00:48:17,240 --> 00:48:23,240
AI research but it's like no no no we

1140
00:48:19,960 --> 00:48:24,920
don't mean that we want like humanlike

1141
00:48:23,240 --> 00:48:26,839
intelligence and then some people have

1142
00:48:24,920 --> 00:48:28,680
to find it like Shane leg as you know

1143
00:48:26,839 --> 00:48:32,760
doing anything a human can do I think is

1144
00:48:28,680 --> 00:48:34,359
a is a short definition of that um so

1145
00:48:32,760 --> 00:48:36,960
stepping

1146
00:48:34,359 --> 00:48:38,599
back you know I came into this field I I

1147
00:48:36,960 --> 00:48:40,520
came to Stanford as

1148
00:48:38,599 --> 00:48:43,240
mdphd in

1149
00:48:40,520 --> 00:48:45,119
neuroscience and my because my primarily

1150
00:48:43,240 --> 00:48:47,079
my primary motivation has been like what

1151
00:48:45,119 --> 00:48:48,760
the heck is going on with neurons and

1152
00:48:47,079 --> 00:48:51,760
vertebrate brains I said I'm just blown

1153
00:48:48,760 --> 00:48:54,319
away I understand physics to my level of

1154
00:48:51,760 --> 00:48:57,559
curiosity and maybe more you know

1155
00:48:54,319 --> 00:48:59,480
biology chemistry but when I went into a

1156
00:48:57,559 --> 00:49:03,240
into I spent a two years in a in a

1157
00:48:59,480 --> 00:49:05,400
undergr as a biophysicist biophysics lab

1158
00:49:03,240 --> 00:49:07,200
as my major with biophysics playing with

1159
00:49:05,400 --> 00:49:09,040
rat sticking rat neurons with little

1160
00:49:07,200 --> 00:49:10,839
electrodes as an undergrad I was

1161
00:49:09,040 --> 00:49:12,359
completely mystified and so I said I'm

1162
00:49:10,839 --> 00:49:14,480
going to study that when I go to grad

1163
00:49:12,359 --> 00:49:16,200
school and my first three or four months

1164
00:49:14,480 --> 00:49:17,799
at Stanford I said you know what I'm

1165
00:49:16,200 --> 00:49:19,480
going to shift to AI because there's

1166
00:49:17,799 --> 00:49:21,920
like no way we're going to make progress

1167
00:49:19,480 --> 00:49:24,040
with sticking single neurons or even

1168
00:49:21,920 --> 00:49:27,000
sets of neurons for a thousand

1169
00:49:24,040 --> 00:49:30,480
years um and I had hoped that we get

1170
00:49:27,000 --> 00:49:33,000
some insights about how brains work um

1171
00:49:30,480 --> 00:49:34,720
and I got very excited about probis

1172
00:49:33,000 --> 00:49:36,280
graphical models and understanding every

1173
00:49:34,720 --> 00:49:37,960
Arc and Link in these models and

1174
00:49:36,280 --> 00:49:40,760
understanding the mathematics and what

1175
00:49:37,960 --> 00:49:43,319
basine updating is and how to apply it

1176
00:49:40,760 --> 00:49:45,640
in exciting applications but I never

1177
00:49:43,319 --> 00:49:48,040
really had the sense that we were

1178
00:49:45,640 --> 00:49:49,079
getting anywhere near what vertebrate

1179
00:49:48,040 --> 00:49:52,599
Minds were

1180
00:49:49,079 --> 00:49:56,880
doing and I have to say and I might be

1181
00:49:52,599 --> 00:49:59,520
completely off here but my conjecture is

1182
00:49:56,880 --> 00:50:01,720
whether it's close or not to what what

1183
00:49:59,520 --> 00:50:04,559
biology has discovered with human

1184
00:50:01,720 --> 00:50:07,280
nervous systems the magic we're seeing

1185
00:50:04,559 --> 00:50:11,160
and we still can't explain and we have

1186
00:50:07,280 --> 00:50:13,680
the most you know seasoned AI

1187
00:50:11,160 --> 00:50:14,920
researchers completely stunned by what

1188
00:50:13,680 --> 00:50:18,680
these large scale language models are

1189
00:50:14,920 --> 00:50:21,040
doing is something there will read on

1190
00:50:18,680 --> 00:50:23,520
human intellect and how vertebrate

1191
00:50:21,040 --> 00:50:24,720
brains work it's unclear what yet but

1192
00:50:23,520 --> 00:50:27,440
the power of

1193
00:50:24,720 --> 00:50:29,440
composition synthesis AB raction

1194
00:50:27,440 --> 00:50:31,880
generalization that seems to be emerging

1195
00:50:29,440 --> 00:50:35,040
in these systems might have some

1196
00:50:31,880 --> 00:50:37,119
relevance is it the pathway forward no

1197
00:50:35,040 --> 00:50:39,880
but I think it'll frame questions and

1198
00:50:37,119 --> 00:50:42,880
and and and provide answers that might

1199
00:50:39,880 --> 00:50:45,400
lead us to The Next Step so do is that a

1200
00:50:42,880 --> 00:50:48,839
piece of this next 25

1201
00:50:45,400 --> 00:50:50,520
years will have a name I think like what

1202
00:50:48,839 --> 00:50:52,200
when when I heard you say the next 25

1203
00:50:50,520 --> 00:50:55,079
years will have a name looking back I

1204
00:50:52,200 --> 00:50:57,160
was thinking at the time you said it in

1205
00:50:55,079 --> 00:51:00,040
the conversation that was about the

1206
00:50:57,160 --> 00:51:02,599
kinds of impacts on you know sort of our

1207
00:51:00,040 --> 00:51:04,280
daily lives but this is more a potential

1208
00:51:02,599 --> 00:51:05,680
sort of intellectual and human

1209
00:51:04,280 --> 00:51:08,240
understanding impact do you think that's

1210
00:51:05,680 --> 00:51:10,160
also an important part of this era we're

1211
00:51:08,240 --> 00:51:12,839
in or do you think maybe it'll be

1212
00:51:10,160 --> 00:51:15,200
outside the 25 year time frame or can I

1213
00:51:12,839 --> 00:51:19,720
say for the sake of this video

1214
00:51:15,200 --> 00:51:22,160
that for me that's an expected

1215
00:51:19,720 --> 00:51:25,480
surprise so people will be able to come

1216
00:51:22,160 --> 00:51:28,880
back and test you on it that's good uh

1217
00:51:25,480 --> 00:51:31,559
so um back to psychiatrists here okay so

1218
00:51:28,880 --> 00:51:34,839
there's a a

1219
00:51:31,559 --> 00:51:38,119
um question here about how Joe Eisen bam

1220
00:51:34,839 --> 00:51:39,520
argued around 1970 I can guess a few

1221
00:51:38,119 --> 00:51:42,200
people in here who might be writing

1222
00:51:39,520 --> 00:51:43,799
about Joe in 1970 uh that a super smart

1223
00:51:42,200 --> 00:51:45,280
dolphin would not be acceptable as a

1224
00:51:43,799 --> 00:51:47,920
psychiatrist could would not have had

1225
00:51:45,280 --> 00:51:50,319
comparable experiences to its

1226
00:51:47,920 --> 00:51:55,079
clients so when you were sort of talking

1227
00:51:50,319 --> 00:51:57,319
about the AI psychiatrist and you know

1228
00:51:55,079 --> 00:52:00,040
does there need to be a human in the

1229
00:51:57,319 --> 00:52:02,440
room was that sort of

1230
00:52:00,040 --> 00:52:05,000
human you know sort of Flesh and

1231
00:52:02,440 --> 00:52:07,240
embodiment was it Human Experience is it

1232
00:52:05,000 --> 00:52:10,920
both how do you think well when I used

1233
00:52:07,240 --> 00:52:14,280
that term earlier I was trying to Define

1234
00:52:10,920 --> 00:52:17,079
a simple notion of being listened to by

1235
00:52:14,280 --> 00:52:19,040
an actual mind who understands and

1236
00:52:17,079 --> 00:52:20,920
understands what it means to be a human

1237
00:52:19,040 --> 00:52:22,400
directly that is a form of experience

1238
00:52:20,920 --> 00:52:24,960
form of experience but it like actually

1239
00:52:22,400 --> 00:52:26,599
someone who's there who's home versus

1240
00:52:24,960 --> 00:52:29,720
potentially an algorithm that doesn't

1241
00:52:26,599 --> 00:52:34,599
have that kind of of sensient or or

1242
00:52:29,720 --> 00:52:34,599
maybe human form of sensient

1243
00:52:35,359 --> 00:52:39,440
um I

1244
00:52:37,599 --> 00:52:42,960
I

1245
00:52:39,440 --> 00:52:44,640
I'm amazed at and I know this is

1246
00:52:42,960 --> 00:52:47,040
controversial right now among people

1247
00:52:44,640 --> 00:52:50,319
studying this um but I've done Bunch

1248
00:52:47,040 --> 00:52:53,839
myself with a team I think the theory of

1249
00:52:50,319 --> 00:52:56,599
Mind capabilities of even the current

1250
00:52:53,839 --> 00:52:59,440
largest large scale language models are

1251
00:52:56,599 --> 00:53:01,440
just mind-blowing the ability of these

1252
00:52:59,440 --> 00:53:03,359
systems to have a good sense for what

1253
00:53:01,440 --> 00:53:05,960
people are thinking about others at

1254
00:53:03,359 --> 00:53:08,680
least how they they express it in

1255
00:53:05,960 --> 00:53:11,200
language um the ability for example I

1256
00:53:08,680 --> 00:53:12,760
mean I and this was was an example I I

1257
00:53:11,200 --> 00:53:16,000
actually created for my colleagues at

1258
00:53:12,760 --> 00:53:18,280
ostp say well give me an example of how

1259
00:53:16,000 --> 00:53:21,079
these back when it was kind of a newish

1260
00:53:18,280 --> 00:53:24,040
thing dpt4 and I created an up thology

1261
00:53:21,079 --> 00:53:26,599
case um an actual you know sort of

1262
00:53:24,040 --> 00:53:29,240
anonymized case but at the end I said

1263
00:53:26,599 --> 00:53:31,640
what are the patient's questions now

1264
00:53:29,240 --> 00:53:35,200
that she received this diagnosis and the

1265
00:53:31,640 --> 00:53:37,839
system is brilliant at writing down um

1266
00:53:35,200 --> 00:53:41,319
the questions that a patient would ask

1267
00:53:37,839 --> 00:53:41,319
so in some ways when it comes to

1268
00:53:41,480 --> 00:53:46,079
psychology don't ask me how but through

1269
00:53:44,640 --> 00:53:49,559
sifting through and learning from

1270
00:53:46,079 --> 00:53:52,359
corpora of fiction and non-fiction large

1271
00:53:49,559 --> 00:53:54,760
scale Corp this the idea of the systems

1272
00:53:52,359 --> 00:53:57,040
are actually learning a remarkable

1273
00:53:54,760 --> 00:53:58,640
amount it seems of psychology

1274
00:53:57,040 --> 00:54:00,480
even without being embodied in a

1275
00:53:58,640 --> 00:54:02,040
relationship with a human and so on they

1276
00:54:00,480 --> 00:54:05,119
just seem to be that's the kind of thing

1277
00:54:02,040 --> 00:54:05,119
that we write a lot about I

1278
00:54:05,480 --> 00:54:11,359
think so yeah I was really assessing the

1279
00:54:08,839 --> 00:54:12,880
how it felt I'm puzzling over that one

1280
00:54:11,359 --> 00:54:16,319
yeah but no but but I just been

1281
00:54:12,880 --> 00:54:18,160
seriously wanting to talk to a human for

1282
00:54:16,319 --> 00:54:20,319
the humanness not the

1283
00:54:18,160 --> 00:54:22,200
competency I I tried to remove that from

1284
00:54:20,319 --> 00:54:28,359
that

1285
00:54:22,200 --> 00:54:28,359
assessment so so one um

1286
00:54:29,000 --> 00:54:33,640
in this AI era are we going to be are we

1287
00:54:31,640 --> 00:54:36,760
going to lose human skills are we going

1288
00:54:33,640 --> 00:54:40,240
to become less effective than we are

1289
00:54:36,760 --> 00:54:43,760
today how do we how do we ra how do we

1290
00:54:40,240 --> 00:54:47,240
raise the the human capacity in addition

1291
00:54:43,760 --> 00:54:49,680
to the the capacity of these systems or

1292
00:54:47,240 --> 00:54:53,400
at least not diminish the human

1293
00:54:49,680 --> 00:54:56,119
capacity so you talked about agency and

1294
00:54:53,400 --> 00:54:58,119
dignity I mean for those of us who sit

1295
00:54:56,119 --> 00:54:59,720
in rooms like this maybe part of our

1296
00:54:58,119 --> 00:55:02,200
agency and dignity is also our

1297
00:54:59,720 --> 00:55:04,839
intellectual capacity but that's

1298
00:55:02,200 --> 00:55:07,040
probably not universally true how how do

1299
00:55:04,839 --> 00:55:09,720
we how do we make sure that you know in

1300
00:55:07,040 --> 00:55:13,559
addition to agency and dignity there's

1301
00:55:09,720 --> 00:55:13,559
we continue to drive the human

1302
00:55:15,359 --> 00:55:20,079
intellect I think it's a really good

1303
00:55:17,200 --> 00:55:21,400
question I mean I'd like to It's that

1304
00:55:20,079 --> 00:55:24,000
kind of question like to take credit for

1305
00:55:21,400 --> 00:55:25,720
it but it was here from from the

1306
00:55:24,000 --> 00:55:27,839
audience yeah no I think it's a really

1307
00:55:25,720 --> 00:55:29,880
good question I think many of the

1308
00:55:27,839 --> 00:55:31,160
questions you've asked today I would

1309
00:55:29,880 --> 00:55:33,799
feel more comfortable sitting in a

1310
00:55:31,160 --> 00:55:36,000
multi-party stakeholder setting because

1311
00:55:33,799 --> 00:55:38,079
it's it's not really my answers I have

1312
00:55:36,000 --> 00:55:39,480
my intuitions but your answers have been

1313
00:55:38,079 --> 00:55:41,319
remarkably

1314
00:55:39,480 --> 00:55:43,839
informative but I've learned from

1315
00:55:41,319 --> 00:55:45,559
colleagues this disciplines and and this

1316
00:55:43,839 --> 00:55:47,480
is one reason we set up the the

1317
00:55:45,559 --> 00:55:49,720
partnership on AI to hear from different

1318
00:55:47,480 --> 00:55:52,240
people and learn from different

1319
00:55:49,720 --> 00:55:55,559
perspectives Microsoft recently uh made

1320
00:55:52,240 --> 00:55:59,960
a deal a deal an arrangement with the

1321
00:55:55,559 --> 00:56:02,240
aflc IO um where we agreed to like three

1322
00:55:59,960 --> 00:56:05,400
different major steps to keep each other

1323
00:56:02,240 --> 00:56:08,039
informed about influences of AI on jobs

1324
00:56:05,400 --> 00:56:12,400
in the economy to work together on

1325
00:56:08,039 --> 00:56:15,240
Skilling uh and uh to stay in touch um

1326
00:56:12,400 --> 00:56:17,160
and I think we need those kinds of links

1327
00:56:15,240 --> 00:56:20,599
between people on the ground people that

1328
00:56:17,160 --> 00:56:23,520
represent uh professionals uh blue

1329
00:56:20,599 --> 00:56:25,359
collar White Collar academics to

1330
00:56:23,520 --> 00:56:26,760
understand how things are going to how

1331
00:56:25,359 --> 00:56:29,000
these technologies will

1332
00:56:26,760 --> 00:56:30,280
influence uh people's daily lives and

1333
00:56:29,000 --> 00:56:32,599
their professions and their jobs and

1334
00:56:30,280 --> 00:56:34,720
their choice of jobs if they're younger

1335
00:56:32,599 --> 00:56:36,720
um in the paper that I mentioned that

1336
00:56:34,720 --> 00:56:40,119
Abby selin and I wrote It's on archive

1337
00:56:36,720 --> 00:56:41,039
right now um it's called the rise of the

1338
00:56:40,119 --> 00:56:45,640
AI

1339
00:56:41,039 --> 00:56:47,839
co-pilot um we talk about

1340
00:56:45,640 --> 00:56:50,559
co-pilots as covered in the human

1341
00:56:47,839 --> 00:56:52,839
factors literature you know plane

1342
00:56:50,559 --> 00:56:54,640
co-pilots and there's a lot that's been

1343
00:56:52,839 --> 00:56:56,880
written about the loss of

1344
00:56:54,640 --> 00:56:59,760
skills uh and

1345
00:56:56,880 --> 00:57:02,000
the ability the need to train and rigor

1346
00:56:59,760 --> 00:57:04,400
that Pilots must go through but to not

1347
00:57:02,000 --> 00:57:07,960
be dependent on their on their on their

1348
00:57:04,400 --> 00:57:10,799
co-pilots or their uh in you know their

1349
00:57:07,960 --> 00:57:13,119
autopilot in the plane I mean

1350
00:57:10,799 --> 00:57:16,400
um and

1351
00:57:13,119 --> 00:57:19,599
um I think it's a great area for

1352
00:57:16,400 --> 00:57:22,720
scholarship to understand uh loss of

1353
00:57:19,599 --> 00:57:25,039
skills um people are saying in the in

1354
00:57:22,720 --> 00:57:27,480
when it comes to the Sciences right now

1355
00:57:25,039 --> 00:57:29,960
that we haven't seen genius yet in these

1356
00:57:27,480 --> 00:57:32,720
systems um and it could be that for a

1357
00:57:29,960 --> 00:57:34,799
long time to come there's something

1358
00:57:32,720 --> 00:57:36,079
remarkable about human beings ability to

1359
00:57:34,799 --> 00:57:38,200
think out of the box and these tools

1360
00:57:36,079 --> 00:57:41,880
will just bolster their abilities to do

1361
00:57:38,200 --> 00:57:45,359
science uh it's interesting when I first

1362
00:57:41,880 --> 00:57:48,520
um uh was was testing gbd4 out I think I

1363
00:57:45,359 --> 00:57:50,559
took it to FR Maria I had Francis Arnold

1364
00:57:48,520 --> 00:57:53,559
play with it at one of our breaks a

1365
00:57:50,559 --> 00:57:55,640
pcast in a noisy bar I was there you

1366
00:57:53,559 --> 00:57:57,359
were there too so so we so Francis

1367
00:57:55,640 --> 00:57:59,400
Arnold was like okay let me see this

1368
00:57:57,359 --> 00:58:01,720
thing I had heard about this and he

1369
00:57:59,400 --> 00:58:06,000
started you know challenging the system

1370
00:58:01,720 --> 00:58:09,039
with uh various silicon Centric

1371
00:58:06,000 --> 00:58:11,160
chemistries and after we were ready to

1372
00:58:09,039 --> 00:58:14,720
move on to dinner and we had closed the

1373
00:58:11,160 --> 00:58:17,760
laptop she said yeah it's kind of like a

1374
00:58:14,720 --> 00:58:20,640
seconde grad student now now imagine

1375
00:58:17,760 --> 00:58:21,839
hearing that 10 years ago where someone

1376
00:58:20,640 --> 00:58:24,920
will

1377
00:58:21,839 --> 00:58:27,400
say yeah second year grad student as if

1378
00:58:24,920 --> 00:58:29,960
that's like a failure of the ice system

1379
00:58:27,400 --> 00:58:32,720
um so here we are with a new bar but

1380
00:58:29,960 --> 00:58:35,240
still it it hasn't blown the minds of

1381
00:58:32,720 --> 00:58:37,119
scientists yet and and sa prometer and I

1382
00:58:35,240 --> 00:58:39,520
as part of another pcast project we're

1383
00:58:37,119 --> 00:58:41,880
looking at we worked with a physicist at

1384
00:58:39,520 --> 00:58:43,720
Princeton looking at the origins of the

1385
00:58:41,880 --> 00:58:46,640
the um universe and some questions about

1386
00:58:43,720 --> 00:58:48,000
cycling versus Big Bang vers models and

1387
00:58:46,640 --> 00:58:50,920
there different models coming to into

1388
00:58:48,000 --> 00:58:53,520
Vogue now into into into um awareness or

1389
00:58:50,920 --> 00:58:56,200
or review and I think what I got back

1390
00:58:53,520 --> 00:58:58,559
from Saul after he engaged with uh his

1391
00:58:56,200 --> 00:59:01,880
Princeton colleague was kind of like a

1392
00:58:58,559 --> 00:59:03,920
seconde grad student so it's interesting

1393
00:59:01,880 --> 00:59:05,520
will we go beyond that someday it could

1394
00:59:03,920 --> 00:59:07,119
be that the Sciences are a place where

1395
00:59:05,520 --> 00:59:11,200
we don't lose our skills and we just get

1396
00:59:07,119 --> 00:59:11,200
bolstered and push ahead with our human

1397
00:59:11,440 --> 00:59:16,240
creativity so I wanted to change gears

1398
00:59:13,839 --> 00:59:18,599
to a different kind of topic here um

1399
00:59:16,240 --> 00:59:22,200
which is there's a lot of concern out

1400
00:59:18,599 --> 00:59:24,039
there about the energy usage and demands

1401
00:59:22,200 --> 00:59:26,839
both of the training of these large

1402
00:59:24,039 --> 00:59:28,920
language models and then uh or large AI

1403
00:59:26,839 --> 00:59:30,880
models generally and then even though

1404
00:59:28,920 --> 00:59:32,760
the individual inferences aren't

1405
00:59:30,880 --> 00:59:34,359
necessarily that expensive when you

1406
00:59:32,760 --> 00:59:37,520
integrate over all the inference for

1407
00:59:34,359 --> 00:59:39,400
doing so how how are you thinking about

1408
00:59:37,520 --> 00:59:41,319
that how you you yourself how's

1409
00:59:39,400 --> 00:59:44,200
Microsoft thinking about

1410
00:59:41,319 --> 00:59:46,000
that so um I think we just discussed

1411
00:59:44,200 --> 00:59:49,079
this earlier in your conference room so

1412
00:59:46,000 --> 00:59:50,839
on Monday on Earth day uh I and several

1413
00:59:49,079 --> 00:59:54,079
colleagues had our a paper published in

1414
00:59:50,839 --> 00:59:56,480
nature it's a comment piece where we

1415
00:59:54,079 --> 00:59:59,520
spent months it was an interdiciplinary

1416
00:59:56,480 --> 01:00:02,480
group we spent months looking at Power

1417
00:59:59,520 --> 01:00:06,280
projections and analyses for AI and the

1418
01:00:02,480 --> 01:00:09,400
paper is all about is AI going to help

1419
01:00:06,280 --> 01:00:12,240
us or or hinder Us in getting to into

1420
01:00:09,400 --> 01:00:13,440
Net Zero on carbon um and we talked

1421
01:00:12,240 --> 01:00:15,319
about the need to do research and

1422
01:00:13,440 --> 01:00:17,319
scenarios but one interesting finding

1423
01:00:15,319 --> 01:00:20,559
that came out was you know a credible

1424
01:00:17,319 --> 01:00:22,520
projection is that we may need 10 times

1425
01:00:20,559 --> 01:00:23,720
the amount of compute for AI in four

1426
01:00:22,520 --> 01:00:26,039
within four

1427
01:00:23,720 --> 01:00:28,039
years and and as I said in my little

1428
01:00:26,039 --> 01:00:31,000
tweet when I was you know roll the paper

1429
01:00:28,039 --> 01:00:34,640
out on Monday uh to my to my colleagues

1430
01:00:31,000 --> 01:00:36,720
to my network I said scary sounds very

1431
01:00:34,640 --> 01:00:40,039
scary but do you realize that will bring

1432
01:00:36,720 --> 01:00:43,039
that power uh amount up to the quantity

1433
01:00:40,039 --> 01:00:46,240
of power used by Americans watching

1434
01:00:43,039 --> 01:00:49,160
television so so so I don't I'm not

1435
01:00:46,240 --> 01:00:51,599
concerned about global power in carbon

1436
01:00:49,160 --> 01:00:54,760
right now I also see interesting trends

1437
01:00:51,599 --> 01:00:57,400
like 53 mini that was just shipped can

1438
01:00:54,760 --> 01:01:00,039
run on a cell phone on a client without

1439
01:00:57,400 --> 01:01:02,599
the cloud for example certainly the

1440
01:01:00,039 --> 01:01:04,839
training of the largest models is taking

1441
01:01:02,599 --> 01:01:07,480
quite a bit of uh impressive amounts of

1442
01:01:04,839 --> 01:01:09,079
electric power uh that's amortised over

1443
01:01:07,480 --> 01:01:14,000
time when it's when these models are

1444
01:01:09,079 --> 01:01:16,559
used um but

1445
01:01:14,000 --> 01:01:18,319
uh when it comes to the scenarios I

1446
01:01:16,559 --> 01:01:21,480
personally fall on the side of AI being

1447
01:01:18,319 --> 01:01:23,160
a boon in leading to some breakthroughs

1448
01:01:21,480 --> 01:01:24,720
on the chemistry's catalysis and

1449
01:01:23,160 --> 01:01:27,760
thinking about how

1450
01:01:24,720 --> 01:01:31,880
to the and designs for how to address

1451
01:01:27,760 --> 01:01:33,520
sustainability challenges um but we we

1452
01:01:31,880 --> 01:01:35,960
should keep an eye on it for example for

1453
01:01:33,520 --> 01:01:39,359
we know it's a thousand times the

1454
01:01:35,960 --> 01:01:41,640
confute yeah well great I want to thank

1455
01:01:39,359 --> 01:01:43,920
the audience but I want to especially

1456
01:01:41,640 --> 01:01:47,400
thank Eric for joining us and for your

1457
01:01:43,920 --> 01:01:50,319
incredibly thoughtful observations and

1458
01:01:47,400 --> 01:01:51,760
willingness to take such a crazy wide

1459
01:01:50,319 --> 01:01:53,520
range of

1460
01:01:51,760 --> 01:01:57,240
questions thanks for the questions

1461
01:01:53,520 --> 01:01:57,240
everybody thanks for the questions

1462
01:01:57,960 --> 01:02:03,039
I'll just like this to you I want you

1463
01:01:59,599 --> 01:02:03,039
get sick BL your

1464
01:02:03,319 --> 01:02:11,200
hand so please do join us across the

1465
01:02:05,920 --> 01:02:11,200
street in stada uh for reception

