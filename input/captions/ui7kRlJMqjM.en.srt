1
00:00:06,210 --> 00:00:08,220
- Hi everybody. I'm Andrew Lo,

2
00:00:08,220 --> 00:00:11,100
a professor of finance at the
MIT Sloan School of Management

3
00:00:11,100 --> 00:00:12,270
and a principal investigator

4
00:00:12,270 --> 00:00:14,370
at the MIT Computer Science and AI Lab,

5
00:00:14,370 --> 00:00:17,430
and I'm looking forward to
answering your questions.

6
00:00:17,430 --> 00:00:19,200
Let's take the first one.

7
00:00:19,200 --> 00:00:20,670
How can large language models

8
00:00:20,670 --> 00:00:23,040
efficiently analyze financial reports

9
00:00:23,040 --> 00:00:26,610
to extract key insights
such as identifying risks,

10
00:00:26,610 --> 00:00:29,460
opportunities, and emerging trends?

11
00:00:29,460 --> 00:00:31,590
Large language models
are actually designed

12
00:00:31,590 --> 00:00:33,330
to be able to read plain text

13
00:00:33,330 --> 00:00:36,840
and be able to digest and summarize them.

14
00:00:36,840 --> 00:00:39,030
Earnings reports, financial statements.

15
00:00:39,030 --> 00:00:40,410
These are the kind of things

16
00:00:40,410 --> 00:00:43,140
that large language models can now read

17
00:00:43,140 --> 00:00:47,760
and be able to generate very,
very short and pithy summaries

18
00:00:47,760 --> 00:00:51,090
to be able to focus on things
like risks, opportunities,

19
00:00:51,090 --> 00:00:52,470
and other issues.

20
00:00:52,470 --> 00:00:54,240
In the financial sector,

21
00:00:54,240 --> 00:00:57,720
we use certain keywords
to be able to highlight

22
00:00:57,720 --> 00:01:02,220
risks and opportunities so
that there are automatic tags

23
00:01:02,220 --> 00:01:03,540
that large language models

24
00:01:03,540 --> 00:01:05,490
can figure out very, very quickly.

25
00:01:05,490 --> 00:01:08,250
Large language models
will have a huge impact

26
00:01:08,250 --> 00:01:10,050
on the life of financial analysts

27
00:01:10,050 --> 00:01:11,850
because their job day in and day out

28
00:01:11,850 --> 00:01:13,440
is to be able to read these reports,

29
00:01:13,440 --> 00:01:15,090
and they can now do that

30
00:01:15,090 --> 00:01:19,383
using these tools in a much
faster and more accurate way.

31
00:01:20,700 --> 00:01:23,970
Can LLMs identify subtle market patterns

32
00:01:23,970 --> 00:01:27,900
and anomalies that may
elude human analysts?

33
00:01:27,900 --> 00:01:30,390
The answer is yes, they can,

34
00:01:30,390 --> 00:01:34,890
but they can also identify
anomalies that don't exist,

35
00:01:34,890 --> 00:01:36,930
in other words, hallucinate.

36
00:01:36,930 --> 00:01:38,340
And this is one of the challenges

37
00:01:38,340 --> 00:01:41,070
that we have to deal with with LLMs.

38
00:01:41,070 --> 00:01:43,710
Right now, we know that
they're very capable

39
00:01:43,710 --> 00:01:47,550
of being able to identify some
really interesting patterns,

40
00:01:47,550 --> 00:01:50,670
but at the same time, they
can also identify things

41
00:01:50,670 --> 00:01:53,100
that aren't there and require humans

42
00:01:53,100 --> 00:01:55,680
to engage in additional oversight.

43
00:01:55,680 --> 00:01:59,310
Now, by the way, humans
are not perfect either,

44
00:01:59,310 --> 00:02:03,240
and we are prone to
hallucinate on occasion.

45
00:02:03,240 --> 00:02:05,737
Paul Samuelson famously said,

46
00:02:05,737 --> 00:02:07,830
"economists have predicted five out

47
00:02:07,830 --> 00:02:09,810
of the last three recessions,"

48
00:02:09,810 --> 00:02:12,360
and I think that's part
of what he was getting at.

49
00:02:12,360 --> 00:02:13,410
We need to think about

50
00:02:13,410 --> 00:02:15,600
how we can make ourselves more accurate.

51
00:02:15,600 --> 00:02:19,440
So the combination of humans and LLMs

52
00:02:19,440 --> 00:02:22,560
could actually be the sweet
spot of being able to produce

53
00:02:22,560 --> 00:02:24,060
really accurate forecasts

54
00:02:24,060 --> 00:02:26,433
for all sorts of economic phenomenon.

55
00:02:27,300 --> 00:02:30,660
How can we build trust in financial advice

56
00:02:30,660 --> 00:02:33,330
and decisions provided by LLMs?

57
00:02:33,330 --> 00:02:35,430
And what strategies can be employed

58
00:02:35,430 --> 00:02:38,580
to maintain human oversight and control?

59
00:02:38,580 --> 00:02:40,320
Now, this is a really interesting one

60
00:02:40,320 --> 00:02:43,560
because it gets at an
issue that my collaborators

61
00:02:43,560 --> 00:02:46,560
and I have been working on
for the last several months,

62
00:02:46,560 --> 00:02:48,360
and we think we have an approach,

63
00:02:48,360 --> 00:02:51,570
but it'll be several years
before we come to it.

64
00:02:51,570 --> 00:02:55,620
And the problem that we're
studying is the issue of trust.

65
00:02:55,620 --> 00:02:58,740
How do we get LLMs to be trusted

66
00:02:58,740 --> 00:03:02,220
and specifically in the
context of financial advice?

67
00:03:02,220 --> 00:03:05,880
So in that domain, we
actually have a concept

68
00:03:05,880 --> 00:03:08,520
that is known as fiduciary duty,

69
00:03:08,520 --> 00:03:10,830
and this really gets
at the heart of trust.

70
00:03:10,830 --> 00:03:15,300
In the financial sector,
a fiduciary is somebody

71
00:03:15,300 --> 00:03:19,920
who puts your interests
ahead of his or her own.

72
00:03:19,920 --> 00:03:23,610
So for example, if you
have a financial advisor,

73
00:03:23,610 --> 00:03:27,570
that financial advisor
owes you a fiduciary duty,

74
00:03:27,570 --> 00:03:29,940
meaning that when they give you advice

75
00:03:29,940 --> 00:03:31,470
about how to manage your money,

76
00:03:31,470 --> 00:03:33,840
they have to be thinking
first and foremost

77
00:03:33,840 --> 00:03:36,060
about your welfare as opposed

78
00:03:36,060 --> 00:03:38,640
to lining their own
pockets with commissions

79
00:03:38,640 --> 00:03:40,650
that might come out of the kind of trades

80
00:03:40,650 --> 00:03:42,900
that they would do on your behalf.

81
00:03:42,900 --> 00:03:47,550
How do we get an LLM
to become a fiduciary?

82
00:03:47,550 --> 00:03:51,720
How do we get it to be a trusted
financial advisor to you?

83
00:03:51,720 --> 00:03:54,210
Our approach is to think about

84
00:03:54,210 --> 00:03:56,550
how this happens in the industry

85
00:03:56,550 --> 00:03:58,530
with human financial advisors.

86
00:03:58,530 --> 00:04:01,650
How do human advisors become fiduciaries?

87
00:04:01,650 --> 00:04:04,200
Well, for one thing, they
have a code of ethics,

88
00:04:04,200 --> 00:04:06,810
so they have to follow certain rules,

89
00:04:06,810 --> 00:04:08,460
but more importantly,

90
00:04:08,460 --> 00:04:13,230
they have to follow a large
number of financial regulations,

91
00:04:13,230 --> 00:04:17,850
laws that we put in place in
order to protect consumers.

92
00:04:17,850 --> 00:04:20,880
And at one point, I
remember studying these laws

93
00:04:20,880 --> 00:04:22,950
because I had to take a securities exam

94
00:04:22,950 --> 00:04:24,480
in order to be involved

95
00:04:24,480 --> 00:04:27,390
in a particular kind of
a financial institution.

96
00:04:27,390 --> 00:04:30,180
And this exam called the Series 65

97
00:04:30,180 --> 00:04:33,690
test you on your knowledge not
only of financial analysis,

98
00:04:33,690 --> 00:04:36,720
but also of the regulatory infrastructure

99
00:04:36,720 --> 00:04:39,630
that guides all of us in the industry.

100
00:04:39,630 --> 00:04:42,660
And so after spending hours and hours

101
00:04:42,660 --> 00:04:44,130
reading all of these rules

102
00:04:44,130 --> 00:04:46,170
and being frustrated that I was forced

103
00:04:46,170 --> 00:04:48,690
to memorize these things,

104
00:04:48,690 --> 00:04:50,400
it finally dawned on me

105
00:04:50,400 --> 00:04:52,650
where these things were coming from.

106
00:04:52,650 --> 00:04:56,250
The historical body of case law

107
00:04:56,250 --> 00:05:00,420
that has been used to
guide lawyers, regulators,

108
00:05:00,420 --> 00:05:04,140
and consumers as to what
they can and cannot do,

109
00:05:04,140 --> 00:05:07,770
that is a fossil record of all of the ways

110
00:05:07,770 --> 00:05:09,660
that one human has decided

111
00:05:09,660 --> 00:05:12,540
to be able to try to take
advantage of another human

112
00:05:12,540 --> 00:05:13,980
and ultimately got caught

113
00:05:13,980 --> 00:05:16,710
and prosecuted successfully for it.

114
00:05:16,710 --> 00:05:19,440
So if we trained LLMs,

115
00:05:19,440 --> 00:05:23,040
not just with the body
of financial knowledge,

116
00:05:23,040 --> 00:05:26,220
but the full case law history

117
00:05:26,220 --> 00:05:28,890
of all of the various different lawsuits

118
00:05:28,890 --> 00:05:32,250
that have been filed
against certain bad actors

119
00:05:32,250 --> 00:05:34,200
in the financial system,

120
00:05:34,200 --> 00:05:36,150
if we do all of that,

121
00:05:36,150 --> 00:05:40,920
we then can train LLMs
to become fiduciaries.

122
00:05:40,920 --> 00:05:44,040
We believe that we're
still a few years away

123
00:05:44,040 --> 00:05:46,260
from something that the SEC

124
00:05:46,260 --> 00:05:49,560
and lawyers would agree
would be a fiduciary,

125
00:05:49,560 --> 00:05:51,570
but we at least know the
direction that we're going,

126
00:05:51,570 --> 00:05:54,570
and we're very excited to
be able to go that route

127
00:05:54,570 --> 00:05:57,360
and ultimately produce a piece of software

128
00:05:57,360 --> 00:06:00,720
that can be fully trusted by humans.

129
00:06:00,720 --> 00:06:03,540
What role could LLMs play in automating

130
00:06:03,540 --> 00:06:05,730
and streamlining risk assessment processes

131
00:06:05,730 --> 00:06:08,040
for banks and financial institutions,

132
00:06:08,040 --> 00:06:11,280
and how should these systems be governed?

133
00:06:11,280 --> 00:06:14,010
So this is also a great question because

134
00:06:14,010 --> 00:06:16,620
that's one of the most important features

135
00:06:16,620 --> 00:06:19,350
that financial institutions
have to carry out

136
00:06:19,350 --> 00:06:20,970
is risk management.

137
00:06:20,970 --> 00:06:24,150
Risk management has two parts, in my view.

138
00:06:24,150 --> 00:06:27,000
The first part is a quantitative part,

139
00:06:27,000 --> 00:06:29,670
and that's actually
relatively easy to automate.

140
00:06:29,670 --> 00:06:33,840
Computing things like value
at risk and scenario analysis

141
00:06:33,840 --> 00:06:35,820
and worst loss scenarios,

142
00:06:35,820 --> 00:06:38,730
all of those things can
now be done pretty much

143
00:06:38,730 --> 00:06:41,040
at a push of a button
and and are being done

144
00:06:41,040 --> 00:06:43,440
by all the big financial institutions.

145
00:06:43,440 --> 00:06:45,120
What is much more difficult

146
00:06:45,120 --> 00:06:47,490
is the second set of tasks,

147
00:06:47,490 --> 00:06:50,940
and that is taking all of those numbers

148
00:06:50,940 --> 00:06:55,710
and putting that into a
narrative that can be given

149
00:06:55,710 --> 00:06:57,330
to a risk manager,

150
00:06:57,330 --> 00:06:59,310
to a policymaker,

151
00:06:59,310 --> 00:07:00,450
to a customer,

152
00:07:00,450 --> 00:07:03,030
so that they understand
what the implications

153
00:07:03,030 --> 00:07:06,780
of those numbers are for their
own personal circumstances.

154
00:07:06,780 --> 00:07:09,570
Imagine that the stock
market just went down

155
00:07:09,570 --> 00:07:12,540
by 15% today,

156
00:07:12,540 --> 00:07:17,340
and that means that your portfolio of

157
00:07:17,340 --> 00:07:20,160
bonds are now in trouble

158
00:07:20,160 --> 00:07:22,620
because the stock market has gone down

159
00:07:22,620 --> 00:07:26,280
and there's a concern that the
Fed is gonna have to step in,

160
00:07:26,280 --> 00:07:28,560
and everybody is worried about

161
00:07:28,560 --> 00:07:31,650
defaults across various
different kinds of scenarios,

162
00:07:31,650 --> 00:07:34,080
and you're holding a
bunch of corporate bonds

163
00:07:34,080 --> 00:07:35,670
that are risky to begin with,

164
00:07:35,670 --> 00:07:39,033
but in the today's
scenario, it's a lot worse.

165
00:07:39,960 --> 00:07:41,820
What do you do about it?

166
00:07:41,820 --> 00:07:46,820
An LLM could analyze the
numerical data instantaneously

167
00:07:47,910 --> 00:07:52,530
and then weave that into a
narrative which says, perhaps

168
00:07:52,530 --> 00:07:55,980
today equity markets
are down by quite a bit.

169
00:07:55,980 --> 00:07:58,080
That means the Fed's gonna step in,

170
00:07:58,080 --> 00:08:01,710
there's gonna be panic
among a bunch of investors,

171
00:08:01,710 --> 00:08:05,490
and that means that certain
assets will sell off

172
00:08:05,490 --> 00:08:08,820
and other assets will
become much more valuable.

173
00:08:08,820 --> 00:08:10,980
If you're holding corporate bonds,

174
00:08:10,980 --> 00:08:13,530
that is likely to get hit hard,

175
00:08:13,530 --> 00:08:16,020
treasury bills are gonna do really well,

176
00:08:16,020 --> 00:08:19,800
but I would wait for
another three to five weeks

177
00:08:19,800 --> 00:08:21,390
before you do anything dramatic

178
00:08:21,390 --> 00:08:24,030
because over the course of history,

179
00:08:24,030 --> 00:08:26,430
if you looked at similar situations,

180
00:08:26,430 --> 00:08:29,130
these kinds of moves are temporary

181
00:08:29,130 --> 00:08:31,380
and things will normalize.

182
00:08:31,380 --> 00:08:34,410
That's the kind of narrative
that an LLM can provide

183
00:08:34,410 --> 00:08:35,820
that is not available

184
00:08:35,820 --> 00:08:39,330
when you're just looking at
a whole table of numbers.

185
00:08:39,330 --> 00:08:43,410
So over time, I suspect that
LLMs will get much better

186
00:08:43,410 --> 00:08:44,940
at weaving those narratives

187
00:08:44,940 --> 00:08:46,920
and also being able to describe

188
00:08:46,920 --> 00:08:49,140
some of the weaknesses in their narratives

189
00:08:49,140 --> 00:08:52,110
and where you might wanna use
your own personal judgment

190
00:08:52,110 --> 00:08:55,710
to make a call for your own portfolio.

191
00:08:55,710 --> 00:08:57,990
So I think we're not that far away

192
00:08:57,990 --> 00:09:00,240
from the use of LLMs for risk management,

193
00:09:00,240 --> 00:09:02,310
and I'm hoping that
that will get everybody

194
00:09:02,310 --> 00:09:04,290
to focus on the right kind of risks

195
00:09:04,290 --> 00:09:05,590
that are relevant to them.

196
00:09:06,930 --> 00:09:09,390
How can the natural language
processing capabilities

197
00:09:09,390 --> 00:09:12,210
of LLMs be used to
perform sentiment analysis

198
00:09:12,210 --> 00:09:13,680
on financial news,

199
00:09:13,680 --> 00:09:16,908
social media, and other
textural data to inform trading

200
00:09:16,908 --> 00:09:18,810
and investment decisions?

201
00:09:18,810 --> 00:09:21,510
So typically, we believe
that financial markets

202
00:09:21,510 --> 00:09:26,510
are moved by two things, fear and greed.

203
00:09:26,640 --> 00:09:29,160
Now, that's the Wall Street
trader's perspective.

204
00:09:29,160 --> 00:09:30,360
If you're asking an economist,

205
00:09:30,360 --> 00:09:32,400
it's all about the numbers, right?

206
00:09:32,400 --> 00:09:34,410
The bottom line is it's both.

207
00:09:34,410 --> 00:09:38,610
It's the numbers, but it's
how we interpret the numbers,

208
00:09:38,610 --> 00:09:41,970
and that's where sentiment comes in.

209
00:09:41,970 --> 00:09:43,950
So the idea behind sentiment analysis

210
00:09:43,950 --> 00:09:45,780
is to try to understand

211
00:09:45,780 --> 00:09:50,780
how human emotion is gonna
react to the numbers.

212
00:09:51,210 --> 00:09:54,030
And over the course of
the last several decades,

213
00:09:54,030 --> 00:09:57,540
there's been a lot
written on human behavior,

214
00:09:57,540 --> 00:10:00,420
psychology and the ability for us

215
00:10:00,420 --> 00:10:02,670
to manage our emotions

216
00:10:02,670 --> 00:10:05,940
in very, very difficult
financial circumstances.

217
00:10:05,940 --> 00:10:08,760
Obviously, most of us
have trouble doing so

218
00:10:08,760 --> 00:10:10,410
because we are all hardwired

219
00:10:10,410 --> 00:10:12,900
to engage in the fight or flight response.

220
00:10:12,900 --> 00:10:14,250
When we are threatened,

221
00:10:14,250 --> 00:10:17,850
we will react in a very
predictable way physiologically,

222
00:10:17,850 --> 00:10:19,350
which is great if you're being chased

223
00:10:19,350 --> 00:10:20,490
by a saber tooth tiger

224
00:10:20,490 --> 00:10:22,110
on the plains of the African savanna

225
00:10:22,110 --> 00:10:23,670
a hundred thousand years ago,

226
00:10:23,670 --> 00:10:25,080
doesn't work so well on the floor

227
00:10:25,080 --> 00:10:27,540
of the New York Stock Exchange today.

228
00:10:27,540 --> 00:10:30,210
So sentiment analysis is an attempt

229
00:10:30,210 --> 00:10:32,940
to try to look at the various
different financial indicators

230
00:10:32,940 --> 00:10:34,650
to get a sense of
whether or not the market

231
00:10:34,650 --> 00:10:36,930
is overreacting or underreacting.

232
00:10:36,930 --> 00:10:40,050
And I think this is where
LLMs will have a field day

233
00:10:40,050 --> 00:10:42,840
because they're gonna be
able to look at the numbers,

234
00:10:42,840 --> 00:10:43,920
but more importantly,

235
00:10:43,920 --> 00:10:45,600
they're gonna be able
to read the literature

236
00:10:45,600 --> 00:10:50,130
of what's being written at that
very moment by news sources

237
00:10:50,130 --> 00:10:52,890
that are freaking the rest of us out.

238
00:10:52,890 --> 00:10:57,720
When the news stations tell
us, is there something in milk

239
00:10:57,720 --> 00:11:00,000
that could be hurting your infants?

240
00:11:00,000 --> 00:11:01,410
More at 11.

241
00:11:01,410 --> 00:11:03,360
You're gonna feel compelled to watch

242
00:11:03,360 --> 00:11:05,430
that news story at 11 o'clock.

243
00:11:05,430 --> 00:11:10,430
So we are very easily moved
by those kinds of threats.

244
00:11:10,470 --> 00:11:13,350
And so LLMs will be
very good at picking up

245
00:11:13,350 --> 00:11:14,580
those kinds of threats

246
00:11:14,580 --> 00:11:18,960
and coordinating the
analysis of the language

247
00:11:18,960 --> 00:11:22,560
with the numbers to be able
to produce sentiment analysis.

248
00:11:22,560 --> 00:11:24,750
In fact, I wouldn't be
surprised if certain hedge funds

249
00:11:24,750 --> 00:11:26,280
were already using LLMs

250
00:11:26,280 --> 00:11:28,650
to be able to detect
these kinds of patterns

251
00:11:28,650 --> 00:11:30,360
and start making use of them.

252
00:11:30,360 --> 00:11:32,820
The hope is that the
typical retail investor,

253
00:11:32,820 --> 00:11:34,350
the rest of us consumers

254
00:11:34,350 --> 00:11:37,110
will have access to those tools soon.

255
00:11:37,110 --> 00:11:39,030
Okay, next question.

256
00:11:39,030 --> 00:11:42,480
How can we mitigate bias in
LLMs for financial applications,

257
00:11:42,480 --> 00:11:44,670
and what other ethical considerations,

258
00:11:44,670 --> 00:11:47,580
like algorithmic transparency
and accountability,

259
00:11:47,580 --> 00:11:49,710
should be prioritized?

260
00:11:49,710 --> 00:11:51,120
So the first thing to note

261
00:11:51,120 --> 00:11:54,750
is that LMS absolutely do have biases.

262
00:11:54,750 --> 00:11:57,360
And I know this because my students

263
00:11:57,360 --> 00:11:59,036
and I documented that in a paper

264
00:11:59,036 --> 00:12:03,240
that we wrote recently
looking at hiring decisions

265
00:12:03,240 --> 00:12:05,580
that an LLM might make

266
00:12:05,580 --> 00:12:08,400
when confronted with a variety
of different candidates.

267
00:12:08,400 --> 00:12:11,250
So it definitely suffers from gender bias

268
00:12:11,250 --> 00:12:13,260
in a variety of different contexts.

269
00:12:13,260 --> 00:12:16,350
And it's not surprising because
what are LLMs reflecting?

270
00:12:16,350 --> 00:12:20,070
They're reflecting the sum
total of the literature

271
00:12:20,070 --> 00:12:23,190
that they're using as
inputs to be trained.

272
00:12:23,190 --> 00:12:27,390
The first step in dealing
with bias is to document it.

273
00:12:27,390 --> 00:12:30,690
We need to understand, depending
on the nature of the LLM,

274
00:12:30,690 --> 00:12:33,690
how it's trained, other
supplements that we use with it,

275
00:12:33,690 --> 00:12:36,240
we have to understand exactly
what those biases are.

276
00:12:36,240 --> 00:12:37,680
We have to quantify them.

277
00:12:37,680 --> 00:12:39,300
Once we quantify them,

278
00:12:39,300 --> 00:12:41,490
then we can start asking the question,

279
00:12:41,490 --> 00:12:44,880
how do we decide to change the biases

280
00:12:44,880 --> 00:12:48,600
to make it more appropriate
for the purpose at hand?

281
00:12:48,600 --> 00:12:50,040
So that's a question

282
00:12:50,040 --> 00:12:52,410
that requires domain specific knowledge.

283
00:12:52,410 --> 00:12:55,020
In certain areas, the
biases may be very small.

284
00:12:55,020 --> 00:12:57,390
In other areas, the biases may be huge.

285
00:12:57,390 --> 00:12:59,520
So for every single application,

286
00:12:59,520 --> 00:13:02,430
I believe that we need to think carefully

287
00:13:02,430 --> 00:13:05,760
about the implicit biases
in the LLM that we're using.

288
00:13:05,760 --> 00:13:07,380
And once we document that,

289
00:13:07,380 --> 00:13:11,610
to be able to then start
engaging in recoding

290
00:13:11,610 --> 00:13:14,130
or adding various different supplements,

291
00:13:14,130 --> 00:13:15,959
retrieval, augmented guides, rags,

292
00:13:15,959 --> 00:13:19,140
that would actually lean
against those kinds of biases

293
00:13:19,140 --> 00:13:21,210
to the degree that we wish.

294
00:13:21,210 --> 00:13:23,760
Over time, we need to understand

295
00:13:23,760 --> 00:13:25,590
how these biases are changing.

296
00:13:25,590 --> 00:13:30,390
They change across time,
across culture, across country.

297
00:13:30,390 --> 00:13:35,250
So understanding just exactly
what these LLMs are doing

298
00:13:35,250 --> 00:13:37,500
is something that is
gonna be a prerequisite

299
00:13:37,500 --> 00:13:40,979
to us being able to put any
of these things into practice.

300
00:13:40,979 --> 00:13:43,980
You have to measure before you can manage.

301
00:13:43,980 --> 00:13:46,950
Can LLMs be used to enhance the detection

302
00:13:46,950 --> 00:13:49,530
and prevention of financial fraud?

303
00:13:49,530 --> 00:13:53,220
I think the answer to this is
unambiguously absolutely yes.

304
00:13:53,220 --> 00:13:56,220
Right now, there are a number
of machine learning tools

305
00:13:56,220 --> 00:13:58,590
that are already being used by the SEC

306
00:13:58,590 --> 00:14:02,580
and other agencies to
identify potential fraud.

307
00:14:02,580 --> 00:14:06,810
A long time ago, it was
suggested by a mathematician

308
00:14:06,810 --> 00:14:09,090
that there are certain
statistical properties

309
00:14:09,090 --> 00:14:12,720
that have to exist among
a table of numbers.

310
00:14:12,720 --> 00:14:15,570
And so if you see a departure

311
00:14:15,570 --> 00:14:18,210
from that statistical regularity,

312
00:14:18,210 --> 00:14:19,590
that's an example of fraud.

313
00:14:19,590 --> 00:14:22,020
So using these kinds of distributions,

314
00:14:22,020 --> 00:14:25,020
already we can detect
certain types of fraud,

315
00:14:25,020 --> 00:14:27,930
but now with LLMs, with
more sophisticated ways

316
00:14:27,930 --> 00:14:31,140
of processing natural
language and numbers together,

317
00:14:31,140 --> 00:14:33,420
we can actually do even better.

318
00:14:33,420 --> 00:14:35,100
So there, there's no doubt in my mind

319
00:14:35,100 --> 00:14:38,190
that this is gonna be a
tremendously powerful tool.

320
00:14:38,190 --> 00:14:41,970
The dark side of this
is that with these LLMs,

321
00:14:41,970 --> 00:14:46,470
we can also create fraud
that is harder to detect.

322
00:14:46,470 --> 00:14:50,340
For example, imagine prompting
your LLM by asking it

323
00:14:50,340 --> 00:14:53,730
to take a look at your tax returns

324
00:14:53,730 --> 00:14:58,290
and suggest ways of putting
in certain kinds of deductions

325
00:14:58,290 --> 00:15:01,710
that will give you a much lower tax bill.

326
00:15:01,710 --> 00:15:04,890
And even if they break the rules,

327
00:15:04,890 --> 00:15:08,340
to do so in a way that makes
it virtually impossible

328
00:15:08,340 --> 00:15:11,550
to detect by a typical IRS auditor.

329
00:15:11,550 --> 00:15:13,620
Now imagine if you gave
that prompt to an LLM

330
00:15:13,620 --> 00:15:17,430
and imagine if it could actually
deliver on that request.

331
00:15:17,430 --> 00:15:18,870
That's the danger.

332
00:15:18,870 --> 00:15:22,590
And one of the reasons why I
think we're in an arms race

333
00:15:22,590 --> 00:15:26,340
between the regulators
and the perpetrators,

334
00:15:26,340 --> 00:15:28,710
and one of the reasons why I believe

335
00:15:28,710 --> 00:15:30,750
that we ought to increase the budgets

336
00:15:30,750 --> 00:15:32,340
of regulatory authorities

337
00:15:32,340 --> 00:15:36,150
because they need to have
the same type of equipment

338
00:15:36,150 --> 00:15:39,330
and sophistication to be able
to address these concerns

339
00:15:39,330 --> 00:15:42,030
so that they can stay
ahead of the fraudsters.

340
00:15:42,030 --> 00:15:44,850
In what ways can LMS
assist in the development

341
00:15:44,850 --> 00:15:47,850
and testing of more
sophisticated trading algorithms?

342
00:15:47,850 --> 00:15:49,680
I believe that they're
actually already being used

343
00:15:49,680 --> 00:15:51,390
for just that purpose.

344
00:15:51,390 --> 00:15:53,370
So it used to be the case

345
00:15:53,370 --> 00:15:55,320
that machine learning algorithms

346
00:15:55,320 --> 00:15:58,410
really had to be focused
on the specific feature

347
00:15:58,410 --> 00:15:59,640
that you were giving it

348
00:15:59,640 --> 00:16:02,460
to be able to detect
patterns in financial data.

349
00:16:02,460 --> 00:16:04,110
And that's really what
trading algorithms are.

350
00:16:04,110 --> 00:16:05,910
It's really just pattern matching

351
00:16:05,910 --> 00:16:08,130
so that you can predict
what's gonna happen tomorrow

352
00:16:08,130 --> 00:16:10,890
and trade today to take
advantage of that prediction.

353
00:16:10,890 --> 00:16:14,550
But now imagine being able to
make those predictions based,

354
00:16:14,550 --> 00:16:18,030
not just on numerical
data, but on textual data.

355
00:16:18,030 --> 00:16:19,800
And to be able to combine the two,

356
00:16:19,800 --> 00:16:23,610
to be able to create this kind
of a sentiment analysis score

357
00:16:23,610 --> 00:16:26,430
and be able to understand how it is

358
00:16:26,430 --> 00:16:29,970
that certain kinds of
language yield predictions

359
00:16:29,970 --> 00:16:32,010
that will ultimately come to pass

360
00:16:32,010 --> 00:16:34,650
in financial stock prices, bond prices,

361
00:16:34,650 --> 00:16:36,573
and other instruments.

362
00:16:37,620 --> 00:16:41,370
So large language models now can analyze

363
00:16:41,370 --> 00:16:45,300
lots of different sources
of text, including news.

364
00:16:45,300 --> 00:16:48,600
News is one of the really key
aspects of financial markets.

365
00:16:48,600 --> 00:16:52,710
Markets are always reacting
to current information,

366
00:16:52,710 --> 00:16:56,280
and the information does not
necessarily have to be accurate

367
00:16:56,280 --> 00:16:58,260
for financial markets to react.

368
00:16:58,260 --> 00:17:00,810
They will react to rumor in many cases

369
00:17:00,810 --> 00:17:03,090
just as quickly as they'll react to

370
00:17:03,090 --> 00:17:06,210
substantive true information.

371
00:17:06,210 --> 00:17:10,020
And so large language models can combine

372
00:17:10,020 --> 00:17:11,340
the kind of information

373
00:17:11,340 --> 00:17:13,020
across various different news sources

374
00:17:13,020 --> 00:17:16,740
and distill it into a
single prediction, which is,

375
00:17:16,740 --> 00:17:20,010
will the asset go up or
down in price tomorrow?

376
00:17:20,010 --> 00:17:21,870
I believe that more sophisticated methods

377
00:17:21,870 --> 00:17:23,370
of prediction are possible,

378
00:17:23,370 --> 00:17:26,940
but the challenge is to come
up with the correct prompts

379
00:17:26,940 --> 00:17:28,530
to make those predictions,

380
00:17:28,530 --> 00:17:31,890
and then to be able to deal
with the issue of hallucination.

381
00:17:31,890 --> 00:17:34,320
So that's one of the
reasons why hedge funds now

382
00:17:34,320 --> 00:17:36,870
are experimenting with
these large language models

383
00:17:36,870 --> 00:17:40,140
and why those of you who are
interested in having a career

384
00:17:40,140 --> 00:17:41,820
in financial analysis,

385
00:17:41,820 --> 00:17:44,400
I would urge you to start
playing around with LLMs

386
00:17:44,400 --> 00:17:45,963
for exactly this purpose.

387
00:17:47,610 --> 00:17:50,610
What regulatory and
compliance considerations

388
00:17:50,610 --> 00:17:54,810
should be addressed when
deploying LLMs in this field?

389
00:17:54,810 --> 00:17:57,150
Well, I mentioned that
there's an arms race going on

390
00:17:57,150 --> 00:17:59,880
between the regulators and the fraudsters,

391
00:17:59,880 --> 00:18:02,250
and I think that that's
something that we really need

392
00:18:02,250 --> 00:18:04,710
to consider over the course
of the next few years

393
00:18:04,710 --> 00:18:08,760
as the pace of innovation
gets faster and faster

394
00:18:08,760 --> 00:18:11,190
on the side of the practitioners.

395
00:18:11,190 --> 00:18:13,110
We need to give regulators the tools

396
00:18:13,110 --> 00:18:16,110
to be able to fight this kind of a battle.

397
00:18:16,110 --> 00:18:19,980
And unless we pass
legislation to help them,

398
00:18:19,980 --> 00:18:23,040
I think it's gonna be
a very one-sided race.

399
00:18:23,040 --> 00:18:25,380
Let me give you an example
of one piece of legislation

400
00:18:25,380 --> 00:18:26,940
that I think we have to consider.

401
00:18:26,940 --> 00:18:30,990
Data is an incredibly powerful currency

402
00:18:30,990 --> 00:18:32,580
in this business.

403
00:18:32,580 --> 00:18:35,970
And so who controls the data?

404
00:18:35,970 --> 00:18:37,560
If I'm a consumer

405
00:18:37,560 --> 00:18:41,190
and I put my data at the disposal

406
00:18:41,190 --> 00:18:44,460
of a particular vendor that's
providing a service for me,

407
00:18:44,460 --> 00:18:46,560
does that vendor have
the right to use that

408
00:18:46,560 --> 00:18:48,030
for any purpose whatsoever,

409
00:18:48,030 --> 00:18:52,140
including purposes that
are detrimental to me?

410
00:18:52,140 --> 00:18:55,890
So this legislation that
needs to be formulated

411
00:18:55,890 --> 00:18:58,800
as to who has the rights to the data,

412
00:18:58,800 --> 00:19:03,120
and if vendors are making
use of customer data,

413
00:19:03,120 --> 00:19:07,830
what can and what can they
not do with that data?

414
00:19:07,830 --> 00:19:10,980
We need to pass legislation
to make this clear

415
00:19:10,980 --> 00:19:14,310
so that we can allow the
data to be used in broader

416
00:19:14,310 --> 00:19:15,510
and more effective ways,

417
00:19:15,510 --> 00:19:18,390
while at the same time
protecting the interests

418
00:19:18,390 --> 00:19:20,700
of those who need protection.

419
00:19:20,700 --> 00:19:23,100
So I think that's the biggest
set of issues right now,

420
00:19:23,100 --> 00:19:26,370
is that the regulators
don't have all of the tools

421
00:19:26,370 --> 00:19:29,040
that they need and they don't
have the budget that they need

422
00:19:29,040 --> 00:19:30,810
to be able to deal with
these kind of issues.

423
00:19:30,810 --> 00:19:32,940
We need to to bite the bullet

424
00:19:32,940 --> 00:19:35,670
and make an investment in
our regulatory infrastructure

425
00:19:35,670 --> 00:19:38,370
to deal with this brave new
world that we're entering.

