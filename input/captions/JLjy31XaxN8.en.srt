1
00:00:08,160 --> 00:00:14,719
so welcome everybody and may I start

2
00:00:11,360 --> 00:00:17,160
with using the word wow I did not expect

3
00:00:14,719 --> 00:00:21,039
for this event we would have about 500

4
00:00:17,160 --> 00:00:23,160
people show up so thank you for spending

5
00:00:21,039 --> 00:00:26,039
your afternoon or evening depending on

6
00:00:23,160 --> 00:00:30,759
how you define it for us uh in the

7
00:00:26,039 --> 00:00:35,160
inaugural fireside chat um with the CTO

8
00:00:30,759 --> 00:00:37,800
of open AI Mira morati and this is the

9
00:00:35,160 --> 00:00:40,160
opening Fireside chart for Circ which is

10
00:00:37,800 --> 00:00:43,239
an important pillar of the College of

11
00:00:40,160 --> 00:00:45,399
computing uh that is about social and

12
00:00:43,239 --> 00:00:48,120
ethical responsibility in

13
00:00:45,399 --> 00:00:50,440
Computing and uh we have here the

14
00:00:48,120 --> 00:00:53,199
inaugural Dean Dan Hatten Locker who

15
00:00:50,440 --> 00:00:55,000
will say more about the college but this

16
00:00:53,199 --> 00:00:57,559
is one of the three important pillars of

17
00:00:55,000 --> 00:01:00,840
the college which has a truly

18
00:00:57,559 --> 00:01:03,359
interdisciplinary goal to bring faculty

19
00:01:00,840 --> 00:01:06,119
and students across all five schools

20
00:01:03,359 --> 00:01:09,840
together and think about these issues

21
00:01:06,119 --> 00:01:12,080
both in research but also our goal is to

22
00:01:09,840 --> 00:01:14,000
have these issues addressed in every

23
00:01:12,080 --> 00:01:17,040
single classroom at

24
00:01:14,000 --> 00:01:19,840
MIT the long-term goal is hopefully as

25
00:01:17,040 --> 00:01:23,000
MIT we will be able to influence policy

26
00:01:19,840 --> 00:01:24,520
in this space both in the US and

27
00:01:23,000 --> 00:01:26,600
hopefully

28
00:01:24,520 --> 00:01:28,720
worldwide before moving on and

29
00:01:26,600 --> 00:01:31,200
introducing Dan I think I want to

30
00:01:28,720 --> 00:01:33,640
mention a few Logistics

31
00:01:31,200 --> 00:01:35,399
first the first part Dan is going to

32
00:01:33,640 --> 00:01:39,840
have a conversation with

33
00:01:35,399 --> 00:01:42,560
mea and for the first about 45 minutes

34
00:01:39,840 --> 00:01:46,159
and then we invite you to go to

35
00:01:42,560 --> 00:01:50,040
slido.com and scan the QR code that you

36
00:01:46,159 --> 00:01:55,280
just uh saw on your screen use firesight

37
00:01:50,040 --> 00:01:57,479
uh- chat f23 to submit your questions

38
00:01:55,280 --> 00:02:01,200
and done live would actually be

39
00:01:57,479 --> 00:02:03,560
discussing your questions with me

40
00:02:01,200 --> 00:02:06,280
on that note it is a real pleasure for

41
00:02:03,560 --> 00:02:08,200
me to introduce Dan Hatten loer who is

42
00:02:06,280 --> 00:02:09,360
the inaugural dean of the Schwartzman

43
00:02:08,200 --> 00:02:12,480
College of

44
00:02:09,360 --> 00:02:15,640
computing we actually brought him back I

45
00:02:12,480 --> 00:02:19,120
should say here from Cordell Tech where

46
00:02:15,640 --> 00:02:21,680
he basically was uh was helped found

47
00:02:19,120 --> 00:02:24,800
Cordell Tech which was about digital

48
00:02:21,680 --> 00:02:27,800
technology and oriented to graduate

49
00:02:24,800 --> 00:02:31,560
school in New York City he was the first

50
00:02:27,800 --> 00:02:34,000
Dean and the Vice Pro most and actually

51
00:02:31,560 --> 00:02:37,840
he came back because he received both

52
00:02:34,000 --> 00:02:39,640
his Masters and his PhD from MIT so it's

53
00:02:37,840 --> 00:02:42,519
wonderful to be able to work with him

54
00:02:39,640 --> 00:02:45,120
and his Deputy Dean ASAS daglar and

55
00:02:42,519 --> 00:02:47,400
before actually uh leaving the floor to

56
00:02:45,120 --> 00:02:50,480
him I also want to introduce my partner

57
00:02:47,400 --> 00:02:54,050
in crime Casper hair from the philosophy

58
00:02:50,480 --> 00:02:55,280
Department who is the other associate de

59
00:02:54,050 --> 00:02:58,239
[Applause]

60
00:02:55,280 --> 00:03:00,239
forer and I also want to extend a big

61
00:02:58,239 --> 00:03:04,319
thank you to Corey Harris who's been

62
00:03:00,239 --> 00:03:07,480
running around helping us with this and

63
00:03:04,319 --> 00:03:09,920
Terry Park Yumi and all the people that

64
00:03:07,480 --> 00:03:11,920
are making this possible on that note I

65
00:03:09,920 --> 00:03:14,239
think we want to listen to Dan and ver

66
00:03:11,920 --> 00:03:14,239
thank you

67
00:03:16,560 --> 00:03:21,400
Dan so I'm I'm really delighted to have

68
00:03:19,159 --> 00:03:23,080
this opportunity for us to speak today

69
00:03:21,400 --> 00:03:26,920
with mea Mara thank you so much for

70
00:03:23,080 --> 00:03:28,560
joining us Mira um I put together a few

71
00:03:26,920 --> 00:03:30,599
questions for our discussion at the

72
00:03:28,560 --> 00:03:32,319
beginning that were informed already by

73
00:03:30,599 --> 00:03:34,920
things that people in the audience had

74
00:03:32,319 --> 00:03:37,120
sent at the time that they registered uh

75
00:03:34,920 --> 00:03:38,840
but let me just quickly give a a little

76
00:03:37,120 --> 00:03:40,080
bit of a quick bio here for people

77
00:03:38,840 --> 00:03:41,239
although I think you don't really need

78
00:03:40,080 --> 00:03:43,680
much of an

79
00:03:41,239 --> 00:03:46,519
introduction um so Mir is the chief

80
00:03:43,680 --> 00:03:49,080
technology officer of open AI uh the

81
00:03:46,519 --> 00:03:51,319
company behind chat GPT and Del she

82
00:03:49,080 --> 00:03:53,120
leads the company's research product and

83
00:03:51,319 --> 00:03:55,640
safety teams which work together to

84
00:03:53,120 --> 00:03:57,319
create safe and Powerful AI that

85
00:03:55,640 --> 00:04:00,760
benefits humanity and has a positive

86
00:03:57,319 --> 00:04:03,239
impact on the world mea and her team are

87
00:04:00,760 --> 00:04:05,519
pushing the frontiers of AI of what the

88
00:04:03,239 --> 00:04:07,319
of what AI can do aiming to make it

89
00:04:05,519 --> 00:04:09,920
safer and aligning with human intentions

90
00:04:07,319 --> 00:04:12,519
and values uh I wanted to say a little

91
00:04:09,920 --> 00:04:14,519
piece about meera's career path and I'll

92
00:04:12,519 --> 00:04:17,320
come back to that at the end of my

93
00:04:14,519 --> 00:04:19,479
questions because uh I think for all of

94
00:04:17,320 --> 00:04:21,600
the students in the room here that's uh

95
00:04:19,479 --> 00:04:23,720
that's uh it's it's quite interesting so

96
00:04:21,600 --> 00:04:26,000
she managed the product and Engineering

97
00:04:23,720 --> 00:04:27,400
teams at Leap Motion led the design

98
00:04:26,000 --> 00:04:31,000
development and launch of vehicle

99
00:04:27,400 --> 00:04:33,600
products at Tesla including the model X

100
00:04:31,000 --> 00:04:35,960
and uh also was involved in Innovative

101
00:04:33,600 --> 00:04:37,199
programs in Aerospace and she earned her

102
00:04:35,960 --> 00:04:39,080
bachelor's degree in engineering from

103
00:04:37,199 --> 00:04:41,960
the dart myth School of

104
00:04:39,080 --> 00:04:43,639
Engineering uh so with that uh I thought

105
00:04:41,960 --> 00:04:45,199
I'd you know start with a few questions

106
00:04:43,639 --> 00:04:47,280
here and the first thing M I just wanted

107
00:04:45,199 --> 00:04:49,120
to give you a chance to comment on a

108
00:04:47,280 --> 00:04:51,759
little bit maybe to sort of level set

109
00:04:49,120 --> 00:04:54,919
the audience was just uh you had quite a

110
00:04:51,759 --> 00:04:58,600
number of announcements this week uh and

111
00:04:54,919 --> 00:05:01,240
um you know so maybe maybe we'll focus

112
00:04:58,600 --> 00:05:03,919
on on on the on the GPT Builder

113
00:05:01,240 --> 00:05:06,039
interface and sort of uh really sort of

114
00:05:03,919 --> 00:05:08,520
a platform for creating your own chat

115
00:05:06,039 --> 00:05:10,120
GPT including for people with no coding

116
00:05:08,520 --> 00:05:13,160
knowledge although that shouldn't be a

117
00:05:10,120 --> 00:05:14,960
problem in this room uh and and sort of

118
00:05:13,160 --> 00:05:19,080
how you're thinking about that with

119
00:05:14,960 --> 00:05:20,800
respect also uh to pray paying creators

120
00:05:19,080 --> 00:05:22,639
uh for their work since that seemed to

121
00:05:20,800 --> 00:05:27,319
also be a a piece of what you were

122
00:05:22,639 --> 00:05:30,479
talking about with the with the GPT

123
00:05:27,319 --> 00:05:33,400
Builder hi everyone can you hear me okay

124
00:05:30,479 --> 00:05:36,039
yep that's great thanks well thanks so

125
00:05:33,400 --> 00:05:40,199
much for having me very excited to be

126
00:05:36,039 --> 00:05:43,520
here um so yeah as you know we have

127
00:05:40,199 --> 00:05:46,520
first day event on Monday

128
00:05:43,520 --> 00:05:51,000
and announced a lot of new things on

129
00:05:46,520 --> 00:05:54,080
thep but also a new prodct like you

130
00:05:51,000 --> 00:05:56,479
mentioned were calling TOs and it's

131
00:05:54,080 --> 00:06:02,400
really you know a step in this direction

132
00:05:56,479 --> 00:06:05,360
of building um agents that that um help

133
00:06:02,400 --> 00:06:08,280
help us collaborate um and basically

134
00:06:05,360 --> 00:06:10,280
teach us really how to collaborate with

135
00:06:08,280 --> 00:06:12,880
AI system and they're going to be

136
00:06:10,280 --> 00:06:17,560
everywhere you know in our uh creative

137
00:06:12,880 --> 00:06:20,039
spaces in our U um you know Works spaces

138
00:06:17,560 --> 00:06:23,800
and really I think something that we

139
00:06:20,039 --> 00:06:26,520
won't be able to leave without um but

140
00:06:23,800 --> 00:06:28,880
imagining future where you know the

141
00:06:26,520 --> 00:06:32,360
systems are everywhere and operate

142
00:06:28,880 --> 00:06:35,960
autonomous ly is both really amazing but

143
00:06:32,360 --> 00:06:38,440
comes also with a lot of um risks and

144
00:06:35,960 --> 00:06:41,960
trepidations and so we need to move

145
00:06:38,440 --> 00:06:44,720
carefully towards that world and try to

146
00:06:41,960 --> 00:06:48,800
understand um iteratively what it looks

147
00:06:44,720 --> 00:06:51,680
like and build a more robust reliable um

148
00:06:48,800 --> 00:06:54,120
and thoughtful path as it's going to

149
00:06:51,680 --> 00:06:57,120
have a lot of implications for society

150
00:06:54,120 --> 00:07:00,440
and so we want to make sure that this

151
00:06:57,120 --> 00:07:02,879
journey towards a world where agents are

152
00:07:00,440 --> 00:07:04,759
you know everywhere um AI agents are

153
00:07:02,879 --> 00:07:08,560
everywhere is

154
00:07:04,759 --> 00:07:12,639
one that is allows for continuous

155
00:07:08,560 --> 00:07:15,520
adaptation to this technology um and so

156
00:07:12,639 --> 00:07:18,000
in our strategy of iterative deployment

157
00:07:15,520 --> 00:07:21,240
we decided to roll out the first

158
00:07:18,000 --> 00:07:24,599
iteration and learned from how people

159
00:07:21,240 --> 00:07:26,800
are using um the gpts and what they're

160
00:07:24,599 --> 00:07:29,560
doing with them and just in the first

161
00:07:26,800 --> 00:07:33,680
couple of days um we have seen a lot lot

162
00:07:29,560 --> 00:07:36,560
of fun and creative uses of it um a lot

163
00:07:33,680 --> 00:07:40,039
of attention On Tools like browsing and

164
00:07:36,560 --> 00:07:42,680
Del so it's just all really fun to

165
00:07:40,039 --> 00:07:45,639
see so I'm gonna I'm gonna already go

166
00:07:42,680 --> 00:07:47,479
off the script of the questions I had

167
00:07:45,639 --> 00:07:49,680
prepared just because when you were

168
00:07:47,479 --> 00:07:52,879
mentioning uh you know sort of

169
00:07:49,680 --> 00:07:55,720
collaboration of humans and and

170
00:07:52,879 --> 00:07:57,800
gpts uh this is this is something that

171
00:07:55,720 --> 00:07:59,720
uh some of us at MIT have been spending

172
00:07:57,800 --> 00:08:03,800
quite a bit of time thinking about out I

173
00:07:59,720 --> 00:08:06,879
think often today's AI is framed sort of

174
00:08:03,800 --> 00:08:09,080
as AI replacing people and if you go

175
00:08:06,879 --> 00:08:12,680
back all the way to

176
00:08:09,080 --> 00:08:14,360
1960 uh uh you know long before pretty

177
00:08:12,680 --> 00:08:16,879
much everyone in this room except me

178
00:08:14,360 --> 00:08:16,879
probably was

179
00:08:17,199 --> 00:08:22,360
born and including on the zoom I'll

180
00:08:19,360 --> 00:08:24,879
include you in the room era uh um jcr

181
00:08:22,360 --> 00:08:27,240
lick lighter at MIT actually was already

182
00:08:24,879 --> 00:08:29,599
postulating that the real value from

183
00:08:27,240 --> 00:08:31,839
machine intelligence was going to be

184
00:08:29,599 --> 00:08:34,479
that the collaboration of humans and

185
00:08:31,839 --> 00:08:38,399
machines would produce results that are

186
00:08:34,479 --> 00:08:41,240
better than either one alone and so uh

187
00:08:38,399 --> 00:08:43,240
and so I think uh just be great to hear

188
00:08:41,240 --> 00:08:45,480
your your thought you know since you're

189
00:08:43,240 --> 00:08:47,680
now really sort of starting to focus on

190
00:08:45,480 --> 00:08:49,320
collaborations and and tools for people

191
00:08:47,680 --> 00:08:51,760
really collaboratively creating these

192
00:08:49,320 --> 00:08:54,800
interactive things just is that changing

193
00:08:51,760 --> 00:08:57,200
your framing from you know sort of AI as

194
00:08:54,800 --> 00:08:59,279
a simulation of people to you know sort

195
00:08:57,200 --> 00:09:00,680
of replace them to something as a really

196
00:08:59,279 --> 00:09:03,240
collaborative tool where we're trying to

197
00:09:00,680 --> 00:09:04,880
get better outcomes than from AI by

198
00:09:03,240 --> 00:09:07,320
itself or people by

199
00:09:04,880 --> 00:09:10,240
themselves yeah I think you

200
00:09:07,320 --> 00:09:12,279
know for a significant period of time

201
00:09:10,240 --> 00:09:14,920
we're talking about

202
00:09:12,279 --> 00:09:19,399
collaboration with AI this hybrid

203
00:09:14,920 --> 00:09:23,480
interactions and um really using AI

204
00:09:19,399 --> 00:09:26,360
systems as tools to elevate us and uh

205
00:09:23,480 --> 00:09:29,640
you know in our day-to-day job but also

206
00:09:26,360 --> 00:09:31,959
in our creativity and you know maybe not

207
00:09:29,640 --> 00:09:33,920
everyone wants to work 40 hours a week

208
00:09:31,959 --> 00:09:36,800
maybe some people want to work 5 hours a

209
00:09:33,920 --> 00:09:41,040
week and so like making things like that

210
00:09:36,800 --> 00:09:43,920
possible that maybe before we not um but

211
00:09:41,040 --> 00:09:47,519
I really believe that we can have this

212
00:09:43,920 --> 00:09:50,519
future where for quite some time AI

213
00:09:47,519 --> 00:09:52,079
systems are tools that we collaborate

214
00:09:50,519 --> 00:09:56,920
with and then at some point you know

215
00:09:52,079 --> 00:09:59,240
they will get Advanced enough and um

216
00:09:56,920 --> 00:10:02,640
there is a question of you know how

217
00:09:59,240 --> 00:10:05,800
exactly we interact with these AI

218
00:10:02,640 --> 00:10:09,040
systems and what do we want them to do

219
00:10:05,800 --> 00:10:12,040
and what do humans focus on um but I

220
00:10:09,040 --> 00:10:14,920
think for quite some time the goal is to

221
00:10:12,040 --> 00:10:17,920
give people access to these tools and

222
00:10:14,920 --> 00:10:19,640
make it very easy to use them and it's

223
00:10:17,920 --> 00:10:22,640
great if you can program but if you

224
00:10:19,640 --> 00:10:25,920
cannot program then you should just be

225
00:10:22,640 --> 00:10:28,519
able to program in natural language and

226
00:10:25,920 --> 00:10:31,120
that's what people can do with gpts so

227
00:10:28,519 --> 00:10:33,640
we showed demo on Monday where a few

228
00:10:31,120 --> 00:10:37,639
demos actually where just in a couple of

229
00:10:33,640 --> 00:10:39,959
minutes you can program gbts and natural

230
00:10:37,639 --> 00:10:43,399
language to assist you with various

231
00:10:39,959 --> 00:10:46,519
tasks and that's what we want to be able

232
00:10:43,399 --> 00:10:50,920
to do to bring the powers of

233
00:10:46,519 --> 00:10:54,079
um U this these superpowers to to humans

234
00:10:50,920 --> 00:10:56,360
um and anyone can just you know build

235
00:10:54,079 --> 00:10:59,600
these tools and use their creativity

236
00:10:56,360 --> 00:11:02,959
they can prototype stuff very quickly

237
00:10:59,600 --> 00:11:04,959
communicates with um team members like

238
00:11:02,959 --> 00:11:07,519
you know designers or product managers

239
00:11:04,959 --> 00:11:10,320
can communicate their ideas very easily

240
00:11:07,519 --> 00:11:13,600
to the engineering team they can iterate

241
00:11:10,320 --> 00:11:15,760
very quickly um so that's sort of how I

242
00:11:13,600 --> 00:11:18,440
imagine the DAT to interactions to be

243
00:11:15,760 --> 00:11:22,600
and you know in a way we're already

244
00:11:18,440 --> 00:11:25,880
there in the way that we use CH GPT or

245
00:11:22,600 --> 00:11:28,240
um you know code interpreter or uh

246
00:11:25,880 --> 00:11:30,040
co-pilot that we launched a couple of

247
00:11:28,240 --> 00:11:33,680
years ago in CL collaboration with

248
00:11:30,040 --> 00:11:37,120
GitHub um so I think these are some good

249
00:11:33,680 --> 00:11:40,680
examples that give you a sense of how

250
00:11:37,120 --> 00:11:42,399
we're already collaborating and working

251
00:11:40,680 --> 00:11:44,800
with the systems and then there is a

252
00:11:42,399 --> 00:11:45,760
question of you know over time how much

253
00:11:44,800 --> 00:11:51,079
do we

254
00:11:45,760 --> 00:11:53,839
delegate and um the nature of the tasks

255
00:11:51,079 --> 00:11:57,800
possibly becomes more and more abstract

256
00:11:53,839 --> 00:12:01,160
um but for now I think we're really

257
00:11:57,800 --> 00:12:04,320
automating the part of that's kind of

258
00:12:01,160 --> 00:12:07,440
repetitive of our tasks and and also

259
00:12:04,320 --> 00:12:08,639
helping using this these tools to help

260
00:12:07,440 --> 00:12:11,680
with

261
00:12:08,639 --> 00:12:15,199
brainstorming um or usually I Define it

262
00:12:11,680 --> 00:12:17,399
as like the first step of any particular

263
00:12:15,199 --> 00:12:21,000
thing that you're doing whether you're

264
00:12:17,399 --> 00:12:23,399
an accountant or analyst or a consultant

265
00:12:21,000 --> 00:12:27,639
or a designer it's really that like

266
00:12:23,399 --> 00:12:30,320
first step um that the AI systems can be

267
00:12:27,639 --> 00:12:32,279
quite helpful with today

268
00:12:30,320 --> 00:12:36,120
great thanks so I'm going to go to some

269
00:12:32,279 --> 00:12:37,920
of the uh things that came in uh

270
00:12:36,120 --> 00:12:39,920
beforehand from people in the audience

271
00:12:37,920 --> 00:12:42,199
and I had a chance to review and try to

272
00:12:39,920 --> 00:12:45,959
kind of group together in some quasy

273
00:12:42,199 --> 00:12:48,440
coherent way so uh so we seem to be in a

274
00:12:45,959 --> 00:12:50,839
world of you know sort of AI optimists

275
00:12:48,440 --> 00:12:54,160
on one extreme and AI pessimists on the

276
00:12:50,839 --> 00:12:55,440
other uh where do you place yourself in

277
00:12:54,160 --> 00:12:57,839
that and how does that affect your

278
00:12:55,440 --> 00:13:02,040
leadership at open

279
00:12:57,839 --> 00:13:06,000
AI um I'm definitely optimistic in

280
00:13:02,040 --> 00:13:09,720
general and I think it's very important

281
00:13:06,000 --> 00:13:13,360
to see to have a sense of

282
00:13:09,720 --> 00:13:14,839
possibility and optimism because you

283
00:13:13,360 --> 00:13:18,680
know things have a sense of being

284
00:13:14,839 --> 00:13:23,000
self-fulfilling as well um and it's

285
00:13:18,680 --> 00:13:25,519
important to have belief and faith that

286
00:13:23,000 --> 00:13:29,199
things are going to go well but not I'm

287
00:13:25,519 --> 00:13:30,040
not saying that one should be optimistic

288
00:13:29,199 --> 00:13:31,880
uh

289
00:13:30,040 --> 00:13:35,040
blindly

290
00:13:31,880 --> 00:13:38,920
and with that optimism comes also you

291
00:13:35,040 --> 00:13:42,079
know a large degree of preparation and

292
00:13:38,920 --> 00:13:45,240
being quite I'd say cleared eyed and

293
00:13:42,079 --> 00:13:48,720
open eyed about all the challenges ahead

294
00:13:45,240 --> 00:13:50,800
and there's you know a set of immediate

295
00:13:48,720 --> 00:13:52,959
urgent challenges that we're facing as

296
00:13:50,800 --> 00:13:56,519
we're releasing these Technologies and

297
00:13:52,959 --> 00:13:59,839
trying to make them as accessible um as

298
00:13:56,519 --> 00:14:04,639
possible but also you know

299
00:13:59,839 --> 00:14:05,759
predicting the capabilities and risks um

300
00:14:04,639 --> 00:14:08,839
as we're

301
00:14:05,759 --> 00:14:10,399
preparing to train more and more capable

302
00:14:08,839 --> 00:14:14,360
models

303
00:14:10,399 --> 00:14:17,720
and I think the the foundational thing

304
00:14:14,360 --> 00:14:19,839
uh kind of in the medium term is really

305
00:14:17,720 --> 00:14:23,399
building the science of prediction and

306
00:14:19,839 --> 00:14:26,360
we did this with with with gbd4 and

307
00:14:23,399 --> 00:14:28,880
we're betting more and more on this

308
00:14:26,360 --> 00:14:31,600
understanding how to predict the cap

309
00:14:28,880 --> 00:14:34,600
abilities of the models that we are

310
00:14:31,600 --> 00:14:37,399
building and with that also preparing

311
00:14:34,600 --> 00:14:41,680
for red teaming preparing for the risks

312
00:14:37,399 --> 00:14:44,160
ahead and how to mitigate um the risks

313
00:14:41,680 --> 00:14:46,920
that are brought along with more

314
00:14:44,160 --> 00:14:49,000
powerful capabilities and it's sort of

315
00:14:46,920 --> 00:14:51,639
like you know I would call that like

316
00:14:49,000 --> 00:14:54,120
kind of medium-term risk or as as we go

317
00:14:51,639 --> 00:14:57,279
from urgent things to more more

318
00:14:54,120 --> 00:15:00,680
long-term uh capabilities and then in

319
00:14:57,279 --> 00:15:04,759
the far end of course we're facing

320
00:15:00,680 --> 00:15:07,440
questions around um catastrophic risk

321
00:15:04,759 --> 00:15:10,240
and you know the systems that are

322
00:15:07,440 --> 00:15:13,880
self-improving and extremely capable and

323
00:15:10,240 --> 00:15:16,079
so how do we get ahead of that what do

324
00:15:13,880 --> 00:15:18,560
we need to understand what kind of

325
00:15:16,079 --> 00:15:22,480
research do we need to be doing today to

326
00:15:18,560 --> 00:15:26,040
be prepared for this um

327
00:15:22,480 --> 00:15:28,120
unknowns and we've you know at open and

328
00:15:26,040 --> 00:15:29,959
I we've invested quite a bit in what we

329
00:15:28,120 --> 00:15:33,120
call Super alignment and so it's this

330
00:15:29,959 --> 00:15:34,920
idea of doing a lot of research to

331
00:15:33,120 --> 00:15:38,120
understand how we align this very

332
00:15:34,920 --> 00:15:42,800
capable increasingly capable models that

333
00:15:38,120 --> 00:15:48,600
eventually become you know AGI or ASI uh

334
00:15:42,800 --> 00:15:51,759
eventually and this is a extremely hard

335
00:15:48,600 --> 00:15:54,680
technical Challenge and we're investing

336
00:15:51,759 --> 00:15:57,880
quite a bit from compute perspective but

337
00:15:54,680 --> 00:16:00,800
also uh in terms of just hiring great

338
00:15:57,880 --> 00:16:03,639
team of research Searchers and giving

339
00:16:00,800 --> 00:16:05,160
them space to focus on some of these

340
00:16:03,639 --> 00:16:09,160
very

341
00:16:05,160 --> 00:16:12,680
uncertain um and difficult research

342
00:16:09,160 --> 00:16:13,920
agendas so in terms of uh given the

343
00:16:12,680 --> 00:16:16,839
importance of

344
00:16:13,920 --> 00:16:19,240
prediction uh of uh given the importance

345
00:16:16,839 --> 00:16:21,199
of prediction being able to predict

346
00:16:19,240 --> 00:16:24,040
risks that you were talking about do you

347
00:16:21,199 --> 00:16:26,560
have uh you know to share with people

348
00:16:24,040 --> 00:16:28,920
who are sort of not involved in you know

349
00:16:26,560 --> 00:16:32,440
what you're doing inside the company

350
00:16:28,920 --> 00:16:34,800
sort of examples of things where uh you

351
00:16:32,440 --> 00:16:36,839
know either you were you know surprised

352
00:16:34,800 --> 00:16:39,120
one way or the other in terms of your

353
00:16:36,839 --> 00:16:41,560
either ability or inability to predict

354
00:16:39,120 --> 00:16:44,240
risks and sort of how how should the

355
00:16:41,560 --> 00:16:47,000
World At Large think about uh risk

356
00:16:44,240 --> 00:16:49,000
prediction and how well that

357
00:16:47,000 --> 00:16:52,199
works yeah

358
00:16:49,000 --> 00:16:55,720
so you know if you think about gbt 4 it

359
00:16:52,199 --> 00:17:00,240
was a brand new frontier uh Model A year

360
00:16:55,720 --> 00:17:03,839
ago and we did very precise capability

361
00:17:00,240 --> 00:17:06,439
predictions ahead of time um we put in

362
00:17:03,839 --> 00:17:10,280
six months of work into safety before

363
00:17:06,439 --> 00:17:14,120
releasing doing a lot of red teaming um

364
00:17:10,280 --> 00:17:16,720
doing a ton of evaluations and uh then

365
00:17:14,120 --> 00:17:20,120
we proceeded to do this very controlled

366
00:17:16,720 --> 00:17:23,520
deployment where we rolled out to a set

367
00:17:20,120 --> 00:17:25,720
of trusted users understood what they

368
00:17:23,520 --> 00:17:29,200
were able to do and what some of the

369
00:17:25,720 --> 00:17:31,480
risks of that were and then over time as

370
00:17:29,200 --> 00:17:34,320
we build more confidence about being

371
00:17:31,480 --> 00:17:36,440
able to handle the risks and mitigating

372
00:17:34,320 --> 00:17:41,679
them we broadened up access to more and

373
00:17:36,440 --> 00:17:46,760
more users um but yeah I think in terms

374
00:17:41,679 --> 00:17:49,440
of our core safety decision has been

375
00:17:46,760 --> 00:17:52,160
really building the science of uh

376
00:17:49,440 --> 00:17:53,400
prediction and following the strategy of

377
00:17:52,160 --> 00:17:57,400
iterative

378
00:17:53,400 --> 00:18:01,000
deployment um before we even

379
00:17:57,400 --> 00:18:03,520
released big models um so let's say gbt

380
00:18:01,000 --> 00:18:06,240
3 for example we had been studying

381
00:18:03,520 --> 00:18:10,080
misinformation for a really long time

382
00:18:06,240 --> 00:18:11,679
and um but we didn't have you know first

383
00:18:10,080 --> 00:18:15,280
experience with it because we had never

384
00:18:11,679 --> 00:18:18,440
deployed such models and so before we

385
00:18:15,280 --> 00:18:21,440
released gbd3 we were extremely worried

386
00:18:18,440 --> 00:18:24,760
about um the effects that it could have

387
00:18:21,440 --> 00:18:27,280
on elections and just misinformation at

388
00:18:24,760 --> 00:18:29,120
large on the internet and not that

389
00:18:27,280 --> 00:18:31,679
that's so important that's extremely

390
00:18:29,120 --> 00:18:33,919
important but what we saw from actual

391
00:18:31,679 --> 00:18:38,600
deployment was that a bigger much bigger

392
00:18:33,919 --> 00:18:43,480
risk was um you know spamming content

393
00:18:38,600 --> 00:18:45,559
which is you know still bad but perhaps

394
00:18:43,480 --> 00:18:48,960
uh not as bad and not exactly what we

395
00:18:45,559 --> 00:18:51,559
expected so we kind of reoriented our

396
00:18:48,960 --> 00:18:52,320
work then to be a to to focus on on

397
00:18:51,559 --> 00:18:55,600
those

398
00:18:52,320 --> 00:18:57,039
mitigations um but I would say you know

399
00:18:55,600 --> 00:18:59,000
a lot of the things that we're dealing

400
00:18:57,039 --> 00:19:01,840
with right now

401
00:18:59,000 --> 00:19:04,799
uh in terms of risks to some degree we

402
00:19:01,840 --> 00:19:06,799
had thought about and had predicted them

403
00:19:04,799 --> 00:19:08,919
many many years ago like let's say even

404
00:19:06,799 --> 00:19:11,919
four or five years ago but you don't

405
00:19:08,919 --> 00:19:15,559
know exactly the degree to which the

406
00:19:11,919 --> 00:19:19,480
problem will be pervasive important

407
00:19:15,559 --> 00:19:23,600
critical and um how precisely it will

408
00:19:19,480 --> 00:19:28,360
impact things like for example education

409
00:19:23,600 --> 00:19:31,640
is um something that you know we knew

410
00:19:28,360 --> 00:19:35,159
bt4 and Char gbt would have some effect

411
00:19:31,640 --> 00:19:40,840
but we didn't know exactly how so we

412
00:19:35,159 --> 00:19:43,000
started studying Provence um of uh of

413
00:19:40,840 --> 00:19:46,919
content and being able to detect what's

414
00:19:43,000 --> 00:19:49,640
sayi generated what's not I generated um

415
00:19:46,919 --> 00:19:52,760
but we don't understand you know the

416
00:19:49,640 --> 00:19:55,640
impact that these Technologies are

417
00:19:52,760 --> 00:19:58,559
having on the education system and how

418
00:19:55,640 --> 00:20:01,840
things will evolve over time and so we

419
00:19:58,559 --> 00:20:03,799
know there is some some sort of impact

420
00:20:01,840 --> 00:20:06,360
but we don't know exactly what how and

421
00:20:03,799 --> 00:20:09,760
so it's very important to actually

422
00:20:06,360 --> 00:20:13,400
continuously study this and be able to

423
00:20:09,760 --> 00:20:16,080
um react and be able to kind of uh

424
00:20:13,400 --> 00:20:19,320
iterate on on the strategy on the

425
00:20:16,080 --> 00:20:22,200
technology um work with different parts

426
00:20:19,320 --> 00:20:25,320
of society that are uh affected by it

427
00:20:22,200 --> 00:20:26,679
and figure out some ways to gather input

428
00:20:25,320 --> 00:20:28,360
not just from people that use the

429
00:20:26,679 --> 00:20:30,600
technology but also people that are

430
00:20:28,360 --> 00:20:33,880
affected by it and are not necessarily

431
00:20:30,600 --> 00:20:36,400
using it so it is quite complex in that

432
00:20:33,880 --> 00:20:39,000
sense because you know you need to

433
00:20:36,400 --> 00:20:41,280
coordinate so many aspects it's a

434
00:20:39,000 --> 00:20:43,000
technology that affects everything it's

435
00:20:41,280 --> 00:20:46,640
such a

436
00:20:43,000 --> 00:20:49,679
fundamental uh unit of things and

437
00:20:46,640 --> 00:20:52,159
changes the fabric of society so it's

438
00:20:49,679 --> 00:20:54,760
you know it's like Global coordination

439
00:20:52,159 --> 00:20:58,559
task it's just a really huge

440
00:20:54,760 --> 00:21:00,240
task and so maybe this is to the

441
00:20:58,559 --> 00:21:02,120
education space I'd thought about it as

442
00:21:00,240 --> 00:21:05,679
a broader question before based on what

443
00:21:02,120 --> 00:21:08,799
people said but um like what role do you

444
00:21:05,679 --> 00:21:10,640
see it's a lot to take on in addition to

445
00:21:08,799 --> 00:21:13,120
developing these models and testing

446
00:21:10,640 --> 00:21:15,279
these models and predicting things to

447
00:21:13,120 --> 00:21:17,880
really think through impact in various

448
00:21:15,279 --> 00:21:21,000
different domains to what degree do you

449
00:21:17,880 --> 00:21:22,840
see you know collaborating with Academia

450
00:21:21,000 --> 00:21:25,840
or with other kinds of

451
00:21:22,840 --> 00:21:27,840
organizations uh who might be able to do

452
00:21:25,840 --> 00:21:30,200
things like help study how these things

453
00:21:27,840 --> 00:21:33,120
are being used and can be used in K12

454
00:21:30,200 --> 00:21:34,279
education or in other places um are

455
00:21:33,120 --> 00:21:36,880
those things that you're looking to do

456
00:21:34,279 --> 00:21:38,760
to sort of build uh you know either with

457
00:21:36,880 --> 00:21:41,559
other companies sort of non-competitive

458
00:21:38,760 --> 00:21:43,200
consortia or with Academia to to study

459
00:21:41,559 --> 00:21:45,640
some of these broader kinds of questions

460
00:21:43,200 --> 00:21:47,440
to give you sort of a a broader platform

461
00:21:45,640 --> 00:21:52,240
for thinking about

462
00:21:47,440 --> 00:21:54,919
risk yes for sure I mean you know we of

463
00:21:52,240 --> 00:21:57,120
course have a lot of responsibility as

464
00:21:54,919 --> 00:22:00,360
developers of the technology but it's

465
00:21:57,120 --> 00:22:04,520
not uh really possible for a small group

466
00:22:00,360 --> 00:22:06,840
of people in an organization to um to

467
00:22:04,520 --> 00:22:09,000
figure out how technology will be

468
00:22:06,840 --> 00:22:11,480
absorbed by society and the entire

469
00:22:09,000 --> 00:22:13,760
infrastructure that needs to be built to

470
00:22:11,480 --> 00:22:16,840
make this you know just really

471
00:22:13,760 --> 00:22:20,679
beneficial to everyone and so I think

472
00:22:16,840 --> 00:22:24,840
one of the big benefits of chbt was

473
00:22:20,679 --> 00:22:27,440
really bringing AI the effects of AI and

474
00:22:24,840 --> 00:22:30,240
the experience of AI in the collective

475
00:22:27,440 --> 00:22:34,360
consciousness because before then I

476
00:22:30,240 --> 00:22:37,559
think people obviously had a sense um

477
00:22:34,360 --> 00:22:39,960
but not everyone could just experience

478
00:22:37,559 --> 00:22:42,720
the power of these very Advanced AI

479
00:22:39,960 --> 00:22:44,960
systems and know what it feels like and

480
00:22:42,720 --> 00:22:47,039
I think that had an incredible effect

481
00:22:44,960 --> 00:22:50,760
because people weren't just reading

482
00:22:47,039 --> 00:22:52,840
about it um or yeah hearing about it

483
00:22:50,760 --> 00:22:57,159
they could just feel it experience it

484
00:22:52,840 --> 00:22:58,480
for themselves and um I think this

485
00:22:57,159 --> 00:23:00,480
really pushed

486
00:22:58,480 --> 00:23:04,000
governments all over the world into

487
00:23:00,480 --> 00:23:06,960
action to think about how to coordinate

488
00:23:04,000 --> 00:23:08,760
organize govern AI systems and we've

489
00:23:06,960 --> 00:23:13,880
seen actually a ton of moment in the

490
00:23:08,760 --> 00:23:17,520
past year um I mean last week the UK uh

491
00:23:13,880 --> 00:23:20,080
uh AI Summit was held where we the all

492
00:23:17,520 --> 00:23:24,640
all the major AI developers were there

493
00:23:20,080 --> 00:23:27,679
and um thinking about you know voluntary

494
00:23:24,640 --> 00:23:28,480
commitments that organizations can make

495
00:23:27,679 --> 00:23:31,840
and

496
00:23:28,480 --> 00:23:35,640
so I think this is a really inflection

497
00:23:31,840 --> 00:23:38,200
point for AI governance um and that's

498
00:23:35,640 --> 00:23:41,240
because it has it has sort of penetrated

499
00:23:38,200 --> 00:23:44,559
the the collective Consciousness and

500
00:23:41,240 --> 00:23:47,039
there was an important moment now you

501
00:23:44,559 --> 00:23:49,960
know when it comes to our role and how

502
00:23:47,039 --> 00:23:53,760
we work with various organizations Civil

503
00:23:49,960 --> 00:23:56,960
Society Academia and so on we try to do

504
00:23:53,760 --> 00:24:00,360
as much as possible and we collaborate

505
00:23:56,960 --> 00:24:03,919
with organizations like code.org or KH

506
00:24:00,360 --> 00:24:06,559
Academy and uh try to empower them to

507
00:24:03,919 --> 00:24:11,279
build with our technology get their

508
00:24:06,559 --> 00:24:13,520
feedback um and they were actually they

509
00:24:11,279 --> 00:24:18,320
were helping us understand some the very

510
00:24:13,520 --> 00:24:20,159
early use cases around AI tutors um and

511
00:24:18,320 --> 00:24:23,640
and how to make them effective and we

512
00:24:20,159 --> 00:24:25,399
got the the uh feedback from Educators

513
00:24:23,640 --> 00:24:27,840
but now we're also working with a few

514
00:24:25,399 --> 00:24:30,000
universities just to understand how we

515
00:24:27,840 --> 00:24:35,000
fects things in the classroom especially

516
00:24:30,000 --> 00:24:37,520
with ch GPT um and so there is quite a

517
00:24:35,000 --> 00:24:39,760
bit of work that we're doing with

518
00:24:37,520 --> 00:24:43,200
various organizations and I think

519
00:24:39,760 --> 00:24:47,039
there's obviously always more uh that we

520
00:24:43,200 --> 00:24:50,640
can do but our priority is really to

521
00:24:47,039 --> 00:24:54,120
just make it accessible easy to use and

522
00:24:50,640 --> 00:24:56,960
to really just Empower everyone to um be

523
00:24:54,120 --> 00:24:59,559
able to use the technology and start

524
00:24:56,960 --> 00:25:02,520
thinking about how it impacts their

525
00:24:59,559 --> 00:25:04,520
specific domain um as we don't

526
00:25:02,520 --> 00:25:07,039
necessarily have expertise say in the

527
00:25:04,520 --> 00:25:10,159
domain of education but we want to give

528
00:25:07,039 --> 00:25:11,679
people the tools to experience and

529
00:25:10,159 --> 00:25:15,600
understand it for themselves and you

530
00:25:11,679 --> 00:25:17,399
know be able to gather that feedback um

531
00:25:15,600 --> 00:25:20,120
and make the technology more robust or

532
00:25:17,399 --> 00:25:23,279
more helpful so with the race to develop

533
00:25:20,120 --> 00:25:25,960
and release larger models

534
00:25:23,279 --> 00:25:28,880
um how do we stop this from sort of

535
00:25:25,960 --> 00:25:30,600
becoming a race to the bottom on safety

536
00:25:28,880 --> 00:25:34,640
concerns uh you know you sort of

537
00:25:30,600 --> 00:25:36,440
mentioned things like uh you know uh the

538
00:25:34,640 --> 00:25:38,720
recent Bletchley Park people getting

539
00:25:36,440 --> 00:25:42,120
together and talking about this but you

540
00:25:38,720 --> 00:25:45,840
know when when realities of competition

541
00:25:42,120 --> 00:25:48,640
come to Bear often uh these things can

542
00:25:45,840 --> 00:25:50,279
be good intention you know uh somebody

543
00:25:48,640 --> 00:25:52,559
I've learned a lot from in my life likes

544
00:25:50,279 --> 00:25:54,720
to say Good Intentions without

545
00:25:52,559 --> 00:25:57,799
mechanisms don't actually get anything

546
00:25:54,720 --> 00:25:59,760
done uh so how how how do you think

547
00:25:57,799 --> 00:26:03,039
about mechanisms there for

548
00:25:59,760 --> 00:26:04,120
that yeah there are a few things well

549
00:26:03,039 --> 00:26:07,960
one I'd

550
00:26:04,120 --> 00:26:10,440
say this is why we spend a lot of time

551
00:26:07,960 --> 00:26:13,399
thinking about the structure of open AI

552
00:26:10,440 --> 00:26:15,799
sort of anticipating a future where

553
00:26:13,399 --> 00:26:18,880
things would get quite competitive and

554
00:26:15,799 --> 00:26:22,080
AI would be um would just have a very

555
00:26:18,880 --> 00:26:25,240
important place in the economy and so we

556
00:26:22,080 --> 00:26:27,960
uh we we kind of change the structure of

557
00:26:25,240 --> 00:26:30,360
open AI into this limited partner ship

558
00:26:27,960 --> 00:26:32,559
which is still governed by the nonprofit

559
00:26:30,360 --> 00:26:34,480
but it allowed us to then you know raise

560
00:26:32,559 --> 00:26:36,240
money so we could buy the supercomputers

561
00:26:34,480 --> 00:26:38,320
to be able to do our research and our

562
00:26:36,240 --> 00:26:41,960
work um but it's still governed by the

563
00:26:38,320 --> 00:26:46,039
nonprofit and it has a limited cup what

564
00:26:41,960 --> 00:26:49,120
we call limited profit cup and uh and so

565
00:26:46,039 --> 00:26:53,480
that that ensures that you know the

566
00:26:49,120 --> 00:26:57,080
primary uh objective is really achieving

567
00:26:53,480 --> 00:27:00,320
the mission uh which is building AI that

568
00:26:57,080 --> 00:27:02,600
that really benefits all of humanity and

569
00:27:00,320 --> 00:27:04,960
the structure the mechanisms the

570
00:27:02,600 --> 00:27:07,640
incentives are important they're not

571
00:27:04,960 --> 00:27:11,559
everything but they're really important

572
00:27:07,640 --> 00:27:13,240
and so I I'll start with that but then I

573
00:27:11,559 --> 00:27:18,320
think you know as

574
00:27:13,240 --> 00:27:20,679
we're deploying things uh it's also yeah

575
00:27:18,320 --> 00:27:24,840
it's also very important to make sure

576
00:27:20,679 --> 00:27:27,279
that um we get the things around

577
00:27:24,840 --> 00:27:29,080
governance right and so right now

578
00:27:27,279 --> 00:27:31,760
there're a lot of there's a lot of

579
00:27:29,080 --> 00:27:34,159
debate around regulation and we've been

580
00:27:31,760 --> 00:27:36,360
pushing for regulation of Frontier

581
00:27:34,159 --> 00:27:38,399
systems that don't necessarily affect

582
00:27:36,360 --> 00:27:41,279
you know the startups or innovation of

583
00:27:38,399 --> 00:27:43,679
small companies and so on but put some

584
00:27:41,279 --> 00:27:47,200
sort of degree of checks and balances

585
00:27:43,679 --> 00:27:48,600
and Regulatory framework around models

586
00:27:47,200 --> 00:27:52,080
that are really at the

587
00:27:48,600 --> 00:27:54,279
frontier and um encouraging the

588
00:27:52,080 --> 00:27:57,519
government to think

589
00:27:54,279 --> 00:28:00,320
about uh yeah regulating this model

590
00:27:57,519 --> 00:28:03,279
models that have incredible

591
00:28:00,320 --> 00:28:05,399
capabilities in the near term I would

592
00:28:03,279 --> 00:28:09,080
say you know with opening I would just

593
00:28:05,399 --> 00:28:12,240
have a whole set of things uh that help

594
00:28:09,080 --> 00:28:15,039
us think and uh think about how to

595
00:28:12,240 --> 00:28:19,480
deploy safely and responsibly and it's

596
00:28:15,039 --> 00:28:21,640
not really a One Stop uh you know it's

597
00:28:19,480 --> 00:28:24,600
not just something that you add at the

598
00:28:21,640 --> 00:28:28,039
very end it's it's something it it's

599
00:28:24,600 --> 00:28:30,000
it's integrated from the data

600
00:28:28,039 --> 00:28:33,600
uh from point point of view of the data

601
00:28:30,000 --> 00:28:36,360
to into the algorithms into um you know

602
00:28:33,600 --> 00:28:39,880
building the product into actual

603
00:28:36,360 --> 00:28:41,880
deployment and looking at uh how people

604
00:28:39,880 --> 00:28:43,960
are using the AI and then feeding it

605
00:28:41,880 --> 00:28:48,240
back into the technology and having this

606
00:28:43,960 --> 00:28:51,200
full loop um that we continuously

607
00:28:48,240 --> 00:28:52,880
iterate on and that's that's really

608
00:28:51,200 --> 00:28:55,159
important and also providing some

609
00:28:52,880 --> 00:28:56,840
accountability by you know when we

610
00:28:55,159 --> 00:28:59,679
whenever we release some new technology

611
00:28:56,840 --> 00:29:03,000
we have Safety Systems card we do a lot

612
00:28:59,679 --> 00:29:07,120
of red teaming uh often we we publish

613
00:29:03,000 --> 00:29:10,120
the outcomes of of the red teaming um

614
00:29:07,120 --> 00:29:12,519
and so I think you know it's important

615
00:29:10,120 --> 00:29:16,360
for for the industry to have some sort

616
00:29:12,519 --> 00:29:17,720
of Standards around deployment and uh

617
00:29:16,360 --> 00:29:20,000
what we have seen I would say in the

618
00:29:17,720 --> 00:29:23,159
past year past couple of years is that

619
00:29:20,000 --> 00:29:26,279
there is a lot of voluntary standard

620
00:29:23,159 --> 00:29:27,519
setting from the big companies we saw

621
00:29:26,279 --> 00:29:31,600
this a bit with the White House

622
00:29:27,519 --> 00:29:34,720
commitments as well um but yeah it's

623
00:29:31,600 --> 00:29:39,279
important to avoid kind of this race to

624
00:29:34,720 --> 00:29:41,399
the bottom um and I would say yeah

625
00:29:39,279 --> 00:29:45,080
that's that's very important and if we

626
00:29:41,399 --> 00:29:48,320
just focus on sort of the worst outcomes

627
00:29:45,080 --> 00:29:50,320
um I would focus on if if we have to

628
00:29:48,320 --> 00:29:52,279
focus on one thing and like the worst

629
00:29:50,320 --> 00:29:55,039
outcomes I would focus on the frontier

630
00:29:52,279 --> 00:29:58,720
systems and being able to work with

631
00:29:55,039 --> 00:30:01,080
governments and other um a developers to

632
00:29:58,720 --> 00:30:05,159
regulate the frontier

633
00:30:01,080 --> 00:30:08,600
models so um so it's

634
00:30:05,159 --> 00:30:11,720
uh the uh executive order that the Biden

635
00:30:08,600 --> 00:30:15,399
Administration put out uh a couple weeks

636
00:30:11,720 --> 00:30:17,279
ago um certainly I mean it has a lot of

637
00:30:15,399 --> 00:30:19,320
breadth to it uh but part of it is

638
00:30:17,279 --> 00:30:21,360
definitely looking at Foundation models

639
00:30:19,320 --> 00:30:23,919
and what they term dual use Foundation

640
00:30:21,360 --> 00:30:28,360
models that can have uh sort of

641
00:30:23,919 --> 00:30:32,000
potential defense uh kinds of uh uses

642
00:30:28,360 --> 00:30:34,039
you know um and uh how are you thinking

643
00:30:32,000 --> 00:30:36,480
about that executive order and how it's

644
00:30:34,039 --> 00:30:38,159
affecting what you're doing because it

645
00:30:36,480 --> 00:30:40,120
has reporting requirements and other

646
00:30:38,159 --> 00:30:41,919
things how are you thinking about and

647
00:30:40,120 --> 00:30:44,960
preparing for

648
00:30:41,919 --> 00:30:46,840
those yeah it's pretty fresh so we're

649
00:30:44,960 --> 00:30:48,399
studying and looking at it and seeing

650
00:30:46,840 --> 00:30:51,399
you know what it actually means in

651
00:30:48,399 --> 00:30:55,159
practice and what the effect of it would

652
00:30:51,399 --> 00:30:58,000
be um but generally I'm very supportive

653
00:30:55,159 --> 00:31:01,039
of figuring out the appropriate

654
00:30:58,000 --> 00:31:03,240
um regulation around Frontier models I

655
00:31:01,039 --> 00:31:04,399
think it's valuable to think of it in

656
00:31:03,240 --> 00:31:08,240
terms of

657
00:31:04,399 --> 00:31:11,679
capabilities and uh you know building

658
00:31:08,240 --> 00:31:16,000
building the science of predicting these

659
00:31:11,679 --> 00:31:19,519
capabilities and and determining any

660
00:31:16,000 --> 00:31:21,960
thresholds or regulation based on that

661
00:31:19,519 --> 00:31:23,760
uh but this is just all very fresh and

662
00:31:21,960 --> 00:31:26,600
you know internally we're looking at

663
00:31:23,760 --> 00:31:27,799
this and and we're figuring out what it

664
00:31:26,600 --> 00:31:30,480
means and

665
00:31:27,799 --> 00:31:34,480
how uh how how to make sure that it's

666
00:31:30,480 --> 00:31:37,840
something that's practical um you know

667
00:31:34,480 --> 00:31:40,120
you don't want regulation and the actual

668
00:31:37,840 --> 00:31:43,440
science of doing things or uh the

669
00:31:40,120 --> 00:31:45,960
Practical actions to be detached so

670
00:31:43,440 --> 00:31:49,960
um i' say the short answer is that you

671
00:31:45,960 --> 00:31:52,120
know we're still reviewing it but on

672
00:31:49,960 --> 00:31:55,440
principle yeah we definitely are have

673
00:31:52,120 --> 00:31:57,440
been encouraging having some sort of

674
00:31:55,440 --> 00:32:01,039
regulatory framework around Frontier

675
00:31:57,440 --> 00:32:05,200
models so there's another piece of that

676
00:32:01,039 --> 00:32:07,200
uh reg of that executive order that I'll

677
00:32:05,200 --> 00:32:10,799
sort of summarize in the following I

678
00:32:07,200 --> 00:32:15,240
mean it's long but I'll summarize it as

679
00:32:10,799 --> 00:32:18,600
you know uh using AI is not an excuse

680
00:32:15,240 --> 00:32:20,919
for you know breaking existing

681
00:32:18,600 --> 00:32:23,120
regulations and laws so that you know if

682
00:32:20,919 --> 00:32:26,799
you make your Frontier Model available

683
00:32:23,120 --> 00:32:30,480
and somebody else uses it in a way or

684
00:32:26,799 --> 00:32:32,320
either uses or misuses it in a way uh

685
00:32:30,480 --> 00:32:33,840
that's you know counter to other

686
00:32:32,320 --> 00:32:36,399
existing laws or

687
00:32:33,840 --> 00:32:39,840
regulations uh it's sort of not an

688
00:32:36,399 --> 00:32:43,519
excuse um do you see that as something

689
00:32:39,840 --> 00:32:45,919
that exposes you all to risk in terms of

690
00:32:43,519 --> 00:32:47,120
how people use things do you think that

691
00:32:45,919 --> 00:32:49,360
you know that's a part of the

692
00:32:47,120 --> 00:32:52,240
responsibility of these very general

693
00:32:49,360 --> 00:32:54,480
models is to have guard rails in there

694
00:32:52,240 --> 00:32:57,519
that actually help guard against misuse

695
00:32:54,480 --> 00:32:59,639
you know like uh somebody pretending to

696
00:32:57,519 --> 00:33:02,679
be a lawyer or pretending to be a doctor

697
00:32:59,639 --> 00:33:04,279
by sort of you know kind of Consulting a

698
00:33:02,679 --> 00:33:07,039
a generative model or those things that

699
00:33:04,279 --> 00:33:09,080
you think fall within scope how do you

700
00:33:07,039 --> 00:33:12,399
how do you see sort of teasing apart in

701
00:33:09,080 --> 00:33:14,600
some sense uh the amazing capabilities

702
00:33:12,399 --> 00:33:17,360
of these General models and then the

703
00:33:14,600 --> 00:33:19,279
ways that they may be used for very good

704
00:33:17,360 --> 00:33:21,120
purposes but also misused in a lot of

705
00:33:19,279 --> 00:33:25,440
different domains and how the

706
00:33:21,120 --> 00:33:29,159
responsibility there kind of uh shakes

707
00:33:25,440 --> 00:33:31,679
out yeah so I I think it's very sort of

708
00:33:29,159 --> 00:33:35,880
domain specific use case specific like

709
00:33:31,679 --> 00:33:39,279
of course there's some uh Common Sense

710
00:33:35,880 --> 00:33:41,399
guidelines and guard reals around uh you

711
00:33:39,279 --> 00:33:43,440
know what's legal and what's not legal

712
00:33:41,399 --> 00:33:47,240
but a lot of it like a lot of the hard

713
00:33:43,440 --> 00:33:48,919
decisions and the nuan decisions are um

714
00:33:47,240 --> 00:33:54,399
quite details like for example you have

715
00:33:48,919 --> 00:33:57,559
to think you know how how let's say how

716
00:33:54,399 --> 00:33:59,600
is a lawyer using an AI system and you

717
00:33:57,559 --> 00:34:02,360
go through the use cases the various use

718
00:33:59,600 --> 00:34:04,159
cases and you try to understand and like

719
00:34:02,360 --> 00:34:05,600
right now for example there is human

720
00:34:04,159 --> 00:34:07,760
oversight so it's not like you're

721
00:34:05,600 --> 00:34:11,879
delegating all the decision making to an

722
00:34:07,760 --> 00:34:14,079
AI system and there is yeah there is

723
00:34:11,879 --> 00:34:18,599
human oversight what we call human in

724
00:34:14,079 --> 00:34:22,679
the loop and uh you know that we came up

725
00:34:18,599 --> 00:34:24,280
with sort of that uh usage guideline by

726
00:34:22,679 --> 00:34:27,599
understanding how people were actually

727
00:34:24,280 --> 00:34:32,119
using the system and where the gaps were

728
00:34:27,599 --> 00:34:34,040
where the concerns were for us um and we

729
00:34:32,119 --> 00:34:35,599
go through this entire process where we

730
00:34:34,040 --> 00:34:38,879
work very closely especially when it

731
00:34:35,599 --> 00:34:42,200
comes to opening up a new domain um or a

732
00:34:38,879 --> 00:34:44,639
new capability um then we will work very

733
00:34:42,200 --> 00:34:46,839
closely with trusted Partners to

734
00:34:44,639 --> 00:34:49,720
understand precisely how they're using

735
00:34:46,839 --> 00:34:51,079
it where the risks are and generally

736
00:34:49,720 --> 00:34:53,760
especially you know with Enterprise

737
00:34:51,079 --> 00:34:57,119
customers they do have an incentive for

738
00:34:53,760 --> 00:35:00,119
the technology to be robust reliable

739
00:34:57,119 --> 00:35:01,480
useful um otherwise you know even from a

740
00:35:00,119 --> 00:35:03,760
business perspective it's not very

741
00:35:01,480 --> 00:35:05,920
useful if it cannot really follow the

742
00:35:03,760 --> 00:35:09,359
intent if it's not reliable if it's not

743
00:35:05,920 --> 00:35:14,119
robust and so there is this very good

744
00:35:09,359 --> 00:35:17,400
incentive alignment um and and you know

745
00:35:14,119 --> 00:35:19,960
we learn a lot we share the tools the

746
00:35:17,400 --> 00:35:22,119
safety tools monitoring tools any

747
00:35:19,960 --> 00:35:27,720
systems that we build to be able to

748
00:35:22,119 --> 00:35:30,000
empower other people um to you know

749
00:35:27,720 --> 00:35:32,599
establish their own guardrails on top of

750
00:35:30,000 --> 00:35:35,640
the model guard rails or the product

751
00:35:32,599 --> 00:35:38,280
guard rails that we put in place um we

752
00:35:35,640 --> 00:35:41,560
also share those with our developers and

753
00:35:38,280 --> 00:35:45,839
with uh our customers at large so I

754
00:35:41,560 --> 00:35:48,560
think it's quite specific um it's it it

755
00:35:45,839 --> 00:35:50,720
really depends like these questions are

756
00:35:48,560 --> 00:35:54,680
going to get harder and harder over time

757
00:35:50,720 --> 00:35:56,119
obviously but uh yeah I I'd say it

758
00:35:54,680 --> 00:35:58,720
really depends on the use case it

759
00:35:56,119 --> 00:36:01,520
depends on the domain it's it depends on

760
00:35:58,720 --> 00:36:03,440
the capabilities of the models where you

761
00:36:01,520 --> 00:36:09,760
sort of draw the

762
00:36:03,440 --> 00:36:12,560
line thanks um so uh so another area of

763
00:36:09,760 --> 00:36:15,079
question um that you know I know you're

764
00:36:12,560 --> 00:36:16,760
very familiar with is there are a number

765
00:36:15,079 --> 00:36:18,839
of creative professionals who are

766
00:36:16,760 --> 00:36:20,920
concerned about chat GPT and their

767
00:36:18,839 --> 00:36:22,160
intellectual property rights but at the

768
00:36:20,920 --> 00:36:24,319
same time there are also creatives who

769
00:36:22,160 --> 00:36:27,720
are very excited about using generative

770
00:36:24,319 --> 00:36:30,079
models like chat GPT and Delhi um so how

771
00:36:27,720 --> 00:36:31,640
are you thinking more generally about

772
00:36:30,079 --> 00:36:33,839
respecting rights of creators and

773
00:36:31,640 --> 00:36:36,240
authors and it certainly seems like at

774
00:36:33,839 --> 00:36:39,040
least a piece of the recent chat GPT

775
00:36:36,240 --> 00:36:41,000
Builder announcement was sort of maybe

776
00:36:39,040 --> 00:36:42,400
one step in that direction of starting

777
00:36:41,000 --> 00:36:44,880
to think about how you might share

778
00:36:42,400 --> 00:36:46,880
Revenue with people who you know put

779
00:36:44,880 --> 00:36:49,640
their creative Talent into into building

780
00:36:46,880 --> 00:36:51,400
customized gpts but what about all of

781
00:36:49,640 --> 00:36:53,160
that other creative material that's out

782
00:36:51,400 --> 00:36:54,520
there that's that's that's being used by

783
00:36:53,160 --> 00:36:57,480
these

784
00:36:54,520 --> 00:37:00,680
models yeah so in building the Del

785
00:36:57,480 --> 00:37:04,040
modules most recently Del 3 um we're

786
00:37:00,680 --> 00:37:07,599
working with a lot of licensed data um

787
00:37:04,040 --> 00:37:10,599
or and uh we've worked very closely with

788
00:37:07,599 --> 00:37:14,680
creators to actually get their feedback

789
00:37:10,599 --> 00:37:19,359
on um how to develop the product and how

790
00:37:14,680 --> 00:37:23,160
to make it most useful and um also just

791
00:37:19,359 --> 00:37:25,720
think about and iterates on you know

792
00:37:23,160 --> 00:37:29,119
data sharing data contributions there is

793
00:37:25,720 --> 00:37:30,760
a this this whole area of you know how

794
00:37:29,119 --> 00:37:35,680
do you

795
00:37:30,760 --> 00:37:38,400
attribute um value to specific data

796
00:37:35,680 --> 00:37:41,480
that's that's sort of contributed into

797
00:37:38,400 --> 00:37:44,839
model training that's a difficult

798
00:37:41,480 --> 00:37:46,480
technical question and then you know as

799
00:37:44,839 --> 00:37:48,119
we address that or while we're

800
00:37:46,480 --> 00:37:52,000
addressing that we also want to think

801
00:37:48,119 --> 00:37:55,400
about the economics of um you know

802
00:37:52,000 --> 00:37:57,520
sharing the benefits of these models

803
00:37:55,400 --> 00:37:58,960
with the creators so we're doing quite a

804
00:37:57,520 --> 00:38:02,880
bit of work there with you know

805
00:37:58,960 --> 00:38:06,720
Publishers with um creators um and then

806
00:38:02,880 --> 00:38:08,280
with the gbts that we just announced we

807
00:38:06,720 --> 00:38:12,560
we're we're going to have a Marketplace

808
00:38:08,280 --> 00:38:16,720
soon where people can um publish their

809
00:38:12,560 --> 00:38:18,560
gpts and be compensated uh we're we're

810
00:38:16,720 --> 00:38:22,359
figuring out the details of this but

811
00:38:18,560 --> 00:38:24,839
this is this is quite nent and we're

812
00:38:22,359 --> 00:38:27,000
basically just getting very involved

813
00:38:24,839 --> 00:38:29,079
with creators and getting a lot of

814
00:38:27,000 --> 00:38:32,920
feedback from people on what would make

815
00:38:29,079 --> 00:38:35,680
sense what would be valuable um and you

816
00:38:32,920 --> 00:38:39,079
know there is the aspect of getting

817
00:38:35,680 --> 00:38:43,160
input and figuring out economic models

818
00:38:39,079 --> 00:38:46,640
but there is also this aspect of

819
00:38:43,160 --> 00:38:49,599
um of of understanding data attribution

820
00:38:46,640 --> 00:38:52,440
this a bit of a challenging technical

821
00:38:49,599 --> 00:38:54,800
problem that we've been working on so I

822
00:38:52,440 --> 00:38:57,800
think there's a lot to do here but this

823
00:38:54,800 --> 00:39:00,560
is I expect this to evolve and to change

824
00:38:57,800 --> 00:39:03,160
a lot um our goal is definitely to

825
00:39:00,560 --> 00:39:05,160
figure out ways to compensate creators

826
00:39:03,160 --> 00:39:07,319
or at least people put people in the

827
00:39:05,160 --> 00:39:10,319
driver's seat to choose if they don't

828
00:39:07,319 --> 00:39:12,839
want their data to be used for uh

829
00:39:10,319 --> 00:39:16,240
training models and so we've done a lot

830
00:39:12,839 --> 00:39:19,599
of work in data privacy and and user

831
00:39:16,240 --> 00:39:23,200
controls to to try to give people you

832
00:39:19,599 --> 00:39:25,040
know as much um as much control and as

833
00:39:23,200 --> 00:39:27,760
much stay as possible

834
00:39:25,040 --> 00:39:29,920
to figure out you know if they don't

835
00:39:27,760 --> 00:39:31,920
want their data to be used for

836
00:39:29,920 --> 00:39:34,920
training great so I'm going to switch

837
00:39:31,920 --> 00:39:38,880
gears now to uh some of the audience

838
00:39:34,920 --> 00:39:43,480
questions that have come in uh and um uh

839
00:39:38,880 --> 00:39:45,680
so one one question is uh open ai's

840
00:39:43,480 --> 00:39:48,440
mission is to create AGI that benefits

841
00:39:45,680 --> 00:39:50,079
all of humanity how can young people

842
00:39:48,440 --> 00:39:52,520
like us and obviously I'm quoting the

843
00:39:50,079 --> 00:39:54,240
audience not myself how can young how

844
00:39:52,520 --> 00:39:56,720
can Young People Like Us build this

845
00:39:54,240 --> 00:39:59,920
future uh what engineering challenge es

846
00:39:56,720 --> 00:40:03,560
and problems stand in our

847
00:39:59,920 --> 00:40:06,760
way there's so many engineering

848
00:40:03,560 --> 00:40:10,359
challenges and even bigger challenges

849
00:40:06,760 --> 00:40:12,640
that are sort of at the intersection of

850
00:40:10,359 --> 00:40:14,839
many domains a lot of the interesting

851
00:40:12,640 --> 00:40:17,359
problems that I see right now is sort of

852
00:40:14,839 --> 00:40:19,079
like they're they're Frontier problems

853
00:40:17,359 --> 00:40:22,920
uh when it comes to building the

854
00:40:19,079 --> 00:40:24,960
Technologies and also a lot of problems

855
00:40:22,920 --> 00:40:28,400
that sit at the intersection of many

856
00:40:24,960 --> 00:40:31,280
things like Society societal impact

857
00:40:28,400 --> 00:40:33,240
technology safety you know the legal

858
00:40:31,280 --> 00:40:34,839
Frameworks you know we're changing

859
00:40:33,240 --> 00:40:38,640
everything

860
00:40:34,839 --> 00:40:42,240
and and we need people to really think

861
00:40:38,640 --> 00:40:45,040
about how to evolve and change

862
00:40:42,240 --> 00:40:47,960
essentially all these different domains

863
00:40:45,040 --> 00:40:51,839
as and build the the infrastructure for

864
00:40:47,960 --> 00:40:53,599
society to be able to adopt and use this

865
00:40:51,839 --> 00:40:55,920
technology in a way that's very

866
00:40:53,599 --> 00:40:59,119
beneficial so in a way I think I don't

867
00:40:55,920 --> 00:41:04,000
know perhaps it's the most exciting

868
00:40:59,119 --> 00:41:05,920
moment um in history to to to do really

869
00:41:04,000 --> 00:41:08,400
important and impactful work that's

870
00:41:05,920 --> 00:41:12,119
really going to shape the way that we

871
00:41:08,400 --> 00:41:13,520
live um and the quality of our life in

872
00:41:12,119 --> 00:41:18,319
the

873
00:41:13,520 --> 00:41:21,079
future great thank you um so open AI is

874
00:41:18,319 --> 00:41:22,040
clearly leading in the domain of natural

875
00:41:21,079 --> 00:41:25,560
language

876
00:41:22,040 --> 00:41:28,079
processing uh and and and also the image

877
00:41:25,560 --> 00:41:31,040
I'll add I'll make an editorial comment

878
00:41:28,079 --> 00:41:33,319
because I think also image synthesis uh

879
00:41:31,040 --> 00:41:36,079
but uh does open AI have any plans to

880
00:41:33,319 --> 00:41:38,040
lead in other domains of AI and then you

881
00:41:36,079 --> 00:41:41,920
know in particular in the question was

882
00:41:38,040 --> 00:41:43,960
applications uh video generation use in

883
00:41:41,920 --> 00:41:46,040
biology uh there's certainly a lot of

884
00:41:43,960 --> 00:41:48,119
interest around MIT and some other

885
00:41:46,040 --> 00:41:51,119
educational institutions in uh

886
00:41:48,119 --> 00:41:53,680
generative AI for scientific discovery

887
00:41:51,119 --> 00:41:55,880
uh with you know biology as one example

888
00:41:53,680 --> 00:41:57,680
uh are those places that you know where

889
00:41:55,880 --> 00:42:00,319
you're Al looking at applications and

890
00:41:57,680 --> 00:42:03,200
other domains or you sort of focused on

891
00:42:00,319 --> 00:42:05,000
the frontier models in in in natural

892
00:42:03,200 --> 00:42:07,800
language and en

893
00:42:05,000 --> 00:42:09,400
Vision so the way that we think about it

894
00:42:07,800 --> 00:42:13,440
is you know

895
00:42:09,400 --> 00:42:16,079
um what what's most likely path to safe

896
00:42:13,440 --> 00:42:19,000
and beneficial AGI and so we kind of

897
00:42:16,079 --> 00:42:22,000
build our research agenda around that

898
00:42:19,000 --> 00:42:24,480
and of course a big bet that we made a

899
00:42:22,000 --> 00:42:26,200
few years ago was on scale or large

900
00:42:24,480 --> 00:42:29,880
language models and scaling those mod

901
00:42:26,200 --> 00:42:35,079
models this was a very significant um

902
00:42:29,880 --> 00:42:38,440
paradigm shift at the time and now we're

903
00:42:35,079 --> 00:42:40,800
looking at basically bringing together a

904
00:42:38,440 --> 00:42:44,800
lot of modalities into these large

905
00:42:40,800 --> 00:42:47,359
models and so we want um a lot of data

906
00:42:44,800 --> 00:42:50,440
and we want different modalities and we

907
00:42:47,359 --> 00:42:52,839
want to continue to be able to scale um

908
00:42:50,440 --> 00:42:55,240
the systems over time and so when you're

909
00:42:52,839 --> 00:42:58,359
thinking about modalities I'm sure

910
00:42:55,240 --> 00:43:00,200
you've seen our uh text to speeech model

911
00:42:58,359 --> 00:43:04,599
but you know we're we're looking at

912
00:43:00,200 --> 00:43:07,520
everything whether it's images um or uh

913
00:43:04,599 --> 00:43:10,240
Voice or video so we're kind of working

914
00:43:07,520 --> 00:43:13,599
on all these different modalities and

915
00:43:10,240 --> 00:43:15,880
then in terms of applications there is

916
00:43:13,599 --> 00:43:18,319
certainly the work that we've been doing

917
00:43:15,880 --> 00:43:21,559
with ch GPT and our first party product

918
00:43:18,319 --> 00:43:23,400
and we're also we really want to bring

919
00:43:21,559 --> 00:43:25,839
these Technologies into the platform so

920
00:43:23,400 --> 00:43:30,240
other people can build with them as well

921
00:43:25,839 --> 00:43:33,480
that's extremely valuable um

922
00:43:30,240 --> 00:43:35,559
and you know I'd say we're not

923
00:43:33,480 --> 00:43:38,839
necessarily investing in a specific

924
00:43:35,559 --> 00:43:41,000
domain like biology unless there is some

925
00:43:38,839 --> 00:43:43,599
very good strategic reason to do so

926
00:43:41,000 --> 00:43:47,440
we're not experts in biology and so

927
00:43:43,599 --> 00:43:49,280
typically we would work with um a

928
00:43:47,440 --> 00:43:51,000
company that has expertise or

929
00:43:49,280 --> 00:43:54,640
institution that has expertise in

930
00:43:51,000 --> 00:43:57,559
another domain and we bring our own um

931
00:43:54,640 --> 00:43:59,760
to to figure out what's possible at the

932
00:43:57,559 --> 00:44:02,839
frontier of some new domain that's how

933
00:43:59,760 --> 00:44:04,839
we normally do it

934
00:44:02,839 --> 00:44:07,319
um

935
00:44:04,839 --> 00:44:09,480
and and in terms of scientific

936
00:44:07,319 --> 00:44:12,240
advancements I think that's very

937
00:44:09,480 --> 00:44:15,720
interesting and it's one area that I'm

938
00:44:12,240 --> 00:44:20,880
personally very excited about seeing how

939
00:44:15,720 --> 00:44:23,960
these AI systems will help us um

940
00:44:20,880 --> 00:44:26,720
basically just you know figure out new

941
00:44:23,960 --> 00:44:29,319
things and discover new things and

942
00:44:26,720 --> 00:44:32,119
um internally of course we've been using

943
00:44:29,319 --> 00:44:36,240
co-pilot like systems and weave systems

944
00:44:32,119 --> 00:44:38,480
to help us um uh with say you know

945
00:44:36,240 --> 00:44:41,839
safety monitoring and these are based on

946
00:44:38,480 --> 00:44:47,160
gp4 so we're already using

947
00:44:41,839 --> 00:44:50,359
gp4 like systems to help us um U to help

948
00:44:47,160 --> 00:44:54,040
us with various aspects of our work um

949
00:44:50,359 --> 00:44:57,280
like a notable example is definitely in

950
00:44:54,040 --> 00:44:59,839
building safety infrastructure and

951
00:44:57,280 --> 00:45:02,400
monitoring uh but I think over time this

952
00:44:59,839 --> 00:45:06,680
will evolve more and more and maybe you

953
00:45:02,400 --> 00:45:10,000
know you can imagine having a AI um

954
00:45:06,680 --> 00:45:13,160
scientist um that runs on gpus and helps

955
00:45:10,000 --> 00:45:15,760
us Advance research and helps us do

956
00:45:13,160 --> 00:45:18,240
alignment research in fact it's one of

957
00:45:15,760 --> 00:45:21,839
our uh core research agendas for

958
00:45:18,240 --> 00:45:24,000
alignment building sort of this AI uh

959
00:45:21,839 --> 00:45:25,520
alignment researcher so I think yeah

960
00:45:24,000 --> 00:45:27,800
this is this in particular is something

961
00:45:25,520 --> 00:45:30,119
that we're very excited about and you

962
00:45:27,800 --> 00:45:31,839
could extend that to other scientific

963
00:45:30,119 --> 00:45:36,200
domains of

964
00:45:31,839 --> 00:45:38,559
course great um so these are just like I

965
00:45:36,200 --> 00:45:40,599
tried to organize the other questions so

966
00:45:38,559 --> 00:45:42,160
these ones have not been organized but

967
00:45:40,599 --> 00:45:45,520
they're they're all quite interesting

968
00:45:42,160 --> 00:45:48,760
not surprising given the audience uh so

969
00:45:45,520 --> 00:45:51,280
um do you have uh thoughts or comments

970
00:45:48,760 --> 00:45:53,280
on human out of the loop systems like

971
00:45:51,280 --> 00:45:55,680
fully autonomous multi-agent generative

972
00:45:53,280 --> 00:45:58,960
AI systems that don't involve any sort

973
00:45:55,680 --> 00:45:58,960
of human oversight or

974
00:45:59,160 --> 00:46:03,680
collaboration yeah obviously these kind

975
00:46:01,960 --> 00:46:06,040
of systems would be very powerful and

976
00:46:03,680 --> 00:46:08,880
you can imagine them being extremely

977
00:46:06,040 --> 00:46:10,359
useful um I mean even in just the in the

978
00:46:08,880 --> 00:46:14,119
use case that we just discussed if

979
00:46:10,359 --> 00:46:16,400
you're doing scientific research um or

980
00:46:14,119 --> 00:46:18,520
any engineering work you go to sleep and

981
00:46:16,400 --> 00:46:21,000
you have got your AI collaborators

982
00:46:18,520 --> 00:46:24,119
continuing to do that work imagine how

983
00:46:21,000 --> 00:46:27,720
quickly things could advance but of

984
00:46:24,119 --> 00:46:30,000
course the uh challenge there is making

985
00:46:27,720 --> 00:46:32,559
sure that these systems are aligned and

986
00:46:30,000 --> 00:46:36,880
they're following intent and not just

987
00:46:32,559 --> 00:46:39,440
you know explicit intent but um implicit

988
00:46:36,880 --> 00:46:41,280
um and sort of high level ambiguous

989
00:46:39,440 --> 00:46:43,040
intent and making sure that they're

990
00:46:41,280 --> 00:46:46,960
really aligned with what you want them

991
00:46:43,040 --> 00:46:48,760
to do that that is a challenge and uh

992
00:46:46,960 --> 00:46:51,240
that's something that we've been working

993
00:46:48,760 --> 00:46:57,520
on for a while we started with

994
00:46:51,240 --> 00:47:00,760
rhf and um we built we built um um

995
00:46:57,520 --> 00:47:03,880
instruction following models uh with

996
00:47:00,760 --> 00:47:05,520
ilhf and there was kind of probably yeah

997
00:47:03,880 --> 00:47:09,480
the first step that we the first time

998
00:47:05,520 --> 00:47:13,720
that we saw safety into production into

999
00:47:09,480 --> 00:47:15,559
um this this Real World products and we

1000
00:47:13,720 --> 00:47:18,520
brought Safety Research into Real World

1001
00:47:15,559 --> 00:47:20,839
products and then after that um

1002
00:47:18,520 --> 00:47:25,200
obviously we continued this work work

1003
00:47:20,839 --> 00:47:28,800
with ch GPT and extended it further we

1004
00:47:25,200 --> 00:47:33,359
did um a lot of alignment work with gb4

1005
00:47:28,800 --> 00:47:35,800
and gbd4 was the most aligned and most

1006
00:47:33,359 --> 00:47:38,319
powerful model at the time so I think if

1007
00:47:35,800 --> 00:47:40,960
we just kind of proceed in this path

1008
00:47:38,319 --> 00:47:43,359
where we continue to do a lot of um

1009
00:47:40,960 --> 00:47:46,960
alignment research

1010
00:47:43,359 --> 00:47:50,440
and capability prediction work build

1011
00:47:46,960 --> 00:47:55,000
build up the science of prediction

1012
00:47:50,440 --> 00:47:57,160
um and and develop uh continue to

1013
00:47:55,000 --> 00:47:59,079
develop and deploy employe iteratively

1014
00:47:57,160 --> 00:48:01,880
learning from every step while the

1015
00:47:59,079 --> 00:48:05,760
stakes are still low I think there is a

1016
00:48:01,880 --> 00:48:10,240
path to getting to a place where these

1017
00:48:05,760 --> 00:48:12,559
autonomous agents um can be robust and

1018
00:48:10,240 --> 00:48:14,359
safe but there are a lot of research

1019
00:48:12,559 --> 00:48:15,680
questions ahead of us that we need to

1020
00:48:14,359 --> 00:48:18,480
figure out and

1021
00:48:15,680 --> 00:48:21,760
answer when you when you mentioned R lhf

1022
00:48:18,480 --> 00:48:25,319
there um uh I guess I had a follow-up

1023
00:48:21,760 --> 00:48:25,830
question which is um uh have you been

1024
00:48:25,319 --> 00:48:27,160
used

1025
00:48:25,830 --> 00:48:29,400
[Music]

1026
00:48:27,160 --> 00:48:31,359
reinforcement learning with AI feedback

1027
00:48:29,400 --> 00:48:34,760
like just in terms of the challenges of

1028
00:48:31,359 --> 00:48:36,520
scaling human feedback and if so are you

1029
00:48:34,760 --> 00:48:38,119
finding that useful like sort of how do

1030
00:48:36,520 --> 00:48:40,599
you look at the scale problems of

1031
00:48:38,119 --> 00:48:43,119
alignment um and and sort of humans in

1032
00:48:40,599 --> 00:48:48,520
that feedback

1033
00:48:43,119 --> 00:48:50,319
loop yeah that's you know um the hiring

1034
00:48:48,520 --> 00:48:53,599
a lot of contractors and giving them

1035
00:48:50,319 --> 00:48:55,599
Direction initially after gbd3 that was

1036
00:48:53,599 --> 00:48:57,880
very important and Building Things

1037
00:48:55,599 --> 00:49:02,799
instruction following models but of

1038
00:48:57,880 --> 00:49:04,960
course with um CH GPT then our Our

1039
00:49:02,799 --> 00:49:06,839
intention was to have this as a research

1040
00:49:04,960 --> 00:49:10,319
preview and get a lot of feedback from

1041
00:49:06,839 --> 00:49:14,240
the research community and um use that

1042
00:49:10,319 --> 00:49:16,480
feedback to for RF or generally to just

1043
00:49:14,240 --> 00:49:18,720
make the the model better and more

1044
00:49:16,480 --> 00:49:20,920
robust and more reliable but of course

1045
00:49:18,720 --> 00:49:23,520
in four days we saw a million users and

1046
00:49:20,920 --> 00:49:25,680
so this opened up a new opportunity

1047
00:49:23,520 --> 00:49:30,760
which was you know now all of sudden

1048
00:49:25,680 --> 00:49:35,000
then if um users um were willing to we

1049
00:49:30,760 --> 00:49:38,640
could um use that input for for

1050
00:49:35,000 --> 00:49:43,760
rhf and so that's that's very valuable

1051
00:49:38,640 --> 00:49:46,920
data fly will and um of course you know

1052
00:49:43,760 --> 00:49:50,000
we had to make sure that um users were

1053
00:49:46,920 --> 00:49:52,000
willing to provide such input and give

1054
00:49:50,000 --> 00:49:55,359
them the controls to decide for

1055
00:49:52,000 --> 00:49:57,799
themselves but in terms of scalability

1056
00:49:55,359 --> 00:50:00,280
that that is one path where you know you

1057
00:49:57,799 --> 00:50:02,240
have access to a large user base but

1058
00:50:00,280 --> 00:50:06,920
more importantly perhaps is also the

1059
00:50:02,240 --> 00:50:09,359
quality of the data and making sure that

1060
00:50:06,920 --> 00:50:13,599
um you know the quality of the data is

1061
00:50:09,359 --> 00:50:16,160
high so that generally requires um very

1062
00:50:13,599 --> 00:50:18,760
specific data collection

1063
00:50:16,160 --> 00:50:20,720
campaigns and there is a lot of work

1064
00:50:18,760 --> 00:50:23,640
that us and

1065
00:50:20,720 --> 00:50:26,599
also I I think a lot of other AI

1066
00:50:23,640 --> 00:50:28,559
developers are doing on high quality

1067
00:50:26,599 --> 00:50:31,640
data

1068
00:50:28,559 --> 00:50:34,799
collection great um so on the Education

1069
00:50:31,640 --> 00:50:37,079
topic uh there's been a and and you

1070
00:50:34,799 --> 00:50:39,000
mentioned uh you know code.org there's

1071
00:50:37,079 --> 00:50:43,319
been a big push for students to learn to

1072
00:50:39,000 --> 00:50:46,400
code in recent years uh if if we're

1073
00:50:43,319 --> 00:50:48,400
really able to code in natural language

1074
00:50:46,400 --> 00:50:51,760
you know how do you see that change in

1075
00:50:48,400 --> 00:50:53,640
K12 education do you see you know like

1076
00:50:51,760 --> 00:50:57,240
what how do you see sort of

1077
00:50:53,640 --> 00:50:59,440
the human coder with a co-pilot the

1078
00:50:57,240 --> 00:51:02,720
non-human coder like what should we be

1079
00:50:59,440 --> 00:51:04,680
thinking about in terms of uh at least K

1080
00:51:02,720 --> 00:51:06,760
through 12 education and and trying to

1081
00:51:04,680 --> 00:51:10,520
get kids to

1082
00:51:06,760 --> 00:51:13,079
code there is you know a lot of code to

1083
00:51:10,520 --> 00:51:16,599
be written in the world and so I think

1084
00:51:13,079 --> 00:51:19,079
for um people that want to learn it's

1085
00:51:16,599 --> 00:51:22,319
just a great opportunity I don't think

1086
00:51:19,079 --> 00:51:24,960
it's going away anytime soon and I think

1087
00:51:22,319 --> 00:51:28,000
it's great to be able to learn and build

1088
00:51:24,960 --> 00:51:29,799
things yourself now the Technologies

1089
00:51:28,000 --> 00:51:31,760
we're building make it much easier both

1090
00:51:29,799 --> 00:51:34,760
to actually learn how to code and make

1091
00:51:31,760 --> 00:51:36,880
sure that it's you know catered to the

1092
00:51:34,760 --> 00:51:39,240
way that you think and you can just ask

1093
00:51:36,880 --> 00:51:40,880
so many questions you know the system is

1094
00:51:39,240 --> 00:51:45,839
just running on GPU so it's quite

1095
00:51:40,880 --> 00:51:47,480
patient and it will uh you know you can

1096
00:51:45,839 --> 00:51:49,839
the the learning can be quite

1097
00:51:47,480 --> 00:51:55,160
personalized and cater to you which can

1098
00:51:49,839 --> 00:51:59,079
be very effective um but you know uh at

1099
00:51:55,160 --> 00:52:01,640
the the same time it's also quite these

1100
00:51:59,079 --> 00:52:06,160
systems are far more accessible than

1101
00:52:01,640 --> 00:52:08,440
ever and so if one just wants to um code

1102
00:52:06,160 --> 00:52:09,920
in natural language they can also do

1103
00:52:08,440 --> 00:52:12,280
that where they provide sort of high

1104
00:52:09,920 --> 00:52:16,760
level guidance uh

1105
00:52:12,280 --> 00:52:20,680
to GPT or other systems like it and can

1106
00:52:16,760 --> 00:52:24,480
build uh prototypes or actual products

1107
00:52:20,680 --> 00:52:26,960
with um very little coding language so I

1108
00:52:24,480 --> 00:52:31,119
think it's become both much easier to

1109
00:52:26,960 --> 00:52:33,359
learn uh probably easier than ever and

1110
00:52:31,119 --> 00:52:36,160
much easier to build all the tools all

1111
00:52:33,359 --> 00:52:38,799
these tools are accessible available and

1112
00:52:36,160 --> 00:52:42,079
so people just need to explore and if

1113
00:52:38,799 --> 00:52:44,280
they're curious all the tools are are at

1114
00:52:42,079 --> 00:52:44,280
your

1115
00:52:44,920 --> 00:52:49,400
hands uh so one of the questions that's

1116
00:52:47,240 --> 00:52:53,359
been rising in popularity here somebody

1117
00:52:49,400 --> 00:52:54,920
asked it is uh our open Ai and MIT

1118
00:52:53,359 --> 00:52:58,040
thinking about a partnership that could

1119
00:52:54,920 --> 00:52:58,040
benefit the MIT

1120
00:52:58,079 --> 00:53:03,200
community so obviously there are a lot

1121
00:53:00,760 --> 00:53:06,319
of students here who are interested

1122
00:53:03,200 --> 00:53:09,520
in in your in in in your tools and your

1123
00:53:06,319 --> 00:53:09,520
engagement with places like

1124
00:53:09,839 --> 00:53:16,319
MIT I am open to your

1125
00:53:13,799 --> 00:53:18,000
proposals good we we we'll we'll uh

1126
00:53:16,319 --> 00:53:19,400
we'll we'll be back uh I mean I know

1127
00:53:18,000 --> 00:53:21,799
there have been some discussions going

1128
00:53:19,400 --> 00:53:23,480
on that that I'm aware of but I'm I'm

1129
00:53:21,799 --> 00:53:26,359
not sure that any of them that any of

1130
00:53:23,480 --> 00:53:29,640
them are that far along at this point so

1131
00:53:26,359 --> 00:53:31,599
uh and um and there's certainly a lot of

1132
00:53:29,640 --> 00:53:33,400
sort of pointwise interactions between

1133
00:53:31,599 --> 00:53:35,920
various faculty and research groups at

1134
00:53:33,400 --> 00:53:38,280
MIT and Folks at at at openai but I

1135
00:53:35,920 --> 00:53:40,000
think it just shows the level of you

1136
00:53:38,280 --> 00:53:42,280
know broad student interest in what

1137
00:53:40,000 --> 00:53:47,960
you're doing and how they can really

1138
00:53:42,280 --> 00:53:50,079
become expert in it um amazing uh so

1139
00:53:47,960 --> 00:53:51,400
um you know when it was lower down on

1140
00:53:50,079 --> 00:53:54,040
the list I would have ignored it but it

1141
00:53:51,400 --> 00:53:57,799
went all the way up to the

1142
00:53:54,040 --> 00:54:01,280
top uh uh um so there's a set of

1143
00:53:57,799 --> 00:54:04,760
questions here sort of around you know

1144
00:54:01,280 --> 00:54:06,000
mechanisms uh to help ensure you know

1145
00:54:04,760 --> 00:54:11,040
sort of

1146
00:54:06,000 --> 00:54:14,319
fairness uh um you know combating bias

1147
00:54:11,040 --> 00:54:17,359
uh um uh you know sort of ethical

1148
00:54:14,319 --> 00:54:19,920
development of Technology you know you

1149
00:54:17,359 --> 00:54:21,880
you've spoken quite a bit to some of

1150
00:54:19,920 --> 00:54:24,160
those kinds of questions more broadly

1151
00:54:21,880 --> 00:54:25,799
but are there are there specific things

1152
00:54:24,160 --> 00:54:27,319
maybe that you're the fact that there's

1153
00:54:25,799 --> 00:54:29,000
still questions coming up my guess is

1154
00:54:27,319 --> 00:54:31,559
people are hoping there might be

1155
00:54:29,000 --> 00:54:34,000
something that's a little bit you know

1156
00:54:31,559 --> 00:54:35,480
uh a concrete example uh or something

1157
00:54:34,000 --> 00:54:37,839
there in terms of mechanisms that you

1158
00:54:35,480 --> 00:54:40,359
see that are that are working in in the

1159
00:54:37,839 --> 00:54:43,640
work that you're doing

1160
00:54:40,359 --> 00:54:45,559
um yeah there's certainly uh I mean

1161
00:54:43,640 --> 00:54:49,000
we're doing so much work in this whether

1162
00:54:45,559 --> 00:54:51,559
it's in text or images um combination of

1163
00:54:49,000 --> 00:54:55,480
all these things um so I'm thinking

1164
00:54:51,559 --> 00:54:58,200
through the work that we did with Del

1165
00:54:55,480 --> 00:55:03,160
recently um Del is definitely the most

1166
00:54:58,200 --> 00:55:05,839
advanced image system out there and uh

1167
00:55:03,160 --> 00:55:09,480
we had to do quite a bit of work in

1168
00:55:05,839 --> 00:55:13,040
making sure that you know we'd mitigate

1169
00:55:09,480 --> 00:55:14,799
some of the harmful biases um within the

1170
00:55:13,040 --> 00:55:17,079
system but

1171
00:55:14,799 --> 00:55:20,799
also you know you you have to get the

1172
00:55:17,079 --> 00:55:27,440
balance right uh with mitigating biases

1173
00:55:20,799 --> 00:55:29,640
with figuring out how um you know let's

1174
00:55:27,440 --> 00:55:31,599
say you don't want to necessarily

1175
00:55:29,640 --> 00:55:33,640
optimize too much for engagement because

1176
00:55:31,599 --> 00:55:36,319
you end up with other issues and so once

1177
00:55:33,640 --> 00:55:40,440
you optimize one metric you can see a

1178
00:55:36,319 --> 00:55:43,280
lot of other things uh uh degrading and

1179
00:55:40,440 --> 00:55:44,839
so one thing that we've learned is that

1180
00:55:43,280 --> 00:55:46,520
whenever you're optimizing for one thing

1181
00:55:44,839 --> 00:55:48,480
you have to be super careful because

1182
00:55:46,520 --> 00:55:51,720
probably bunch of other things that you

1183
00:55:48,480 --> 00:55:54,720
didn't expect would be degrading and

1184
00:55:51,720 --> 00:55:57,079
so um you have to take a very holistic

1185
00:55:54,720 --> 00:56:00,640
View especially with very complex issues

1186
00:55:57,079 --> 00:56:03,280
like harmful bias and constantly be

1187
00:56:00,640 --> 00:56:06,920
auditing the system end to end when you

1188
00:56:03,280 --> 00:56:09,960
tweak something and so we we've seen

1189
00:56:06,920 --> 00:56:13,280
this a lot you know there was uh I think

1190
00:56:09,960 --> 00:56:19,119
Del to we tweaked something when it came

1191
00:56:13,280 --> 00:56:22,480
to uh the ratio of um gendered images

1192
00:56:19,119 --> 00:56:24,319
and then we ended up with some uh result

1193
00:56:22,480 --> 00:56:25,799
that we didn't expect that a lot of the

1194
00:56:24,319 --> 00:56:27,440
images of women were sort of

1195
00:56:25,799 --> 00:56:30,119
hypersexualized because that was the

1196
00:56:27,440 --> 00:56:35,160
content in the data so then we had to go

1197
00:56:30,119 --> 00:56:37,200
in and intervene again and uh make sure

1198
00:56:35,160 --> 00:56:38,599
that you know it was more balanced and

1199
00:56:37,200 --> 00:56:41,760
we were not ending up with these

1200
00:56:38,599 --> 00:56:45,440
unintended consequences so these issues

1201
00:56:41,760 --> 00:56:48,680
are very complex and they require having

1202
00:56:45,440 --> 00:56:51,440
quite diverse talent in the team to be

1203
00:56:48,680 --> 00:56:55,200
able to handle them and doing a lot of

1204
00:56:51,440 --> 00:56:57,160
red seaming with people from um you know

1205
00:56:55,200 --> 00:56:59,880
with different expertise because we

1206
00:56:57,160 --> 00:57:03,520
learn a ton from that early red teaming

1207
00:56:59,880 --> 00:57:06,240
initially with trusted users and then um

1208
00:57:03,520 --> 00:57:08,640
with a broader set of people and be

1209
00:57:06,240 --> 00:57:11,880
constantly intervening and tweaking but

1210
00:57:08,640 --> 00:57:14,480
at almost like all stages from data to

1211
00:57:11,880 --> 00:57:17,440
the algorithm itself to the way that we

1212
00:57:14,480 --> 00:57:19,400
deploy the technology in the product and

1213
00:57:17,440 --> 00:57:23,200
the way we design the product the access

1214
00:57:19,400 --> 00:57:26,319
around it as well as um you know the

1215
00:57:23,200 --> 00:57:28,760
post deployment so if figuring out uh

1216
00:57:26,319 --> 00:57:32,000
stuff around like you know classifiers

1217
00:57:28,760 --> 00:57:33,760
monitoring uh usage policies and things

1218
00:57:32,000 --> 00:57:38,119
like that

1219
00:57:33,760 --> 00:57:40,799
um so yeah it's a d is probably great

1220
00:57:38,119 --> 00:57:46,480
example of this a lot of examples around

1221
00:57:40,799 --> 00:57:49,359
our um base model on chbt so we you know

1222
00:57:46,480 --> 00:57:52,720
we have our usage policies and then we

1223
00:57:49,359 --> 00:57:54,559
build systems that help us Monitor and

1224
00:57:52,720 --> 00:57:57,720
like automatically monitor according to

1225
00:57:54,559 --> 00:57:59,599
our usage policies um and sometimes you

1226
00:57:57,720 --> 00:58:03,000
know we have over refusals because we're

1227
00:57:59,599 --> 00:58:04,880
being too conservative and that's kind

1228
00:58:03,000 --> 00:58:08,480
of annoying for the user it's not so

1229
00:58:04,880 --> 00:58:10,359
much fun and the system Hedges a lot so

1230
00:58:08,480 --> 00:58:12,720
we have to optimize for that perhaps

1231
00:58:10,359 --> 00:58:17,359
collect more data higher quality data

1232
00:58:12,720 --> 00:58:20,640
better data and uh intervene again uh so

1233
00:58:17,359 --> 00:58:23,799
we have to do tweaks like that and in

1234
00:58:20,640 --> 00:58:27,720
some cases you know there is kind of

1235
00:58:23,799 --> 00:58:28,680
this tradeoff between um truthfulness

1236
00:58:27,720 --> 00:58:32,119
and

1237
00:58:28,680 --> 00:58:34,039
factuality is and and like creativity or

1238
00:58:32,119 --> 00:58:35,960
the the personality of the model like it

1239
00:58:34,039 --> 00:58:38,720
being fun and

1240
00:58:35,960 --> 00:58:41,079
creative um and so this is actually

1241
00:58:38,720 --> 00:58:43,920
quite a hard balance to strike between

1242
00:58:41,079 --> 00:58:46,799
creativity and factuality perhaps even

1243
00:58:43,920 --> 00:58:48,440
in humans um and so this is something

1244
00:58:46,799 --> 00:58:51,920
that we've learned a lot about in the

1245
00:58:48,440 --> 00:58:54,799
past year and have iterated a lot on and

1246
00:58:51,920 --> 00:58:59,240
also people have different expectations

1247
00:58:54,799 --> 00:59:02,119
or preferences and so we introduce

1248
00:58:59,240 --> 00:59:05,359
custom that puts people more in charge

1249
00:59:02,119 --> 00:59:08,480
of kind of the personality of uh the

1250
00:59:05,359 --> 00:59:11,839
model and how the model behaves within

1251
00:59:08,480 --> 00:59:13,599
certain bounds and guard rails of course

1252
00:59:11,839 --> 00:59:14,920
but yeah I would say these are probably

1253
00:59:13,599 --> 00:59:17,400
some of the things we've learned some of

1254
00:59:14,920 --> 00:59:19,480
the things we've done over the past year

1255
00:59:17,400 --> 00:59:20,920
in these different models now that's

1256
00:59:19,480 --> 00:59:23,119
that that that's great I think that's

1257
00:59:20,920 --> 00:59:24,760
some of the kind of next level down

1258
00:59:23,119 --> 00:59:26,920
things that that help people understand

1259
00:59:24,760 --> 00:59:28,359
understand a little more um so another

1260
00:59:26,920 --> 00:59:29,599
question that's been voted way up and

1261
00:59:28,359 --> 00:59:31,160
I'm going to read the question but I'm

1262
00:59:29,599 --> 00:59:33,319
going to presume it's been voted way up

1263
00:59:31,160 --> 00:59:36,079
for part of the question the question is

1264
00:59:33,319 --> 00:59:38,000
as a member of the MIT Albanian

1265
00:59:36,079 --> 00:59:40,680
Association I'd love to hear a little

1266
00:59:38,000 --> 00:59:42,880
bit about how you ended up where you are

1267
00:59:40,680 --> 00:59:44,440
today and any advice you may have I

1268
00:59:42,880 --> 00:59:46,440
presume that some of the people voting

1269
00:59:44,440 --> 00:59:48,359
that up just want to hear about your

1270
00:59:46,440 --> 00:59:50,680
career and aren't all members of the MIT

1271
00:59:48,359 --> 00:59:52,839
Albanian associ but may may maybe we

1272
00:59:50,680 --> 00:59:55,240
have a big contingent who came to hear

1273
00:59:52,839 --> 00:59:57,599
you today but but but probably not

1274
00:59:55,240 --> 00:59:59,079
hundreds of them so uh but anyways I I

1275
00:59:57,599 --> 01:00:01,119
think people would love to hear a little

1276
00:59:59,079 --> 01:00:04,160
bit and this will kind of get to one of

1277
01:00:01,119 --> 01:00:06,880
the questions I had also just um you

1278
01:00:04,160 --> 01:00:10,400
know how you got to where you are what

1279
01:00:06,880 --> 01:00:12,319
your thoughts about you know careers are

1280
01:00:10,400 --> 01:00:14,200
um you know on the one hand you're

1281
01:00:12,319 --> 01:00:15,960
you're way past being an undergraduate

1282
01:00:14,200 --> 01:00:19,240
but on the other hand you know you've

1283
01:00:15,960 --> 01:00:21,440
had a you you you know you've you've had

1284
01:00:19,240 --> 01:00:23,280
a sort of you know very rapid rise in

1285
01:00:21,440 --> 01:00:25,680
the kind of responsibilities for things

1286
01:00:23,280 --> 01:00:29,839
that you're doing so be really great to

1287
01:00:25,680 --> 01:00:34,160
to hear your thoughts on Career

1288
01:00:29,839 --> 01:00:39,200
advice um so yeah I I grew up in Albania

1289
01:00:34,160 --> 01:00:41,119
and uh I I then studied I I did my high

1290
01:00:39,200 --> 01:00:44,559
school in Canada or half of my High

1291
01:00:41,119 --> 01:00:47,000
School uh at United World College uh in

1292
01:00:44,559 --> 01:00:48,760
Canada and then I came to the US for

1293
01:00:47,000 --> 01:00:53,960
college but I've always been interested

1294
01:00:48,760 --> 01:00:56,880
in math uh and Science and I was always

1295
01:00:53,960 --> 01:01:00,240
interested in and kind of just building

1296
01:00:56,880 --> 01:01:02,960
stuff and I studied mechanical engineer

1297
01:01:00,240 --> 01:01:05,760
I studed math mechanical engineering and

1298
01:01:02,960 --> 01:01:08,039
eventually right after school um ended

1299
01:01:05,760 --> 01:01:10,480
up working in Aerospace and then in

1300
01:01:08,039 --> 01:01:13,039
automotive where I learned a lot about

1301
01:01:10,480 --> 01:01:16,960
how to build complex things you know

1302
01:01:13,039 --> 01:01:19,200
you're talking about uh you know very

1303
01:01:16,960 --> 01:01:23,680
complex systems with thousands and

1304
01:01:19,200 --> 01:01:25,480
thousands of Parts um and um you know

1305
01:01:23,680 --> 01:01:28,359
everything comes together software

1306
01:01:25,480 --> 01:01:31,240
Hardware um electrical engineering

1307
01:01:28,359 --> 01:01:34,680
regulation all everything comes together

1308
01:01:31,240 --> 01:01:37,200
there and in terms of you

1309
01:01:34,680 --> 01:01:41,160
know the career path I never really had

1310
01:01:37,200 --> 01:01:43,000
a very specific plan in terms of I

1311
01:01:41,160 --> 01:01:46,039
didn't have a specific plan of where I

1312
01:01:43,000 --> 01:01:48,880
wanted to end up but I always kind of

1313
01:01:46,039 --> 01:01:54,599
followed my curiosity and always wanted

1314
01:01:48,880 --> 01:01:58,279
to just build things and uh I wasn't

1315
01:01:54,599 --> 01:01:59,559
afraid to jump domains so after building

1316
01:01:58,279 --> 01:02:02,400
a lot of

1317
01:01:59,559 --> 01:02:04,520
expertise um and knowledge in a specific

1318
01:02:02,400 --> 01:02:07,440
domain I wasn't afraid to just kind of

1319
01:02:04,520 --> 01:02:10,039
start from scratch and I've done that a

1320
01:02:07,440 --> 01:02:11,799
few times in my career so you know in

1321
01:02:10,039 --> 01:02:13,599
Aerospace and and then going to

1322
01:02:11,799 --> 01:02:16,440
automotive and then going into virtual

1323
01:02:13,599 --> 01:02:19,760
reality and augmented reality and more

1324
01:02:16,440 --> 01:02:22,920
computer vision and AI work and then

1325
01:02:19,760 --> 01:02:25,839
obviously as in the past five years into

1326
01:02:22,920 --> 01:02:29,880
working in Ai and really being dedicated

1327
01:02:25,839 --> 01:02:32,559
to AGI so there is something there but I

1328
01:02:29,880 --> 01:02:35,200
also think that's quite specific to how

1329
01:02:32,559 --> 01:02:39,079
I learn and uh you know the things that

1330
01:02:35,200 --> 01:02:41,880
I'm interested in and uh usually I I'm

1331
01:02:39,079 --> 01:02:44,200
interested that in the intersection of

1332
01:02:41,880 --> 01:02:46,520
many things technology that that sort of

1333
01:02:44,200 --> 01:02:51,039
affects many things and working at the

1334
01:02:46,520 --> 01:02:53,279
intersection of many domains um but

1335
01:02:51,039 --> 01:02:55,359
generally you know that would be my

1336
01:02:53,279 --> 01:02:57,920
advice to just do really the thing that

1337
01:02:55,359 --> 01:03:01,520
you're most curious about and most

1338
01:02:57,920 --> 01:03:05,240
excited about I think it's kind of

1339
01:03:01,520 --> 01:03:08,440
maybe uh people hear this a lot but it's

1340
01:03:05,240 --> 01:03:11,440
really true and because when things get

1341
01:03:08,440 --> 01:03:13,599
hard then it doesn't feel like work um

1342
01:03:11,440 --> 01:03:16,119
and things will inevitably get hard and

1343
01:03:13,599 --> 01:03:18,480
so I would say especially earlier in

1344
01:03:16,119 --> 01:03:20,440
your career like but perhaps always just

1345
01:03:18,480 --> 01:03:22,359
focus on learning and maximizing your

1346
01:03:20,440 --> 01:03:25,319
learning and the things you're curious

1347
01:03:22,359 --> 01:03:26,520
and excited about no I think I I

1348
01:03:25,319 --> 01:03:28,279
actually think it's great advice you

1349
01:03:26,520 --> 01:03:31,599
were saying it's it's maybe just how you

1350
01:03:28,279 --> 01:03:33,680
are but I would say I think often people

1351
01:03:31,599 --> 01:03:35,880
focus a little too much on sort of a

1352
01:03:33,680 --> 01:03:38,240
linear career path and trying to plan it

1353
01:03:35,880 --> 01:03:41,079
out and I think you know when you said

1354
01:03:38,240 --> 01:03:42,920
you didn't really think about a a goal

1355
01:03:41,079 --> 01:03:44,440
you thought about the moment and really

1356
01:03:42,920 --> 01:03:47,079
learning what you could learn and doing

1357
01:03:44,440 --> 01:03:49,160
what you could do in that moment uh I

1358
01:03:47,079 --> 01:03:50,920
just I think you can strain your

1359
01:03:49,160 --> 01:03:53,520
opportunities by trying to think about

1360
01:03:50,920 --> 01:03:54,839
the linear path so I I really I think

1361
01:03:53,520 --> 01:03:56,559
your your career career and your

1362
01:03:54,839 --> 01:03:58,960
attitude is something that that that

1363
01:03:56,559 --> 01:04:01,359
everybody should hear that's and I could

1364
01:03:58,960 --> 01:04:03,279
sort of see it from your the set of

1365
01:04:01,359 --> 01:04:04,920
things you'd worked on so i' sort of

1366
01:04:03,279 --> 01:04:08,359
already written a question about this

1367
01:04:04,920 --> 01:04:10,359
but but I'm I'm glad it came up um so

1368
01:04:08,359 --> 01:04:13,520
this is related but maybe different

1369
01:04:10,359 --> 01:04:15,760
enough that um so just general advice

1370
01:04:13,520 --> 01:04:17,680
you have for aspiring entrepreneurs and

1371
01:04:15,760 --> 01:04:20,039
innovators about what to do in their

1372
01:04:17,680 --> 01:04:22,119
early 20s so it's getting a little more

1373
01:04:20,039 --> 01:04:23,760
specific uh and then may maybe

1374
01:04:22,119 --> 01:04:26,200
especially for people who are from a

1375
01:04:23,760 --> 01:04:26,200
minority

1376
01:04:26,440 --> 01:04:29,440
background

1377
01:04:29,680 --> 01:04:35,839
um I mean I would say for everyone just

1378
01:04:33,760 --> 01:04:40,279
uh especially right out of school it's

1379
01:04:35,839 --> 01:04:43,839
so much easier to take risks um and just

1380
01:04:40,279 --> 01:04:45,920
try things it's like the best thing to

1381
01:04:43,839 --> 01:04:47,799
learn about what you want what you will

1382
01:04:45,920 --> 01:04:50,960
enjoy what you'll be good at is to

1383
01:04:47,799 --> 01:04:54,960
really try and build stuff uh make

1384
01:04:50,960 --> 01:04:57,480
things like really make things and

1385
01:04:54,960 --> 01:04:59,599
try as many things as possible and I

1386
01:04:57,480 --> 01:05:02,960
think that really gives you a sense of

1387
01:04:59,599 --> 01:05:06,000
taste about what you want to do and what

1388
01:05:02,960 --> 01:05:08,079
you're good at um and it also helps

1389
01:05:06,000 --> 01:05:11,520
evolve your taste you know whether it's

1390
01:05:08,079 --> 01:05:14,559
in research or you know any domain it

1391
01:05:11,520 --> 01:05:16,799
doesn't really matter but yeah I'd say

1392
01:05:14,559 --> 01:05:19,480
it sounds generic but I I really think

1393
01:05:16,799 --> 01:05:22,680
it's the best advice to really just go

1394
01:05:19,480 --> 01:05:24,160
do stuff and not really overthink it

1395
01:05:22,680 --> 01:05:27,400
especially like that early in your care

1396
01:05:24,160 --> 01:05:31,279
career where you know you don't have a

1397
01:05:27,400 --> 01:05:34,359
ton of built up expertise and so the

1398
01:05:31,279 --> 01:05:36,440
opportunity cost may not be as high I

1399
01:05:34,359 --> 01:05:41,279
think best thing to do is to just go

1400
01:05:36,440 --> 01:05:41,279
make things and try things and build

1401
01:05:41,720 --> 01:05:46,160
things so this is another question

1402
01:05:44,000 --> 01:05:48,119
that's related to something I'd written

1403
01:05:46,160 --> 01:05:50,279
before so it's good on my questions said

1404
01:05:48,119 --> 01:05:54,319
overlap uh what what what actions are

1405
01:05:50,279 --> 01:05:57,520
being taken uh at open AI to address the

1406
01:05:54,319 --> 01:06:01,640
environmental impact of the AI systems

1407
01:05:57,520 --> 01:06:03,960
uh the extremely large amount of uh GPU

1408
01:06:01,640 --> 01:06:06,839
Compu resources being used in the

1409
01:06:03,960 --> 01:06:08,359
training uh and and then of course as

1410
01:06:06,839 --> 01:06:10,839
they're integrated in society in the

1411
01:06:08,359 --> 01:06:13,119
long term also a lot of aggregate

1412
01:06:10,839 --> 01:06:15,359
inference uh you know like right now

1413
01:06:13,119 --> 01:06:17,640
maybe the training is using most of the

1414
01:06:15,359 --> 01:06:20,079
but as you most of the compute power but

1415
01:06:17,640 --> 01:06:21,839
as you start to integrate these into

1416
01:06:20,079 --> 01:06:23,000
society broadly it becomes certainly

1417
01:06:21,839 --> 01:06:25,799
becomes

1418
01:06:23,000 --> 01:06:28,880
both the specific action that we're

1419
01:06:25,799 --> 01:06:33,160
taking is making the systems more

1420
01:06:28,880 --> 01:06:35,680
efficient um but yeah I say yeah General

1421
01:06:33,160 --> 01:06:39,279
we work with Microsoft that's really

1422
01:06:35,680 --> 01:06:42,520
thinking about how to offset some of uh

1423
01:06:39,279 --> 01:06:46,440
the impact of this but longer term what

1424
01:06:42,520 --> 01:06:48,960
I'm really excited about is um getting

1425
01:06:46,440 --> 01:06:52,200
these Advanced systems to help us figure

1426
01:06:48,960 --> 01:06:55,520
out maybe either new scientific um

1427
01:06:52,200 --> 01:06:59,319
solutions to dealing with climate change

1428
01:06:55,520 --> 01:07:02,319
um or perhaps just you know figuring out

1429
01:06:59,319 --> 01:07:05,880
the infrastructure of more sustainable

1430
01:07:02,319 --> 01:07:09,039
energy and making that just really um

1431
01:07:05,880 --> 01:07:12,880
the the default way of consuming energy

1432
01:07:09,039 --> 01:07:14,680
generating consuming energy um and this

1433
01:07:12,880 --> 01:07:16,920
is something that I'm particularly

1434
01:07:14,680 --> 01:07:19,559
excited about figuring out how to use

1435
01:07:16,920 --> 01:07:22,559
Advanced AI systems to help us deal with

1436
01:07:19,559 --> 01:07:25,279
these extreme challenges that you know

1437
01:07:22,559 --> 01:07:27,799
just left to our own devices we haven't

1438
01:07:25,279 --> 01:07:32,000
really figured out and they're not

1439
01:07:27,799 --> 01:07:35,680
exactly on track I would say to solve on

1440
01:07:32,000 --> 01:07:38,680
our own um so I'm hoping that you know

1441
01:07:35,680 --> 01:07:42,319
medium medium to longterm AI can help us

1442
01:07:38,680 --> 01:07:44,000
with that but in in in the in the near

1443
01:07:42,319 --> 01:07:46,880
term

1444
01:07:44,000 --> 01:07:49,240
um we're we're working on things like

1445
01:07:46,880 --> 01:07:52,119
yeah figuring out how to optimize data

1446
01:07:49,240 --> 01:07:54,359
centers and the design of data centers

1447
01:07:52,119 --> 01:07:56,279
and also working with Microsoft s on

1448
01:07:54,359 --> 01:07:58,680
that and also just making the systems

1449
01:07:56,279 --> 01:08:00,880
more and more

1450
01:07:58,680 --> 01:08:03,440
efficient so here here's a different

1451
01:08:00,880 --> 01:08:05,480
form of question thank thanks for the

1452
01:08:03,440 --> 01:08:07,319
that one do you agree with the

1453
01:08:05,480 --> 01:08:09,039
prediction that having a job will be

1454
01:08:07,319 --> 01:08:12,319
optional in the

1455
01:08:09,039 --> 01:08:14,680
future um it depends how far in the

1456
01:08:12,319 --> 01:08:18,560
future but yeah if you

1457
01:08:14,680 --> 01:08:21,199
look far enough into future possibly yes

1458
01:08:18,560 --> 01:08:24,520
yes but not in not you don't see it as a

1459
01:08:21,199 --> 01:08:26,400
short to medium term thing maybe in the

1460
01:08:24,520 --> 01:08:28,000
next 10 years let's say yeah yeah well

1461
01:08:26,400 --> 01:08:30,239
and I'm sure you're doing the work of at

1462
01:08:28,000 --> 01:08:32,159
least several people right now in your

1463
01:08:30,239 --> 01:08:33,600
job so it's probably it's probably hard

1464
01:08:32,159 --> 01:08:36,040
to think

1465
01:08:33,600 --> 01:08:37,799
about jobs being optional where you're

1466
01:08:36,040 --> 01:08:43,359
sitting right

1467
01:08:37,799 --> 01:08:44,920
now um so uh so a 100 years from now how

1468
01:08:43,359 --> 01:08:47,199
do you think people will look back on

1469
01:08:44,920 --> 01:08:49,199
open AI what do you want open ai's

1470
01:08:47,199 --> 01:08:52,239
impact on the future and Humanity to be

1471
01:08:49,199 --> 01:08:52,239
in that sort of time

1472
01:08:52,799 --> 01:08:59,239
scale is a really long time the you can

1473
01:08:57,120 --> 01:09:02,480
pick a shorter one I'm just reading the

1474
01:08:59,239 --> 01:09:06,000
question know it was hard to predict

1475
01:09:02,480 --> 01:09:08,400
what was going to happen this year we

1476
01:09:06,000 --> 01:09:11,080
didn't predict the chbd moment but you

1477
01:09:08,400 --> 01:09:14,000
know looking far enough into the future

1478
01:09:11,080 --> 01:09:18,600
I I would hope

1479
01:09:14,000 --> 01:09:21,960
that at the very least open

1480
01:09:18,600 --> 01:09:24,319
AI um has

1481
01:09:21,960 --> 01:09:28,520
had a crucial

1482
01:09:24,319 --> 01:09:32,560
impact in helping Society build AI

1483
01:09:28,520 --> 01:09:34,199
systems in a way that's um safe and we

1484
01:09:32,560 --> 01:09:38,279
figure out how

1485
01:09:34,199 --> 01:09:41,480
to distribute the benefits of AI systems

1486
01:09:38,279 --> 01:09:43,759
to everyone and so whether it's open AI

1487
01:09:41,480 --> 01:09:45,440
that has figured that out or led the

1488
01:09:43,759 --> 01:09:49,000
work or

1489
01:09:45,440 --> 01:09:51,080
inspired um people to do that I think

1490
01:09:49,000 --> 01:09:54,480
that that's great but the end impact

1491
01:09:51,080 --> 01:09:58,199
like really figuring out how

1492
01:09:54,480 --> 01:10:03,480
the path to to safe and aligned Advanced

1493
01:09:58,199 --> 01:10:05,760
AI systems um and in addition to that I

1494
01:10:03,480 --> 01:10:08,679
would say

1495
01:10:05,760 --> 01:10:11,280
also inspiring

1496
01:10:08,679 --> 01:10:15,719
or helping

1497
01:10:11,280 --> 01:10:19,520
Drive um Innovative and practical ways

1498
01:10:15,719 --> 01:10:22,880
to good Global governance of AI systems

1499
01:10:19,520 --> 01:10:25,640
I think that's a huge huge challenge um

1500
01:10:22,880 --> 01:10:29,080
and a lot of governments and Nations

1501
01:10:25,640 --> 01:10:30,719
have sort of um become very aware about

1502
01:10:29,080 --> 01:10:32,480
this Challenge and are trying to

1503
01:10:30,719 --> 01:10:35,960
coordinate in different ways you know

1504
01:10:32,480 --> 01:10:39,000
through the UN or uh various Global

1505
01:10:35,960 --> 01:10:42,280
Summits and I hope that I think this is

1506
01:10:39,000 --> 01:10:46,679
incredibly important and I hope that

1507
01:10:42,280 --> 01:10:46,679
opening ey helps to get this

1508
01:10:48,239 --> 01:10:55,080
right thank you

1509
01:10:51,000 --> 01:10:57,560
um so uh how do you think about the role

1510
01:10:55,080 --> 01:11:00,000
of open academic research with respect

1511
01:10:57,560 --> 01:11:03,719
to sort of

1512
01:11:00,000 --> 01:11:05,600
um leading generative AI models so in

1513
01:11:03,719 --> 01:11:07,440
the early days of open AI you were

1514
01:11:05,600 --> 01:11:10,520
fairly active in publishing in the more

1515
01:11:07,440 --> 01:11:12,480
open academic literature uh and and more

1516
01:11:10,520 --> 01:11:14,960
recently that's less the case and not

1517
01:11:12,480 --> 01:11:16,920
just for open AI for you know is the as

1518
01:11:14,960 --> 01:11:19,040
this has become a more competitive space

1519
01:11:16,920 --> 01:11:21,600
companies that were sort of part of an

1520
01:11:19,040 --> 01:11:22,600
open research Community for very

1521
01:11:21,600 --> 01:11:24,679
understandable reasons have been

1522
01:11:22,600 --> 01:11:26,880
starting to do that less but how do we

1523
01:11:24,679 --> 01:11:29,360
how should we think about the the role

1524
01:11:26,880 --> 01:11:31,120
of open academic research uh and the

1525
01:11:29,360 --> 01:11:32,840
importance of that in the advancement of

1526
01:11:31,120 --> 01:11:37,080
things in this

1527
01:11:32,840 --> 01:11:40,760
area yeah so um first of all you know we

1528
01:11:37,080 --> 01:11:42,639
still try to do um open research in

1529
01:11:40,760 --> 01:11:46,800
areas where we can or you know when we

1530
01:11:42,639 --> 01:11:49,600
have systems that we can open source and

1531
01:11:46,800 --> 01:11:51,600
the larger Community can benefit from

1532
01:11:49,600 --> 01:11:54,199
them and do so in a way that we consider

1533
01:11:51,600 --> 01:11:57,239
to be safe we do that so we've open

1534
01:11:54,199 --> 01:12:01,199
sourced um elements of Del 3 and we've

1535
01:11:57,239 --> 01:12:02,840
open sourced our whisper system

1536
01:12:01,199 --> 01:12:06,800
um

1537
01:12:02,840 --> 01:12:10,440
and when yeah when when we look at what

1538
01:12:06,800 --> 01:12:12,400
we can open source um I know this a heed

1539
01:12:10,440 --> 01:12:16,159
debate currently open source say versus

1540
01:12:12,400 --> 01:12:20,679
not but I think the question is

1541
01:12:16,159 --> 01:12:24,600
really how do you think about alignment

1542
01:12:20,679 --> 01:12:28,120
safety second third degree consequen

1543
01:12:24,600 --> 01:12:30,080
of um releasing the systems and what

1544
01:12:28,120 --> 01:12:33,199
happens once they're released because

1545
01:12:30,080 --> 01:12:35,920
once they're uh open source you know we

1546
01:12:33,199 --> 01:12:38,960
don't have a lot of we can really

1547
01:12:35,920 --> 01:12:40,639
enforce guard rails and for with gbd4

1548
01:12:38,960 --> 01:12:42,880
for example you know we spent a really

1549
01:12:40,639 --> 01:12:45,440
long time figuring out the guard rails

1550
01:12:42,880 --> 01:12:50,679
and um around the system and how to make

1551
01:12:45,440 --> 01:12:52,480
it safe and how to deploy safely um and

1552
01:12:50,679 --> 01:12:53,960
we' have continued to build and invest

1553
01:12:52,480 --> 01:12:57,760
in the infrastructure

1554
01:12:53,960 --> 01:13:01,480
and so I think these are very important

1555
01:12:57,760 --> 01:13:03,280
questions not to be taken lightly but uh

1556
01:13:01,480 --> 01:13:05,239
in terms of like open academic research

1557
01:13:03,280 --> 01:13:09,120
it doesn't have to be all centered

1558
01:13:05,239 --> 01:13:12,600
around building large models um we want

1559
01:13:09,120 --> 01:13:15,800
to make them easily accessible and when

1560
01:13:12,600 --> 01:13:17,920
it comes to these large models and uh

1561
01:13:15,800 --> 01:13:22,480
for specific purposes like for example

1562
01:13:17,920 --> 01:13:25,639
figuring out um risks or understanding

1563
01:13:22,480 --> 01:13:28,760
capabilities we do a lot of Clos work

1564
01:13:25,639 --> 01:13:32,239
with um experts like for example in

1565
01:13:28,760 --> 01:13:33,440
misinformation or bio or whatever things

1566
01:13:32,239 --> 01:13:35,960
we're worried about when we're

1567
01:13:33,440 --> 01:13:40,560
developing a new model and we give them

1568
01:13:35,960 --> 01:13:44,000
more access than we give you know uh

1569
01:13:40,560 --> 01:13:46,480
General user or Customer because uh we

1570
01:13:44,000 --> 01:13:49,880
want them to really

1571
01:13:46,480 --> 01:13:52,760
understand the the capabilities and and

1572
01:13:49,880 --> 01:13:56,080
risks of the model so I think this is a

1573
01:13:52,760 --> 01:13:57,679
really important way to contribute um

1574
01:13:56,080 --> 01:13:59,560
with understanding the capabilities of

1575
01:13:57,679 --> 01:14:01,679
the risks of the models especially when

1576
01:13:59,560 --> 01:14:03,639
it comes to certain domains like you

1577
01:14:01,679 --> 01:14:10,120
know when we're thinking about cyber

1578
01:14:03,639 --> 01:14:12,199
security risk or um bio risk um or

1579
01:14:10,120 --> 01:14:15,199
misinformation um when we're developing

1580
01:14:12,199 --> 01:14:19,159
new models I think these are important

1581
01:14:15,199 --> 01:14:21,320
areas to collaborate or um or you know

1582
01:14:19,159 --> 01:14:22,960
figuring out even just new research

1583
01:14:21,320 --> 01:14:26,840
directions like for example we have

1584
01:14:22,960 --> 01:14:29,320
funded did a lot of um Safety Research

1585
01:14:26,840 --> 01:14:30,679
recently and both through open Ai and

1586
01:14:29,320 --> 01:14:33,440
also in a few

1587
01:14:30,679 --> 01:14:35,880
collaborations um through the AI safety

1588
01:14:33,440 --> 01:14:37,880
Summit so I think there are a ton of

1589
01:14:35,880 --> 01:14:40,520
opportunities for actually advancing

1590
01:14:37,880 --> 01:14:44,239
open academic research just about doing

1591
01:14:40,520 --> 01:14:48,760
so in ways that are not uh sort of

1592
01:14:44,239 --> 01:14:50,360
undermining the uh colossal amount of

1593
01:14:48,760 --> 01:14:52,880
safety work that we're doing when it

1594
01:14:50,360 --> 01:14:55,280
comes to developing and deploying large

1595
01:14:52,880 --> 01:14:55,280
language

1596
01:14:55,760 --> 01:15:00,159
models great so there are two two

1597
01:14:58,320 --> 01:15:04,440
threads there that I'd like to pick up

1598
01:15:00,159 --> 01:15:06,960
on um uh the first was you mentioned

1599
01:15:04,440 --> 01:15:08,400
about open source um and one of the

1600
01:15:06,960 --> 01:15:11,440
questions here had been are are there

1601
01:15:08,400 --> 01:15:15,679
any steps that can be taken uh you know

1602
01:15:11,440 --> 01:15:18,520
whether by companies like open AI or uh

1603
01:15:15,679 --> 01:15:22,560
you know governments or others consumers

1604
01:15:18,520 --> 01:15:25,760
even uh so that technology that is open

1605
01:15:22,560 --> 01:15:29,320
sourced in the generative AI realm isn't

1606
01:15:25,760 --> 01:15:29,320
uh modified for malicious

1607
01:15:29,800 --> 01:15:35,199
purposes um we're actually doing some

1608
01:15:32,679 --> 01:15:38,120
research on this right now but I I don't

1609
01:15:35,199 --> 01:15:39,480
have anything Noble to share but I think

1610
01:15:38,120 --> 01:15:42,920
I mean the bottom line is that it's

1611
01:15:39,480 --> 01:15:44,960
really hard once you're uh open sourcing

1612
01:15:42,920 --> 01:15:46,480
this models you don't really have

1613
01:15:44,960 --> 01:15:49,000
control

1614
01:15:46,480 --> 01:15:50,639
um over them anymore but we are doing

1615
01:15:49,000 --> 01:15:54,040
some research on it because clearly

1616
01:15:50,639 --> 01:15:56,040
there is also a lot of appetite to have

1617
01:15:54,040 --> 01:15:58,159
uh access to these Technologies and it's

1618
01:15:56,040 --> 01:16:00,199
completely understandable so we we've

1619
01:15:58,159 --> 01:16:03,040
been doing some research for a few

1620
01:16:00,199 --> 01:16:04,800
months now to figure out this question

1621
01:16:03,040 --> 01:16:06,480
and it's pretty challenging we'll

1622
01:16:04,800 --> 01:16:08,480
probably have something to share next

1623
01:16:06,480 --> 01:16:10,880
year I

1624
01:16:08,480 --> 01:16:12,560
think it's great to know you're you're

1625
01:16:10,880 --> 01:16:13,880
thinking about that I know I know others

1626
01:16:12,560 --> 01:16:16,880
are as well but as you said it's a

1627
01:16:13,880 --> 01:16:21,199
really really really tough uh tough

1628
01:16:16,880 --> 01:16:24,679
problem um so uh let me see I knew I had

1629
01:16:21,199 --> 01:16:26,520
a second question from that I forgot

1630
01:16:24,679 --> 01:16:29,800
should have uh it wasn't one of the ones

1631
01:16:26,520 --> 01:16:31,159
that was in the list um so another one

1632
01:16:29,800 --> 01:16:34,719
from the list say do you see open AI

1633
01:16:31,159 --> 01:16:38,239
branching out into more embodied

1634
01:16:34,719 --> 01:16:41,679
AI so that um you know robotics other

1635
01:16:38,239 --> 01:16:43,520
things where

1636
01:16:41,679 --> 01:16:48,400
uh

1637
01:16:43,520 --> 01:16:48,400
um I mean not in the next

1638
01:16:48,480 --> 01:16:54,760
year but is but know future but uh no we

1639
01:16:52,760 --> 01:16:56,520
you know I I don't think in the

1640
01:16:54,760 --> 01:16:58,840
immediate term we're not thinking about

1641
01:16:56,520 --> 01:17:01,280
robotics we are thinking about I'm sure

1642
01:16:58,840 --> 01:17:05,800
you've seen sort of yeah speculation in

1643
01:17:01,280 --> 01:17:08,639
the news and um thinking about device

1644
01:17:05,800 --> 01:17:12,639
integration and so so on like thinking

1645
01:17:08,639 --> 01:17:15,840
about uh what this future with AI

1646
01:17:12,639 --> 01:17:17,960
systems could look like and what future

1647
01:17:15,840 --> 01:17:20,320
devices could look like but that's just

1648
01:17:17,960 --> 01:17:23,320
really you know very early thinking and

1649
01:17:20,320 --> 01:17:26,280
nothing really concrete to share and

1650
01:17:23,320 --> 01:17:29,520
then on there's a lot of interest in

1651
01:17:26,280 --> 01:17:31,960
sort of explainability of these

1652
01:17:29,520 --> 01:17:34,040
models and sometimes some of us like to

1653
01:17:31,960 --> 01:17:35,760
say interpretability because

1654
01:17:34,040 --> 01:17:38,639
explainability sometimes suggests the

1655
01:17:35,760 --> 01:17:40,480
model can explain itself uh which which

1656
01:17:38,639 --> 01:17:44,440
is maybe a higher bar even than being

1657
01:17:40,480 --> 01:17:46,000
interpretable by people uh are those um

1658
01:17:44,440 --> 01:17:49,280
you know what are you doing looking at

1659
01:17:46,000 --> 01:17:52,199
those kinds of problems

1660
01:17:49,280 --> 01:17:54,719
does actually you know Alexander MRE

1661
01:17:52,199 --> 01:17:59,120
from MIT joined us recently to help us

1662
01:17:54,719 --> 01:18:02,719
think about these problems and actually

1663
01:17:59,120 --> 01:18:06,080
uh yeah supercharge the research around

1664
01:18:02,719 --> 01:18:09,080
interpretability and uh General

1665
01:18:06,080 --> 01:18:11,800
generally the science of predicting

1666
01:18:09,080 --> 01:18:16,440
capabilities as well

1667
01:18:11,800 --> 01:18:20,600
um obviously some work that uh we've

1668
01:18:16,440 --> 01:18:24,800
done is actually started ended up in

1669
01:18:20,600 --> 01:18:28,639
chbt but it it started as is uh

1670
01:18:24,800 --> 01:18:30,960
understanding um uh improving the

1671
01:18:28,639 --> 01:18:33,960
truthfulness and the factuality of this

1672
01:18:30,960 --> 01:18:35,360
models and understanding what was

1673
01:18:33,960 --> 01:18:37,440
happening so having some sort of

1674
01:18:35,360 --> 01:18:40,320
interface dialogue interface to get a

1675
01:18:37,440 --> 01:18:44,239
sense of what was happening with the

1676
01:18:40,320 --> 01:18:46,400
models um and it it ended up after a few

1677
01:18:44,239 --> 01:18:50,360
iterations of research and you know how

1678
01:18:46,400 --> 01:18:53,000
research evolves it ended up um in in

1679
01:18:50,360 --> 01:18:55,639
GPT but initially it started with this

1680
01:18:53,000 --> 01:18:58,480
question of how do we understand what is

1681
01:18:55,639 --> 01:19:01,480
happening with these models and Socratic

1682
01:18:58,480 --> 01:19:03,320
dialogue is a good way to even uh you

1683
01:19:01,480 --> 01:19:04,880
know understand what students are

1684
01:19:03,320 --> 01:19:06,719
thinking about or how well they

1685
01:19:04,880 --> 01:19:08,600
understand the subject or your colleague

1686
01:19:06,719 --> 01:19:10,760
or someone this is how we kind of

1687
01:19:08,600 --> 01:19:14,880
communicate and get a sense of you know

1688
01:19:10,760 --> 01:19:19,920
even in interviews how uh much expertise

1689
01:19:14,880 --> 01:19:21,280
someone has in a given subject and so uh

1690
01:19:19,920 --> 01:19:23,199
then there is a question of how much you

1691
01:19:21,280 --> 01:19:25,239
can trust that and hallucination and

1692
01:19:23,199 --> 01:19:27,360
other problems but it does give you some

1693
01:19:25,239 --> 01:19:29,239
insight when the model when you can when

1694
01:19:27,360 --> 01:19:31,120
you have this ability to interact with

1695
01:19:29,239 --> 01:19:35,320
the model in a way that we interact with

1696
01:19:31,120 --> 01:19:38,600
each other um so and and so I think that

1697
01:19:35,320 --> 01:19:40,679
is interesting and and the work that uh

1698
01:19:38,600 --> 01:19:43,719
We've started with our super alignment

1699
01:19:40,679 --> 01:19:46,560
team doing on interpretability and uh

1700
01:19:43,719 --> 01:19:48,480
new research that we are seeding um in

1701
01:19:46,560 --> 01:19:49,360
this direction but it's still really

1702
01:19:48,480 --> 01:19:52,000
quite

1703
01:19:49,360 --> 01:19:53,920
early great thank you so there are way

1704
01:19:52,000 --> 01:19:56,480
too many questions here so I'm going to

1705
01:19:53,920 --> 01:19:59,120
stop for a second and just say uh in

1706
01:19:56,480 --> 01:20:00,800
this in this whole long list of question

1707
01:19:59,120 --> 01:20:02,880
I I feel like you know we we're giving

1708
01:20:00,800 --> 01:20:06,679
you an oral exam or something one

1709
01:20:02,880 --> 01:20:08,040
question after another uh so uh what

1710
01:20:06,679 --> 01:20:10,840
have we not

1711
01:20:08,040 --> 01:20:14,239
asked that we should

1712
01:20:10,840 --> 01:20:14,239
have is there

1713
01:20:21,320 --> 01:20:30,520
anything no all right so uh that's good

1714
01:20:27,239 --> 01:20:32,040
so so do do you have any question you

1715
01:20:30,520 --> 01:20:34,159
would like to ask us because otherwise

1716
01:20:32,040 --> 01:20:35,400
with that I think I'll there's no way

1717
01:20:34,159 --> 01:20:36,960
I'm going to get through this list so

1718
01:20:35,400 --> 01:20:39,199
we're going to run out of time but uh

1719
01:20:36,960 --> 01:20:40,560
but I thought I'd see if there's a you

1720
01:20:39,199 --> 01:20:43,280
know any question you think we should

1721
01:20:40,560 --> 01:20:46,520
have asked or uh or any question you'd

1722
01:20:43,280 --> 01:20:48,719
like to hear uh from the collective uh

1723
01:20:46,520 --> 01:20:51,719
audience

1724
01:20:48,719 --> 01:20:54,679
here I think we covered quite a broad

1725
01:20:51,719 --> 01:20:57,400
range I hope it was

1726
01:20:54,679 --> 01:20:59,360
helpful yeah yeah so but we really

1727
01:20:57,400 --> 01:21:02,360
appreciate your your time mirror and

1728
01:20:59,360 --> 01:21:04,239
your thoughtful answers to uh all of our

1729
01:21:02,360 --> 01:21:06,080
questions as you can see there's a huge

1730
01:21:04,239 --> 01:21:08,360
amount of Interest here at MIT and what

1731
01:21:06,080 --> 01:21:11,520
you all are doing and an engagement uh

1732
01:21:08,360 --> 01:21:13,040
with open Ai and we look forward to uh

1733
01:21:11,520 --> 01:21:14,480
continuing to work with you all thank

1734
01:21:13,040 --> 01:21:18,159
thank you very much we really appreciate

1735
01:21:14,480 --> 01:21:18,159
your time so much thank

1736
01:21:19,840 --> 01:21:22,840
you

