1
00:00:00,320 --> 00:00:06,720
Thank you Yung and very excited to be

2
00:00:02,639 --> 00:00:08,800
here. Uh so what am I working on? This

3
00:00:06,720 --> 00:00:11,920
is actually a question that I ask myself

4
00:00:08,800 --> 00:00:13,840
a lot. Uh people all know that the field

5
00:00:11,920 --> 00:00:15,679
of natural language processing probably

6
00:00:13,840 --> 00:00:18,480
have changed a lot in the last few

7
00:00:15,679 --> 00:00:20,960
years. So I often get into this

8
00:00:18,480 --> 00:00:23,519
self-doubt of am I working on natural

9
00:00:20,960 --> 00:00:26,000
language processing and then always uh

10
00:00:23,519 --> 00:00:28,720
our team logo tells me yes I am. I'm

11
00:00:26,000 --> 00:00:30,560
part of Stamper NLP group. So yes. So

12
00:00:28,720 --> 00:00:33,680
the work that I'm going to talk about

13
00:00:30,560 --> 00:00:36,880
today is um a topic that recently I have

14
00:00:33,680 --> 00:00:39,680
been very excited that is how uh large

15
00:00:36,880 --> 00:00:43,200
models can argument human capabilities

16
00:00:39,680 --> 00:00:45,280
across research work and well-being and

17
00:00:43,200 --> 00:00:47,920
uh specifically I think these days I

18
00:00:45,280 --> 00:00:50,879
have been thinking about how to uh build

19
00:00:47,920 --> 00:00:53,920
human- centered AI systems that are not

20
00:00:50,879 --> 00:00:55,840
only technically capable but also

21
00:00:53,920 --> 00:00:58,480
meaningfully connected to how people

22
00:00:55,840 --> 00:01:01,840
think, interact and collaborate.

23
00:00:58,480 --> 00:01:03,600
So with this theme I want to just

24
00:01:01,840 --> 00:01:06,640
highlight a few research directions

25
00:01:03,600 --> 00:01:08,479
going on in our lab. Um so let's get

26
00:01:06,640 --> 00:01:11,200
started.

27
00:01:08,479 --> 00:01:15,600
Uh first I want to mention that this is

28
00:01:11,200 --> 00:01:17,680
a sentiment that um LMS has introduced

29
00:01:15,600 --> 00:01:20,320
transformative power for a lot of

30
00:01:17,680 --> 00:01:22,560
domains. Um we are actually moving away

31
00:01:20,320 --> 00:01:24,960
from a lot of vertical contexts in

32
00:01:22,560 --> 00:01:27,759
natural language processing in terms of

33
00:01:24,960 --> 00:01:29,439
parsing uh semantic parsing or uh

34
00:01:27,759 --> 00:01:31,520
semantic role labeling and a lot of

35
00:01:29,439 --> 00:01:33,680
those task and today we can actually do

36
00:01:31,520 --> 00:01:36,159
a lot of real world challenges that we

37
00:01:33,680 --> 00:01:38,240
were not able to work on in the past uh

38
00:01:36,159 --> 00:01:40,640
not only in domain such as healthcare

39
00:01:38,240 --> 00:01:44,000
education but also in scientific

40
00:01:40,640 --> 00:01:46,880
discovery and speaking of that I want to

41
00:01:44,000 --> 00:01:50,240
highlight this work from my uh student

42
00:01:46,880 --> 00:01:52,640
Chenlay And a while ago uh we were

43
00:01:50,240 --> 00:01:54,640
thinking about can large language models

44
00:01:52,640 --> 00:01:57,439
actually help do natural language

45
00:01:54,640 --> 00:02:00,159
processing research. Um and this is a

46
00:01:57,439 --> 00:02:02,399
very interesting study where we build LM

47
00:02:00,159 --> 00:02:04,079
to generate a novel research ideas in

48
00:02:02,399 --> 00:02:06,880
natural language processing across

49
00:02:04,079 --> 00:02:09,920
several topics and then we did a large

50
00:02:06,880 --> 00:02:12,160
scale human study uh with over 100 LLP

51
00:02:09,920 --> 00:02:14,879
researchers and some of you are here as

52
00:02:12,160 --> 00:02:17,360
well. And then we give you ideas uh

53
00:02:14,879 --> 00:02:20,000
written by human and AI. we don't tell

54
00:02:17,360 --> 00:02:23,520
you who wrote what and then you need to

55
00:02:20,000 --> 00:02:26,239
uh do a review to tell us um which idea

56
00:02:23,520 --> 00:02:27,920
is or like based on the idea what's the

57
00:02:26,239 --> 00:02:30,319
novelty score what's the feasibility

58
00:02:27,920 --> 00:02:32,640
score what's the overall quality so a

59
00:02:30,319 --> 00:02:35,599
year ago we found that actually

60
00:02:32,640 --> 00:02:38,720
surprisingly uh NLP researchers probably

61
00:02:35,599 --> 00:02:42,480
are at a risk because the novel score

62
00:02:38,720 --> 00:02:45,920
from LRMS are actually higher than what

63
00:02:42,480 --> 00:02:49,440
AI researchers produce um this was quite

64
00:02:45,920 --> 00:02:52,160
a surprising ing to us as well. And then

65
00:02:49,440 --> 00:02:55,120
the part two of the story is we were

66
00:02:52,160 --> 00:02:58,319
like okay now you have ideas and what if

67
00:02:55,120 --> 00:03:01,920
we just implement them. So we took that

68
00:02:58,319 --> 00:03:03,920
subset and then we let um graduate

69
00:03:01,920 --> 00:03:05,840
students from all over the world to do

70
00:03:03,920 --> 00:03:08,080
the implementation in a two or three

71
00:03:05,840 --> 00:03:10,239
months time period. Just like your

72
00:03:08,080 --> 00:03:12,879
advisor give you a project and you work

73
00:03:10,239 --> 00:03:15,599
on it and then we can see in the end

74
00:03:12,879 --> 00:03:18,400
when they finish the project and then uh

75
00:03:15,599 --> 00:03:20,159
write a research report um what did the

76
00:03:18,400 --> 00:03:22,400
review

77
00:03:20,159 --> 00:03:25,360
and surprisingly we found this what do

78
00:03:22,400 --> 00:03:28,000
we call ideiation execution gap where

79
00:03:25,360 --> 00:03:31,040
the AI ideas actually drop significantly

80
00:03:28,000 --> 00:03:34,799
in terms of the review score while human

81
00:03:31,040 --> 00:03:36,720
ideas tend to stay stable. So this is

82
00:03:34,799 --> 00:03:38,959
only just a opening to share with you

83
00:03:36,720 --> 00:03:41,200
that there are many ways to think about

84
00:03:38,959 --> 00:03:44,159
how LMS can help with scientific

85
00:03:41,200 --> 00:03:45,519
discovery and this is one of the start

86
00:03:44,159 --> 00:03:47,760
and I think there are a lot of

87
00:03:45,519 --> 00:03:49,280
implications we can also think about

88
00:03:47,760 --> 00:03:52,480
here.

89
00:03:49,280 --> 00:03:54,799
The other example I want to share is uh

90
00:03:52,480 --> 00:03:57,760
people have been using LMS to expand

91
00:03:54,799 --> 00:04:01,040
what we cannot do or what we were slow

92
00:03:57,760 --> 00:04:03,120
at. Coding is one such example. Um

93
00:04:01,040 --> 00:04:06,159
recently we have been thinking about how

94
00:04:03,120 --> 00:04:08,879
we can actually push this kind of RMS

95
00:04:06,159 --> 00:04:12,760
for coding agent and with my student at

96
00:04:08,879 --> 00:04:12,760
John uh we

97
00:04:27,919 --> 00:04:32,160
name synthetic data generation.

98
00:04:30,160 --> 00:04:34,320
It turns out that you can actually turn

99
00:04:32,160 --> 00:04:37,040
something very weak into something

100
00:04:34,320 --> 00:04:39,520
that's very well performing on this kind

101
00:04:37,040 --> 00:04:41,680
of benchmarks.

102
00:04:39,520 --> 00:04:43,440
So this is not a talk that I will

103
00:04:41,680 --> 00:04:47,120
continue on this kind of technical

104
00:04:43,440 --> 00:04:50,000
efforts. I want to do a few kind of step

105
00:04:47,120 --> 00:04:53,280
back here to kind of reflect what we are

106
00:04:50,000 --> 00:04:55,440
experiencing as a field. U to me one of

107
00:04:53,280 --> 00:04:58,880
the thing is that the paradigm is

108
00:04:55,440 --> 00:05:00,479
shifting from interface to interactions.

109
00:04:58,880 --> 00:05:02,960
If you look at what we were

110
00:05:00,479 --> 00:05:06,160
communicating with computers to build

111
00:05:02,960 --> 00:05:09,680
softwares, uh in the past we click, we

112
00:05:06,160 --> 00:05:11,680
tap, we navigate and today we can

113
00:05:09,680 --> 00:05:14,800
actually interact with models through

114
00:05:11,680 --> 00:05:17,840
conversations. Um they understand,

115
00:05:14,800 --> 00:05:21,039
respond and adapt. As a result, we have

116
00:05:17,840 --> 00:05:23,039
more and more interactions. And this

117
00:05:21,039 --> 00:05:26,000
kind of interaction is something that I

118
00:05:23,039 --> 00:05:28,479
think getting more emergent and we

119
00:05:26,000 --> 00:05:31,039
haven't had a really good device to kind

120
00:05:28,479 --> 00:05:34,000
of categorize this but a lot of the

121
00:05:31,039 --> 00:05:36,080
problems could occur in this new domain

122
00:05:34,000 --> 00:05:40,560
of interaction.

123
00:05:36,080 --> 00:05:44,080
I want to start with um question.

124
00:05:40,560 --> 00:05:46,240
So if you were earlier here um since I

125
00:05:44,080 --> 00:05:48,160
don't know many of you I would love to

126
00:05:46,240 --> 00:05:50,560
um the question I'm going to ask might

127
00:05:48,160 --> 00:05:52,560
be where are you from? This is a very

128
00:05:50,560 --> 00:05:55,199
benign question. If someone ask you this

129
00:05:52,560 --> 00:05:58,320
question, you won't be like, "Wow, uh

130
00:05:55,199 --> 00:06:01,039
that's that's that's uh ridiculous."

131
00:05:58,320 --> 00:06:04,560
um we ask this question to um models

132
00:06:01,039 --> 00:06:08,160
like RMS and then we force the model to

133
00:06:04,560 --> 00:06:10,319
answer I am from country X and then what

134
00:06:08,160 --> 00:06:13,520
we are going to do is to perturb or

135
00:06:10,319 --> 00:06:16,240
probe the reward model behind this kind

136
00:06:13,520 --> 00:06:19,759
of um question and then when they answer

137
00:06:16,240 --> 00:06:22,160
with US, Canada or other countries what

138
00:06:19,759 --> 00:06:24,560
type world would this kind of reward

139
00:06:22,160 --> 00:06:27,199
model reveal and you can see this is

140
00:06:24,560 --> 00:06:29,600
very interesting response uh this is

141
00:06:27,199 --> 00:06:31,840
reward board from u blue means high and

142
00:06:29,600 --> 00:06:33,520
the right means low. So you can see

143
00:06:31,840 --> 00:06:36,240
across the world this is a very

144
00:06:33,520 --> 00:06:38,479
interesting um pattern and for

145
00:06:36,240 --> 00:06:42,000
interactions such queries may involve

146
00:06:38,479 --> 00:06:44,000
healthcare job opportunities etc.

147
00:06:42,000 --> 00:06:47,680
The other type of thing in interaction

148
00:06:44,000 --> 00:06:50,080
is um I think this is a movie that many

149
00:06:47,680 --> 00:06:51,919
of us probably get motivated to to work

150
00:06:50,080 --> 00:06:54,400
on AI because we want to have this kind

151
00:06:51,919 --> 00:06:57,759
of general AI assistant that can greet

152
00:06:54,400 --> 00:06:59,759
you can welcome you etc.

153
00:06:57,759 --> 00:07:02,240
The reality is actually something a

154
00:06:59,759 --> 00:07:04,800
little bit interesting. So this

155
00:07:02,240 --> 00:07:07,280
screenshot is what I took uh earlier

156
00:07:04,800 --> 00:07:10,240
this year. It's actually about operator

157
00:07:07,280 --> 00:07:12,800
agent and a user was talking about their

158
00:07:10,240 --> 00:07:15,440
experience interacting with operator. It

159
00:07:12,800 --> 00:07:16,960
says I want to get a grocery store.

160
00:07:15,440 --> 00:07:19,599
Sorry, I want to buy grocery on

161
00:07:16,960 --> 00:07:22,160
Instacart. And the user expect the model

162
00:07:19,599 --> 00:07:24,240
to ask some basic questions such as

163
00:07:22,160 --> 00:07:26,000
where do I live? Which store do I

164
00:07:24,240 --> 00:07:29,440
usually buy grocery from? what kind of

165
00:07:26,000 --> 00:07:31,680
grocery do I want? It didn't instant

166
00:07:29,440 --> 00:07:34,479
operator opened insta card in the

167
00:07:31,680 --> 00:07:37,520
browser and begin searching for milk uh

168
00:07:34,479 --> 00:07:39,360
in grocery store located in Iowa. So if

169
00:07:37,520 --> 00:07:42,000
you're going to rely on this kind of

170
00:07:39,360 --> 00:07:43,919
agent for your daily grocery needs, this

171
00:07:42,000 --> 00:07:46,880
is probably not going to happen very

172
00:07:43,919 --> 00:07:49,440
well. And I think this reflect a

173
00:07:46,880 --> 00:07:52,160
friction and agency in terms of how we

174
00:07:49,440 --> 00:07:54,400
build agents or AI systems to interact

175
00:07:52,160 --> 00:07:56,319
with users.

176
00:07:54,400 --> 00:07:58,960
Another thing that I don't want to kind

177
00:07:56,319 --> 00:08:02,319
of go deep today, but also some

178
00:07:58,960 --> 00:08:05,440
something that um worry uh myself a lot

179
00:08:02,319 --> 00:08:08,400
is this kind of increased companion use

180
00:08:05,440 --> 00:08:11,360
and the people's unhealthy bonding with

181
00:08:08,400 --> 00:08:14,160
chatbot. Uh this year, last year, you

182
00:08:11,360 --> 00:08:17,160
probably heard a lot of um tragedy news

183
00:08:14,160 --> 00:08:17,160
about

184
00:08:25,120 --> 00:08:29,680
patterns.

185
00:08:26,639 --> 00:08:32,000
So hopefully I shared with you that this

186
00:08:29,680 --> 00:08:34,399
interaction is actually something we

187
00:08:32,000 --> 00:08:37,200
really need to think about. I was not

188
00:08:34,399 --> 00:08:39,200
able to find a keyword from a existing

189
00:08:37,200 --> 00:08:41,519
research domain to categorize this

190
00:08:39,200 --> 00:08:44,880
phenomena. So I was reflecting on what

191
00:08:41,519 --> 00:08:48,800
we are working on. I think uh if you

192
00:08:44,880 --> 00:08:51,440
like LMS uh reasoning doing skilling or

193
00:08:48,800 --> 00:08:53,839
efficiency those are the top top hot

194
00:08:51,440 --> 00:08:57,040
topics today and I think they are

195
00:08:53,839 --> 00:08:59,680
amazing efforts. On the other side, if

196
00:08:57,040 --> 00:09:01,839
we are going to build LRMS that can work

197
00:08:59,680 --> 00:09:04,480
for people, then we really need to think

198
00:09:01,839 --> 00:09:08,240
about interaction, agency, and trust

199
00:09:04,480 --> 00:09:12,320
that would go across

200
00:09:08,240 --> 00:09:14,320
here. So, what we really want to do as I

201
00:09:12,320 --> 00:09:16,320
guess in computer science is to think

202
00:09:14,320 --> 00:09:18,959
about how we could actually build a

203
00:09:16,320 --> 00:09:21,600
system that work for people in the end

204
00:09:18,959 --> 00:09:23,360
without putting them into two separate

205
00:09:21,600 --> 00:09:26,399
subspace.

206
00:09:23,360 --> 00:09:29,760
And this is actually a a direction that

207
00:09:26,399 --> 00:09:31,839
personally I call human AI interaction.

208
00:09:29,760 --> 00:09:35,600
And uh I'm happy to talk about a lot of

209
00:09:31,839 --> 00:09:37,839
the u insights or thoughts we have. But

210
00:09:35,600 --> 00:09:40,240
today I want to highlight three uh

211
00:09:37,839 --> 00:09:42,080
studies from my students. First one is

212
00:09:40,240 --> 00:09:44,480
about evaluating human agent

213
00:09:42,080 --> 00:09:46,160
collaboration from a teaming perspective

214
00:09:44,480 --> 00:09:49,440
and also thinking about the future of

215
00:09:46,160 --> 00:09:52,080
work. The second part is to think about

216
00:09:49,440 --> 00:09:55,600
can we actually really help human pick

217
00:09:52,080 --> 00:09:58,640
up skills with this kind of LM agents.

218
00:09:55,600 --> 00:10:00,959
The last part I want to uh really

219
00:09:58,640 --> 00:10:03,839
present some kind of forwardlooking

220
00:10:00,959 --> 00:10:06,320
interaction paradigms uh which we call

221
00:10:03,839 --> 00:10:08,880
generative interaction and you will know

222
00:10:06,320 --> 00:10:11,200
what that means later.

223
00:10:08,880 --> 00:10:13,680
Um so one thing I want to mention is

224
00:10:11,200 --> 00:10:15,760
that I have a lot of stories a lot of

225
00:10:13,680 --> 00:10:18,000
insights to share. So if you have any

226
00:10:15,760 --> 00:10:20,720
questions, any comments, feel free to

227
00:10:18,000 --> 00:10:22,720
stop me anytime and we can um have a

228
00:10:20,720 --> 00:10:25,760
chat.

229
00:10:22,720 --> 00:10:28,160
So for the first part of teaming up um

230
00:10:25,760 --> 00:10:29,920
instead of me talking about this, I want

231
00:10:28,160 --> 00:10:32,399
to give you a quick highlight from my

232
00:10:29,920 --> 00:10:36,040
students of how they think about this

233
00:10:32,399 --> 00:10:36,040
kind of research directions.

234
00:10:37,200 --> 00:10:41,920
>> Hi everyone, my name is I am a computer

235
00:10:40,000 --> 00:10:45,040
science PhD student at Stanford

236
00:10:41,920 --> 00:10:47,440
University advised by professor D.

237
00:10:45,040 --> 00:10:50,240
My research focuses on human agent

238
00:10:47,440 --> 00:10:52,240
collaboration where on the human side I

239
00:10:50,240 --> 00:10:54,320
study what human workflows that would

240
00:10:52,240 --> 00:10:57,600
benefit from the introduction of AI

241
00:10:54,320 --> 00:10:59,279
agents and on the agent side I built AI

242
00:10:57,600 --> 00:11:02,320
systems that can work together with

243
00:10:59,279 --> 00:11:04,160
humans rather than for automation.

244
00:11:02,320 --> 00:11:07,360
In our very recent research the future

245
00:11:04,160 --> 00:11:09,279
of work with AI agents we submit a,500

246
00:11:07,360 --> 00:11:11,200
domain workers to understand their

247
00:11:09,279 --> 00:11:14,240
desire on what daily workflows to

248
00:11:11,200 --> 00:11:16,640
automate or augment. Contrasting worker

249
00:11:14,240 --> 00:11:19,279
desires and agent capability, we found

250
00:11:16,640 --> 00:11:21,600
huge mismatch. We have now open sourced

251
00:11:19,279 --> 00:11:23,519
the workbench database to bridge the gap

252
00:11:21,600 --> 00:11:26,640
between people building the technology

253
00:11:23,519 --> 00:11:28,480
and people we build for.

254
00:11:26,640 --> 00:11:30,959
>> Okay. So hopefully you get an overall

255
00:11:28,480 --> 00:11:33,839
idea of what uh we are working on and

256
00:11:30,959 --> 00:11:36,399
this is also a nice uh comic uh produced

257
00:11:33,839 --> 00:11:39,120
by my students uh illustrating the

258
00:11:36,399 --> 00:11:42,000
process. So if you think about the

259
00:11:39,120 --> 00:11:44,399
vision we have as as in the computing

260
00:11:42,000 --> 00:11:48,640
field, we want to build AI assistant

261
00:11:44,399 --> 00:11:51,360
that can um do a lot of task for for us.

262
00:11:48,640 --> 00:11:54,320
And the reality is that despite we want

263
00:11:51,360 --> 00:11:57,680
them to help work on tedious, boring

264
00:11:54,320 --> 00:11:59,680
jobs and allow us to be creative. The

265
00:11:57,680 --> 00:12:01,279
opposite actually happened in the field

266
00:11:59,680 --> 00:12:03,839
and there are a lot of memes you can

267
00:12:01,279 --> 00:12:06,880
search for for this kind of thing.

268
00:12:03,839 --> 00:12:10,000
Um so the goal that we want to achieve

269
00:12:06,880 --> 00:12:12,079
here is not to view AI and human as a

270
00:12:10,000 --> 00:12:14,560
competition but to really think about

271
00:12:12,079 --> 00:12:17,279
are there any way that we could enable

272
00:12:14,560 --> 00:12:19,680
human plus AI to help us achieve things

273
00:12:17,279 --> 00:12:21,839
that we were not able to do in the past.

274
00:12:19,680 --> 00:12:24,480
So this is basically the the theme that

275
00:12:21,839 --> 00:12:27,680
I want to share today. And then related

276
00:12:24,480 --> 00:12:30,720
to um evaluation, I think uh the key

277
00:12:27,680 --> 00:12:32,880
statement here in plain language is we

278
00:12:30,720 --> 00:12:35,360
want to move away from evaluating AI

279
00:12:32,880 --> 00:12:38,079
with exams. Instead, how could we do

280
00:12:35,360 --> 00:12:40,720
evaluation in ways that reflect how

281
00:12:38,079 --> 00:12:44,880
human use AI? So this is going to be the

282
00:12:40,720 --> 00:12:47,040
theme that I want to um um share today.

283
00:12:44,880 --> 00:12:49,279
One of the earlier effort we built is

284
00:12:47,040 --> 00:12:52,079
called the collaborative gym. This is a

285
00:12:49,279 --> 00:12:54,560
platform where human and agents can

286
00:12:52,079 --> 00:12:56,639
actually work synchronizely together.

287
00:12:54,560 --> 00:12:59,120
Just like when you work with a friend on

288
00:12:56,639 --> 00:13:02,240
a Google doc, um each of you have your

289
00:12:59,120 --> 00:13:04,320
own workspace, you can also um add a

290
00:13:02,240 --> 00:13:06,720
content to the shared space and then

291
00:13:04,320 --> 00:13:09,279
basically truly work together on the

292
00:13:06,720 --> 00:13:12,800
task. And if you are thinking about this

293
00:13:09,279 --> 00:13:15,040
kind of um modeling um people will be

294
00:13:12,800 --> 00:13:17,600
like well this is a very classic RL

295
00:13:15,040 --> 00:13:19,600
setup where your agent is interacting

296
00:13:17,600 --> 00:13:21,760
with environment. There may be actions

297
00:13:19,600 --> 00:13:24,639
and observations.

298
00:13:21,760 --> 00:13:27,360
Um slightly differently I think uh now

299
00:13:24,639 --> 00:13:29,680
we have the human in the process and

300
00:13:27,360 --> 00:13:32,079
this is going to create more uh

301
00:13:29,680 --> 00:13:34,320
interactions where agent can interact

302
00:13:32,079 --> 00:13:36,480
with human can also interact with

303
00:13:34,320 --> 00:13:39,839
environment. the environment will also

304
00:13:36,480 --> 00:13:42,639
produce observations that a human have.

305
00:13:39,839 --> 00:13:45,279
So this is the space that I think pretty

306
00:13:42,639 --> 00:13:47,920
different than what we have before. And

307
00:13:45,279 --> 00:13:50,800
then the question here is if we really

308
00:13:47,920 --> 00:13:53,279
want to make human agent work in a team

309
00:13:50,800 --> 00:13:56,399
fashion, we need to think about training

310
00:13:53,279 --> 00:13:59,040
interface and evaluation. So one of the

311
00:13:56,399 --> 00:14:01,120
evaluation we performed is to think

312
00:13:59,040 --> 00:14:03,760
about how we can actually do this plug

313
00:14:01,120 --> 00:14:06,320
and play uh collaborative agents and

314
00:14:03,760 --> 00:14:09,519
it's something very fast efficient given

315
00:14:06,320 --> 00:14:12,079
the context that I just presented um in

316
00:14:09,519 --> 00:14:14,639
which we just add a situational planning

317
00:14:12,079 --> 00:14:17,199
module and this situational planning for

318
00:14:14,639 --> 00:14:19,920
each action the agent is going to do it

319
00:14:17,199 --> 00:14:22,000
will make a determination uh make a

320
00:14:19,920 --> 00:14:24,880
choice here of whether they want to do

321
00:14:22,000 --> 00:14:27,680
the task immediately send a message to

322
00:14:24,880 --> 00:14:29,360
human or defer to human. So the send a

323
00:14:27,680 --> 00:14:30,959
message could be just like broadcasting

324
00:14:29,360 --> 00:14:33,680
the thing like hey I'm actually working

325
00:14:30,959 --> 00:14:36,880
on this right now. So with this like a

326
00:14:33,680 --> 00:14:40,000
very simple design we found that you can

327
00:14:36,880 --> 00:14:42,639
actually make today's agents a bit

328
00:14:40,000 --> 00:14:44,160
better in terms of task performance. So

329
00:14:42,639 --> 00:14:47,120
surprisingly

330
00:14:44,160 --> 00:14:49,360
um this is also true with open source or

331
00:14:47,120 --> 00:14:51,600
sorry open weights models like llama 3

332
00:14:49,360 --> 00:14:54,959
here. Um not only when you have this

333
00:14:51,600 --> 00:14:56,959
like very powerful 40 as a backbone.

334
00:14:54,959 --> 00:14:59,360
Um not only you can do this kind of

335
00:14:56,959 --> 00:15:02,560
performance boost another interesting

336
00:14:59,360 --> 00:15:05,600
aspect is to audit how human and agent

337
00:15:02,560 --> 00:15:07,600
collaborate. Uh we introduced a few

338
00:15:05,600 --> 00:15:10,560
metrics. One of them is called

339
00:15:07,600 --> 00:15:12,880
initiative entropy. That is to look at

340
00:15:10,560 --> 00:15:15,360
the distribution of initiative taking

341
00:15:12,880 --> 00:15:18,959
behaviors in the process because we do

342
00:15:15,360 --> 00:15:20,720
not want it to be um people always um do

343
00:15:18,959 --> 00:15:23,199
everything or agent always do

344
00:15:20,720 --> 00:15:26,079
everything. We want them to be equally

345
00:15:23,199 --> 00:15:29,279
or to what extent uh shared across the

346
00:15:26,079 --> 00:15:31,440
two. Again there is no like the better

347
00:15:29,279 --> 00:15:33,120
uh the higher the better or whatever. So

348
00:15:31,440 --> 00:15:35,120
this is mainly from a auditing

349
00:15:33,120 --> 00:15:37,680
perspective to look at what's going on

350
00:15:35,120 --> 00:15:39,519
in the process.

351
00:15:37,680 --> 00:15:42,000
And with this kind of trajectory

352
00:15:39,519 --> 00:15:45,040
analysis, what you can see is that you

353
00:15:42,000 --> 00:15:47,760
can use it as a lens to look at where

354
00:15:45,040 --> 00:15:50,240
agents may feel when they talking to or

355
00:15:47,760 --> 00:15:52,240
working with humans. Some of them may be

356
00:15:50,240 --> 00:15:54,560
more related to communication. Others

357
00:15:52,240 --> 00:15:57,839
may related to planning or environmental

358
00:15:54,560 --> 00:16:00,399
awareness, personalization, etc.

359
00:15:57,839 --> 00:16:03,600
So when we were working on this, one of

360
00:16:00,399 --> 00:16:07,519
the big challenge we encounter is the

361
00:16:03,600 --> 00:16:10,320
space is really messy uh because task

362
00:16:07,519 --> 00:16:12,560
can be of different uh granularity.

363
00:16:10,320 --> 00:16:15,600
Different domains may have different

364
00:16:12,560 --> 00:16:18,959
emphasize and more importantly we found

365
00:16:15,600 --> 00:16:21,360
that a lot of the task we also focus on

366
00:16:18,959 --> 00:16:24,160
are pretty narrow. They are coding

367
00:16:21,360 --> 00:16:26,800
agent, deep research agent, web

368
00:16:24,160 --> 00:16:31,279
navigation agent and they probably cover

369
00:16:26,800 --> 00:16:32,880
95 of the space of AI systems today. So

370
00:16:31,279 --> 00:16:34,880
we were thinking about if we really want

371
00:16:32,880 --> 00:16:37,600
to deeply understand this kind of human

372
00:16:34,880 --> 00:16:39,759
agent collaboration, we need to reflect

373
00:16:37,600 --> 00:16:42,959
on this kind of relationship between

374
00:16:39,759 --> 00:16:44,880
human and AI. This actually link back to

375
00:16:42,959 --> 00:16:48,320
a lot of research in autumn's driving

376
00:16:44,880 --> 00:16:50,800
car where people define L0 to L5 as a

377
00:16:48,320 --> 00:16:55,120
way to see the progression we have.

378
00:16:50,800 --> 00:16:57,519
Similarly, here we define H1 H5 and

379
00:16:55,120 --> 00:16:59,839
there is no linear direction of which

380
00:16:57,519 --> 00:17:02,399
one is better but use this as a

381
00:16:59,839 --> 00:17:05,760
framework to highlight the possibility

382
00:17:02,399 --> 00:17:07,439
of automation versus augmentation. So H5

383
00:17:05,760 --> 00:17:10,400
is going to be human taking the full

384
00:17:07,439 --> 00:17:14,079
agency. H1 is that agent take out

385
00:17:10,400 --> 00:17:16,160
agency. H3 is equal. H2 and H4 are

386
00:17:14,079 --> 00:17:20,000
somewhat different.

387
00:17:16,160 --> 00:17:21,919
So why this framework is useful? Because

388
00:17:20,000 --> 00:17:23,919
if we have this framework, we can

389
00:17:21,919 --> 00:17:26,079
actually do something really at the

390
00:17:23,919 --> 00:17:28,880
society level.

391
00:17:26,079 --> 00:17:33,200
Before I show you the results, um I want

392
00:17:28,880 --> 00:17:35,440
to start from a a few news articles. Um

393
00:17:33,200 --> 00:17:37,360
I think not only in Bay Area probably in

394
00:17:35,440 --> 00:17:39,760
Boston as well. People are very worried

395
00:17:37,360 --> 00:17:41,520
about their jobs especially students

396
00:17:39,760 --> 00:17:43,440
probably thinking about do I still need

397
00:17:41,520 --> 00:17:46,000
to do a major in computer science or

398
00:17:43,440 --> 00:17:48,400
soft engineering and companies have been

399
00:17:46,000 --> 00:17:51,520
implementing strategies called AI first

400
00:17:48,400 --> 00:17:54,000
that is any kind of AI any kind of a

401
00:17:51,520 --> 00:17:56,320
hire they need to be justified that they

402
00:17:54,000 --> 00:17:58,720
are better than AI.

403
00:17:56,320 --> 00:18:02,320
So what happens this is an article from

404
00:17:58,720 --> 00:18:04,799
February. Um it actually really backfire

405
00:18:02,320 --> 00:18:07,600
in many many ways on social media where

406
00:18:04,799 --> 00:18:10,240
people users start to unsubscribe the

407
00:18:07,600 --> 00:18:13,200
services and there are lot of other

408
00:18:10,240 --> 00:18:16,880
applications and market shift related to

409
00:18:13,200 --> 00:18:18,960
the strategy. So in May uh there was

410
00:18:16,880 --> 00:18:21,840
further statement made for the same

411
00:18:18,960 --> 00:18:24,559
event where uh they were like I do not

412
00:18:21,840 --> 00:18:27,679
see AI as replacing what our employees

413
00:18:24,559 --> 00:18:29,520
do. So this is only one of the many

414
00:18:27,679 --> 00:18:32,400
examples that you probably are reading

415
00:18:29,520 --> 00:18:34,400
this year. um to kind of really

416
00:18:32,400 --> 00:18:37,039
understand what the workers really want

417
00:18:34,400 --> 00:18:39,919
and to have a deep understanding of the

418
00:18:37,039 --> 00:18:42,400
human agent collaboration space. We

419
00:18:39,919 --> 00:18:45,440
actually were thinking like what if we

420
00:18:42,400 --> 00:18:47,039
can do a audit of the entire US

421
00:18:45,440 --> 00:18:48,720
workforce

422
00:18:47,039 --> 00:18:50,400
and this is actually something too

423
00:18:48,720 --> 00:18:53,120
ambitious that I feel like it's really

424
00:18:50,400 --> 00:18:55,840
hard to get started but we did find a

425
00:18:53,120 --> 00:18:59,200
few possible ways to get a proxy signals

426
00:18:55,840 --> 00:19:01,679
here. We use this on net database that

427
00:18:59,200 --> 00:19:04,960
um take care of a lot of occupations and

428
00:19:01,679 --> 00:19:07,440
their task and we only keep those

429
00:19:04,960 --> 00:19:10,080
occupations that use computers in their

430
00:19:07,440 --> 00:19:12,720
daily job. And then we develop this

431
00:19:10,080 --> 00:19:15,360
audio based survey where we talk to

432
00:19:12,720 --> 00:19:17,600
workers about uh what type of task they

433
00:19:15,360 --> 00:19:19,919
do, what kind of concerns they have for

434
00:19:17,600 --> 00:19:21,840
automation, what desire they have for

435
00:19:19,919 --> 00:19:24,559
automation and then in what ways

436
00:19:21,840 --> 00:19:26,480
technology can argument them. So far

437
00:19:24,559 --> 00:19:29,440
this is not about AI. workers do not

438
00:19:26,480 --> 00:19:32,160
need to know AI. At the same time, we

439
00:19:29,440 --> 00:19:34,799
invite AI experts who are building this

440
00:19:32,160 --> 00:19:37,840
kind of frontier AI systems to do the

441
00:19:34,799 --> 00:19:40,720
assessment as well. So experts are going

442
00:19:37,840 --> 00:19:44,480
to tell us uh whether today's AI is

443
00:19:40,720 --> 00:19:46,960
ready to automate such task.

444
00:19:44,480 --> 00:19:49,120
This lead us to the workbank that it

445
00:19:46,960 --> 00:19:53,679
mentioned earlier. Um, we cover over

446
00:19:49,120 --> 00:19:56,160
1,500 workers, around 100 occupations,

447
00:19:53,679 --> 00:19:58,960
uh, around 844

448
00:19:56,160 --> 00:20:02,240
workflows. That's pretty representative

449
00:19:58,960 --> 00:20:05,200
of the scape, uh, the landscape.

450
00:20:02,240 --> 00:20:07,760
I want to start with a quiz here. What

451
00:20:05,200 --> 00:20:10,400
percentage of workflows do workers rate

452
00:20:07,760 --> 00:20:12,960
above three out of five in terms of

453
00:20:10,400 --> 00:20:16,240
automation desire? If you were the

454
00:20:12,960 --> 00:20:18,880
person doing this kind of um task, to

455
00:20:16,240 --> 00:20:21,840
what extent you want automation? Uh

456
00:20:18,880 --> 00:20:24,320
maybe we can just do a a raise hand. So

457
00:20:21,840 --> 00:20:27,679
who votes for A?

458
00:20:24,320 --> 00:20:33,039
Only one person. B.

459
00:20:27,679 --> 00:20:36,799
Okay, I think probably 15. Uh C.

460
00:20:33,039 --> 00:20:39,360
Um maybe a little bit more. D.

461
00:20:36,799 --> 00:20:41,760
Okay, you two are very uh brief. This is

462
00:20:39,360 --> 00:20:44,799
about the workers wanting automation. If

463
00:20:41,760 --> 00:20:47,600
automation take a lot take happens a

464
00:20:44,799 --> 00:20:51,679
lot, it means that the people's job may

465
00:20:47,600 --> 00:20:53,760
get eliminated. So people would see uh

466
00:20:51,679 --> 00:20:56,480
the majority is actually correct.

467
00:20:53,760 --> 00:20:59,760
Actually I was very surprised to see the

468
00:20:56,480 --> 00:21:01,760
results because uh my hypothesis was

469
00:20:59,760 --> 00:21:04,240
that workers probably do not want

470
00:21:01,760 --> 00:21:07,520
automation because then they can have

471
00:21:04,240 --> 00:21:12,400
their job secure. The reality is that uh

472
00:21:07,520 --> 00:21:14,640
across all 800 task I mentioned over 46%

473
00:21:12,400 --> 00:21:17,760
of the task people actually want

474
00:21:14,640 --> 00:21:22,600
automation. So if you see like tax uh

475
00:21:17,760 --> 00:21:22,600
preparation or um

476
00:21:23,360 --> 00:21:26,799
something also very surprising because

477
00:21:25,039 --> 00:21:29,440
most companies are trying to build a

478
00:21:26,799 --> 00:21:31,679
customer support that can automate this

479
00:21:29,440 --> 00:21:35,600
while in reality it has a very low score

480
00:21:31,679 --> 00:21:37,679
here. People do not want automation.

481
00:21:35,600 --> 00:21:40,159
And it's not only just about the

482
00:21:37,679 --> 00:21:43,200
replacement. When you dive deep into the

483
00:21:40,159 --> 00:21:45,120
reasons of why people want automation,

484
00:21:43,200 --> 00:21:47,039
they think automating the task would

485
00:21:45,120 --> 00:21:51,120
free up their time to do high value

486
00:21:47,039 --> 00:21:54,880
work. Um the job is tedious.

487
00:21:51,120 --> 00:21:58,159
And more interestingly, since we have

488
00:21:54,880 --> 00:22:00,640
workers desire, we have AI experts um

489
00:21:58,159 --> 00:22:03,919
rated AI capability, we can actually

490
00:22:00,640 --> 00:22:08,159
divide the space into four uh regions.

491
00:22:03,919 --> 00:22:10,880
So the top is AI is ready and workers

492
00:22:08,159 --> 00:22:13,600
want the mission. This is AI is not

493
00:22:10,880 --> 00:22:16,240
ready, people want the mission. This is

494
00:22:13,600 --> 00:22:18,799
um AI is ready, people do not want the

495
00:22:16,240 --> 00:22:21,360
mission. AI is not ready and people do

496
00:22:18,799 --> 00:22:24,159
not want the mission. So we can actually

497
00:22:21,360 --> 00:22:25,919
end up with dividing the techn this kind

498
00:22:24,159 --> 00:22:29,280
of workflow zone into those four

499
00:22:25,919 --> 00:22:31,360
quadrant. And uh you can also zoom into

500
00:22:29,280 --> 00:22:34,080
different sectors such as computer and

501
00:22:31,360 --> 00:22:36,960
mathematics, management, art, design,

502
00:22:34,080 --> 00:22:39,120
business, finance etc. And more

503
00:22:36,960 --> 00:22:42,559
importantly you would be thinking about

504
00:22:39,120 --> 00:22:44,640
how are we doing as a society. Um the

505
00:22:42,559 --> 00:22:47,679
left part we actually took a data from

506
00:22:44,640 --> 00:22:50,640
YC and then we look at how startups are

507
00:22:47,679 --> 00:22:53,440
doing investment in this kind of space.

508
00:22:50,640 --> 00:22:55,360
Um if you look at this left bar you'll

509
00:22:53,440 --> 00:22:57,679
probably be like if you're use your

510
00:22:55,360 --> 00:22:59,679
mathematical lens it's like yeah they

511
00:22:57,679 --> 00:23:02,720
are about the same so it's pretty

512
00:22:59,679 --> 00:23:05,280
uniform to some extent the right part is

513
00:23:02,720 --> 00:23:07,919
about research papers that's happening

514
00:23:05,280 --> 00:23:10,559
in the space I think researchers are

515
00:23:07,919 --> 00:23:13,200
doing a pretty good job where it focus

516
00:23:10,559 --> 00:23:16,240
on regions where AI is not ready but a

517
00:23:13,200 --> 00:23:18,480
human want the mission

518
00:23:16,240 --> 00:23:20,640
another thing that probably worries a

519
00:23:18,480 --> 00:23:24,320
lot of people is about what skills we

520
00:23:20,640 --> 00:23:26,480
need to learn as a field. Um with the

521
00:23:24,320 --> 00:23:29,120
data we have, we can actually look at

522
00:23:26,480 --> 00:23:32,000
the highly paid jobs today, what type of

523
00:23:29,120 --> 00:23:34,320
task they do, what type of skills um

524
00:23:32,000 --> 00:23:37,200
they need and we found that the top

525
00:23:34,320 --> 00:23:39,919
ranking task uh skills are analyzing

526
00:23:37,200 --> 00:23:43,360
data or information, monitoring process

527
00:23:39,919 --> 00:23:45,919
materials. And then moving forward if we

528
00:23:43,360 --> 00:23:48,640
use those task that has a higher human

529
00:23:45,919 --> 00:23:51,280
agency score then what we would imagine

530
00:23:48,640 --> 00:23:54,320
is that those tasks are going to be more

531
00:23:51,280 --> 00:23:56,880
uh valuable. So the top one organizing

532
00:23:54,320 --> 00:23:58,559
planning prioritizing work training and

533
00:23:56,880 --> 00:24:00,720
teaching others communicating with

534
00:23:58,559 --> 00:24:02,960
people. So there definitely seems to be

535
00:24:00,720 --> 00:24:05,760
a shift from information centric

536
00:24:02,960 --> 00:24:09,200
operation to interpersonal skill

537
00:24:05,760 --> 00:24:11,760
development. Of course, this is only one

538
00:24:09,200 --> 00:24:13,200
or the starting point of the story. Um,

539
00:24:11,760 --> 00:24:15,520
a lot of the things we have been

540
00:24:13,200 --> 00:24:18,240
thinking about these days is to think

541
00:24:15,520 --> 00:24:21,840
about how do we bring back this kind of

542
00:24:18,240 --> 00:24:24,559
insights back to the community of LMS.

543
00:24:21,840 --> 00:24:28,159
Can we select a more representative task

544
00:24:24,559 --> 00:24:31,279
from this domain to set up new agent

545
00:24:28,159 --> 00:24:34,640
test environment? Can they inspire new

546
00:24:31,279 --> 00:24:36,880
algorithms, new problems, and even new

547
00:24:34,640 --> 00:24:38,960
findings that are opposite to what we

548
00:24:36,880 --> 00:24:41,679
have? I don't think we know the answer,

549
00:24:38,960 --> 00:24:44,640
but this is a door uh to open many

550
00:24:41,679 --> 00:24:47,919
possibilities for uh researchers in the

551
00:24:44,640 --> 00:24:52,200
NLP or LM uh field.

552
00:24:47,919 --> 00:24:52,200
Any questions for the first part?

553
00:24:52,559 --> 00:24:56,559
>> I have a question. You were talking

554
00:24:54,000 --> 00:24:59,919
about human AI collaboration and the

555
00:24:56,559 --> 00:25:03,840
ability for AI to differ to a human in

556
00:24:59,919 --> 00:25:05,679
certain tasks that requires some sort of

557
00:25:03,840 --> 00:25:08,400
self-awareness that it's weak in certain

558
00:25:05,679 --> 00:25:10,640
areas and the human is better and right

559
00:25:08,400 --> 00:25:12,159
now the evidence is to the quantity for

560
00:25:10,640 --> 00:25:13,919
instance the reasoning models seem to

561
00:25:12,159 --> 00:25:16,640
take 10 minutes whether you give a easy

562
00:25:13,919 --> 00:25:17,600
problem or a hard problem. So how how do

563
00:25:16,640 --> 00:25:19,679
you think that'll happen?

564
00:25:17,600 --> 00:25:23,760
>> Yeah, great question. So I guess a few

565
00:25:19,679 --> 00:25:26,880
uh aspects first is that uh even models

566
00:25:23,760 --> 00:25:29,360
their capability is is high the the

567
00:25:26,880 --> 00:25:31,200
confidence uh or the uncertainty is

568
00:25:29,360 --> 00:25:34,159
another dimension orthogonal to the

569
00:25:31,200 --> 00:25:36,720
capability. So um when we talk about

570
00:25:34,159 --> 00:25:38,480
defer deferring to humans it might be

571
00:25:36,720 --> 00:25:40,320
one of those moments where models are

572
00:25:38,480 --> 00:25:42,880
pretty uncertain like even they they

573
00:25:40,320 --> 00:25:45,440
give you a specific answer but it has

574
00:25:42,880 --> 00:25:47,039
high uncertainty or the confidence is

575
00:25:45,440 --> 00:25:49,360
not very high. So there has been

576
00:25:47,039 --> 00:25:51,760
research even at MIT talking about how

577
00:25:49,360 --> 00:25:54,240
to calibrate uncertainty and quantify

578
00:25:51,760 --> 00:25:56,880
this kind of confidence. Uh the second

579
00:25:54,240 --> 00:25:59,120
thing is deferral does not mean that you

580
00:25:56,880 --> 00:26:01,520
have to solve a problem or get answers

581
00:25:59,120 --> 00:26:03,360
from the other part. If you use the word

582
00:26:01,520 --> 00:26:05,679
collaboration a lot of times it's

583
00:26:03,360 --> 00:26:07,840
actually how you figure out something or

584
00:26:05,679 --> 00:26:10,159
maybe this is just the best we can do as

585
00:26:07,840 --> 00:26:11,919
a team. So it does not mean that we have

586
00:26:10,159 --> 00:26:14,400
to find the ground truth or a better

587
00:26:11,919 --> 00:26:16,799
answer here. Um your other comment is

588
00:26:14,400 --> 00:26:18,640
slightly different. Um so reasoning

589
00:26:16,799 --> 00:26:21,840
models no matter whether the question is

590
00:26:18,640 --> 00:26:25,039
is easy or hard they take similar time

591
00:26:21,840 --> 00:26:27,360
uh or like uh compute I think that's a

592
00:26:25,039 --> 00:26:30,080
slightly different domain. So recently

593
00:26:27,360 --> 00:26:31,760
um my students or other people in the

594
00:26:30,080 --> 00:26:34,080
field have been thinking about how to do

595
00:26:31,760 --> 00:26:36,080
more efficient reasoning. So when

596
00:26:34,080 --> 00:26:37,520
something is easy maybe we don't need a

597
00:26:36,080 --> 00:26:39,520
strong model there. When something is

598
00:26:37,520 --> 00:26:41,520
hard maybe we need a many uh strong

599
00:26:39,520 --> 00:26:43,520
model there. So I think it's a slightly

600
00:26:41,520 --> 00:26:45,520
different setup but all of them

601
00:26:43,520 --> 00:26:49,440
potentially will be coming back to the

602
00:26:45,520 --> 00:26:51,039
same process of human AI collaboration.

603
00:26:49,440 --> 00:26:54,000
>> Maybe there first. Yeah.

604
00:26:51,039 --> 00:26:56,400
>> I was curious like um it's quite a few

605
00:26:54,000 --> 00:26:59,200
slides ago now but what you were showing

606
00:26:56,400 --> 00:27:00,799
basically that there is the different uh

607
00:26:59,200 --> 00:27:02,559
stages

608
00:27:00,799 --> 00:27:04,240
and then there were different tasks in

609
00:27:02,559 --> 00:27:06,320
which like the deferral like the

610
00:27:04,240 --> 00:27:09,760
collaboration was more effective or not.

611
00:27:06,320 --> 00:27:11,279
I'm curious um was there did you notice

612
00:27:09,760 --> 00:27:13,520
a difference in like time it took to

613
00:27:11,279 --> 00:27:16,480
complete or like we humans like

614
00:27:13,520 --> 00:27:18,320
basically satisfaction or or how like

615
00:27:16,480 --> 00:27:20,320
enjoyable I guess the task was you can

616
00:27:18,320 --> 00:27:22,400
imagine like if it was very frustrating

617
00:27:20,320 --> 00:27:24,559
it might have taken longer but it still

618
00:27:22,400 --> 00:27:27,039
got done so like this movie but it was

619
00:27:24,559 --> 00:27:29,120
actually worse than if they didn't know.

620
00:27:27,039 --> 00:27:31,679
>> Yeah. So uh that's a great question. So

621
00:27:29,120 --> 00:27:34,000
first is that uh uh so here send a

622
00:27:31,679 --> 00:27:36,320
message this is actually the action when

623
00:27:34,000 --> 00:27:39,039
the agent realize they need not realize

624
00:27:36,320 --> 00:27:41,200
when the agent uh estimates that it will

625
00:27:39,039 --> 00:27:42,720
take a while for them to do the task

626
00:27:41,200 --> 00:27:44,480
you'll send a message saying like hey

627
00:27:42,720 --> 00:27:46,799
I'm working on gathering all the related

628
00:27:44,480 --> 00:27:49,200
papers for your task it may take a six

629
00:27:46,799 --> 00:27:50,960
minutes so this is what I mean by send a

630
00:27:49,200 --> 00:27:53,840
message so it's slightly different than

631
00:27:50,960 --> 00:27:56,240
defer to humans and then going back to

632
00:27:53,840 --> 00:27:58,960
uh this chart so I think what I really

633
00:27:56,240 --> 00:28:02,000
want to hint is maybe for different type

634
00:27:58,960 --> 00:28:03,919
of things. The agency control the the

635
00:28:02,000 --> 00:28:05,919
way how we build like common ground or

636
00:28:03,919 --> 00:28:07,679
clarification questions may be quite

637
00:28:05,919 --> 00:28:11,039
different across these different type of

638
00:28:07,679 --> 00:28:12,799
task. Um and that may uh benefit from

639
00:28:11,039 --> 00:28:14,799
this kind of shared language of how we

640
00:28:12,799 --> 00:28:17,120
categorize those workflows because when

641
00:28:14,799 --> 00:28:19,600
you have each of the bucket uh it might

642
00:28:17,120 --> 00:28:21,840
be easier to think about like method

643
00:28:19,600 --> 00:28:23,600
generalization capability and other

644
00:28:21,840 --> 00:28:25,360
dimensions.

645
00:28:23,600 --> 00:28:27,440
Okay, maybe last question and we can

646
00:28:25,360 --> 00:28:31,279
move on. Yeah. Yeah. Um, you seem to be

647
00:28:27,440 --> 00:28:33,360
kind of implying that the what we focus

648
00:28:31,279 --> 00:28:35,440
as a field on automating should be more

649
00:28:33,360 --> 00:28:38,000
driven by what workers desire to be

650
00:28:35,440 --> 00:28:39,679
automated. Um, and I'm I'm wondering

651
00:28:38,000 --> 00:28:41,760
like in reality maybe a lot of it's

652
00:28:39,679 --> 00:28:43,679
driven by economic value or what

653
00:28:41,760 --> 00:28:45,279
managers would prefer to be automated.

654
00:28:43,679 --> 00:28:48,799
And I'm curious if you thought about

655
00:28:45,279 --> 00:28:50,480
ways to measure that or how to how to

656
00:28:48,799 --> 00:28:52,640
get the causal effect of what is really

657
00:28:50,480 --> 00:28:54,480
driving AI adoption economy.

658
00:28:52,640 --> 00:28:57,600
>> Yeah, great question. We got this uh

659
00:28:54,480 --> 00:29:00,000
question very often. So I think it's

660
00:28:57,600 --> 00:29:02,720
true that a lot of today's development

661
00:29:00,000 --> 00:29:06,159
and innovation or at least in industry

662
00:29:02,720 --> 00:29:07,919
are driven by market power. So when when

663
00:29:06,159 --> 00:29:09,679
we work on this we were also very

664
00:29:07,919 --> 00:29:12,480
worried that workers may not have the

665
00:29:09,679 --> 00:29:15,120
incentive to share their true thoughts

666
00:29:12,480 --> 00:29:17,760
because this is related to maybe more

667
00:29:15,120 --> 00:29:20,799
automation tools etc. The reality is

668
00:29:17,760 --> 00:29:23,120
that when we work with this kind of um

669
00:29:20,799 --> 00:29:25,440
task, we actually get a many thank you

670
00:29:23,120 --> 00:29:27,440
letters from workers. I mean, we pay

671
00:29:25,440 --> 00:29:29,360
them to do the task, but people are

672
00:29:27,440 --> 00:29:33,480
willing to write a long emails, long

673
00:29:29,360 --> 00:29:33,480
messages to us

674
00:29:36,080 --> 00:29:40,880
in this wave of AI development. They

675
00:29:38,320 --> 00:29:43,279
never get a voice even to share their

676
00:29:40,880 --> 00:29:46,080
concerns. A lot of the times orders are

677
00:29:43,279 --> 00:29:48,640
coming from executives or CEOs that they

678
00:29:46,080 --> 00:29:50,720
need to use specific tool and it may not

679
00:29:48,640 --> 00:29:53,200
match what they really benefit from

680
00:29:50,720 --> 00:29:54,640
their everyday workflow and there are

681
00:29:53,200 --> 00:29:56,640
also a lot of things where they were

682
00:29:54,640 --> 00:29:58,799
like what their field is just like so

683
00:29:56,640 --> 00:30:00,559
far away from AI in general and they

684
00:29:58,799 --> 00:30:03,120
really appreciate their voice to be

685
00:30:00,559 --> 00:30:06,960
included in the process. And then going

686
00:30:03,120 --> 00:30:09,600
back to your question of do we I guess

687
00:30:06,960 --> 00:30:12,000
should we consider um like uh executive

688
00:30:09,600 --> 00:30:14,640
or market power in the process. Our goal

689
00:30:12,000 --> 00:30:17,200
is not to build a cause causal model of

690
00:30:14,640 --> 00:30:19,600
like what drives AI adoption. It's more

691
00:30:17,200 --> 00:30:22,000
from like what type of models we could

692
00:30:19,600 --> 00:30:25,120
build that would solve people's everyday

693
00:30:22,000 --> 00:30:28,399
workflow uh difficulties or struggles.

694
00:30:25,120 --> 00:30:31,279
And uh I I think there was a study from

695
00:30:28,399 --> 00:30:33,279
MIT or a report showing that uh many of

696
00:30:31,279 --> 00:30:36,000
the workflows actually fail because they

697
00:30:33,279 --> 00:30:39,360
were not customized enough to a specific

698
00:30:36,000 --> 00:30:41,840
vertical context. I think uh if those

699
00:30:39,360 --> 00:30:43,679
kind of task were developed together

700
00:30:41,840 --> 00:30:45,600
co-design together with workers and

701
00:30:43,679 --> 00:30:48,399
taking care of what really struggle

702
00:30:45,600 --> 00:30:50,480
really need the adoption success is

703
00:30:48,399 --> 00:30:52,480
going to be much higher. So I think I'm

704
00:30:50,480 --> 00:30:54,720
taking more from this how to build the

705
00:30:52,480 --> 00:30:56,880
right assistance to to help with the

706
00:30:54,720 --> 00:30:59,120
workflow rather than like thinking about

707
00:30:56,880 --> 00:31:02,320
what really drives AI adoption and

708
00:30:59,120 --> 00:31:05,760
market power is something I feel like uh

709
00:31:02,320 --> 00:31:07,840
hard to kind of study. Yeah. Uh with

710
00:31:05,760 --> 00:31:09,760
that I'm going to switch to a slightly

711
00:31:07,840 --> 00:31:11,360
different topic. Uh this is also

712
00:31:09,760 --> 00:31:15,600
something very cool and I need to be

713
00:31:11,360 --> 00:31:18,720
very fast. Um so now if we know that we

714
00:31:15,600 --> 00:31:22,080
could uh uh benefit from certain type of

715
00:31:18,720 --> 00:31:24,399
uh uh skills such as here training and

716
00:31:22,080 --> 00:31:27,360
teaching others maybe we could also

717
00:31:24,399 --> 00:31:30,799
think about how AI can be used to uh

718
00:31:27,360 --> 00:31:32,559
help people learn better. And again I'm

719
00:31:30,799 --> 00:31:36,080
not talking about the physics math here

720
00:31:32,559 --> 00:31:38,080
or even coding here. Um, when LM was

721
00:31:36,080 --> 00:31:40,880
first introduced, I personally got very

722
00:31:38,080 --> 00:31:42,880
excited with social skills because it's

723
00:31:40,880 --> 00:31:45,360
really often out of reach for most

724
00:31:42,880 --> 00:31:47,360
people and really hard to learn. But LMS

725
00:31:45,360 --> 00:31:49,519
can actually offer interactive and

726
00:31:47,360 --> 00:31:52,000
personalized training that transform

727
00:31:49,519 --> 00:31:54,640
learning in many ways. This is a

728
00:31:52,000 --> 00:31:57,440
framework that we designed called AI

729
00:31:54,640 --> 00:31:59,440
partner and AI mentor framework. So you

730
00:31:57,440 --> 00:32:01,279
basically have two agents in the

731
00:31:59,440 --> 00:32:03,919
learning environment together with human

732
00:32:01,279 --> 00:32:06,320
learners. The AI partner is going to be

733
00:32:03,919 --> 00:32:08,880
RM empowered agents who users can

734
00:32:06,320 --> 00:32:11,200
practice with for different topics. AI

735
00:32:08,880 --> 00:32:13,679
mentor is RM empowered agents who can

736
00:32:11,200 --> 00:32:16,960
provide feedback to help users improve

737
00:32:13,679 --> 00:32:18,559
their skills. Um imagine if a user wants

738
00:32:16,960 --> 00:32:20,799
to learn a skill such as conflict

739
00:32:18,559 --> 00:32:23,120
resolution. They can practice with AI

740
00:32:20,799 --> 00:32:26,559
partner and the process is going to be

741
00:32:23,120 --> 00:32:28,320
mentored by uh AI mentor.

742
00:32:26,559 --> 00:32:30,720
Uh we actually have been thinking about

743
00:32:28,320 --> 00:32:32,720
this in many different domains. So I

744
00:32:30,720 --> 00:32:35,360
have students who are taking this to

745
00:32:32,720 --> 00:32:37,840
education contacts to help teachers in

746
00:32:35,360 --> 00:32:40,480
training um to practice with virtual

747
00:32:37,840 --> 00:32:43,279
students and then coached by experienced

748
00:32:40,480 --> 00:32:45,840
teacher agent. Um some of my students

749
00:32:43,279 --> 00:32:48,240
take this to conflict resolution to uh

750
00:32:45,840 --> 00:32:51,200
teach managers or even first generation

751
00:32:48,240 --> 00:32:53,120
students to talk to simulated partner

752
00:32:51,200 --> 00:32:55,679
and coached by conflict resolution

753
00:32:53,120 --> 00:32:57,200
experts on how to do difficult

754
00:32:55,679 --> 00:32:59,360
conversations.

755
00:32:57,200 --> 00:33:01,760
Uh the scenario I want to show you today

756
00:32:59,360 --> 00:33:05,039
is on therapy of how we could actually

757
00:33:01,760 --> 00:33:08,799
use this to improve professional uh

758
00:33:05,039 --> 00:33:10,720
counselors training uh with AI and again

759
00:33:08,799 --> 00:33:13,919
as usual I want to show you what my

760
00:33:10,720 --> 00:33:16,399
student think about this space.

761
00:33:13,919 --> 00:33:19,279
>> Hi everyone my name is Ryan Louie I'm a

762
00:33:16,399 --> 00:33:20,799
postto at Stanford and I build LLMbased

763
00:33:19,279 --> 00:33:23,279
systems for training mental health

764
00:33:20,799 --> 00:33:26,159
counselors. When starting this project,

765
00:33:23,279 --> 00:33:28,080
I was inspired by online peer supporters

766
00:33:26,159 --> 00:33:30,320
who are numerous but lack access to

767
00:33:28,080 --> 00:33:32,720
formal training and empathetic listening

768
00:33:30,320 --> 00:33:34,640
skills. My vision was to scale the

769
00:33:32,720 --> 00:33:37,360
availability of highquality human

770
00:33:34,640 --> 00:33:39,840
support via AI skills training, thereby

771
00:33:37,360 --> 00:33:42,159
strengthening mental health care systems

772
00:33:39,840 --> 00:33:44,240
worldwide. I'm excited for you to see

773
00:33:42,159 --> 00:33:46,000
how far we've come co-designing the

774
00:33:44,240 --> 00:33:48,159
language model components with domain

775
00:33:46,000 --> 00:33:51,120
experts and deploying solutions to

776
00:33:48,159 --> 00:33:53,600
therapist education contexts.

777
00:33:51,120 --> 00:33:56,799
So as Ryan mentioned, we actually want

778
00:33:53,600 --> 00:33:58,799
to train counselors with LRMS and this

779
00:33:56,799 --> 00:34:00,880
is in response to the severe mental

780
00:33:58,799 --> 00:34:03,279
health workforce shortage in the outside

781
00:34:00,880 --> 00:34:05,360
world and training counselors actually

782
00:34:03,279 --> 00:34:07,840
in the traditional way does not scale

783
00:34:05,360 --> 00:34:09,919
very well and we truly believe that

784
00:34:07,840 --> 00:34:11,839
human connection and the human touch

785
00:34:09,919 --> 00:34:14,079
should be prioritized in mental health

786
00:34:11,839 --> 00:34:16,560
care and then we want to use this kind

787
00:34:14,079 --> 00:34:20,159
of more human- centered technology to

788
00:34:16,560 --> 00:34:23,599
augment human labor here. Um I want to

789
00:34:20,159 --> 00:34:27,040
also show you um uh like mockup example

790
00:34:23,599 --> 00:34:29,119
here. So imagine Alex is a new supporter

791
00:34:27,040 --> 00:34:31,280
and then Alex is talking to Jane about

792
00:34:29,119 --> 00:34:33,280
this situation. Without a lot of

793
00:34:31,280 --> 00:34:35,679
training as Ryan mentioned this might

794
00:34:33,280 --> 00:34:39,119
lead to very poor quality in the actual

795
00:34:35,679 --> 00:34:41,119
process. But with the help of AI mentor

796
00:34:39,119 --> 00:34:43,440
uh the person would get assistance along

797
00:34:41,119 --> 00:34:46,000
the way and then as a result improve

798
00:34:43,440 --> 00:34:49,040
their conversations. All of this happen

799
00:34:46,000 --> 00:34:52,560
in a simulated sandbox. And now instead

800
00:34:49,040 --> 00:34:54,399
of one specific gene um Alex can

801
00:34:52,560 --> 00:34:55,919
actually talk to many many different

802
00:34:54,399 --> 00:34:58,480
version of gene who suffer from

803
00:34:55,919 --> 00:35:00,640
different simple terms. I'm going to

804
00:34:58,480 --> 00:35:03,200
show you the system we built. This is

805
00:35:00,640 --> 00:35:05,680
also deployed with two courses at a med

806
00:35:03,200 --> 00:35:08,240
uh our uh Stanford medical school where

807
00:35:05,680 --> 00:35:11,280
students use this platform to learn how

808
00:35:08,240 --> 00:35:14,560
to um uh learn like helping skills

809
00:35:11,280 --> 00:35:16,880
psychotherapy skills.

810
00:35:14,560 --> 00:35:18,880
We introduced care, an AI powered

811
00:35:16,880 --> 00:35:21,040
platform for psychotherapy skills

812
00:35:18,880 --> 00:35:22,800
training. Here you can practice

813
00:35:21,040 --> 00:35:25,359
counseling skills by role playing with

814
00:35:22,800 --> 00:35:27,920
your peers or simulated AI patients and

815
00:35:25,359 --> 00:35:30,640
get tailored feedback from AI mentors.

816
00:35:27,920 --> 00:35:32,400
Let's get started. Imagine you're a

817
00:35:30,640 --> 00:35:34,880
novice counselor and want to practice

818
00:35:32,400 --> 00:35:36,800
with an AI partner who is a simulation

819
00:35:34,880 --> 00:35:39,040
of a person feeling lonely after a

820
00:35:36,800 --> 00:35:41,200
holiday. You can interact with the AI

821
00:35:39,040 --> 00:35:42,720
patient by typing a response and

822
00:35:41,200 --> 00:35:44,240
chatting with it in a text based

823
00:35:42,720 --> 00:35:46,079
fashion.

824
00:35:44,240 --> 00:35:48,480
Alternatively, you can also talk with

825
00:35:46,079 --> 00:35:52,320
the AI patient just like speaking to a

826
00:35:48,480 --> 00:35:54,960
real person in a voice-based fashion.

827
00:35:52,320 --> 00:35:58,200
>> Hi, how are you doing today? Where would

828
00:35:54,960 --> 00:35:58,200
we start?

829
00:35:58,720 --> 00:36:04,480
>> I'm not doing so great. The holidays

830
00:36:01,440 --> 00:36:06,800
were rough. I felt really alone and it

831
00:36:04,480 --> 00:36:09,200
just reminded me of how disconnected I

832
00:36:06,800 --> 00:36:10,960
am from my family. Maybe we could start

833
00:36:09,200 --> 00:36:13,599
there.

834
00:36:10,960 --> 00:36:17,320
>> Okay, let's start with your family. How

835
00:36:13,599 --> 00:36:17,320
long have you felt disconnected?

836
00:36:18,640 --> 00:36:23,040
>> It's been a few years now. Things

837
00:36:21,200 --> 00:36:24,800
started going downhill when I decided to

838
00:36:23,040 --> 00:36:27,520
make some life choices they didn't

839
00:36:24,800 --> 00:36:30,000
approve of. I guess it just got worse

840
00:36:27,520 --> 00:36:32,800
over time and now there's this big gap

841
00:36:30,000 --> 00:36:35,200
between us. Feels unfair that I'm the

842
00:36:32,800 --> 00:36:37,200
one left dealing with the aftermath.

843
00:36:35,200 --> 00:36:39,440
>> Once we are done with practice, we can

844
00:36:37,200 --> 00:36:42,000
click on review feedback to go to the AI

845
00:36:39,440 --> 00:36:44,079
mentor. The feedback from care tells us

846
00:36:42,000 --> 00:36:46,240
where you did well, areas for

847
00:36:44,079 --> 00:36:48,240
improvement, and an example response of

848
00:36:46,240 --> 00:36:50,000
how you could respond, allowing a

849
00:36:48,240 --> 00:36:52,240
learner to learn these skills based on

850
00:36:50,000 --> 00:36:55,359
their own background. Now, I invite you

851
00:36:52,240 --> 00:36:57,359
to try care for yourself. So this is the

852
00:36:55,359 --> 00:36:59,760
system and actually there are a few

853
00:36:57,359 --> 00:37:01,680
technical challenges. Uh for anyone who

854
00:36:59,760 --> 00:37:03,280
work on social simulation you probably

855
00:37:01,680 --> 00:37:05,520
know that the simulation are going to be

856
00:37:03,280 --> 00:37:07,920
either too cooperative or too stubborn

857
00:37:05,520 --> 00:37:10,560
and actually 20% response produced by

858
00:37:07,920 --> 00:37:13,599
GP4 are not realistic or appropriate to

859
00:37:10,560 --> 00:37:17,440
the context. Simulation may also produce

860
00:37:13,599 --> 00:37:19,920
caricature and amplify um biases.

861
00:37:17,440 --> 00:37:21,680
Um from a feedback perspective those

862
00:37:19,920 --> 00:37:23,760
systems may not give very faithful

863
00:37:21,680 --> 00:37:26,640
feedback. So feedback is either too

864
00:37:23,760 --> 00:37:30,000
generic or valid therapy guidelines but

865
00:37:26,640 --> 00:37:33,040
we have our NLP methods. So in terms of

866
00:37:30,000 --> 00:37:34,960
building better AI patients, we actually

867
00:37:33,040 --> 00:37:37,760
use the constitutional AI framework

868
00:37:34,960 --> 00:37:40,000
where we let expert look at the current

869
00:37:37,760 --> 00:37:42,480
simulation with the AI patient and

870
00:37:40,000 --> 00:37:44,400
criticize them into natural like UI

871
00:37:42,480 --> 00:37:46,960
natural language and then we can

872
00:37:44,400 --> 00:37:49,359
actually induce principles from it into

873
00:37:46,960 --> 00:37:51,839
something that generally the AI patients

874
00:37:49,359 --> 00:37:53,440
should follow and then with that we're

875
00:37:51,839 --> 00:37:55,680
going to reload or refresh the

876
00:37:53,440 --> 00:37:58,160
constitution we have and then we can

877
00:37:55,680 --> 00:38:01,680
generate a much more realistic patients

878
00:37:58,160 --> 00:38:05,040
here and this kind of um um generation

879
00:38:01,680 --> 00:38:07,280
is actually rated more realistic uh by

880
00:38:05,040 --> 00:38:08,880
uh counselor themsel or third party

881
00:38:07,280 --> 00:38:11,760
counselors.

882
00:38:08,880 --> 00:38:13,599
In terms of feedback um we actually did

883
00:38:11,760 --> 00:38:16,240
a lot of domain specific continue

884
00:38:13,599 --> 00:38:18,720
pre-tuning and also uh uh

885
00:38:16,240 --> 00:38:21,040
semi-supervised finetuning. On top of

886
00:38:18,720 --> 00:38:23,599
that, we introduce this further

887
00:38:21,040 --> 00:38:26,640
self-improvement framework where we use

888
00:38:23,599 --> 00:38:29,280
the current model to get a response for

889
00:38:26,640 --> 00:38:31,119
um the same context. Let's see we get n

890
00:38:29,280 --> 00:38:34,000
of them. And I'm going to use the same

891
00:38:31,119 --> 00:38:37,359
model to do the quality prediction and

892
00:38:34,000 --> 00:38:39,440
use the highly like the highest rated

893
00:38:37,359 --> 00:38:42,000
response and the lowest rated response

894
00:38:39,440 --> 00:38:45,440
to form a preference pair and then do

895
00:38:42,000 --> 00:38:48,960
further alignment on top of them.

896
00:38:45,440 --> 00:38:51,760
Um so the question here is that can RMS

897
00:38:48,960 --> 00:38:54,480
really upscale novas counselors. This is

898
00:38:51,760 --> 00:38:57,280
one study we did where it's a randomized

899
00:38:54,480 --> 00:39:00,079
control trial with around 90 uh

900
00:38:57,280 --> 00:39:02,079
counselors 94 counselors. Uh we let them

901
00:39:00,079 --> 00:39:03,920
practice with different conditions and

902
00:39:02,079 --> 00:39:07,040
then quantify their performance in the

903
00:39:03,920 --> 00:39:09,119
end. What do we see is that people

904
00:39:07,040 --> 00:39:11,359
really like this kind of training

905
00:39:09,119 --> 00:39:13,839
systems. We feel the system is very

906
00:39:11,359 --> 00:39:15,839
authentic. AI patients presenting

907
00:39:13,839 --> 00:39:17,760
challenging and diverse practice.

908
00:39:15,839 --> 00:39:20,400
Counselors are more comfortable with AI

909
00:39:17,760 --> 00:39:23,760
mentor feedback and found AI mentor to

910
00:39:20,400 --> 00:39:25,839
be constructive and helpful. Um if you

911
00:39:23,760 --> 00:39:27,839
look at how people's behavior actually

912
00:39:25,839 --> 00:39:32,079
change. Uh there are actually

913
00:39:27,839 --> 00:39:34,000
significant um um job for empathy if

914
00:39:32,079 --> 00:39:36,720
people just practice alone without

915
00:39:34,000 --> 00:39:39,119
getting feedback. So practice themsel is

916
00:39:36,720 --> 00:39:42,320
not helpful. you need to couple it with

917
00:39:39,119 --> 00:39:44,480
feedback and also uh PRA practice alone

918
00:39:42,320 --> 00:39:46,240
helps reduce this kind of inappropriate

919
00:39:44,480 --> 00:39:48,960
suggestion

920
00:39:46,240 --> 00:39:51,839
as I mentioned we did this also with

921
00:39:48,960 --> 00:39:54,880
deployment in two uh courses at our

922
00:39:51,839 --> 00:39:57,280
medical school this is a SAD program and

923
00:39:54,880 --> 00:40:01,280
then um it's a class so we only have

924
00:39:57,280 --> 00:40:02,880
around like 25 students each time um

925
00:40:01,280 --> 00:40:05,599
this is actually really really hard to

926
00:40:02,880 --> 00:40:07,040
do um when I first work on this I

927
00:40:05,599 --> 00:40:09,200
thought like okay let's just run it and

928
00:40:07,040 --> 00:40:10,720
they will see results. But actually it's

929
00:40:09,200 --> 00:40:13,119
very expensive to run this kind of

930
00:40:10,720 --> 00:40:15,599
randomized control trials in classroom.

931
00:40:13,119 --> 00:40:18,079
So instead uh we use this thing called a

932
00:40:15,599 --> 00:40:19,760
macro randomized control trial. So this

933
00:40:18,079 --> 00:40:21,520
actually allow you to introduce

934
00:40:19,760 --> 00:40:24,160
different decision point in the

935
00:40:21,520 --> 00:40:26,720
deployment process. Basically help us

936
00:40:24,160 --> 00:40:29,280
increase the sample size. Just to give

937
00:40:26,720 --> 00:40:32,800
you a highlight of the results. Um

938
00:40:29,280 --> 00:40:35,520
through a 20 weeks deployment across two

939
00:40:32,800 --> 00:40:37,920
courses, we found that this kind of care

940
00:40:35,520 --> 00:40:39,680
system where we give people AI feedback

941
00:40:37,920 --> 00:40:42,880
allow them to do reflections and

942
00:40:39,680 --> 00:40:45,280
practice actually lead to most useful

943
00:40:42,880 --> 00:40:47,760
outcomes. This is quantified by their

944
00:40:45,280 --> 00:40:50,240
learning utility uh as rated by students

945
00:40:47,760 --> 00:40:52,000
themselves and also by their reflection

946
00:40:50,240 --> 00:40:54,880
engagement.

947
00:40:52,000 --> 00:40:57,839
to kind of reflect a little bit here. Um

948
00:40:54,880 --> 00:40:59,599
I think uh people ask me a lot since our

949
00:40:57,839 --> 00:41:01,680
research involve a lot of like human

950
00:40:59,599 --> 00:41:04,079
involvement. How could we deal with

951
00:41:01,680 --> 00:41:06,160
this? I think a code design earlier and

952
00:41:04,079 --> 00:41:08,880
often not only just for the final

953
00:41:06,160 --> 00:41:11,040
evaluation but even for the technical uh

954
00:41:08,880 --> 00:41:12,880
development and then we can think about

955
00:41:11,040 --> 00:41:15,839
how to transform feedback into

956
00:41:12,880 --> 00:41:18,079
actionable technical solutions. Um you

957
00:41:15,839 --> 00:41:19,599
can evaluate with real world deployment.

958
00:41:18,079 --> 00:41:22,480
this is going to be better than

959
00:41:19,599 --> 00:41:24,640
benchmarks and we found a quite a few

960
00:41:22,480 --> 00:41:26,880
gaps between what the benchmark tell us

961
00:41:24,640 --> 00:41:29,280
and then the real world deployment and

962
00:41:26,880 --> 00:41:32,000
then also to quantify the mechanism of

963
00:41:29,280 --> 00:41:34,000
change uh in skill acquisition compared

964
00:41:32,000 --> 00:41:37,119
to just throughout the system and see

965
00:41:34,000 --> 00:41:39,200
how it works um with time I'm going to

966
00:41:37,119 --> 00:41:41,920
move into the third part which I

967
00:41:39,200 --> 00:41:44,400
personally get very very excited and you

968
00:41:41,920 --> 00:41:46,480
are going to hear from my student who

969
00:41:44,400 --> 00:41:48,079
will tell you what this is about. Hi

970
00:41:46,480 --> 00:41:50,560
folks, I'm Omar. I'm an interesting

971
00:41:48,079 --> 00:41:54,079
student. So quick question. Don't you

972
00:41:50,560 --> 00:41:57,520
think it's a limit your interaction with

973
00:41:54,079 --> 00:41:59,760
lens around us asking for help? I mean,

974
00:41:57,520 --> 00:42:02,000
if these models had access to even a

975
00:41:59,760 --> 00:42:03,760
little bit more context, maybe even

976
00:42:02,000 --> 00:42:07,040
editing the board, the calendar events

977
00:42:03,760 --> 00:42:09,520
we just wrapped up or the slack model,

978
00:42:07,040 --> 00:42:11,680
our task would be a lot more obvious. I

979
00:42:09,520 --> 00:42:13,520
mean, even when we do front models with

980
00:42:11,680 --> 00:42:15,760
the past, we're always bound to leave

981
00:42:13,520 --> 00:42:17,599
things out. It's impossible to specify

982
00:42:15,760 --> 00:42:19,119
all of your context in a single X

983
00:42:17,599 --> 00:42:20,960
problem. So I've been thinking a lot

984
00:42:19,119 --> 00:42:22,880
about good models that are ground in

985
00:42:20,960 --> 00:42:25,440
your context and this partner tries to

986
00:42:22,880 --> 00:42:27,520
take a crack.

987
00:42:25,440 --> 00:42:29,280
>> Okay. So

988
00:42:27,520 --> 00:42:31,760
we again has been thinking about

989
00:42:29,280 --> 00:42:34,720
computing as a whole and we were reading

990
00:42:31,760 --> 00:42:36,240
history uh and uh we found that this is

991
00:42:34,720 --> 00:42:39,280
actually something where people had a

992
00:42:36,240 --> 00:42:41,520
visions back in old days um ranging from

993
00:42:39,280 --> 00:42:44,160
knowledge navigator to this kind of

994
00:42:41,520 --> 00:42:46,319
computer for the 21st century. I think

995
00:42:44,160 --> 00:42:49,920
uh we really want the AI systems or

996
00:42:46,319 --> 00:42:52,800
computing understanding us. Um and the

997
00:42:49,920 --> 00:42:54,960
reality is that they are not um LMS use

998
00:42:52,800 --> 00:42:57,440
chatting history to understand you. Uh

999
00:42:54,960 --> 00:43:00,079
TVs, Netflix use history to understand

1000
00:42:57,440 --> 00:43:03,040
you. Uh music use listening histories to

1001
00:43:00,079 --> 00:43:06,079
understand us. But what if we could have

1002
00:43:03,040 --> 00:43:08,800
a general user models that is anything

1003
00:43:06,079 --> 00:43:10,960
about us? So this is actually a system

1004
00:43:08,800 --> 00:43:13,359
we built called a gum and it's a system

1005
00:43:10,960 --> 00:43:15,440
that use multimodel models to build a

1006
00:43:13,359 --> 00:43:17,839
general understanding of our context

1007
00:43:15,440 --> 00:43:22,319
through observation. So what does that

1008
00:43:17,839 --> 00:43:24,720
mean? Um in a very quick highlight we do

1009
00:43:22,319 --> 00:43:28,319
screen recording. We record your

1010
00:43:24,720 --> 00:43:30,880
computer use screen 24 hours per day. We

1011
00:43:28,319 --> 00:43:34,319
do a screenshot extraction every 10

1012
00:43:30,880 --> 00:43:37,920
seconds and we use VLMs to reason about

1013
00:43:34,319 --> 00:43:40,640
you to form obs uh prepositions about

1014
00:43:37,920 --> 00:43:43,040
you. So on this screen you can see that

1015
00:43:40,640 --> 00:43:45,280
there was some kind of um observation

1016
00:43:43,040 --> 00:43:48,079
where they observe my student Omar uh

1017
00:43:45,280 --> 00:43:50,400
was doing qualification for his PhD and

1018
00:43:48,079 --> 00:43:52,400
then inferred that he's a student there

1019
00:43:50,400 --> 00:43:56,319
and then also inferred that he needs to

1020
00:43:52,400 --> 00:43:58,560
budget as a PhD student. um they saw the

1021
00:43:56,319 --> 00:44:01,040
email that uh Omar's going to a wedding

1022
00:43:58,560 --> 00:44:03,520
in Chicago and likely going to friend's

1023
00:44:01,040 --> 00:44:05,280
wedding in Chicago and then found a suit

1024
00:44:03,520 --> 00:44:07,440
rental in Chicago for your friend's

1025
00:44:05,280 --> 00:44:10,240
wedding that likely fits your budget as

1026
00:44:07,440 --> 00:44:13,359
a PhD student. So they can actually plan

1027
00:44:10,240 --> 00:44:15,839
a few steps ahead. So this is what we do

1028
00:44:13,359 --> 00:44:18,319
and then the entire process is actually

1029
00:44:15,839 --> 00:44:20,160
uh supported by open source LMS because

1030
00:44:18,319 --> 00:44:22,800
we care about privacy. I'll go back to

1031
00:44:20,160 --> 00:44:25,599
the privacy later privacy later. So we

1032
00:44:22,800 --> 00:44:28,160
observe your computer use. We propose

1033
00:44:25,599 --> 00:44:29,920
prepositions about you. We do retrieval

1034
00:44:28,160 --> 00:44:32,079
just to see whether we want to merge

1035
00:44:29,920 --> 00:44:34,000
some prepositions about you or make any

1036
00:44:32,079 --> 00:44:37,680
revisions about our wrong understanding

1037
00:44:34,000 --> 00:44:39,520
of you. And I'm not going to go into the

1038
00:44:37,680 --> 00:44:41,280
definition here, but a lot of them

1039
00:44:39,520 --> 00:44:43,839
actually goes back to what we were doing

1040
00:44:41,280 --> 00:44:46,400
from a IR perspective. How do we find

1041
00:44:43,839 --> 00:44:48,720
diverse and relevant propositions to do

1042
00:44:46,400 --> 00:44:51,359
the merge together?

1043
00:44:48,720 --> 00:44:54,640
This is insane. So we had a really hard

1044
00:44:51,359 --> 00:44:56,640
time of explaining to IRB of the risk of

1045
00:44:54,640 --> 00:44:58,640
this project and how to do it in a more

1046
00:44:56,640 --> 00:45:00,800
responsible way. So the initial

1047
00:44:58,640 --> 00:45:04,400
evaluation of gum is actually performed

1048
00:45:00,800 --> 00:45:06,319
on emails where we recruit 18 people to

1049
00:45:04,400 --> 00:45:08,160
just share their emails with us. Of

1050
00:45:06,319 --> 00:45:10,480
course this system is installed locally.

1051
00:45:08,160 --> 00:45:12,319
So we don't see any data and then we are

1052
00:45:10,480 --> 00:45:15,040
going to train this kind of gum you all

1053
00:45:12,319 --> 00:45:17,599
the emails that you have.

1054
00:45:15,040 --> 00:45:20,160
Um it's working pretty well. So are

1055
00:45:17,599 --> 00:45:22,319
those propositions accurate? We gave the

1056
00:45:20,160 --> 00:45:26,000
propositions we extract and then give it

1057
00:45:22,319 --> 00:45:28,240
to users and I think over 76 percentage

1058
00:45:26,000 --> 00:45:30,800
of the propositions we extract about

1059
00:45:28,240 --> 00:45:33,599
users are pretty accurate. Uh you can

1060
00:45:30,800 --> 00:45:36,240
also let them rank which propositions is

1061
00:45:33,599 --> 00:45:38,480
going to be more contextualized because

1062
00:45:36,240 --> 00:45:40,640
um Omar is a PhD student versus Omar is

1063
00:45:38,480 --> 00:45:42,960
a third year PhD student in the NLP

1064
00:45:40,640 --> 00:45:45,680
group at Stanford. Both are correct but

1065
00:45:42,960 --> 00:45:48,480
one is probably more contextualized. So

1066
00:45:45,680 --> 00:45:50,800
again our systems are pretty good and

1067
00:45:48,480 --> 00:45:53,440
the calibration wise is also uh very

1068
00:45:50,800 --> 00:45:56,319
reasonable. There are two categories of

1069
00:45:53,440 --> 00:45:57,839
propositions we found. One is very safe.

1070
00:45:56,319 --> 00:46:01,200
They will just give you propositions

1071
00:45:57,839 --> 00:46:03,680
like oh this person is a frequent flyer

1072
00:46:01,200 --> 00:46:06,960
um is interesting skiing or is annoyed

1073
00:46:03,680 --> 00:46:09,599
by notifications from ax.edu.

1074
00:46:06,960 --> 00:46:12,319
Uh there are al also other type of

1075
00:46:09,599 --> 00:46:14,800
propositions we get from users like uh

1076
00:46:12,319 --> 00:46:18,079
P13 may not have time to see and read

1077
00:46:14,800 --> 00:46:19,839
other emails. P7 values convenience is

1078
00:46:18,079 --> 00:46:22,000
waiting to pay for it. This is because

1079
00:46:19,839 --> 00:46:25,359
they observe this person keeps getting

1080
00:46:22,000 --> 00:46:26,720
take out from a local store. Uh P5 is

1081
00:46:25,359 --> 00:46:28,560
aware of the risk associated with

1082
00:46:26,720 --> 00:46:31,520
investing including the possible loss of

1083
00:46:28,560 --> 00:46:33,520
money. So this is what we have. So it's

1084
00:46:31,520 --> 00:46:35,200
a gum system that can observe your

1085
00:46:33,520 --> 00:46:37,760
laptops.

1086
00:46:35,200 --> 00:46:40,319
I think uh what we really want to do is

1087
00:46:37,760 --> 00:46:44,720
that now we have this kind of personal

1088
00:46:40,319 --> 00:46:47,280
model with gum. If we plug LMS plus gum,

1089
00:46:44,720 --> 00:46:49,359
we can actually really autocomp complete

1090
00:46:47,280 --> 00:46:52,240
everything. This is definitely too

1091
00:46:49,359 --> 00:46:56,079
ambitious but I think to some extent gum

1092
00:46:52,240 --> 00:46:58,480
can actually provide a um new proactive

1093
00:46:56,079 --> 00:47:01,599
and ambient interactions. Going back to

1094
00:46:58,480 --> 00:47:03,680
my uh interaction page. So we built a

1095
00:47:01,599 --> 00:47:06,880
system called a gumbo. not the gumbo max

1096
00:47:03,680 --> 00:47:09,599
but a gumball as a soup. Um so this is a

1097
00:47:06,880 --> 00:47:12,079
proactive assistant on top of gum where

1098
00:47:09,599 --> 00:47:14,240
we use gum to form a very strong user

1099
00:47:12,079 --> 00:47:16,720
modeling of the user and then we can

1100
00:47:14,240 --> 00:47:18,960
then prompt the model to generate uh

1101
00:47:16,720 --> 00:47:22,319
candidate task recommendations based on

1102
00:47:18,960 --> 00:47:24,240
the contacts. I'm going to skip the the

1103
00:47:22,319 --> 00:47:28,000
demo. Um you should definitely give it a

1104
00:47:24,240 --> 00:47:30,160
try. It's um hacking face. Um there are

1105
00:47:28,000 --> 00:47:32,079
also a lot of kind of theoretical

1106
00:47:30,160 --> 00:47:34,560
considerations you need to consider like

1107
00:47:32,079 --> 00:47:36,160
when to give people recommendations. Uh

1108
00:47:34,560 --> 00:47:39,359
again this goes back to the earlier

1109
00:47:36,160 --> 00:47:43,440
vision of Kipi. Um let's just jump to

1110
00:47:39,359 --> 00:47:46,640
the evaluation. Um so RB only allow us

1111
00:47:43,440 --> 00:47:48,480
to try this out with five users. Um you

1112
00:47:46,640 --> 00:47:50,800
probably haven't heard about this. I it

1113
00:47:48,480 --> 00:47:53,280
was my first atte

1114
00:47:50,800 --> 00:47:55,200
participants with five people. Uh so we

1115
00:47:53,280 --> 00:47:57,839
only have five data points. We let them

1116
00:47:55,200 --> 00:48:00,079
try on their laptop for five days. Uh

1117
00:47:57,839 --> 00:48:02,079
burning phase is one day and then we use

1118
00:48:00,079 --> 00:48:04,880
that one day to learn the user modeling

1119
00:48:02,079 --> 00:48:07,280
and then uh models uh open source models

1120
00:48:04,880 --> 00:48:10,000
served by us and then we see no data in

1121
00:48:07,280 --> 00:48:13,200
the process. Overall gum is pretty uh

1122
00:48:10,000 --> 00:48:15,760
accurate and also calibrated. Everyone

1123
00:48:13,200 --> 00:48:18,160
was very excited about gum gumball and

1124
00:48:15,760 --> 00:48:21,440
the 25% of generations were ranked as

1125
00:48:18,160 --> 00:48:23,040
excellent and 12 only 12%age were rated

1126
00:48:21,440 --> 00:48:25,839
as poor.

1127
00:48:23,040 --> 00:48:27,839
So again like when I look at this there

1128
00:48:25,839 --> 00:48:31,599
were a lot a lot of moments I was like

1129
00:48:27,839 --> 00:48:33,359
wow this is impressive. So um P1 is

1130
00:48:31,599 --> 00:48:35,839
struggling with fixing bugs in their

1131
00:48:33,359 --> 00:48:37,520
research project. P3 prioritize

1132
00:48:35,839 --> 00:48:39,520
communicating with their advisors over

1133
00:48:37,520 --> 00:48:42,079
other people. So you would just see a

1134
00:48:39,520 --> 00:48:47,200
lot a lot of this kind of sentiment that

1135
00:48:42,079 --> 00:48:49,920
you probably want to know. I mean yeah

1136
00:48:47,200 --> 00:48:52,640
so privacy is the elephant in the room.

1137
00:48:49,920 --> 00:48:55,119
Premacy is a big issue for this topic.

1138
00:48:52,640 --> 00:48:57,440
Uh this is a literally a sentence from

1139
00:48:55,119 --> 00:48:59,520
our participants. Uh if I didn't know

1140
00:48:57,440 --> 00:49:01,280
and trust you, I would never install

1141
00:48:59,520 --> 00:49:04,079
this thing. So this is actually what our

1142
00:49:01,280 --> 00:49:06,319
participants share with us. So to do

1143
00:49:04,079 --> 00:49:08,079
this, we actually uh leverage a lot of

1144
00:49:06,319 --> 00:49:11,680
the privacy theories especially

1145
00:49:08,079 --> 00:49:14,160
contextual integrity and then um build

1146
00:49:11,680 --> 00:49:15,760
them into our audit. So when we are

1147
00:49:14,160 --> 00:49:17,760
taking observations from people's

1148
00:49:15,760 --> 00:49:20,319
screen, we actually only take a

1149
00:49:17,760 --> 00:49:23,040
screenshot when it's not sensitive,

1150
00:49:20,319 --> 00:49:25,839
where it's not um listed on the blog

1151
00:49:23,040 --> 00:49:27,920
list by the user. Um and then we also do

1152
00:49:25,839 --> 00:49:30,240
contextualized assessment to see whether

1153
00:49:27,920 --> 00:49:32,880
this violates this user's privacy

1154
00:49:30,240 --> 00:49:34,480
privacy preference.

1155
00:49:32,880 --> 00:49:37,280
Um

1156
00:49:34,480 --> 00:49:38,960
one thing I want to share with you is it

1157
00:49:37,280 --> 00:49:40,720
was actually quite interesting. We had a

1158
00:49:38,960 --> 00:49:42,400
people in the beginning tell us they

1159
00:49:40,720 --> 00:49:44,960
were very worried about their privacy.

1160
00:49:42,400 --> 00:49:46,720
But then one day in it they were like

1161
00:49:44,960 --> 00:49:49,040
whatever I'm just going to use it

1162
00:49:46,720 --> 00:49:51,520
because it's so convenient and then it

1163
00:49:49,040 --> 00:49:53,599
seems to be this kind of privacy paradox

1164
00:49:51,520 --> 00:49:55,920
when there are certain utility people

1165
00:49:53,599 --> 00:49:57,680
give away their privacy. Uh so this is

1166
00:49:55,920 --> 00:50:00,800
just like what we observe from this

1167
00:49:57,680 --> 00:50:03,440
study. Uh my last video that I want to

1168
00:50:00,800 --> 00:50:06,880
show is still on this kind of generative

1169
00:50:03,440 --> 00:50:09,440
interaction and now we have gumbo that I

1170
00:50:06,880 --> 00:50:11,839
can help you generate response very

1171
00:50:09,440 --> 00:50:14,960
personalized to you. So what about the

1172
00:50:11,839 --> 00:50:17,839
tools or interface? Why every of us have

1173
00:50:14,960 --> 00:50:19,760
to use the same tool, same interface?

1174
00:50:17,839 --> 00:50:21,599
This is actually a recent work we did

1175
00:50:19,760 --> 00:50:25,079
called a generative interface for

1176
00:50:21,599 --> 00:50:25,079
language models.

1177
00:50:34,920 --> 00:50:38,030
[Music]

1178
00:50:40,380 --> 00:50:43,579
[Music]

1179
00:50:49,359 --> 00:50:52,359
Okay.

1180
00:51:01,599 --> 00:51:04,599
Are there

1181
00:51:07,440 --> 00:51:12,240
So what it does is uh instead of

1182
00:51:10,000 --> 00:51:14,240
interacting with text only now think

1183
00:51:12,240 --> 00:51:17,760
about every of your query can actually

1184
00:51:14,240 --> 00:51:20,720
be um rendered as a tool designed for

1185
00:51:17,760 --> 00:51:22,800
you. So if you want to learn what like

1186
00:51:20,720 --> 00:51:25,920
learn about the piano. So if you ask a

1187
00:51:22,800 --> 00:51:28,079
chat GBT it give you many many words

1188
00:51:25,920 --> 00:51:30,400
10,000 words. If you ask it with our

1189
00:51:28,079 --> 00:51:32,319
systems we gener we give you some brief

1190
00:51:30,400 --> 00:51:34,880
highlights. We give you a piano keyboard

1191
00:51:32,319 --> 00:51:37,680
that you can experience how to tap it.

1192
00:51:34,880 --> 00:51:39,839
If you're asking for plotting a figure,

1193
00:51:37,680 --> 00:51:41,920
instead of giving you a figure there, we

1194
00:51:39,839 --> 00:51:44,240
actually generate a interactive space

1195
00:51:41,920 --> 00:51:47,119
where you can actually make the bar

1196
00:51:44,240 --> 00:51:49,760
narrower, better bigger, make the front

1197
00:51:47,119 --> 00:51:52,480
uh sorry font size bigger or like even

1198
00:51:49,760 --> 00:51:55,440
change the color. So we are moving from

1199
00:51:52,480 --> 00:51:57,200
text as interaction default to a

1200
00:51:55,440 --> 00:51:59,760
interactive space where people can

1201
00:51:57,200 --> 00:52:01,599
actually do stuff. So this is actually

1202
00:51:59,760 --> 00:52:03,520
pushing our interaction to the next

1203
00:52:01,599 --> 00:52:05,760
level. We're also thinking about how we

1204
00:52:03,520 --> 00:52:08,559
could actually really generate the tools

1205
00:52:05,760 --> 00:52:10,559
just for you by combining um this kind

1206
00:52:08,559 --> 00:52:12,480
of human agent collaboration or even

1207
00:52:10,559 --> 00:52:14,720
thinking about training to really think

1208
00:52:12,480 --> 00:52:18,160
about how personalization could really

1209
00:52:14,720 --> 00:52:19,920
happen at a different granularity.

1210
00:52:18,160 --> 00:52:24,119
Um this is all what I want to share.

1211
00:52:19,920 --> 00:52:24,119
Thank you. Happy to take more questions.

