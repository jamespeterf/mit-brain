1
00:00:02,960 --> 00:00:07,919
Good evening everyone and welcome to the

2
00:00:05,279 --> 00:00:10,480
Nexus and the MIT libraries. Uh my name

3
00:00:07,919 --> 00:00:14,960
is Mark Sarco and I am a librarian here

4
00:00:10,480 --> 00:00:18,240
in Hayden and uh I will be introducing

5
00:00:14,960 --> 00:00:20,800
tonight's speaker. So

6
00:00:18,240 --> 00:00:24,720
Eric Robsky Huntley is a spatial data

7
00:00:20,800 --> 00:00:27,359
scientist, GIS analysist and geographer

8
00:00:24,720 --> 00:00:30,000
at MIT. Eric is a lecturer in urban

9
00:00:27,359 --> 00:00:32,320
science and planning in DUSP, director

10
00:00:30,000 --> 00:00:35,280
of the spatial analysis and action

11
00:00:32,320 --> 00:00:38,160
research group and lead systems engineer

12
00:00:35,280 --> 00:00:40,000
for the data and feminism lab. They are

13
00:00:38,160 --> 00:00:43,040
also founder and principal of

14
00:00:40,000 --> 00:00:46,079
okography's research and design, a

15
00:00:43,040 --> 00:00:48,640
missiondriven GIS mapping and spatial

16
00:00:46,079 --> 00:00:50,960
data science consultancy.

17
00:00:48,640 --> 00:00:53,920
Eric is interested in what they call

18
00:00:50,960 --> 00:00:56,000
quote mapping up which involves using

19
00:00:53,920 --> 00:00:58,079
data science and mapping methods to

20
00:00:56,000 --> 00:01:00,640
analyze dominant actors and

21
00:00:58,079 --> 00:01:03,039
institutions. Their current research

22
00:01:00,640 --> 00:01:06,000
focuses on identifying shared ownership

23
00:01:03,039 --> 00:01:08,479
of property owning LLC's in

24
00:01:06,000 --> 00:01:11,840
Massachusetts using spatial network

25
00:01:08,479 --> 00:01:13,680
analysis and building interactive tools

26
00:01:11,840 --> 00:01:15,760
and visualizations to make this

27
00:01:13,680 --> 00:01:18,400
information accessible to tenants,

28
00:01:15,760 --> 00:01:23,479
activists, and local governments. Please

29
00:01:18,400 --> 00:01:23,479
join me in welcome Eric Robsy Hudley.

30
00:01:28,960 --> 00:01:32,720
Uh, thanks for being here, y'all. Um,

31
00:01:31,200 --> 00:01:34,320
I'm glad some of you have moved up. I

32
00:01:32,720 --> 00:01:36,720
was about to do a bit where I refused to

33
00:01:34,320 --> 00:01:38,240
speak unless some of you did. Um, and

34
00:01:36,720 --> 00:01:40,000
you've made I can't do that bit anymore,

35
00:01:38,240 --> 00:01:42,720
so thanks. We'll just knock that off the

36
00:01:40,000 --> 00:01:47,200
list. Um, so this is this is sort of an

37
00:01:42,720 --> 00:01:49,119
interesting uh format. Um,

38
00:01:47,200 --> 00:01:50,479
uh, yeah. Yeah, sure. This is this is an

39
00:01:49,119 --> 00:01:54,320
interesting format in the sense that I

40
00:01:50,479 --> 00:01:57,680
am kind of introducing a film before you

41
00:01:54,320 --> 00:01:59,119
see it. Um meaning that I don't want to

42
00:01:57,680 --> 00:02:00,640
this isn't I'm not going to give you a

43
00:01:59,119 --> 00:02:03,119
critique of the film because I want you

44
00:02:00,640 --> 00:02:05,600
to form your own opinions. Um you don't

45
00:02:03,119 --> 00:02:07,119
know what you think of it yet. Um if I

46
00:02:05,600 --> 00:02:09,360
reference things that are actually in

47
00:02:07,119 --> 00:02:11,280
the film,

48
00:02:09,360 --> 00:02:13,680
uh you may or may not hold them in your

49
00:02:11,280 --> 00:02:15,599
head until they show up. Um, so I think

50
00:02:13,680 --> 00:02:19,840
what I'm going to try to do here is kind

51
00:02:15,599 --> 00:02:22,640
of uh in relatively abstract and open

52
00:02:19,840 --> 00:02:24,879
ways, try to give you things to key into

53
00:02:22,640 --> 00:02:27,280
as you watch the film that might make

54
00:02:24,879 --> 00:02:29,920
that might change the way you view it.

55
00:02:27,280 --> 00:02:32,160
Um because I think that we're looking at

56
00:02:29,920 --> 00:02:34,720
an artifact in such a rapidly changing

57
00:02:32,160 --> 00:02:36,720
landscape that much of this film, even

58
00:02:34,720 --> 00:02:38,720
though it it came out two years ago,

59
00:02:36,720 --> 00:02:42,400
it's only two years old. Um already

60
00:02:38,720 --> 00:02:43,920
reads as being somewhat out of date, um

61
00:02:42,400 --> 00:02:46,160
for reasons that are both probably going

62
00:02:43,920 --> 00:02:50,720
to be obvious to you and those that

63
00:02:46,160 --> 00:02:54,800
might not be quite so obvious. Um so the

64
00:02:50,720 --> 00:02:58,640
first thing that I would say is that um

65
00:02:54,800 --> 00:03:00,560
I sat down to watch it last night. Um I

66
00:02:58,640 --> 00:03:01,920
am last minute perpetually and I was

67
00:03:00,560 --> 00:03:05,280
sitting there with my partner and we

68
00:03:01,920 --> 00:03:08,000
were watching this film. Um and the

69
00:03:05,280 --> 00:03:10,800
thing that jumped out at me first um was

70
00:03:08,000 --> 00:03:13,200
that it was falling really heavily on a

71
00:03:10,800 --> 00:03:15,760
trope that many of us probably remember

72
00:03:13,200 --> 00:03:19,519
from the months after chat GPT first

73
00:03:15,760 --> 00:03:22,400
launched. Um, ChatGpt. Anyone any nerds

74
00:03:19,519 --> 00:03:24,480
in the room? Um, chat GPT

75
00:03:22,400 --> 00:03:26,879
beta the platform that the tool went

76
00:03:24,480 --> 00:03:28,560
live when approximately.

77
00:03:26,879 --> 00:03:30,000
Anyone?

78
00:03:28,560 --> 00:03:32,799
I'm so glad you don't actually know

79
00:03:30,000 --> 00:03:35,920
that. Thank you. Um, that's good. You

80
00:03:32,799 --> 00:03:38,159
pass. Um, it went so uh, chat GPT went

81
00:03:35,920 --> 00:03:40,480
public uh, no in the sense of the tool

82
00:03:38,159 --> 00:03:44,480
becoming available uh, November 30th,

83
00:03:40,480 --> 00:03:47,760
2022. Um, by January it had hit 100

84
00:03:44,480 --> 00:03:50,239
million users. Um, and so around between

85
00:03:47,760 --> 00:03:53,360
November and January was when we started

86
00:03:50,239 --> 00:03:55,519
to see this particular thing emerge. And

87
00:03:53,360 --> 00:03:57,920
the thing I'm referring to is this. Um,

88
00:03:55,519 --> 00:04:01,680
you begin reading an article. You sit

89
00:03:57,920 --> 00:04:04,000
down to watch a film. Uh, it unfolds. In

90
00:04:01,680 --> 00:04:06,560
a film, it might be a minute. In an

91
00:04:04,000 --> 00:04:08,319
article, it might be two paragraphs. And

92
00:04:06,560 --> 00:04:12,239
then at the end of that second

93
00:04:08,319 --> 00:04:15,439
paragraph, uh, record scratch.

94
00:04:12,239 --> 00:04:17,680
What if I told you this was actually AI,

95
00:04:15,439 --> 00:04:19,359
right? Um where the grand reveal was

96
00:04:17,680 --> 00:04:21,680
that you've been engaging with this tool

97
00:04:19,359 --> 00:04:25,520
this entire time whether you knew it or

98
00:04:21,680 --> 00:04:28,560
not. Um and throughout this film um you

99
00:04:25,520 --> 00:04:31,680
see fairly extensive use of at this

100
00:04:28,560 --> 00:04:33,280
point relatively uh again it shocks me

101
00:04:31,680 --> 00:04:36,160
that I have to say this but slightly out

102
00:04:33,280 --> 00:04:40,560
ofd um generative AI models producing

103
00:04:36,160 --> 00:04:45,600
imagery. Um so for example we see um we

104
00:04:40,560 --> 00:04:48,479
see uh what AI thinks that uh say forced

105
00:04:45,600 --> 00:04:50,240
weager laborers in western China might

106
00:04:48,479 --> 00:04:52,639
look like when they're returning from

107
00:04:50,240 --> 00:04:55,600
the lithium mines, right? Um, we're

108
00:04:52,639 --> 00:04:58,479
seeing uh what it might look like to be

109
00:04:55,600 --> 00:05:02,240
in a uh to might be in what what it

110
00:04:58,479 --> 00:05:06,720
might look like to be in a um uh a data

111
00:05:02,240 --> 00:05:08,560
labeling uh workplace uh where it sort

112
00:05:06,720 --> 00:05:10,960
of we're led to assume relatively

113
00:05:08,560 --> 00:05:13,759
vulnerable populations in say subsaharan

114
00:05:10,960 --> 00:05:15,680
Africa are labeling exploitative data

115
00:05:13,759 --> 00:05:18,560
exploitative visuals that are posted to

116
00:05:15,680 --> 00:05:19,919
the internet for meta whatever it is um

117
00:05:18,560 --> 00:05:22,080
one of the big tech companies. So, it

118
00:05:19,919 --> 00:05:23,919
falls back really hard on this this what

119
00:05:22,080 --> 00:05:26,800
I think of as kind of a dated gimmick

120
00:05:23,919 --> 00:05:29,120
where it shows you AI and you're left in

121
00:05:26,800 --> 00:05:31,520
a really mystified place where you don't

122
00:05:29,120 --> 00:05:34,479
know how exactly how to engage with it.

123
00:05:31,520 --> 00:05:37,840
Um, it's an image. It's speaking to you

124
00:05:34,479 --> 00:05:40,080
in a way that is recognizable as an

125
00:05:37,840 --> 00:05:41,360
image. Uh, you're engaging with it in a

126
00:05:40,080 --> 00:05:43,280
way that is close to how you might

127
00:05:41,360 --> 00:05:44,639
engage with the real thing, the actual

128
00:05:43,280 --> 00:05:46,960
artifact, the actual photograph,

129
00:05:44,639 --> 00:05:48,560
whatever it is, but you're at a distance

130
00:05:46,960 --> 00:05:49,919
from it. And I think I I have some

131
00:05:48,560 --> 00:05:52,320
questions actually about how the film

132
00:05:49,919 --> 00:05:54,400
uses sort of these these um AI generated

133
00:05:52,320 --> 00:05:57,520
images, but I actually want you to think

134
00:05:54,400 --> 00:05:59,360
about how you respond, right? What is

135
00:05:57,520 --> 00:06:01,680
your how do you understand your ethical

136
00:05:59,360 --> 00:06:04,160
obligation in a way that is affected by

137
00:06:01,680 --> 00:06:05,120
the use of AI to produce these images as

138
00:06:04,160 --> 00:06:07,600
opposed to what would be more

139
00:06:05,120 --> 00:06:09,840
traditional journalistic practice of you

140
00:06:07,600 --> 00:06:12,639
know embedding doing these kind of

141
00:06:09,840 --> 00:06:14,800
long-term trust building journalistic or

142
00:06:12,639 --> 00:06:17,120
or ethnographic approaches and and

143
00:06:14,800 --> 00:06:18,800
really kind of coming from a place of uh

144
00:06:17,120 --> 00:06:22,720
more grounded research in the course of

145
00:06:18,800 --> 00:06:25,360
of um film making or or writing. Um,

146
00:06:22,720 --> 00:06:29,120
another question that I have. Okay. So,

147
00:06:25,360 --> 00:06:31,360
um, I'm a I'm a geographer. Um, I am a

148
00:06:29,120 --> 00:06:34,479
uh, GI scientist, which is geographic

149
00:06:31,360 --> 00:06:36,479
information scientist. Um, and what that

150
00:06:34,479 --> 00:06:37,680
means is I'm in some ways a ctographer.

151
00:06:36,479 --> 00:06:41,680
Um, how many of you have met a

152
00:06:37,680 --> 00:06:46,720
ctographer before, right? Ah, yes, I

153
00:06:41,680 --> 00:06:49,199
know. Wonderful. Um, um, uh, okay. So in

154
00:06:46,720 --> 00:06:52,080
the film um much of your narration is

155
00:06:49,199 --> 00:06:53,919
going to be carried forward by not the

156
00:06:52,080 --> 00:06:56,000
narration much of the sort of um

157
00:06:53,919 --> 00:06:57,280
argumentative arc of the film is going

158
00:06:56,000 --> 00:07:00,880
to be carried forward by a

159
00:06:57,280 --> 00:07:04,000
self-described cgrapher. Um we meet uh

160
00:07:00,880 --> 00:07:06,400
this this uh guy named Vladen Jober

161
00:07:04,000 --> 00:07:10,000
Joler Joler I can't read my own writing.

162
00:07:06,400 --> 00:07:12,080
It could be one of the two. Um and um

163
00:07:10,000 --> 00:07:13,599
his his role in the film is to take us

164
00:07:12,080 --> 00:07:15,680
through the sort of you could say the

165
00:07:13,599 --> 00:07:18,560
back end in some ways. He's he's being

166
00:07:15,680 --> 00:07:20,800
given the the task of saying um this

167
00:07:18,560 --> 00:07:22,319
tool that you see this robot that's in

168
00:07:20,800 --> 00:07:25,759
front of you this interface that's

169
00:07:22,319 --> 00:07:29,199
presenting itself um it's actually when

170
00:07:25,759 --> 00:07:31,039
you run a query when you when you um uh

171
00:07:29,199 --> 00:07:32,800
when you give chat GPT a prompt right

172
00:07:31,039 --> 00:07:34,800
when you give DeepSeek a prompt how does

173
00:07:32,800 --> 00:07:37,039
that how does that relate to the entire

174
00:07:34,800 --> 00:07:40,240
universe of things that sit behind the

175
00:07:37,039 --> 00:07:44,560
technology right so how does it how does

176
00:07:40,240 --> 00:07:47,280
your prompt to chat GPT uh trace outward

177
00:07:44,560 --> 00:07:50,960
through your sort of standard uh HTTP

178
00:07:47,280 --> 00:07:52,720
requests um and actually bring forth

179
00:07:50,960 --> 00:07:55,919
sort of muster forth this whole world of

180
00:07:52,720 --> 00:07:57,440
resources of metals and labor and blood

181
00:07:55,919 --> 00:08:00,080
and all of these things that that go

182
00:07:57,440 --> 00:08:03,120
into however quietly um the the tech we

183
00:08:00,080 --> 00:08:05,039
use in our daily lives. Um and I'd

184
00:08:03,120 --> 00:08:07,840
actually I' I'd encourage you to think

185
00:08:05,039 --> 00:08:11,199
about um the fact that this is a

186
00:08:07,840 --> 00:08:15,440
self-described cgrapher. Um cgraphy is

187
00:08:11,199 --> 00:08:18,400
mapmaking, right? Uh ctography is right

188
00:08:15,440 --> 00:08:23,759
parttoraphy. So it's drawing the world,

189
00:08:18,400 --> 00:08:25,840
writing the world. Um and um the uh uh

190
00:08:23,759 --> 00:08:30,000
the uh actually geography is writing the

191
00:08:25,840 --> 00:08:31,599
world. Cgraphy would be writing the

192
00:08:30,000 --> 00:08:33,279
sheet actually. It's sort of sort of a

193
00:08:31,599 --> 00:08:36,479
strange word for what it is, but the the

194
00:08:33,279 --> 00:08:38,000
um but it's a mapmaker, right? Um, and

195
00:08:36,479 --> 00:08:39,919
in the sense that we're dealing with

196
00:08:38,000 --> 00:08:41,680
these maps that exceed sort of your

197
00:08:39,919 --> 00:08:43,839
traditional latitude, longitude pieces

198
00:08:41,680 --> 00:08:46,320
of of uh, you know, traditional

199
00:08:43,839 --> 00:08:50,240
mapmaking, um, I'd really kind of

200
00:08:46,320 --> 00:08:53,200
encourage you to think about how how you

201
00:08:50,240 --> 00:08:56,560
think about the uh, interaction between

202
00:08:53,200 --> 00:08:58,080
tooling and supply chains and

203
00:08:56,560 --> 00:08:59,920
technology. I think the thing that the

204
00:08:58,080 --> 00:09:05,200
film is fairly successful at is drawing

205
00:08:59,920 --> 00:09:08,399
out both how um drawing out really how

206
00:09:05,200 --> 00:09:10,640
these individual pieces of technology

207
00:09:08,399 --> 00:09:13,040
trace immediately outwards into these

208
00:09:10,640 --> 00:09:15,839
these enormous networks of of resource

209
00:09:13,040 --> 00:09:17,200
and and the like. Um, and then I guess

210
00:09:15,839 --> 00:09:18,640
the the last thing that I wanted to say

211
00:09:17,200 --> 00:09:22,640
that's and maybe this is where it starts

212
00:09:18,640 --> 00:09:25,040
to feel the most um uh dated to me is

213
00:09:22,640 --> 00:09:26,880
the the implications that it traces are

214
00:09:25,040 --> 00:09:30,399
to it feels a little constrained

215
00:09:26,880 --> 00:09:32,160
actually. Um it it deals with the supply

216
00:09:30,399 --> 00:09:34,480
chains, right? So where do the where do

217
00:09:32,160 --> 00:09:36,160
the um rare earth metals and lithium and

218
00:09:34,480 --> 00:09:37,920
these sorts of things that go into NVIDI

219
00:09:36,160 --> 00:09:39,839
NVIDIA chips and whatever it is, where

220
00:09:37,920 --> 00:09:42,640
do they come from? Who extracts the

221
00:09:39,839 --> 00:09:44,080
metal? Who refineses it? Um who labels

222
00:09:42,640 --> 00:09:45,760
the data sets? These sorts of questions.

223
00:09:44,080 --> 00:09:47,920
It doesn't really get, and I think this

224
00:09:45,760 --> 00:09:51,120
is the 2023 part, it doesn't get into

225
00:09:47,920 --> 00:09:53,600
the downstream effects on 2025 labor

226
00:09:51,120 --> 00:09:56,320
markets in developed and elite

227
00:09:53,600 --> 00:09:58,480
economies. Um, I don't know how many of

228
00:09:56,320 --> 00:10:01,760
you have been watching the job outlook

229
00:09:58,480 --> 00:10:04,959
for CS graduates recently.

230
00:10:01,760 --> 00:10:06,800
It's not it's not what it once was. Um,

231
00:10:04,959 --> 00:10:08,560
and for me, one of the fascinating

232
00:10:06,800 --> 00:10:09,920
things about AI in the past year in

233
00:10:08,560 --> 00:10:11,519
particular has been watching how it's

234
00:10:09,920 --> 00:10:14,079
reshaped a lot of these previously

235
00:10:11,519 --> 00:10:16,160
rarified labor markets. um to the point

236
00:10:14,079 --> 00:10:18,079
that actually we're starting to see some

237
00:10:16,160 --> 00:10:19,519
ripple effects where junior developers

238
00:10:18,079 --> 00:10:23,519
while they're not being entirely

239
00:10:19,519 --> 00:10:25,440
replaced um by uh by AIS by any stretch

240
00:10:23,519 --> 00:10:27,360
of the imagination

241
00:10:25,440 --> 00:10:30,240
sufficient quantities of that work are

242
00:10:27,360 --> 00:10:32,160
doable by AI that they are hiring fewer

243
00:10:30,240 --> 00:10:34,000
at slower rates and the responsibilities

244
00:10:32,160 --> 00:10:35,839
are shifting to editing existing code

245
00:10:34,000 --> 00:10:37,920
rather than writing novel code at the

246
00:10:35,839 --> 00:10:39,680
very sort of simple um when you're doing

247
00:10:37,920 --> 00:10:43,279
very simple things in the course of a um

248
00:10:39,680 --> 00:10:44,640
developing a codebase Um,

249
00:10:43,279 --> 00:10:46,720
and then I think frankly, I don't know,

250
00:10:44,640 --> 00:10:48,880
and this is maybe my own thing. Um, I

251
00:10:46,720 --> 00:10:51,120
was, um, I was watching this

252
00:10:48,880 --> 00:10:53,200
documentary, and this is maybe a very

253
00:10:51,120 --> 00:10:55,600
like public DMI place to say this, my

254
00:10:53,200 --> 00:10:57,680
partner's pregnant. Um, and I I was sort

255
00:10:55,600 --> 00:10:59,040
of like scanning outward, right? I' I've

256
00:10:57,680 --> 00:11:01,600
been thinking very few in a few very

257
00:10:59,040 --> 00:11:03,519
futurity saturated kind of way recently.

258
00:11:01,600 --> 00:11:05,760
And I've I've had a switch flip in the

259
00:11:03,519 --> 00:11:07,519
last six months on AI in general where

260
00:11:05,760 --> 00:11:09,200
I've gone from being a skeptic to being

261
00:11:07,519 --> 00:11:12,160
some actually, frankly, kind of a

262
00:11:09,200 --> 00:11:13,760
doomer. Um, I'm I'm becoming less and

263
00:11:12,160 --> 00:11:15,440
less open to the idea that there's much

264
00:11:13,760 --> 00:11:19,440
good that comes out of consumerf facing

265
00:11:15,440 --> 00:11:22,480
AI. Um, I'm really fairly skeptical of

266
00:11:19,440 --> 00:11:24,079
of a lot of this and I I don't um my my

267
00:11:22,480 --> 00:11:26,079
take on it has suddenly started to be

268
00:11:24,079 --> 00:11:27,440
much more like, well, if it doesn't say

269
00:11:26,079 --> 00:11:28,959
what it says it can do, we probably

270
00:11:27,440 --> 00:11:30,160
shouldn't invest in it. And if it does

271
00:11:28,959 --> 00:11:31,920
say what it says it can do, we

272
00:11:30,160 --> 00:11:33,519
definitely shouldn't invest in it. Um,

273
00:11:31,920 --> 00:11:36,959
and that's that's sort of become my

274
00:11:33,519 --> 00:11:41,680
position to some extent, frankly. Um and

275
00:11:36,959 --> 00:11:46,399
um I guess to to me um I've I've started

276
00:11:41,680 --> 00:11:49,279
to I've started to um this film like so

277
00:11:46,399 --> 00:11:50,640
many things in the in the ether puts me

278
00:11:49,279 --> 00:11:52,720
in mind of a class that I've always

279
00:11:50,640 --> 00:11:55,279
wanted to teach at MIT and I don't think

280
00:11:52,720 --> 00:11:57,040
it really belongs in my department. Um

281
00:11:55,279 --> 00:11:58,720
but if someone if someone is a

282
00:11:57,040 --> 00:12:00,160
department head and is interested in

283
00:11:58,720 --> 00:12:01,600
this, let me know. I can do it. I don't

284
00:12:00,160 --> 00:12:03,120
know. I'll do it for free. Um, I've

285
00:12:01,600 --> 00:12:05,040
wanted forever to teach a class at MIT

286
00:12:03,120 --> 00:12:08,480
that's just called what if we didn't.

287
00:12:05,040 --> 00:12:10,000
Um, alternative title, nah. Um, and the

288
00:12:08,480 --> 00:12:11,519
premise of the class would be taking

289
00:12:10,000 --> 00:12:13,040
very seriously that the best thing to do

290
00:12:11,519 --> 00:12:15,760
with a piece of technology would

291
00:12:13,040 --> 00:12:18,800
actually be to not do anything with it

292
00:12:15,760 --> 00:12:20,079
at all. Um, in STS, this is sometimes

293
00:12:18,800 --> 00:12:21,600
referred to as the inevitability

294
00:12:20,079 --> 00:12:24,000
hypothesis, right? That the minute

295
00:12:21,600 --> 00:12:25,839
something comes out of Pandora's box, it

296
00:12:24,000 --> 00:12:28,560
can't be put back in. There's nothing we

297
00:12:25,839 --> 00:12:30,079
can do to refuse it. Um, and I actually

298
00:12:28,560 --> 00:12:31,680
think this is a moment where we're at

299
00:12:30,079 --> 00:12:33,920
something like a really historical

300
00:12:31,680 --> 00:12:35,440
turning point and maybe the the best

301
00:12:33,920 --> 00:12:39,279
path forward is to find these little

302
00:12:35,440 --> 00:12:42,160
ways of just frankly refusing. Um, I'm

303
00:12:39,279 --> 00:12:43,760
um I uh you know and and and uh

304
00:12:42,160 --> 00:12:47,200
enrolling in my class called What If We

305
00:12:43,760 --> 00:12:49,360
Didn't? Um the uh thing I wanted to uh

306
00:12:47,200 --> 00:12:51,680
close with. Um I have a favorite uh I

307
00:12:49,360 --> 00:12:54,480
have a favorite ASAP rock song. He's a

308
00:12:51,680 --> 00:12:56,639
rapper. Um, and uh, this is from a this

309
00:12:54,480 --> 00:12:59,680
is from a um, it's from a song called

310
00:12:56,639 --> 00:13:02,320
uh, Mindful Solutionism. Um, and it goes

311
00:12:59,680 --> 00:13:04,399
like this. Uh, uh, you could get a robot

312
00:13:02,320 --> 00:13:06,240
limb for your blownoff limb. Later on,

313
00:13:04,399 --> 00:13:08,800
the same technology will automate your

314
00:13:06,240 --> 00:13:10,160
gig. As awesome as it is. Wait, it gets

315
00:13:08,800 --> 00:13:12,079
awful. You could split an atom

316
00:13:10,160 --> 00:13:13,760
willy-nilly. If it's energy that can be

317
00:13:12,079 --> 00:13:15,680
used for killing, then it will be.

318
00:13:13,760 --> 00:13:16,880
Landmines, agent orans, leaded gas,

319
00:13:15,680 --> 00:13:18,639
cigarettes, cameras in your favorite

320
00:13:16,880 --> 00:13:20,000
corners, plastic in the wilderness. We

321
00:13:18,639 --> 00:13:21,360
cannot be trusted with the stuff that we

322
00:13:20,000 --> 00:13:23,600
come up with. The machinery could eat

323
00:13:21,360 --> 00:13:26,079
us. We just really love our buttons. Um,

324
00:13:23,600 --> 00:13:28,000
technology, focus on the other 3D

325
00:13:26,079 --> 00:13:29,600
printed body parts, dehydrated onion

326
00:13:28,000 --> 00:13:33,200
dip. You can buy a jet ski from a cell

327
00:13:29,600 --> 00:13:33,440
phone on a jumbo jet. T E C H N O L O G

328
00:13:33,200 --> 00:13:35,519
Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y Y

329
00:13:33,440 --> 00:13:37,680
It's the ultimate. And he closes the

330
00:13:35,519 --> 00:13:38,959
song with a rousing chorus of if it's

331
00:13:37,680 --> 00:13:42,639
out of the bag, then it's out of the

332
00:13:38,959 --> 00:13:46,720
bag. Now that is a powerful cat. Um and

333
00:13:42,639 --> 00:13:51,399
uh this is a powerful cat. Um so thanks.

334
00:13:46,720 --> 00:13:51,399
I don't know. Are we ready? Let's do it.

