1
00:00:00,000 --> 00:00:05,180

2
00:00:05,180 --> 00:00:06,320
Hello, everybody.

3
00:00:06,320 --> 00:00:11,060
We're going to start the
health care session now.

4
00:00:11,060 --> 00:00:13,590
Thanks so much for being here.

5
00:00:13,590 --> 00:00:16,520
We have a really exciting
session for you today.

6
00:00:16,520 --> 00:00:18,150
My name is Jon Gruber.

7
00:00:18,150 --> 00:00:20,750
I'm the chairman of the
Economics Department

8
00:00:20,750 --> 00:00:22,050
here at MIT.

9
00:00:22,050 --> 00:00:24,920
And my work is in health care
economics and health care

10
00:00:24,920 --> 00:00:26,330
policy.

11
00:00:26,330 --> 00:00:29,990
We also have speaking
Kate Kellogg, who's

12
00:00:29,990 --> 00:00:32,450
the David J McGrath junior
professor of Management

13
00:00:32,450 --> 00:00:35,600
and Innovation and a professor
of Business Administration

14
00:00:35,600 --> 00:00:37,320
at the MIT Sloan School.

15
00:00:37,320 --> 00:00:41,690
Kate's work is helping knowledge
workers and organizations

16
00:00:41,690 --> 00:00:44,570
develop and implement
narrow AI and generative

17
00:00:44,570 --> 00:00:47,090
AI solutions, on-the-ground
and everyday work

18
00:00:47,090 --> 00:00:49,970
to improve decision-making,
collaboration, and learning.

19
00:00:49,970 --> 00:00:52,560
We have Joe Doyle,
who's the Erwin H Schell

20
00:00:52,560 --> 00:00:55,100
Professor of Management
and applied econometrics

21
00:00:55,100 --> 00:00:59,000
at MIT Sloan School as
well as the co-chair

22
00:00:59,000 --> 00:01:02,320
of the health sector
at the JPAL Action Lab

23
00:01:02,320 --> 00:01:05,890
and co-principal investigator
of the NBER Roybal

24
00:01:05,890 --> 00:01:07,820
Center for Behavioral
Change and Health.

25
00:01:07,820 --> 00:01:09,620
Through all these
many initiatives,

26
00:01:09,620 --> 00:01:12,910
Joe partners with large health
care providers and payers

27
00:01:12,910 --> 00:01:15,970
to conduct randomized controlled
trials aimed at improving

28
00:01:15,970 --> 00:01:17,500
health care delivery.

29
00:01:17,500 --> 00:01:19,790
And then finally, we
have Michael Yaffe,

30
00:01:19,790 --> 00:01:22,750
who's the director of the MIT
Center for Precision Cancer

31
00:01:22,750 --> 00:01:26,150
Medicine, the David H
Coke professor in science,

32
00:01:26,150 --> 00:01:28,870
professor of the MIT departments
of Biological Engineering

33
00:01:28,870 --> 00:01:32,240
and Biology, intramural
faculty at the Koch Institute,

34
00:01:32,240 --> 00:01:34,100
and also, a real doctor.

35
00:01:34,100 --> 00:01:36,050
So if any of you are
having pains or whatever,

36
00:01:36,050 --> 00:01:39,140
you can ask him
during the session.

37
00:01:39,140 --> 00:01:42,760
His laboratory studies how
signaling cancer cells focusing

38
00:01:42,760 --> 00:01:45,590
on cell stress, DNA
damage, inflammation

39
00:01:45,590 --> 00:01:49,660
control the response of tumors
to conventional and novel types

40
00:01:49,660 --> 00:01:50,750
of cancer treatment.

41
00:01:50,750 --> 00:01:52,780
And he's particularly
interested in building

42
00:01:52,780 --> 00:01:55,060
the bridge between
basic biological science

43
00:01:55,060 --> 00:01:57,440
and clinical treatments
for human disease.

44
00:01:57,440 --> 00:01:59,195
So great panel here today--

45
00:01:59,195 --> 00:02:01,070
we're just going to talk
for about 10 minutes

46
00:02:01,070 --> 00:02:03,960
and then hopefully have a
little time for Q&A at the end,

47
00:02:03,960 --> 00:02:06,030
depending on how much
we abuse our privileges.

48
00:02:06,030 --> 00:02:08,250
So I will go first.

49
00:02:08,250 --> 00:02:09,780
And I'm going to talk--

50
00:02:09,780 --> 00:02:11,840
if you can put my
slides up, I'm going

51
00:02:11,840 --> 00:02:16,290
to talk today about risk
sharing in the life sciences.

52
00:02:16,290 --> 00:02:18,335
So we're just waiting,
I think, for my slides.

53
00:02:18,335 --> 00:02:22,220

54
00:02:22,220 --> 00:02:25,910
Anyway, so there we go.

55
00:02:25,910 --> 00:02:28,500
Insurance, paying for health
insurance in the life sciences,

56
00:02:28,500 --> 00:02:29,000
great.

57
00:02:29,000 --> 00:02:29,790
So let me see.

58
00:02:29,790 --> 00:02:31,920
I think if I go like this,
it'll go to the-- no,

59
00:02:31,920 --> 00:02:33,000
that's the pointer.

60
00:02:33,000 --> 00:02:35,870
If I go like that--

61
00:02:35,870 --> 00:02:37,080
how do I get to the next one?

62
00:02:37,080 --> 00:02:37,800
How about that?

63
00:02:37,800 --> 00:02:38,610
There you go.

64
00:02:38,610 --> 00:02:40,980
So today, I'm going to
focus on risk sharing.

65
00:02:40,980 --> 00:02:43,050
This is a term
economists use glibly.

66
00:02:43,050 --> 00:02:45,180
But let me be clear
on what I mean,

67
00:02:45,180 --> 00:02:49,400
which is that medical risks
are the greatest risk facing

68
00:02:49,400 --> 00:02:50,900
most Americans.

69
00:02:50,900 --> 00:02:54,710
They're difficult to
prepare for in that they're

70
00:02:54,710 --> 00:02:56,910
large and unexpected.

71
00:02:56,910 --> 00:02:59,815
So what do we do as society,
as developed societies

72
00:02:59,815 --> 00:03:00,690
all around the world?

73
00:03:00,690 --> 00:03:03,360
We spread that risk
through insurance.

74
00:03:03,360 --> 00:03:04,890
How does insurance work?

75
00:03:04,890 --> 00:03:08,000
We prepay a fixed
amount in return

76
00:03:08,000 --> 00:03:11,480
for coverage of our medical
costs when we need care.

77
00:03:11,480 --> 00:03:15,453
OK, so that's how we share the
risk of getting hit by a car

78
00:03:15,453 --> 00:03:17,370
or having a heart attack
or developing cancer.

79
00:03:17,370 --> 00:03:18,650
We prepay now.

80
00:03:18,650 --> 00:03:22,160
And then later, those
costs are covered.

81
00:03:22,160 --> 00:03:26,060
Now, in that world,
we have to recognize

82
00:03:26,060 --> 00:03:29,130
that life science innovation
is incredibly expensive.

83
00:03:29,130 --> 00:03:32,850
New innovations in life sciences
are expensive, becoming more so.

84
00:03:32,850 --> 00:03:36,320
The estimates suggest that the
cost of developing a new drug

85
00:03:36,320 --> 00:03:38,360
is $2 billion.

86
00:03:38,360 --> 00:03:43,310
That results in high prices to
the end users of those drugs.

87
00:03:43,310 --> 00:03:46,940
Those high prices are what
we need insurance for.

88
00:03:46,940 --> 00:03:52,880
And we need it because we are
making miraculous steps in life

89
00:03:52,880 --> 00:03:55,908
sciences innovation every day.

90
00:03:55,908 --> 00:03:58,200
We can talk about a moderately
expensive miracle, which

91
00:03:58,200 --> 00:04:01,080
is GLP-1s, these
new drugs, which

92
00:04:01,080 --> 00:04:03,550
can be used to control weight
and help with diabetes,

93
00:04:03,550 --> 00:04:05,850
have been shown in large
trials to be massively

94
00:04:05,850 --> 00:04:10,200
successful at curbing weight
and increasingly shown impacts

95
00:04:10,200 --> 00:04:12,540
on curbing other behaviors.

96
00:04:12,540 --> 00:04:14,530
And that's moderately expensive.

97
00:04:14,530 --> 00:04:15,970
That's $15,000 a year.

98
00:04:15,970 --> 00:04:18,070
Then, we have incredibly
expensive miracles,

99
00:04:18,070 --> 00:04:20,339
which is cell and
gene therapies, which

100
00:04:20,339 --> 00:04:24,880
are coming online, which are
literally curing the incurable.

101
00:04:24,880 --> 00:04:29,040
These are diseases which through
loss of a genetic lottery

102
00:04:29,040 --> 00:04:30,190
was killing people.

103
00:04:30,190 --> 00:04:32,370
We can now literally
cure them and let

104
00:04:32,370 --> 00:04:35,290
people who would have died
at age two live to old age.

105
00:04:35,290 --> 00:04:36,370
They're miraculous.

106
00:04:36,370 --> 00:04:38,050
They're unbelievably expensive.

107
00:04:38,050 --> 00:04:41,020
A typical cell and gene therapy
costs millions of dollars.

108
00:04:41,020 --> 00:04:44,470
So you talk about something
you need insurance for.

109
00:04:44,470 --> 00:04:48,300
You need insurance against
losing a genetic lottery

110
00:04:48,300 --> 00:04:51,550
and having to face a
$2 million expense.

111
00:04:51,550 --> 00:04:55,600
But the US insurance
system has key holes.

112
00:04:55,600 --> 00:04:58,330
The first hole is
discrimination.

113
00:04:58,330 --> 00:05:02,990
So before the Affordable
Care Act was passed in 2010,

114
00:05:02,990 --> 00:05:06,130
it was completely
legal in the US

115
00:05:06,130 --> 00:05:09,820
to discriminate against those
who were sick in your insurance

116
00:05:09,820 --> 00:05:10,550
decisions.

117
00:05:10,550 --> 00:05:12,220
You could deny
insurance coverage

118
00:05:12,220 --> 00:05:13,400
to someone who was sick.

119
00:05:13,400 --> 00:05:15,647
You could charge a sick
person or a woman more

120
00:05:15,647 --> 00:05:17,230
for health insurance
than you'd charge

121
00:05:17,230 --> 00:05:18,710
a healthy person or a man.

122
00:05:18,710 --> 00:05:21,010
Or you could exclude those
with pre-existing conditions

123
00:05:21,010 --> 00:05:23,866
from coverage.

124
00:05:23,866 --> 00:05:25,820
That ended with the
Affordable Care Act.

125
00:05:25,820 --> 00:05:28,160
We banned discrimination
in insurance.

126
00:05:28,160 --> 00:05:30,370
Now, insurers have to
charge men and women

127
00:05:30,370 --> 00:05:34,060
and the sick and healthy the
same price for health insurance.

128
00:05:34,060 --> 00:05:37,960
This is critical in a
world where we increasingly

129
00:05:37,960 --> 00:05:38,882
predict risk.

130
00:05:38,882 --> 00:05:40,840
Increasingly, I can tell
who's going to be sick

131
00:05:40,840 --> 00:05:42,440
before they walk in my door.

132
00:05:42,440 --> 00:05:45,640
What that means is any
profit-maximizing insurer

133
00:05:45,640 --> 00:05:47,630
is going to want
to avoid the sick.

134
00:05:47,630 --> 00:05:50,360
So in a world where we know how
sick people are going to be,

135
00:05:50,360 --> 00:05:53,290
it's critical that we don't
allow insurers to discriminate.

136
00:05:53,290 --> 00:05:55,420
Now, the ACA solved
this problem.

137
00:05:55,420 --> 00:05:56,680
It seems safe for now.

138
00:05:56,680 --> 00:05:59,340
But there are still
opponents of this protection

139
00:05:59,340 --> 00:06:00,632
now in power in the government.

140
00:06:00,632 --> 00:06:02,298
And that's something
we need to monitor.

141
00:06:02,298 --> 00:06:03,640
I think the ACA is pretty safe.

142
00:06:03,640 --> 00:06:06,300
But this would be a critical
loss for risk protection

143
00:06:06,300 --> 00:06:07,770
if this went away.

144
00:06:07,770 --> 00:06:09,460
There's also the
uninsured, however.

145
00:06:09,460 --> 00:06:12,640
About half of 25 million
Americans lack insurance.

146
00:06:12,640 --> 00:06:14,830
About 25 million Americans,
sorry, lack insurance.

147
00:06:14,830 --> 00:06:17,140
This was cut in about
half by the ACA.

148
00:06:17,140 --> 00:06:18,700
But it could rise significantly.

149
00:06:18,700 --> 00:06:21,910
There are policy decisions
that matter here.

150
00:06:21,910 --> 00:06:23,700
The number of uninsured
covered by the ACA

151
00:06:23,700 --> 00:06:25,908
went down by about four
million under the first Trump

152
00:06:25,908 --> 00:06:28,200
administration, rose
by about 5 million

153
00:06:28,200 --> 00:06:30,090
under the Biden
administration, and is

154
00:06:30,090 --> 00:06:31,950
anticipated to fall again.

155
00:06:31,950 --> 00:06:33,582
Who are these remaining insured?

156
00:06:33,582 --> 00:06:35,790
Many of them are actually
eligible for free insurance

157
00:06:35,790 --> 00:06:38,050
today if we could just
automatically enroll them.

158
00:06:38,050 --> 00:06:40,560
Some are young
invincibles, people

159
00:06:40,560 --> 00:06:43,350
who feel that they could just--
they don't need insurance

160
00:06:43,350 --> 00:06:44,560
because they're healthy.

161
00:06:44,560 --> 00:06:46,390
And some are
undocumented immigrants,

162
00:06:46,390 --> 00:06:48,400
which is by itself a
political minefield.

163
00:06:48,400 --> 00:06:50,650
So it's going to be challenging
to get these remaining

164
00:06:50,650 --> 00:06:51,200
uninsured.

165
00:06:51,200 --> 00:06:53,658
But it's an important goal if
we want to once again protect

166
00:06:53,658 --> 00:06:55,270
against medical risk.

167
00:06:55,270 --> 00:06:58,400
There's also the fact that
we have a multi-payer system.

168
00:06:58,400 --> 00:07:00,770
Individuals move across
different payers.

169
00:07:00,770 --> 00:07:04,900
So if I'm an insurer, and I
have a decision about investing

170
00:07:04,900 --> 00:07:07,712
in preventive care for you,
but by the time you're sick,

171
00:07:07,712 --> 00:07:09,170
you'll be with a
different insurer,

172
00:07:09,170 --> 00:07:10,712
I don't want to make
that investment.

173
00:07:10,712 --> 00:07:13,072
Why should I pay that $2
million for a cell and gene

174
00:07:13,072 --> 00:07:14,530
therapy that's
mostly going to save

175
00:07:14,530 --> 00:07:17,230
money for some other
insurer down the line.

176
00:07:17,230 --> 00:07:21,760
So insurers don't want
to provide investments

177
00:07:21,760 --> 00:07:24,080
because people might leave
and go to other insurers.

178
00:07:24,080 --> 00:07:26,260
On top of that,
insurers are worried

179
00:07:26,260 --> 00:07:28,190
about these really
expensive risks,

180
00:07:28,190 --> 00:07:29,850
these things like
cell and gene therapy

181
00:07:29,850 --> 00:07:31,100
that are incredibly expensive.

182
00:07:31,100 --> 00:07:32,210
So what do they do?

183
00:07:32,210 --> 00:07:33,890
Insurers get insured.

184
00:07:33,890 --> 00:07:35,360
They buy reinsurance.

185
00:07:35,360 --> 00:07:37,460
There's a contract
that says, look,

186
00:07:37,460 --> 00:07:40,480
you're insuring everyone at MIT,
but if someone has costs above

187
00:07:40,480 --> 00:07:42,680
$50,000, we'll pick
up the difference.

188
00:07:42,680 --> 00:07:45,320
They buy it from big companies
that offer that reinsurance.

189
00:07:45,320 --> 00:07:47,520
The problem I
realized recently is

190
00:07:47,520 --> 00:07:49,990
the ACA did not fix
the reinsurance market.

191
00:07:49,990 --> 00:07:52,330
It's completely legal
to discriminate.

192
00:07:52,330 --> 00:07:54,310
So MIT could go to
a reinsurer and say,

193
00:07:54,310 --> 00:07:56,080
look, we're going to cover
the cell and gene therapy.

194
00:07:56,080 --> 00:07:58,747
And the reinsurer can say, nope,
I'm not going to reinsure that.

195
00:07:58,747 --> 00:08:00,010
You're on your own for that.

196
00:08:00,010 --> 00:08:02,230
So there's still
holes in the system

197
00:08:02,230 --> 00:08:05,610
even for those insured
we need to address.

198
00:08:05,610 --> 00:08:08,198
So let's do an
important case study.

199
00:08:08,198 --> 00:08:10,240
We're going to talk about
innovation a lot today.

200
00:08:10,240 --> 00:08:12,282
Let's talk about one of
the most innovative areas

201
00:08:12,282 --> 00:08:15,870
in medical care, which is
cell and gene therapies.

202
00:08:15,870 --> 00:08:17,520
The adoption has
actually been slow.

203
00:08:17,520 --> 00:08:20,020
There are some new cell and
gene therapies being introduced.

204
00:08:20,020 --> 00:08:21,978
And the adoption has been
slow despite the fact

205
00:08:21,978 --> 00:08:24,240
that they're literally
miraculous and life-saving.

206
00:08:24,240 --> 00:08:25,660
Why is that?

207
00:08:25,660 --> 00:08:28,763
Well, insurers are very
wary of paying for them.

208
00:08:28,763 --> 00:08:30,180
First of all, as
I said, they have

209
00:08:30,180 --> 00:08:32,216
to make a major investment
in paying for them,

210
00:08:32,216 --> 00:08:34,049
but they might not get
the lifetime benefits

211
00:08:34,049 --> 00:08:35,970
of people staying on the plan.

212
00:08:35,970 --> 00:08:39,010
Second of all, these
drugs are miraculous,

213
00:08:39,010 --> 00:08:41,158
but the trials are so
far very short-term.

214
00:08:41,158 --> 00:08:42,700
We don't know if
five years from now,

215
00:08:42,700 --> 00:08:43,929
they're going to stop working.

216
00:08:43,929 --> 00:08:46,012
And insurers are like, I
could pay all this money,

217
00:08:46,012 --> 00:08:49,530
and five years from now, it
could wear out and stop working.

218
00:08:49,530 --> 00:08:52,700
And so as a result, I'm wary
of making that investment.

219
00:08:52,700 --> 00:08:56,970
And finally, reinsurers won't
cover these drugs in many cases.

220
00:08:56,970 --> 00:08:58,705
So insurers have
been slow to adopt.

221
00:08:58,705 --> 00:09:00,080
When they do adopt,
they try very

222
00:09:00,080 --> 00:09:02,280
hard to limit who can get
these new innovations.

223
00:09:02,280 --> 00:09:04,800
The problem is people can't
afford them without insurance,

224
00:09:04,800 --> 00:09:07,950
and life-saving opportunities
are being missed.

225
00:09:07,950 --> 00:09:11,498
And this can in turn feed back
to investment in these new areas

226
00:09:11,498 --> 00:09:13,790
because people are reluctant
to invest in cell and gene

227
00:09:13,790 --> 00:09:16,280
therapies because they're
afraid they won't be covered.

228
00:09:16,280 --> 00:09:18,080
So what do we do?

229
00:09:18,080 --> 00:09:20,292
Well, single-payer
health care is not

230
00:09:20,292 --> 00:09:21,750
walking through
the door in the US.

231
00:09:21,750 --> 00:09:25,243
It's too politically
difficult. But we do actually

232
00:09:25,243 --> 00:09:26,910
have single-payer
health care in the US.

233
00:09:26,910 --> 00:09:27,720
People don't realize it.

234
00:09:27,720 --> 00:09:29,428
We actually have
single-payer health care

235
00:09:29,428 --> 00:09:32,640
in the US for a particular
disease, renal failure.

236
00:09:32,640 --> 00:09:34,350
For those with
renal failure, they

237
00:09:34,350 --> 00:09:36,600
are almost called the end-stage
renal disease program.

238
00:09:36,600 --> 00:09:38,808
And it's a universal
government-provided single-payer

239
00:09:38,808 --> 00:09:39,600
insurance.

240
00:09:39,600 --> 00:09:41,700
Why couldn't we do this
for other diseases,

241
00:09:41,700 --> 00:09:43,620
say genetic abnormalities?

242
00:09:43,620 --> 00:09:46,660
Why couldn't we say for those
born with genetic abnormalities,

243
00:09:46,660 --> 00:09:49,410
they will be on a
government-funded single-payer

244
00:09:49,410 --> 00:09:50,650
insurance system?

245
00:09:50,650 --> 00:09:52,750
That would solve
all these problems.

246
00:09:52,750 --> 00:09:54,463
And basically, as
a society, we'd

247
00:09:54,463 --> 00:09:56,130
all pay a little
higher taxes because we

248
00:09:56,130 --> 00:09:58,380
won the genetic lottery
to support those

249
00:09:58,380 --> 00:09:59,890
who lost the genetic lottery.

250
00:09:59,890 --> 00:10:02,790
It's a perfect case for
government intervention

251
00:10:02,790 --> 00:10:04,740
where we as a society
share the risk.

252
00:10:04,740 --> 00:10:08,080
Alternatively, if
that's not feasible,

253
00:10:08,080 --> 00:10:10,410
we can think about developing
private sector models

254
00:10:10,410 --> 00:10:13,800
to spread the cost with things
like value-based reimbursement

255
00:10:13,800 --> 00:10:17,730
that allow us to have
the manufacturers share

256
00:10:17,730 --> 00:10:21,540
the risk of drugs not working
and subscription models that

257
00:10:21,540 --> 00:10:23,320
put the risk on manufacturers.

258
00:10:23,320 --> 00:10:26,040
These are things that
I'm working on, trying

259
00:10:26,040 --> 00:10:28,740
to see if we can push
these in the private sector

260
00:10:28,740 --> 00:10:32,340
in the absence of a government
solution to this problem.

261
00:10:32,340 --> 00:10:34,260
Now, one of the last
thing I'll talk about

262
00:10:34,260 --> 00:10:35,230
is what about pricing.

263
00:10:35,230 --> 00:10:36,280
These treatments
are very expensive.

264
00:10:36,280 --> 00:10:37,750
You might say, well,
why do they have to be?

265
00:10:37,750 --> 00:10:39,420
Why don't we just tell
these manufacturers

266
00:10:39,420 --> 00:10:40,378
they can't charge much?

267
00:10:40,378 --> 00:10:42,840
And the answer is because
innovation won't happen.

268
00:10:42,840 --> 00:10:45,860
Innovation does respond
to financial incentives.

269
00:10:45,860 --> 00:10:48,045
So the traditional
approach to dealing

270
00:10:48,045 --> 00:10:50,420
with this around the world is
we measure something called

271
00:10:50,420 --> 00:10:51,800
comparative effectiveness.

272
00:10:51,800 --> 00:10:54,630
All around the world they have
organizations which say, look,

273
00:10:54,630 --> 00:10:57,360
if a drug only delivers
this much of health value,

274
00:10:57,360 --> 00:10:59,690
we'll only pay this much for it.

275
00:10:59,690 --> 00:11:02,070
The US is the only country
that doesn't do that.

276
00:11:02,070 --> 00:11:05,000
As a result, we pay dramatically
more for our medical treatments

277
00:11:05,000 --> 00:11:06,552
than anywhere else in the world.

278
00:11:06,552 --> 00:11:08,510
The typical medical
treatment costs about twice

279
00:11:08,510 --> 00:11:12,440
in the US for innovative
treatments as it does in Europe.

280
00:11:12,440 --> 00:11:14,900
So that's something we
could do in a step that

281
00:11:14,900 --> 00:11:17,610
was started with the
Inflation Reduction Act.

282
00:11:17,610 --> 00:11:21,197
The problem is for these
incredibly expensive new drugs,

283
00:11:21,197 --> 00:11:23,280
they're actually worth the
price they're charging.

284
00:11:23,280 --> 00:11:24,778
So they're charging $2 million.

285
00:11:24,778 --> 00:11:25,320
And you know?

286
00:11:25,320 --> 00:11:27,180
They're delivering $2
million of benefit.

287
00:11:27,180 --> 00:11:29,930
So dealing with pricing
in this new sector

288
00:11:29,930 --> 00:11:31,500
is going to be much
more challenging.

289
00:11:31,500 --> 00:11:33,792
And we're going to need to
think about other mechanisms

290
00:11:33,792 --> 00:11:34,580
to deal with that.

291
00:11:34,580 --> 00:11:36,580
So finally, I guess the
last thing I want to say

292
00:11:36,580 --> 00:11:38,163
is, how do we finance
this innovation?

293
00:11:38,163 --> 00:11:40,290
This is another area
economists are thinking about,

294
00:11:40,290 --> 00:11:43,960
which is basically we
need to not only cover

295
00:11:43,960 --> 00:11:45,970
these things for insurance
but actually promote

296
00:11:45,970 --> 00:11:48,950
more R&D funding of
these valuable things.

297
00:11:48,950 --> 00:11:51,530
This is discussed in my
book, Jump-Starting America,

298
00:11:51,530 --> 00:11:54,340
where we talk about the role
of public funding of research

299
00:11:54,340 --> 00:11:56,200
and development, which
actually at its peak

300
00:11:56,200 --> 00:12:01,640
amounted to 2% of US GDP
and is now only 0.5% today.

301
00:12:01,640 --> 00:12:04,900
This funding complements private
R&D. It promotes private R&D

302
00:12:04,900 --> 00:12:06,830
and has huge economic returns.

303
00:12:06,830 --> 00:12:09,170
Yet, as I said, investments
have been falling.

304
00:12:09,170 --> 00:12:12,010
So the bottom line is we need
to think about creative solution

305
00:12:12,010 --> 00:12:14,080
to get more innovative
funding in this area

306
00:12:14,080 --> 00:12:17,110
and to help bear or share
the risks that people face

307
00:12:17,110 --> 00:12:18,650
with expensive new treatments.

308
00:12:18,650 --> 00:12:19,400
I'll stop there.

309
00:12:19,400 --> 00:12:20,450
Thank you very much.

310
00:12:20,450 --> 00:12:23,943
[APPLAUSE]

311
00:12:23,943 --> 00:12:36,430

312
00:12:36,430 --> 00:12:39,770
I am delighted to be here today
to talk to you about my research

313
00:12:39,770 --> 00:12:41,850
on narrow and generative AI.

314
00:12:41,850 --> 00:12:44,630
And both of these forms
of AI have great potential

315
00:12:44,630 --> 00:12:47,060
to transform health care.

316
00:12:47,060 --> 00:12:49,020
Here's an example of narrow AI.

317
00:12:49,020 --> 00:12:52,910
Some of my coauthors at the Duke
Institute for Health Innovation

318
00:12:52,910 --> 00:12:57,170
worked with hospital doctors to
develop a highly accurate tool

319
00:12:57,170 --> 00:12:59,780
using machine
learning which detects

320
00:12:59,780 --> 00:13:04,530
unexpected patterns in patients
in the ER to predict sepsis.

321
00:13:04,530 --> 00:13:08,390
And sepsis is a life-threatening
illness infection

322
00:13:08,390 --> 00:13:10,400
where you need to
predict it quickly

323
00:13:10,400 --> 00:13:12,800
in order to treat it quickly.

324
00:13:12,800 --> 00:13:16,400
Here's a generative
AI example developed

325
00:13:16,400 --> 00:13:20,870
by some of my coauthors at
NYU Langone Health where

326
00:13:20,870 --> 00:13:25,790
they use GPT-4 to read
clinical notes of patients who

327
00:13:25,790 --> 00:13:30,080
are inpatients and create
patient-friendly discharge

328
00:13:30,080 --> 00:13:31,500
summaries for patients.

329
00:13:31,500 --> 00:13:33,410
And discharge
summaries have been

330
00:13:33,410 --> 00:13:36,020
shown to be really important
to transitions of care

331
00:13:36,020 --> 00:13:37,970
from inpatient to outpatient.

332
00:13:37,970 --> 00:13:40,900
And yet, 88% of current
discharge summaries

333
00:13:40,900 --> 00:13:43,180
are unreadable to patients.

334
00:13:43,180 --> 00:13:47,260
So clearly, these
AI solutions can

335
00:13:47,260 --> 00:13:50,500
result in improved quality,
reduced costs, and increased

336
00:13:50,500 --> 00:13:51,260
revenues.

337
00:13:51,260 --> 00:13:54,970
And that's often what
developers are focused on.

338
00:13:54,970 --> 00:13:58,000
However, what they
often do not focus on

339
00:13:58,000 --> 00:14:01,490
is the workers who need to
implement the solutions.

340
00:14:01,490 --> 00:14:04,580
And based on my work
with coauthors at Duke,

341
00:14:04,580 --> 00:14:07,690
we found that narrow
AI tools often

342
00:14:07,690 --> 00:14:11,900
offer few benefits to workers,
require laborious development,

343
00:14:11,900 --> 00:14:14,660
and also, are a threat
to user autonomy.

344
00:14:14,660 --> 00:14:16,750
And with colleagues
at Langone, we

345
00:14:16,750 --> 00:14:19,300
found that generative
AI solutions often

346
00:14:19,300 --> 00:14:23,630
have unclear applications,
require laborious iteration,

347
00:14:23,630 --> 00:14:26,710
and result in harmful outputs.

348
00:14:26,710 --> 00:14:29,380
And workers don't
like these solutions

349
00:14:29,380 --> 00:14:31,660
that make their lives worse.

350
00:14:31,660 --> 00:14:34,450
So how can developers
design AI solutions

351
00:14:34,450 --> 00:14:36,550
for successful implementation?

352
00:14:36,550 --> 00:14:39,510
I've done research at Duke
looking at their narrow AI

353
00:14:39,510 --> 00:14:43,440
solutions and NYU Langone
looking at their generative AI

354
00:14:43,440 --> 00:14:44,650
solutions.

355
00:14:44,650 --> 00:14:47,550
And what we found is that
each of these solutions

356
00:14:47,550 --> 00:14:49,740
have particular
characteristics that

357
00:14:49,740 --> 00:14:51,850
raise challenges for workers.

358
00:14:51,850 --> 00:14:54,240
So the challenges
with narrow AI are

359
00:14:54,240 --> 00:14:57,460
that it's predictive,
laborious and prescriptive,

360
00:14:57,460 --> 00:15:01,510
and with generative AI are that
it has unclear applications,

361
00:15:01,510 --> 00:15:05,520
is laborious, and
results in new risks.

362
00:15:05,520 --> 00:15:09,780
So developers need to design AI
tools with worker implementation

363
00:15:09,780 --> 00:15:11,790
in mind.

364
00:15:11,790 --> 00:15:15,330
So with narrow AI, let's look at
this predictive characteristic

365
00:15:15,330 --> 00:15:16,270
first.

366
00:15:16,270 --> 00:15:18,930
At Duke, what they found
is that specialists

367
00:15:18,930 --> 00:15:21,420
who treat patients for
particular diseases

368
00:15:21,420 --> 00:15:25,410
often asked the developers to
develop tools to flag patients

369
00:15:25,410 --> 00:15:27,640
at risk of developing
these diseases.

370
00:15:27,640 --> 00:15:32,400
So, as Jonathan just
told you, AI tools are--

371
00:15:32,400 --> 00:15:35,300
in medicine, we're going to be
able to predict a lot of things.

372
00:15:35,300 --> 00:15:36,560
And so this is great.

373
00:15:36,560 --> 00:15:38,950
We can now intervene
earlier with patients

374
00:15:38,950 --> 00:15:42,580
with all of these disease states
in order to have higher impact.

375
00:15:42,580 --> 00:15:45,820
The problem is it means
that upstream clinicians are

376
00:15:45,820 --> 00:15:47,780
expected to use these tools.

377
00:15:47,780 --> 00:15:51,340
And now, they're bombarded
with a host of narrow AI tools

378
00:15:51,340 --> 00:15:54,730
that they didn't ask
for in the first place.

379
00:15:54,730 --> 00:15:59,060
So what they do at Duke is they
identify the true end users,

380
00:15:59,060 --> 00:16:01,340
in this case, the
primary care physicians.

381
00:16:01,340 --> 00:16:04,910
And they develop solutions
to address their pain points.

382
00:16:04,910 --> 00:16:08,020
So in the case of a
narrow AI tool which

383
00:16:08,020 --> 00:16:12,040
can detect chronic kidney
disease early, what they found

384
00:16:12,040 --> 00:16:14,350
is that the
challenge for PCPs is

385
00:16:14,350 --> 00:16:17,450
that they already have limited
time to prepare for patients.

386
00:16:17,450 --> 00:16:20,360
So they don't have time
to use another tool,

387
00:16:20,360 --> 00:16:24,100
and they also don't have time
to ensure appropriate follow-up

388
00:16:24,100 --> 00:16:26,420
for patients who get
flagged by the tool.

389
00:16:26,420 --> 00:16:29,780
So the Duke development
team worked for solutions

390
00:16:29,780 --> 00:16:31,300
where it was the
nephrologist who

391
00:16:31,300 --> 00:16:33,840
asked for the tool who
actually are the ones who

392
00:16:33,840 --> 00:16:36,280
use it to detect this problem.

393
00:16:36,280 --> 00:16:41,220
They send their recommendations
to an EHR system,

394
00:16:41,220 --> 00:16:43,740
which gives doctors the
recommendations directly

395
00:16:43,740 --> 00:16:45,280
before the patient visit.

396
00:16:45,280 --> 00:16:47,910
And they provide
dedicated care managers

397
00:16:47,910 --> 00:16:50,250
so that any patients who
are flagged to be at risk

398
00:16:50,250 --> 00:16:52,470
get appropriate follow-up.

399
00:16:52,470 --> 00:16:57,370
Another issue with narrow AI is
that it's laborious to develop.

400
00:16:57,370 --> 00:16:59,460
It requires a lot
of back and forth

401
00:16:59,460 --> 00:17:02,040
between clinicians
and developers

402
00:17:02,040 --> 00:17:04,060
before you can make
these tools work.

403
00:17:04,060 --> 00:17:07,119
And so specialists came to
Duke developers and said,

404
00:17:07,119 --> 00:17:10,770
help us detect low-risk
pulmonary embolism in the ER

405
00:17:10,770 --> 00:17:13,170
because a lot of patients
with pulmonary embolism

406
00:17:13,170 --> 00:17:15,089
are getting admitted
to the hospital who

407
00:17:15,089 --> 00:17:17,190
don't need to be admitted.

408
00:17:17,190 --> 00:17:19,260
And it's the emergency
room physicians

409
00:17:19,260 --> 00:17:22,050
who need to use this
tool who were initially

410
00:17:22,050 --> 00:17:23,740
asked to help develop it.

411
00:17:23,740 --> 00:17:25,950
And they said, we've got
a lot of other things

412
00:17:25,950 --> 00:17:27,490
we're already working on.

413
00:17:27,490 --> 00:17:30,650
So Duke then went to
the downstream vascular

414
00:17:30,650 --> 00:17:32,660
and cardiology
specialists and got

415
00:17:32,660 --> 00:17:35,120
them involved in this
laborious development

416
00:17:35,120 --> 00:17:38,810
to do things like validate
outcome definition,

417
00:17:38,810 --> 00:17:41,790
make sure that model
inputs were fit for use,

418
00:17:41,790 --> 00:17:46,550
and do a silent trial to test
the performance of the tool.

419
00:17:46,550 --> 00:17:49,350
Finally, narrow AI
is prescriptive.

420
00:17:49,350 --> 00:17:53,190
It tells doctors what to do,
and that can threaten autonomy.

421
00:17:53,190 --> 00:17:55,950
In the case of the sepsis
tool I told you about,

422
00:17:55,950 --> 00:17:57,980
it was rapid
response team nurses

423
00:17:57,980 --> 00:18:02,250
who wanted ER doctors to use
this tool to detect sepsis.

424
00:18:02,250 --> 00:18:05,180
And initially, they were
the ones using the tool.

425
00:18:05,180 --> 00:18:07,220
And ER doctors
didn't like having

426
00:18:07,220 --> 00:18:09,770
these people far away from
the ER all of a sudden

427
00:18:09,770 --> 00:18:13,830
being the ones to detect sepsis
on the ER doctor's patients.

428
00:18:13,830 --> 00:18:16,010
So the rapid
response team nurses

429
00:18:16,010 --> 00:18:19,890
started, whenever an AI tool
flagged someone with sepsis,

430
00:18:19,890 --> 00:18:21,300
they would do a chart review.

431
00:18:21,300 --> 00:18:23,840
But they would call
the doctors in the ER

432
00:18:23,840 --> 00:18:26,960
to make sure the doctors were
the ones who did the diagnosis

433
00:18:26,960 --> 00:18:30,360
and placed all the orders
for patients with sepsis.

434
00:18:30,360 --> 00:18:33,240
So Duke developers have
learned to increase benefits

435
00:18:33,240 --> 00:18:36,790
for true end users, reduce
labor for end users,

436
00:18:36,790 --> 00:18:39,990
and protect the autonomy
of true end users.

437
00:18:39,990 --> 00:18:44,040
Generative AI similarly
raises particular challenges

438
00:18:44,040 --> 00:18:46,230
for workers.

439
00:18:46,230 --> 00:18:49,740
At NYU Langone, one challenge
is that generative AI

440
00:18:49,740 --> 00:18:51,750
is general purpose,
so it has a million

441
00:18:51,750 --> 00:18:53,890
different potential
applications.

442
00:18:53,890 --> 00:18:57,720
And what they do is they
involve the workers themselves

443
00:18:57,720 --> 00:19:00,880
in widespread decentralized
experimentation

444
00:19:00,880 --> 00:19:03,600
so that it's the workers,
not the developers, who

445
00:19:03,600 --> 00:19:05,700
are deciding what are
the applications that

446
00:19:05,700 --> 00:19:09,240
are going to be most valuable
for the health care workers.

447
00:19:09,240 --> 00:19:12,460
And one way they do
this is prompt-a-thons,

448
00:19:12,460 --> 00:19:15,960
where they provide workers with
basic education in generative

449
00:19:15,960 --> 00:19:22,080
AI and hands-on ability to use
the generative AI with health

450
00:19:22,080 --> 00:19:24,760
care data sets in order
to get a feel for things

451
00:19:24,760 --> 00:19:28,650
and begin to develop ideas
for good applications.

452
00:19:28,650 --> 00:19:32,090
One developer said, "GPT has
these fascinating moments where

453
00:19:32,090 --> 00:19:33,150
it just does stuff."

454
00:19:33,150 --> 00:19:37,400
And that's motivating for people
working with generative AI.

455
00:19:37,400 --> 00:19:39,770
Another challenge is
that these solutions

456
00:19:39,770 --> 00:19:41,730
require laborious iteration.

457
00:19:41,730 --> 00:19:44,300
So for that tool I
told you about with

458
00:19:44,300 --> 00:19:46,340
the patient-friendly
discharge summaries,

459
00:19:46,340 --> 00:19:52,220
that required clinicians to go
through dozens of patients who

460
00:19:52,220 --> 00:19:54,710
had been in inpatients
and their clinical notes

461
00:19:54,710 --> 00:19:57,560
to figure out how to
draft an accurate prompt

462
00:19:57,560 --> 00:20:00,860
that resulted in a good output
for the patient-friendly

463
00:20:00,860 --> 00:20:02,160
discharge notes.

464
00:20:02,160 --> 00:20:04,370
What the developers
do is they need

465
00:20:04,370 --> 00:20:08,360
to sustain worker motivation
for doing this iteration on top

466
00:20:08,360 --> 00:20:10,230
of their regular job.

467
00:20:10,230 --> 00:20:14,280
And so they provide on-demand
assistance in office hours,

468
00:20:14,280 --> 00:20:17,360
provide help with what they
call "prompt tricking,"

469
00:20:17,360 --> 00:20:22,310
which in this case involves
things like with GPT-4 using all

470
00:20:22,310 --> 00:20:24,600
capital letters when
giving instructions,

471
00:20:24,600 --> 00:20:27,840
and even telling GPT-4 that
you'll give it a tip if it gives

472
00:20:27,840 --> 00:20:30,540
you good outputs.

473
00:20:30,540 --> 00:20:33,090
Finally, generative
AI presents new risks

474
00:20:33,090 --> 00:20:37,210
because of hallucinations, bias,
and lack of interpretability.

475
00:20:37,210 --> 00:20:41,170
So developers use a
Gen AI risk screen

476
00:20:41,170 --> 00:20:44,010
where they screen all
potential applications up front

477
00:20:44,010 --> 00:20:46,150
to see which ones
are worth developing.

478
00:20:46,150 --> 00:20:49,750
So in one department, clinicians
came to developers and said,

479
00:20:49,750 --> 00:20:54,000
we really want to use generative
AI to translate consent forms

480
00:20:54,000 --> 00:20:55,480
into other languages.

481
00:20:55,480 --> 00:20:57,150
But what the
developers discovered

482
00:20:57,150 --> 00:21:00,790
is that GPT-4 tends to
miss important nuances.

483
00:21:00,790 --> 00:21:03,760
So it's not a good technical
fit with generative AI,

484
00:21:03,760 --> 00:21:07,530
and it also raises
regulatory concerns related

485
00:21:07,530 --> 00:21:10,000
to this bad interpretation.

486
00:21:10,000 --> 00:21:12,960
So they stopped the
development of that tool.

487
00:21:12,960 --> 00:21:15,750
So at Langone, they
catalyzed this decentralized

488
00:21:15,750 --> 00:21:17,950
experimentation
with prompt-a-thons,

489
00:21:17,950 --> 00:21:20,500
used this on-demand
technical support,

490
00:21:20,500 --> 00:21:23,670
and also, use a risk
score to prioritize which

491
00:21:23,670 --> 00:21:26,000
projects are going to be best.

492
00:21:26,000 --> 00:21:30,020
So in sum, what I want you
to take away from today

493
00:21:30,020 --> 00:21:33,920
is we often focus on how these
tools are great for quality,

494
00:21:33,920 --> 00:21:35,490
costs, and revenues.

495
00:21:35,490 --> 00:21:37,760
But we forget that
they often present

496
00:21:37,760 --> 00:21:40,940
problems for the workers
who need to implement them.

497
00:21:40,940 --> 00:21:43,310
Developers need to
address these challenges

498
00:21:43,310 --> 00:21:47,180
to design AI solutions for
successful implementation.

499
00:21:47,180 --> 00:21:48,697
Thank you.

500
00:21:48,697 --> 00:21:52,593
[APPLAUSE]

501
00:21:52,593 --> 00:22:03,800

502
00:22:03,800 --> 00:22:04,860
All right, hi, everyone.

503
00:22:04,860 --> 00:22:06,000
My name is Joe Doyle.

504
00:22:06,000 --> 00:22:10,220
I'm an economist at MIT
Sloan, and a lot of my day job

505
00:22:10,220 --> 00:22:12,320
is partnering with
payers and providers

506
00:22:12,320 --> 00:22:14,270
to try to get patients
to be healthier

507
00:22:14,270 --> 00:22:16,730
and providers to be
more guideline adherent

508
00:22:16,730 --> 00:22:18,650
and do a better job.

509
00:22:18,650 --> 00:22:22,050
So I help run our Health
Systems Initiative at Sloan.

510
00:22:22,050 --> 00:22:24,400
And we have three
pillars of research

511
00:22:24,400 --> 00:22:27,220
backed by about 30 faculty
members and a bunch of PhD

512
00:22:27,220 --> 00:22:30,550
students working on health
analytics, healthcare

513
00:22:30,550 --> 00:22:32,570
operations, and health
care incentives.

514
00:22:32,570 --> 00:22:35,733
On the analytics side, we
build new frontier tools,

515
00:22:35,733 --> 00:22:37,150
but also, we have
people like Kate

516
00:22:37,150 --> 00:22:39,610
who help think about how do
you get those tools adopted

517
00:22:39,610 --> 00:22:41,060
in a meaningful way.

518
00:22:41,060 --> 00:22:43,500
Operations are to get more
efficiency in our health care

519
00:22:43,500 --> 00:22:44,000
system.

520
00:22:44,000 --> 00:22:45,432
And then, as an
economist, I think

521
00:22:45,432 --> 00:22:46,640
a lot about these incentives.

522
00:22:46,640 --> 00:22:48,790
How do you get people
to behave healthier?

523
00:22:48,790 --> 00:22:52,300
How do you set up the right
procedures and incentives

524
00:22:52,300 --> 00:22:53,860
to get that to happen?

525
00:22:53,860 --> 00:22:56,380
So let me talk about one
part of my research, which

526
00:22:56,380 --> 00:22:58,760
is randomized trials in
health care delivery.

527
00:22:58,760 --> 00:23:02,590
Most drugs or medical devices
will have some randomized trials

528
00:23:02,590 --> 00:23:04,790
behind them to show you
that they're effective.

529
00:23:04,790 --> 00:23:07,370
But when we go to change how
we're going to deliver care,

530
00:23:07,370 --> 00:23:09,730
there's much less that's
done in that rigorous way

531
00:23:09,730 --> 00:23:12,110
that we can learn whether
things work or don't work.

532
00:23:12,110 --> 00:23:15,290
And so I'm here to evangelize
that we need more of this.

533
00:23:15,290 --> 00:23:18,530
So let me talk a bit about
the economics underlying it.

534
00:23:18,530 --> 00:23:19,880
Here's a map of Boston.

535
00:23:19,880 --> 00:23:22,470
And if you think
about the Bus Route 1,

536
00:23:22,470 --> 00:23:27,650
if you get on in Roxbury, the
life expectancy is 59 years old.

537
00:23:27,650 --> 00:23:29,940
If you go a couple of
miles north to Back Bay,

538
00:23:29,940 --> 00:23:32,520
life expectancy is 92 years old.

539
00:23:32,520 --> 00:23:36,710
So that's 3 miles buys you
30 years of life expectancy.

540
00:23:36,710 --> 00:23:39,282
So you often hear people
say there's genetic code.

541
00:23:39,282 --> 00:23:39,990
There's zip code.

542
00:23:39,990 --> 00:23:42,320
This is the zip code part of it.

543
00:23:42,320 --> 00:23:44,570
Through poverty and
other social risk

544
00:23:44,570 --> 00:23:47,000
factors are major
drivers of whether people

545
00:23:47,000 --> 00:23:48,360
are healthy or not.

546
00:23:48,360 --> 00:23:51,140
So health care in
the clinic needs

547
00:23:51,140 --> 00:23:53,660
to think a bit more
broadly and think, well,

548
00:23:53,660 --> 00:23:54,660
what can we do upstream?

549
00:23:54,660 --> 00:23:56,868
How can we address the issues
that are really driving

550
00:23:56,868 --> 00:24:00,230
a 30-year drop in
life expectancy?

551
00:24:00,230 --> 00:24:02,150
So one way we
think about it is--

552
00:24:02,150 --> 00:24:04,070
and John talked about
the need for insurance

553
00:24:04,070 --> 00:24:05,280
if you get unlucky.

554
00:24:05,280 --> 00:24:09,110
Well this graphic here shows
you that 1% of the population

555
00:24:09,110 --> 00:24:13,670
drives about 30% of health
care costs in any given year.

556
00:24:13,670 --> 00:24:17,700
And we're spending $4.5 trillion
in health care in the US.

557
00:24:17,700 --> 00:24:21,280
So 1% of people are driving,
have over $1 trillion

558
00:24:21,280 --> 00:24:22,527
of spending on them.

559
00:24:22,527 --> 00:24:24,610
If we could figure out how
to treat those patients

560
00:24:24,610 --> 00:24:27,640
in a healthier way, in a
more effective way, more

561
00:24:27,640 --> 00:24:29,800
efficient way, we could
save a lot of money

562
00:24:29,800 --> 00:24:31,460
and improve their lives.

563
00:24:31,460 --> 00:24:34,460
So in the health care
delivery literature,

564
00:24:34,460 --> 00:24:37,720
these people are called super
utilizers or health care

565
00:24:37,720 --> 00:24:38,860
hotspots.

566
00:24:38,860 --> 00:24:42,310
And there was this really
interesting article

567
00:24:42,310 --> 00:24:43,690
in The New Yorker
a few years ago

568
00:24:43,690 --> 00:24:47,620
about hotspotting could make
people's lives better and save

569
00:24:47,620 --> 00:24:49,040
money at the same time.

570
00:24:49,040 --> 00:24:52,280
So it was a profile
of Camden, New Jersey.

571
00:24:52,280 --> 00:24:55,730
It's a high-poverty city
outside of Philadelphia.

572
00:24:55,730 --> 00:24:58,580
And Dr. Jeff Brenner, who's
sort of a hero of mine--

573
00:24:58,580 --> 00:25:01,720
I'd say he's a very
inspiring physician there

574
00:25:01,720 --> 00:25:05,500
who organized the collection
of data from all the health

575
00:25:05,500 --> 00:25:07,925
care providers into one
health information exchange.

576
00:25:07,925 --> 00:25:09,550
And then, they would
try to figure out,

577
00:25:09,550 --> 00:25:13,660
who are these hotspots, and
how can we treat them better?

578
00:25:13,660 --> 00:25:17,110
Their idea was to create
a team of about 10 people,

579
00:25:17,110 --> 00:25:20,130
including nurses, psychologists,
social workers who

580
00:25:20,130 --> 00:25:23,850
would make home visits, I
think about 5 to 10 home visits

581
00:25:23,850 --> 00:25:26,760
to the super utilizers,
send nurses with them

582
00:25:26,760 --> 00:25:30,930
to their appointments, and
try to navigate that system

583
00:25:30,930 --> 00:25:32,305
in a more effective way.

584
00:25:32,305 --> 00:25:33,930
If you had all the
health care problems

585
00:25:33,930 --> 00:25:37,360
the people in the 1% have,
1% of health care spending,

586
00:25:37,360 --> 00:25:40,650
then you would really appreciate
having this team of people there

587
00:25:40,650 --> 00:25:42,240
to help you.

588
00:25:42,240 --> 00:25:46,170
So here are the number
of admissions leading up

589
00:25:46,170 --> 00:25:48,250
to entry into that program
and then afterwards.

590
00:25:48,250 --> 00:25:51,100
And you can see in the quarters
before, you see a ramp-up.

591
00:25:51,100 --> 00:25:52,150
People got unlucky.

592
00:25:52,150 --> 00:25:54,660
They lost the genetic lottery
or some kind of lottery,

593
00:25:54,660 --> 00:25:57,100
and they are going
to the hospital more.

594
00:25:57,100 --> 00:25:58,960
That's how you get
into the program.

595
00:25:58,960 --> 00:26:01,960
And then, after they're in
the program, their costs fall.

596
00:26:01,960 --> 00:26:05,970
And that's exactly what The New
Yorker article and hotspotting

597
00:26:05,970 --> 00:26:08,560
programs that were popping
up all across the country

598
00:26:08,560 --> 00:26:10,230
were pointing to this
graph and saying,

599
00:26:10,230 --> 00:26:12,750
we could save a lot
of money and improve

600
00:26:12,750 --> 00:26:14,740
the lives of these
people at the same time.

601
00:26:14,740 --> 00:26:17,650
Now, economists and
health service researchers

602
00:26:17,650 --> 00:26:19,300
wonder, well, what
would have happened

603
00:26:19,300 --> 00:26:22,390
if they didn't get that program,
what we call the counterfactual?

604
00:26:22,390 --> 00:26:24,400
Against the fact that
they got the program, what

605
00:26:24,400 --> 00:26:25,445
would have happened?

606
00:26:25,445 --> 00:26:27,070
Well, the nice thing
about a randomized

607
00:26:27,070 --> 00:26:28,445
controlled trial,
you flip a coin

608
00:26:28,445 --> 00:26:30,528
and figure out who's in
the treatment group, who's

609
00:26:30,528 --> 00:26:31,460
in the control group.

610
00:26:31,460 --> 00:26:34,310
The control group gives
you the counterfactual,

611
00:26:34,310 --> 00:26:36,820
what would have happened
if we flipped the coin

612
00:26:36,820 --> 00:26:38,840
and got heads instead of tails.

613
00:26:38,840 --> 00:26:40,960
And so in the
orange bars, that's

614
00:26:40,960 --> 00:26:42,770
exactly what happened
to the control group.

615
00:26:42,770 --> 00:26:46,060
It exactly mirrors what happened
in that treatment group that

616
00:26:46,060 --> 00:26:50,150
got lavished care, all these
home visits, phone calls,

617
00:26:50,150 --> 00:26:53,090
appointment collaborations.

618
00:26:53,090 --> 00:26:55,270
And so what we learned
from this experiment

619
00:26:55,270 --> 00:27:00,620
is that the drop in health care
spending was a bit of a mirage.

620
00:27:00,620 --> 00:27:03,160
It wasn't the program
that did that.

621
00:27:03,160 --> 00:27:05,390
When people get
healthy and unlucky,

622
00:27:05,390 --> 00:27:08,000
they tend to get luckier
than they were before.

623
00:27:08,000 --> 00:27:11,380
And they sort of
revert to the mean.

624
00:27:11,380 --> 00:27:14,122
Now, I wanted to go
into this program

625
00:27:14,122 --> 00:27:16,080
and say, well, let's show
that this thing works

626
00:27:16,080 --> 00:27:18,030
so that it will spread even
faster than it's already

627
00:27:18,030 --> 00:27:18,550
spreading.

628
00:27:18,550 --> 00:27:20,970
This is a very inspiring
program, helping people

629
00:27:20,970 --> 00:27:23,220
both in their clinical
needs and their social needs

630
00:27:23,220 --> 00:27:24,460
to improve their health.

631
00:27:24,460 --> 00:27:26,130
And what I learned
was that it's not

632
00:27:26,130 --> 00:27:29,143
going to be sustained by
lower health care costs.

633
00:27:29,143 --> 00:27:31,060
We might want to pay for
it for other reasons,

634
00:27:31,060 --> 00:27:32,970
but we can't make
the claim that we're

635
00:27:32,970 --> 00:27:34,507
saving money at the same time.

636
00:27:34,507 --> 00:27:36,840
And it might have felt good
to pat ourselves on the back

637
00:27:36,840 --> 00:27:38,440
and say, we're saving
money at the same time.

638
00:27:38,440 --> 00:27:39,940
But if you're not
actually doing it,

639
00:27:39,940 --> 00:27:41,690
then you're just going
to chase your tail.

640
00:27:41,690 --> 00:27:43,290
We need to know
what works in order

641
00:27:43,290 --> 00:27:48,120
to have that spread so we
can make real progress.

642
00:27:48,120 --> 00:27:50,860
So the paper came out
just before COVID.

643
00:27:50,860 --> 00:27:55,050
There was a lot of attention
devoted to it because this

644
00:27:55,050 --> 00:27:57,600
was a very popular program,
like I said, springing

645
00:27:57,600 --> 00:27:59,710
up all across the country.

646
00:27:59,710 --> 00:28:01,780
And what folks said
across the country was,

647
00:28:01,780 --> 00:28:03,718
well, I used to assume
that this works.

648
00:28:03,718 --> 00:28:05,010
This seems like it should work.

649
00:28:05,010 --> 00:28:06,670
In our own data,
our own dashboards,

650
00:28:06,670 --> 00:28:08,770
people get healthier after
they go on the program.

651
00:28:08,770 --> 00:28:11,103
And what they had to do was
go back to the drawing board

652
00:28:11,103 --> 00:28:14,450
and figure out, does it
work in my area or not?

653
00:28:14,450 --> 00:28:16,910
So going from a
presumption that something

654
00:28:16,910 --> 00:28:19,200
works to actually let's
figure out if it works,

655
00:28:19,200 --> 00:28:20,640
that's what we do at MIT.

656
00:28:20,640 --> 00:28:24,120
We actually try to figure out
what works in a rigorous way.

657
00:28:24,120 --> 00:28:27,320
So then, we can march down the
path of progress as opposed

658
00:28:27,320 --> 00:28:29,420
to window dressing
that feels good when

659
00:28:29,420 --> 00:28:31,430
you say that you're doing it.

660
00:28:31,430 --> 00:28:34,850
So I partnered with Geisinger
on another study that

661
00:28:34,850 --> 00:28:37,350
targeted patients
who had diabetes,

662
00:28:37,350 --> 00:28:40,240
and they were uncontrolled
according to the clinicians

663
00:28:40,240 --> 00:28:40,740
there.

664
00:28:40,740 --> 00:28:44,580
They had a blood sugar
that's HBA1C level over 8.

665
00:28:44,580 --> 00:28:48,260
So to put it in perspective
would be diabetic.

666
00:28:48,260 --> 00:28:50,730
And over 8, they call
it an uncontrolled.

667
00:28:50,730 --> 00:28:53,900
On average, people had
a score of over 10.

668
00:28:53,900 --> 00:28:56,550
And if your doctor tells
you have an A1C over 10,

669
00:28:56,550 --> 00:28:58,430
that's kind of a scary number.

670
00:28:58,430 --> 00:29:01,440
So this program
was also inspiring.

671
00:29:01,440 --> 00:29:04,670
It gives lots of healthy food
to these low-income patients

672
00:29:04,670 --> 00:29:05,730
with diabetes.

673
00:29:05,730 --> 00:29:08,700
It gives about 10 meals per
week, not only for the patient,

674
00:29:08,700 --> 00:29:10,170
but for their entire family.

675
00:29:10,170 --> 00:29:13,180
They go to a clinic every week
to pick up their groceries.

676
00:29:13,180 --> 00:29:15,780
And at the clinic, there's
a dietitian, a nurse,

677
00:29:15,780 --> 00:29:17,850
and a community
health worker that

678
00:29:17,850 --> 00:29:20,350
will try to address their
needs through foot exams,

679
00:29:20,350 --> 00:29:25,740
have dietitian consultations,
have cooking classes,

680
00:29:25,740 --> 00:29:27,910
diabetes self-management
training.

681
00:29:27,910 --> 00:29:31,620
And these are brand new
clinics that are well-lit,

682
00:29:31,620 --> 00:29:33,610
staffed with
super-friendly people.

683
00:29:33,610 --> 00:29:37,710
People love going
to these clinics.

684
00:29:37,710 --> 00:29:40,650
But what I found was, again,
a mean reversion story,

685
00:29:40,650 --> 00:29:44,160
that when they were targeting
the people with very high levels

686
00:29:44,160 --> 00:29:46,690
of A1C, the treatment
group got healthier,

687
00:29:46,690 --> 00:29:49,210
but the control group
also got healthier.

688
00:29:49,210 --> 00:29:51,940
And this is back to this
mean reversion story.

689
00:29:51,940 --> 00:29:54,900
And so what am I taking away
from this sort of now growing

690
00:29:54,900 --> 00:29:57,905
evidence that when you target
people with a pretty sick,

691
00:29:57,905 --> 00:29:59,530
some measure that
they're pretty sick--

692
00:29:59,530 --> 00:30:01,920
so it was a high blood
sugar level or high spending

693
00:30:01,920 --> 00:30:03,070
in the last year--

694
00:30:03,070 --> 00:30:06,160
you really have to be worried
that they might mean revert.

695
00:30:06,160 --> 00:30:09,290
So one idea is you can't
just trust those dashboards

696
00:30:09,290 --> 00:30:11,523
that you're seeing that
people are getting healthier.

697
00:30:11,523 --> 00:30:13,940
If mean reversion is a story
that you have to worry about,

698
00:30:13,940 --> 00:30:16,470
you can't just rely on
a pre-post comparison.

699
00:30:16,470 --> 00:30:18,620
You need a credible
control group.

700
00:30:18,620 --> 00:30:21,810
The other idea is to target
durably eligible people.

701
00:30:21,810 --> 00:30:24,440
So the research
I'm doing right now

702
00:30:24,440 --> 00:30:27,530
is I'm finding that if you could
predict who would get healthier

703
00:30:27,530 --> 00:30:29,688
regardless of the
program, then you

704
00:30:29,688 --> 00:30:31,730
could target the scarce
resources of this program

705
00:30:31,730 --> 00:30:34,440
toward the people who would
actually need the program.

706
00:30:34,440 --> 00:30:36,680
And when I did a reanalysis
of the people who

707
00:30:36,680 --> 00:30:38,850
predicted to not get
healthier on their own

708
00:30:38,850 --> 00:30:41,060
based on what we could
observe when they enrolled

709
00:30:41,060 --> 00:30:43,950
in the study, those
people, their HBA1C

710
00:30:43,950 --> 00:30:45,950
would fall by more than
a point if you gave them

711
00:30:45,950 --> 00:30:48,090
the program, which is
better than a lot of drugs,

712
00:30:48,090 --> 00:30:51,290
so getting people off of drugs
toward healthier food, which

713
00:30:51,290 --> 00:30:52,800
was the initial
goal of the program,

714
00:30:52,800 --> 00:30:56,330
but learning how to target it
in a way that actually makes

715
00:30:56,330 --> 00:30:58,700
that difference as opposed
to just hoping that you're

716
00:30:58,700 --> 00:31:00,770
making the difference.

717
00:31:00,770 --> 00:31:04,260
And also, just back
to that bus route map,

718
00:31:04,260 --> 00:31:06,995
social risk factors are
obviously important.

719
00:31:06,995 --> 00:31:08,370
So there's just
more work that we

720
00:31:08,370 --> 00:31:10,300
need to do to learn
how to address them.

721
00:31:10,300 --> 00:31:12,250
We don't just give
up on those 1%.

722
00:31:12,250 --> 00:31:14,633
We think about, well, what
we were doing before wasn't

723
00:31:14,633 --> 00:31:15,550
working to save money.

724
00:31:15,550 --> 00:31:17,790
Let's see if we could--
are there other programs

725
00:31:17,790 --> 00:31:20,320
you could do, like
better remote monitoring,

726
00:31:20,320 --> 00:31:23,940
other technological solutions
that you'll be seeing here

727
00:31:23,940 --> 00:31:26,670
at MIT throughout the day.

728
00:31:26,670 --> 00:31:28,530
Just really quickly,
just to point out that

729
00:31:28,530 --> 00:31:32,837
I work with Geisinger on these
large-scale messaging campaigns.

730
00:31:32,837 --> 00:31:34,920
So you might get these
text messages reminding you

731
00:31:34,920 --> 00:31:36,010
of your appointment.

732
00:31:36,010 --> 00:31:37,860
Well, what should
be in those messages

733
00:31:37,860 --> 00:31:40,800
to get you to do healthy
behaviors like get your cancer

734
00:31:40,800 --> 00:31:43,860
screenings on time, get
vaccinated, and so on?

735
00:31:43,860 --> 00:31:45,600
And with machine
learning, we can

736
00:31:45,600 --> 00:31:47,460
try to target those
messages, figuring out

737
00:31:47,460 --> 00:31:49,290
exactly what types
of messages get

738
00:31:49,290 --> 00:31:51,610
you to behave in the
healthiest ways, so at least

739
00:31:51,610 --> 00:31:53,193
giving you that
advice so that you can

740
00:31:53,193 --> 00:31:55,650
choose to do so for yourself.

741
00:31:55,650 --> 00:31:58,920
At Sloan, we're working
with Quest Diagnostics,

742
00:31:58,920 --> 00:32:02,520
which takes blood from
almost every American

743
00:32:02,520 --> 00:32:03,340
every three years.

744
00:32:03,340 --> 00:32:05,200
Everybody will go into
there at some point.

745
00:32:05,200 --> 00:32:07,323
And we're doing randomized
controlled trials

746
00:32:07,323 --> 00:32:09,740
to see what we could do to get
people to be healthier when

747
00:32:09,740 --> 00:32:12,440
they get those results back.

748
00:32:12,440 --> 00:32:15,740
And then I just wanted to end
on you don't need to always do

749
00:32:15,740 --> 00:32:17,040
a randomized controlled trial.

750
00:32:17,040 --> 00:32:19,770
Sometimes with data
analytics, we can do more.

751
00:32:19,770 --> 00:32:24,200
So just my last example here is
the x-axis here is birth weight.

752
00:32:24,200 --> 00:32:29,150
And at 1,500 grams,
3 pounds 5 ounces,

753
00:32:29,150 --> 00:32:32,670
newborns below that are
labeled very low birth weight.

754
00:32:32,670 --> 00:32:36,930
And what you can see here
is that as you get lighter,

755
00:32:36,930 --> 00:32:38,180
we spend more on you.

756
00:32:38,180 --> 00:32:42,030
And you see a discrete jump
if you go across 1,500 grams

757
00:32:42,030 --> 00:32:46,530
because especially at
lower-level hospitals,

758
00:32:46,530 --> 00:32:49,590
they increase the amount
of treatment they give you.

759
00:32:49,590 --> 00:32:51,510
If you get categorized
in that way.

760
00:32:51,510 --> 00:32:53,602
And then, what we see
is as you get lighter,

761
00:32:53,602 --> 00:32:54,810
the mortality rate is rising.

762
00:32:54,810 --> 00:32:57,650
But it falls against
the trend if you

763
00:32:57,650 --> 00:33:00,660
cross that magical threshold
that got you the extra care.

764
00:33:00,660 --> 00:33:03,670
And so here, if we're rooting
around for waste and value

765
00:33:03,670 --> 00:33:06,340
in health care, here's a
place where people born just

766
00:33:06,340 --> 00:33:08,740
above and below that threshold
is like a genetic lottery

767
00:33:08,740 --> 00:33:10,100
or a birth weight lottery.

768
00:33:10,100 --> 00:33:12,140
Some people win the lottery,
and they're actually too light.

769
00:33:12,140 --> 00:33:13,515
They're just below
the threshold.

770
00:33:13,515 --> 00:33:15,580
They get more health
care, and actually, they

771
00:33:15,580 --> 00:33:17,360
get better outcomes.

772
00:33:17,360 --> 00:33:19,940
And then, this was replicated
in other countries like Chile.

773
00:33:19,940 --> 00:33:21,680
Here's the mortality
result for Chile.

774
00:33:21,680 --> 00:33:25,450
But here's eighth grade test
scores or first to eighth grade

775
00:33:25,450 --> 00:33:26,120
test scores.

776
00:33:26,120 --> 00:33:28,960
If you were born too late, you
do better in school in Chile.

777
00:33:28,960 --> 00:33:31,630
And you do better
in school in Norway.

778
00:33:31,630 --> 00:33:34,570
But in high school,
you're better off

779
00:33:34,570 --> 00:33:37,600
if you were born a few
grams less at the time

780
00:33:37,600 --> 00:33:39,100
that you were born.

781
00:33:39,100 --> 00:33:41,450
So there's a lot to do.

782
00:33:41,450 --> 00:33:43,930
And I really appreciate you
guys spending the day with us

783
00:33:43,930 --> 00:33:45,980
here at MIT to figure out
how we can do it together.

784
00:33:45,980 --> 00:33:46,688
Thanks very much.

785
00:33:46,688 --> 00:33:49,430
[APPLAUSE]

786
00:33:49,430 --> 00:34:03,890

787
00:34:03,890 --> 00:34:07,430
I'm Mike Yaffee, and it's a
great pleasure to be here today.

788
00:34:07,430 --> 00:34:10,070
As you heard, I'm a
professor in biology

789
00:34:10,070 --> 00:34:11,310
and biological engineering.

790
00:34:11,310 --> 00:34:14,639
And I direct the MIT Center
for Precision Cancer Medicine.

791
00:34:14,639 --> 00:34:16,469
But I'm also a
practicing clinician.

792
00:34:16,469 --> 00:34:19,739
I'm an intensive care physician,
and I'm a practicing surgeon.

793
00:34:19,739 --> 00:34:22,699
And what I'm hoping to do in
the next nine minutes or so

794
00:34:22,699 --> 00:34:26,510
is to connect this session on
health care delivery with some

795
00:34:26,510 --> 00:34:28,639
of the basic science
and engineering concepts

796
00:34:28,639 --> 00:34:31,199
that we heard about during
the first half of this,

797
00:34:31,199 --> 00:34:34,100
during this morning's session.

798
00:34:34,100 --> 00:34:36,840
And so to start off, I'm
going to focus on cancer.

799
00:34:36,840 --> 00:34:41,210
And just for fun, I took every
paper that had been published

800
00:34:41,210 --> 00:34:45,860
in Cell or Nature 43 years ago
in 1981 that contained the word

801
00:34:45,860 --> 00:34:46,560
"cancer."

802
00:34:46,560 --> 00:34:49,920
And I used deep learning
to build a word cloud.

803
00:34:49,920 --> 00:34:51,800
And the words that
jump out at us

804
00:34:51,800 --> 00:34:56,929
are things like gene, protein,
DNA, virus, transforming,

805
00:34:56,929 --> 00:34:57,810
principle.

806
00:34:57,810 --> 00:35:01,660
And in fact, that fit
with 1981's view of cancer

807
00:35:01,660 --> 00:35:04,840
as a genetic disease in which
oncogenes were turned off

808
00:35:04,840 --> 00:35:06,830
or tumor suppressor
genes were silenced.

809
00:35:06,830 --> 00:35:11,240
And MIT was at the very
center of this work.

810
00:35:11,240 --> 00:35:14,890
In fact, many of those papers
in science, in Cell or Nature,

811
00:35:14,890 --> 00:35:19,210
were authored by the luminaries
here at MIT, Bob Weinberg

812
00:35:19,210 --> 00:35:23,060
and Phil Sharp, David Houseman,
Rudy Jaenisch, Sheldon Penman,

813
00:35:23,060 --> 00:35:24,730
David Baltimore, all
the people that you

814
00:35:24,730 --> 00:35:26,480
heard alluded to this morning.

815
00:35:26,480 --> 00:35:29,930
The problem is, let's
look 42 years later.

816
00:35:29,930 --> 00:35:33,020
There are many more papers
about cancer in Cell or Nature.

817
00:35:33,020 --> 00:35:36,370
And so I limited myself
to the first 160 papers

818
00:35:36,370 --> 00:35:37,940
that I could identify.

819
00:35:37,940 --> 00:35:41,470
And now, all of a sudden, the
main text in the abstracts

820
00:35:41,470 --> 00:35:46,280
are things like patients, human
treatment, therapeutic response.

821
00:35:46,280 --> 00:35:48,490
This should be very
worrisome for us

822
00:35:48,490 --> 00:35:51,740
here at MIT because we
don't have a hospital.

823
00:35:51,740 --> 00:35:53,930
We don't own a medical school.

824
00:35:53,930 --> 00:35:57,410
Fortunately, however, if
you look a little deeper,

825
00:35:57,410 --> 00:35:59,890
there are many ways
that MIT can contribute

826
00:35:59,890 --> 00:36:03,750
because underlying this is a
much more systems-based approach

827
00:36:03,750 --> 00:36:06,330
where instead of
just oncogenes, now

828
00:36:06,330 --> 00:36:08,640
we see the immune
system and the tumor

829
00:36:08,640 --> 00:36:12,310
microenvironment, metabolic
defects, and signaling defects.

830
00:36:12,310 --> 00:36:15,570
And so using that, I want to
talk about how in the future

831
00:36:15,570 --> 00:36:20,520
we're going to bridge the divide
between the clinical world

832
00:36:20,520 --> 00:36:22,150
and the basic science world.

833
00:36:22,150 --> 00:36:24,120
And I'm going to use
my own lab's research

834
00:36:24,120 --> 00:36:27,780
just as an example of
how to illustrate this.

835
00:36:27,780 --> 00:36:30,550
So I want to leave you with
three take-home messages.

836
00:36:30,550 --> 00:36:33,240
The first is that the future
of precision health care

837
00:36:33,240 --> 00:36:37,020
has to be an active and
ongoing dynamic collaboration

838
00:36:37,020 --> 00:36:39,520
between clinicians
and basic scientists.

839
00:36:39,520 --> 00:36:42,880
In 1981, the basic
science was revolutionary.

840
00:36:42,880 --> 00:36:45,810
And it was directing
clinical care

841
00:36:45,810 --> 00:36:48,250
or beginning to
illuminate clinical care.

842
00:36:48,250 --> 00:36:51,400
Now, we have to go both
ways in order to do this.

843
00:36:51,400 --> 00:36:55,380
And an example that
I'll use for this

844
00:36:55,380 --> 00:36:58,490
is our lab's research
related to tissue injury.

845
00:36:58,490 --> 00:37:00,730
We've been very interested
in tissue injury

846
00:37:00,730 --> 00:37:03,710
in the setting of both trauma
and cancer, and in particular,

847
00:37:03,710 --> 00:37:06,670
how tissue injury is sensed
by the innate immune system

848
00:37:06,670 --> 00:37:09,490
and how that interacts with
the blood clotting pathways

849
00:37:09,490 --> 00:37:13,360
to further stimulate the immune
system for or against tissue

850
00:37:13,360 --> 00:37:14,090
injury.

851
00:37:14,090 --> 00:37:16,360
And this was work that is
the typical kind of thing

852
00:37:16,360 --> 00:37:18,050
that a biologist
does in his lab,

853
00:37:18,050 --> 00:37:20,860
doesn't think too much
more other than the system

854
00:37:20,860 --> 00:37:24,157
in which we're looking at until
the COVID pandemic came about.

855
00:37:24,157 --> 00:37:26,740
And I say that because during
the COVID pandemic, as you know,

856
00:37:26,740 --> 00:37:29,200
many additional
intensive care units

857
00:37:29,200 --> 00:37:30,560
were opened across the city.

858
00:37:30,560 --> 00:37:32,650
All of us that are intensive
care physicians got

859
00:37:32,650 --> 00:37:35,030
recruited to take care
of those diseases.

860
00:37:35,030 --> 00:37:37,000
And it was clear
to us that this was

861
00:37:37,000 --> 00:37:40,450
a very different type of
respiratory failure caused

862
00:37:40,450 --> 00:37:41,360
by this virus.

863
00:37:41,360 --> 00:37:43,270
It was not the same
respiratory failure

864
00:37:43,270 --> 00:37:46,120
that we saw in patients
with pneumonia or patients

865
00:37:46,120 --> 00:37:47,990
with sepsis that
Kate alluded to.

866
00:37:47,990 --> 00:37:50,350
This was a disease where
the mechanics of the lung

867
00:37:50,350 --> 00:37:51,830
were completely normal.

868
00:37:51,830 --> 00:37:55,350
But for some reason, the lungs
were unable to transfer oxygen

869
00:37:55,350 --> 00:37:56,290
into the blood.

870
00:37:56,290 --> 00:37:59,430
And that turned out to be due
to a blood clotting problem that

871
00:37:59,430 --> 00:38:01,150
was induced by the virus.

872
00:38:01,150 --> 00:38:02,850
In fact, what the
virus was doing

873
00:38:02,850 --> 00:38:05,850
was activating the
immune system and causing

874
00:38:05,850 --> 00:38:07,860
microvascular
thrombosis, something

875
00:38:07,860 --> 00:38:10,990
that's been referred
to as immunothrombosis.

876
00:38:10,990 --> 00:38:14,610
Well, this turned out to
be exactly the process

877
00:38:14,610 --> 00:38:17,760
that we had been studying in
a completely separate context

878
00:38:17,760 --> 00:38:20,440
related to cancer
and tissue trauma.

879
00:38:20,440 --> 00:38:24,120
And this ultimately then led
us to use clot-busting drugs

880
00:38:24,120 --> 00:38:26,970
like alteplase, a drug that
we use for strokes and heart

881
00:38:26,970 --> 00:38:28,860
attacks, to treat
these patients who

882
00:38:28,860 --> 00:38:30,970
had COVID-induced
respiratory failure.

883
00:38:30,970 --> 00:38:34,290
That was the STARS trial that
grew out of the basic research

884
00:38:34,290 --> 00:38:35,903
here at MIT.

885
00:38:35,903 --> 00:38:37,320
Let me spend a
minute and tell you

886
00:38:37,320 --> 00:38:39,450
how a failed
operation completely

887
00:38:39,450 --> 00:38:43,290
changed my view of inflammation,
tissue injury, and cancer.

888
00:38:43,290 --> 00:38:46,600
In 1987, when I was
a medical student,

889
00:38:46,600 --> 00:38:49,110
I was invited to
participate, which

890
00:38:49,110 --> 00:38:52,140
really means hold retractors,
in a case where a patient had

891
00:38:52,140 --> 00:38:53,410
pancreatic cancer.

892
00:38:53,410 --> 00:38:56,550
And in those days in 1987,
we didn't have the imaging

893
00:38:56,550 --> 00:38:57,370
that we have now.

894
00:38:57,370 --> 00:38:59,280
And every patient who
had pancreatic cancer

895
00:38:59,280 --> 00:39:01,830
went to the operating room, and
we would make a big incision

896
00:39:01,830 --> 00:39:04,230
and take a look and make
a decision based on what

897
00:39:04,230 --> 00:39:05,590
we saw about what to do.

898
00:39:05,590 --> 00:39:07,540
And this operation,
unfortunately,

899
00:39:07,540 --> 00:39:09,550
was what we called
a "peek and shriek."

900
00:39:09,550 --> 00:39:10,725
We made a big incision.

901
00:39:10,725 --> 00:39:11,350
We took a look.

902
00:39:11,350 --> 00:39:12,640
There was cancer everywhere.

903
00:39:12,640 --> 00:39:15,480
It was obvious there was no
benefit to operating further

904
00:39:15,480 --> 00:39:16,300
on this patient.

905
00:39:16,300 --> 00:39:18,330
And so we simply
closed the incision

906
00:39:18,330 --> 00:39:20,610
and told the patient
to go home and enjoy

907
00:39:20,610 --> 00:39:22,720
the last few weeks of her life.

908
00:39:22,720 --> 00:39:25,360
Two years later,
when I was an intern,

909
00:39:25,360 --> 00:39:29,130
this patient reappeared in the
emergency room with a bowel

910
00:39:29,130 --> 00:39:32,230
obstruction, two years later.

911
00:39:32,230 --> 00:39:34,210
Now, no one wanted
to operate on her.

912
00:39:34,210 --> 00:39:35,320
But eventually, we had to.

913
00:39:35,320 --> 00:39:37,920
And when we operated on
her, her bowel obstruction

914
00:39:37,920 --> 00:39:40,090
was due to a single
adhesive band.

915
00:39:40,090 --> 00:39:44,950
There was not a spot of cancer
left anywhere inside her body.

916
00:39:44,950 --> 00:39:49,830
And this idea that injury and
trauma combined with live tumor

917
00:39:49,830 --> 00:39:52,630
cells because we hadn't
resected any of the tumor

918
00:39:52,630 --> 00:39:55,350
could somehow activate the
immune system and result

919
00:39:55,350 --> 00:39:58,800
in a complete cure has motivated
a fair amount of our research

920
00:39:58,800 --> 00:39:59,350
ever since.

921
00:39:59,350 --> 00:40:01,740
And here's a paper that we
published just two years ago

922
00:40:01,740 --> 00:40:04,590
that shows how the injury
response in live tumor cells

923
00:40:04,590 --> 00:40:06,430
can promote anti-tumor immunity.

924
00:40:06,430 --> 00:40:09,540
And I think that this ability to
go back and forth between what

925
00:40:09,540 --> 00:40:12,060
we see in the clinic and
what we see in the laboratory

926
00:40:12,060 --> 00:40:13,560
is going to lead
to new approaches

927
00:40:13,560 --> 00:40:16,920
where we can combine surgery,
chemotherapy, and immunotherapy

928
00:40:16,920 --> 00:40:17,890
more effective.

929
00:40:17,890 --> 00:40:19,950
Now, of course,
we're always taught

930
00:40:19,950 --> 00:40:22,600
as physicians above
all else, do no harm.

931
00:40:22,600 --> 00:40:25,390
And so this has
also made me wonder,

932
00:40:25,390 --> 00:40:28,140
maybe the tissue damage and
inflammation and wound healing

933
00:40:28,140 --> 00:40:32,350
response from surgery is making
some of my patients do worse.

934
00:40:32,350 --> 00:40:35,560
Perhaps, I'm adding
fuel to their cancers.

935
00:40:35,560 --> 00:40:39,360
What determines if surgery or
injury or inflammation results

936
00:40:39,360 --> 00:40:41,950
in cancer cure or
cancer progression?

937
00:40:41,950 --> 00:40:42,635
We don't know.

938
00:40:42,635 --> 00:40:44,010
But those are the
types of things

939
00:40:44,010 --> 00:40:45,882
that we should be working on.

940
00:40:45,882 --> 00:40:48,090
The second take-home message
I want to leave you with

941
00:40:48,090 --> 00:40:50,460
is we need to think about
diseases and treatments

942
00:40:50,460 --> 00:40:53,760
not in terms of single genes
but in terms of cell networks.

943
00:40:53,760 --> 00:40:56,070
Now, some of you may
have a cardiologist,

944
00:40:56,070 --> 00:40:58,860
and maybe you have
a pulmonologist,

945
00:40:58,860 --> 00:41:01,440
and maybe you also have
a gastroenterologist.

946
00:41:01,440 --> 00:41:03,200
And you know how
frustrating it is

947
00:41:03,200 --> 00:41:05,010
when you go to see
those physicians

948
00:41:05,010 --> 00:41:06,710
because your
cardiologist only wants

949
00:41:06,710 --> 00:41:08,190
to treat your heart problem.

950
00:41:08,190 --> 00:41:10,670
And he doesn't understand or
doesn't think about the fact

951
00:41:10,670 --> 00:41:12,295
that when you treat
the heart, you also

952
00:41:12,295 --> 00:41:14,920
affect the kidneys in the lungs
and the brain and everything

953
00:41:14,920 --> 00:41:15,420
else.

954
00:41:15,420 --> 00:41:17,310
And as an intensive
care physician,

955
00:41:17,310 --> 00:41:20,190
I don't have the luxury to
treat just one organ system.

956
00:41:20,190 --> 00:41:21,830
I have to think
about my patients

957
00:41:21,830 --> 00:41:24,780
in terms of how all those
organ systems work together.

958
00:41:24,780 --> 00:41:28,500
The same is true moving forward
with how we think about cancer.

959
00:41:28,500 --> 00:41:32,010
We can't think of cancer as
just a mutation in an oncogene

960
00:41:32,010 --> 00:41:33,650
because the cancer
cells are living

961
00:41:33,650 --> 00:41:36,750
surrounded in their
microenvironment by other cells,

962
00:41:36,750 --> 00:41:37,800
including normal--

963
00:41:37,800 --> 00:41:39,720
and this is colon
cancer shown here--

964
00:41:39,720 --> 00:41:42,540
including normal cells and
additional stromal cells.

965
00:41:42,540 --> 00:41:44,550
And the same is true
with genes in cancer.

966
00:41:44,550 --> 00:41:47,332
We can't think of a particular
gene as being mutated.

967
00:41:47,332 --> 00:41:49,290
We have to think about
how that gene is working

968
00:41:49,290 --> 00:41:50,890
in the setting of a network.

969
00:41:50,890 --> 00:41:52,740
And this leads to a
new concept, which

970
00:41:52,740 --> 00:41:54,610
I'm calling network medicine.

971
00:41:54,610 --> 00:41:57,150
And that's because the
connection between genes

972
00:41:57,150 --> 00:42:00,330
and proteins is different
in a disease state

973
00:42:00,330 --> 00:42:02,410
than it is in a
non-disease state.

974
00:42:02,410 --> 00:42:04,380
And this gives us
a new way to be

975
00:42:04,380 --> 00:42:06,210
able to use
disease-specific drug

976
00:42:06,210 --> 00:42:09,360
combinations because if
certain nodes in this network

977
00:42:09,360 --> 00:42:11,590
are connected together
in a disease state,

978
00:42:11,590 --> 00:42:14,800
I could use two drugs that
target two different nodes.

979
00:42:14,800 --> 00:42:17,850
And those drugs would be
particularly effective in cells

980
00:42:17,850 --> 00:42:20,890
in which this network
has been rewired,

981
00:42:20,890 --> 00:42:24,030
whereas it would be much less
effective in the normal state

982
00:42:24,030 --> 00:42:25,770
where only one of
those two nodes

983
00:42:25,770 --> 00:42:28,740
is functionally wired together.

984
00:42:28,740 --> 00:42:30,840
In our own laboratory,
we've discovered

985
00:42:30,840 --> 00:42:32,170
a way in which this works.

986
00:42:32,170 --> 00:42:33,930
We found a
vulnerability that seems

987
00:42:33,930 --> 00:42:37,320
to be cancer specific
that relates to how

988
00:42:37,320 --> 00:42:39,190
cancer cells undergo division.

989
00:42:39,190 --> 00:42:41,970
And this has now led to
two clinical trials and one

990
00:42:41,970 --> 00:42:42,480
proposed.

991
00:42:42,480 --> 00:42:45,480
One clinical trial uses
two different drugs

992
00:42:45,480 --> 00:42:47,760
to treat castrate-resistant
prostate cancer.

993
00:42:47,760 --> 00:42:50,660
Another uses two different
drugs to treat metastatic colon

994
00:42:50,660 --> 00:42:52,860
cancers that contain
a KRAS mutation.

995
00:42:52,860 --> 00:42:54,980
And a proposed
trial is now going

996
00:42:54,980 --> 00:42:57,950
to look at using two
particular drugs in a way

997
00:42:57,950 --> 00:43:00,020
that we think will
dramatically benefit patients

998
00:43:00,020 --> 00:43:01,670
with ovarian cancer.

999
00:43:01,670 --> 00:43:04,520
And the very third point
that I want to leave you with

1000
00:43:04,520 --> 00:43:07,730
is the fact that many of the
drugs and treatments we now use

1001
00:43:07,730 --> 00:43:10,710
work differently than we
previously have thought.

1002
00:43:10,710 --> 00:43:14,400
Now, oftentimes, when
we prescribe a drug,

1003
00:43:14,400 --> 00:43:18,710
we're fully aware that in
addition to the way this drug we

1004
00:43:18,710 --> 00:43:21,780
think works, there are a variety
of side effects of that drug.

1005
00:43:21,780 --> 00:43:25,020
But sometimes, the side effects
could actually be the mechanism.

1006
00:43:25,020 --> 00:43:29,120
And as an example of this, we
were working on a drug called

1007
00:43:29,120 --> 00:43:30,270
5-fluorouracil.

1008
00:43:30,270 --> 00:43:32,670
This is a drug that's
been around for 40 years.

1009
00:43:32,670 --> 00:43:37,130
It is a standard of care that is
used to treat advanced-stage GI

1010
00:43:37,130 --> 00:43:39,380
cancers like pancreatic
cancer and especially

1011
00:43:39,380 --> 00:43:40,680
colorectal cancer.

1012
00:43:40,680 --> 00:43:42,260
And in fact, the
textbooks will all

1013
00:43:42,260 --> 00:43:45,730
tell you that this drug works
very effectively because it

1014
00:43:45,730 --> 00:43:47,150
blocks DNA synthesis.

1015
00:43:47,150 --> 00:43:49,730
It targets this enzyme
called thymidylate synthase.

1016
00:43:49,730 --> 00:43:51,500
And that's how this drug works.

1017
00:43:51,500 --> 00:43:53,390
There's just one
problem with this.

1018
00:43:53,390 --> 00:43:56,020
And that's if you look
objectively at patients

1019
00:43:56,020 --> 00:43:59,290
who respond to the drug or don't
respond to the drug, responders

1020
00:43:59,290 --> 00:44:01,420
in red, non-responders
in blue, and you

1021
00:44:01,420 --> 00:44:04,700
look at the level at which
this target is expressed,

1022
00:44:04,700 --> 00:44:06,710
there's absolutely
no difference.

1023
00:44:06,710 --> 00:44:08,740
How can the drug
work in some people

1024
00:44:08,740 --> 00:44:11,350
and not work in other people
if the target of the drug

1025
00:44:11,350 --> 00:44:13,370
is equally expressed in both?

1026
00:44:13,370 --> 00:44:15,290
And I'll simply cut
to the chase and say,

1027
00:44:15,290 --> 00:44:18,130
a few months ago, we were
able to publish a paper

1028
00:44:18,130 --> 00:44:20,710
and show that, in fact, that's
because the drug does not

1029
00:44:20,710 --> 00:44:21,650
work this way.

1030
00:44:21,650 --> 00:44:24,760
The drug works effectively
in cancer patients

1031
00:44:24,760 --> 00:44:26,510
because of a side effect.

1032
00:44:26,510 --> 00:44:30,190
The drug gets incorporated into
RNA, what Phil Sharp talked

1033
00:44:30,190 --> 00:44:31,280
about this morning.

1034
00:44:31,280 --> 00:44:33,860
And by being
incorporated into RNA,

1035
00:44:33,860 --> 00:44:36,830
it blocks the production
of functional ribosomes,

1036
00:44:36,830 --> 00:44:38,840
the machinery that
makes proteins.

1037
00:44:38,840 --> 00:44:40,870
And these functional
ribosomes turn out

1038
00:44:40,870 --> 00:44:43,950
to be essential in
certain GI cancers.

1039
00:44:43,950 --> 00:44:47,910
And that's the reason that this
drug works particularly well.

1040
00:44:47,910 --> 00:44:50,120
Now, my friends in
computer science

1041
00:44:50,120 --> 00:44:53,760
like to say it's not
a bug, it's a feature.

1042
00:44:53,760 --> 00:44:55,560
And the same is
true with medicines.

1043
00:44:55,560 --> 00:44:57,400
Sometimes, the side
effects aren't bugs.

1044
00:44:57,400 --> 00:44:59,150
Sometimes, they're
actually the mechanisms

1045
00:44:59,150 --> 00:45:00,300
of how the drugs work.

1046
00:45:00,300 --> 00:45:03,200
So I hope I've left you with
these three take-home messages

1047
00:45:03,200 --> 00:45:06,840
for how I think MIT as part
of HEAL can move forward.

1048
00:45:06,840 --> 00:45:09,000
We have to have
better collaborations,

1049
00:45:09,000 --> 00:45:11,100
active collaborations
with clinicians.

1050
00:45:11,100 --> 00:45:12,890
We have to start
thinking about diseases

1051
00:45:12,890 --> 00:45:14,490
in terms of network medicine.

1052
00:45:14,490 --> 00:45:17,060
And we have to recognize
and explore the fact

1053
00:45:17,060 --> 00:45:20,580
that many drugs we use probably
work differently than we think.

1054
00:45:20,580 --> 00:45:21,960
Thank you very much.

1055
00:45:21,960 --> 00:45:24,918
[APPLAUSE]

1056
00:45:24,918 --> 00:45:29,360

1057
00:45:29,360 --> 00:45:31,970
All right, so we are
going to sit on the stage

1058
00:45:31,970 --> 00:45:35,452
and do Q&A. I've got a
couple of questions teed up,

1059
00:45:35,452 --> 00:45:36,660
but I'd rather hear from you.

1060
00:45:36,660 --> 00:45:38,702
So I'm going to start with
one to get us started.

1061
00:45:38,702 --> 00:45:40,980

1062
00:45:40,980 --> 00:45:42,230
And we're all going to answer.

1063
00:45:42,230 --> 00:45:44,935
But I really would hope we
can hear from all of you.

1064
00:45:44,935 --> 00:45:46,810
So if you have a question,
you can come on up

1065
00:45:46,810 --> 00:45:48,190
and stand at the mics.

1066
00:45:48,190 --> 00:45:50,630
And I'll call on you if there
people that are interested.

1067
00:45:50,630 --> 00:45:53,050
But let me start
with one question.

1068
00:45:53,050 --> 00:45:54,690
And Mike, you sort
of hit on this,

1069
00:45:54,690 --> 00:45:56,690
but I'll let you flesh
it out a little bit more,

1070
00:45:56,690 --> 00:46:00,280
which is what makes MIT such a
unique place to do what you do?

1071
00:46:00,280 --> 00:46:02,740
I think the thing that's
so special about MIT

1072
00:46:02,740 --> 00:46:07,090
is how incredibly smart
all of our colleagues

1073
00:46:07,090 --> 00:46:12,830
are, the students, the postdocs,
the staff physicians, the staff

1074
00:46:12,830 --> 00:46:14,740
scientists and engineers,
and particularly

1075
00:46:14,740 --> 00:46:16,880
all of the other faculty.

1076
00:46:16,880 --> 00:46:20,380
Really, a great piece of
advice I got from Phil Sharp

1077
00:46:20,380 --> 00:46:22,190
was if you really
want to be successful,

1078
00:46:22,190 --> 00:46:24,110
just surround yourself
with smart people.

1079
00:46:24,110 --> 00:46:26,560
And you can't find that
anywhere at the density

1080
00:46:26,560 --> 00:46:29,800
that you can find
them here at MIT.

1081
00:46:29,800 --> 00:46:30,670
Kate?

1082
00:46:30,670 --> 00:46:33,430
For me, as someone
who studies AI,

1083
00:46:33,430 --> 00:46:35,600
MIT is just an
embarrassment of riches.

1084
00:46:35,600 --> 00:46:38,840
Literally, in every department
across the institute,

1085
00:46:38,840 --> 00:46:43,060
we have experts studying AI
development and implementation.

1086
00:46:43,060 --> 00:46:45,240
And that's great
because it creates

1087
00:46:45,240 --> 00:46:49,780
this ecosystem of researchers
not only within MIT.

1088
00:46:49,780 --> 00:46:53,110
But because of conferences
and speaker series,

1089
00:46:53,110 --> 00:46:55,800
it means we get lots of
industry practitioners

1090
00:46:55,800 --> 00:46:58,590
and other researchers
from around the globe

1091
00:46:58,590 --> 00:47:00,840
to work with on the problem.

1092
00:47:00,840 --> 00:47:02,225
Yeah, I'll just echo this.

1093
00:47:02,225 --> 00:47:03,600
When it comes to
innovation, it's

1094
00:47:03,600 --> 00:47:07,530
that levels of expertise at
such world-class level across so

1095
00:47:07,530 --> 00:47:09,750
many different domains,
they work together

1096
00:47:09,750 --> 00:47:12,450
to make those innovations
first get developed

1097
00:47:12,450 --> 00:47:14,040
and then get implemented.

1098
00:47:14,040 --> 00:47:15,900
That's pretty unique.

1099
00:47:15,900 --> 00:47:18,910
I think, simply, MIT is
just a no-bullshit place.

1100
00:47:18,910 --> 00:47:22,830
I mean, basically, you can't
do good work of the type we

1101
00:47:22,830 --> 00:47:24,720
talked about unless
you're incredibly

1102
00:47:24,720 --> 00:47:26,290
rigorous and self-critical.

1103
00:47:26,290 --> 00:47:28,450
And that happens more at
MIT than any place I know.

1104
00:47:28,450 --> 00:47:30,720
That's why our students
sometimes are unhappy.

1105
00:47:30,720 --> 00:47:34,500
And basically, we are just a
place that believes enormously

1106
00:47:34,500 --> 00:47:35,890
in scientific rigor.

1107
00:47:35,890 --> 00:47:37,600
You saw it in
Joe's presentation,

1108
00:47:37,600 --> 00:47:40,570
the willingness to take
on strongly held beliefs

1109
00:47:40,570 --> 00:47:41,920
and challenge them.

1110
00:47:41,920 --> 00:47:43,960
And you saw in the
incredible science

1111
00:47:43,960 --> 00:47:47,200
that's being done up here that
MIT is just the perfect place

1112
00:47:47,200 --> 00:47:50,710
to be working in areas where
lives depend on it because we

1113
00:47:50,710 --> 00:47:52,840
don't get so full of our
beliefs that we're not

1114
00:47:52,840 --> 00:47:54,620
willing to challenge them
and take them seriously.

1115
00:47:54,620 --> 00:47:55,910
So I would add that as well.

1116
00:47:55,910 --> 00:47:57,702
It seems like we might
have some questions.

1117
00:47:57,702 --> 00:47:58,630
Please, go ahead.

1118
00:47:58,630 --> 00:48:03,310
Sure, [INAUDIBLE] Walter
Bender [AUDIO OUT]

1119
00:48:03,310 --> 00:48:03,942
Scream it.

1120
00:48:03,942 --> 00:48:05,650
I'll scream it, and
then you can echo it.

1121
00:48:05,650 --> 00:48:06,733
Yeah, no, we hear you now.

1122
00:48:06,733 --> 00:48:08,680
Oh, OK.

1123
00:48:08,680 --> 00:48:10,150
My question has to do--

1124
00:48:10,150 --> 00:48:11,950
let me make an
assertion first, which

1125
00:48:11,950 --> 00:48:14,710
is scientists
write to scientists

1126
00:48:14,710 --> 00:48:16,190
or write for scientists.

1127
00:48:16,190 --> 00:48:18,580
When I open up a
nature journal article,

1128
00:48:18,580 --> 00:48:20,180
in order to understand
that article,

1129
00:48:20,180 --> 00:48:22,460
I've got to have
some background.

1130
00:48:22,460 --> 00:48:24,940
And I noticed that a lot of
you and a lot of the speakers

1131
00:48:24,940 --> 00:48:30,130
today have behind their
name, MD, comma, PhD.

1132
00:48:30,130 --> 00:48:32,560
So when you talk
about communication

1133
00:48:32,560 --> 00:48:37,900
between the scientific community
here at MIT and the clinicians,

1134
00:48:37,900 --> 00:48:41,490
the practitioners across
the river, the ones that

1135
00:48:41,490 --> 00:48:46,080
don't have that PhD
as well as the MD,

1136
00:48:46,080 --> 00:48:49,127
how are we going to bridge
that communication gap?

1137
00:48:49,127 --> 00:48:51,460
Well, Michael, you're a natural
person to start on this.

1138
00:48:51,460 --> 00:48:54,040
I think that's a
brilliant question,

1139
00:48:54,040 --> 00:48:56,170
and I don't have an
easy answer for it.

1140
00:48:56,170 --> 00:48:59,190
But something that I've been
arguing for for a long time

1141
00:48:59,190 --> 00:49:04,560
is we need to embed
PhD scientists

1142
00:49:04,560 --> 00:49:09,570
for at least internships or
short visits into the clinic

1143
00:49:09,570 --> 00:49:11,340
itself.

1144
00:49:11,340 --> 00:49:13,380
The only way that
people are going

1145
00:49:13,380 --> 00:49:14,880
to learn to speak
the same language

1146
00:49:14,880 --> 00:49:16,930
is if they live in
the same community.

1147
00:49:16,930 --> 00:49:21,510
And so I think some interchange
in which basic scientists spend

1148
00:49:21,510 --> 00:49:24,130
some time with
clinicians and clinicians

1149
00:49:24,130 --> 00:49:27,540
have the opportunity to spend
some time in our laboratories

1150
00:49:27,540 --> 00:49:28,630
here at MIT--

1151
00:49:28,630 --> 00:49:31,750
in fact, one of my colleagues,
one of my surgical colleagues,

1152
00:49:31,750 --> 00:49:34,610
is sitting here in the third
row because he's decided

1153
00:49:34,610 --> 00:49:36,210
that he wants to understand.

1154
00:49:36,210 --> 00:49:38,150
He wants to get a
deeper understanding

1155
00:49:38,150 --> 00:49:39,450
of health economics.

1156
00:49:39,450 --> 00:49:41,820
And I think that ability
to go back and forth

1157
00:49:41,820 --> 00:49:43,670
and to embed in each
other's cultures

1158
00:49:43,670 --> 00:49:46,160
is the only way we're
going to make progress.

1159
00:49:46,160 --> 00:49:47,035
Kate, Joe?

1160
00:49:47,035 --> 00:49:48,660
Yeah, I mean, I guess
I would just add,

1161
00:49:48,660 --> 00:49:51,690
so I'm an ethnographer, which
means I'm an anthropologist.

1162
00:49:51,690 --> 00:49:56,000
So I train my students to go do
exactly this where they embed.

1163
00:49:56,000 --> 00:49:58,580
But I think one other
thing we try and teach

1164
00:49:58,580 --> 00:50:00,740
them is to really
understand what

1165
00:50:00,740 --> 00:50:03,507
are the problems of the people
in the setting, not problems

1166
00:50:03,507 --> 00:50:05,840
that you come in with, what
problems that they're really

1167
00:50:05,840 --> 00:50:08,150
dealing with because
that's where the most

1168
00:50:08,150 --> 00:50:10,220
interesting work happens.

1169
00:50:10,220 --> 00:50:12,560
I'll just say really briefly,
at the Poverty Action Lab

1170
00:50:12,560 --> 00:50:15,860
here at MIT, we put out a lot
of policy insights and things

1171
00:50:15,860 --> 00:50:18,750
that are geared toward policy
makers as well as practitioners,

1172
00:50:18,750 --> 00:50:22,018
and something that you guys
can look up after today.

1173
00:50:22,018 --> 00:50:23,060
Yeah, I just want to add.

1174
00:50:23,060 --> 00:50:26,220
The question was talking about
clinicians and researchers,

1175
00:50:26,220 --> 00:50:28,460
but let's extend that
further, which is let's

1176
00:50:28,460 --> 00:50:31,220
talk about Kate's work and
about how AI needs to speak

1177
00:50:31,220 --> 00:50:32,250
to the people using it.

1178
00:50:32,250 --> 00:50:33,790
And we need to be able to--

1179
00:50:33,790 --> 00:50:35,915
and this is something I
think MIT needs to work on.

1180
00:50:35,915 --> 00:50:38,540
I think that's why I'm glad the
School of Humanities and Social

1181
00:50:38,540 --> 00:50:40,950
Sciences is involved with
this health and life sciences

1182
00:50:40,950 --> 00:50:43,510
collaborative, which we need
to get better communication.

1183
00:50:43,510 --> 00:50:45,490
We need to just not
create the new science.

1184
00:50:45,490 --> 00:50:47,448
We need to explain to
people why it's important

1185
00:50:47,448 --> 00:50:49,780
and what we do is important,
how it can change lives.

1186
00:50:49,780 --> 00:50:51,280
And I think that
communication needs

1187
00:50:51,280 --> 00:50:56,083
to go not just PhD to MD but
PhD and MD all the way down

1188
00:50:56,083 --> 00:50:58,500
to people who don't have a
college degree so that everyone

1189
00:50:58,500 --> 00:51:02,010
can really understand why we
do and why it's important.

1190
00:51:02,010 --> 00:51:03,150
Yeah, please?

1191
00:51:03,150 --> 00:51:06,060
Yeah, thank you all so much
for the insightful comments

1192
00:51:06,060 --> 00:51:07,840
and for the wonderful
work you do.

1193
00:51:07,840 --> 00:51:11,250
I was curious about this
conflict between AI and autonomy

1194
00:51:11,250 --> 00:51:13,500
because I think there have
been some studies published

1195
00:51:13,500 --> 00:51:17,050
that show that these AI models
are able to, for example,

1196
00:51:17,050 --> 00:51:19,990
diagnose diseases better
than a radiologist.

1197
00:51:19,990 --> 00:51:21,910
But also, there is this human--

1198
00:51:21,910 --> 00:51:24,600

1199
00:51:24,600 --> 00:51:28,060
as radiologists, you are
trained to do the same job.

1200
00:51:28,060 --> 00:51:31,125
So I was curious
about, should AI

1201
00:51:31,125 --> 00:51:32,750
be taking over the
role of radiologists

1202
00:51:32,750 --> 00:51:34,280
if they are able
to predict better?

1203
00:51:34,280 --> 00:51:37,760
And sort of, where do
we draw the balance?

1204
00:51:37,760 --> 00:51:39,800
I guess I can start with that.

1205
00:51:39,800 --> 00:51:43,760
So what we know from other
technological revolutions

1206
00:51:43,760 --> 00:51:47,390
is what we see is that some
job categories get completely

1207
00:51:47,390 --> 00:51:52,820
displaced and that humans learn
to do higher-order thinking,

1208
00:51:52,820 --> 00:51:54,330
higher-order tasks.

1209
00:51:54,330 --> 00:51:57,650
And so what's going to
happen with AI, I think,

1210
00:51:57,650 --> 00:52:00,530
depends on how fast the
technology progresses

1211
00:52:00,530 --> 00:52:03,360
and what frictions there
are for deployment.

1212
00:52:03,360 --> 00:52:07,350
But one thing, for sure,
is that in the near term,

1213
00:52:07,350 --> 00:52:10,790
many AI solutions are going
to require human and AI

1214
00:52:10,790 --> 00:52:12,120
working together.

1215
00:52:12,120 --> 00:52:15,500
And so what we're going to see
is role reconfiguration where

1216
00:52:15,500 --> 00:52:17,160
humans used to do everything.

1217
00:52:17,160 --> 00:52:20,240
Now, they need to figure out
what is the AI best at versus

1218
00:52:20,240 --> 00:52:22,100
what are they best at.

1219
00:52:22,100 --> 00:52:25,760
Let me just add, the Economics
Department and the Sloan School

1220
00:52:25,760 --> 00:52:27,590
were blessed with the
recent Nobel Prize

1221
00:52:27,590 --> 00:52:29,250
jointly in economics.

1222
00:52:29,250 --> 00:52:32,260
And the two people won the Nobel
Prize, Daron Acemoglu and Simon

1223
00:52:32,260 --> 00:52:34,690
Johnson, wrote a book called
Power and Progress, which

1224
00:52:34,690 --> 00:52:37,400
is very much about this history
and exactly this set of issues.

1225
00:52:37,400 --> 00:52:39,700
So I urge people to
take a look at that.

1226
00:52:39,700 --> 00:52:41,620
Yeah?

1227
00:52:41,620 --> 00:52:43,550
Obviously, from
your presentations,

1228
00:52:43,550 --> 00:52:45,260
health care is a
very complex area.

1229
00:52:45,260 --> 00:52:47,020
There's so many
different stakeholders,

1230
00:52:47,020 --> 00:52:49,120
and there's
competing incentives.

1231
00:52:49,120 --> 00:52:51,590
It seems like health
care, we're very reactive.

1232
00:52:51,590 --> 00:52:53,990
We wait for all these
trends and these diseases.

1233
00:52:53,990 --> 00:52:56,860
Someone has cancer,
obesity, diabetes.

1234
00:52:56,860 --> 00:52:58,340
There's cost disparities.

1235
00:52:58,340 --> 00:53:00,280
There's an aging population.

1236
00:53:00,280 --> 00:53:03,610
Has there been any efforts
to look at health care

1237
00:53:03,610 --> 00:53:05,270
at time of birth?

1238
00:53:05,270 --> 00:53:08,000
I know it's the long term, and
we're very reactive society.

1239
00:53:08,000 --> 00:53:09,970
I want instant
gratification, and we're

1240
00:53:09,970 --> 00:53:12,820
dealing with the crisis
when the thing is burning.

1241
00:53:12,820 --> 00:53:15,040
But is there any
analysis or any appetite

1242
00:53:15,040 --> 00:53:17,570
for dealing with health
care at an early stage

1243
00:53:17,570 --> 00:53:22,278
so we're not dealing with such
a snowballing crisis effect?

1244
00:53:22,278 --> 00:53:23,320
Michael, do you want to--

1245
00:53:23,320 --> 00:53:26,650
I would say the
extent to which we've

1246
00:53:26,650 --> 00:53:28,790
been able to address
health care at birth

1247
00:53:28,790 --> 00:53:31,502
is really through
early disease testing.

1248
00:53:31,502 --> 00:53:32,960
So there's some
immune deficiencies

1249
00:53:32,960 --> 00:53:36,050
that are now standard
of care in all newborns

1250
00:53:36,050 --> 00:53:37,550
so that we can
diagnose those early

1251
00:53:37,550 --> 00:53:39,110
rather than wait
for those patients

1252
00:53:39,110 --> 00:53:42,860
to then develop some
immune-related disease, at which

1253
00:53:42,860 --> 00:53:45,210
point, it becomes
much more expensive.

1254
00:53:45,210 --> 00:53:48,180
Now, as genomics becomes
more personalized,

1255
00:53:48,180 --> 00:53:51,470
I think we will be in a better
position to at least risk assess

1256
00:53:51,470 --> 00:53:54,510
and decide who should get
a test and who shouldn't.

1257
00:53:54,510 --> 00:53:56,300
As you're probably
aware, we're all

1258
00:53:56,300 --> 00:53:59,520
in this dilemma about
Prostate-Specific Antigen, PSA.

1259
00:53:59,520 --> 00:54:00,330
Should we use it?

1260
00:54:00,330 --> 00:54:01,440
Should we not use it?

1261
00:54:01,440 --> 00:54:03,890
And it only works if the
prevalence of the disease

1262
00:54:03,890 --> 00:54:04,500
is high.

1263
00:54:04,500 --> 00:54:05,990
So I do think the
place this will

1264
00:54:05,990 --> 00:54:09,660
be helpful is once personal
genomics really catches on,

1265
00:54:09,660 --> 00:54:12,710
we should be able to risk
stratify and better decide what

1266
00:54:12,710 --> 00:54:14,220
tests people should get when.

1267
00:54:14,220 --> 00:54:18,170
But I can't tell you about
the economics of that.

1268
00:54:18,170 --> 00:54:21,440
Well, when we think about the
incentives of the repair shop

1269
00:54:21,440 --> 00:54:23,480
gets paid, there's going
to be a lot of focus

1270
00:54:23,480 --> 00:54:26,378
on repairing as opposed to if
the prevention shop gets paid,

1271
00:54:26,378 --> 00:54:27,920
there'll be more
focus on prevention.

1272
00:54:27,920 --> 00:54:31,390
And so the move
in the US has been

1273
00:54:31,390 --> 00:54:33,848
toward trying to put
more risk on providers

1274
00:54:33,848 --> 00:54:35,390
so that if they get
people healthier,

1275
00:54:35,390 --> 00:54:36,940
they would actually benefit
from that as opposed

1276
00:54:36,940 --> 00:54:38,802
to welcoming them
back through the door.

1277
00:54:38,802 --> 00:54:41,510
And that's a slow-moving process
because of all the stakeholders.

1278
00:54:41,510 --> 00:54:44,330
But it's, I think, slowly but
surely going that direction.

1279
00:54:44,330 --> 00:54:46,180
What do you think?

1280
00:54:46,180 --> 00:54:47,775
One of my colleagues,
Nathan Hendron,

1281
00:54:47,775 --> 00:54:50,150
is running something called
the Policy Opportunities Lab.

1282
00:54:50,150 --> 00:54:51,280
And what this lab--

1283
00:54:51,280 --> 00:54:52,790
Policy Impacts Lab, I'm sorry.

1284
00:54:52,790 --> 00:54:56,680
What this lab does is actually
quantify the social value

1285
00:54:56,680 --> 00:54:58,010
of different interventions.

1286
00:54:58,010 --> 00:54:59,468
And the ones that
are most valuable

1287
00:54:59,468 --> 00:55:02,380
are the ones on kids, things
that intervene at birth

1288
00:55:02,380 --> 00:55:04,460
and early age are incredibly
socially valuable.

1289
00:55:04,460 --> 00:55:06,130
And I think we do
need to be directing

1290
00:55:06,130 --> 00:55:07,750
our focus in that area.

1291
00:55:07,750 --> 00:55:09,920
Joe talked about some
research on low birth weight,

1292
00:55:09,920 --> 00:55:11,950
which does that.

1293
00:55:11,950 --> 00:55:13,610
I talked about cell
and gene therapies,

1294
00:55:13,610 --> 00:55:15,930
which are really for these
birth defect diseases.

1295
00:55:15,930 --> 00:55:18,430
I think we need to be shifting
our focus that way, for sure.

1296
00:55:18,430 --> 00:55:19,120
Yeah?

1297
00:55:19,120 --> 00:55:21,280
Building up on a
comment, building up

1298
00:55:21,280 --> 00:55:24,280
on the comment on the
stakeholders misalignment

1299
00:55:24,280 --> 00:55:26,900
and the reimbursement
model, we have

1300
00:55:26,900 --> 00:55:29,250
been talking about value-based
care for a long time now

1301
00:55:29,250 --> 00:55:31,260
and have not been able
really to accomplish,

1302
00:55:31,260 --> 00:55:34,560
at least in oncology,
very significant progress.

1303
00:55:34,560 --> 00:55:38,528
Where do you see is the best
way to try to accelerate that?

1304
00:55:38,528 --> 00:55:40,070
I'm sorry, I didn't
quite understand.

1305
00:55:40,070 --> 00:55:42,140
The value-based care model.

1306
00:55:42,140 --> 00:55:43,140
Oh, value-based payment.

1307
00:55:43,140 --> 00:55:45,470
Yeah, so basically, this is a
really interesting and important

1308
00:55:45,470 --> 00:55:45,970
issue.

1309
00:55:45,970 --> 00:55:47,690
Like I said, we've
got these drugs that

1310
00:55:47,690 --> 00:55:49,435
look incredibly good
in early trials,

1311
00:55:49,435 --> 00:55:51,060
but we're not sure
how well they'll do.

1312
00:55:51,060 --> 00:55:53,480
And I think that, actually,
the government has recently

1313
00:55:53,480 --> 00:55:55,860
put together an initiative
on this for sickle cell.

1314
00:55:55,860 --> 00:55:57,818
There's new genetic
treatments for sickle cell.

1315
00:55:57,818 --> 00:55:59,545
And the government
just today announced

1316
00:55:59,545 --> 00:56:00,920
that they've got
a new initiative

1317
00:56:00,920 --> 00:56:03,877
to work with states to set
up essentially a rebate

1318
00:56:03,877 --> 00:56:05,460
system where if the
drug doesn't work,

1319
00:56:05,460 --> 00:56:07,880
the drug companies will rebate
the money that the states paid

1320
00:56:07,880 --> 00:56:08,160
in.

1321
00:56:08,160 --> 00:56:09,993
So I think we're making
good progress there.

1322
00:56:09,993 --> 00:56:11,570
But I also think
it's been somewhat

1323
00:56:11,570 --> 00:56:15,000
of a failure of-- the
economist said, like I said,

1324
00:56:15,000 --> 00:56:17,528
just change the incentives,
and value-based payment

1325
00:56:17,528 --> 00:56:18,570
will take care of itself.

1326
00:56:18,570 --> 00:56:19,800
And it turns out
it's incredibly hard.

1327
00:56:19,800 --> 00:56:22,230
You have to put a lot of work
into it, and that takes time.

1328
00:56:22,230 --> 00:56:23,855
So it wasn't going
to happen overnight.

1329
00:56:23,855 --> 00:56:26,020
I don't think-- people
who knew this area knew

1330
00:56:26,020 --> 00:56:26,840
it wouldn't happen overnight.

1331
00:56:26,840 --> 00:56:29,215
But I did think it was going
to happen faster than it has

1332
00:56:29,215 --> 00:56:31,940
because I thought the providers
would know where the waste is

1333
00:56:31,940 --> 00:56:33,432
a bit more than the payers did.

1334
00:56:33,432 --> 00:56:35,390
And it turns out, we all
need to figure it out.

1335
00:56:35,390 --> 00:56:38,581
But changing the incentives is
the first step, not the last.

1336
00:56:38,581 --> 00:56:40,060
Last question.

1337
00:56:40,060 --> 00:56:43,870
Hey, sorry, this is kind
of a technical question,

1338
00:56:43,870 --> 00:56:45,620
and we've got a
broad panel today.

1339
00:56:45,620 --> 00:56:48,370
But still, I'd like to ask it.

1340
00:56:48,370 --> 00:56:51,430
The sobering statistic
that I've come across

1341
00:56:51,430 --> 00:56:54,590
in a couple of different places,
including with Michael's lab,

1342
00:56:54,590 --> 00:56:57,700
is that maybe only about
a third of patients

1343
00:56:57,700 --> 00:57:00,700
benefit from putting
all of our best data

1344
00:57:00,700 --> 00:57:02,600
together for
personalized medicine,

1345
00:57:02,600 --> 00:57:06,140
so in the context of the best
care we can bring to patients.

1346
00:57:06,140 --> 00:57:08,470
And as Michael
knows, I'm sort of

1347
00:57:08,470 --> 00:57:11,260
betting on the computational
biology AI racehorse

1348
00:57:11,260 --> 00:57:12,260
to really help us there.

1349
00:57:12,260 --> 00:57:14,420
But that's only a piece
of the picture, I think.

1350
00:57:14,420 --> 00:57:16,370
So from a collaboration
standpoint,

1351
00:57:16,370 --> 00:57:18,740
I just wonder if the rest
of you, and together,

1352
00:57:18,740 --> 00:57:21,840
Michael, just have some
initial ideas about how

1353
00:57:21,840 --> 00:57:25,780
we may make progress through
collaboration on this really,

1354
00:57:25,780 --> 00:57:29,520
again, very sobering fact
that all of our best efforts

1355
00:57:29,520 --> 00:57:31,260
with all this great
technology, it's

1356
00:57:31,260 --> 00:57:35,370
only helping a small portion
of our patients right now?

1357
00:57:35,370 --> 00:57:38,730
All the omics, proteomics,
genomics, all the rest of it

1358
00:57:38,730 --> 00:57:41,670
combined, it's not getting
us nearly as far as we

1359
00:57:41,670 --> 00:57:43,630
need to as fast as we want to.

1360
00:57:43,630 --> 00:57:46,170
So I'd like your
thoughts on that.

1361
00:57:46,170 --> 00:57:48,570
So I think part of the reason--

1362
00:57:48,570 --> 00:57:52,140
so what Scott Ritter Bush is
talking about is the fact that

1363
00:57:52,140 --> 00:57:55,140
at least using genomics,
right-- genomics hasn't really--

1364
00:57:55,140 --> 00:57:57,090
only 10% or so of
cancer patients

1365
00:57:57,090 --> 00:57:59,620
really benefit from genomic
analysis of their tumor.

1366
00:57:59,620 --> 00:58:02,850
Only 10% or less actually
have a change in therapy

1367
00:58:02,850 --> 00:58:05,200
that results in improvement.

1368
00:58:05,200 --> 00:58:07,170
And part of that, though,
is because we've been

1369
00:58:07,170 --> 00:58:09,340
so heavily focused on genomics.

1370
00:58:09,340 --> 00:58:13,450
So I do think just integrating
in in a patient-specific manner

1371
00:58:13,450 --> 00:58:17,110
other types of omic analysis,
looking at metabolism,

1372
00:58:17,110 --> 00:58:20,590
looking at immune infiltration,
looking at proteomics-- now,

1373
00:58:20,590 --> 00:58:22,420
that's harder to do
because we have not

1374
00:58:22,420 --> 00:58:25,240
evolved the technology
that allows us to do this

1375
00:58:25,240 --> 00:58:27,820
in a rapid, high-throughput
way on patient samples

1376
00:58:27,820 --> 00:58:29,930
like we can do with genomics.

1377
00:58:29,930 --> 00:58:32,120
That's exactly the kind
of thing we can do at MIT.

1378
00:58:32,120 --> 00:58:34,640
But the other part of it is
we can't do it in isolation.

1379
00:58:34,640 --> 00:58:36,910
We have to do this
together with looking

1380
00:58:36,910 --> 00:58:42,800
at patient historical records,
risk assessment, family history.

1381
00:58:42,800 --> 00:58:44,920
And that's even
harder to incorporate

1382
00:58:44,920 --> 00:58:46,960
into that type of thing.

1383
00:58:46,960 --> 00:58:49,790
Yeah, I would just add-- it's a
great question, great comment.

1384
00:58:49,790 --> 00:58:53,170
I would just add,
as a society, we

1385
00:58:53,170 --> 00:58:55,370
play a role in directing
technological advance.

1386
00:58:55,370 --> 00:58:57,850
And I think we need to be
thinking about directing

1387
00:58:57,850 --> 00:59:00,460
technological advance to
things which are helping

1388
00:59:00,460 --> 00:59:02,890
a narrow slice of people
very well versus a broader

1389
00:59:02,890 --> 00:59:06,440
set of people maybe through
birth improvements less well.

1390
00:59:06,440 --> 00:59:08,590
And that's the kind of
trade-off that economists

1391
00:59:08,590 --> 00:59:10,730
love to think about and
that we'll be working on.

1392
00:59:10,730 --> 00:59:11,990
So I think we just stop there.

1393
00:59:11,990 --> 00:59:14,157
Thank you very much for
your comments and questions.

1394
00:59:14,157 --> 00:59:16,890
[APPLAUSE]

1395
00:59:16,890 --> 00:59:19,000

