1
00:00:05,600 --> 00:00:10,719
Hello again. I'm Vicki. I'm a first year

2
00:00:08,800 --> 00:00:13,120
posttock at the Schmidt Center. And

3
00:00:10,719 --> 00:00:15,839
today I have the pleasure of introducing

4
00:00:13,120 --> 00:00:17,600
our next speaker, Maria Barbettich.

5
00:00:15,839 --> 00:00:19,359
Maria is an assistant professor in

6
00:00:17,600 --> 00:00:21,240
computer science and life science at the

7
00:00:19,359 --> 00:00:23,600
Swiss Federal Institute of Technology,

8
00:00:21,240 --> 00:00:26,320
EPFL, and the head of the machine

9
00:00:23,600 --> 00:00:28,400
learning for biio medicine lab. Before

10
00:00:26,320 --> 00:00:30,240
taking on these impressive roles, she

11
00:00:28,400 --> 00:00:33,079
did her postto in Stanford and received

12
00:00:30,240 --> 00:00:35,440
her PhD from the University of Zagreb in

13
00:00:33,079 --> 00:00:38,000
Croatia. She has received numerous

14
00:00:35,440 --> 00:00:40,800
scholarships and awards already and most

15
00:00:38,000 --> 00:00:43,840
recently the prestigious SNSF starting

16
00:00:40,800 --> 00:00:48,480
grant. Maria's research is a wonderful

17
00:00:43,840 --> 00:00:51,200
example um of uh the what we do here at

18
00:00:48,480 --> 00:00:52,800
the symposium. Um her lab works on

19
00:00:51,200 --> 00:00:55,480
cutting edge machine learning methods

20
00:00:52,800 --> 00:00:57,840
informed by biological problems and is

21
00:00:55,480 --> 00:00:59,840
contributing impressive advancements in

22
00:00:57,840 --> 00:01:02,800
multimodal generative modeling for

23
00:00:59,840 --> 00:01:05,600
cellular bi biology. I'm very excited,

24
00:01:02,800 --> 00:01:07,920
Maria, to hear your talk and the floor

25
00:01:05,600 --> 00:01:07,920
is all

26
00:01:12,280 --> 00:01:17,040
yours. Okay. Thank you so much Victoria

27
00:01:15,360 --> 00:01:19,759
for a nice introduction. Thank you also

28
00:01:17,040 --> 00:01:21,840
Caroline for inviting me. and uh super

29
00:01:19,759 --> 00:01:23,880
excited to be here and be part of these

30
00:01:21,840 --> 00:01:26,720
exciting two days and this

31
00:01:23,880 --> 00:01:28,960
workshop. Um so today I'll tell you

32
00:01:26,720 --> 00:01:32,159
about how can generative AI help us to

33
00:01:28,960 --> 00:01:34,320
unlock the complexity of cells. So we

34
00:01:32,159 --> 00:01:36,400
are in the midst of genomics revolution

35
00:01:34,320 --> 00:01:38,320
that is transforming uh our

36
00:01:36,400 --> 00:01:40,560
understanding of biology at a cellular

37
00:01:38,320 --> 00:01:43,040
level with the advances of single cell

38
00:01:40,560 --> 00:01:45,759
RNA sequencing technologies. We can now

39
00:01:43,040 --> 00:01:47,680
imagine expressions in hundreds uh or

40
00:01:45,759 --> 00:01:49,840
even millions of cells in a single

41
00:01:47,680 --> 00:01:52,159
experiment leading to the creation of

42
00:01:49,840 --> 00:01:54,000
these large scale salatless data sets

43
00:01:52,159 --> 00:01:56,520
that offer the opportunity to

44
00:01:54,000 --> 00:01:58,560
characterize ourselves at unprecedented

45
00:01:56,520 --> 00:02:00,320
resolution. And not only that we can

46
00:01:58,560 --> 00:02:03,920
measure cell transcriptto we can also

47
00:02:00,320 --> 00:02:05,840
measure proteins epiggenome pro phenome

48
00:02:03,920 --> 00:02:07,759
and with multiomics technologies we can

49
00:02:05,840 --> 00:02:10,239
even do multiple measurements in the

50
00:02:07,759 --> 00:02:12,720
same time at the same cells and very

51
00:02:10,239 --> 00:02:14,319
latest spatial technologies enable us to

52
00:02:12,720 --> 00:02:16,480
study cells in their native tissue

53
00:02:14,319 --> 00:02:18,680
context while at the same time capturing

54
00:02:16,480 --> 00:02:20,959
the gene expressions of the

55
00:02:18,680 --> 00:02:23,040
cells. At the same time we are also in

56
00:02:20,959 --> 00:02:24,879
the midst of AI revolution. So with the

57
00:02:23,040 --> 00:02:26,640
rise of generative AI paradigm, we've

58
00:02:24,879 --> 00:02:31,440
seen completely new capabilities of AI

59
00:02:26,640 --> 00:02:33,040
system um that um that um kind of offer

60
00:02:31,440 --> 00:02:35,920
completely new capabilities that were

61
00:02:33,040 --> 00:02:37,840
not possible before. Uh and this brings

62
00:02:35,920 --> 00:02:39,599
us all to the era of foundation models

63
00:02:37,840 --> 00:02:42,080
where kind of most famous example would

64
00:02:39,599 --> 00:02:43,840
be large language models such as CH GPT

65
00:02:42,080 --> 00:02:46,400
that are pre-trained on internet scale

66
00:02:43,840 --> 00:02:48,480
data to predict next token in a sentence

67
00:02:46,400 --> 00:02:51,280
and then can be applied to a variety of

68
00:02:48,480 --> 00:02:53,680
downstream tasks.

69
00:02:51,280 --> 00:02:56,319
But the question is how can AI help us

70
00:02:53,680 --> 00:02:58,400
to advance our understanding of biology?

71
00:02:56,319 --> 00:03:00,319
How can really it brings us closer to

72
00:02:58,400 --> 00:03:02,159
understanding ourselves, how their

73
00:03:00,319 --> 00:03:05,280
function and what goes wrong on a

74
00:03:02,159 --> 00:03:07,280
cellular level in disease state. And our

75
00:03:05,280 --> 00:03:10,159
goal is ideally is not just to make

76
00:03:07,280 --> 00:03:12,239
predictions but really to use AI to help

77
00:03:10,159 --> 00:03:14,159
us to discover new biological

78
00:03:12,239 --> 00:03:15,920
mechanisms.

79
00:03:14,159 --> 00:03:17,840
And one of the kind of big challenges

80
00:03:15,920 --> 00:03:19,599
when you think about biological data

81
00:03:17,840 --> 00:03:22,319
compared to for example computer vision

82
00:03:19,599 --> 00:03:25,000
or natural language processing is that

83
00:03:22,319 --> 00:03:27,440
biological data is inherently

84
00:03:25,000 --> 00:03:30,120
heterogeneous. So we collect these data

85
00:03:27,440 --> 00:03:32,959
sets from different tissues, different

86
00:03:30,120 --> 00:03:36,280
donors, different time points, different

87
00:03:32,959 --> 00:03:38,720
species and also we measure different

88
00:03:36,280 --> 00:03:41,440
modalities. And the ultimate goal is

89
00:03:38,720 --> 00:03:43,599
really to develop an integr integrative

90
00:03:41,440 --> 00:03:46,080
holistic understanding of biology that

91
00:03:43,599 --> 00:03:48,400
captures the complexity of life across

92
00:03:46,080 --> 00:03:50,640
different scales. and artificial

93
00:03:48,400 --> 00:03:53,440
intelligent technologies really kind of

94
00:03:50,640 --> 00:03:55,680
hold this promise to help us to unify

95
00:03:53,440 --> 00:03:58,480
these kind of fragmented views and to

96
00:03:55,680 --> 00:04:01,080
discover generalizable patterns across

97
00:03:58,480 --> 00:04:03,280
these different biological

98
00:04:01,080 --> 00:04:05,120
contexts. And in our prior work, we

99
00:04:03,280 --> 00:04:07,040
tackled individually some of these kind

100
00:04:05,120 --> 00:04:09,280
of challenges. For example, how can we

101
00:04:07,040 --> 00:04:11,439
transfer learn across different tissues,

102
00:04:09,280 --> 00:04:13,760
different donors? How can we integrate

103
00:04:11,439 --> 00:04:15,280
data sets across different species? How

104
00:04:13,760 --> 00:04:17,040
can we analyze data sets across

105
00:04:15,280 --> 00:04:18,840
different time points to kind of mention

106
00:04:17,040 --> 00:04:21,519
few

107
00:04:18,840 --> 00:04:23,759
examples but in this talk what I want to

108
00:04:21,519 --> 00:04:26,400
focus on is more kind of this aspect of

109
00:04:23,759 --> 00:04:28,400
different modalities in particular uh

110
00:04:26,400 --> 00:04:29,840
combining kind of information from the

111
00:04:28,400 --> 00:04:32,400
transcript of the cells and their

112
00:04:29,840 --> 00:04:34,479
spatial organization. So how can AI help

113
00:04:32,400 --> 00:04:36,560
us to learn the relationship between the

114
00:04:34,479 --> 00:04:39,040
spatial organization of the cells and

115
00:04:36,560 --> 00:04:40,639
their gene expressions.

116
00:04:39,040 --> 00:04:42,639
And in the second part of the talk, I

117
00:04:40,639 --> 00:04:44,160
want to kind of briefly uh touch upon

118
00:04:42,639 --> 00:04:46,240
how can we leverage kind of these big

119
00:04:44,160 --> 00:04:48,800
foundation models that we have nowadays

120
00:04:46,240 --> 00:04:50,560
to help for unsupervised inference and

121
00:04:48,800 --> 00:04:52,880
kind of to help us to discover what

122
00:04:50,560 --> 00:04:54,560
classes automatically exist in the data.

123
00:04:52,880 --> 00:04:58,960
For example, what kind of cell types we

124
00:04:54,560 --> 00:05:01,759
have um in in our data set.

125
00:04:58,960 --> 00:05:03,759
So while single cell RNA sequencing

126
00:05:01,759 --> 00:05:06,479
allows us to kind of study our cells at

127
00:05:03,759 --> 00:05:08,320
unprecedented resolution, the

128
00:05:06,479 --> 00:05:10,639
fundamental problem and challenge is

129
00:05:08,320 --> 00:05:12,600
that cells are dissociated from tissue

130
00:05:10,639 --> 00:05:14,639
and we lose the spatial context of the

131
00:05:12,600 --> 00:05:17,600
cells. On the other hand, we have

132
00:05:14,639 --> 00:05:20,560
spatial transcries that kind of capture

133
00:05:17,600 --> 00:05:22,000
this native native organ that capture

134
00:05:20,560 --> 00:05:24,160
organization of the cells in their

135
00:05:22,000 --> 00:05:27,919
native tissue context. But the number of

136
00:05:24,160 --> 00:05:30,560
genes that we can capture is limited.

137
00:05:27,919 --> 00:05:33,120
But can we overcome the limitations of

138
00:05:30,560 --> 00:05:35,039
these uh technologies with computational

139
00:05:33,120 --> 00:05:37,120
approaches and kind of can we get the

140
00:05:35,039 --> 00:05:39,280
best of both worlds to really obtain a

141
00:05:37,120 --> 00:05:41,800
more complete pictures of how tissue

142
00:05:39,280 --> 00:05:44,160
function and understand cell

143
00:05:41,800 --> 00:05:45,919
interactions. So ideally we would like

144
00:05:44,160 --> 00:05:47,600
to have kind of spatial context of the

145
00:05:45,919 --> 00:05:50,639
cells captured as well as like a full

146
00:05:47,600 --> 00:05:52,880
transatomic information.

147
00:05:50,639 --> 00:05:56,039
So we started this project with a

148
00:05:52,880 --> 00:05:58,639
question and idea how well or can we

149
00:05:56,039 --> 00:06:00,960
reconstruct complex tissue structures

150
00:05:58,639 --> 00:06:03,840
denovo. So if someone gives me

151
00:06:00,960 --> 00:06:05,919
dissociated single cells can I be able

152
00:06:03,840 --> 00:06:08,000
to predict the kind of complex tissue

153
00:06:05,919 --> 00:06:09,639
organization? Can I predict how cells

154
00:06:08,000 --> 00:06:12,479
get organized in the

155
00:06:09,639 --> 00:06:14,720
space? And we call this process of going

156
00:06:12,479 --> 00:06:17,840
from dissociated single cells to kind of

157
00:06:14,720 --> 00:06:19,919
these tissue structures uh the tissue

158
00:06:17,840 --> 00:06:22,080
reassembly because kind of we want to

159
00:06:19,919 --> 00:06:24,240
reassemble tissues from these kind of

160
00:06:22,080 --> 00:06:28,199
dissociated cells and predict how they

161
00:06:24,240 --> 00:06:30,319
get how uh cells get spatially

162
00:06:28,199 --> 00:06:32,479
organized and conceptually you can

163
00:06:30,319 --> 00:06:34,720
imagine this as kind of alpha fold for

164
00:06:32,479 --> 00:06:36,000
cells where in alpha fold we go from the

165
00:06:34,720 --> 00:06:38,160
protein sequence to the protein

166
00:06:36,000 --> 00:06:39,680
structure. So here what we want to do is

167
00:06:38,160 --> 00:06:42,000
we want to go from the expressions of

168
00:06:39,680 --> 00:06:44,199
dissociated single cells to these kind

169
00:06:42,000 --> 00:06:46,400
of uh tissue

170
00:06:44,199 --> 00:06:49,199
structures. But the question is of

171
00:06:46,400 --> 00:06:50,880
course to what extent and uh is this uh

172
00:06:49,199 --> 00:06:52,720
tissue reassembly possible and of course

173
00:06:50,880 --> 00:06:54,319
the answer will probably defend will

174
00:06:52,720 --> 00:06:57,199
also depend on different biological

175
00:06:54,319 --> 00:06:58,599
systems. But given kind of that if you

176
00:06:57,199 --> 00:07:00,680
think about you know vast cellular

177
00:06:58,599 --> 00:07:02,880
heterogeneity, complexity of tissue

178
00:07:00,680 --> 00:07:05,039
structures, intricate cell cell

179
00:07:02,880 --> 00:07:08,080
interactions and also elusive biological

180
00:07:05,039 --> 00:07:10,880
mechanisms that underly tissue formation

181
00:07:08,080 --> 00:07:12,319
and if it is indeed possible, how can we

182
00:07:10,880 --> 00:07:15,000
learn it? How can we learn these

183
00:07:12,319 --> 00:07:17,680
underlying rules of complex spatial

184
00:07:15,000 --> 00:07:19,199
organization? So the good news is that

185
00:07:17,680 --> 00:07:22,880
uh we have like bunch of existing

186
00:07:19,199 --> 00:07:26,880
spatial transatomics data sets. uh so we

187
00:07:22,880 --> 00:07:26,880
could leverage these existing spatial

188
00:07:30,120 --> 00:07:35,080
transcri proper spatial priors of uh

189
00:07:33,280 --> 00:07:38,000
tissue

190
00:07:35,080 --> 00:07:40,080
organization. So in order to do so what

191
00:07:38,000 --> 00:07:42,960
we need to do is we need to capture the

192
00:07:40,080 --> 00:07:47,440
cellular interactions locally as well as

193
00:07:42,960 --> 00:07:49,360
globally across the entire slice.

194
00:07:47,440 --> 00:07:51,840
So in particular what we need to learn

195
00:07:49,360 --> 00:07:55,120
in or order to capture these um cell

196
00:07:51,840 --> 00:07:57,440
cell interactions is how each cell

197
00:07:55,120 --> 00:07:59,720
should kind of attend to other cells

198
00:07:57,440 --> 00:08:02,000
based on its own location and the gene

199
00:07:59,720 --> 00:08:03,840
expression. Uh and we can kind of in

200
00:08:02,000 --> 00:08:06,080
machine learning language the way we

201
00:08:03,840 --> 00:08:08,160
could kind of capture this is using kind

202
00:08:06,080 --> 00:08:10,720
of well-known principle which is a self

203
00:08:08,160 --> 00:08:14,639
attention mechanism. So basically what

204
00:08:10,720 --> 00:08:17,680
we can do is we can learn how each cell

205
00:08:14,639 --> 00:08:20,000
should attend to other cell based on the

206
00:08:17,680 --> 00:08:23,199
relevance of other cells to a cell's own

207
00:08:20,000 --> 00:08:25,120
location and its own gene expressions.

208
00:08:23,199 --> 00:08:28,160
So in this way kind of we can enable

209
00:08:25,120 --> 00:08:31,199
each cell to learn how to communicate to

210
00:08:28,160 --> 00:08:34,039
other cells guided by the attention

211
00:08:31,199 --> 00:08:37,680
weights or attention

212
00:08:34,039 --> 00:08:39,760
scores. And the secondly we one we model

213
00:08:37,680 --> 00:08:41,919
this kind of tissue reassembly process

214
00:08:39,760 --> 00:08:44,159
as a generative process. So in

215
00:08:41,919 --> 00:08:45,760
particular we model it as a conditional

216
00:08:44,159 --> 00:08:48,240
point cloud generation where the

217
00:08:45,760 --> 00:08:50,000
conditioning is applied uh on each

218
00:08:48,240 --> 00:08:52,040
individual point using the gene

219
00:08:50,000 --> 00:08:55,600
expressions of the

220
00:08:52,040 --> 00:08:57,200
cells. So during inference ideally and

221
00:08:55,600 --> 00:08:58,880
Joey kind of gave a really nice

222
00:08:57,200 --> 00:09:01,040
introduction to the diffusion model. So

223
00:08:58,880 --> 00:09:03,920
it was really um a good introduction

224
00:09:01,040 --> 00:09:05,519
before this talk. uh but ideally then

225
00:09:03,920 --> 00:09:08,640
you know during the inference what we

226
00:09:05,519 --> 00:09:10,480
want to do is we want to sample noise uh

227
00:09:08,640 --> 00:09:12,560
and then these noisy cell coordinates

228
00:09:10,480 --> 00:09:14,720
and I want want to condition these noisy

229
00:09:12,560 --> 00:09:16,880
cell coordinates on the gene expressions

230
00:09:14,720 --> 00:09:19,440
of the cells and then with a number of

231
00:09:16,880 --> 00:09:22,440
steps I want to be able to kind of go to

232
00:09:19,440 --> 00:09:25,519
reconstruct these um tissue

233
00:09:22,440 --> 00:09:27,600
structures and these two ideas to kind

234
00:09:25,519 --> 00:09:29,760
of first model cell interactions via

235
00:09:27,600 --> 00:09:32,000
attention mechanism and secondly the

236
00:09:29,760 --> 00:09:34,399
model tissue reassembly process as

237
00:09:32,000 --> 00:09:36,640
generative process uh bring us to the

238
00:09:34,399 --> 00:09:38,959
framework that we call Luna and this was

239
00:09:36,640 --> 00:09:42,000
work done by my amazing PhD student

240
00:09:38,959 --> 00:09:45,880
yeast and master student Shanaka uh who

241
00:09:42,000 --> 00:09:51,120
is now PhD student actually here at

242
00:09:45,880 --> 00:09:52,959
MIT. Uh so what Luna does is uh like

243
00:09:51,120 --> 00:09:54,959
Luna leverages this existing spatial

244
00:09:52,959 --> 00:09:57,760
transatomics data sets with ground truth

245
00:09:54,959 --> 00:09:59,760
cell locations uh and as a diffusion

246
00:09:57,760 --> 00:10:01,920
model it consists of two main steps. So

247
00:09:59,760 --> 00:10:03,600
first step is corruption step. We are

248
00:10:01,920 --> 00:10:05,200
going to add noise to the ground true

249
00:10:03,600 --> 00:10:07,120
cell location. We are going to sample

250
00:10:05,200 --> 00:10:08,640
diffusion time and then we are going

251
00:10:07,120 --> 00:10:11,800
going from this ground truth cell

252
00:10:08,640 --> 00:10:13,600
locations to this kind of noise cell

253
00:10:11,800 --> 00:10:15,519
coordinates and then during the

254
00:10:13,600 --> 00:10:18,000
denoising process the model will learn

255
00:10:15,519 --> 00:10:20,000
to recover the original cell coordinates

256
00:10:18,000 --> 00:10:22,560
through the denoising process. So

257
00:10:20,000 --> 00:10:24,560
basically we are going to learn these uh

258
00:10:22,560 --> 00:10:26,800
cell embeddings using the tension

259
00:10:24,560 --> 00:10:28,560
mechanism and then from these output

260
00:10:26,800 --> 00:10:30,480
cell embeddings we just add in one fully

261
00:10:28,560 --> 00:10:32,880
connected neural network layer to go to

262
00:10:30,480 --> 00:10:35,440
these uh predicted physical locations

263
00:10:32,880 --> 00:10:37,600
predicted cell coordinates and then

264
00:10:35,440 --> 00:10:42,640
during a training the model needs to

265
00:10:37,600 --> 00:10:42,640
learn to den noiseise um the uh cell

266
00:10:43,240 --> 00:10:47,200
coordinates. Another challenge is

267
00:10:45,279 --> 00:10:48,800
basically that uh when you think about

268
00:10:47,200 --> 00:10:50,399
this data set that the spatial

269
00:10:48,800 --> 00:10:52,320
arrangements of the cells in a tissue

270
00:10:50,399 --> 00:10:55,040
actually it may undergo transformations

271
00:10:52,320 --> 00:10:56,800
due to an experimental artifacts while

272
00:10:55,040 --> 00:10:59,519
for example tissue slices can be

273
00:10:56,800 --> 00:11:01,480
arbitrarily rotated and reflected while

274
00:10:59,519 --> 00:11:04,560
gene expressions actually remain

275
00:11:01,480 --> 00:11:06,320
unchange. So so this means that we want

276
00:11:04,560 --> 00:11:08,160
to basically design the objective

277
00:11:06,320 --> 00:11:10,480
function that is invariant to these

278
00:11:08,160 --> 00:11:12,480
rotations and reflection. So this means

279
00:11:10,480 --> 00:11:14,160
that we want to basically be able we are

280
00:11:12,480 --> 00:11:15,920
not care really about the absolute

281
00:11:14,160 --> 00:11:18,399
positions of the cells but what we care

282
00:11:15,920 --> 00:11:20,560
about is about the relative spatial rel

283
00:11:18,399 --> 00:11:22,880
relationships of the cells. So we design

284
00:11:20,560 --> 00:11:24,959
an objective function that kind of is uh

285
00:11:22,880 --> 00:11:28,680
is based on this pair wise distances of

286
00:11:24,959 --> 00:11:30,959
the cells rather on their absolute

287
00:11:28,680 --> 00:11:32,720
positions and then during the inference

288
00:11:30,959 --> 00:11:34,320
stage we sample from the standard normal

289
00:11:32,720 --> 00:11:36,399
distribution and then with a number of

290
00:11:34,320 --> 00:11:39,800
diffusion steps eventually ideally

291
00:11:36,399 --> 00:11:42,399
converge to the ground truth uh cell

292
00:11:39,800 --> 00:11:44,480
locations. Uh Luna has also linear time

293
00:11:42,399 --> 00:11:46,720
and uh memory complexity during the

294
00:11:44,480 --> 00:11:48,880
inference uh because we use attention

295
00:11:46,720 --> 00:11:51,360
with the linear complexity. Uh so this

296
00:11:48,880 --> 00:11:53,839
means is like a highly scalable. It can

297
00:11:51,360 --> 00:11:56,079
um it can infer locations of tens of

298
00:11:53,839 --> 00:11:57,959
thousands of cells in just a few minutes

299
00:11:56,079 --> 00:12:00,800
um on a single

300
00:11:57,959 --> 00:12:03,440
GPU. And next I'll show some of uh some

301
00:12:00,800 --> 00:12:05,200
results um of uh applying Luna. So first

302
00:12:03,440 --> 00:12:07,200
we applied it on the Murfish mouse brain

303
00:12:05,200 --> 00:12:08,800
atlas where actually we have the ground

304
00:12:07,200 --> 00:12:12,079
truth cell location. So we trained the

305
00:12:08,800 --> 00:12:13,680
Luna on the 2.8 million cells from one

306
00:12:12,079 --> 00:12:15,200
mouse and then we were interested

307
00:12:13,680 --> 00:12:18,079
whether we can kind of reassemble the

308
00:12:15,200 --> 00:12:20,680
brain of another um mouse. So we applied

309
00:12:18,079 --> 00:12:23,920
it to 1.2 million cells from another

310
00:12:20,680 --> 00:12:25,680
mouse and here results of uh on the left

311
00:12:23,920 --> 00:12:28,720
I show the ground truth locations on the

312
00:12:25,680 --> 00:12:29,920
right are Luna on the right are ground

313
00:12:28,720 --> 00:12:32,240
locations on the left are Luna's

314
00:12:29,920 --> 00:12:35,680
predictions and this is how for one

315
00:12:32,240 --> 00:12:37,279
slice of isocortex region and here is

316
00:12:35,680 --> 00:12:38,959
how the diffusion process happens. So

317
00:12:37,279 --> 00:12:41,760
with the number of diffusion step model

318
00:12:38,959 --> 00:12:43,440
eventually converges um to the locations

319
00:12:41,760 --> 00:12:46,079
that really highly resemble the ground

320
00:12:43,440 --> 00:12:48,040
truth locations. And here uh cells are

321
00:12:46,079 --> 00:12:51,839
colored based on about

322
00:12:48,040 --> 00:12:53,839
338 different uh uh subtypes that are

323
00:12:51,839 --> 00:12:55,360
annotated in the data set. But we also

324
00:12:53,839 --> 00:12:57,279
see high accuracy if you look at the

325
00:12:55,360 --> 00:12:59,040
more finer grain annotations over a

326
00:12:57,279 --> 00:13:01,240
thousand types and also cell cluster

327
00:12:59,040 --> 00:13:03,839
level which has over 5,000

328
00:13:01,240 --> 00:13:06,079
types. And overall overall Luna's

329
00:13:03,839 --> 00:13:09,120
predictions agree well across 11 major

330
00:13:06,079 --> 00:13:13,160
regions that identified in the ABC atlas

331
00:13:09,120 --> 00:13:15,519
despite their um distinct structural

332
00:13:13,160 --> 00:13:18,320
characteristics. If you look within a

333
00:13:15,519 --> 00:13:19,760
particular um cell subtype so for

334
00:13:18,320 --> 00:13:22,160
example if you look at this is just an

335
00:13:19,760 --> 00:13:23,920
example of glutathic neurons. We further

336
00:13:22,160 --> 00:13:25,600
look whether basically Luna is able to

337
00:13:23,920 --> 00:13:27,760
infer this kind of spatial distribution

338
00:13:25,600 --> 00:13:29,440
of gene expressions correctly. So these

339
00:13:27,760 --> 00:13:35,279
are example of two spatially variable

340
00:13:29,440 --> 00:13:37,279
genes RX FP FP1 and RB1 gene RB gene uh

341
00:13:35,279 --> 00:13:39,200
where here uh on the top are ground

342
00:13:37,279 --> 00:13:43,279
truth locations on the bottom Luna's

343
00:13:39,200 --> 00:13:45,200
predictions uh and this RXF FP1 gene is

344
00:13:43,279 --> 00:13:47,440
actually uh highly expressed in this uh

345
00:13:45,200 --> 00:13:50,880
lower part of the region and here uh a

346
00:13:47,440 --> 00:13:52,560
bit um mildly expressed uh here and this

347
00:13:50,880 --> 00:13:53,920
is actually correctly captured by the

348
00:13:52,560 --> 00:13:57,760
model as well as also spatial

349
00:13:53,920 --> 00:13:59,440
distribution of the RRB B gene.

350
00:13:57,760 --> 00:14:01,040
This is also case for some of the

351
00:13:59,440 --> 00:14:03,279
non-neuronal types. So this is an

352
00:14:01,040 --> 00:14:05,279
example of appendimal asteroite

353
00:14:03,279 --> 00:14:08,240
appendimal cells where there's a small

354
00:14:05,279 --> 00:14:10,639
group of cells expressing the CF AP 206

355
00:14:08,240 --> 00:14:12,519
gene which is actually also correctly

356
00:14:10,639 --> 00:14:14,720
captured by the

357
00:14:12,519 --> 00:14:16,320
model. And given the kind of high

358
00:14:14,720 --> 00:14:18,000
similarity of the two brains, we were

359
00:14:16,320 --> 00:14:20,399
also next interested to see whether

360
00:14:18,000 --> 00:14:22,639
model could generalize if we for example

361
00:14:20,399 --> 00:14:26,639
exclude certain major cell class during

362
00:14:22,639 --> 00:14:29,440
training. So we excluded um one uh there

363
00:14:26,639 --> 00:14:31,760
we uh went to make the ch task most

364
00:14:29,440 --> 00:14:33,440
challenging we actually excluded uh we

365
00:14:31,760 --> 00:14:37,040
went to the most coarse grained

366
00:14:33,440 --> 00:14:40,120
annotation. So on 30 uh there are 34

367
00:14:37,040 --> 00:14:43,600
different major classes defined in ABC

368
00:14:40,120 --> 00:14:45,440
atlas. Um and here basically we during

369
00:14:43,600 --> 00:14:47,839
the training we train on all cells from

370
00:14:45,440 --> 00:14:50,000
animal one but we completely exclude. So

371
00:14:47,839 --> 00:14:53,199
here is just an example of glutamatergic

372
00:14:50,000 --> 00:14:55,120
neuron cells in layer 6b. uh and then we

373
00:14:53,199 --> 00:14:57,519
were interested to see whether model can

374
00:14:55,120 --> 00:14:59,600
still be able to correctly predict where

375
00:14:57,519 --> 00:15:01,519
to locate these cells in another animal

376
00:14:59,600 --> 00:15:04,639
even though this cell class has not been

377
00:15:01,519 --> 00:15:06,320
seen during training.

378
00:15:04,639 --> 00:15:09,199
Um indeed we found that the model

379
00:15:06,320 --> 00:15:10,959
correctly presositioned this unseen uh

380
00:15:09,199 --> 00:15:12,639
cell class in the spatial architecture

381
00:15:10,959 --> 00:15:14,800
of the tissue. And again if you look at

382
00:15:12,639 --> 00:15:19,360
the specially variable genes for example

383
00:15:14,800 --> 00:15:21,839
TS HZ2 gene and SYT6 gene um it was able

384
00:15:19,360 --> 00:15:23,519
to correctly capture the spatial

385
00:15:21,839 --> 00:15:25,560
distribution of this gene within a

386
00:15:23,519 --> 00:15:27,839
certain within this cell

387
00:15:25,560 --> 00:15:29,279
class. Uh this is also the case for

388
00:15:27,839 --> 00:15:31,199
other cell classes. So this is just an

389
00:15:29,279 --> 00:15:33,680
example of another cell class which was

390
00:15:31,199 --> 00:15:34,959
the excluded vascular class. uh which

391
00:15:33,680 --> 00:15:36,639
has also very different distribution

392
00:15:34,959 --> 00:15:38,160
across different slices and was actually

393
00:15:36,639 --> 00:15:41,120
correctly captured by the model as well

394
00:15:38,160 --> 00:15:42,880
as uh the expression of the GFAP uh gene

395
00:15:41,120 --> 00:15:45,079
which is specially variable gene for the

396
00:15:42,880 --> 00:15:48,480
vascular

397
00:15:45,079 --> 00:15:50,399
class. So Luna is uh unique in terms of

398
00:15:48,480 --> 00:15:53,920
kind of learning these spatial priors

399
00:15:50,399 --> 00:15:55,759
across multiple slices. Uh but one could

400
00:15:53,920 --> 00:15:58,000
potentially apply some of the existing

401
00:15:55,759 --> 00:15:59,759
methods to this task. Uh for example,

402
00:15:58,000 --> 00:16:01,920
the most closest methods would be

403
00:15:59,759 --> 00:16:04,800
methods such as tanggram. Uh so these

404
00:16:01,920 --> 00:16:06,720
are methods that are uh that uh map

405
00:16:04,800 --> 00:16:08,959
dissociated cells to a reference data

406
00:16:06,720 --> 00:16:11,040
set but the limitation of these methods

407
00:16:08,959 --> 00:16:13,279
is that you have to have exact spatial

408
00:16:11,040 --> 00:16:16,800
reference to map your dissociated single

409
00:16:13,279 --> 00:16:19,040
cells uh to um to uh your data set.

410
00:16:16,800 --> 00:16:21,120
Another class of methods are methods

411
00:16:19,040 --> 00:16:22,959
such as nova spark where do kind of also

412
00:16:21,120 --> 00:16:25,600
this kind of denovo reconstruction but

413
00:16:22,959 --> 00:16:27,920
they are based on the assumption that if

414
00:16:25,600 --> 00:16:29,360
two cells have similar gene expressions

415
00:16:27,920 --> 00:16:31,600
they should also be specially close

416
00:16:29,360 --> 00:16:33,680
together close to each other which is

417
00:16:31,600 --> 00:16:35,279
not satisfied in complex tissues and

418
00:16:33,680 --> 00:16:37,360
actually the core idea in Luna is that

419
00:16:35,279 --> 00:16:39,079
we try to learn these spatial priors

420
00:16:37,360 --> 00:16:41,680
using AI

421
00:16:39,079 --> 00:16:43,360
methods. So we compared Luna on this

422
00:16:41,680 --> 00:16:44,639
kind of smaller mouse cortex data set

423
00:16:43,360 --> 00:16:47,920
because some of these methods are also

424
00:16:44,639 --> 00:16:50,000
not very scalable. Um and uh if you

425
00:16:47,920 --> 00:16:52,079
don't have any prior information put

426
00:16:50,000 --> 00:16:53,759
into these models, Aluna is actually the

427
00:16:52,079 --> 00:16:55,759
only method that is able to correctly

428
00:16:53,759 --> 00:16:57,279
infer the spatial priors of the tissue.

429
00:16:55,759 --> 00:16:59,759
Here the performance is measured as

430
00:16:57,279 --> 00:17:02,079
spermman's rank correlation where we uh

431
00:16:59,759 --> 00:17:04,240
order cells based on their similarity

432
00:17:02,079 --> 00:17:07,160
and compare uh closeness and then

433
00:17:04,240 --> 00:17:09,679
compare to the ground truth um

434
00:17:07,160 --> 00:17:11,520
ordering. If we put some prior

435
00:17:09,679 --> 00:17:13,679
information about the reference lies for

436
00:17:11,520 --> 00:17:17,039
these baseline methods uh which Luna

437
00:17:13,679 --> 00:17:20,360
does not use uh we still see um large

438
00:17:17,039 --> 00:17:24,079
improvement over existing

439
00:17:20,360 --> 00:17:26,079
methods. So and uh next we were actually

440
00:17:24,079 --> 00:17:27,520
um wanted to really apply Luna to infer

441
00:17:26,079 --> 00:17:29,120
the locations of the single cell RNA

442
00:17:27,520 --> 00:17:31,039
sequencing atlas. So in particular we

443
00:17:29,120 --> 00:17:33,679
take the single cell central nervous

444
00:17:31,039 --> 00:17:36,160
system single cell RNA sequencing atlas.

445
00:17:33,679 --> 00:17:38,400
Um and of course for this we don't have

446
00:17:36,160 --> 00:17:40,400
ground truth cell locations. Uh so in

447
00:17:38,400 --> 00:17:42,320
particular the good thing about this

448
00:17:40,400 --> 00:17:45,200
particular atlas is that we could

449
00:17:42,320 --> 00:17:48,080
estimate the ground truth. Uh because u

450
00:17:45,200 --> 00:17:49,919
there is um star map atlas which one can

451
00:17:48,080 --> 00:17:52,080
integrate with a single cell sequencing

452
00:17:49,919 --> 00:17:54,480
atlas. We don't use it to train Luna or

453
00:17:52,080 --> 00:17:57,200
anything. We just use it to validate uh

454
00:17:54,480 --> 00:17:58,320
whether these predictions would be

455
00:17:57,200 --> 00:18:00,480
correct.

456
00:17:58,320 --> 00:18:02,080
So this is the estimated ground truth

457
00:18:00,480 --> 00:18:04,160
that we observe that we obtained by

458
00:18:02,080 --> 00:18:05,520
integrating uh this uh single cell RNA

459
00:18:04,160 --> 00:18:07,280
sequencing atlas with the star map

460
00:18:05,520 --> 00:18:09,520
atlas. So these are estimated locations

461
00:18:07,280 --> 00:18:11,840
of this single cell RNA sequencing data

462
00:18:09,520 --> 00:18:14,160
and these are predictions that we obtain

463
00:18:11,840 --> 00:18:15,919
by um training Luna on this smurfish

464
00:18:14,160 --> 00:18:17,960
mouse brain atlas and then applying it

465
00:18:15,919 --> 00:18:21,360
to single cell RNA sequencing

466
00:18:17,960 --> 00:18:22,880
data. Um again we see a good resembles

467
00:18:21,360 --> 00:18:25,760
between the estimated ground truth and

468
00:18:22,880 --> 00:18:27,679
Luna's predictions uh also on the

469
00:18:25,760 --> 00:18:29,440
broader classes. And here again I

470
00:18:27,679 --> 00:18:31,679
visualize how the process happens in

471
00:18:29,440 --> 00:18:33,600
Luna. If you look again on the gene

472
00:18:31,679 --> 00:18:34,960
expressions for example the model is

473
00:18:33,600 --> 00:18:36,160
able to correctly capture the

474
00:18:34,960 --> 00:18:39,280
expressions of some of the spatially

475
00:18:36,160 --> 00:18:41,120
variable genes. For example IQ Gap2 gene

476
00:18:39,280 --> 00:18:43,760
which shows down regulation across the

477
00:18:41,120 --> 00:18:45,679
ammon region. It's kind of this um down

478
00:18:43,760 --> 00:18:49,760
reggulation is correctly captured by the

479
00:18:45,679 --> 00:18:53,679
model uh is uh as well as um for example

480
00:18:49,760 --> 00:18:55,679
MFTt gene uh tc gene whose expression is

481
00:18:53,679 --> 00:18:58,919
minimal in the c2 region but then it's

482
00:18:55,679 --> 00:19:03,200
subst substantially upregulated in

483
00:18:58,919 --> 00:19:05,200
risoortex and all all factory areas. So

484
00:19:03,200 --> 00:19:07,360
we can now also use Luna for example to

485
00:19:05,200 --> 00:19:09,200
infer uh the expressions of genes that

486
00:19:07,360 --> 00:19:10,640
have not been seen during model training

487
00:19:09,200 --> 00:19:12,799
because now we can infer this for the

488
00:19:10,640 --> 00:19:14,320
single cell RNA sequencing atlas uh and

489
00:19:12,799 --> 00:19:18,799
look at the spatial distribution of

490
00:19:14,320 --> 00:19:18,799
these unseen genes across different

491
00:19:19,880 --> 00:19:24,559
regions. Um next we were also

492
00:19:22,640 --> 00:19:26,160
interesting whether Luna can overcome

493
00:19:24,559 --> 00:19:27,919
have to overcome limitations on some of

494
00:19:26,160 --> 00:19:30,240
other existing methods. For example,

495
00:19:27,919 --> 00:19:33,520
yesterday we heard about Slitex from F

496
00:19:30,240 --> 00:19:36,000
Chan's lab uh and Evan Makosko's lab. Uh

497
00:19:33,520 --> 00:19:37,840
and Slitex enables this profiling of

498
00:19:36,000 --> 00:19:39,600
single sentence spially resolved uh

499
00:19:37,840 --> 00:19:41,919
transcryto. But the limitation of this

500
00:19:39,600 --> 00:19:43,840
technology is that many nuclei lose

501
00:19:41,919 --> 00:19:46,160
spatial information during the barcoding

502
00:19:43,840 --> 00:19:48,160
process. So we asked the question can we

503
00:19:46,160 --> 00:19:50,480
now use Luna to actually predict the

504
00:19:48,160 --> 00:19:53,400
tissue locations of the nuclei that have

505
00:19:50,480 --> 00:19:56,240
been lost during a cell profiling with

506
00:19:53,400 --> 00:19:58,000
slideex. So here uh we applied it for

507
00:19:56,240 --> 00:19:59,760
the human melanoma data set. So these

508
00:19:58,000 --> 00:20:02,480
are spatially mapped nuclei with slide

509
00:19:59,760 --> 00:20:04,880
tags and then we enriched this data set

510
00:20:02,480 --> 00:20:07,039
uh using Luna by predicting the

511
00:20:04,880 --> 00:20:08,960
locations of these specially um map

512
00:20:07,039 --> 00:20:13,679
nuclei. So Luna successfully enriched

513
00:20:08,960 --> 00:20:17,360
this data set from 4,800 to 6,00 500

514
00:20:13,679 --> 00:20:20,720
cells. If we look at in particular like

515
00:20:17,360 --> 00:20:23,520
two tumor cells that are placed into two

516
00:20:20,720 --> 00:20:25,840
tumor subopuls that are placed into

517
00:20:23,520 --> 00:20:27,840
spatially distant compartments. Uh so

518
00:20:25,840 --> 00:20:30,320
these are uh spatially mapped nuclei

519
00:20:27,840 --> 00:20:33,280
with slide text here are nuclei mapped

520
00:20:30,320 --> 00:20:35,520
by Luna and uh this one group of tumor

521
00:20:33,280 --> 00:20:37,760
cells is expressed in the clue gene

522
00:20:35,520 --> 00:20:40,159
other is expressing the disk 2FP one

523
00:20:37,760 --> 00:20:41,640
gene and this is actually also correctly

524
00:20:40,159 --> 00:20:44,320
captured by the

525
00:20:41,640 --> 00:20:46,640
model. We next investigated whether this

526
00:20:44,320 --> 00:20:48,159
Luna enriched Slitex data set can help

527
00:20:46,640 --> 00:20:50,799
us to increase the power to detect

528
00:20:48,159 --> 00:20:53,600
spatially variable genes um in the human

529
00:20:50,799 --> 00:20:55,679
metastatic melanoma tissue. So we detect

530
00:20:53,600 --> 00:20:58,080
detected specially variable genes using

531
00:20:55,679 --> 00:21:00,400
only on original sample where we were we

532
00:20:58,080 --> 00:21:03,360
were able to find about 400 genes that

533
00:21:00,400 --> 00:21:05,039
are detectable both uh in the original

534
00:21:03,360 --> 00:21:07,280
data set just using original data set

535
00:21:05,039 --> 00:21:09,039
and also Luna enriched data set but

536
00:21:07,280 --> 00:21:11,280
using Luna enriched data set we were

537
00:21:09,039 --> 00:21:13,440
also able to detect additionally uh

538
00:21:11,280 --> 00:21:15,520
about 400 additional spatial variable

539
00:21:13,440 --> 00:21:18,559
genes. If you look at the gene pathways

540
00:21:15,520 --> 00:21:20,320
that are enriched in this um this set of

541
00:21:18,559 --> 00:21:24,240
genes, these are indeed for example the

542
00:21:20,320 --> 00:21:28,600
most significant pathway uh is um the

543
00:21:24,240 --> 00:21:31,360
pathway uh regulated by the uh NYC

544
00:21:28,600 --> 00:21:32,720
gene. We also applied it to other slight

545
00:21:31,360 --> 00:21:34,720
data. So this is for example for the

546
00:21:32,720 --> 00:21:38,640
mouse embriionic data set where it help

547
00:21:34,720 --> 00:21:38,640
to enrich the data set by uh

548
00:21:39,640 --> 00:21:45,280
twice and

549
00:21:42,159 --> 00:21:47,679
uh and so actually Luna could also be

550
00:21:45,280 --> 00:21:50,559
used potentially you know to um to think

551
00:21:47,679 --> 00:21:52,400
about kind of knocking out the effect of

552
00:21:50,559 --> 00:21:54,000
knocking out certain gene and kind of

553
00:21:52,400 --> 00:21:55,840
going potentially towards this kind of

554
00:21:54,000 --> 00:21:57,840
virtual tissue models when we can kind

555
00:21:55,840 --> 00:22:00,000
of knock out certain gene and then ask

556
00:21:57,840 --> 00:22:01,280
what happens to the spatial architecture

557
00:22:00,000 --> 00:22:04,400
of the tissue.

558
00:22:01,280 --> 00:22:07,520
Uh so what we did is we um just trained

559
00:22:04,400 --> 00:22:09,679
the Luna on the one on the all data set

560
00:22:07,520 --> 00:22:11,760
of on the Murphish mouse bin atlas from

561
00:22:09,679 --> 00:22:13,440
an animal one and then during the

562
00:22:11,760 --> 00:22:16,320
inference stage we were interesting to

563
00:22:13,440 --> 00:22:20,240
see what happens if with the predictions

564
00:22:16,320 --> 00:22:22,799
of the Luna if I knock out one gene uh

565
00:22:20,240 --> 00:22:25,039
for per study and then measure the

566
00:22:22,799 --> 00:22:26,799
effect basically of knocking out this

567
00:22:25,039 --> 00:22:29,520
gene on the prediction and the spatial

568
00:22:26,799 --> 00:22:31,760
architecture of the tissue. So in

569
00:22:29,520 --> 00:22:33,679
particular what we can do is we can uh

570
00:22:31,760 --> 00:22:36,480
for every study if I knock out certain

571
00:22:33,679 --> 00:22:38,159
gene I can then measure the effect

572
00:22:36,480 --> 00:22:40,400
knocking out this gene has on the

573
00:22:38,159 --> 00:22:42,559
spatial arc structure. So basically I

574
00:22:40,400 --> 00:22:45,039
get the maximum performance if I include

575
00:22:42,559 --> 00:22:47,720
all genes and then I measure how much

576
00:22:45,039 --> 00:22:50,159
excluding this gene is affecting my

577
00:22:47,720 --> 00:22:52,799
performance and then I can have then I

578
00:22:50,159 --> 00:22:55,280
can identify these genes that can kind

579
00:22:52,799 --> 00:22:58,559
of that have the highest effect on the

580
00:22:55,280 --> 00:23:00,960
spatial on affect on perturbing actually

581
00:22:58,559 --> 00:23:04,760
uh the spatial um uh the spatial

582
00:23:00,960 --> 00:23:04,760
structure of a tissue

583
00:23:04,799 --> 00:23:08,960
um and we can do this for example for

584
00:23:06,880 --> 00:23:10,559
different also cell subclasses. So here

585
00:23:08,960 --> 00:23:12,799
I show example for five different

586
00:23:10,559 --> 00:23:16,120
subclasses and here are genes that have

587
00:23:12,799 --> 00:23:18,640
highest effect on this uh on the spatial

588
00:23:16,120 --> 00:23:21,200
architecture. So for example these are

589
00:23:18,640 --> 00:23:23,120
indeed genes that are very important uh

590
00:23:21,200 --> 00:23:25,280
the crucial genes for the cell identity

591
00:23:23,120 --> 00:23:28,720
and function of these cell classes. For

592
00:23:25,280 --> 00:23:30,880
example, cludin 11 gene um is

593
00:23:28,720 --> 00:23:32,559
oligodendrite specific protein and

594
00:23:30,880 --> 00:23:35,520
previous work has shown that mutant

595
00:23:32,559 --> 00:23:37,520
mouse that lack this protein um exhibit

596
00:23:35,520 --> 00:23:39,760
central auditory deficits uh and the

597
00:23:37,520 --> 00:23:43,039
neurotransmitter imbalances. We can also

598
00:23:39,760 --> 00:23:45,440
do this for pairs like basically seeing

599
00:23:43,039 --> 00:23:47,039
what happens if we exclude pairs of

600
00:23:45,440 --> 00:23:49,039
genes. But of course the number of

601
00:23:47,039 --> 00:23:50,960
combinations in that case is very large.

602
00:23:49,039 --> 00:23:54,000
But what we do is we use existing

603
00:23:50,960 --> 00:23:56,960
receptor lyan databases and only focused

604
00:23:54,000 --> 00:24:01,480
um on certain sub a certain sub only

605
00:23:56,960 --> 00:24:01,480
subset of possible pairs.

606
00:24:02,159 --> 00:24:06,799
And in the last uh part of the my talk,

607
00:24:04,559 --> 00:24:09,280
I want to also quickly talk about how

608
00:24:06,799 --> 00:24:11,120
can we you know now in this kind of era

609
00:24:09,280 --> 00:24:12,559
of big foundation models how can we

610
00:24:11,120 --> 00:24:14,559
actually do something like unsupervised

611
00:24:12,559 --> 00:24:16,080
transfer. How can we use this big

612
00:24:14,559 --> 00:24:18,000
foundation models that people have been

613
00:24:16,080 --> 00:24:19,760
developing to actually tell us you know

614
00:24:18,000 --> 00:24:23,880
what kind of categories exist in the

615
00:24:19,760 --> 00:24:26,720
data in a fully unsupervised fashion.

616
00:24:23,880 --> 00:24:28,880
Um so if you think about you know

617
00:24:26,720 --> 00:24:31,120
current paradigms in which we use these

618
00:24:28,880 --> 00:24:33,760
existing foundation models they still

619
00:24:31,120 --> 00:24:35,200
majorly require supervision. So the

620
00:24:33,760 --> 00:24:37,200
standard way you know if someone gives

621
00:24:35,200 --> 00:24:38,799
you this foundation model we kind of

622
00:24:37,200 --> 00:24:40,799
fine-tune it on the task of interest

623
00:24:38,799 --> 00:24:42,480
using label data. So this is just a toy

624
00:24:40,799 --> 00:24:44,320
example where you know someone gives me

625
00:24:42,480 --> 00:24:46,720
large pre-train foundation model and

626
00:24:44,320 --> 00:24:48,559
then I have my labels and I want to

627
00:24:46,720 --> 00:24:50,720
solve a new task. I would kind of add a

628
00:24:48,559 --> 00:24:54,000
linear classifier uh to distinguish

629
00:24:50,720 --> 00:24:55,520
between different um cell classes.

630
00:24:54,000 --> 00:24:57,200
uh but as we've seen yesterday in

631
00:24:55,520 --> 00:24:59,679
Jennifer's talk as well is basically

632
00:24:57,200 --> 00:25:00,799
that in many cases in in biology we

633
00:24:59,679 --> 00:25:03,039
actually don't have these labeled

634
00:25:00,799 --> 00:25:05,600
examples. So we cannot kind of just add

635
00:25:03,039 --> 00:25:07,360
a linear classifier to solve our task of

636
00:25:05,600 --> 00:25:09,679
interest because we lack the labeled

637
00:25:07,360 --> 00:25:11,440
examples. We cannot have ab abundantly

638
00:25:09,679 --> 00:25:13,600
labeled examples for every possible

639
00:25:11,440 --> 00:25:15,760
class. Ideally we want to use these kind

640
00:25:13,600 --> 00:25:19,279
of big models to tell us what exist in

641
00:25:15,760 --> 00:25:20,960
the data in a fully unsupervised manner.

642
00:25:19,279 --> 00:25:22,480
So alternatively in machine learning

643
00:25:20,960 --> 00:25:24,159
community people do like a zeros

644
00:25:22,480 --> 00:25:26,000
transfer on the task of interest using

645
00:25:24,159 --> 00:25:28,320
instruction set where you basically the

646
00:25:26,000 --> 00:25:30,159
mostam famous example is open eyes clip

647
00:25:28,320 --> 00:25:32,240
model where you have this image encoder

648
00:25:30,159 --> 00:25:34,320
you have the text encoder that encodes

649
00:25:32,240 --> 00:25:36,240
this kind of class descriptions that you

650
00:25:34,320 --> 00:25:38,720
expect to see in the data then you match

651
00:25:36,240 --> 00:25:40,880
them but in this case you still kind of

652
00:25:38,720 --> 00:25:43,120
require to know which kind of classes

653
00:25:40,880 --> 00:25:45,120
you expect to see in your data and this

654
00:25:43,120 --> 00:25:48,799
is still not kind of suitable often for

655
00:25:45,120 --> 00:25:51,120
biology and for biomedical applications.

656
00:25:48,799 --> 00:25:52,640
So the question we ask is how can we

657
00:25:51,120 --> 00:25:54,559
infer this underlying classes without

658
00:25:52,640 --> 00:25:56,320
any supervision. So if one gives me you

659
00:25:54,559 --> 00:25:57,679
know large pre-chain foundation model

660
00:25:56,320 --> 00:25:59,200
and this is like we think about this

661
00:25:57,679 --> 00:26:01,039
very generally. It could be kind of

662
00:25:59,200 --> 00:26:03,600
computer vision foundation model trained

663
00:26:01,039 --> 00:26:06,240
on real world images. It can be single

664
00:26:03,600 --> 00:26:08,640
cell foundation model trained on single

665
00:26:06,240 --> 00:26:10,240
cell data. Uh it could be you know

666
00:26:08,640 --> 00:26:12,559
hisystopathology foundation model

667
00:26:10,240 --> 00:26:14,960
trained on histopathology images. How

668
00:26:12,559 --> 00:26:17,360
can I in an unsupervised way say you

669
00:26:14,960 --> 00:26:18,720
know what classes exist in this data

670
00:26:17,360 --> 00:26:21,039
set? Of course the trivial solution

671
00:26:18,720 --> 00:26:23,360
would be I just run the clustering uh uh

672
00:26:21,039 --> 00:26:25,039
to discover these uh set of classes. But

673
00:26:23,360 --> 00:26:28,640
the question is can we do better than

674
00:26:25,039 --> 00:26:30,080
that um and we call this um unsupervised

675
00:26:28,640 --> 00:26:31,440
transfer because we want to leverage

676
00:26:30,080 --> 00:26:33,440
these foundation models to in an

677
00:26:31,440 --> 00:26:35,960
unsupervised ways transfer knowledge to

678
00:26:33,440 --> 00:26:38,640
a new task that I'm interested to

679
00:26:35,960 --> 00:26:40,559
solve. And the key insight to solve this

680
00:26:38,640 --> 00:26:42,240
problem uh for us was that if you think

681
00:26:40,559 --> 00:26:45,200
about you know how we currently use

682
00:26:42,240 --> 00:26:46,799
these models we use this uh models uh

683
00:26:45,200 --> 00:26:48,559
the key insight is basically that linear

684
00:26:46,799 --> 00:26:50,159
models in representation space

685
00:26:48,559 --> 00:26:51,760
generalize well. So that's why we can

686
00:26:50,159 --> 00:26:53,120
just add a linear classifier on top of

687
00:26:51,760 --> 00:26:54,720
this preient representation and

688
00:26:53,120 --> 00:26:56,360
typically we get very high performance

689
00:26:54,720 --> 00:26:59,520
if we have the

690
00:26:56,360 --> 00:27:01,279
labels. Um and what we observe indeed

691
00:26:59,520 --> 00:27:03,440
that if you look kind of at the

692
00:27:01,279 --> 00:27:05,440
agreement of uh with the ground truth

693
00:27:03,440 --> 00:27:06,720
human labeling of the possible labeling.

694
00:27:05,440 --> 00:27:08,400
If you imagine kind of I give you

695
00:27:06,720 --> 00:27:10,480
labeling you can generate many possible

696
00:27:08,400 --> 00:27:11,840
labelings of these data sets. But if I

697
00:27:10,480 --> 00:27:13,840
measure the agreement with the ground

698
00:27:11,840 --> 00:27:15,760
truth human labeling the generalization

699
00:27:13,840 --> 00:27:18,880
of error of the linear classifier will

700
00:27:15,760 --> 00:27:20,960
be lower. If this is this uh uh is kind

701
00:27:18,880 --> 00:27:22,440
of agrees labeling agrees better with

702
00:27:20,960 --> 00:27:25,520
the ground truth human

703
00:27:22,440 --> 00:27:27,679
labeling. And this um kind of idea

704
00:27:25,520 --> 00:27:30,080
motivates us to develop a framework that

705
00:27:27,679 --> 00:27:33,440
we call turtle. And this is work done by

706
00:27:30,080 --> 00:27:35,600
uh my PhD student and Yulun. Um and the

707
00:27:33,440 --> 00:27:37,440
key idea is that basically we want to

708
00:27:35,600 --> 00:27:38,799
rephrase this unsupervised learning

709
00:27:37,440 --> 00:27:41,279
problem from the representations of

710
00:27:38,799 --> 00:27:43,279
foundation models as a problem of

711
00:27:41,279 --> 00:27:45,360
searching for such labeling so that

712
00:27:43,279 --> 00:27:47,679
linear classifiers will generalize well

713
00:27:45,360 --> 00:27:49,760
in this representation space. So I can

714
00:27:47,679 --> 00:27:51,600
take my data set of interest again it

715
00:27:49,760 --> 00:27:53,600
can be you know whatever foundation

716
00:27:51,600 --> 00:27:55,600
model or whatever data set it is. So

717
00:27:53,600 --> 00:27:58,080
this is just for example this kind of

718
00:27:55,600 --> 00:28:00,000
real world images I can embed it to one

719
00:27:58,080 --> 00:28:01,760
or multiple foundation models. If I have

720
00:28:00,000 --> 00:28:04,960
multiple representations I can leverage

721
00:28:01,760 --> 00:28:07,600
multiple foundation models and now I can

722
00:28:04,960 --> 00:28:11,200
basically solve this problem by finding

723
00:28:07,600 --> 00:28:12,720
such labeling that will give me um low

724
00:28:11,200 --> 00:28:14,640
generalization error of a linear

725
00:28:12,720 --> 00:28:17,600
classifier. So we formulate ble

726
00:28:14,640 --> 00:28:20,960
optimization procedure when in upper uh

727
00:28:17,600 --> 00:28:22,880
in uh in the upper loop we basically

728
00:28:20,960 --> 00:28:24,640
search we generate the possible

729
00:28:22,880 --> 00:28:27,279
labelings and then in the inner part of

730
00:28:24,640 --> 00:28:29,840
the optimization we evaluate how well

731
00:28:27,279 --> 00:28:31,679
does that labeling uh gen how well can

732
00:28:29,840 --> 00:28:33,679
linear classifier generalize on that

733
00:28:31,679 --> 00:28:36,559
labeling and then we optimize over

734
00:28:33,679 --> 00:28:38,240
possible labelings of a given data set.

735
00:28:36,559 --> 00:28:41,039
So I won't go into many details. I'm

736
00:28:38,240 --> 00:28:42,480
happy to chat later. But um basically

737
00:28:41,039 --> 00:28:44,240
searching for these labelings requires

738
00:28:42,480 --> 00:28:46,559
solving discrete optimization problem

739
00:28:44,240 --> 00:28:48,480
over all possible labelings. So instead

740
00:28:46,559 --> 00:28:50,640
we resort to continuous optimization via

741
00:28:48,480 --> 00:28:52,320
relaxation. Uh instead of doing discrete

742
00:28:50,640 --> 00:28:53,919
search over labelings, we do continuous

743
00:28:52,320 --> 00:28:56,159
search over parameters of the task

744
00:28:53,919 --> 00:28:57,840
encoder. uh and we can also show

745
00:28:56,159 --> 00:28:59,919
theoretically that the labelings that

746
00:28:57,840 --> 00:29:01,760
will be uh found by this optimization

747
00:28:59,919 --> 00:29:03,279
procedure under some assumptions are the

748
00:29:01,760 --> 00:29:05,440
labelings that maximize margin of a

749
00:29:03,279 --> 00:29:07,320
linear model in the corresponding uh

750
00:29:05,440 --> 00:29:09,399
representation

751
00:29:07,320 --> 00:29:12,159
space. So

752
00:29:09,399 --> 00:29:13,919
um interestingly this kind of idea

753
00:29:12,159 --> 00:29:16,480
although very simple it actually allows

754
00:29:13,919 --> 00:29:18,159
us to leverage now any foundation models

755
00:29:16,480 --> 00:29:19,919
train linear classifiers on top and

756
00:29:18,159 --> 00:29:21,360
search for the possible labelings. We

757
00:29:19,919 --> 00:29:23,600
show that it achieves state-of-the-art

758
00:29:21,360 --> 00:29:25,039
unsupervised performance across various

759
00:29:23,600 --> 00:29:26,640
benchmarks. So it's available on the

760
00:29:25,039 --> 00:29:28,399
papers with code. You can use it with

761
00:29:26,640 --> 00:29:31,360
any you know your favorite foundation

762
00:29:28,399 --> 00:29:33,640
models. Um uh so this is an example of

763
00:29:31,360 --> 00:29:36,080
the imageet data set where actually the

764
00:29:33,640 --> 00:29:37,600
state-of-the-art method as well as also

765
00:29:36,080 --> 00:29:38,679
many other data sets including for

766
00:29:37,600 --> 00:29:41,039
example

767
00:29:38,679 --> 00:29:42,840
pathological data set of histopathology

768
00:29:41,039 --> 00:29:45,200
images like patch

769
00:29:42,840 --> 00:29:47,279
chameleon. Uh we went also a step

770
00:29:45,200 --> 00:29:49,679
further to compare it to the zeroot

771
00:29:47,279 --> 00:29:51,440
transfer uh like for example to the clip

772
00:29:49,679 --> 00:29:53,320
model that actually uses information

773
00:29:51,440 --> 00:29:55,679
about the classes while turtle is fully

774
00:29:53,320 --> 00:29:57,440
unsupervised and we show if we run the

775
00:29:55,679 --> 00:29:59,840
turtle in the same embedding space as a

776
00:29:57,440 --> 00:30:01,760
clip we have the same performance. Uh

777
00:29:59,840 --> 00:30:03,279
but if you just add another foundation

778
00:30:01,760 --> 00:30:05,120
model which is much cheaper than getting

779
00:30:03,279 --> 00:30:06,960
the labelings for example for from the

780
00:30:05,120 --> 00:30:08,720
clip model uh you just add another

781
00:30:06,960 --> 00:30:12,720
representation space for example here we

782
00:30:08,720 --> 00:30:15,279
just add a dino model um then uh model

783
00:30:12,720 --> 00:30:17,279
significantly outperforms um clip model

784
00:30:15,279 --> 00:30:22,600
across this 26 benchmark data set

785
00:30:17,279 --> 00:30:24,880
achieves a large margin uh a large uh

786
00:30:22,600 --> 00:30:27,200
improvements. We can also apply it to

787
00:30:24,880 --> 00:30:28,960
single cell data. So this is an example

788
00:30:27,200 --> 00:30:31,679
of you know using existing foundation

789
00:30:28,960 --> 00:30:34,159
models for a single cell where you can

790
00:30:31,679 --> 00:30:36,000
do this unsurprised transfer to discover

791
00:30:34,159 --> 00:30:38,320
for example different cell classes that

792
00:30:36,000 --> 00:30:40,480
ex exist in the data. But I would say

793
00:30:38,320 --> 00:30:42,880
disclaimer here is that the core idea of

794
00:30:40,480 --> 00:30:44,720
the method uh is that you know you

795
00:30:42,880 --> 00:30:46,720
assume that the representation space of

796
00:30:44,720 --> 00:30:48,720
foundation model is strong enough which

797
00:30:46,720 --> 00:30:50,880
is sometimes case for this foundation

798
00:30:48,720 --> 00:30:52,480
models by uh that we have for single

799
00:30:50,880 --> 00:30:55,440
cell but sometimes still I think we have

800
00:30:52,480 --> 00:30:57,679
a large space uh to improve uh these

801
00:30:55,440 --> 00:31:00,240
models

802
00:30:57,679 --> 00:31:02,240
um and uh so this is just one slide so

803
00:31:00,240 --> 00:31:04,159
very latest uh in the very latest work

804
00:31:02,240 --> 00:31:07,279
we um also show that we can do

805
00:31:04,159 --> 00:31:08,640
unsupervised adaptation of uh these big

806
00:31:07,279 --> 00:31:10,640
foundation model. So we can even

807
00:31:08,640 --> 00:31:12,159
fine-tune them in an unsupervised way.

808
00:31:10,640 --> 00:31:13,840
So instead of doing this kind of linear

809
00:31:12,159 --> 00:31:15,600
classifier in the inner loop, we kind of

810
00:31:13,840 --> 00:31:18,000
perform in context learning in the inner

811
00:31:15,600 --> 00:31:19,840
loop. Uh and now we show that uh

812
00:31:18,000 --> 00:31:21,360
basically this kind of framework that we

813
00:31:19,840 --> 00:31:23,200
call joint inference that was just

814
00:31:21,360 --> 00:31:24,799
presented as I clear. It can make

815
00:31:23,200 --> 00:31:26,799
predictions simultaneously for all

816
00:31:24,799 --> 00:31:29,120
inputs in a given task. And we show that

817
00:31:26,799 --> 00:31:31,039
this is applicable to for example large

818
00:31:29,120 --> 00:31:32,559
language models, vision language models

819
00:31:31,039 --> 00:31:34,559
and achieves state-of-the-art

820
00:31:32,559 --> 00:31:37,039
performance on the difficult math

821
00:31:34,559 --> 00:31:38,799
reasoning benchmark compared to even uh

822
00:31:37,039 --> 00:31:41,039
this unsupervised ICL can even

823
00:31:38,799 --> 00:31:42,679
outperform supervised ICL on these math

824
00:31:41,039 --> 00:31:45,279
reasoning

825
00:31:42,679 --> 00:31:47,760
benchmarks. Um and finally I would like

826
00:31:45,279 --> 00:31:49,679
to thank my uh amazing PhD students

827
00:31:47,760 --> 00:31:51,760
whose work I presented here. So in

828
00:31:49,679 --> 00:31:55,440
particular yeast who worked on Luna uh

829
00:31:51,760 --> 00:31:57,120
Yulun and Artium uh who worked on turtle

830
00:31:55,440 --> 00:31:59,440
and this also unsupervised inference

831
00:31:57,120 --> 00:32:00,960
framework and uh happy uh to take

832
00:31:59,440 --> 00:32:05,910
questions.

833
00:32:00,960 --> 00:32:05,910
[Applause]

834
00:32:05,919 --> 00:32:10,679
Thank you so much for an amazing talk

835
00:32:07,760 --> 00:32:14,080
Maria. Um

836
00:32:10,679 --> 00:32:15,679
questions thank you fantastic work. Um

837
00:32:14,080 --> 00:32:17,039
just a question about Luna. If I

838
00:32:15,679 --> 00:32:18,960
understand well it's a generative

839
00:32:17,039 --> 00:32:21,360
process that is stochastic. So if you

840
00:32:18,960 --> 00:32:23,760
run it 10 times, you get 10 different

841
00:32:21,360 --> 00:32:26,080
images. Could you comment on the

842
00:32:23,760 --> 00:32:28,000
diversity that you get or is it always

843
00:32:26,080 --> 00:32:29,840
consistent? No, it's not always

844
00:32:28,000 --> 00:32:32,080
consistent because it's a generative

845
00:32:29,840 --> 00:32:36,000
model, right? So of course if you run it

846
00:32:32,080 --> 00:32:38,399
different with different uh you run it

847
00:32:36,000 --> 00:32:40,320
different times, you will see some

848
00:32:38,399 --> 00:32:42,159
variation. Cells would not be exactly

849
00:32:40,320 --> 00:32:43,600
positioned at exactly same locations,

850
00:32:42,159 --> 00:32:45,440
but still this kind of spatial

851
00:32:43,600 --> 00:32:47,360
distribution that I was showing would be

852
00:32:45,440 --> 00:32:49,240
preserved. So in terms of maybe you know

853
00:32:47,360 --> 00:32:52,960
two cells will be kind of switched

854
00:32:49,240 --> 00:32:54,880
locations because but these kind of

855
00:32:52,960 --> 00:32:57,480
results that I was showing they are kind

856
00:32:54,880 --> 00:33:01,279
of consistent across different fronts.

857
00:32:57,480 --> 00:33:03,279
Okay. Thank you. Hi.

858
00:33:01,279 --> 00:33:06,799
Hi there. Uh thanks a lot for a great

859
00:33:03,279 --> 00:33:09,360
talk. Uh I have a question on Luna. So

860
00:33:06,799 --> 00:33:11,039
what are the limitations of L Luna and

861
00:33:09,360 --> 00:33:13,679
what are some of the future directions

862
00:33:11,039 --> 00:33:15,519
that you see? Yeah. So uh yeah uh so

863
00:33:13,679 --> 00:33:19,440
first the major limitation is actually

864
00:33:15,519 --> 00:33:21,760
that tissues are not 2D right uh so for

865
00:33:19,440 --> 00:33:23,919
single cell RNA like ideally one would

866
00:33:21,760 --> 00:33:26,399
actually be able to embed any single

867
00:33:23,919 --> 00:33:28,640
cell sequencing data set in the 3D space

868
00:33:26,399 --> 00:33:31,120
right uh so here for example for single

869
00:33:28,640 --> 00:33:32,960
cell RNA sequencing data we do this kind

870
00:33:31,120 --> 00:33:36,240
of integration you know with the star

871
00:33:32,960 --> 00:33:38,399
map atlas we have this uh you know uh we

872
00:33:36,240 --> 00:33:40,640
know from this which 2D slice the single

873
00:33:38,399 --> 00:33:43,600
cell RNA sequencing data comes but

874
00:33:40,640 --> 00:33:45,200
ideally one would apply this to 3D data.

875
00:33:43,600 --> 00:33:47,039
Uh but I would say this is not really

876
00:33:45,200 --> 00:33:49,039
limitation of Luna because the idea the

877
00:33:47,039 --> 00:33:50,720
core idea would be applicable to the 3D

878
00:33:49,039 --> 00:33:53,279
case but we just don't have the good

879
00:33:50,720 --> 00:33:55,360
data to train this model on the 3D on

880
00:33:53,279 --> 00:33:56,880
the 3D data. So the objective function

881
00:33:55,360 --> 00:34:00,080
and everything we can immediately apply

882
00:33:56,880 --> 00:34:02,720
it for 3D case as well. But uh another

883
00:34:00,080 --> 00:34:05,279
limitation um is that um and what we're

884
00:34:02,720 --> 00:34:08,480
looking now further is basically that uh

885
00:34:05,279 --> 00:34:10,879
currently so when we train the model we

886
00:34:08,480 --> 00:34:12,639
um you assume that you measure for

887
00:34:10,879 --> 00:34:16,599
example existing special transatomics

888
00:34:12,639 --> 00:34:18,639
data sets that you capture this uh the

889
00:34:16,599 --> 00:34:20,639
the basically for example for the

890
00:34:18,639 --> 00:34:22,720
merfish it's okay because we have the

891
00:34:20,639 --> 00:34:24,159
same gene panels but ideally you would

892
00:34:22,720 --> 00:34:26,320
kind of be able to train it across like

893
00:34:24,159 --> 00:34:28,240
different spatial technologies uh then

894
00:34:26,320 --> 00:34:30,079
we would need to kind of be able to

895
00:34:28,240 --> 00:34:31,760
capture also like different gene panels

896
00:34:30,079 --> 00:34:33,839
that can be measured across different uh

897
00:34:31,760 --> 00:34:37,800
data sets. So this is another uh

898
00:34:33,839 --> 00:34:37,800
limitation of the method.

899
00:34:38,320 --> 00:34:43,599
So much uh we sadly have to stop at the

900
00:34:41,040 --> 00:34:47,639
two questions. Thanks again. Uh please

901
00:34:43,599 --> 00:34:47,639
applaud Maria again.

