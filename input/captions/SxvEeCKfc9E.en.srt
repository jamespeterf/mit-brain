1
00:00:04,759 --> 00:00:13,440
Our next session is about AI and

2
00:00:11,759 --> 00:00:17,039
uh it's called interdisciplinary

3
00:00:13,440 --> 00:00:21,840
research around AI. Our first speaker of

4
00:00:17,039 --> 00:00:24,960
this session is an MIT major, MIT senior

5
00:00:21,840 --> 00:00:29,000
uh in computer science, economics and

6
00:00:24,960 --> 00:00:33,399
data science and she has studied AI

7
00:00:29,000 --> 00:00:37,960
safety and AI safety and she's going to

8
00:00:33,399 --> 00:00:42,640
introduce some MIT student uh le

9
00:00:37,960 --> 00:00:46,600
activities around how to control the AI

10
00:00:42,640 --> 00:00:51,520
risk. Let's welcome uh Audrey.

11
00:00:46,600 --> 00:00:53,680
[Applause]

12
00:00:51,520 --> 00:00:55,440
Hi everyone. My name is Audrey Lurville

13
00:00:53,680 --> 00:00:57,600
and today I'll be talking to you about

14
00:00:55,440 --> 00:00:59,520
AI risk mitigation and sharing students

15
00:00:57,600 --> 00:01:02,879
perspectives from the MIT future tech

16
00:00:59,520 --> 00:01:04,640
group and the MIT AI alignment group.

17
00:01:02,879 --> 00:01:06,960
What could go wrong as we develop more

18
00:01:04,640 --> 00:01:08,880
powerful AI? This is a question that I

19
00:01:06,960 --> 00:01:11,600
and some others at MIT are working to

20
00:01:08,880 --> 00:01:13,600
address.

21
00:01:11,600 --> 00:01:15,040
Why is this question relevant to you?

22
00:01:13,600 --> 00:01:16,799
Could you raise your hand if you work

23
00:01:15,040 --> 00:01:18,680
for a company currently developing or

24
00:01:16,799 --> 00:01:20,720
deploying

25
00:01:18,680 --> 00:01:22,799
AI? Could you raise your hand if you

26
00:01:20,720 --> 00:01:25,119
work in academia or the government and

27
00:01:22,799 --> 00:01:26,759
you're wondering how AI will impact your

28
00:01:25,119 --> 00:01:28,799
um field of

29
00:01:26,759 --> 00:01:30,240
interest? Now, could you raise your hand

30
00:01:28,799 --> 00:01:32,400
if you're simply a human being who's

31
00:01:30,240 --> 00:01:34,240
been seeing how AI has been improving

32
00:01:32,400 --> 00:01:36,720
over the past few years and you're

33
00:01:34,240 --> 00:01:39,360
wondering what's going to happen? Well,

34
00:01:36,720 --> 00:01:41,280
AI is clearly going to impact all of us.

35
00:01:39,360 --> 00:01:43,680
We can imagine many futures where AI

36
00:01:41,280 --> 00:01:45,759
will help us reduce um the big problems

37
00:01:43,680 --> 00:01:47,840
that we're faced with by accelerating

38
00:01:45,759 --> 00:01:50,640
scientific knowledge and lowering uh

39
00:01:47,840 --> 00:01:53,600
barriers to knowledge. However, we can

40
00:01:50,640 --> 00:01:56,640
also imagine many futures where AI will

41
00:01:53,600 --> 00:01:59,200
exacerbate exa existing issues or even

42
00:01:56,640 --> 00:02:00,880
lead to a loss of human control if we

43
00:01:59,200 --> 00:02:02,920
lose control of these powerful systems

44
00:02:00,880 --> 00:02:05,600
that we've

45
00:02:02,920 --> 00:02:08,640
built. So, why did I start working on AI

46
00:02:05,600 --> 00:02:10,399
risks? I'm a current senior at MIT. I'm

47
00:02:08,640 --> 00:02:12,000
majoring in computer science, economics,

48
00:02:10,399 --> 00:02:13,200
and data science. And I have a

49
00:02:12,000 --> 00:02:14,959
concentration in international

50
00:02:13,200 --> 00:02:16,959
development. I grew up all over the

51
00:02:14,959 --> 00:02:18,800
world in France, Argentina, Japan,

52
00:02:16,959 --> 00:02:20,160
Brazil, Mexico, and the US. And for a

53
00:02:18,800 --> 00:02:22,319
long time, I thought I would work in

54
00:02:20,160 --> 00:02:23,760
economic global development. But three

55
00:02:22,319 --> 00:02:26,480
years ago, I learned about some of the

56
00:02:23,760 --> 00:02:28,599
potential risks that AI poses. And this

57
00:02:26,480 --> 00:02:30,560
completely shifted all my

58
00:02:28,599 --> 00:02:32,080
priorities. I'd like to take a few

59
00:02:30,560 --> 00:02:36,280
moments to see if you can think of three

60
00:02:32,080 --> 00:02:36,280
different risks that AI poses.

61
00:02:36,560 --> 00:02:40,200
Can you raise your hand if you could

62
00:02:37,760 --> 00:02:43,440
think of three different risks from

63
00:02:40,200 --> 00:02:44,879
AI? Okay. Can you continue raising your

64
00:02:43,440 --> 00:02:47,840
hand if one of the risks you thought of

65
00:02:44,879 --> 00:02:47,840
was existential

66
00:02:47,959 --> 00:02:52,879
risk?

67
00:02:49,800 --> 00:02:55,120
Okay. So, you may be wondering what is

68
00:02:52,879 --> 00:02:57,519
existential risk? Existential risk

69
00:02:55,120 --> 00:03:00,560
refers to the fact that AI may cause

70
00:02:57,519 --> 00:03:02,720
human extinction or lead to irreversible

71
00:03:00,560 --> 00:03:05,440
limits on human potential and humanity's

72
00:03:02,720 --> 00:03:07,360
future. for example, by humans losing

73
00:03:05,440 --> 00:03:08,440
control of AI and AI starting to take

74
00:03:07,360 --> 00:03:10,239
all

75
00:03:08,440 --> 00:03:11,840
decisions, especially if you're hearing

76
00:03:10,239 --> 00:03:13,760
for this for the first time. It may

77
00:03:11,840 --> 00:03:16,400
sound a little bit extreme. However,

78
00:03:13,760 --> 00:03:18,319
many AI leaders and um executives in the

79
00:03:16,400 --> 00:03:20,319
field believe that this is an outcome

80
00:03:18,319 --> 00:03:22,720
that we shouldn't dismiss. In fact, in

81
00:03:20,319 --> 00:03:25,280
2023, the Center for AI Safety released

82
00:03:22,720 --> 00:03:26,800
a single sentence statement. Here it is.

83
00:03:25,280 --> 00:03:29,200
Mitigating the risk of extinction from

84
00:03:26,800 --> 00:03:30,879
AI should be a global priority alongside

85
00:03:29,200 --> 00:03:33,680
other society scale risks such as

86
00:03:30,879 --> 00:03:36,159
pandemics and nuclear war. Since then,

87
00:03:33,680 --> 00:03:38,159
over 350 leading AI scientists, some of

88
00:03:36,159 --> 00:03:40,680
whose names you can see on the image,

89
00:03:38,159 --> 00:03:43,040
have signed this

90
00:03:40,680 --> 00:03:44,319
statement. Existential risk is what I

91
00:03:43,040 --> 00:03:46,159
learned about three years ago, and this

92
00:03:44,319 --> 00:03:47,599
completely changed my life. I also

93
00:03:46,159 --> 00:03:50,000
learned about the two types of advanced

94
00:03:47,599 --> 00:03:51,760
AI. You may have heard AGI floating

95
00:03:50,000 --> 00:03:53,519
around there, or artificial general

96
00:03:51,760 --> 00:03:55,519
intelligence. There's many different

97
00:03:53,519 --> 00:03:57,120
definitions out there, but a common one

98
00:03:55,519 --> 00:03:59,840
is general purpose intelligence

99
00:03:57,120 --> 00:04:01,680
comparable to humans on many points. The

100
00:03:59,840 --> 00:04:04,319
step above that is artificial super

101
00:04:01,680 --> 00:04:06,280
intelligence or ASI which surpasses

102
00:04:04,319 --> 00:04:09,439
human intelligence in essentially every

103
00:04:06,280 --> 00:04:11,120
domain. AI is already superhuman at some

104
00:04:09,439 --> 00:04:14,159
tasks such as playing games like chess

105
00:04:11,120 --> 00:04:15,760
and go. Um and current AIs are getting

106
00:04:14,159 --> 00:04:18,079
increasingly better at different tasks

107
00:04:15,760 --> 00:04:20,320
such as software engineering. We can

108
00:04:18,079 --> 00:04:22,639
imagine that ASI will provide much

109
00:04:20,320 --> 00:04:24,880
better biologists, better physicists,

110
00:04:22,639 --> 00:04:26,320
and better inventors.

111
00:04:24,880 --> 00:04:28,240
I'd like to argue that the continued

112
00:04:26,320 --> 00:04:30,639
improvement of AI poses some serious

113
00:04:28,240 --> 00:04:32,800
risks to humanity's future. My argument

114
00:04:30,639 --> 00:04:34,000
can be broken down into three parts. The

115
00:04:32,800 --> 00:04:36,639
first is that we're building

116
00:04:34,000 --> 00:04:38,560
increasingly capable AI agents. The

117
00:04:36,639 --> 00:04:41,440
second is that AI doesn't always do what

118
00:04:38,560 --> 00:04:43,160
we intended to do. And finally, AI could

119
00:04:41,440 --> 00:04:45,199
plausibly take actions that endanger

120
00:04:43,160 --> 00:04:47,120
humans. Allow me to walk you through

121
00:04:45,199 --> 00:04:49,840
these points. The first is we're

122
00:04:47,120 --> 00:04:51,680
building increasingly capable AI agents.

123
00:04:49,840 --> 00:04:53,759
Well, the main factors that go into

124
00:04:51,680 --> 00:04:56,080
improving AI are energy, data, and

125
00:04:53,759 --> 00:04:58,240
hardware, and there are no no very

126
00:04:56,080 --> 00:05:00,080
strong bottlenecks on these. We also

127
00:04:58,240 --> 00:05:01,600
have abundant funds going into AI, as

128
00:05:00,080 --> 00:05:03,320
can be seen in recent large-scale

129
00:05:01,600 --> 00:05:06,400
investments such as the Stargate

130
00:05:03,320 --> 00:05:09,039
project. Third, AI is currently starting

131
00:05:06,400 --> 00:05:11,120
to work on AI R&D, which means AI is

132
00:05:09,039 --> 00:05:12,880
accelerating its own improvement.

133
00:05:11,120 --> 00:05:14,639
Finally, the competitive pressures and

134
00:05:12,880 --> 00:05:16,400
the potential gains are too great for

135
00:05:14,639 --> 00:05:18,240
organizations to completely desist from

136
00:05:16,400 --> 00:05:20,000
working on AI.

137
00:05:18,240 --> 00:05:22,160
I'm going to show you two graphs that

138
00:05:20,000 --> 00:05:23,759
illustrate this point. The first is work

139
00:05:22,160 --> 00:05:26,479
from meter that was released in March of

140
00:05:23,759 --> 00:05:28,800
this year. This shows um the length of

141
00:05:26,479 --> 00:05:30,800
tasks can do um that length of tasks AI

142
00:05:28,800 --> 00:05:32,880
can do is doubling every seven months.

143
00:05:30,800 --> 00:05:35,120
Along the x-axis you have model release

144
00:05:32,880 --> 00:05:38,960
year and along the y-axis you have the

145
00:05:35,120 --> 00:05:42,160
length of tasks that AI um can get 50%

146
00:05:38,960 --> 00:05:45,280
success rate on. So this metric the 50%

147
00:05:42,160 --> 00:05:47,120
task completion rate is a good proxy for

148
00:05:45,280 --> 00:05:49,440
tracking progress in model autonomy over

149
00:05:47,120 --> 00:05:51,960
time. And as you can see um model

150
00:05:49,440 --> 00:05:54,720
performance is showing exponential

151
00:05:51,960 --> 00:05:56,320
growth. Next this graph shows the test

152
00:05:54,720 --> 00:05:57,880
scores of AI systems on various

153
00:05:56,320 --> 00:06:00,000
capabilities relative to human

154
00:05:57,880 --> 00:06:01,600
performance. The different colored lines

155
00:06:00,000 --> 00:06:02,880
correspond to different benchmarks. So

156
00:06:01,600 --> 00:06:04,160
different capabilities that we're

157
00:06:02,880 --> 00:06:06,080
interested in such as reading

158
00:06:04,160 --> 00:06:07,759
comprehension or predictive reasoning.

159
00:06:06,080 --> 00:06:09,520
Along the x-axis you have years and

160
00:06:07,759 --> 00:06:11,960
along the y- axis you have AI

161
00:06:09,520 --> 00:06:14,160
performance compared to model to human

162
00:06:11,960 --> 00:06:15,600
performance. Um human performance is

163
00:06:14,160 --> 00:06:18,720
used as the baseline. So it corresponds

164
00:06:15,600 --> 00:06:21,440
to that gray horizontal line at zero.

165
00:06:18,720 --> 00:06:24,400
And once the colored lines go above that

166
00:06:21,440 --> 00:06:26,479
gray um baseline that means that AI

167
00:06:24,400 --> 00:06:28,240
performance has gotten better than human

168
00:06:26,479 --> 00:06:30,960
performance. And as you can see over the

169
00:06:28,240 --> 00:06:33,039
past couple years AIs have pretty much

170
00:06:30,960 --> 00:06:35,919
surpassed or come level with humans on

171
00:06:33,039 --> 00:06:37,759
many different capabilities.

172
00:06:35,919 --> 00:06:39,919
My second point is that AI doesn't

173
00:06:37,759 --> 00:06:41,280
always do what we intended to do. And to

174
00:06:39,919 --> 00:06:43,280
illustrate this, I'd like to share some

175
00:06:41,280 --> 00:06:46,080
work from a lab I'm a part of at MIT

176
00:06:43,280 --> 00:06:48,319
called Future Techch. Future is part of

177
00:06:46,080 --> 00:06:50,800
CEL, which is MIT's computer science and

178
00:06:48,319 --> 00:06:52,319
AI lab. And Future Techch studies the

179
00:06:50,800 --> 00:06:54,319
economic and technical foundations of

180
00:06:52,319 --> 00:06:56,240
progress in computing. Under the

181
00:06:54,319 --> 00:06:57,759
direction of Neil Thompson and Peter

182
00:06:56,240 --> 00:07:00,639
Slatterie, we've built the most

183
00:06:57,759 --> 00:07:02,960
comprehensive repository of AI risks um

184
00:07:00,639 --> 00:07:04,720
by synthesizing 56 different AI risk

185
00:07:02,960 --> 00:07:06,880
frameworks. The result is what we call

186
00:07:04,720 --> 00:07:08,880
the AI risk repository. A living

187
00:07:06,880 --> 00:07:11,120
database of over a thousand AI risks

188
00:07:08,880 --> 00:07:13,360
categorized into two taxonomies. We have

189
00:07:11,120 --> 00:07:15,680
the causal taxonomy which looks at the

190
00:07:13,360 --> 00:07:18,160
if, the how, the when, and the why these

191
00:07:15,680 --> 00:07:20,080
risks occur. And then we have the domain

192
00:07:18,160 --> 00:07:21,840
taxonomy. So every single risk is

193
00:07:20,080 --> 00:07:24,400
divided into seven different domains and

194
00:07:21,840 --> 00:07:26,800
23 different subdomains. Future Tech has

195
00:07:24,400 --> 00:07:29,280
also built an AI incident tracker which

196
00:07:26,800 --> 00:07:30,960
records the details of different risks

197
00:07:29,280 --> 00:07:33,000
that um of different incidents that

198
00:07:30,960 --> 00:07:35,759
occurred that are linked to

199
00:07:33,000 --> 00:07:38,240
AI. This is an example of the domain

200
00:07:35,759 --> 00:07:40,000
taxonomy we have for AI risks. So the

201
00:07:38,240 --> 00:07:42,599
seven main domains are discrimination

202
00:07:40,000 --> 00:07:44,560
and toxicity, privacy and security,

203
00:07:42,599 --> 00:07:46,880
misinformation, malicious actors and

204
00:07:44,560 --> 00:07:48,960
misuse, human computer interaction,

205
00:07:46,880 --> 00:07:50,880
socioeconomic and environmental harms,

206
00:07:48,960 --> 00:07:53,599
and AI system safety failures and

207
00:07:50,880 --> 00:07:55,360
limitations. This risk repository is

208
00:07:53,599 --> 00:07:57,440
completely free and it can be used by

209
00:07:55,360 --> 00:08:00,240
anyone right now if you just access our

210
00:07:57,440 --> 00:08:02,000
website. Um you can browse through the

211
00:08:00,240 --> 00:08:03,680
different types of domains and you the

212
00:08:02,000 --> 00:08:06,240
different taxonomies and explore

213
00:08:03,680 --> 00:08:07,680
different risks linked to AI. The

214
00:08:06,240 --> 00:08:09,520
Brazilian government for example has

215
00:08:07,680 --> 00:08:11,360
already used this repository to inform

216
00:08:09,520 --> 00:08:13,360
its AI act which is one of the largest

217
00:08:11,360 --> 00:08:16,160
pieces of legislation that a country has

218
00:08:13,360 --> 00:08:19,680
worked on to try to shape how AI is

219
00:08:16,160 --> 00:08:21,599
being used inside its borders.

220
00:08:19,680 --> 00:08:23,120
So back to AI doesn't always do what we

221
00:08:21,599 --> 00:08:25,919
intended to. What do I mean by this?

222
00:08:23,120 --> 00:08:28,400
This is known as misalignment in the AI

223
00:08:25,919 --> 00:08:31,360
risk repository. This corresponds to

224
00:08:28,400 --> 00:08:33,360
subdomain 7.1. AI pursuing its own goals

225
00:08:31,360 --> 00:08:35,360
in conflict with human goals or values.

226
00:08:33,360 --> 00:08:37,320
So AI doing something that the human

227
00:08:35,360 --> 00:08:40,800
designers didn't actually intend or

228
00:08:37,320 --> 00:08:42,479
plan. Um, every domain has a more

229
00:08:40,800 --> 00:08:44,080
specific and detailed description if

230
00:08:42,479 --> 00:08:45,680
you're interested in that. And we can

231
00:08:44,080 --> 00:08:47,600
look at the AI incident tracker for an

232
00:08:45,680 --> 00:08:51,680
example of misalignment. A very common

233
00:08:47,600 --> 00:08:53,440
one um that appeared in 2023 is GPT4

234
00:08:51,680 --> 00:08:55,360
which pretended to be a blind person to

235
00:08:53,440 --> 00:08:57,120
hire a remote worker to solve a capture

236
00:08:55,360 --> 00:08:58,399
test for it ones where you have to pick

237
00:08:57,120 --> 00:08:59,920
like the number of palm trees for

238
00:08:58,399 --> 00:09:03,040
example and the whole point of capture

239
00:08:59,920 --> 00:09:05,279
is that it can't be solved by AI but two

240
00:09:03,040 --> 00:09:07,279
years ago this already happened and we

241
00:09:05,279 --> 00:09:10,040
can think of many different examples of

242
00:09:07,279 --> 00:09:12,720
AI behaving in unexpected or deceptive

243
00:09:10,040 --> 00:09:14,480
ways. We can try to train out examples

244
00:09:12,720 --> 00:09:16,240
of misalignment that we catch when we're

245
00:09:14,480 --> 00:09:17,680
training AI, but it's really hard to

246
00:09:16,240 --> 00:09:19,360
guarantee that we aren't selecting poor

247
00:09:17,680 --> 00:09:21,399
AI that's just really good at hiding its

248
00:09:19,360 --> 00:09:23,920
misbehavior from

249
00:09:21,399 --> 00:09:25,200
us. So far, we've seen that AI systems

250
00:09:23,920 --> 00:09:26,480
are getting smarter and that they

251
00:09:25,200 --> 00:09:29,480
sometimes pursue goals that are in

252
00:09:26,480 --> 00:09:31,920
conflict with what their human designers

253
00:09:29,480 --> 00:09:33,680
want. Now, to my third point, AI could

254
00:09:31,920 --> 00:09:36,320
plausibly take actions that endanger

255
00:09:33,680 --> 00:09:38,320
humans. What do I mean by this? Again,

256
00:09:36,320 --> 00:09:40,800
I'm going to use the AI risk repository

257
00:09:38,320 --> 00:09:43,040
to highlight categories of risks of

258
00:09:40,800 --> 00:09:45,680
three categories of risks that endanger

259
00:09:43,040 --> 00:09:47,120
humans. The first is misuse. In the

260
00:09:45,680 --> 00:09:48,800
repository, we find this as cyber

261
00:09:47,120 --> 00:09:51,519
attacks, weapon development or use, and

262
00:09:48,800 --> 00:09:53,839
mass harm. An example from the incident

263
00:09:51,519 --> 00:09:55,279
tracker is Amazon recommending explosive

264
00:09:53,839 --> 00:09:58,279
producing ingredients as frequently

265
00:09:55,279 --> 00:10:00,480
bought together items for

266
00:09:58,279 --> 00:10:02,959
chemicals. Second category of risk is

267
00:10:00,480 --> 00:10:04,560
systemic risk. This is AI possessing

268
00:10:02,959 --> 00:10:06,399
dangerous capabilities, by which we mean

269
00:10:04,560 --> 00:10:08,240
deception, weapons development and

270
00:10:06,399 --> 00:10:10,320
acquisition, manipulation, cyber

271
00:10:08,240 --> 00:10:12,800
offense, or AI development. An example

272
00:10:10,320 --> 00:10:16,399
of this was a taser manufacturer which

273
00:10:12,800 --> 00:10:18,000
proposed um AI enabled drones that were

274
00:10:16,399 --> 00:10:20,800
equipped with tasers that would

275
00:10:18,000 --> 00:10:22,120
supposedly defend um US high schools

276
00:10:20,800 --> 00:10:24,480
against school

277
00:10:22,120 --> 00:10:26,399
shootings. Finally, the third category

278
00:10:24,480 --> 00:10:28,399
of risk is gradual disempowerment. This

279
00:10:26,399 --> 00:10:30,800
is defined as loss of human agency and

280
00:10:28,399 --> 00:10:32,320
autonomy. An example of this was Madison

281
00:10:30,800 --> 00:10:35,120
Square Garden in New York, which used

282
00:10:32,320 --> 00:10:36,880
facial recognition to identify and then

283
00:10:35,120 --> 00:10:39,040
deny entry to lawyers that were involved

284
00:10:36,880 --> 00:10:40,320
in litigation against them, including a

285
00:10:39,040 --> 00:10:42,880
mother who was just trying to go see a

286
00:10:40,320 --> 00:10:44,560
Rockhead show with her Girl Scout troop.

287
00:10:42,880 --> 00:10:47,040
So, the examples that I have right now

288
00:10:44,560 --> 00:10:48,160
might not seem too serious. However, you

289
00:10:47,040 --> 00:10:50,240
have to remember that these are

290
00:10:48,160 --> 00:10:52,240
incidents that already happened. They

291
00:10:50,240 --> 00:10:54,320
involve AI that we have today that is AI

292
00:10:52,240 --> 00:10:56,079
that is not super intelligent. So you

293
00:10:54,320 --> 00:10:58,640
can imagine that as AI capabilities

294
00:10:56,079 --> 00:11:01,120
improve the severity of these risks will

295
00:10:58,640 --> 00:11:01,120
evolve

296
00:11:01,320 --> 00:11:06,000
adequately. Now you may be thinking okay

297
00:11:04,560 --> 00:11:07,920
there's maybe some potential risks from

298
00:11:06,000 --> 00:11:09,839
AI but for every new technology there's

299
00:11:07,920 --> 00:11:11,760
bound to be some alarmist and lites.

300
00:11:09,839 --> 00:11:13,279
However the people who created and who

301
00:11:11,760 --> 00:11:14,959
are currently building AI are also

302
00:11:13,279 --> 00:11:16,959
worried about these risks. Let's hear

303
00:11:14,959 --> 00:11:18,800
from them. Here's Jeffrey Hinton the

304
00:11:16,959 --> 00:11:21,360
godfather of AI entering award laurette

305
00:11:18,800 --> 00:11:22,959
in 2023. The existential risk is what

306
00:11:21,360 --> 00:11:24,560
I'm worried about. The existential risk

307
00:11:22,959 --> 00:11:25,839
is that humanity gets wiped out because

308
00:11:24,560 --> 00:11:27,480
we've developed a better form of

309
00:11:25,839 --> 00:11:30,000
intelligence that decides to take

310
00:11:27,480 --> 00:11:32,399
control. Now, here's Sam Alman, CEO of

311
00:11:30,000 --> 00:11:34,000
OpenAI in 2015. Development of

312
00:11:32,399 --> 00:11:35,279
superhuman machine intelligence is

313
00:11:34,000 --> 00:11:37,680
probably the greatest threat to the

314
00:11:35,279 --> 00:11:40,560
continued existence of humanity. Okay,

315
00:11:37,680 --> 00:11:42,000
that was 2015. Well, in 2023, he said

316
00:11:40,560 --> 00:11:43,920
the bad case, and I think this is

317
00:11:42,000 --> 00:11:46,800
important to say, is like lights out for

318
00:11:43,920 --> 00:11:49,600
all of us. Finally, Dario Ammoday, CEO

319
00:11:46,800 --> 00:11:50,480
of Anthropic, in 2023. My chance that

320
00:11:49,600 --> 00:11:52,079
something goes really quite

321
00:11:50,480 --> 00:11:53,680
catastrophically wrong on the scale of

322
00:11:52,079 --> 00:11:56,120
human civilization might be somewhere

323
00:11:53,680 --> 00:11:58,959
between 10% and

324
00:11:56,120 --> 00:12:00,720
25%. Okay, hopefully I've convinced you

325
00:11:58,959 --> 00:12:02,720
that AI risk is something we should be

326
00:12:00,720 --> 00:12:04,079
pretty worried about. So, we would hope

327
00:12:02,720 --> 00:12:06,959
that quite a lot of people are doing

328
00:12:04,079 --> 00:12:08,560
work about this. Well, NGO, the Emerging

329
00:12:06,959 --> 00:12:10,160
Technology Institute, actually counted

330
00:12:08,560 --> 00:12:12,160
the total number of English language

331
00:12:10,160 --> 00:12:14,720
articles on AI published between 2018

332
00:12:12,160 --> 00:12:16,880
and 2023. Then they counted the articles

333
00:12:14,720 --> 00:12:18,800
that were specifically about AI safety.

334
00:12:16,880 --> 00:12:22,120
Can you guess the percentage of all AI

335
00:12:18,800 --> 00:12:24,200
articles that were focused on AI

336
00:12:22,120 --> 00:12:28,240
safety?

337
00:12:24,200 --> 00:12:30,480
2%. Yeah. So AI safety work only makes

338
00:12:28,240 --> 00:12:32,240
up a tiny fraction of work on AI. Now

339
00:12:30,480 --> 00:12:34,079
let me tell you about some of this 2%

340
00:12:32,240 --> 00:12:36,800
research. What are we doing about these

341
00:12:34,079 --> 00:12:38,399
risks at MIT? Well, at Future, we're

342
00:12:36,800 --> 00:12:40,720
starting a new initiative called the MIT

343
00:12:38,399 --> 00:12:41,839
AI risk index. The idea is that for

344
00:12:40,720 --> 00:12:43,360
every single risk in the risk

345
00:12:41,839 --> 00:12:44,959
repository, we're going to provide a

346
00:12:43,360 --> 00:12:46,639
risk profile with an assessment of the

347
00:12:44,959 --> 00:12:48,000
risk, relevance for different types of

348
00:12:46,639 --> 00:12:50,000
organizations, and then the most

349
00:12:48,000 --> 00:12:52,160
promising mitigations. We're going to be

350
00:12:50,000 --> 00:12:53,680
doing this by evaluating how over 200

351
00:12:52,160 --> 00:12:55,519
different organizations are responding

352
00:12:53,680 --> 00:12:56,639
to the different risks that they face.

353
00:12:55,519 --> 00:12:58,079
And then we're going to work with

354
00:12:56,639 --> 00:13:00,959
hundreds of different experts to help us

355
00:12:58,079 --> 00:13:02,639
evaluate these risks and responses. This

356
00:13:00,959 --> 00:13:04,320
is an example of what a risk profile

357
00:13:02,639 --> 00:13:05,680
would look like. So you have the name of

358
00:13:04,320 --> 00:13:07,920
the risk at the top. You have the

359
00:13:05,680 --> 00:13:09,519
description, expert risk assessment and

360
00:13:07,920 --> 00:13:11,680
relevance to ecosystem actors which

361
00:13:09,519 --> 00:13:14,079
would be AI developers, large companies

362
00:13:11,680 --> 00:13:15,279
deploying AI, government and consumers.

363
00:13:14,079 --> 00:13:17,240
And then finally at the bottom you have

364
00:13:15,279 --> 00:13:20,320
the most promising mitigations with

365
00:13:17,240 --> 00:13:22,000
details. I'm also a part of the MIT AI

366
00:13:20,320 --> 00:13:24,560
alignment group at MIT which is known as

367
00:13:22,000 --> 00:13:26,000
MIA. MIA brings together over a 100

368
00:13:24,560 --> 00:13:27,920
different students and researchers at

369
00:13:26,000 --> 00:13:30,560
MIT that are conducting research to try

370
00:13:27,920 --> 00:13:32,480
to reduce the risk from advanced AI. We

371
00:13:30,560 --> 00:13:34,720
work on both technical and governance.

372
00:13:32,480 --> 00:13:36,800
So that means um policy research related

373
00:13:34,720 --> 00:13:39,519
to AI safety and some of our published

374
00:13:36,800 --> 00:13:41,120
work can be found on our website. Maya

375
00:13:39,519 --> 00:13:42,880
also organizes weekly meetings as well

376
00:13:41,120 --> 00:13:44,399
as educational programs for students to

377
00:13:42,880 --> 00:13:46,480
develop knowledge and skills in AI

378
00:13:44,399 --> 00:13:47,959
safety which brings hundreds of students

379
00:13:46,480 --> 00:13:51,040
from MIT every

380
00:13:47,959 --> 00:13:52,880
year. Maya also plays an active role not

381
00:13:51,040 --> 00:13:55,040
only in the MIT community or the Boston

382
00:13:52,880 --> 00:13:56,800
area but beyond. And last month four

383
00:13:55,040 --> 00:13:58,320
Maya members actually went to DC and

384
00:13:56,800 --> 00:13:59,760
presented to policy makers at a

385
00:13:58,320 --> 00:14:01,360
congressional exhibition for advanced

386
00:13:59,760 --> 00:14:02,720
AI. and some of them are in the room

387
00:14:01,360 --> 00:14:05,240
right now if you want to talk to them

388
00:14:02,720 --> 00:14:08,880
about it.

389
00:14:05,240 --> 00:14:11,040
Um finally

390
00:14:08,880 --> 00:14:13,120
um some Maya members are working on a

391
00:14:11,040 --> 00:14:15,040
new initiative called the MIT AI Safety

392
00:14:13,120 --> 00:14:16,639
Institute or Macy and the idea is to

393
00:14:15,040 --> 00:14:18,000
foster better collaboration between the

394
00:14:16,639 --> 00:14:20,000
different labs and individuals that are

395
00:14:18,000 --> 00:14:21,399
working on AI safety at MIT but might be

396
00:14:20,000 --> 00:14:24,800
kind of

397
00:14:21,399 --> 00:14:26,959
isolated. Okay. Based on all of this

398
00:14:24,800 --> 00:14:28,959
interest and work that's being done at

399
00:14:26,959 --> 00:14:30,959
MIT on AI safety, I truly believe that

400
00:14:28,959 --> 00:14:33,120
MIT stands to be one of the global hubs

401
00:14:30,959 --> 00:14:34,199
for independent review and work on AI

402
00:14:33,120 --> 00:14:36,240
risk

403
00:14:34,199 --> 00:14:37,920
mitigation. What are the next steps for

404
00:14:36,240 --> 00:14:39,839
you? If anything I've said during this

405
00:14:37,920 --> 00:14:41,920
talk piqu your interest, I invite you to

406
00:14:39,839 --> 00:14:43,760
read more about AI risks. A good place

407
00:14:41,920 --> 00:14:46,399
to start is the International AI Safety

408
00:14:43,760 --> 00:14:47,600
Report. This was compiled by almost a

409
00:14:46,399 --> 00:14:48,800
hundred different experts from all

410
00:14:47,600 --> 00:14:50,880
around the world and published in

411
00:14:48,800 --> 00:14:53,920
January of this year. and it's a good

412
00:14:50,880 --> 00:14:55,440
overview of different risks from AI. I

413
00:14:53,920 --> 00:14:57,199
also invite you to browse through the AI

414
00:14:55,440 --> 00:14:59,760
risk repository and the AI incident

415
00:14:57,199 --> 00:15:01,600
tracker from Future Techch. Finally, I'm

416
00:14:59,760 --> 00:15:04,160
happy to answer any questions on the MIT

417
00:15:01,600 --> 00:15:05,839
AI alignment group or on the Future AI

418
00:15:04,160 --> 00:15:07,920
risk index which is currently open to

419
00:15:05,839 --> 00:15:08,930
support and collaboration. Thank you

420
00:15:07,920 --> 00:15:17,399
very much for your

421
00:15:08,930 --> 00:15:21,199
[Applause]

422
00:15:17,399 --> 00:15:21,199
time first.

423
00:15:23,360 --> 00:15:27,199
Okay, one of the questions is, how do

424
00:15:25,120 --> 00:15:28,880
you justify prioritizing AI existential

425
00:15:27,199 --> 00:15:30,560
risk over immediate harms like bias and

426
00:15:28,880 --> 00:15:31,920
misinformation given the lack of

427
00:15:30,560 --> 00:15:33,839
concrete evidence for a doomsday

428
00:15:31,920 --> 00:15:35,120
scenario? Hopefully, I've convinced you,

429
00:15:33,839 --> 00:15:37,040
and if you're not convinced enough, you

430
00:15:35,120 --> 00:15:39,279
can go into the AI incident tracker that

431
00:15:37,040 --> 00:15:43,440
there is at least some evidence um that

432
00:15:39,279 --> 00:15:45,560
we may have existential risk um in the

433
00:15:43,440 --> 00:15:49,120
sometime in the future.

434
00:15:45,560 --> 00:15:50,800
Um, I guess what I'll say here is that

435
00:15:49,120 --> 00:15:52,800
quite a lot of attention, especially in

436
00:15:50,800 --> 00:15:54,639
the mid2010s, was already being paid to

437
00:15:52,800 --> 00:15:57,199
bias and misinformation and there's

438
00:15:54,639 --> 00:15:58,880
enough people working on it. Even if

439
00:15:57,199 --> 00:16:00,240
there's only a really small chance that

440
00:15:58,880 --> 00:16:02,560
something really catastrophic could

441
00:16:00,240 --> 00:16:05,920
happen, um, it's sort of like with

442
00:16:02,560 --> 00:16:07,360
nuclear weapons, you want enough people

443
00:16:05,920 --> 00:16:08,880
working on this and thinking about how

444
00:16:07,360 --> 00:16:10,399
we can have global coordination on this

445
00:16:08,880 --> 00:16:12,800
to ensure that even if there's only a

446
00:16:10,399 --> 00:16:16,639
small chance this will happen, it won't

447
00:16:12,800 --> 00:16:20,000
happen. hopefully. So, we have some

448
00:16:16,639 --> 00:16:20,000
people working on this.

449
00:16:23,959 --> 00:16:29,360
Um, okay. I can't figure out how to pass

450
00:16:27,600 --> 00:16:30,959
questions, so I'll just answer this. As

451
00:16:29,360 --> 00:16:32,560
a student, what is your opinion on how

452
00:16:30,959 --> 00:16:34,160
the industry can help you cope with the

453
00:16:32,560 --> 00:16:35,480
legacy of AI we are leaving you to

454
00:16:34,160 --> 00:16:38,639
figure

455
00:16:35,480 --> 00:16:41,360
out? It's pretty exciting. It's a good

456
00:16:38,639 --> 00:16:44,320
moment to be studying CS and economics

457
00:16:41,360 --> 00:16:44,320
at MIT.

458
00:16:46,279 --> 00:16:50,480
Um, I think it's a problem that we all

459
00:16:48,880 --> 00:16:54,399
have to face. As we've seen, everyone is

460
00:16:50,480 --> 00:16:54,399
concerned in some way or another.

461
00:16:55,560 --> 00:16:59,600
Um, it's really a unique moment in

462
00:16:57,839 --> 00:17:01,360
history and I guess I'm glad that I get

463
00:16:59,600 --> 00:17:03,759
to work on it or even talk to you about

464
00:17:01,360 --> 00:17:07,360
it and even have maybe a very small say

465
00:17:03,759 --> 00:17:10,919
in it by doing research on this. Yes.

466
00:17:07,360 --> 00:17:10,919
Thank you very much.

