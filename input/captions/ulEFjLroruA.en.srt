1
00:00:00,719 --> 00:00:05,440
Thank you everyone for joining us um

2
00:00:02,960 --> 00:00:07,040
this afternoon. Uh welcome to our next

3
00:00:05,440 --> 00:00:10,320
installment of the equity and biio

4
00:00:07,040 --> 00:00:12,000
medicine seminar or EBM. Uh the EBM

5
00:00:10,320 --> 00:00:14,160
series is an interactive meeting that

6
00:00:12,000 --> 00:00:16,359
focuses on critical issues related to

7
00:00:14,160 --> 00:00:18,640
equity and representation in biomedical

8
00:00:16,359 --> 00:00:21,199
research especially as it applies to

9
00:00:18,640 --> 00:00:24,240
genomics and research genomics research

10
00:00:21,199 --> 00:00:26,080
and uh precision medicine. Speakers dive

11
00:00:24,240 --> 00:00:28,240
into the equal or unequal social

12
00:00:26,080 --> 00:00:30,560
implications of their work. uh really

13
00:00:28,240 --> 00:00:33,120
thinking about who's affected, how are

14
00:00:30,560 --> 00:00:35,200
they affected, and why. The overall goal

15
00:00:33,120 --> 00:00:37,520
of this series is to foster discussions

16
00:00:35,200 --> 00:00:40,079
about equity and inspire scientists and

17
00:00:37,520 --> 00:00:42,440
trainees and others in our community to

18
00:00:40,079 --> 00:00:46,320
reflect on the social impact of our own

19
00:00:42,440 --> 00:00:48,239
research. Uh the EBM series is sponsored

20
00:00:46,320 --> 00:00:51,200
by the idea office here at the Broad

21
00:00:48,239 --> 00:00:53,120
Institute of MIT in Harvard. Um, and for

22
00:00:51,200 --> 00:00:55,680
more information about the series or to

23
00:00:53,120 --> 00:00:57,920
make suggestions for future sessions,

24
00:00:55,680 --> 00:01:00,000
please reach out to our office. Uh, Lois

25
00:00:57,920 --> 00:01:02,480
will will be putting information into

26
00:01:00,000 --> 00:01:04,879
the chat there. Please note that EBM

27
00:01:02,480 --> 00:01:08,000
seminars are recorded and will be shared

28
00:01:04,879 --> 00:01:09,760
on Broad's YouTube site. to to ground

29
00:01:08,000 --> 00:01:13,280
our conversation, I just want to start

30
00:01:09,760 --> 00:01:16,400
what by presenting um kind of uh some

31
00:01:13,280 --> 00:01:18,880
data that can um lead us into what I

32
00:01:16,400 --> 00:01:23,360
think would be uh an interesting way to

33
00:01:18,880 --> 00:01:25,680
think about um uh um this topic thinking

34
00:01:23,360 --> 00:01:26,960
about building trust and understanding

35
00:01:25,680 --> 00:01:29,600
through accessible science

36
00:01:26,960 --> 00:01:31,360
communication. uh post pandemic there

37
00:01:29,600 --> 00:01:33,759
have been measurable declines in public

38
00:01:31,360 --> 00:01:36,240
trust in scientists as you can see here

39
00:01:33,759 --> 00:01:38,000
evidenced by the pe research study here

40
00:01:36,240 --> 00:01:40,079
in this slide um there have been

41
00:01:38,000 --> 00:01:44,640
reductions of about 10% between the

42
00:01:40,079 --> 00:01:46,079
years of 2020 and 2024 in response many

43
00:01:44,640 --> 00:01:48,079
have been thinking about ways to

44
00:01:46,079 --> 00:01:50,320
understand and address the public's

45
00:01:48,079 --> 00:01:52,560
waning confidence with numerous

46
00:01:50,320 --> 00:01:54,640
organizations um like ours making

47
00:01:52,560 --> 00:01:58,000
efforts to foster trust in science and

48
00:01:54,640 --> 00:01:59,920
enhance outreach to broader audiences is

49
00:01:58,000 --> 00:02:01,840
in today's session. We will focus on the

50
00:01:59,920 --> 00:02:03,600
role that accessible and inclusive

51
00:02:01,840 --> 00:02:06,880
science communication can play in this

52
00:02:03,600 --> 00:02:09,399
work. And now it's my honor to introduce

53
00:02:06,880 --> 00:02:12,160
our panelists for the

54
00:02:09,399 --> 00:02:14,640
discussion. Uh first we have Brian

55
00:02:12,160 --> 00:02:17,599
Southwell. Uh Brian is a distinguished

56
00:02:14,640 --> 00:02:19,879
fellow at um RTI International where he

57
00:02:17,599 --> 00:02:22,400
oversees research to assess risk

58
00:02:19,879 --> 00:02:24,720
perceptions, mental models of scientific

59
00:02:22,400 --> 00:02:28,480
concepts and public trust in science and

60
00:02:24,720 --> 00:02:30,640
scientists. Um, Brian also is an adject

61
00:02:28,480 --> 00:02:32,640
professor of internal medicine with Duke

62
00:02:30,640 --> 00:02:35,360
University where he co-founded the Duke

63
00:02:32,640 --> 00:02:36,959
program on medical misinformation and is

64
00:02:35,360 --> 00:02:39,440
a faculty affiliate with the Duke

65
00:02:36,959 --> 00:02:41,519
Initiative for Science and Society. In

66
00:02:39,440 --> 00:02:44,480
addition, Brian is an adject associate

67
00:02:41,519 --> 00:02:46,720
professor um with at UNC Chapel Hills

68
00:02:44,480 --> 00:02:49,760
Gilling School of Global Public Health

69
00:02:46,720 --> 00:02:53,840
and an adject faculty uh member at the

70
00:02:49,760 --> 00:02:56,160
University of Delaware. Uh at RTI, Brian

71
00:02:53,840 --> 00:02:58,319
co-leads um the All of Us Researcher

72
00:02:56,160 --> 00:02:59,959
Academy to improve equity and scholarly

73
00:02:58,319 --> 00:03:02,720
publishing to the National Institutes of

74
00:02:59,959 --> 00:03:04,800
Health. He's an um active participating

75
00:03:02,720 --> 00:03:06,720
in uh several efforts to address public

76
00:03:04,800 --> 00:03:09,200
understanding of science through peer

77
00:03:06,720 --> 00:03:11,840
reviewed publications as well as public

78
00:03:09,200 --> 00:03:14,959
commentary talks and venues such as the

79
00:03:11,840 --> 00:03:17,200
Aspen Ideas Festival. Um he advises for

80
00:03:14,959 --> 00:03:19,599
projects such as NOVA science studio and

81
00:03:17,200 --> 00:03:21,680
as um and a consensus study for the

82
00:03:19,599 --> 00:03:24,080
nationalmies of science and engineering

83
00:03:21,680 --> 00:03:26,720
and medicine. His research appears in

84
00:03:24,080 --> 00:03:29,120
more than 150 journal articles and a

85
00:03:26,720 --> 00:03:30,959
chapters in um in in several books

86
00:03:29,120 --> 00:03:32,920
including measuring everyday life

87
00:03:30,959 --> 00:03:36,480
talking about research and why it

88
00:03:32,920 --> 00:03:40,080
matters. Um thanks so much Brian for

89
00:03:36,480 --> 00:03:42,879
joining us today. Uh our next panelist

90
00:03:40,080 --> 00:03:44,799
will be Sheniqua Collier um is uh who's

91
00:03:42,879 --> 00:03:46,400
an associate professor with tenure in

92
00:03:44,799 --> 00:03:48,040
the department of clinical research and

93
00:03:46,400 --> 00:03:50,440
leadership at the George Washington

94
00:03:48,040 --> 00:03:52,720
University of medicine and health

95
00:03:50,440 --> 00:03:54,159
sciences. She has also served as a

96
00:03:52,720 --> 00:03:55,920
special volunteer at the center for

97
00:03:54,159 --> 00:03:59,120
research on genomics and global health

98
00:03:55,920 --> 00:04:02,239
at the national human um genome research

99
00:03:59,120 --> 00:04:04,640
institute NI at the NIH for over 13

100
00:04:02,239 --> 00:04:06,879
years. She has nearly two decades of

101
00:04:04,640 --> 00:04:09,400
experience analyzing the ethical, legal

102
00:04:06,879 --> 00:04:11,519
and social implications of emerging

103
00:04:09,400 --> 00:04:14,000
technologies which includes her time as

104
00:04:11,519 --> 00:04:16,639
a post-doal scholar at the center for

105
00:04:14,000 --> 00:04:18,720
genetic research ethics and law at Case

106
00:04:16,639 --> 00:04:21,040
Western Reserve University School of

107
00:04:18,720 --> 00:04:23,040
Medicine. She works on various projects

108
00:04:21,040 --> 00:04:25,440
funded by the US National Institutes of

109
00:04:23,040 --> 00:04:27,680
Health and with a focus on LC of

110
00:04:25,440 --> 00:04:30,800
genomics research and data science.

111
00:04:27,680 --> 00:04:33,639
Thanks so much Fenikua for joining us.

112
00:04:30,800 --> 00:04:36,560
And last but not least, um our own

113
00:04:33,639 --> 00:04:38,400
Namita uh Sang Gupta, uh who's an

114
00:04:36,560 --> 00:04:40,160
associate director for scientific

115
00:04:38,400 --> 00:04:42,960
engagement, policy, and advocacy at the

116
00:04:40,160 --> 00:04:44,639
Broad Institute. Um based in Bro's

117
00:04:42,960 --> 00:04:46,560
office of communication and government

118
00:04:44,639 --> 00:04:48,479
relations, she develops and leads

119
00:04:46,560 --> 00:04:50,960
programs and community partnerships to

120
00:04:48,479 --> 00:04:53,199
advance Bro's public engagement efforts

121
00:04:50,960 --> 00:04:54,880
and organizes strategic thinking and

122
00:04:53,199 --> 00:04:57,280
connectivity on matters concerning

123
00:04:54,880 --> 00:05:00,000
government relations and public pro

124
00:04:57,280 --> 00:05:01,680
policy across the institute. Namtha

125
00:05:00,000 --> 00:05:03,600
facilitates Broad's relationships with

126
00:05:01,680 --> 00:05:05,919
neighboring communities, local, state,

127
00:05:03,600 --> 00:05:08,560
and federal government, international

128
00:05:05,919 --> 00:05:10,320
delegations, and the general public. She

129
00:05:08,560 --> 00:05:12,000
is also responsible for overseeing the

130
00:05:10,320 --> 00:05:15,080
Broad Discovery Center and related

131
00:05:12,000 --> 00:05:18,000
public outreach and educational

132
00:05:15,080 --> 00:05:20,000
efforts. Um, Narmtha holds a PhD in

133
00:05:18,000 --> 00:05:22,560
environmental toxicology from Clemson

134
00:05:20,000 --> 00:05:24,960
University and a mass uh uh MS in

135
00:05:22,560 --> 00:05:26,720
biotechnology from Bangalore University

136
00:05:24,960 --> 00:05:28,960
and a BS in chemistry from the

137
00:05:26,720 --> 00:05:31,759
University of Kolkata. Thanks so much

138
00:05:28,960 --> 00:05:33,759
for joining us, Namtha. And uh with

139
00:05:31,759 --> 00:05:37,039
those introductions, I would like to

140
00:05:33,759 --> 00:05:39,840
pass um it over to Brian. We will start

141
00:05:37,039 --> 00:05:42,479
with a few grounding talks before we um

142
00:05:39,840 --> 00:05:45,759
enter a panel discussion on the topic.

143
00:05:42,479 --> 00:05:48,800
Um and then we'll end the last uh 15 or

144
00:05:45,759 --> 00:05:50,720
so minutes. We'll um save for questions

145
00:05:48,800 --> 00:05:52,320
from the audience. So please, if you

146
00:05:50,720 --> 00:05:56,240
have those, share them in the chat with

147
00:05:52,320 --> 00:05:58,240
us. Um with further without further ado,

148
00:05:56,240 --> 00:06:00,800
I'll pass it over to Brian.

149
00:05:58,240 --> 00:06:03,360
Thanks so much and I I really appreciate

150
00:06:00,800 --> 00:06:05,039
the broad interest in this topic and and

151
00:06:03,360 --> 00:06:07,360
willingness to have this discussion.

152
00:06:05,039 --> 00:06:09,360
It's a really important one. So um I

153
00:06:07,360 --> 00:06:11,360
wanted to offer just a couple of initial

154
00:06:09,360 --> 00:06:14,319
remarks just to help set the stage um

155
00:06:11,360 --> 00:06:16,639
you know for uh this discussion um and

156
00:06:14,319 --> 00:06:17,840
then I'll pass it along to my colleagues

157
00:06:16,639 --> 00:06:20,880
as well. But I'm just going to share

158
00:06:17,840 --> 00:06:24,000
screen here for a moment. offer just a

159
00:06:20,880 --> 00:06:26,160
few thoughts uh that might help to um

160
00:06:24,000 --> 00:06:27,919
offer a foundation for our exploration

161
00:06:26,160 --> 00:06:30,479
of of this larger you know discussion of

162
00:06:27,919 --> 00:06:32,240
of trust. So you know there are a lot of

163
00:06:30,479 --> 00:06:34,560
questions that we often will come to a

164
00:06:32,240 --> 00:06:36,639
discussion like this with uh you know

165
00:06:34,560 --> 00:06:39,039
related to uh you whether or not various

166
00:06:36,639 --> 00:06:41,360
communities may or may not trust um you

167
00:06:39,039 --> 00:06:43,199
know scientific institutions. um we

168
00:06:41,360 --> 00:06:45,199
might get to a point of of asking

169
00:06:43,199 --> 00:06:46,560
questions about a sense of connection

170
00:06:45,199 --> 00:06:48,319
you know to those those different

171
00:06:46,560 --> 00:06:50,080
institutions but I think it's also

172
00:06:48,319 --> 00:06:52,160
important for us to step back um and

173
00:06:50,080 --> 00:06:53,440
both look at the landscape as it exists

174
00:06:52,160 --> 00:06:55,919
um in terms of the information

175
00:06:53,440 --> 00:06:57,520
environment and um to also think a bit

176
00:06:55,919 --> 00:06:59,520
you know historically um you know as

177
00:06:57,520 --> 00:07:01,840
well and so let's just do that for just

178
00:06:59,520 --> 00:07:03,199
a second um and then kind of move ahead

179
00:07:01,840 --> 00:07:04,960
I I think it's important for us

180
00:07:03,199 --> 00:07:07,199
conceptually to also ask questions about

181
00:07:04,960 --> 00:07:08,960
you what we mean by some of these words

182
00:07:07,199 --> 00:07:10,479
uh and and I think we can um you do that

183
00:07:08,960 --> 00:07:12,479
in a productive

184
00:07:10,479 --> 00:07:14,400
So um you with regards to our

185
00:07:12,479 --> 00:07:15,680
information environment um this won't

186
00:07:14,400 --> 00:07:17,759
necessarily be surprising to you but I

187
00:07:15,680 --> 00:07:19,520
think it is worthwhile to just you know

188
00:07:17,759 --> 00:07:21,840
take stock of this that you know we are

189
00:07:19,520 --> 00:07:23,919
in a landscape right now where um you

190
00:07:21,840 --> 00:07:25,440
know larger you know institutions

191
00:07:23,919 --> 00:07:27,280
themselves sometimes are sources of

192
00:07:25,440 --> 00:07:29,039
information. Um but in the larger

193
00:07:27,280 --> 00:07:31,360
information landscape they are often

194
00:07:29,039 --> 00:07:33,039
competing with smaller more nimble you

195
00:07:31,360 --> 00:07:34,160
know organizations or even individuals

196
00:07:33,039 --> 00:07:36,400
you know sometimes and those

197
00:07:34,160 --> 00:07:37,919
organizations and individuals themselves

198
00:07:36,400 --> 00:07:40,479
may or may not be professionally

199
00:07:37,919 --> 00:07:43,120
trained. It may or may not be aligned

200
00:07:40,479 --> 00:07:45,120
with a formal um you know news outlet or

201
00:07:43,120 --> 00:07:47,759
a scientific scientific institution. And

202
00:07:45,120 --> 00:07:49,840
yet um you know through social media

203
00:07:47,759 --> 00:07:50,960
through the internet there are ways in

204
00:07:49,840 --> 00:07:53,039
which um you know those types of

205
00:07:50,960 --> 00:07:55,120
organizations also are contributing um

206
00:07:53,039 --> 00:07:57,680
and sometimes complicating um you know

207
00:07:55,120 --> 00:07:59,759
the landscape. We've seen in past

208
00:07:57,680 --> 00:08:02,080
decades a shift from um you know

209
00:07:59,759 --> 00:08:03,680
broadcast to uh you know various um you

210
00:08:02,080 --> 00:08:05,840
know other you know channels different

211
00:08:03,680 --> 00:08:07,680
ways. What that means for us is that we

212
00:08:05,840 --> 00:08:09,520
may not at any one given point in time

213
00:08:07,680 --> 00:08:11,840
know what community members are

214
00:08:09,520 --> 00:08:14,160
necessarily you know watching what it is

215
00:08:11,840 --> 00:08:16,160
that um you audiences see and hear and

216
00:08:14,160 --> 00:08:18,160
encounter isn't necessarily always

217
00:08:16,160 --> 00:08:20,160
easily um you know assessed. We've moved

218
00:08:18,160 --> 00:08:22,400
away from a landscape where they're just

219
00:08:20,160 --> 00:08:24,080
a handful of of network um you know

220
00:08:22,400 --> 00:08:26,479
broadcast channels towards something you

221
00:08:24,080 --> 00:08:28,960
know much more uh you know diffuse and

222
00:08:26,479 --> 00:08:30,960
diverse um in different ways and people

223
00:08:28,960 --> 00:08:33,039
are also inconstant social networks.

224
00:08:30,960 --> 00:08:35,120
some of those networks are better or

225
00:08:33,039 --> 00:08:36,880
less uh connected to um you know

226
00:08:35,120 --> 00:08:38,719
different information sources and those

227
00:08:36,880 --> 00:08:40,719
networks themselves may or may not be

228
00:08:38,719 --> 00:08:42,880
particularly uh identifiable or

229
00:08:40,719 --> 00:08:45,120
available to to institutions. So there's

230
00:08:42,880 --> 00:08:46,720
a really complicated landscape uh there

231
00:08:45,120 --> 00:08:50,000
that we've got to recognize people are

232
00:08:46,720 --> 00:08:51,360
operating in. Um and we also are seeing

233
00:08:50,000 --> 00:08:53,040
um you know artificial intelligence

234
00:08:51,360 --> 00:08:54,480
really shape and change that landscape

235
00:08:53,040 --> 00:08:56,640
in different ways both in terms of

236
00:08:54,480 --> 00:08:58,160
production of content but even um in

237
00:08:56,640 --> 00:09:00,080
different ways shaping people's

238
00:08:58,160 --> 00:09:01,440
experiences with something like a search

239
00:09:00,080 --> 00:09:03,760
engine um you know where now

240
00:09:01,440 --> 00:09:05,839
increasingly you've got results that are

241
00:09:03,760 --> 00:09:08,240
assisted by artificial intelligence um

242
00:09:05,839 --> 00:09:11,040
and are becoming uh simultaneously

243
00:09:08,240 --> 00:09:13,279
seemingly more seamless um you know and

244
00:09:11,040 --> 00:09:15,040
for people and summarized but yet people

245
00:09:13,279 --> 00:09:16,800
are maybe less aware of um all the

246
00:09:15,040 --> 00:09:18,040
different places where information might

247
00:09:16,800 --> 00:09:22,399
be being pulled

248
00:09:18,040 --> 00:09:24,399
from. So that landscape um also is often

249
00:09:22,399 --> 00:09:26,000
complicated and we'll and you'll hear

250
00:09:24,399 --> 00:09:27,920
many people worry about you know the

251
00:09:26,000 --> 00:09:29,839
extent to which it's cluttered by um

252
00:09:27,920 --> 00:09:33,360
misinformation, inaccurate information

253
00:09:29,839 --> 00:09:35,279
about science. And I that's uh an

254
00:09:33,360 --> 00:09:36,959
important aspect of of the landscape

255
00:09:35,279 --> 00:09:39,480
that people are dealing with. But it's

256
00:09:36,959 --> 00:09:42,640
also important um I think to recognize

257
00:09:39,480 --> 00:09:44,240
that uh it's not necessarily a new

258
00:09:42,640 --> 00:09:46,320
phenomenon um that we've actually been

259
00:09:44,240 --> 00:09:48,399
dealing with misinformation about health

260
00:09:46,320 --> 00:09:50,640
and medicine and science for quite some

261
00:09:48,399 --> 00:09:52,560
time. Uh you can you turn back a century

262
00:09:50,640 --> 00:09:54,880
or so ago, we were dealing with um you

263
00:09:52,560 --> 00:09:57,279
know the original um you know episode of

264
00:09:54,880 --> 00:09:59,120
snake oil um sales uh you know and

265
00:09:57,279 --> 00:10:00,959
promotions um that now has echoed

266
00:09:59,120 --> 00:10:03,279
through our our popular language such

267
00:10:00,959 --> 00:10:05,200
that a snake oil salesman or snake oil

268
00:10:03,279 --> 00:10:06,720
salesperson is something that you would

269
00:10:05,200 --> 00:10:08,480
not want to be called. Um but it

270
00:10:06,720 --> 00:10:10,399
actually has roots in real historical

271
00:10:08,480 --> 00:10:12,160
incident. Um we've been dealing with a

272
00:10:10,399 --> 00:10:14,720
fraudulent promotion of claims for quite

273
00:10:12,160 --> 00:10:17,360
some for quite some time. But it's not

274
00:10:14,720 --> 00:10:19,519
just bad actors and and folks you know

275
00:10:17,360 --> 00:10:21,519
promoting uh fraudulent claims outside

276
00:10:19,519 --> 00:10:24,079
of scientific institutions. Uh

277
00:10:21,519 --> 00:10:27,120
institutions that themselves have um you

278
00:10:24,079 --> 00:10:29,440
know supposedly been scientific um have

279
00:10:27,120 --> 00:10:31,200
often and or sometimes anyway um you

280
00:10:29,440 --> 00:10:32,720
know offered um you know information and

281
00:10:31,200 --> 00:10:34,640
conclusions that was really not rooted

282
00:10:32,720 --> 00:10:36,560
in the best you know available evidence.

283
00:10:34,640 --> 00:10:38,000
Um you we can look at you know in the

284
00:10:36,560 --> 00:10:40,000
early part of the 20th century for

285
00:10:38,000 --> 00:10:41,760
example the eugenics movement um you

286
00:10:40,000 --> 00:10:43,680
know in the United States where um you

287
00:10:41,760 --> 00:10:46,240
know formally scientific institutions

288
00:10:43,680 --> 00:10:48,560
were sometimes not universally but

289
00:10:46,240 --> 00:10:50,320
sometimes making claims that uh really

290
00:10:48,560 --> 00:10:51,880
of course were not really rooted in um

291
00:10:50,320 --> 00:10:54,800
particularly useful uh you know

292
00:10:51,880 --> 00:10:56,880
evidence. So I think we can see that

293
00:10:54,800 --> 00:10:58,320
this is a complicated environment. Lots

294
00:10:56,880 --> 00:10:59,760
of different authors you know in our

295
00:10:58,320 --> 00:11:01,600
information landscape lots of different

296
00:10:59,760 --> 00:11:03,839
places that we might look to for um

297
00:11:01,600 --> 00:11:05,040
remedy. Um, and it also is important for

298
00:11:03,839 --> 00:11:06,480
us to ask questions about what it is

299
00:11:05,040 --> 00:11:08,399
that we mean by trust. You know, we're

300
00:11:06,480 --> 00:11:10,480
here today to talk about trust. And I

301
00:11:08,399 --> 00:11:13,360
think trust itself is really importantly

302
00:11:10,480 --> 00:11:15,440
multifaceted in the line in the minds of

303
00:11:13,360 --> 00:11:17,120
um, you know, many audiences. Uh, trust

304
00:11:15,440 --> 00:11:18,880
often means for people a sense of

305
00:11:17,120 --> 00:11:20,959
intellectual credibility. It's our

306
00:11:18,880 --> 00:11:23,120
common sense use of it. And sometimes

307
00:11:20,959 --> 00:11:25,200
we'll get to a point of recognizing that

308
00:11:23,120 --> 00:11:26,959
you know consistency or reliability also

309
00:11:25,200 --> 00:11:28,640
matters. But something that I think gets

310
00:11:26,959 --> 00:11:30,640
left out a lot of these discussions is

311
00:11:28,640 --> 00:11:32,560
the idea that trust for many people is

312
00:11:30,640 --> 00:11:34,560
also a matter of perceived shared

313
00:11:32,560 --> 00:11:36,320
interest. You know, it may be the case

314
00:11:34,560 --> 00:11:38,079
that a particular person you're hearing

315
00:11:36,320 --> 00:11:39,839
talk um you has lots of letters after

316
00:11:38,079 --> 00:11:41,680
their name, uh they're perceived to be

317
00:11:39,839 --> 00:11:42,800
an expert in their field, but if you

318
00:11:41,680 --> 00:11:44,640
don't get the sense that they care

319
00:11:42,800 --> 00:11:46,480
whether or not you live or die, they may

320
00:11:44,640 --> 00:11:47,839
not necessarily be deemed trustworthy by

321
00:11:46,480 --> 00:11:50,000
you. And I think the same is true for

322
00:11:47,839 --> 00:11:51,519
institutions as well. So generally

323
00:11:50,000 --> 00:11:53,440
speaking, something that we might do is

324
00:11:51,519 --> 00:11:56,240
shift our our our stance here slightly

325
00:11:53,440 --> 00:11:57,839
to think about um issues of trust um

326
00:11:56,240 --> 00:11:59,440
less in terms of worrying about blaming

327
00:11:57,839 --> 00:12:01,279
those that don't necessarily trust us or

328
00:11:59,440 --> 00:12:02,800
trust institutions and more thinking

329
00:12:01,279 --> 00:12:05,360
about what could be done to build and

330
00:12:02,800 --> 00:12:07,040
maintain trust. Um and that's I think a

331
00:12:05,360 --> 00:12:08,959
really productive way to um you know

332
00:12:07,040 --> 00:12:12,720
think about you know this this uh

333
00:12:08,959 --> 00:12:14,480
discussion. Now alam noted here some of

334
00:12:12,720 --> 00:12:16,320
the the slippage we've seen with regards

335
00:12:14,480 --> 00:12:17,600
to trust in in science. But I also want

336
00:12:16,320 --> 00:12:20,560
to put that into larger historical

337
00:12:17,600 --> 00:12:22,399
context because relatively speaking we

338
00:12:20,560 --> 00:12:24,399
still have a fair amount of trust

339
00:12:22,399 --> 00:12:26,079
actually in many scientific institutions

340
00:12:24,399 --> 00:12:28,079
when you look at it compared to other

341
00:12:26,079 --> 00:12:30,639
types of institutions you know over and

342
00:12:28,079 --> 00:12:33,040
if you look at it over time um here's an

343
00:12:30,639 --> 00:12:34,959
example of a figure that appears in um a

344
00:12:33,040 --> 00:12:36,560
new consensus study which just appeared

345
00:12:34,959 --> 00:12:38,320
in December and I had the um privilege

346
00:12:36,560 --> 00:12:40,560
of being a part of that committee um

347
00:12:38,320 --> 00:12:42,720
that the nationalmy's um you just

348
00:12:40,560 --> 00:12:45,120
released and they helped to set the

349
00:12:42,720 --> 00:12:46,639
stage here as well by by noting that um

350
00:12:45,120 --> 00:12:48,320
we do need to worry about you know this

351
00:12:46,639 --> 00:12:49,920
issue of trust but yet it's also

352
00:12:48,320 --> 00:12:52,079
important to recognize that relatively

353
00:12:49,920 --> 00:12:55,440
speaking trust in institutions has been

354
00:12:52,079 --> 00:12:57,200
has been high so we can ask I think

355
00:12:55,440 --> 00:12:59,279
important questions though about what

356
00:12:57,200 --> 00:13:00,800
ways in which people might uh ways in

357
00:12:59,279 --> 00:13:02,240
which people might differ with regards

358
00:13:00,800 --> 00:13:04,320
to trust what are some predictors of

359
00:13:02,240 --> 00:13:05,680
trust and here's where I think we might

360
00:13:04,320 --> 00:13:07,920
um you move into a really interesting

361
00:13:05,680 --> 00:13:10,639
part of our discussion something that we

362
00:13:07,920 --> 00:13:12,079
pointed out um another report that I had

363
00:13:10,639 --> 00:13:13,920
a chance to be a part of for the

364
00:13:12,079 --> 00:13:16,160
national science foundation uh their

365
00:13:13,920 --> 00:13:19,680
science and engineering indicator 's um

366
00:13:16,160 --> 00:13:22,560
report of spring 2024 uh noted that when

367
00:13:19,680 --> 00:13:24,320
you look at um confidence in scientists,

368
00:13:22,560 --> 00:13:25,519
one of the predictive factors is the

369
00:13:24,320 --> 00:13:27,519
extent to which people actually

370
00:13:25,519 --> 00:13:28,959
understand the process of science, the

371
00:13:27,519 --> 00:13:31,120
extent to which they actually understand

372
00:13:28,959 --> 00:13:32,399
what it is that is constituted um by

373
00:13:31,120 --> 00:13:34,320
science rather than just thinking about

374
00:13:32,399 --> 00:13:35,760
team science. What extent is there

375
00:13:34,320 --> 00:13:37,680
transparency? Is there a sense that

376
00:13:35,760 --> 00:13:39,360
people have an understanding of what

377
00:13:37,680 --> 00:13:41,519
scientific research is all about and

378
00:13:39,360 --> 00:13:43,440
what it's like? And those that have um

379
00:13:41,519 --> 00:13:45,040
you know express more understanding

380
00:13:43,440 --> 00:13:46,959
actually tend to express more confidence

381
00:13:45,040 --> 00:13:48,399
in science. So one step forward for here

382
00:13:46,959 --> 00:13:49,760
this might be for us to offer more

383
00:13:48,399 --> 00:13:51,839
transparency about what it is that

384
00:13:49,760 --> 00:13:54,160
actually is happening um in scientific

385
00:13:51,839 --> 00:13:55,600
institutions. But another aspect of this

386
00:13:54,160 --> 00:13:58,320
really builds on the earlier point that

387
00:13:55,600 --> 00:14:00,399
I just raised. We've got to find ways to

388
00:13:58,320 --> 00:14:01,600
think about um perhaps pointing out

389
00:14:00,399 --> 00:14:03,040
those connections, those shared

390
00:14:01,600 --> 00:14:04,480
interests that do exist between

391
00:14:03,040 --> 00:14:06,720
different communities and different

392
00:14:04,480 --> 00:14:08,800
institutions. That's a question for us

393
00:14:06,720 --> 00:14:11,120
then here is not just how who who should

394
00:14:08,800 --> 00:14:12,959
we blame for losses and trust but rather

395
00:14:11,120 --> 00:14:14,560
what can we do to actually identify and

396
00:14:12,959 --> 00:14:16,000
uplift shared interests between

397
00:14:14,560 --> 00:14:17,519
organizations and people outside of

398
00:14:16,000 --> 00:14:19,199
those organizations. That's something

399
00:14:17,519 --> 00:14:20,800
that we often don't spend enough time

400
00:14:19,199 --> 00:14:22,800
doing and I think it's something that we

401
00:14:20,800 --> 00:14:24,399
could actually productively do here um

402
00:14:22,800 --> 00:14:26,079
together. So I think that's something

403
00:14:24,399 --> 00:14:27,920
that's going to um it be useful to keep

404
00:14:26,079 --> 00:14:29,519
in mind for a lot of the um points that

405
00:14:27,920 --> 00:14:31,279
we want to raise. I'm happy to raise

406
00:14:29,519 --> 00:14:33,440
more um a little bit later, but I want

407
00:14:31,279 --> 00:14:35,440
to pass it now to Sheniqua for some

408
00:14:33,440 --> 00:14:39,360
additional thoughts.

409
00:14:35,440 --> 00:14:41,360
Okay, thank you. Great. Good afternoon

410
00:14:39,360 --> 00:14:43,920
everyone and thanks to the Broad

411
00:14:41,360 --> 00:14:46,880
Institute and my co-panelists for the

412
00:14:43,920 --> 00:14:49,680
opportunity to engage with all of you. I

413
00:14:46,880 --> 00:14:52,639
don't have any conflicts of interest and

414
00:14:49,680 --> 00:14:54,800
opinions expressed are my own. My short

415
00:14:52,639 --> 00:14:58,240
remarks will focus on the link between

416
00:14:54,800 --> 00:15:01,519
actions and messaging. Do our actions

417
00:14:58,240 --> 00:15:04,040
support or undermine what we communicate

418
00:15:01,519 --> 00:15:06,560
to build trust? I want to talk

419
00:15:04,040 --> 00:15:08,800
specifically about how transparency

420
00:15:06,560 --> 00:15:11,120
regarding the scientific process can

421
00:15:08,800 --> 00:15:12,720
enhance trust in science and pick up on

422
00:15:11,120 --> 00:15:15,680
threads Brian introduced about

423
00:15:12,720 --> 00:15:18,079
transparency and also shared interest.

424
00:15:15,680 --> 00:15:19,680
First, scientists generally acknowledge

425
00:15:18,079 --> 00:15:21,360
that we should emphasize our common

426
00:15:19,680 --> 00:15:24,800
interests. Uh Brian talked about shared

427
00:15:21,360 --> 00:15:26,079
interests as scientists, communities and

428
00:15:24,800 --> 00:15:27,760
scientists who are members of

429
00:15:26,079 --> 00:15:29,760
communities. The importance of

430
00:15:27,760 --> 00:15:31,600
understanding and prioritizing the

431
00:15:29,760 --> 00:15:33,920
research needs of different communities

432
00:15:31,600 --> 00:15:36,320
has been highlighted in the literature.

433
00:15:33,920 --> 00:15:38,800
However, investment in these areas

434
00:15:36,320 --> 00:15:41,360
should be monitored, assessed, and

435
00:15:38,800 --> 00:15:44,720
increased to prevent operation in a

436
00:15:41,360 --> 00:15:47,279
scientific va vacuum or overly top- down

437
00:15:44,720 --> 00:15:49,199
approaches. Otherwise, how can we

438
00:15:47,279 --> 00:15:51,360
identify our shared interests? How can

439
00:15:49,199 --> 00:15:54,079
we grasp the nuances and complexity

440
00:15:51,360 --> 00:15:56,639
surrounding them? Second, scientists

441
00:15:54,079 --> 00:15:58,639
with abundant funding sometimes treat

442
00:15:56,639 --> 00:16:01,360
community partners or researchers

443
00:15:58,639 --> 00:16:03,800
working in low resource settings merely

444
00:16:01,360 --> 00:16:06,639
as data collectors rather than as

445
00:16:03,800 --> 00:16:09,920
partners. Here is a specific example

446
00:16:06,639 --> 00:16:11,680
from Dr. Ambrose Wankam who researched

447
00:16:09,920 --> 00:16:13,920
what happened to collaborations between

448
00:16:11,680 --> 00:16:16,240
scientists in Cameroon and external

449
00:16:13,920 --> 00:16:18,240
scientists. He found that few African

450
00:16:16,240 --> 00:16:20,000
researchers and institutions were

451
00:16:18,240 --> 00:16:22,399
involved in priority setting and

452
00:16:20,000 --> 00:16:24,959
authorship. research topics did not

453
00:16:22,399 --> 00:16:26,880
align with local health priorities.

454
00:16:24,959 --> 00:16:29,519
Almost all DNA samples were stored

455
00:16:26,880 --> 00:16:32,160
outside of the African continent and no

456
00:16:29,519 --> 00:16:32,839
studies addressed the ethnic diversity

457
00:16:32,160 --> 00:16:35,040
in

458
00:16:32,839 --> 00:16:37,199
Cameroon. I'm concerned that if we

459
00:16:35,040 --> 00:16:39,120
continue to see a divide between

460
00:16:37,199 --> 00:16:40,959
scientists and our partners from

461
00:16:39,120 --> 00:16:42,399
marginalized and underrepresented

462
00:16:40,959 --> 00:16:44,399
communities, including those who are

463
00:16:42,399 --> 00:16:45,759
also scientists, it will limit the

464
00:16:44,399 --> 00:16:48,800
opportunity for equity in these

465
00:16:45,759 --> 00:16:51,360
partnerships. So, how can we adjust our

466
00:16:48,800 --> 00:16:53,199
communications and also our actions to

467
00:16:51,360 --> 00:16:56,320
reduce the negative impact of power

468
00:16:53,199 --> 00:16:58,800
dynamics and imbalanced resources while

469
00:16:56,320 --> 00:17:00,880
also fostering trust and adhering to

470
00:16:58,800 --> 00:17:02,880
important research ethics practices?

471
00:17:00,880 --> 00:17:05,199
Additionally, how can we show that we

472
00:17:02,880 --> 00:17:07,919
value community input about what is

473
00:17:05,199 --> 00:17:09,520
ethical and equitable? Importantly,

474
00:17:07,919 --> 00:17:11,919
research structures and frameworks

475
00:17:09,520 --> 00:17:16,160
lacking transparency and accountability

476
00:17:11,919 --> 00:17:17,919
undermine our efforts to build trust.

477
00:17:16,160 --> 00:17:20,079
We have been ringing the alarm bells

478
00:17:17,919 --> 00:17:22,559
about the urgent need to recruit diverse

479
00:17:20,079 --> 00:17:25,039
populations and groups to research for a

480
00:17:22,559 --> 00:17:26,559
long time. And we often communicate to

481
00:17:25,039 --> 00:17:28,960
the populations and groups we are

482
00:17:26,559 --> 00:17:31,360
targeting that we need them and their

483
00:17:28,960 --> 00:17:33,840
samples and data in order for everyone

484
00:17:31,360 --> 00:17:35,840
globally to benefit from the promises of

485
00:17:33,840 --> 00:17:38,000
genomics medicine and genomics research.

486
00:17:35,840 --> 00:17:40,720
And that's the area I'm in. So I'll be

487
00:17:38,000 --> 00:17:43,039
using genomics examples. Um there's our

488
00:17:40,720 --> 00:17:44,960
shared interest. And I think this is

489
00:17:43,039 --> 00:17:47,679
true. I've argued for inclusion as well

490
00:17:44,960 --> 00:17:51,080
with my geneticist colleagues. Um, our

491
00:17:47,679 --> 00:17:53,280
field has spent less time on improving

492
00:17:51,080 --> 00:17:56,039
transparency and accountability

493
00:17:53,280 --> 00:17:59,160
mechanisms to ensure that the scientific

494
00:17:56,039 --> 00:18:02,320
practices align with our community

495
00:17:59,160 --> 00:18:06,480
messages. I there's real concern that we

496
00:18:02,320 --> 00:18:08,559
treat equity as if it is in inevitable

497
00:18:06,480 --> 00:18:10,000
and we neglect transparency and

498
00:18:08,559 --> 00:18:12,720
accountability in our research

499
00:18:10,000 --> 00:18:15,440
processes. We are all facing the

500
00:18:12,720 --> 00:18:17,600
possibility of declining resources and

501
00:18:15,440 --> 00:18:19,840
the United States and other countries

502
00:18:17,600 --> 00:18:21,919
grapple with discrimination, power

503
00:18:19,840 --> 00:18:24,400
disparities and shifting political

504
00:18:21,919 --> 00:18:26,960
systems. So equity in any environment

505
00:18:24,400 --> 00:18:26,960
cannot be

506
00:18:27,000 --> 00:18:33,520
guaranteed. Here is a point um an

507
00:18:30,320 --> 00:18:35,919
on-point example. In 2020, Ben Egan and

508
00:18:33,520 --> 00:18:38,400
colleagues reported in Nature that even

509
00:18:35,919 --> 00:18:40,960
when geneticists have access to diverse

510
00:18:38,400 --> 00:18:43,760
data, they commonly discard these data

511
00:18:40,960 --> 00:18:45,760
or use them only to replicate or confirm

512
00:18:43,760 --> 00:18:48,559
findings from studies focused on

513
00:18:45,760 --> 00:18:51,280
European ancestries. Usually, they found

514
00:18:48,559 --> 00:18:53,919
the authors do not offer a reason or

515
00:18:51,280 --> 00:18:55,840
justification why. In the few instances

516
00:18:53,919 --> 00:18:57,760
when authors provide their reasons for

517
00:18:55,840 --> 00:19:00,080
exclusion, the most common one is a fear

518
00:18:57,760 --> 00:19:01,840
of confounding variables. The second

519
00:19:00,080 --> 00:19:04,160
most frequently mentioned reason is a

520
00:19:01,840 --> 00:19:06,320
lack of statistical power. Additionally,

521
00:19:04,160 --> 00:19:08,720
they found one study that explicitly

522
00:19:06,320 --> 00:19:11,120
stated that they follow the methods from

523
00:19:08,720 --> 00:19:13,679
past publications and that contributes

524
00:19:11,120 --> 00:19:15,679
to their decision to exclude data. So,

525
00:19:13,679 --> 00:19:18,480
while the authors agree that these

526
00:19:15,679 --> 00:19:20,960
reasons um are legitimate, they also

527
00:19:18,480 --> 00:19:22,960
emphasize that they do not necessarily

528
00:19:20,960 --> 00:19:25,760
prevent the analysis of data from

529
00:19:22,960 --> 00:19:28,080
diverse populations and they argue that

530
00:19:25,760 --> 00:19:30,799
uh authors should analyze such data. The

531
00:19:28,080 --> 00:19:34,240
authors um argue that finding a genetic

532
00:19:30,799 --> 00:19:36,400
association can secure publication, but

533
00:19:34,240 --> 00:19:39,679
analyzing different populations may

534
00:19:36,400 --> 00:19:42,280
complicate the process and delay it. My

535
00:19:39,679 --> 00:19:45,600
perspective is that this dynamic between

536
00:19:42,280 --> 00:19:48,240
researchers, reviewers, and editors may

537
00:19:45,600 --> 00:19:52,720
be seen by the public as a preference by

538
00:19:48,240 --> 00:19:52,720
scientists for studying European

539
00:19:53,080 --> 00:20:00,280
cohorts. In 2020, some of the same

540
00:19:56,799 --> 00:20:02,320
authors um apply the accountability for

541
00:20:00,280 --> 00:20:05,280
reasonable accountability for

542
00:20:02,320 --> 00:20:08,000
reasonleness framework to allow for

543
00:20:05,280 --> 00:20:09,919
difficult choices and tradeoffs like the

544
00:20:08,000 --> 00:20:12,559
ones I described, but also promote

545
00:20:09,919 --> 00:20:14,480
procedural fairness. As part of this,

546
00:20:12,559 --> 00:20:17,039
they discussed transparency,

547
00:20:14,480 --> 00:20:18,880
enforcement, and revision, as well as

548
00:20:17,039 --> 00:20:21,679
other factors, proposing that funders

549
00:20:18,880 --> 00:20:23,280
and journals develop ways to enforce a

550
00:20:21,679 --> 00:20:25,520
framework that's aligned with evidence

551
00:20:23,280 --> 00:20:27,120
and principles and values, and that

552
00:20:25,520 --> 00:20:29,679
assessments of transparency and

553
00:20:27,120 --> 00:20:31,120
revalence. Um, they understand that

554
00:20:29,679 --> 00:20:33,919
these assessments will change with

555
00:20:31,120 --> 00:20:35,760
society and methodology. Um, in

556
00:20:33,919 --> 00:20:38,320
conclusion, I just want to suggest that

557
00:20:35,760 --> 00:20:40,880
the scientific community reward journals

558
00:20:38,320 --> 00:20:42,640
that create space and opportunities for

559
00:20:40,880 --> 00:20:44,640
scientists to openly share their

560
00:20:42,640 --> 00:20:47,840
decision-making processes related to

561
00:20:44,640 --> 00:20:49,919
equitable inclusion or dis or exclusion.

562
00:20:47,840 --> 00:20:51,760
Journals are major gatekeepers of

563
00:20:49,919 --> 00:20:54,159
information and communication and our

564
00:20:51,760 --> 00:20:56,320
journalists pick up what's written in

565
00:20:54,159 --> 00:20:58,400
these scientific um publications.

566
00:20:56,320 --> 00:21:02,320
Members of the public sometimes do as

567
00:20:58,400 --> 00:21:04,640
well. So um by fostering transparency

568
00:21:02,320 --> 00:21:07,200
and account and accountability I believe

569
00:21:04,640 --> 00:21:09,280
we can begin to eliminate some some of

570
00:21:07,200 --> 00:21:12,320
these barriers to communication and

571
00:21:09,280 --> 00:21:14,559
build trust. Thank you so much.

572
00:21:12,320 --> 00:21:17,760
Good afternoon everyone. Uh my name is

573
00:21:14,559 --> 00:21:20,480
Namita Sang Gupta and uh I am the third

574
00:21:17,760 --> 00:21:22,480
panelist for today. I don't have slides

575
00:21:20,480 --> 00:21:25,840
uh but I'm going to share perspectives

576
00:21:22,480 --> 00:21:29,280
from another lens here as a practitioner

577
00:21:25,840 --> 00:21:31,360
of science communication. um and public

578
00:21:29,280 --> 00:21:34,240
engagement. Uh I'm going to share some

579
00:21:31,360 --> 00:21:37,200
best practices and uh things that I

580
00:21:34,240 --> 00:21:40,000
apply in my own work in my with my

581
00:21:37,200 --> 00:21:42,240
colleagues I work with and uh outside of

582
00:21:40,000 --> 00:21:44,080
the broad I'm also an adjun professor of

583
00:21:42,240 --> 00:21:46,880
science communication at Clemson

584
00:21:44,080 --> 00:21:49,200
University in South Carolina uh from

585
00:21:46,880 --> 00:21:51,600
where I also graduated did my PhD from

586
00:21:49,200 --> 00:21:53,280
there. So uh these are fundamental

587
00:21:51,600 --> 00:21:55,120
concepts that I also teach graduate

588
00:21:53,280 --> 00:21:58,159
students when they are thinking about

589
00:21:55,120 --> 00:21:59,480
communicating uh complex research topics

590
00:21:58,159 --> 00:22:01,840
with a broader

591
00:21:59,480 --> 00:22:03,760
audience. But uh from the broad

592
00:22:01,840 --> 00:22:05,600
institute context uh just for a quick

593
00:22:03,760 --> 00:22:07,919
note actually what you see as my

594
00:22:05,600 --> 00:22:10,000
backdrop is the broad discovery center.

595
00:22:07,919 --> 00:22:11,520
It's a wallpaper here but uh I'm not

596
00:22:10,000 --> 00:22:14,559
sitting at the discovery center right

597
00:22:11,520 --> 00:22:16,159
now but it is a public science museum uh

598
00:22:14,559 --> 00:22:19,280
which is free to the public open

599
00:22:16,159 --> 00:22:21,840
Thursday through Saturday. uh and uh we

600
00:22:19,280 --> 00:22:23,280
offer a lot of tours and uh you know

601
00:22:21,840 --> 00:22:25,520
public engagement programming through

602
00:22:23,280 --> 00:22:28,480
the broad discovery center. We are a

603
00:22:25,520 --> 00:22:32,080
2-year-old uh science museum that uh

604
00:22:28,480 --> 00:22:34,640
aims to demystify complex biomedical

605
00:22:32,080 --> 00:22:36,400
research uh happening at Broad and our

606
00:22:34,640 --> 00:22:39,360
partner institutions and collaborator

607
00:22:36,400 --> 00:22:41,200
sites nationally and worldwide. Uh we

608
00:22:39,360 --> 00:22:43,840
try to demystify that for the general

609
00:22:41,200 --> 00:22:46,000
public. The exhibits at the discovery

610
00:22:43,840 --> 00:22:50,320
center are uh centered around different

611
00:22:46,000 --> 00:22:52,159
disease areas and uh I wanted to use the

612
00:22:50,320 --> 00:22:54,880
discovery center as an example in

613
00:22:52,159 --> 00:22:57,280
explaining some of the concepts uh that

614
00:22:54,880 --> 00:22:59,600
I'll talk about through the panel today.

615
00:22:57,280 --> 00:23:01,520
But uh going back to like three or four

616
00:22:59,600 --> 00:23:03,360
concepts that I want to touch on. Uh I'm

617
00:23:01,520 --> 00:23:04,679
assuming some of you on this panel uh

618
00:23:03,360 --> 00:23:06,960
sorry some of you in the in the

619
00:23:04,679 --> 00:23:08,080
participants are either scientists or

620
00:23:06,960 --> 00:23:10,159
you're science communication

621
00:23:08,080 --> 00:23:13,440
practitioners or you're just interested

622
00:23:10,159 --> 00:23:16,480
uh you know you uh in this topic. So one

623
00:23:13,440 --> 00:23:19,039
concept that uh I always uh teach my

624
00:23:16,480 --> 00:23:21,600
students uh graduate students when uh we

625
00:23:19,039 --> 00:23:23,360
talk about science communication best

626
00:23:21,600 --> 00:23:26,880
practices and how to like help coach

627
00:23:23,360 --> 00:23:29,440
them to communicate about their work. I

628
00:23:26,880 --> 00:23:31,280
tell them that there is no concept as

629
00:23:29,440 --> 00:23:34,240
lay audience or general public like I

630
00:23:31,280 --> 00:23:35,919
know we talk about that very commonly in

631
00:23:34,240 --> 00:23:37,919
everything we say what's the messaging

632
00:23:35,919 --> 00:23:39,640
for the general public or how will you

633
00:23:37,919 --> 00:23:42,880
say this as a scientist to a lay

634
00:23:39,640 --> 00:23:46,720
audience audience uh audience is much

635
00:23:42,880 --> 00:23:49,200
more nuanced so I encourage anybody I

636
00:23:46,720 --> 00:23:51,679
coach uh and work with on the science

637
00:23:49,200 --> 00:23:54,320
communication project is let's do an

638
00:23:51,679 --> 00:23:56,400
exercise to uh reframe our audience like

639
00:23:54,320 --> 00:23:58,880
so we do an audience reframing develop

640
00:23:56,400 --> 00:24:00,400
audience personas and kind of see what

641
00:23:58,880 --> 00:24:02,000
is some common goal in science

642
00:24:00,400 --> 00:24:04,159
communication that would cater to that

643
00:24:02,000 --> 00:24:05,600
audience group. A lot of scientists and

644
00:24:04,159 --> 00:24:08,480
early career researchers I've worked

645
00:24:05,600 --> 00:24:10,080
with uh we've often identified like five

646
00:24:08,480 --> 00:24:11,520
top personas for them you know like

647
00:24:10,080 --> 00:24:13,679
which are very important for them for

648
00:24:11,520 --> 00:24:15,360
example experts outside their field so

649
00:24:13,679 --> 00:24:16,640
they may be a biologist who's trying to

650
00:24:15,360 --> 00:24:19,120
communicate and collaborate with a

651
00:24:16,640 --> 00:24:21,440
mathematician uh interdisciplinary

652
00:24:19,120 --> 00:24:24,480
audience so they are possibly invited to

653
00:24:21,440 --> 00:24:26,480
a conference that has a larger like

654
00:24:24,480 --> 00:24:29,520
group of people from across different

655
00:24:26,480 --> 00:24:31,679
expertise area collaborators you know

656
00:24:29,520 --> 00:24:33,840
like uh it's it ties into the

657
00:24:31,679 --> 00:24:36,000
interdisciplinary audience as well. But

658
00:24:33,840 --> 00:24:37,840
sometimes you may be a chemist who's

659
00:24:36,000 --> 00:24:40,080
trying to develop some kind of machine

660
00:24:37,840 --> 00:24:43,679
learning model with someone else or

661
00:24:40,080 --> 00:24:47,039
you're working with u someone who's a

662
00:24:43,679 --> 00:24:48,400
history uh you know uh expert. So it

663
00:24:47,039 --> 00:24:49,679
just kind of depends how you're

664
00:24:48,400 --> 00:24:52,000
communicating with your work with your

665
00:24:49,679 --> 00:24:53,840
collaborators and stakeholders.

666
00:24:52,000 --> 00:24:55,600
Stakeholders is anybody who benefits

667
00:24:53,840 --> 00:24:57,760
from your work. Stakeholders anybody

668
00:24:55,600 --> 00:24:59,600
who's like helping you with your work

669
00:24:57,760 --> 00:25:01,679
and of course decision makers. Decision

670
00:24:59,600 --> 00:25:04,320
makers are policy makers. decision

671
00:25:01,679 --> 00:25:06,400
makers are your agency, grant agencies

672
00:25:04,320 --> 00:25:10,159
or donors who will be deciding to give

673
00:25:06,400 --> 00:25:11,919
you uh funding for your work maybe. So

674
00:25:10,159 --> 00:25:13,840
helping develop some kind of audience

675
00:25:11,919 --> 00:25:15,760
persona before any type of public

676
00:25:13,840 --> 00:25:18,480
engagement work you do is really a

677
00:25:15,760 --> 00:25:20,400
really helpful exercise. The other

678
00:25:18,480 --> 00:25:22,240
aspect since we talked uh Brian and

679
00:25:20,400 --> 00:25:24,640
Shaikica laid the foundation of trust in

680
00:25:22,240 --> 00:25:26,559
science communication. I think the other

681
00:25:24,640 --> 00:25:28,799
aspect or best practice that goes along

682
00:25:26,559 --> 00:25:30,960
with that is balancing simplicity and

683
00:25:28,799 --> 00:25:33,200
accuracy. So as a communicator, as a

684
00:25:30,960 --> 00:25:35,760
science communicator, as a scientist,

685
00:25:33,200 --> 00:25:38,000
there are strategies that you can employ

686
00:25:35,760 --> 00:25:40,240
to break down very complex scientific

687
00:25:38,000 --> 00:25:43,120
concepts in ways that can be accessible

688
00:25:40,240 --> 00:25:44,559
and engaging to your audience. Uh, of

689
00:25:43,120 --> 00:25:46,799
course you I know a lot of scientists

690
00:25:44,559 --> 00:25:49,440
are nervous about uh making things sound

691
00:25:46,799 --> 00:25:51,120
too simple, oversimplification or that

692
00:25:49,440 --> 00:25:53,200
can get carried through misinformation.

693
00:25:51,120 --> 00:25:56,159
So there are ways there are layered ways

694
00:25:53,200 --> 00:25:58,960
that you can address this. Uh I always

695
00:25:56,159 --> 00:26:01,760
encourage as a best practice uh when I'm

696
00:25:58,960 --> 00:26:03,360
coaching for these projects is focus

697
00:26:01,760 --> 00:26:05,600
first on the why. If you're

698
00:26:03,360 --> 00:26:08,159
communicating about something, what is

699
00:26:05,600 --> 00:26:11,320
that larger societal, global, public

700
00:26:08,159 --> 00:26:14,080
health uh problem that your research is

701
00:26:11,320 --> 00:26:16,080
addressing? And I have experienced this

702
00:26:14,080 --> 00:26:18,320
that a lot of early career researchers

703
00:26:16,080 --> 00:26:20,720
are hesitant. They say that no but I'm

704
00:26:18,320 --> 00:26:23,760
working on a very specific pathway in

705
00:26:20,720 --> 00:26:26,080
this particular organism, this disease

706
00:26:23,760 --> 00:26:28,960
model. And I say okay let's zoom out.

707
00:26:26,080 --> 00:26:31,520
Let's think about why even do this work.

708
00:26:28,960 --> 00:26:33,760
So let's start at that level. Then the

709
00:26:31,520 --> 00:26:35,919
second layer goes into what like what

710
00:26:33,760 --> 00:26:38,240
are you doing about the research? Like

711
00:26:35,919 --> 00:26:39,760
maybe you're uh it's not even at the

712
00:26:38,240 --> 00:26:41,679
technique level. What is your larger

713
00:26:39,760 --> 00:26:44,240
research question? Let's frame that for

714
00:26:41,679 --> 00:26:45,919
your audience. And then we get into how.

715
00:26:44,240 --> 00:26:48,559
That's your what technique you're

716
00:26:45,919 --> 00:26:49,919
approaching uh with this. What method

717
00:26:48,559 --> 00:26:52,000
you're implementing? What data set

718
00:26:49,919 --> 00:26:53,919
you're using. And when researchers tend

719
00:26:52,000 --> 00:26:57,039
to apply this in the way they

720
00:26:53,919 --> 00:26:59,200
communicate like the why, the what and

721
00:26:57,039 --> 00:27:00,880
then the how. And even science

722
00:26:59,200 --> 00:27:02,480
communicators as science communicators

723
00:27:00,880 --> 00:27:04,799
we've experienced this at the broad

724
00:27:02,480 --> 00:27:07,360
discovery center we to a lot of public

725
00:27:04,799 --> 00:27:08,960
tours and uh our really talented visitor

726
00:27:07,360 --> 00:27:11,200
experience assistants when they engage

727
00:27:08,960 --> 00:27:13,039
with the public they've applied this

728
00:27:11,200 --> 00:27:15,279
framework because in our museum our

729
00:27:13,039 --> 00:27:16,880
content is also displayed in this way.

730
00:27:15,279 --> 00:27:18,799
We have a lot of people who just like

731
00:27:16,880 --> 00:27:21,360
engaging at the why level the big

732
00:27:18,799 --> 00:27:23,440
picture question. But we also get we are

733
00:27:21,360 --> 00:27:25,159
located in Kendall Square, a biotech

734
00:27:23,440 --> 00:27:27,520
ecosystem here in Cambridge,

735
00:27:25,159 --> 00:27:30,000
Massachusetts. So we also get a lot of

736
00:27:27,520 --> 00:27:32,559
people who come to our museum who are

737
00:27:30,000 --> 00:27:35,760
biotech CEOs or collaborators from

738
00:27:32,559 --> 00:27:39,120
Europe or uh students who are pursuing

739
00:27:35,760 --> 00:27:40,720
STEM majors in undergrad and they ask a

740
00:27:39,120 --> 00:27:44,640
lot of questions that are associated

741
00:27:40,720 --> 00:27:46,080
with the how. So for for kind of like we

742
00:27:44,640 --> 00:27:48,880
want to make sure that we're offering

743
00:27:46,080 --> 00:27:50,880
content in our exhibits, in our tours,

744
00:27:48,880 --> 00:27:53,279
in our discussions, and in the dialogues

745
00:27:50,880 --> 00:27:54,880
that we have with the public in these

746
00:27:53,279 --> 00:27:58,159
nuanced and layered ways. So I think

747
00:27:54,880 --> 00:27:59,840
that framework has really helped us. Uh

748
00:27:58,159 --> 00:28:01,679
and finally, anyone I'm going to close

749
00:27:59,840 --> 00:28:04,640
out with like just one final tip with

750
00:28:01,679 --> 00:28:07,679
again like a what and how framework is

751
00:28:04,640 --> 00:28:10,960
anyone here if you're like you know as

752
00:28:07,679 --> 00:28:12,240
as you saw the uh the data that Alham

753
00:28:10,960 --> 00:28:14,640
presented at the beginning of the

754
00:28:12,240 --> 00:28:17,120
session and Brian and Shaunie also

755
00:28:14,640 --> 00:28:19,760
presented really great like uh data sets

756
00:28:17,120 --> 00:28:21,520
and uh information. If any of that is

757
00:28:19,760 --> 00:28:23,440
encouraging you that I need to step up

758
00:28:21,520 --> 00:28:25,200
my science communication game or as a

759
00:28:23,440 --> 00:28:28,960
scientist or a science communicator, I

760
00:28:25,200 --> 00:28:31,279
have a role to play going into 2025. Um

761
00:28:28,960 --> 00:28:33,600
I just kind of do this simple formula

762
00:28:31,279 --> 00:28:36,559
that I apply to every almost science

763
00:28:33,600 --> 00:28:39,200
communication initiative is let's take a

764
00:28:36,559 --> 00:28:42,399
sheet of paper and write down the what,

765
00:28:39,200 --> 00:28:44,159
the who and the how. That means what

766
00:28:42,399 --> 00:28:46,399
every great science communication

767
00:28:44,159 --> 00:28:48,720
projects begins with a great science

768
00:28:46,399 --> 00:28:50,480
communication goal. That means what are

769
00:28:48,720 --> 00:28:52,559
you trying to communicate? What is the

770
00:28:50,480 --> 00:28:55,600
purpose of that? So let's first write a

771
00:28:52,559 --> 00:28:57,279
clear goal. Then you write who who would

772
00:28:55,600 --> 00:29:00,000
benefit from knowing about this. And

773
00:28:57,279 --> 00:29:01,600
sometimes in some cases based on the

774
00:29:00,000 --> 00:29:04,080
kind of researcher you are or the

775
00:29:01,600 --> 00:29:06,960
program you're running maybe the who is

776
00:29:04,080 --> 00:29:08,320
first and then goes the what. But who is

777
00:29:06,960 --> 00:29:09,919
your audience? And that kind of

778
00:29:08,320 --> 00:29:12,720
reframing that I talked about in the

779
00:29:09,919 --> 00:29:15,760
beginning. And most important after that

780
00:29:12,720 --> 00:29:17,279
is the how the medium of communicating

781
00:29:15,760 --> 00:29:19,440
you know then that's where like if you

782
00:29:17,279 --> 00:29:21,120
remember the first slide that Brian

783
00:29:19,440 --> 00:29:22,640
showed about trust and the different

784
00:29:21,120 --> 00:29:24,960
mediums of communication that has

785
00:29:22,640 --> 00:29:26,880
evolved over time we really have to pay

786
00:29:24,960 --> 00:29:28,960
attention to that. If you are really

787
00:29:26,880 --> 00:29:31,760
communicating to teenagers who are only

788
00:29:28,960 --> 00:29:34,240
on Tik Tok or Instagram reels, your

789
00:29:31,760 --> 00:29:36,480
approach might be very different than if

790
00:29:34,240 --> 00:29:37,840
you're communicating with community

791
00:29:36,480 --> 00:29:39,840
members in a neighborhood that is

792
00:29:37,840 --> 00:29:41,840
impacted by a particular environmental

793
00:29:39,840 --> 00:29:44,000
health problem in their community and

794
00:29:41,840 --> 00:29:46,320
you have to meet them maybe at the city

795
00:29:44,000 --> 00:29:48,799
hall or wherever the convening is. So

796
00:29:46,320 --> 00:29:50,399
these are different ways to think about

797
00:29:48,799 --> 00:29:51,440
who your audience is, where do you meet

798
00:29:50,399 --> 00:29:53,360
them and what are you going to

799
00:29:51,440 --> 00:29:54,880
communicate. I hope some of these best

800
00:29:53,360 --> 00:29:56,559
practices are things you can apply

801
00:29:54,880 --> 00:29:59,039
immediately to your work or share with

802
00:29:56,559 --> 00:30:00,720
your colleagues and uh I try to apply

803
00:29:59,039 --> 00:30:02,840
this every time I'm taking on a

804
00:30:00,720 --> 00:30:06,799
communication project for any

805
00:30:02,840 --> 00:30:09,760
audience. I'll pass it on to Alhung.

806
00:30:06,799 --> 00:30:11,080
Thanks so much Namatha Sheni and Brian.

807
00:30:09,760 --> 00:30:13,679
Um so

808
00:30:11,080 --> 00:30:15,440
many interesting points raised from

809
00:30:13,679 --> 00:30:16,880
different perspectives and I think it's

810
00:30:15,440 --> 00:30:20,080
going to make for hopefully a rich

811
00:30:16,880 --> 00:30:23,520
conversation in our panel. Um, I'd love

812
00:30:20,080 --> 00:30:25,679
to start with a few questions for for

813
00:30:23,520 --> 00:30:27,840
our our panel. Um, the first one is

814
00:30:25,679 --> 00:30:30,320
focused on combating misinformation,

815
00:30:27,840 --> 00:30:32,559
something that um we've we've heard

816
00:30:30,320 --> 00:30:34,120
about a little bit today. Uh, with the

817
00:30:32,559 --> 00:30:36,480
rise of misinformation and

818
00:30:34,120 --> 00:30:38,799
disinformation, what role do you do

819
00:30:36,480 --> 00:30:40,320
science communicators and other

820
00:30:38,799 --> 00:30:42,399
communicators of science play in

821
00:30:40,320 --> 00:30:44,320
addressing these challenges? And what

822
00:30:42,399 --> 00:30:48,120
are some practical ways to foster trust

823
00:30:44,320 --> 00:30:48,120
and counter false narratives?

824
00:30:48,159 --> 00:30:52,640
Yeah, I can I can jump in there. Um,

825
00:30:50,880 --> 00:30:55,039
it's such an important question and one

826
00:30:52,640 --> 00:30:56,399
that's I think you know bothering and

827
00:30:55,039 --> 00:30:58,320
um, you know, worrying lots of folks

828
00:30:56,399 --> 00:31:01,120
that are probably on the call right now.

829
00:30:58,320 --> 00:31:03,360
And I think something there there's a

830
00:31:01,120 --> 00:31:05,200
lot to be done. Um, there one thing to

831
00:31:03,360 --> 00:31:07,600
that's important to keep in mind is that

832
00:31:05,200 --> 00:31:08,720
there's a role for all of us. Um, and

833
00:31:07,600 --> 00:31:10,320
there's a role for many different

834
00:31:08,720 --> 00:31:11,360
institutions. Um, you know, this isn't

835
00:31:10,320 --> 00:31:13,120
going to be a problem that's going to be

836
00:31:11,360 --> 00:31:14,640
solved, you know, overnight you per se.

837
00:31:13,120 --> 00:31:16,960
It's took us a long time to get here and

838
00:31:14,640 --> 00:31:19,039
it's and it's a problem that is partly

839
00:31:16,960 --> 00:31:21,039
reflective of the nature of the society

840
00:31:19,039 --> 00:31:22,799
that we live in. Um the nature of our

841
00:31:21,039 --> 00:31:24,880
media system, the nature of lots of

842
00:31:22,799 --> 00:31:26,159
things. Um and we're not going to

843
00:31:24,880 --> 00:31:27,760
necessarily change that, you know,

844
00:31:26,159 --> 00:31:30,480
overnight, nor would we necessarily want

845
00:31:27,760 --> 00:31:32,960
to. Uh we the openness of our media

846
00:31:30,480 --> 00:31:34,240
system, for example, is is a strength.

847
00:31:32,960 --> 00:31:35,760
And so we wouldn't want to be in a

848
00:31:34,240 --> 00:31:37,440
situation where we were censoring or

849
00:31:35,760 --> 00:31:38,880
sanitizing everything because then you

850
00:31:37,440 --> 00:31:41,120
might worry about who it is that has the

851
00:31:38,880 --> 00:31:42,240
reigns of of censorship. Um, so we're

852
00:31:41,120 --> 00:31:45,679
going to have to live with some degree

853
00:31:42,240 --> 00:31:47,760
of of um, open-endedness, I think. But

854
00:31:45,679 --> 00:31:49,840
something that I think is an instinct

855
00:31:47,760 --> 00:31:51,200
that many people have is to somehow um,

856
00:31:49,840 --> 00:31:52,760
you know, want to go to all corners of

857
00:31:51,200 --> 00:31:54,480
the internet and to vanquish, you know,

858
00:31:52,760 --> 00:31:55,919
misinformation. Um, you know, because

859
00:31:54,480 --> 00:31:57,360
they're worried about, you know, that I

860
00:31:55,919 --> 00:31:59,120
think the bigger issue, the bigger

861
00:31:57,360 --> 00:32:00,960
concern is, you know, what are you doing

862
00:31:59,120 --> 00:32:02,799
to actively provide good and credible

863
00:32:00,960 --> 00:32:04,720
information to, um, you know, different

864
00:32:02,799 --> 00:32:06,320
communities. And I think that there's a

865
00:32:04,720 --> 00:32:08,159
lot that you could be doing um, beyond

866
00:32:06,320 --> 00:32:10,000
what what you currently are. And that's

867
00:32:08,159 --> 00:32:11,279
something that um this goes to a lot of

868
00:32:10,000 --> 00:32:13,600
the points we've had raised here

869
00:32:11,279 --> 00:32:16,799
already. But I think if we get to a spot

870
00:32:13,600 --> 00:32:19,120
where people um were and and dependent

871
00:32:16,799 --> 00:32:20,799
upon and and relied on and turned to

872
00:32:19,120 --> 00:32:22,559
different sources of information more

873
00:32:20,799 --> 00:32:24,000
regularly. Right now there's a sort of

874
00:32:22,559 --> 00:32:26,159
wide openen sense people are not

875
00:32:24,000 --> 00:32:28,000
necessarily always paying attention to

876
00:32:26,159 --> 00:32:29,840
um you know sources of of information.

877
00:32:28,000 --> 00:32:31,279
Those sources of institutions and

878
00:32:29,840 --> 00:32:33,039
scientific institutions aren't always

879
00:32:31,279 --> 00:32:34,720
providing the answers to the questions

880
00:32:33,039 --> 00:32:36,240
people have. So if we can be better

881
00:32:34,720 --> 00:32:37,919
oriented towards understanding what is

882
00:32:36,240 --> 00:32:39,519
it that people want uh what is it

883
00:32:37,919 --> 00:32:41,360
they're interested in how can we better

884
00:32:39,519 --> 00:32:43,760
provide that information in a in an

885
00:32:41,360 --> 00:32:45,440
accessible way that will help um you

886
00:32:43,760 --> 00:32:47,279
know rather than always chasing after

887
00:32:45,440 --> 00:32:49,440
and trying to vanquish um you know the

888
00:32:47,279 --> 00:32:51,200
misinformation that's out there. Um I

889
00:32:49,440 --> 00:32:53,120
think that's that's a really healthy you

890
00:32:51,200 --> 00:32:55,279
know starting place. Now we can't have

891
00:32:53,120 --> 00:32:57,200
any illusion that somehow if you just

892
00:32:55,279 --> 00:32:58,559
post something on your own uh private

893
00:32:57,200 --> 00:32:59,919
blog somewhere that you know lots of

894
00:32:58,559 --> 00:33:01,200
people are going to necessarily read it.

895
00:32:59,919 --> 00:33:03,440
we do have to pay attention to the

896
00:33:01,200 --> 00:33:05,200
landscape and to the importance of

897
00:33:03,440 --> 00:33:07,440
different institutions, news outlets and

898
00:33:05,200 --> 00:33:09,120
others for amplifying ideas. Um, you

899
00:33:07,440 --> 00:33:11,120
know, we live in a a landscape where

900
00:33:09,120 --> 00:33:12,480
it's not immediately guaranteed that you

901
00:33:11,120 --> 00:33:14,080
know what you say privately on the

902
00:33:12,480 --> 00:33:15,440
corner is going to make its way out

903
00:33:14,080 --> 00:33:17,360
there. So, you have to think about the

904
00:33:15,440 --> 00:33:19,600
systems and think strategically, but to

905
00:33:17,360 --> 00:33:22,240
what extent are you um oriented to be a

906
00:33:19,600 --> 00:33:24,399
good and available and credible source?

907
00:33:22,240 --> 00:33:25,600
Um, you can you work in conjunction with

908
00:33:24,399 --> 00:33:27,200
um, you know, journalists and and

909
00:33:25,600 --> 00:33:28,399
community based organizations and others

910
00:33:27,200 --> 00:33:30,399
to provide, you know, that kind of

911
00:33:28,399 --> 00:33:32,080
information. All those are think are

912
00:33:30,399 --> 00:33:33,279
going to be um really crucial and

913
00:33:32,080 --> 00:33:34,960
important and I think there's a lot more

914
00:33:33,279 --> 00:33:37,120
that could be done you know than we are

915
00:33:34,960 --> 00:33:39,440
we are doing right now. Um but I do

916
00:33:37,120 --> 00:33:41,919
think there is hope um this we're we're

917
00:33:39,440 --> 00:33:43,919
not necessarily um you know at risk of

918
00:33:41,919 --> 00:33:45,279
completely drowning in uh in

919
00:33:43,919 --> 00:33:46,799
misinformation although it might seem

920
00:33:45,279 --> 00:33:48,640
like that sometimes because actually

921
00:33:46,799 --> 00:33:50,240
there are lots of situations where

922
00:33:48,640 --> 00:33:51,840
people are um you know getting

923
00:33:50,240 --> 00:33:53,279
information from others in their

924
00:33:51,840 --> 00:33:55,200
community. There are there are lots of

925
00:33:53,279 --> 00:33:56,640
good community level initiatives. we

926
00:33:55,200 --> 00:33:58,240
what we need to do is strengthen and

927
00:33:56,640 --> 00:33:59,919
build up and amplify you know some of

928
00:33:58,240 --> 00:34:01,440
what's been happening you know already

929
00:33:59,919 --> 00:34:03,519
and not necessarily just throw our hands

930
00:34:01,440 --> 00:34:08,200
up in despair and I think that's one one

931
00:34:03,519 --> 00:34:08,200
my take on it um you know anyway so

932
00:34:08,480 --> 00:34:12,879
yeah thanks so much Brian um I think the

933
00:34:10,560 --> 00:34:15,520
the point you raised specifically around

934
00:34:12,879 --> 00:34:17,760
kind of thinking about how to prevent um

935
00:34:15,520 --> 00:34:19,280
misinformation from spreading and being

936
00:34:17,760 --> 00:34:21,119
kind of thinking creatively and

937
00:34:19,280 --> 00:34:22,720
strategically about that and finding the

938
00:34:21,119 --> 00:34:25,599
right partners to help with that I think

939
00:34:22,720 --> 00:34:28,079
is is a really important on um any other

940
00:34:25,599 --> 00:34:31,079
thoughts on this topic from Chini or

941
00:34:28,079 --> 00:34:31,079
Nam.

942
00:34:31,440 --> 00:34:35,359
I think that was just such a great

943
00:34:32,800 --> 00:34:37,560
answer. I don't have much to add, but I

944
00:34:35,359 --> 00:34:40,399
really do think it's every scientist's

945
00:34:37,560 --> 00:34:43,679
responsibility to think about also

946
00:34:40,399 --> 00:34:46,800
internally the the words they're using.

947
00:34:43,679 --> 00:34:49,040
Understand sort the the debate and the

948
00:34:46,800 --> 00:34:51,440
discussion about how we talk about

949
00:34:49,040 --> 00:34:53,839
groups and populations.

950
00:34:51,440 --> 00:34:56,399
um engaging groups to understand what

951
00:34:53,839 --> 00:34:59,839
they need you to communicate about as

952
00:34:56,399 --> 00:35:02,160
Brian said is just so so critical and

953
00:34:59,839 --> 00:35:03,839
then just recognizing that people have

954
00:35:02,160 --> 00:35:08,560
individuals and institutions that they

955
00:35:03,839 --> 00:35:10,720
trust on the ground where they go to for

956
00:35:08,560 --> 00:35:14,680
trust and information. I you know I

957
00:35:10,720 --> 00:35:14,680
couldn't agree more with that point.

958
00:35:15,119 --> 00:35:19,119
Thanks

959
00:35:16,280 --> 00:35:20,800
Shenqua. I'm I don't want to take away I

960
00:35:19,119 --> 00:35:22,560
I have one other point to to raise and

961
00:35:20,800 --> 00:35:24,400
add there but go ahead if you had other

962
00:35:22,560 --> 00:35:26,400
thoughts. Uh I was just going to build

963
00:35:24,400 --> 00:35:29,280
on uh some of the things that have

964
00:35:26,400 --> 00:35:30,960
already been said but you know since

965
00:35:29,280 --> 00:35:33,599
give a call to action to our scientists

966
00:35:30,960 --> 00:35:35,680
that it is also your responsibility. I

967
00:35:33,599 --> 00:35:37,839
as a as someone who practices science

968
00:35:35,680 --> 00:35:39,920
communication I just thought like I

969
00:35:37,839 --> 00:35:42,160
could share some tips or on of course

970
00:35:39,920 --> 00:35:45,440
like there's no foolproof method that

971
00:35:42,160 --> 00:35:47,119
everything is going to work but uh if

972
00:35:45,440 --> 00:35:48,720
just there are some approaches like for

973
00:35:47,119 --> 00:35:50,400
example if you're in a situation where

974
00:35:48,720 --> 00:35:52,320
you're experiencing you're in a

975
00:35:50,400 --> 00:35:55,680
one-on-one dialogue situation and you're

976
00:35:52,320 --> 00:35:57,359
experiencing uh misinformation uh which

977
00:35:55,680 --> 00:35:59,680
is kind of important for us those who

978
00:35:57,359 --> 00:36:02,960
work in community spaces or engage with

979
00:35:59,680 --> 00:36:04,800
the public on a one-on-one direct basis

980
00:36:02,960 --> 00:36:06,320
is uh let that be a two-way

981
00:36:04,800 --> 00:36:08,800
conversation. I think it's very

982
00:36:06,320 --> 00:36:10,800
important that when you're experiencing

983
00:36:08,800 --> 00:36:13,040
misinformation, it's also important to

984
00:36:10,800 --> 00:36:14,640
listen and I think it it can from both

985
00:36:13,040 --> 00:36:16,880
sides. It can be you block you

986
00:36:14,640 --> 00:36:19,839
completely shut down or block the other

987
00:36:16,880 --> 00:36:22,720
person's views. But uh and addressing

988
00:36:19,839 --> 00:36:25,599
the misin misinformation respectfully is

989
00:36:22,720 --> 00:36:27,200
key. If you if you're hearing skepticism

990
00:36:25,599 --> 00:36:29,839
about certain things, just listen to

991
00:36:27,200 --> 00:36:31,839
where that person is coming from and

992
00:36:29,839 --> 00:36:34,560
then have a dialogue driven approach in

993
00:36:31,839 --> 00:36:36,079
the conversation. I do agree that you

994
00:36:34,560 --> 00:36:38,400
know sometimes we have to take a more

995
00:36:36,079 --> 00:36:40,240
community engagement approach to things

996
00:36:38,400 --> 00:36:42,720
when you know like misinformation is

997
00:36:40,240 --> 00:36:44,640
already spreading about a topic. Maybe

998
00:36:42,720 --> 00:36:46,560
it's important to think about who are

999
00:36:44,640 --> 00:36:48,720
the trusted members in that community

1000
00:36:46,560 --> 00:36:50,320
like maybe working with faith leaders,

1001
00:36:48,720 --> 00:36:53,280
educators,

1002
00:36:50,320 --> 00:36:55,599
um cultural organizations, uh you know,

1003
00:36:53,280 --> 00:36:58,800
social media influencers even for for

1004
00:36:55,599 --> 00:37:00,720
that uh instance and counter false

1005
00:36:58,800 --> 00:37:03,520
narratives with some kind of

1006
00:37:00,720 --> 00:37:05,280
communitybacked messages. So that could

1007
00:37:03,520 --> 00:37:07,520
be like another thing to think about.

1008
00:37:05,280 --> 00:37:09,680
And I am actually I really loved a point

1009
00:37:07,520 --> 00:37:12,720
that was brought up in one of by one of

1010
00:37:09,680 --> 00:37:14,720
our speakers earlier u was it's also

1011
00:37:12,720 --> 00:37:17,000
important it's becoming increasingly

1012
00:37:14,720 --> 00:37:20,079
important to talk about how science is

1013
00:37:17,000 --> 00:37:22,240
done and we've seen we've continue to

1014
00:37:20,079 --> 00:37:24,240
see this and I think the general public

1015
00:37:22,240 --> 00:37:25,599
like when you're engaging with them when

1016
00:37:24,240 --> 00:37:27,680
you're speaking when you're tackling

1017
00:37:25,599 --> 00:37:30,000
topics of misinformation it's not about

1018
00:37:27,680 --> 00:37:32,520
what the findings are but like breaking

1019
00:37:30,000 --> 00:37:35,680
down how the data was collected analyzed

1020
00:37:32,520 --> 00:37:37,440
revised will help demystify by the kind

1021
00:37:35,680 --> 00:37:39,920
of iterative nature needed for science

1022
00:37:37,440 --> 00:37:42,800
and how policies get made, how those

1023
00:37:39,920 --> 00:37:45,200
become regulations. Uh this is something

1024
00:37:42,800 --> 00:37:48,000
that scientists need to think about and

1025
00:37:45,200 --> 00:37:51,359
have patience to also talk about that

1026
00:37:48,000 --> 00:37:53,119
aspect of science.

1027
00:37:51,359 --> 00:37:55,359
Yeah. And if if I could raise just one

1028
00:37:53,119 --> 00:37:57,839
other point here given the audience that

1029
00:37:55,359 --> 00:37:59,440
we have um there is another role I think

1030
00:37:57,839 --> 00:38:01,200
for all of for those that that work

1031
00:37:59,440 --> 00:38:02,720
directly in research for those that um

1032
00:38:01,200 --> 00:38:04,960
you know have sometimes a spotlight put

1033
00:38:02,720 --> 00:38:06,160
on their own you know research and so

1034
00:38:04,960 --> 00:38:07,520
here this is a little bit of an

1035
00:38:06,160 --> 00:38:09,119
uncomfortable aspect of this but I think

1036
00:38:07,520 --> 00:38:11,520
it's important for us ethically to um

1037
00:38:09,119 --> 00:38:13,200
you think about um you know there are

1038
00:38:11,520 --> 00:38:15,040
many times which our university you know

1039
00:38:13,200 --> 00:38:17,520
press relations offices others you put

1040
00:38:15,040 --> 00:38:19,440
out press releases that are perhaps a

1041
00:38:17,520 --> 00:38:21,920
bit more sensational than really the you

1042
00:38:19,440 --> 00:38:24,000
know the articles you know warrant But

1043
00:38:21,920 --> 00:38:25,200
we're tempted to go along with that

1044
00:38:24,000 --> 00:38:26,320
because perhaps you know there's a

1045
00:38:25,200 --> 00:38:27,839
little bit of celebrity or fame that

1046
00:38:26,320 --> 00:38:29,680
might come to our work you know as a

1047
00:38:27,839 --> 00:38:32,079
result and we all sort of let that slide

1048
00:38:29,680 --> 00:38:34,320
a little bit. There is this is not to

1049
00:38:32,079 --> 00:38:36,800
lay blame on um you know researchers

1050
00:38:34,320 --> 00:38:39,119
solely but there are moments where you

1051
00:38:36,800 --> 00:38:40,880
know our own amplification of of work

1052
00:38:39,119 --> 00:38:42,800
sometimes leads to some of the

1053
00:38:40,880 --> 00:38:44,560
misunderstanding or misperceptions. We

1054
00:38:42,800 --> 00:38:45,599
tend to over sensationalize sometimes.

1055
00:38:44,560 --> 00:38:48,000
So you might think about the

1056
00:38:45,599 --> 00:38:49,920
relationship you have with um you know

1057
00:38:48,000 --> 00:38:51,680
press office and with others. It may

1058
00:38:49,920 --> 00:38:53,119
mean maybe it means one or two less

1059
00:38:51,680 --> 00:38:54,480
headlines, you know, for your work, but

1060
00:38:53,119 --> 00:38:55,760
if you can provide a little bit more

1061
00:38:54,480 --> 00:38:57,520
context and a little bit more of a

1062
00:38:55,760 --> 00:38:59,040
tempered view of what it is that you're

1063
00:38:57,520 --> 00:39:01,119
actually saying and try to have some

1064
00:38:59,040 --> 00:39:03,040
arrangement whereby you're happy to work

1065
00:39:01,119 --> 00:39:04,400
with a group to um, you know, put out a

1066
00:39:03,040 --> 00:39:06,160
press release, but are you getting a

1067
00:39:04,400 --> 00:39:08,480
final check and and a look on and can

1068
00:39:06,160 --> 00:39:10,240
you stand behind and sign off on that on

1069
00:39:08,480 --> 00:39:12,000
that press release? That might help to

1070
00:39:10,240 --> 00:39:13,359
temper things a bit as well. It's

1071
00:39:12,000 --> 00:39:16,480
actually a recommendation that comes out

1072
00:39:13,359 --> 00:39:19,040
of the um national academyy's consensus

1073
00:39:16,480 --> 00:39:21,119
study which is part of the point there

1074
00:39:19,040 --> 00:39:22,640
of raising that we all do have a role.

1075
00:39:21,119 --> 00:39:24,320
There's a lot that's complicated, a lot

1076
00:39:22,640 --> 00:39:26,079
we can bemoone and worry about what

1077
00:39:24,320 --> 00:39:28,320
happens outside of our control, but

1078
00:39:26,079 --> 00:39:30,800
let's also think about um how much

1079
00:39:28,320 --> 00:39:32,960
sometimes um you know in our own seeking

1080
00:39:30,800 --> 00:39:34,800
to you know glorify our own work um we

1081
00:39:32,960 --> 00:39:36,320
may be providing a little bit more hype

1082
00:39:34,800 --> 00:39:40,320
than is is warranted and that may be

1083
00:39:36,320 --> 00:39:43,280
contributing to the noise as well.

1084
00:39:40,320 --> 00:39:45,520
Thanks everyone. That was a fantastic um

1085
00:39:43,280 --> 00:39:48,320
kind of thinking about this from a lot

1086
00:39:45,520 --> 00:39:50,079
of different perspectives. And um I

1087
00:39:48,320 --> 00:39:52,320
think Brian, your your last point is a

1088
00:39:50,079 --> 00:39:54,119
really important one as well. Um as uh

1089
00:39:52,320 --> 00:39:56,320
as we think about kind of the the

1090
00:39:54,119 --> 00:39:59,200
sensationalization of of sharing of

1091
00:39:56,320 --> 00:40:01,760
scientific information. Um I'd love to

1092
00:39:59,200 --> 00:40:03,680
move on to another uh our next question

1093
00:40:01,760 --> 00:40:06,000
really thinking about kind of the topic

1094
00:40:03,680 --> 00:40:09,440
of tailoring messages for diverse

1095
00:40:06,000 --> 00:40:11,680
audiences. Um and the question is you

1096
00:40:09,440 --> 00:40:14,000
know how can um science communicators

1097
00:40:11,680 --> 00:40:16,000
effectively tailor their message to meet

1098
00:40:14,000 --> 00:40:19,280
the needs values and cultural contexts

1099
00:40:16,000 --> 00:40:21,520
of diverse communities again to um this

1100
00:40:19,280 --> 00:40:23,440
has been discussed earlier but without

1101
00:40:21,520 --> 00:40:25,359
compromising the accuracy or integrity

1102
00:40:23,440 --> 00:40:26,960
of the science like we've already talked

1103
00:40:25,359 --> 00:40:28,480
about some ways to do it. I would love

1104
00:40:26,960 --> 00:40:30,560
to hear if there's any additional

1105
00:40:28,480 --> 00:40:33,560
thoughts um on how to do that

1106
00:40:30,560 --> 00:40:33,560
effectively.

1107
00:40:40,640 --> 00:40:45,119
Shanica, do you want to go

1108
00:40:42,760 --> 00:40:48,040
first? Well, I was just thinking about

1109
00:40:45,119 --> 00:40:50,040
the fact that many of us are from

1110
00:40:48,040 --> 00:40:54,240
communities and

1111
00:40:50,040 --> 00:40:56,000
um you know, we bring a perspective

1112
00:40:54,240 --> 00:40:58,000
um from our communities to these

1113
00:40:56,000 --> 00:41:00,240
conversations and to this messaging. And

1114
00:40:58,000 --> 00:41:03,160
I think that's valuable. I think that's

1115
00:41:00,240 --> 00:41:05,599
why workforce diversity is so critically

1116
00:41:03,160 --> 00:41:08,880
important. Um, in addition to meeting

1117
00:41:05,599 --> 00:41:12,240
people where they are, um, I really

1118
00:41:08,880 --> 00:41:15,560
would I do hope that in the future there

1119
00:41:12,240 --> 00:41:19,240
are opportunities to increase our

1120
00:41:15,560 --> 00:41:21,599
investment in long-term sustainable

1121
00:41:19,240 --> 00:41:23,240
relationships with the groups that we

1122
00:41:21,599 --> 00:41:25,839
really want to be a part of our

1123
00:41:23,240 --> 00:41:29,520
research. because it's not just

1124
00:41:25,839 --> 00:41:31,920
education level, it's cultural values.

1125
00:41:29,520 --> 00:41:35,040
a native indigenous group, for example,

1126
00:41:31,920 --> 00:41:39,680
having very significant beliefs and

1127
00:41:35,040 --> 00:41:42,079
values around DNA and being able to

1128
00:41:39,680 --> 00:41:44,560
understand those values, talk about

1129
00:41:42,079 --> 00:41:47,520
those values, and develop a project and

1130
00:41:44,560 --> 00:41:52,480
then describe the results in a way that

1131
00:41:47,520 --> 00:41:55,720
is respectful of and integrates the

1132
00:41:52,480 --> 00:41:59,440
groups or nations beliefs.

1133
00:41:55,720 --> 00:42:01,920
Um, you just can't do that without

1134
00:41:59,440 --> 00:42:04,800
birectional education. We're not just

1135
00:42:01,920 --> 00:42:07,800
educating communities. They must educate

1136
00:42:04,800 --> 00:42:07,800
us.

1137
00:42:09,119 --> 00:42:12,960
I I want to emphasize that point a

1138
00:42:11,119 --> 00:42:14,800
little further to Shenika just to say,

1139
00:42:12,960 --> 00:42:16,720
you know, there there are all kinds of

1140
00:42:14,800 --> 00:42:18,800
reasons why the diversity of our

1141
00:42:16,720 --> 00:42:20,640
workforce matters, but it also it

1142
00:42:18,800 --> 00:42:22,000
matters both in terms of good science

1143
00:42:20,640 --> 00:42:23,280
and it matters in terms of building

1144
00:42:22,000 --> 00:42:24,800
these bridges. And it's just something

1145
00:42:23,280 --> 00:42:27,040
we have to have the courage to come out

1146
00:42:24,800 --> 00:42:28,480
and say that it does matter not just

1147
00:42:27,040 --> 00:42:30,000
that we have a range of people

1148
00:42:28,480 --> 00:42:31,440
participating in studies, but we have a

1149
00:42:30,000 --> 00:42:33,280
range of researchers conducting those

1150
00:42:31,440 --> 00:42:35,200
studies and that we've got um you know

1151
00:42:33,280 --> 00:42:37,680
folks that have and it goes back to that

1152
00:42:35,200 --> 00:42:39,760
sense of um of shared interest. It goes

1153
00:42:37,680 --> 00:42:41,040
back but it also just adds to um you

1154
00:42:39,760 --> 00:42:42,640
know the range of perspectives. It's

1155
00:42:41,040 --> 00:42:44,800
better science um you know often from

1156
00:42:42,640 --> 00:42:47,359
that perspective. This is something that

1157
00:42:44,800 --> 00:42:48,560
um I noted earlier on one of the

1158
00:42:47,359 --> 00:42:50,560
initiatives that we're working on with

1159
00:42:48,560 --> 00:42:52,640
the national institutes of health um for

1160
00:42:50,560 --> 00:42:54,960
the all of us you know initiative um

1161
00:42:52,640 --> 00:42:56,960
which I'll put the a link in the chat

1162
00:42:54,960 --> 00:42:58,960
here but where we are actively trying to

1163
00:42:56,960 --> 00:43:00,640
think about who is it that's actually

1164
00:42:58,960 --> 00:43:02,160
conducting research using you know

1165
00:43:00,640 --> 00:43:03,440
publicly available data sets who is it

1166
00:43:02,160 --> 00:43:05,119
that's actually contributing to research

1167
00:43:03,440 --> 00:43:06,960
literature and what could we do to

1168
00:43:05,119 --> 00:43:09,760
broaden and and extend you know that

1169
00:43:06,960 --> 00:43:12,400
that range of people um and geographic

1170
00:43:09,760 --> 00:43:14,319
areas uh demographic differences all of

1171
00:43:12,400 --> 00:43:16,560
those things uh you really do matter um

1172
00:43:14,319 --> 00:43:18,160
in really important ways. So I I think

1173
00:43:16,560 --> 00:43:20,400
that's one way to think about how it is

1174
00:43:18,160 --> 00:43:22,240
we might better um you know shape

1175
00:43:20,400 --> 00:43:23,839
messages for different communities is to

1176
00:43:22,240 --> 00:43:25,839
have a better sense and representation

1177
00:43:23,839 --> 00:43:28,319
from those communities in the in the

1178
00:43:25,839 --> 00:43:30,240
groups that are doing that work. Yeah.

1179
00:43:28,319 --> 00:43:31,680
Yeah. Thanks Brian and Chenika. Really

1180
00:43:30,240 --> 00:43:34,319
important point about kind of the

1181
00:43:31,680 --> 00:43:36,680
relationship between um who's delivering

1182
00:43:34,319 --> 00:43:40,400
the message and how it's being received.

1183
00:43:36,680 --> 00:43:42,160
Um yeah, thanks. uh could go on on that

1184
00:43:40,400 --> 00:43:44,800
topic for a little bit longer, but I

1185
00:43:42,160 --> 00:43:47,280
think in um I'd love to kind of move on

1186
00:43:44,800 --> 00:43:50,240
to our next question, which is uh

1187
00:43:47,280 --> 00:43:53,280
thinking about shared community spaces

1188
00:43:50,240 --> 00:43:55,520
um uh and and the role that they can

1189
00:43:53,280 --> 00:43:58,319
play in increasing access and inclusion

1190
00:43:55,520 --> 00:44:01,200
in science uh the the receiving of

1191
00:43:58,319 --> 00:44:03,680
science and um science communication. Um

1192
00:44:01,200 --> 00:44:06,400
what opportunities exist um kind of from

1193
00:44:03,680 --> 00:44:07,760
your perspectives and uh for community

1194
00:44:06,400 --> 00:44:10,079
spaces? So here you're thinking of

1195
00:44:07,760 --> 00:44:12,240
museums, libraries and other

1196
00:44:10,079 --> 00:44:14,880
institutions to increase access and

1197
00:44:12,240 --> 00:44:16,960
inclusion uh to broaden the audience for

1198
00:44:14,880 --> 00:44:18,800
science communication efforts and how

1199
00:44:16,960 --> 00:44:22,280
can these spaces help bridge gaps in

1200
00:44:18,800 --> 00:44:22,280
trust and understanding.

1201
00:44:23,920 --> 00:44:30,240
uh I can try to address that question

1202
00:44:28,160 --> 00:44:34,400
given you know from my firsthand

1203
00:44:30,240 --> 00:44:36,240
experience uh trying to uh develop a

1204
00:44:34,400 --> 00:44:39,200
programming that is associated with our

1205
00:44:36,240 --> 00:44:41,119
public engagement space uh one of the

1206
00:44:39,200 --> 00:44:44,040
things that really helped us from this

1207
00:44:41,119 --> 00:44:47,440
is even before our museum was built or

1208
00:44:44,040 --> 00:44:49,520
launched is uh we decided to bring

1209
00:44:47,440 --> 00:44:52,640
together a group uh as a community

1210
00:44:49,520 --> 00:44:55,440
advisory group. So we try to identify

1211
00:44:52,640 --> 00:44:58,400
who would be our variety of audiences

1212
00:44:55,440 --> 00:45:01,680
and the types of research that we do who

1213
00:44:58,400 --> 00:45:04,720
would that impact and uh we identified

1214
00:45:01,680 --> 00:45:07,920
members from a variety of uh groups like

1215
00:45:04,720 --> 00:45:09,920
education, patient engagement community,

1216
00:45:07,920 --> 00:45:12,400
uh art community, accessibility

1217
00:45:09,920 --> 00:45:15,599
community and brought them together and

1218
00:45:12,400 --> 00:45:18,480
uh had them kind of advise us on uh a

1219
00:45:15,599 --> 00:45:20,640
variety of our programming on we worked

1220
00:45:18,480 --> 00:45:22,480
very closely with uh the patient

1221
00:45:20,640 --> 00:45:25,040
engagement community for two of our

1222
00:45:22,480 --> 00:45:26,880
galleries in the museum that is based on

1223
00:45:25,040 --> 00:45:30,319
psychiatric research and cancer

1224
00:45:26,880 --> 00:45:32,400
research. So pretty much every content

1225
00:45:30,319 --> 00:45:34,560
that is in the museum for those two

1226
00:45:32,400 --> 00:45:38,400
galleries were thoroughly reviewed,

1227
00:45:34,560 --> 00:45:40,000
feedback was given and uh even the title

1228
00:45:38,400 --> 00:45:41,599
of one of the galleries was changed

1229
00:45:40,000 --> 00:45:44,160
because of feedback from the patient

1230
00:45:41,599 --> 00:45:45,839
community. So I think that's one way I

1231
00:45:44,160 --> 00:45:48,800
would say in a shared community space

1232
00:45:45,839 --> 00:45:51,000
how we thought about having community

1233
00:45:48,800 --> 00:45:53,200
voices represented through local

1234
00:45:51,000 --> 00:45:56,119
partnerships. And then the other thing I

1235
00:45:53,200 --> 00:45:59,119
would like to say is

1236
00:45:56,119 --> 00:46:00,400
um that really helps in these spaces is

1237
00:45:59,119 --> 00:46:02,079
doing some kind of layered

1238
00:46:00,400 --> 00:46:03,920
communication. That means all your

1239
00:46:02,079 --> 00:46:06,319
audience or visitors will not be

1240
00:46:03,920 --> 00:46:08,640
engaging with the same thing. So we make

1241
00:46:06,319 --> 00:46:10,960
sure that they have different levels of

1242
00:46:08,640 --> 00:46:13,040
engagement opportunities available for

1243
00:46:10,960 --> 00:46:14,960
them. So when we participate in the

1244
00:46:13,040 --> 00:46:17,359
Cambridge Science Festival or when we do

1245
00:46:14,960 --> 00:46:19,359
public talks or even when we are doing

1246
00:46:17,359 --> 00:46:21,119
tours, we are always mindful of

1247
00:46:19,359 --> 00:46:23,920
different learning abilities and

1248
00:46:21,119 --> 00:46:26,560
learning skills and uh interest levels

1249
00:46:23,920 --> 00:46:28,240
of our visitors and how we can try to

1250
00:46:26,560 --> 00:46:30,880
meet them where they are based on

1251
00:46:28,240 --> 00:46:32,240
different types of offerings. Uh and we

1252
00:46:30,880 --> 00:46:34,160
are learning of course from visitor

1253
00:46:32,240 --> 00:46:35,599
feedback and try to improve our

1254
00:46:34,160 --> 00:46:37,760
programming and offerings on a

1255
00:46:35,599 --> 00:46:41,560
year-to-year basis based on the feedback

1256
00:46:37,760 --> 00:46:41,560
we get from the community.

1257
00:46:42,160 --> 00:46:46,480
That's fantastic, Namartha. Um, I think

1258
00:46:44,240 --> 00:46:51,119
that speaks to how important it is to

1259
00:46:46,480 --> 00:46:53,440
engage with um with a broad uh audience

1260
00:46:51,119 --> 00:46:55,520
uh broad and diverse audience from kind

1261
00:46:53,440 --> 00:46:58,640
of thinking about this from many um

1262
00:46:55,520 --> 00:47:00,640
demographic points of view in in helping

1263
00:46:58,640 --> 00:47:04,000
you as you all did with the museum

1264
00:47:00,640 --> 00:47:06,160
project to to formulate um scientific

1265
00:47:04,000 --> 00:47:08,640
messages and things like that. So,

1266
00:47:06,160 --> 00:47:11,839
really wonderful example. Brian and

1267
00:47:08,640 --> 00:47:15,400
Sheniqua, any thoughts on how um shared

1268
00:47:11,839 --> 00:47:18,400
community spaces can play a role in um

1269
00:47:15,400 --> 00:47:20,480
increasing access? I think this is one

1270
00:47:18,400 --> 00:47:22,480
of the central questions of one of the

1271
00:47:20,480 --> 00:47:24,319
central concerns we face as a society at

1272
00:47:22,480 --> 00:47:26,000
the moment is the the decline of

1273
00:47:24,319 --> 00:47:28,800
so-called you know third spaces or

1274
00:47:26,000 --> 00:47:30,480
public spaces or forums different ways.

1275
00:47:28,800 --> 00:47:33,200
Um I've been doing a lot of work you

1276
00:47:30,480 --> 00:47:36,240
know thinking in the space of um

1277
00:47:33,200 --> 00:47:38,480
hesitancy about vaccination and lots of

1278
00:47:36,240 --> 00:47:40,319
different you know um explanations get

1279
00:47:38,480 --> 00:47:42,640
uh put forward for that but one one

1280
00:47:40,319 --> 00:47:44,800
important predictor um is actually um

1281
00:47:42,640 --> 00:47:46,720
you know living in areas that uh have

1282
00:47:44,800 --> 00:47:49,359
suffered from declines in in healthcare

1283
00:47:46,720 --> 00:47:50,720
access um you know access to and

1284
00:47:49,359 --> 00:47:52,960
engagement with with healthcare

1285
00:47:50,720 --> 00:47:54,240
facilities really matters um in really

1286
00:47:52,960 --> 00:47:56,640
important ways in terms of building up

1287
00:47:54,240 --> 00:47:58,800
trust you know over time um and I think

1288
00:47:56,640 --> 00:48:00,480
similarly when it comes to science um at

1289
00:47:58,800 --> 00:48:02,400
the end of the day uh you know

1290
00:48:00,480 --> 00:48:04,960
libraries, science museums, science

1291
00:48:02,400 --> 00:48:07,200
centers have the real potential to be a

1292
00:48:04,960 --> 00:48:09,359
path forward and to be just centrally

1293
00:48:07,200 --> 00:48:11,839
important for our society. So if you are

1294
00:48:09,359 --> 00:48:13,839
in a rural area that doesn't have such a

1295
00:48:11,839 --> 00:48:16,000
a thing, thinking about finding a way to

1296
00:48:13,839 --> 00:48:18,160
invest in, you know, could be a really

1297
00:48:16,000 --> 00:48:19,760
crucial path forward. Many of us live in

1298
00:48:18,160 --> 00:48:21,520
places where there's a plethora and a

1299
00:48:19,760 --> 00:48:22,800
luxury and we don't recognize just how

1300
00:48:21,520 --> 00:48:25,599
much it matters when those things are

1301
00:48:22,800 --> 00:48:28,160
absent. Um, and so it's incredible to

1302
00:48:25,599 --> 00:48:30,640
have um, you know, uh, resources like

1303
00:48:28,160 --> 00:48:32,240
what Namartha, you know, runs and has

1304
00:48:30,640 --> 00:48:33,280
available there. That's great, but what

1305
00:48:32,240 --> 00:48:35,040
about those communities that don't

1306
00:48:33,280 --> 00:48:36,480
necessarily have those physical spaces?

1307
00:48:35,040 --> 00:48:38,800
What could we be done to partner with

1308
00:48:36,480 --> 00:48:40,720
those as well? I think if we did a lot

1309
00:48:38,800 --> 00:48:42,400
more of that, um, we may have some of

1310
00:48:40,720 --> 00:48:44,640
the less we'd have to ring our hands

1311
00:48:42,400 --> 00:48:47,040
less about some of the other concerns

1312
00:48:44,640 --> 00:48:49,200
because, um, I a lot of people do show

1313
00:48:47,040 --> 00:48:51,440
interest and and and excitement and are

1314
00:48:49,200 --> 00:48:53,280
willing to bring families to to

1315
00:48:51,440 --> 00:48:55,359
situations like that. if you can frame

1316
00:48:53,280 --> 00:48:58,359
them um as being useful and relevant to

1317
00:48:55,359 --> 00:48:58,359
them.

1318
00:48:58,400 --> 00:49:04,240
And I I have limited experience in these

1319
00:49:00,640 --> 00:49:07,520
kinds of um spaces as an academic. I I

1320
00:49:04,240 --> 00:49:10,480
belong to so many places where I engage

1321
00:49:07,520 --> 00:49:12,400
so many third spaces, but um I've had

1322
00:49:10,480 --> 00:49:16,160
limited opportunities, one time at a

1323
00:49:12,400 --> 00:49:18,040
museum in DC and another at a church

1324
00:49:16,160 --> 00:49:21,359
collaboration with an

1325
00:49:18,040 --> 00:49:23,440
H.B.CU. And um the HB.CU BCU church

1326
00:49:21,359 --> 00:49:25,440
collaboration was organized by a

1327
00:49:23,440 --> 00:49:28,480
professor who's very active at her

1328
00:49:25,440 --> 00:49:32,640
H.B.CU and also at her church and there

1329
00:49:28,480 --> 00:49:34,880
were maybe 400 people who attended and

1330
00:49:32,640 --> 00:49:36,400
one of the reasons I think so many

1331
00:49:34,880 --> 00:49:38,160
people came was because we talked about

1332
00:49:36,400 --> 00:49:40,400
an area of interest to them. We talked

1333
00:49:38,160 --> 00:49:44,319
about genetic testing. Many of them were

1334
00:49:40,400 --> 00:49:45,920
using um online companies at the time or

1335
00:49:44,319 --> 00:49:48,559
exploring their own stories through

1336
00:49:45,920 --> 00:49:51,520
genetic genealogy. And so we brought in

1337
00:49:48,559 --> 00:49:53,760
an expert who um works in genetic

1338
00:49:51,520 --> 00:49:56,240
genealogy to speak and then we also

1339
00:49:53,760 --> 00:49:59,280
brought in a biologist to speak and then

1340
00:49:56,240 --> 00:50:02,160
an Elsie scholar uh two Elsie scholars

1341
00:49:59,280 --> 00:50:04,160
and we had this sort of mixture of

1342
00:50:02,160 --> 00:50:07,440
speakers who could talk about different

1343
00:50:04,160 --> 00:50:09,280
aspects of the technology, how it works.

1344
00:50:07,440 --> 00:50:11,839
People were very interested simply in

1345
00:50:09,280 --> 00:50:14,800
practical implications, how it works and

1346
00:50:11,839 --> 00:50:16,079
then also some of the social um issues.

1347
00:50:14,800 --> 00:50:17,440
And I think what's really great about

1348
00:50:16,079 --> 00:50:18,800
that is when you're bringing together

1349
00:50:17,440 --> 00:50:21,119
people who are members of the same

1350
00:50:18,800 --> 00:50:22,800
church, they're in their own space. We

1351
00:50:21,119 --> 00:50:24,559
were in the church auditorium.

1352
00:50:22,800 --> 00:50:27,040
Everyone's in their own space. They're

1353
00:50:24,559 --> 00:50:29,359
with people they see every single week.

1354
00:50:27,040 --> 00:50:32,160
Um and I felt as if the audience members

1355
00:50:29,359 --> 00:50:34,319
were very comfortable talking and giving

1356
00:50:32,160 --> 00:50:37,359
feedback among their other church

1357
00:50:34,319 --> 00:50:40,480
members. And the students of course um

1358
00:50:37,359 --> 00:50:43,520
were very vocal as well. And so there's

1359
00:50:40,480 --> 00:50:46,920
just something very special about being

1360
00:50:43,520 --> 00:50:49,280
in a community space that belongs to the

1361
00:50:46,920 --> 00:50:50,520
community and really letting the

1362
00:50:49,280 --> 00:50:54,640
community

1363
00:50:50,520 --> 00:50:57,800
members sort of develop the program. Uh

1364
00:50:54,640 --> 00:51:00,960
the professor we work

1365
00:50:57,800 --> 00:51:02,800
together thinking about what our

1366
00:51:00,960 --> 00:51:04,960
colleagues and community members might

1367
00:51:02,800 --> 00:51:07,440
be interested in. And I think that led

1368
00:51:04,960 --> 00:51:09,599
to it being you know such a success. And

1369
00:51:07,440 --> 00:51:12,240
for the museum event, it's it was at the

1370
00:51:09,599 --> 00:51:13,920
African-American Museum in in DC. And

1371
00:51:12,240 --> 00:51:16,000
people are on the list serve. They

1372
00:51:13,920 --> 00:51:17,280
follow what talks and events are

1373
00:51:16,000 --> 00:51:19,280
happening at the museum. And if

1374
00:51:17,280 --> 00:51:22,559
something's interesting, they come. It's

1375
00:51:19,280 --> 00:51:24,880
so special. So um if you are presenting

1376
00:51:22,559 --> 00:51:27,839
work and you have the opportunity to or

1377
00:51:24,880 --> 00:51:30,160
want to pursue um going to a local

1378
00:51:27,839 --> 00:51:32,880
organization that people can travel to

1379
00:51:30,160 --> 00:51:34,640
or going to travel to a community,

1380
00:51:32,880 --> 00:51:37,200
there's just really nothing like it. I I

1381
00:51:34,640 --> 00:51:39,599
I really think everyone should do it if

1382
00:51:37,200 --> 00:51:42,000
they can.

1383
00:51:39,599 --> 00:51:43,760
Thanks. That's lovely lovely story.

1384
00:51:42,000 --> 00:51:46,160
Thanks for sharing that, Sheniwen. It it

1385
00:51:43,760 --> 00:51:48,000
reminds me of um some really amazing

1386
00:51:46,160 --> 00:51:49,839
work that's happening with uh our

1387
00:51:48,000 --> 00:51:52,079
colleagues in Count Me In going into

1388
00:51:49,839 --> 00:51:53,520
community spaces and asking the

1389
00:51:52,079 --> 00:51:55,440
important question of what would you

1390
00:51:53,520 --> 00:51:57,880
like to learn about and what can we

1391
00:51:55,440 --> 00:52:00,079
bring that would be useful to the

1392
00:51:57,880 --> 00:52:02,640
community to your community in

1393
00:52:00,079 --> 00:52:05,839
particular. Um, so that that I love that

1394
00:52:02,640 --> 00:52:08,400
I love that example. Um, I'm going to we

1395
00:52:05,839 --> 00:52:09,760
uh as we had predicted we that we were

1396
00:52:08,400 --> 00:52:12,480
going to have such a rich conversation

1397
00:52:09,760 --> 00:52:14,720
and and run out of time. I'd love to

1398
00:52:12,480 --> 00:52:17,119
transition to an um question from the

1399
00:52:14,720 --> 00:52:19,680
audience. Uh

1400
00:52:17,119 --> 00:52:21,839
uh from Curtis uh who is one of your

1401
00:52:19,680 --> 00:52:25,400
favorite science communicators, a hero,

1402
00:52:21,839 --> 00:52:25,400
past or present?

1403
00:52:28,000 --> 00:52:33,920
I'll go first. Um, I think the larger

1404
00:52:31,839 --> 00:52:37,599
majority community sometimes often

1405
00:52:33,920 --> 00:52:40,240
forgets that we had people like web deo

1406
00:52:37,599 --> 00:52:40,240
in the

1407
00:52:40,280 --> 00:52:45,680
1800s

1408
00:52:42,280 --> 00:52:48,480
following and rigorously measuring the

1409
00:52:45,680 --> 00:52:50,640
state of black communities. And I

1410
00:52:48,480 --> 00:52:54,960
purchased a book recently where of

1411
00:52:50,640 --> 00:52:56,880
graphs he produced of um black the state

1412
00:52:54,960 --> 00:53:00,319
of black communities and families over

1413
00:52:56,880 --> 00:53:02,960
time. and he communicated so many

1414
00:53:00,319 --> 00:53:05,440
different ways. He had newsletters for

1415
00:53:02,960 --> 00:53:08,800
communities. He had the manuscripts that

1416
00:53:05,440 --> 00:53:12,440
he wrote. And um so I just really

1417
00:53:08,800 --> 00:53:15,680
appreciated his his ability

1418
00:53:12,440 --> 00:53:17,760
to work through these newsletters where

1419
00:53:15,680 --> 00:53:21,520
that are very community and public

1420
00:53:17,760 --> 00:53:24,480
facing and accessible and then all or

1421
00:53:21,520 --> 00:53:26,680
newspapers and then also continue with

1422
00:53:24,480 --> 00:53:31,880
his academic work and just leave us such

1423
00:53:26,680 --> 00:53:31,880
nuggets that are still relevant today.

1424
00:53:33,040 --> 00:53:37,520
as a as a no as a social scientist

1425
00:53:35,599 --> 00:53:38,960
myself. I love that answer like it's

1426
00:53:37,520 --> 00:53:40,400
just because it just in terms of

1427
00:53:38,960 --> 00:53:42,640
thinking about and you're absolutely

1428
00:53:40,400 --> 00:53:44,559
right because so much of that was

1429
00:53:42,640 --> 00:53:46,880
contribution to public dialogue that

1430
00:53:44,559 --> 00:53:48,400
wasn't framed as capital S science um

1431
00:53:46,880 --> 00:53:50,160
you know per se it was more like you

1432
00:53:48,400 --> 00:53:52,559
let's organize some of these ideas and

1433
00:53:50,160 --> 00:53:54,160
knowledge you know in different ways um

1434
00:53:52,559 --> 00:53:55,520
yeah I was thinking there's there's lots

1435
00:53:54,160 --> 00:53:57,680
of different examples but you know in

1436
00:53:55,520 --> 00:53:59,839
terms of popular culture you know folks

1437
00:53:57,680 --> 00:54:03,200
that have been able to tap into and and

1438
00:53:59,839 --> 00:54:05,839
and talk about more abstract um concepts

1439
00:54:03,200 --> 00:54:07,359
that maybe wouldn't have been thought to

1440
00:54:05,839 --> 00:54:09,440
uh you know all that you know

1441
00:54:07,359 --> 00:54:11,280
interesting or relevant. Carl Sean's a

1442
00:54:09,440 --> 00:54:13,440
good you know example of somebody that

1443
00:54:11,280 --> 00:54:14,559
um you know here's here's a wide and

1444
00:54:13,440 --> 00:54:16,079
expansive a lot to think about

1445
00:54:14,559 --> 00:54:18,559
philosophically you know there but that

1446
00:54:16,079 --> 00:54:19,920
and people have interest in those um in

1447
00:54:18,559 --> 00:54:21,920
those ideas that's something people that

1448
00:54:19,920 --> 00:54:23,920
have the courage to talk about things

1449
00:54:21,920 --> 00:54:25,280
that are deemed to not necessarily be

1450
00:54:23,920 --> 00:54:27,599
particularly interesting. there's an

1451
00:54:25,280 --> 00:54:29,280
awful lot of heroism um heroins and

1452
00:54:27,599 --> 00:54:31,760
heroes you know during the course of the

1453
00:54:29,280 --> 00:54:33,599
pandemic people that on a local level

1454
00:54:31,760 --> 00:54:35,599
were explaining things for their um

1455
00:54:33,599 --> 00:54:37,440
local community um and and that's

1456
00:54:35,599 --> 00:54:39,520
something that I um you know really

1457
00:54:37,440 --> 00:54:41,920
appreciated um you know as well just

1458
00:54:39,520 --> 00:54:43,920
seeing so much of of was happening on on

1459
00:54:41,920 --> 00:54:45,280
social media in a smaller scale of

1460
00:54:43,920 --> 00:54:46,800
people who were trying to make sense of

1461
00:54:45,280 --> 00:54:49,920
this really confusing time for many

1462
00:54:46,800 --> 00:54:51,920
families. Yeah.

1463
00:54:49,920 --> 00:54:53,760
Yeah. Uh my favorite science

1464
00:54:51,920 --> 00:54:56,640
communicator is possibly not a popular

1465
00:54:53,760 --> 00:54:58,880
name in the US but uh just I have a

1466
00:54:56,640 --> 00:55:00,960
personal association uh personal

1467
00:54:58,880 --> 00:55:03,920
inspiration story associated with this

1468
00:55:00,960 --> 00:55:06,119
person is a scientist who ba who's based

1469
00:55:03,920 --> 00:55:09,599
in India his name is Dr. Dr. Tupil

1470
00:55:06,119 --> 00:55:11,440
Bankesh and uh I when I was in college I

1471
00:55:09,599 --> 00:55:13,599
happened to hear him speak once in an

1472
00:55:11,440 --> 00:55:16,480
auditorium packed with hundreds of

1473
00:55:13,599 --> 00:55:19,760
students uh undergrad and mast's level

1474
00:55:16,480 --> 00:55:22,000
students and I was just it's just not me

1475
00:55:19,760 --> 00:55:24,960
most of us in the audience were just

1476
00:55:22,000 --> 00:55:27,119
amazed uh he talked about lead toxicity

1477
00:55:24,960 --> 00:55:28,559
and how that was a pretty big public

1478
00:55:27,119 --> 00:55:30,800
health concern in different parts of

1479
00:55:28,559 --> 00:55:33,040
India occupational health and some other

1480
00:55:30,800 --> 00:55:36,240
ways and

1481
00:55:33,040 --> 00:55:38,240
uh that really inspired me to be a

1482
00:55:36,240 --> 00:55:40,400
science communicator. I did not have the

1483
00:55:38,240 --> 00:55:42,640
vocabulary for the word science

1484
00:55:40,400 --> 00:55:44,720
communicator. This is years ago. I

1485
00:55:42,640 --> 00:55:47,280
thought uh I just called him a science

1486
00:55:44,720 --> 00:55:49,599
motivational speaker because I was just

1487
00:55:47,280 --> 00:55:51,680
impressed but how like widely he would

1488
00:55:49,599 --> 00:55:53,680
travel and talk to communities about

1489
00:55:51,680 --> 00:55:55,440
lead toxicity and that actually piqued

1490
00:55:53,680 --> 00:55:58,480
my interest in pursuing my PhD in

1491
00:55:55,440 --> 00:56:00,480
toxicology as well. So he will always be

1492
00:55:58,480 --> 00:56:02,799
uh my favorite science communicator who

1493
00:56:00,480 --> 00:56:06,960
got me on this path of both my

1494
00:56:02,799 --> 00:56:08,960
scientific training and my profession.

1495
00:56:06,960 --> 00:56:12,400
Fantastic. That's a lovely story. Thanks

1496
00:56:08,960 --> 00:56:15,680
Namita for sharing that. Uh we have just

1497
00:56:12,400 --> 00:56:17,680
a couple minutes away. Um and I think we

1498
00:56:15,680 --> 00:56:19,960
could go on for another easily for

1499
00:56:17,680 --> 00:56:24,240
another 30 minutes on this discussion.

1500
00:56:19,960 --> 00:56:26,000
Um uh but I want to first thank our our

1501
00:56:24,240 --> 00:56:28,720
fantastic panelists for joining us

1502
00:56:26,000 --> 00:56:30,799
today. Thank you to the audience for for

1503
00:56:28,720 --> 00:56:33,119
joining us and engaging with the this

1504
00:56:30,799 --> 00:56:36,079
important topic. And I'd like to end

1505
00:56:33,119 --> 00:56:41,599
with um just a quick um announcement for

1506
00:56:36,079 --> 00:56:45,200
a feature um uh EBM that is uh our next

1507
00:56:41,599 --> 00:56:48,559
EBM that's scheduled for February 14th.

1508
00:56:45,200 --> 00:56:50,720
Um and that is going to take place in

1509
00:56:48,559 --> 00:56:54,240
partnership with the LC Friday forum. So

1510
00:56:50,720 --> 00:56:56,079
this is uh the Sarah the folks at Sarah

1511
00:56:54,240 --> 00:56:57,920
um and the topic will be building

1512
00:56:56,079 --> 00:57:00,640
infrastructures

1513
00:56:57,920 --> 00:57:04,079
um to enhance community engagement work.

1514
00:57:00,640 --> 00:57:06,960
So uh keep an eye out for um kind of

1515
00:57:04,079 --> 00:57:08,960
links to register for that session. Uh

1516
00:57:06,960 --> 00:57:12,240
they'll be this this as I mentioned will

1517
00:57:08,960 --> 00:57:15,440
be co-hosted with uh Sarah's LChub uh

1518
00:57:12,240 --> 00:57:17,599
Friday forum um folks. Again, thanks

1519
00:57:15,440 --> 00:57:20,319
everyone for joining us uh this

1520
00:57:17,599 --> 00:57:23,200
afternoon for a rich discussion um and

1521
00:57:20,319 --> 00:57:25,280
we hope to keep the conversation going

1522
00:57:23,200 --> 00:57:27,200
um across kind of thinking about uh

1523
00:57:25,280 --> 00:57:29,200
trust and science from a lot of

1524
00:57:27,200 --> 00:57:31,599
different dimensions and we appreciate

1525
00:57:29,200 --> 00:57:34,839
you all joining us and being part of it.

1526
00:57:31,599 --> 00:57:34,839
Thank you.

