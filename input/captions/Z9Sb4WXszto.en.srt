1
00:00:04,400 --> 00:00:09,599
It is my pleasure now to introduce you

2
00:00:06,480 --> 00:00:12,639
to Neil Gersonfeld who is the director

3
00:00:09,599 --> 00:00:15,639
for the MIT Center for Bits and Atoms.

4
00:00:12,639 --> 00:00:15,639
Welcome.

5
00:00:19,039 --> 00:00:25,119
>> Thank you, Jim. So I'm going to end the

6
00:00:22,560 --> 00:00:27,920
day by pointing out a number of

7
00:00:25,119 --> 00:00:30,720
assumptions that have been uh throughout

8
00:00:27,920 --> 00:00:33,680
the day without being considered and the

9
00:00:30,720 --> 00:00:35,920
opportunity to revisit them. So I lead

10
00:00:33,680 --> 00:00:37,760
the center for bits and atoms. We were

11
00:00:35,920 --> 00:00:40,160
uh created to study the boundary of

12
00:00:37,760 --> 00:00:42,000
digital and physical. Over the year

13
00:00:40,160 --> 00:00:44,079
we've done things like the first faster

14
00:00:42,000 --> 00:00:46,239
than classical quantum computations. We

15
00:00:44,079 --> 00:00:48,399
introduced cryptography and materials,

16
00:00:46,239 --> 00:00:49,760
logic and micrfluidics.

17
00:00:48,399 --> 00:00:51,440
uh we were part of creating the first

18
00:00:49,760 --> 00:00:53,920
synthetic life, part of the group

19
00:00:51,440 --> 00:00:55,520
creating what became internet of things.

20
00:00:53,920 --> 00:00:57,840
That that's what's come from working at

21
00:00:55,520 --> 00:01:00,239
that boundary. And so with all of that

22
00:00:57,840 --> 00:01:03,199
experience, uh it leads up to my

23
00:01:00,239 --> 00:01:04,799
observation in this TED talk that

24
00:01:03,199 --> 00:01:07,040
computer science was one of the worst

25
00:01:04,799 --> 00:01:09,200
things ever to happen to computers or to

26
00:01:07,040 --> 00:01:11,600
science.

27
00:01:09,200 --> 00:01:13,520
And uh to I'll be explaining that

28
00:01:11,600 --> 00:01:16,080
because computer science is based on a

29
00:01:13,520 --> 00:01:18,080
fiction that ignores physics. And today

30
00:01:16,080 --> 00:01:19,840
is going to be all about what does it

31
00:01:18,080 --> 00:01:23,040
mean for computer science to embrace

32
00:01:19,840 --> 00:01:26,000
rather than avoid physics.

33
00:01:23,040 --> 00:01:27,840
So uh here are three thesis from my

34
00:01:26,000 --> 00:01:31,280
group that almost surely you'll be

35
00:01:27,840 --> 00:01:33,520
uninterested in. Um one is on optical

36
00:01:31,280 --> 00:01:36,000
pumping for NMR, one is on micros slots

37
00:01:33,520 --> 00:01:39,520
for electromagnetic instrumentation. Um

38
00:01:36,000 --> 00:01:41,520
one is for embedding internet zero. So

39
00:01:39,520 --> 00:01:45,280
those three authors, one of them was

40
00:01:41,520 --> 00:01:47,439
Jason. Jason built uh Facebook's

41
00:01:45,280 --> 00:01:50,240
computers, then Meta's computers. He was

42
00:01:47,439 --> 00:01:52,640
their VP of infrastructure. Um he helped

43
00:01:50,240 --> 00:01:54,399
lead running servers hot. Uh Microsoft

44
00:01:52,640 --> 00:01:56,560
hired him recently to be in charge of

45
00:01:54,399 --> 00:01:57,920
their accelerated computing. Right now

46
00:01:56,560 --> 00:02:01,040
he's pushing the frontiers of

47
00:01:57,920 --> 00:02:03,040
interconnect. So uh that was uh Jason

48
00:02:01,040 --> 00:02:05,439
who studied optical pumping and did

49
00:02:03,040 --> 00:02:07,200
that. Um another one of those thesis was

50
00:02:05,439 --> 00:02:09,280
Rafie. He'll he built Twitter's

51
00:02:07,200 --> 00:02:12,239
computers. He had Jason's job. He was VP

52
00:02:09,280 --> 00:02:14,400
of infrastructure at uh Twitter.

53
00:02:12,239 --> 00:02:16,080
um uh he's done a number of interesting

54
00:02:14,400 --> 00:02:18,959
things since most recently just became

55
00:02:16,080 --> 00:02:23,360
Mozilla's CTO at Twitter. He helped

56
00:02:18,959 --> 00:02:25,200
observe that uh the scaling of their

57
00:02:23,360 --> 00:02:28,080
platform which was exploding

58
00:02:25,200 --> 00:02:30,720
exponentially was very manual. Uh the

59
00:02:28,080 --> 00:02:33,120
cloud was not was elastic only in

60
00:02:30,720 --> 00:02:35,360
limited degrees of freedom and they had

61
00:02:33,120 --> 00:02:37,920
an urgent need to be able to compile

62
00:02:35,360 --> 00:02:40,879
constraints into computation flexibly in

63
00:02:37,920 --> 00:02:43,760
a way that they um just weren't able to.

64
00:02:40,879 --> 00:02:47,200
So another one of those thesis was Yale

65
00:02:43,760 --> 00:02:50,879
and Yale at then at Facebook with Jason

66
00:02:47,200 --> 00:02:52,480
uh founded open compute and so they

67
00:02:50,879 --> 00:02:55,200
realized as they were building out

68
00:02:52,480 --> 00:02:57,360
Facebook that they needed to control

69
00:02:55,200 --> 00:02:59,760
what was critical to the company and

70
00:02:57,360 --> 00:03:02,400
they needed to commoditize what wasn't

71
00:02:59,760 --> 00:03:05,599
critical to the company. And so that led

72
00:03:02,400 --> 00:03:09,120
to uh roughly they killed Sun and almost

73
00:03:05,599 --> 00:03:10,879
killed Cisco. They replaced proprietary

74
00:03:09,120 --> 00:03:12,400
data centers with modular building

75
00:03:10,879 --> 00:03:16,000
blocks and many of you are now familiar

76
00:03:12,400 --> 00:03:17,920
with uh open compute which is uh

77
00:03:16,000 --> 00:03:20,720
arguably the largest open hardware

78
00:03:17,920 --> 00:03:23,599
project uh ever. So those students did

79
00:03:20,720 --> 00:03:25,920
all of that. Uh a more recent one uh

80
00:03:23,599 --> 00:03:30,000
Matt is now

81
00:03:25,920 --> 00:03:32,480
um uh an embedded developer at uh Oxide

82
00:03:30,000 --> 00:03:34,560
where they're doing private clouds. So a

83
00:03:32,480 --> 00:03:36,159
cloud in Iraq continuing that uh

84
00:03:34,560 --> 00:03:38,640
scaling.

85
00:03:36,159 --> 00:03:40,720
And so these students uh I don't think

86
00:03:38,640 --> 00:03:42,879
of myself as a computer scientist. I

87
00:03:40,720 --> 00:03:45,280
didn't teach them computer science but I

88
00:03:42,879 --> 00:03:46,799
taught them to believe in physics. So

89
00:03:45,280 --> 00:03:49,519
this is a class I teach on the physics

90
00:03:46,799 --> 00:03:51,280
of information technology. And to do it

91
00:03:49,519 --> 00:03:53,519
you can't believe in the abstraction of

92
00:03:51,280 --> 00:03:55,760
software from hardware. You need to

93
00:03:53,519 --> 00:03:59,040
think about thermodynamics and mass and

94
00:03:55,760 --> 00:04:02,400
power and time and physical resources.

95
00:03:59,040 --> 00:04:03,920
Uh so now with that as background in

96
00:04:02,400 --> 00:04:05,360
part to help earn the right to say

97
00:04:03,920 --> 00:04:07,760
computer science was one of the worst

98
00:04:05,360 --> 00:04:10,560
things. Now let's start looking ahead.

99
00:04:07,760 --> 00:04:13,120
So that was history up to the present

100
00:04:10,560 --> 00:04:15,760
and you know CBA is not that large. It's

101
00:04:13,120 --> 00:04:18,479
a disproportionate impact in data

102
00:04:15,760 --> 00:04:23,280
centers. Now let's look ahead to where

103
00:04:18,479 --> 00:04:25,600
it's going. So uh here is 11 orders of

104
00:04:23,280 --> 00:04:29,199
magnitude in scaling compute performance

105
00:04:25,600 --> 00:04:30,720
from the CDC to the Cray to Aurora.

106
00:04:29,199 --> 00:04:32,639
And if you look at the circuit boards,

107
00:04:30,720 --> 00:04:34,720
they all look the same. The chips are

108
00:04:32,639 --> 00:04:37,919
slightly different, but it's chips on a

109
00:04:34,720 --> 00:04:40,400
board in a box.

110
00:04:37,919 --> 00:04:42,720
In my lab, we've been studying the idea

111
00:04:40,400 --> 00:04:44,960
of digital materials. So in the same way

112
00:04:42,720 --> 00:04:47,040
that comput communication and

113
00:04:44,960 --> 00:04:49,280
computation became digital, we're

114
00:04:47,040 --> 00:04:51,840
looking at discreetly assembled simple

115
00:04:49,280 --> 00:04:53,360
building blocks, but we've used that to

116
00:04:51,840 --> 00:04:56,320
make the world record for the highest

117
00:04:53,360 --> 00:04:58,320
modulus ultralight structures. Uh with

118
00:04:56,320 --> 00:05:00,720
NASA made the largest morphing aeros

119
00:04:58,320 --> 00:05:02,800
structures with Toyota uh ultraefficient

120
00:05:00,720 --> 00:05:06,720
race cars.

121
00:05:02,800 --> 00:05:08,560
Uh then by routing power and data

122
00:05:06,720 --> 00:05:11,520
through these building blocks, we can

123
00:05:08,560 --> 00:05:13,039
make u structural electronics. So rather

124
00:05:11,520 --> 00:05:15,120
than electronics that goes into

125
00:05:13,039 --> 00:05:16,800
structures, we make electronics that is

126
00:05:15,120 --> 00:05:19,440
a structure but with state-of-the-art

127
00:05:16,800 --> 00:05:21,680
material properties. And then it leads

128
00:05:19,440 --> 00:05:24,800
to a really interesting kind of robotics

129
00:05:21,680 --> 00:05:26,479
where we can make robots that locomote,

130
00:05:24,800 --> 00:05:28,000
error correct and navigate on these

131
00:05:26,479 --> 00:05:30,560
cellular structures to build the

132
00:05:28,000 --> 00:05:32,560
structures. And we've been exploring

133
00:05:30,560 --> 00:05:34,000
that with NASA for space construction.

134
00:05:32,560 --> 00:05:35,759
And we have a really exciting project

135
00:05:34,000 --> 00:05:37,520
with Bhutan to build a city this way

136
00:05:35,759 --> 00:05:39,680
using sustainable forest products to

137
00:05:37,520 --> 00:05:42,720
make building blocks to build a um a

138
00:05:39,680 --> 00:05:46,479
mindfulness uh city.

139
00:05:42,720 --> 00:05:48,160
And um just as a a note uh there are a

140
00:05:46,479 --> 00:05:50,000
number of joint systems we use in those

141
00:05:48,160 --> 00:05:53,199
functional voxels based on whether

142
00:05:50,000 --> 00:05:55,840
they're um reconfigurable or static. For

143
00:05:53,199 --> 00:05:57,840
the static joints, we're developing ways

144
00:05:55,840 --> 00:06:00,240
to create solid state plasmas that let

145
00:05:57,840 --> 00:06:02,960
you join materials at the properties of

146
00:06:00,240 --> 00:06:05,360
the base material. And so that all adds

147
00:06:02,960 --> 00:06:07,840
up to this as one of the punchlines from

148
00:06:05,360 --> 00:06:09,759
today, which is we're building

149
00:06:07,840 --> 00:06:12,800
airplanes, we're building ships, we're

150
00:06:09,759 --> 00:06:17,039
building developing space settlements.

151
00:06:12,800 --> 00:06:20,720
Um applied to data centers, it means

152
00:06:17,039 --> 00:06:23,199
swarms. You see, we have a paper soon to

153
00:06:20,720 --> 00:06:24,960
um be sent out on how this is going to

154
00:06:23,199 --> 00:06:26,560
replace 3D printing buildings. 3D

155
00:06:24,960 --> 00:06:28,720
printing buildings is slow, expensive,

156
00:06:26,560 --> 00:06:30,880
and irreversible. Here we can do it

157
00:06:28,720 --> 00:06:33,039
incrementally and reversible. And so

158
00:06:30,880 --> 00:06:35,440
applied to a data center, swarms of

159
00:06:33,039 --> 00:06:37,440
these relative robots with these digital

160
00:06:35,440 --> 00:06:39,840
material functional building blocks can

161
00:06:37,440 --> 00:06:41,680
build the mechanical structure and the

162
00:06:39,840 --> 00:06:44,479
electrical structure and the electronic

163
00:06:41,680 --> 00:06:46,000
interconnect. And rather than a big

164
00:06:44,479 --> 00:06:47,680
static investment, it can be done

165
00:06:46,000 --> 00:06:50,080
incrementally. It can grow

166
00:06:47,680 --> 00:06:51,919
incrementally. It can be reconfigured

167
00:06:50,080 --> 00:06:54,160
and you can also unbuild through the

168
00:06:51,919 --> 00:06:55,680
life cycle. And so this was just to do

169
00:06:54,160 --> 00:06:57,360
this, we've had to develop new kinds of

170
00:06:55,680 --> 00:06:59,120
design tools. This was just a mockup of

171
00:06:57,360 --> 00:07:01,919
what would it look like for Swarm

172
00:06:59,120 --> 00:07:04,639
Robotics to build data centers that way.

173
00:07:01,919 --> 00:07:07,759
Now, that's the physical plant. Then

174
00:07:04,639 --> 00:07:11,199
comes what goes into the data center.

175
00:07:07,759 --> 00:07:13,360
And so in computing, there's a hierarchy

176
00:07:11,199 --> 00:07:16,960
from chiplets to chips to boards to

177
00:07:13,360 --> 00:07:19,120
blades, the system full of geometrical

178
00:07:16,960 --> 00:07:22,160
descriptions.

179
00:07:19,120 --> 00:07:25,520
in the applications that run there's

180
00:07:22,160 --> 00:07:27,599
naturally geometry in the algorithms

181
00:07:25,520 --> 00:07:30,160
and those have nothing to do with the

182
00:07:27,599 --> 00:07:32,160
almost nothing to do with each other. So

183
00:07:30,160 --> 00:07:33,919
um I started thinking about this years

184
00:07:32,160 --> 00:07:36,160
ago I did a keynote for the annual

185
00:07:33,919 --> 00:07:38,639
supercomputing meeting and that's where

186
00:07:36,160 --> 00:07:41,680
I really came to appreciate to be not

187
00:07:38,639 --> 00:07:44,479
appreciate to be horrified by how you

188
00:07:41,680 --> 00:07:47,199
preserve the fiction that software runs

189
00:07:44,479 --> 00:07:49,280
separate from hardware. So programmers

190
00:07:47,199 --> 00:07:52,560
like Metropolis, programmers frolic in

191
00:07:49,280 --> 00:07:54,560
the garden describing virtual software.

192
00:07:52,560 --> 00:07:56,319
Um the engineers in the basement are

193
00:07:54,560 --> 00:07:58,400
moving the levers to make it work. And

194
00:07:56,319 --> 00:08:01,199
you have to map from the pretend to the

195
00:07:58,400 --> 00:08:04,400
physical and a lot goes wrong there

196
00:08:01,199 --> 00:08:05,919
among many other problems. Uh here's an

197
00:08:04,400 --> 00:08:08,319
estimate that it's two orders of

198
00:08:05,919 --> 00:08:10,960
magnitude of power is in the cost just

199
00:08:08,319 --> 00:08:12,879
of moving data to get from where it goes

200
00:08:10,960 --> 00:08:15,199
in the pretend place to the physical

201
00:08:12,879 --> 00:08:16,879
place. um two orders of magnitude

202
00:08:15,199 --> 00:08:20,400
compared to the actual computational

203
00:08:16,879 --> 00:08:23,199
work. And so uh that experience led me

204
00:08:20,400 --> 00:08:25,039
to start looking at how we align the

205
00:08:23,199 --> 00:08:26,560
descriptions of software and hardware

206
00:08:25,039 --> 00:08:28,879
rather than pretending they live in

207
00:08:26,560 --> 00:08:31,039
different worlds. Uh this was a DARPER

208
00:08:28,879 --> 00:08:32,800
project where we took the blas the

209
00:08:31,039 --> 00:08:35,599
routines underlying high performance

210
00:08:32,800 --> 00:08:37,519
computing and we implemented them

211
00:08:35,599 --> 00:08:39,519
essentially with physics. So this is a

212
00:08:37,519 --> 00:08:42,159
model of computation where the blas has

213
00:08:39,519 --> 00:08:43,519
a dotproduct that has a multiplier that

214
00:08:42,159 --> 00:08:45,279
has an adder tree. This is one of the

215
00:08:43,519 --> 00:08:47,519
core functions in a machine learning

216
00:08:45,279 --> 00:08:49,360
system. Adder tree then has an adder and

217
00:08:47,519 --> 00:08:52,320
then you can fly all the way in to the

218
00:08:49,360 --> 00:08:56,160
devices and fly out to the application.

219
00:08:52,320 --> 00:08:58,959
So when you zoom a map you go from you

220
00:08:56,160 --> 00:09:00,560
this building to a street to a town to

221
00:08:58,959 --> 00:09:02,080
the city state country. It's

222
00:09:00,560 --> 00:09:04,880
hierarchical but you don't change

223
00:09:02,080 --> 00:09:08,720
geometry when you zoom the map. When you

224
00:09:04,880 --> 00:09:10,800
go from uh transistors to micro code to

225
00:09:08,720 --> 00:09:12,880
embedded code to the system architecture

226
00:09:10,800 --> 00:09:14,480
up finally to the application, we go

227
00:09:12,880 --> 00:09:16,800
through maybe five changes of

228
00:09:14,480 --> 00:09:18,880
representation from what you want to do

229
00:09:16,800 --> 00:09:20,480
to what the system does and with you

230
00:09:18,880 --> 00:09:23,120
know tremendous inefficiency at each of

231
00:09:20,480 --> 00:09:24,959
those translations. So here we showed

232
00:09:23,120 --> 00:09:26,800
you don't need to do that. You can use a

233
00:09:24,959 --> 00:09:28,080
consistent representation from the

234
00:09:26,800 --> 00:09:31,600
physics all the way up to the

235
00:09:28,080 --> 00:09:33,360
application. So to start building those

236
00:09:31,600 --> 00:09:36,080
uh there's some interesting seeds.

237
00:09:33,360 --> 00:09:38,160
Post-war NIST a collaborator in this

238
00:09:36,080 --> 00:09:42,480
work started project Tinker Toy which

239
00:09:38,160 --> 00:09:44,880
was robotic assembly of vacuum tubes.

240
00:09:42,480 --> 00:09:48,240
And then a colleague at DARPA Andreas

241
00:09:44,880 --> 00:09:50,160
Olivesson um started the chips project

242
00:09:48,240 --> 00:09:52,959
which is how the whole chip industry has

243
00:09:50,160 --> 00:09:54,720
pivoted to instead of a big chip you

244
00:09:52,959 --> 00:09:57,360
make chiplets and assemble system on

245
00:09:54,720 --> 00:10:00,800
chip out of the chiplets.

246
00:09:57,360 --> 00:10:03,120
But the chiplets are constrained. So 3D

247
00:10:00,800 --> 00:10:06,800
heterogeneous integration is very hot

248
00:10:03,120 --> 00:10:09,279
right now. In a chips solicitation, they

249
00:10:06,800 --> 00:10:11,279
stretch to say we want to stack six

250
00:10:09,279 --> 00:10:13,200
chiplets and the big vision is to

251
00:10:11,279 --> 00:10:15,279
eventually get to 20 chiplets. So it's

252
00:10:13,200 --> 00:10:17,040
really two-dimensional with very limited

253
00:10:15,279 --> 00:10:19,360
three dimension.

254
00:10:17,040 --> 00:10:21,200
Um my lab's done a lot of work on

255
00:10:19,360 --> 00:10:22,640
advanced manufacturing. So in the video

256
00:10:21,200 --> 00:10:25,040
you saw before the session, they showed

257
00:10:22,640 --> 00:10:28,160
the Formlabs printers that came from my

258
00:10:25,040 --> 00:10:31,920
lab among other uh machine companies

259
00:10:28,160 --> 00:10:34,240
that make machines. And so that led to

260
00:10:31,920 --> 00:10:37,120
with DARPA and a few other agencies

261
00:10:34,240 --> 00:10:40,240
asking can we make a cross between a

262
00:10:37,120 --> 00:10:42,079
pick and place and a 3D printer to

263
00:10:40,240 --> 00:10:43,680
compile the 3D construction of

264
00:10:42,079 --> 00:10:47,440
computation.

265
00:10:43,680 --> 00:10:50,480
So instead of chiplets in a package in a

266
00:10:47,440 --> 00:10:52,480
chip on an interposer on a board with

267
00:10:50,480 --> 00:10:54,399
discretets on blades and system can we

268
00:10:52,480 --> 00:10:58,079
consistently just do the whole thing in

269
00:10:54,399 --> 00:11:00,720
one process now that isn't done at a

270
00:10:58,079 --> 00:11:05,279
monolithic level so we've made building

271
00:11:00,720 --> 00:11:07,200
blocks with uh GPUs CPUs MCUs

272
00:11:05,279 --> 00:11:10,320
discretetses and down to base material

273
00:11:07,200 --> 00:11:12,079
properties and the granularity what that

274
00:11:10,320 --> 00:11:13,360
you want to do the assembly depends on

275
00:11:12,079 --> 00:11:15,519
the application

276
00:11:13,360 --> 00:11:17,360
So, we've done versions at each scale,

277
00:11:15,519 --> 00:11:22,000
but what I'm showing here is work done

278
00:11:17,360 --> 00:11:25,920
by Alan in the back. Uh, that's Alan.

279
00:11:22,000 --> 00:11:28,640
Uh, which is a version of this cross

280
00:11:25,920 --> 00:11:30,480
between a pick and place in a 3D printer

281
00:11:28,640 --> 00:11:32,320
that takes these building blocks and

282
00:11:30,480 --> 00:11:34,480
instead of chips on boards and blades

283
00:11:32,320 --> 00:11:36,959
and system, it just fills volumes with

284
00:11:34,480 --> 00:11:39,279
computing structure to compile the

285
00:11:36,959 --> 00:11:41,200
construction of computation in a direct

286
00:11:39,279 --> 00:11:43,600
process. And that has many implications

287
00:11:41,200 --> 00:11:47,200
I'll talk about in a few minutes. Uh

288
00:11:43,600 --> 00:11:51,680
then next to Allan is Eric. That's Eric.

289
00:11:47,200 --> 00:11:54,160
And there's no 3D electronic design

290
00:11:51,680 --> 00:11:57,600
automation software. You you you design

291
00:11:54,160 --> 00:11:59,920
a 2D circuit and then somebody else and

292
00:11:57,600 --> 00:12:01,440
then designing the schematic is

293
00:11:59,920 --> 00:12:04,000
completely separate from the PCB

294
00:12:01,440 --> 00:12:07,839
separate from the fabrication. So Eric

295
00:12:04,000 --> 00:12:10,160
has made sort of CAD and CAM for

296
00:12:07,839 --> 00:12:12,160
computation where you can design the

297
00:12:10,160 --> 00:12:14,800
construction of a computation as a

298
00:12:12,160 --> 00:12:16,800
geometrical structure and so one version

299
00:12:14,800 --> 00:12:19,600
is bottom up and here what he's showing

300
00:12:16,800 --> 00:12:21,360
is an adder and then you can just simply

301
00:12:19,600 --> 00:12:23,839
stack those adders to build a bigger

302
00:12:21,360 --> 00:12:27,200
adder in 3D and under the hood this does

303
00:12:23,839 --> 00:12:29,279
mix signal simulation and then in

304
00:12:27,200 --> 00:12:31,440
parallel top down is compilation where

305
00:12:29,279 --> 00:12:34,800
you can start with verarilog but you can

306
00:12:31,440 --> 00:12:37,519
compile vera log into a 3D construction.

307
00:12:34,800 --> 00:12:40,160
And so here's an example of a uh

308
00:12:37,519 --> 00:12:44,320
compiling the construction of a 3D uh

309
00:12:40,160 --> 00:12:46,800
microprocessor in that design workflow.

310
00:12:44,320 --> 00:12:48,880
And so for the data center again as

311
00:12:46,800 --> 00:12:50,480
concept art what this leads to is the

312
00:12:48,880 --> 00:12:53,760
possibility

313
00:12:50,480 --> 00:12:55,200
of direct write fabrication of 3D

314
00:12:53,760 --> 00:12:57,200
computing structures where you can

315
00:12:55,200 --> 00:13:00,079
compile the construction of the

316
00:12:57,200 --> 00:13:02,399
application into the data center rather

317
00:13:00,079 --> 00:13:04,959
than all of the pain of cache

318
00:13:02,399 --> 00:13:07,519
concurrency and interconnect bandwidth

319
00:13:04,959 --> 00:13:11,040
and levels of cache and the orders of

320
00:13:07,519 --> 00:13:13,760
magnitude overhead in uh data moves. you

321
00:13:11,040 --> 00:13:15,920
can actually compile the geometry of a

322
00:13:13,760 --> 00:13:19,040
computation into the construction to do

323
00:13:15,920 --> 00:13:20,800
it. Starting with these uh base parts

324
00:13:19,040 --> 00:13:23,519
for things that you can't integrate on

325
00:13:20,800 --> 00:13:26,880
chip um then with chiplet scale

326
00:13:23,519 --> 00:13:29,360
integration um commoditized in very much

327
00:13:26,880 --> 00:13:31,920
the same way OCP open compute

328
00:13:29,360 --> 00:13:35,040
commoditized uh what used to buy from

329
00:13:31,920 --> 00:13:36,639
sun. This is now commoditizing to a much

330
00:13:35,040 --> 00:13:39,120
finer grain building block of these

331
00:13:36,639 --> 00:13:41,120
simple parts we can assemble into 3D

332
00:13:39,120 --> 00:13:44,399
data structures and they assemble with

333
00:13:41,120 --> 00:13:46,000
all the properties of digital of error

334
00:13:44,399 --> 00:13:48,240
correction in the construction is what

335
00:13:46,000 --> 00:13:51,200
gives it the scalability.

336
00:13:48,240 --> 00:13:54,000
So uh one of the nice benefits of this

337
00:13:51,200 --> 00:13:57,839
is instead of heat trans heat

338
00:13:54,000 --> 00:14:01,040
dissipation being localized in 2D in hot

339
00:13:57,839 --> 00:14:03,760
processors we can distribute heat

340
00:14:01,040 --> 00:14:05,600
throughout a 3D volume and we can

341
00:14:03,760 --> 00:14:08,160
interdigitate heat transfer through the

342
00:14:05,600 --> 00:14:10,000
volume. So it has nice properties uh for

343
00:14:08,160 --> 00:14:12,240
cooling.

344
00:14:10,000 --> 00:14:15,040
So we're exploring this in a number of

345
00:14:12,240 --> 00:14:18,079
domains. One of them is Arya is the UK

346
00:14:15,040 --> 00:14:20,480
recent UK analog to DARPA and with them

347
00:14:18,079 --> 00:14:22,480
we're looking at can machine learning

348
00:14:20,480 --> 00:14:25,760
systems learn their construction. So

349
00:14:22,480 --> 00:14:28,240
rather than we all use transformers and

350
00:14:25,760 --> 00:14:30,000
Nvidia GPUs, we're looking at very

351
00:14:28,240 --> 00:14:32,880
different heterogeneous architectures

352
00:14:30,000 --> 00:14:35,360
for learning that have a lot of very

353
00:14:32,880 --> 00:14:39,199
interesting properties like um learning

354
00:14:35,360 --> 00:14:41,440
much more efficiently in time and power

355
00:14:39,199 --> 00:14:43,120
um and are not monolithic but are

356
00:14:41,440 --> 00:14:44,880
heterogeneous. So we're looking at

357
00:14:43,120 --> 00:14:47,760
compiling the construction of machine

358
00:14:44,880 --> 00:14:49,920
learning systems with these and uh this

359
00:14:47,760 --> 00:14:52,639
is an interesting proposal we did with a

360
00:14:49,920 --> 00:14:54,480
group of interesting companies

361
00:14:52,639 --> 00:14:56,720
to take

362
00:14:54,480 --> 00:14:58,720
uh circuit boards today are made with

363
00:14:56,720 --> 00:15:00,480
SMD surface mount devices and that's

364
00:14:58,720 --> 00:15:03,120
been true throughout the whole history

365
00:15:00,480 --> 00:15:04,639
of those PCBs I showed you. You can

366
00:15:03,120 --> 00:15:08,000
think about what I'm describing here is

367
00:15:04,639 --> 00:15:11,199
VMDS or volume mount devices. And among

368
00:15:08,000 --> 00:15:13,600
the benefits are uh there's a long path

369
00:15:11,199 --> 00:15:15,680
from a chiplet to design and test with

370
00:15:13,600 --> 00:15:18,160
the system. We can just paste a chiplet

371
00:15:15,680 --> 00:15:20,320
into the system. Um there are a lot of

372
00:15:18,160 --> 00:15:22,320
things you can't integrate on chip high

373
00:15:20,320 --> 00:15:25,519
power high current. For that we can

374
00:15:22,320 --> 00:15:27,920
assemble uh um discretets as uh building

375
00:15:25,519 --> 00:15:29,600
blocks. Uh these are reversible

376
00:15:27,920 --> 00:15:31,600
processes. So rather than when you're

377
00:15:29,600 --> 00:15:34,639
done you make e-waste and you have to

378
00:15:31,600 --> 00:15:36,560
try to melt down the the underlying

379
00:15:34,639 --> 00:15:40,160
components, we can disassemble and

380
00:15:36,560 --> 00:15:42,639
reassemble these building blocks. Um you

381
00:15:40,160 --> 00:15:45,040
don't you can go from system on chip

382
00:15:42,639 --> 00:15:47,279
today you then need to design PCBs and

383
00:15:45,040 --> 00:15:49,040
racks around it. We can span that in a

384
00:15:47,279 --> 00:15:52,560
single process

385
00:15:49,040 --> 00:15:54,560
and uh it's a direct rate process. So an

386
00:15:52,560 --> 00:15:56,880
Nvidia GPU is a billion-dollar

387
00:15:54,560 --> 00:15:58,800
development cycle. Here we're

388
00:15:56,880 --> 00:16:02,079
commoditizing. You can think about it as

389
00:15:58,800 --> 00:16:04,079
the cores in the GPU. And then it's a

390
00:16:02,079 --> 00:16:06,639
direct rate process to fabricate it.

391
00:16:04,079 --> 00:16:08,639
Analogous to injection molding versus 3D

392
00:16:06,639 --> 00:16:11,680
printing. Instead instead of significant

393
00:16:08,639 --> 00:16:13,759
capital overhead, it's direct assembly

394
00:16:11,680 --> 00:16:16,639
are all opportunities with this move

395
00:16:13,759 --> 00:16:18,959
towards uh volume mount devices.

396
00:16:16,639 --> 00:16:21,360
Now there's an overhead to discreetly

397
00:16:18,959 --> 00:16:24,639
assembling. It's a strong function of

398
00:16:21,360 --> 00:16:28,399
dimension. This is looking at uh time

399
00:16:24,639 --> 00:16:31,199
and power cost of this discrete assembly

400
00:16:28,399 --> 00:16:33,519
rather than integrating on chip. And

401
00:16:31,199 --> 00:16:37,040
when you get down to roughly micron

402
00:16:33,519 --> 00:16:38,800
scale um as the feature size is where

403
00:16:37,040 --> 00:16:42,079
the overhead becomes competitive with

404
00:16:38,800 --> 00:16:44,480
how it's done today. And so one of the

405
00:16:42,079 --> 00:16:46,880
most interesting for me projects in this

406
00:16:44,480 --> 00:16:50,639
collaboration is with another group at

407
00:16:46,880 --> 00:16:52,160
MIT Farnas Naroy and Schmidt Foundation

408
00:16:50,639 --> 00:16:54,320
where we're looking at essentially when

409
00:16:52,160 --> 00:16:56,399
Lego becomes quantum. So looking at the

410
00:16:54,320 --> 00:16:59,759
crossover between mechanical joints in

411
00:16:56,399 --> 00:17:01,839
these building blocks and um bonding in

412
00:16:59,759 --> 00:17:04,480
these building blocks scaling the

413
00:17:01,839 --> 00:17:07,280
building blocks down to uh tens of

414
00:17:04,480 --> 00:17:08,959
nanometers. So that's much finer scale

415
00:17:07,280 --> 00:17:10,799
than what I showed you in Ericson

416
00:17:08,959 --> 00:17:12,959
Allen's work. But in the road map, we're

417
00:17:10,799 --> 00:17:15,199
filling in the limiting point where you

418
00:17:12,959 --> 00:17:17,839
can assemble um all the way down to

419
00:17:15,199 --> 00:17:21,839
nanocale.

420
00:17:17,839 --> 00:17:23,919
So first step in this path was this

421
00:17:21,839 --> 00:17:25,919
discussion today has been about data

422
00:17:23,919 --> 00:17:27,919
centers as static things. With these

423
00:17:25,919 --> 00:17:31,039
digital materials and these swarm

424
00:17:27,919 --> 00:17:33,440
robots, we can build dynamic physically

425
00:17:31,039 --> 00:17:35,760
reconfigurable data centers.

426
00:17:33,440 --> 00:17:38,160
Then the second assumption is the chips

427
00:17:35,760 --> 00:17:41,360
are static. You know, you you wait for

428
00:17:38,160 --> 00:17:42,960
Nvidia to burp out their next chip. Um

429
00:17:41,360 --> 00:17:45,039
we're leading to a world where that's

430
00:17:42,960 --> 00:17:47,600
just not true, where the computation

431
00:17:45,039 --> 00:17:49,919
itself is discreetly assembled,

432
00:17:47,600 --> 00:17:52,320
reconfigurable, and can match workloads

433
00:17:49,919 --> 00:17:56,720
and grow with workloads.

434
00:17:52,320 --> 00:17:58,799
Um the biggest step then after that is

435
00:17:56,720 --> 00:18:00,400
uh my lab I mentioned did the first

436
00:17:58,799 --> 00:18:05,280
faster than classical quantum

437
00:18:00,400 --> 00:18:06,960
computations. We did um uh or searching

438
00:18:05,280 --> 00:18:09,440
an unordered list and part of the

439
00:18:06,960 --> 00:18:12,160
collaboration was implementing factoring

440
00:18:09,440 --> 00:18:15,840
uh based on spin resonance techniques.

441
00:18:12,160 --> 00:18:19,200
Once we push the scaling as I showed you

442
00:18:15,840 --> 00:18:20,960
down to the um feature sizes, we then

443
00:18:19,200 --> 00:18:22,960
run into quantum effects. The quantum

444
00:18:20,960 --> 00:18:26,960
effects can be a bug, but they can be a

445
00:18:22,960 --> 00:18:30,240
significant feature. And so

446
00:18:26,960 --> 00:18:33,200
I'd say the most exciting graph you're

447
00:18:30,240 --> 00:18:34,720
likely to see today is upper in the

448
00:18:33,200 --> 00:18:37,679
upper right corner and that's from Alex

449
00:18:34,720 --> 00:18:39,360
in the back. That's Alex from Lincoln

450
00:18:37,679 --> 00:18:42,880
Labs.

451
00:18:39,360 --> 00:18:45,760
Uh with Lincoln Labs we've been looking

452
00:18:42,880 --> 00:18:48,160
at what are called adiabatic quantum

453
00:18:45,760 --> 00:18:50,880
flux parametrons.

454
00:18:48,160 --> 00:18:54,080
And so that's using uh quantum mechanics

455
00:18:50,880 --> 00:18:56,880
in superconducting circuits to do logic.

456
00:18:54,080 --> 00:18:59,280
And the wiggly lines at the bottom are

457
00:18:56,880 --> 00:19:02,000
showing schematically

458
00:18:59,280 --> 00:19:03,919
the superconducting quantum. So to be

459
00:19:02,000 --> 00:19:06,480
clear, we're using quantum mechanics but

460
00:19:03,919 --> 00:19:08,480
not for quantum computation. We're using

461
00:19:06,480 --> 00:19:11,120
quantum mechanics to do classical

462
00:19:08,480 --> 00:19:14,160
computation much more efficiently. So

463
00:19:11,120 --> 00:19:17,600
the AQFP acts like a double well

464
00:19:14,160 --> 00:19:20,320
potential and then you can dynamically

465
00:19:17,600 --> 00:19:24,799
um tilt the potential to move the state

466
00:19:20,320 --> 00:19:28,000
in those wells. And so in today's CMOS

467
00:19:24,799 --> 00:19:30,880
logic, it's insane.

468
00:19:28,000 --> 00:19:34,080
Every bit at every clock tick in every

469
00:19:30,880 --> 00:19:36,160
transistor is destroyed. And uh all that

470
00:19:34,080 --> 00:19:38,640
power ridiculous power going in is to

471
00:19:36,160 --> 00:19:41,679
destroy every the energy in every bit at

472
00:19:38,640 --> 00:19:44,240
every clock tick. It's just crazy. So

473
00:19:41,679 --> 00:19:45,919
these adabatic quantum flux parametrons

474
00:19:44,240 --> 00:19:47,840
store state in these double well

475
00:19:45,919 --> 00:19:50,320
potentials and then carefully move it

476
00:19:47,840 --> 00:19:53,280
around. And so what the graph in the

477
00:19:50,320 --> 00:19:55,679
upper right is showing you is KT is the

478
00:19:53,280 --> 00:19:57,600
minimum unit of thermodynamic energy

479
00:19:55,679 --> 00:20:01,280
sort of that's in a thermal system

480
00:19:57,600 --> 00:20:06,160
that's as low as you can go. Uh COS

481
00:20:01,280 --> 00:20:08,240
today is about 10 the 5 KT these AQFPs

482
00:20:06,160 --> 00:20:10,559
not as a projection just in the lab are

483
00:20:08,240 --> 00:20:12,160
just over KT within a factor of a few of

484
00:20:10,559 --> 00:20:16,240
KT.

485
00:20:12,160 --> 00:20:18,400
So this is a 10 to the5 improvement in

486
00:20:16,240 --> 00:20:20,960
the power efficiency of a logic device.

487
00:20:18,400 --> 00:20:23,679
Now before you get too excited, I'll end

488
00:20:20,960 --> 00:20:25,679
by telling you about the overheads. And

489
00:20:23,679 --> 00:20:27,840
with the overheads, conservatively, we

490
00:20:25,679 --> 00:20:29,919
might lose 10 to the three. So I can

491
00:20:27,840 --> 00:20:32,400
only give you a factor of 100 power

492
00:20:29,919 --> 00:20:36,320
gain, but very conservatively I can give

493
00:20:32,400 --> 00:20:39,760
you a factor of 100 power gain. So what

494
00:20:36,320 --> 00:20:41,600
makes this possible is Lincoln Labs has

495
00:20:39,760 --> 00:20:44,159
probably the leading superconducting

496
00:20:41,600 --> 00:20:47,840
chip fab and that's what we were using

497
00:20:44,159 --> 00:20:50,559
to be able to uh develop these uh AQFP

498
00:20:47,840 --> 00:20:53,679
chips and there's a very close

499
00:20:50,559 --> 00:20:55,679
connection between the previous work I

500
00:20:53,679 --> 00:20:57,440
showed you on discrete assembly and the

501
00:20:55,679 --> 00:21:00,000
superconductivity and this is how it

502
00:20:57,440 --> 00:21:02,000
started because of both yield but also

503
00:21:00,000 --> 00:21:04,880
the underlying physics there's a limit

504
00:21:02,000 --> 00:21:06,559
to the complexity of a superconducting

505
00:21:04,880 --> 00:21:08,159
chip. We're not going to make a billion

506
00:21:06,559 --> 00:21:10,400
Josephs and Junction superconducting

507
00:21:08,159 --> 00:21:12,080
chip. And so we need an architecture

508
00:21:10,400 --> 00:21:13,919
that can take much smaller chips and

509
00:21:12,080 --> 00:21:15,760
build big systems out of that. And

510
00:21:13,919 --> 00:21:18,320
that's what led to this collaboration to

511
00:21:15,760 --> 00:21:20,880
use these electronic assemblers to make

512
00:21:18,320 --> 00:21:24,640
superconducting systems. And so now what

513
00:21:20,880 --> 00:21:26,799
I'm showing you is we've implemented

514
00:21:24,640 --> 00:21:29,120
everything you need to make a computing

515
00:21:26,799 --> 00:21:30,880
system. So it includes control

516
00:21:29,120 --> 00:21:34,240
architecture,

517
00:21:30,880 --> 00:21:37,840
um, math units, uh, memory, both larger

518
00:21:34,240 --> 00:21:39,520
and smaller memories. Um, on top row are

519
00:21:37,840 --> 00:21:42,480
chips we've designed and fabricated

520
00:21:39,520 --> 00:21:44,159
through Lincoln's fab. And then in the

521
00:21:42,480 --> 00:21:46,000
lower left is what looks like

522
00:21:44,159 --> 00:21:48,159
simulation, but is data coming from the

523
00:21:46,000 --> 00:21:49,360
chips. Um, you know, if I gave this talk

524
00:21:48,159 --> 00:21:50,799
two years ago, it wouldn't look like

525
00:21:49,360 --> 00:21:53,280
that. They were very homely. At this

526
00:21:50,799 --> 00:21:55,520
point, um, the process technology is

527
00:21:53,280 --> 00:21:57,440
really dialed in and works beautifully.

528
00:21:55,520 --> 00:22:01,200
And part of the idea of this is we don't

529
00:21:57,440 --> 00:22:04,320
need to go now to billion-dollar fab to

530
00:22:01,200 --> 00:22:06,880
make um giant chips. We can yield these

531
00:22:04,320 --> 00:22:08,880
smaller super superconducting chips and

532
00:22:06,880 --> 00:22:10,880
then uh build larger systems out of

533
00:22:08,880 --> 00:22:13,360
them.

534
00:22:10,880 --> 00:22:18,159
So then given all of that, what we're

535
00:22:13,360 --> 00:22:20,880
now up to is we we can measure

536
00:22:18,159 --> 00:22:24,480
all the the individual adabatic quantum

537
00:22:20,880 --> 00:22:26,880
flux parametrons and we can measure

538
00:22:24,480 --> 00:22:29,200
um the the math processors, the memory

539
00:22:26,880 --> 00:22:30,960
units, the control units out of them.

540
00:22:29,200 --> 00:22:33,679
And then we can take all of that and

541
00:22:30,960 --> 00:22:36,080
push it through simulation tools and do

542
00:22:33,679 --> 00:22:37,440
simulation of full computing systems

543
00:22:36,080 --> 00:22:39,120
where we're simulating the full

544
00:22:37,440 --> 00:22:41,280
computing system but it's based on the

545
00:22:39,120 --> 00:22:44,480
experimental uh measurements at the

546
00:22:41,280 --> 00:22:47,919
device level. And what that gives us is

547
00:22:44,480 --> 00:22:50,799
the factor of 100. So the factor of 100

548
00:22:47,919 --> 00:22:53,440
starts with the 10 to the five and then

549
00:22:50,799 --> 00:22:55,840
we lose about 10 to the three again

550
00:22:53,440 --> 00:22:57,600
conservatively from cryogenic overhead

551
00:22:55,840 --> 00:22:59,360
and various inefficiencies of the

552
00:22:57,600 --> 00:23:01,919
system.

553
00:22:59,360 --> 00:23:05,520
And so it leads to the opportunity for

554
00:23:01,919 --> 00:23:09,039
cryogenic uh data centers. And so in

555
00:23:05,520 --> 00:23:12,880
this sketch what we're showing is

556
00:23:09,039 --> 00:23:16,159
surprisingly there is an existing market

557
00:23:12,880 --> 00:23:19,120
for rack scale cryogenics.

558
00:23:16,159 --> 00:23:21,919
Uh I suspect a lot of that is classified

559
00:23:19,120 --> 00:23:24,799
applications we can't know about. That

560
00:23:21,919 --> 00:23:27,280
includes things like optics and radios

561
00:23:24,799 --> 00:23:31,679
um that have to work at physical limits.

562
00:23:27,280 --> 00:23:34,159
But at rack scale you so um th this runs

563
00:23:31,679 --> 00:23:35,919
happily at 4K and we're you know inching

564
00:23:34,159 --> 00:23:38,720
a little bit above that but at rack

565
00:23:35,919 --> 00:23:40,720
scale you can have a 4K cryo system

566
00:23:38,720 --> 00:23:43,760
where you put in electricity and it's a

567
00:23:40,720 --> 00:23:47,520
closed system and then we'll fill the

568
00:23:43,760 --> 00:23:50,559
rack with these uh VMDs

569
00:23:47,520 --> 00:23:53,919
uh to build uh physically reconfigurable

570
00:23:50,559 --> 00:23:56,159
superun superconducting electronics and

571
00:23:53,919 --> 00:23:58,480
then what we get is a ballpark 10

572
00:23:56,159 --> 00:24:02,240
kilowatt rack doing the work of a

573
00:23:58,480 --> 00:24:04,720
megawatt rack today. So there's a lot of

574
00:24:02,240 --> 00:24:06,960
work to commercialize that. We haven't

575
00:24:04,720 --> 00:24:09,520
yet come anywhere making this at full

576
00:24:06,960 --> 00:24:12,720
rack scale, but we validated all the

577
00:24:09,520 --> 00:24:14,480
steps that come into it. And it's it's I

578
00:24:12,720 --> 00:24:17,200
think by far the most dramatic

579
00:24:14,480 --> 00:24:18,400
opportunity for a 100x win that would

580
00:24:17,200 --> 00:24:21,360
radically change a lot of the

581
00:24:18,400 --> 00:24:25,039
calculations about cost and power for

582
00:24:21,360 --> 00:24:28,240
data centers. So we're left with this

583
00:24:25,039 --> 00:24:31,279
hierarchy of

584
00:24:28,240 --> 00:24:33,919
uh MIT made the last great analog

585
00:24:31,279 --> 00:24:37,600
computer, Venever Bush. It was a room

586
00:24:33,919 --> 00:24:40,559
full of gears and pulleys and the answer

587
00:24:37,600 --> 00:24:43,120
got worse the longer you ran it. And uh

588
00:24:40,559 --> 00:24:45,760
the student working on that wrote the

589
00:24:43,120 --> 00:24:47,279
best master thesis anybody did ever. It

590
00:24:45,760 --> 00:24:49,520
was Claude Shannon and he invented

591
00:24:47,279 --> 00:24:51,120
digital in his master's thesis because

592
00:24:49,520 --> 00:24:54,960
he was so annoyed at the differential

593
00:24:51,120 --> 00:24:57,279
analyzer and crucially digital

594
00:24:54,960 --> 00:24:59,840
uh he went on to help show what are

595
00:24:57,279 --> 00:25:02,480
called threshold theorems where in a

596
00:24:59,840 --> 00:25:04,640
system where you can restore a state

597
00:25:02,480 --> 00:25:08,159
digital doesn't mean one and zero. What

598
00:25:04,640 --> 00:25:10,640
it means is if you restore state

599
00:25:08,159 --> 00:25:12,400
uh for a linear increase in the physical

600
00:25:10,640 --> 00:25:15,600
representation of the state there's an

601
00:25:12,400 --> 00:25:17,679
exponential reduction in the error um to

602
00:25:15,600 --> 00:25:19,360
decode it and that e there's very few

603
00:25:17,679 --> 00:25:23,039
engineering exponentials that

604
00:25:19,360 --> 00:25:24,559
exponential is what lets uh us compute

605
00:25:23,039 --> 00:25:29,360
reliably

606
00:25:24,559 --> 00:25:31,600
you know now on exoscale then uh uh

607
00:25:29,360 --> 00:25:34,159
after uh so Shannon did that for

608
00:25:31,600 --> 00:25:35,679
communication van uh developed it for

609
00:25:34,159 --> 00:25:37,600
computation.

610
00:25:35,679 --> 00:25:41,120
State-of-the-art chip fabrication is

611
00:25:37,600 --> 00:25:43,039
fundamentally analog. Um, it's it's feed

612
00:25:41,120 --> 00:25:45,440
forward. There's no information in the

613
00:25:43,039 --> 00:25:47,200
construction. Your ribosome, 4 billion

614
00:25:45,440 --> 00:25:49,600
years old, is digital. It detects and

615
00:25:47,200 --> 00:25:51,840
corrects errors. And so, at heart in

616
00:25:49,600 --> 00:25:54,400
what I'm describing is building

617
00:25:51,840 --> 00:25:57,039
properties of digital state and error

618
00:25:54,400 --> 00:25:59,039
correction into construction. Uh, it's

619
00:25:57,039 --> 00:26:01,840
true in Lego bricks. Lego is more

620
00:25:59,039 --> 00:26:03,679
accurate than the child. Um, uh, you

621
00:26:01,840 --> 00:26:05,760
don't need a ruler for Lego. And there's

622
00:26:03,679 --> 00:26:07,600
no Lego trash. And those are also

623
00:26:05,760 --> 00:26:10,080
properties of ribosomes. And they're not

624
00:26:07,600 --> 00:26:12,480
properties of chip fab. And so these

625
00:26:10,080 --> 00:26:13,760
assemblers I'm describing start from

626
00:26:12,480 --> 00:26:15,760
simple building blocks, make

627
00:26:13,760 --> 00:26:17,760
micromechanical systems that build

628
00:26:15,760 --> 00:26:20,320
larger systems that build microobotic

629
00:26:17,760 --> 00:26:23,039
systems that build still larger robots

630
00:26:20,320 --> 00:26:25,919
that build u macro systems. And we've

631
00:26:23,039 --> 00:26:29,600
shown that at each of those scales.

632
00:26:25,919 --> 00:26:31,840
And before Jim invited me for this, even

633
00:26:29,600 --> 00:26:33,600
though my lab has done so much on

634
00:26:31,840 --> 00:26:35,039
previous work on data centers, as I

635
00:26:33,600 --> 00:26:36,480
showed you at the beginning, I hadn't

636
00:26:35,039 --> 00:26:38,799
really thought about pulling all this

637
00:26:36,480 --> 00:26:40,320
together in this context. But it leads

638
00:26:38,799 --> 00:26:43,039
to this radically different version of

639
00:26:40,320 --> 00:26:44,400
the future of data centers where swarm

640
00:26:43,039 --> 00:26:46,080
robots are going to build the macro

641
00:26:44,400 --> 00:26:48,480
structures, micro assemblers are going

642
00:26:46,080 --> 00:26:49,679
to build the reconfigurable computation,

643
00:26:48,480 --> 00:26:51,760
and we're going to use superc

644
00:26:49,679 --> 00:26:54,159
conductivity to save orders of magnitude

645
00:26:51,760 --> 00:26:55,919
and power. um addressing many of the

646
00:26:54,159 --> 00:26:58,000
crucial issues you've talked about

647
00:26:55,919 --> 00:27:01,279
throughout the day.

648
00:26:58,000 --> 00:27:04,400
Now, all of that may sound

649
00:27:01,279 --> 00:27:08,320
like a big stretch from from where you

650
00:27:04,400 --> 00:27:10,640
were, but what's interesting is

651
00:27:08,320 --> 00:27:12,720
Vanoyman is credited for Vanoyman

652
00:27:10,640 --> 00:27:15,039
architecture.

653
00:27:12,720 --> 00:27:17,520
Vanoyman wrote beautifully about many

654
00:27:15,039 --> 00:27:20,159
things. Computing architecture was not

655
00:27:17,520 --> 00:27:22,400
one of them. He he only wrote about the

656
00:27:20,159 --> 00:27:26,000
Vaniman architecture in a dreadful memo,

657
00:27:22,400 --> 00:27:28,080
the Edvac report. Um that was just kind

658
00:27:26,000 --> 00:27:29,760
of an hack.

659
00:27:28,080 --> 00:27:33,840
What he did write about towards the end

660
00:27:29,760 --> 00:27:36,000
of his life is self-reproducing automa.

661
00:27:33,840 --> 00:27:38,000
So you this pioneer of computation

662
00:27:36,000 --> 00:27:41,039
towards the end of his life was studying

663
00:27:38,000 --> 00:27:42,640
how a computation can communicate its

664
00:27:41,039 --> 00:27:45,120
construction

665
00:27:42,640 --> 00:27:48,640
and he was asking that question towards

666
00:27:45,120 --> 00:27:51,760
trying to understand life.

667
00:27:48,640 --> 00:27:54,399
uh other pioneer of computing touring

668
00:27:51,760 --> 00:27:56,159
again well known for computing

669
00:27:54,399 --> 00:27:58,320
uh towards the end of his life what he

670
00:27:56,159 --> 00:28:00,640
was studying was morphogenesis

671
00:27:58,320 --> 00:28:04,720
was the evode deeo problem of how genes

672
00:28:00,640 --> 00:28:06,399
give rise to form so the founders of

673
00:28:04,720 --> 00:28:08,480
computation

674
00:28:06,399 --> 00:28:11,200
towards the end of their life were

675
00:28:08,480 --> 00:28:13,679
studying fabrication they were studying

676
00:28:11,200 --> 00:28:15,919
the construction of computation

677
00:28:13,679 --> 00:28:18,240
you know in a computer science

678
00:28:15,919 --> 00:28:21,440
curriculum you're unlike ly to ever hear

679
00:28:18,240 --> 00:28:24,399
about the physical form of computation.

680
00:28:21,440 --> 00:28:26,880
The pioneers of computing saw that as

681
00:28:24,399 --> 00:28:29,279
absolutely essential of describing the

682
00:28:26,880 --> 00:28:30,880
construction of computation as integral

683
00:28:29,279 --> 00:28:32,480
to understanding and developing

684
00:28:30,880 --> 00:28:34,799
computation

685
00:28:32,480 --> 00:28:37,679
and ignoring it has led to orders of

686
00:28:34,799 --> 00:28:40,559
magnitude of efficiency, waste and power

687
00:28:37,679 --> 00:28:42,960
of e-waste of just all kinds of problems

688
00:28:40,559 --> 00:28:45,200
you've been talking about today. I

689
00:28:42,960 --> 00:28:47,279
embracing rather than avoiding the

690
00:28:45,200 --> 00:28:49,520
construction of computation gives you

691
00:28:47,279 --> 00:28:52,080
the opportunity in all these different

692
00:28:49,520 --> 00:28:55,279
areas. And so on the right there's a

693
00:28:52,080 --> 00:28:56,960
magisterial series of uh papers from the

694
00:28:55,279 --> 00:28:59,520
Santa Fe Institute on classics of

695
00:28:56,960 --> 00:29:02,320
complexity. This is an essay I wrote on

696
00:28:59,520 --> 00:29:05,039
revisiting uh Vanoyman's ideas in the

697
00:29:02,320 --> 00:29:08,399
Santa Fe complexity series.

698
00:29:05,039 --> 00:29:10,399
And so to follow up on the left is uh my

699
00:29:08,399 --> 00:29:13,840
most recent book where I describe this

700
00:29:10,399 --> 00:29:17,600
research roadmap um and a long form lex

701
00:29:13,840 --> 00:29:20,080
podcast where I talk about it. And the

702
00:29:17,600 --> 00:29:21,679
challenge now is we've proved out all

703
00:29:20,080 --> 00:29:23,200
the things I've shown you. There's a

704
00:29:21,679 --> 00:29:25,200
challenge now and an opportunity to

705
00:29:23,200 --> 00:29:28,640
really scale to do that at data center

706
00:29:25,200 --> 00:29:30,640
scale. But uh it it's something we're

707
00:29:28,640 --> 00:29:33,279
eager to tackle. And I think by

708
00:29:30,640 --> 00:29:36,000
revisiting the assumptions through today

709
00:29:33,279 --> 00:29:37,360
of a data center, you build a data

710
00:29:36,000 --> 00:29:38,880
center and it doesn't change and

711
00:29:37,360 --> 00:29:41,200
somebody else makes chips and they don't

712
00:29:38,880 --> 00:29:43,440
change and we use CMOS. By revisiting

713
00:29:41,200 --> 00:29:47,039
all of those assumptions, it has just

714
00:29:43,440 --> 00:29:48,399
profound impa implications for being

715
00:29:47,039 --> 00:29:50,880
faster, better, cheaper, more

716
00:29:48,399 --> 00:29:53,360
environmentally friendly, more scalable

717
00:29:50,880 --> 00:29:57,320
uh through life cycle.

718
00:29:53,360 --> 00:29:57,320
So thank you

719
00:30:01,440 --> 00:30:04,440
>> here

720
00:30:05,200 --> 00:30:08,760
question over there.

721
00:30:11,440 --> 00:30:14,720
>> That was a very interesting

722
00:30:12,559 --> 00:30:17,840
presentation. Um, one question that I

723
00:30:14,720 --> 00:30:21,120
want to pose is um what's can you give

724
00:30:17,840 --> 00:30:23,120
me a sense of the largest uh processor

725
00:30:21,120 --> 00:30:25,360
I'll call it uh of this sort that you've

726
00:30:23,120 --> 00:30:28,159
run any kind of input and output on some

727
00:30:25,360 --> 00:30:30,480
kind of algorithm and I'm also curious

728
00:30:28,159 --> 00:30:31,919
if something physically happens to one

729
00:30:30,480 --> 00:30:33,840
of these assembled structures does it

730
00:30:31,919 --> 00:30:34,320
degrade gracefully what's that behavior

731
00:30:33,840 --> 00:30:36,159
look like

732
00:30:34,320 --> 00:30:38,000
>> when you say well for the first question

733
00:30:36,159 --> 00:30:41,159
so Alex what's the biggest chip in fab

734
00:30:38,000 --> 00:30:41,159
right now

735
00:30:43,200 --> 00:30:46,679
How many JJs?

736
00:30:47,440 --> 00:30:53,360
>> Okay. Um, so you know in the history of

737
00:30:50,000 --> 00:30:56,320
IC industry right now we're at the um

738
00:30:53,360 --> 00:30:59,360
sort of MSI era at the be you know sort

739
00:30:56,320 --> 00:31:01,600
of at the edge of the VLSI era. So so

740
00:30:59,360 --> 00:31:03,360
we're roughly at this this is about the

741
00:31:01,600 --> 00:31:06,320
almost exactly the complexity of the

742
00:31:03,360 --> 00:31:08,399
Intel 404 is is roughly where we are

743
00:31:06,320 --> 00:31:11,840
technologically.

744
00:31:08,399 --> 00:31:14,640
But that that's slightly unfair just in

745
00:31:11,840 --> 00:31:17,360
that the the modeling I showed you is

746
00:31:14,640 --> 00:31:20,000
what you do when you uh model a billion

747
00:31:17,360 --> 00:31:21,600
transistor chip in that you prove out

748
00:31:20,000 --> 00:31:23,200
you don't in one go make the billion

749
00:31:21,600 --> 00:31:25,120
transistor chip. You prove out the

750
00:31:23,200 --> 00:31:27,520
device physics of you know your two

751
00:31:25,120 --> 00:31:29,440
nanometer process and then there are

752
00:31:27,520 --> 00:31:31,120
these very sophisticated modeling tools

753
00:31:29,440 --> 00:31:32,960
that take the physics measurements and

754
00:31:31,120 --> 00:31:34,960
use that to do the system modeling. And

755
00:31:32,960 --> 00:31:37,840
so, um, the biggest we've made is

756
00:31:34,960 --> 00:31:40,000
roughly the equivalent of the Intel 404,

757
00:31:37,840 --> 00:31:41,600
um, microprocessor. And that, but we're

758
00:31:40,000 --> 00:31:45,200
using industrial strength modeling tools

759
00:31:41,600 --> 00:31:47,120
to predict the full size system scale.

760
00:31:45,200 --> 00:31:50,120
>> 30

761
00:31:47,120 --> 00:31:50,120
again.

762
00:31:50,159 --> 00:31:53,840
>> Okay. 30 million junction memory chip.

763
00:31:52,559 --> 00:31:55,679
Um, and then you're asking about how

764
00:31:53,840 --> 00:31:58,679
these systems fail.

765
00:31:55,679 --> 00:31:58,679
>> Yes.

766
00:31:58,720 --> 00:32:04,480
Um so one of the um

767
00:32:02,720 --> 00:32:06,399
really interesting opportunities here is

768
00:32:04,480 --> 00:32:09,919
you should view the robotic assemblers

769
00:32:06,399 --> 00:32:11,679
I'm describing as not at the factory and

770
00:32:09,919 --> 00:32:13,200
then you send it to the data center but

771
00:32:11,679 --> 00:32:15,279
you you view this as part of the data

772
00:32:13,200 --> 00:32:17,840
center. So central to this project is

773
00:32:15,279 --> 00:32:20,240
the reconfigurability. So if something

774
00:32:17,840 --> 00:32:22,000
breaks or you want to change application

775
00:32:20,240 --> 00:32:25,039
you can disassemble and reassemble it.

776
00:32:22,000 --> 00:32:27,039
And so the re reconfigurability is a

777
00:32:25,039 --> 00:32:28,480
central part of the project. So you you

778
00:32:27,039 --> 00:32:30,799
should think view this as robotic

779
00:32:28,480 --> 00:32:32,799
assemblers running around uh fixing and

780
00:32:30,799 --> 00:32:34,720
rebuilding systems. We started looking

781
00:32:32,799 --> 00:32:36,960
at that just for structural failure for

782
00:32:34,720 --> 00:32:39,120
things like airplanes. But it then is I

783
00:32:36,960 --> 00:32:41,919
I should explain that the sequence was

784
00:32:39,120 --> 00:32:43,919
we were making discreetly assembled

785
00:32:41,919 --> 00:32:46,240
record setting micro robots and we had

786
00:32:43,919 --> 00:32:47,840
to put brains into the robots and then

787
00:32:46,240 --> 00:32:49,039
when we put the brains in the robots we

788
00:32:47,840 --> 00:32:50,720
realized we were building computing

789
00:32:49,039 --> 00:32:53,200
systems and that's what started this

790
00:32:50,720 --> 00:32:55,919
whole lineage.

791
00:32:53,200 --> 00:32:56,880
Let's see. I think he um Yeah, I think

792
00:32:55,919 --> 00:33:00,240
he had his hand up next.

793
00:32:56,880 --> 00:33:02,399
>> Jim, come on. Jim, start.

794
00:33:00,240 --> 00:33:04,159
>> So, uh Neil, this is this a fantastic. I

795
00:33:02,399 --> 00:33:05,679
wonder I was very interested by the um

796
00:33:04,159 --> 00:33:07,519
cryogenic computer but more from

797
00:33:05,679 --> 00:33:09,600
conventional computer and I wonder you

798
00:33:07,519 --> 00:33:11,919
mean earlier today we heard about the

799
00:33:09,600 --> 00:33:14,320
prediction of Jensen Wong around quantum

800
00:33:11,919 --> 00:33:16,000
computing being 20 years out. Is there

801
00:33:14,320 --> 00:33:18,640
an opportunity to leverage some of the

802
00:33:16,000 --> 00:33:21,200
technology that's developed for uh

803
00:33:18,640 --> 00:33:23,519
quantum computing in cooling wise but

804
00:33:21,200 --> 00:33:24,960
leverage that for conventional cryogenic

805
00:33:23,519 --> 00:33:26,159
cooling on the racks and do you have any

806
00:33:24,960 --> 00:33:27,919
sense of a time frame?

807
00:33:26,159 --> 00:33:28,880
>> Oh, you mean cryogenic cooling for for

808
00:33:27,919 --> 00:33:30,799
conventional racks

809
00:33:28,880 --> 00:33:32,399
>> for Yeah, the one that that you that you

810
00:33:30,799 --> 00:33:32,880
show wasn't qu you said was not quantum

811
00:33:32,399 --> 00:33:35,919
computing.

812
00:33:32,880 --> 00:33:37,200
>> Oh, no. But but the it was quantum No,

813
00:33:35,919 --> 00:33:41,440
let me say it really carefully.

814
00:33:37,200 --> 00:33:44,880
>> Okay. Um so real quantum computing like

815
00:33:41,440 --> 00:33:47,600
the the the first things I did use two

816
00:33:44,880 --> 00:33:49,200
essential properties of entanglement and

817
00:33:47,600 --> 00:33:52,159
superposition

818
00:33:49,200 --> 00:33:54,000
and for a subset of problems it lets you

819
00:33:52,159 --> 00:33:57,279
do things exponentially faster than

820
00:33:54,000 --> 00:33:59,360
classical. Now the caution is quantum is

821
00:33:57,279 --> 00:34:01,120
far from universally applicable and

822
00:33:59,360 --> 00:34:03,519
there's a lot of overselling going on.

823
00:34:01,120 --> 00:34:05,519
There's some problems for which quantum

824
00:34:03,519 --> 00:34:08,480
um has radical implications but it's

825
00:34:05,519 --> 00:34:10,879
it's a subset. What I was doing is

826
00:34:08,480 --> 00:34:12,720
quantum computing.

827
00:34:10,879 --> 00:34:15,280
It's fundamentally quantum mechanical

828
00:34:12,720 --> 00:34:17,839
but the difference is the quantum

829
00:34:15,280 --> 00:34:19,200
mechanics is within the logic elements

830
00:34:17,839 --> 00:34:22,240
the thing I'm calling Joseph's and

831
00:34:19,200 --> 00:34:24,639
junctions. So I'm not implementing.

832
00:34:22,240 --> 00:34:26,639
So in in quantum computing the bits

833
00:34:24,639 --> 00:34:28,800
themselves are quantum. They can be in

834
00:34:26,639 --> 00:34:30,480
superposition entanglement. In what I

835
00:34:28,800 --> 00:34:32,639
described the bits are classical.

836
00:34:30,480 --> 00:34:36,879
They're either one or zero. But

837
00:34:32,639 --> 00:34:39,119
crucially, my ability to run 10 the 5

838
00:34:36,879 --> 00:34:40,800
more efficiently comes from I'm

839
00:34:39,119 --> 00:34:42,800
representing the bits using quantum

840
00:34:40,800 --> 00:34:43,839
mechanics and I'm moving the bits using

841
00:34:42,800 --> 00:34:44,399
quantum mechanics.

842
00:34:43,839 --> 00:34:47,200
>> That makes sense.

843
00:34:44,399 --> 00:34:49,359
>> And so the underlying device so in

844
00:34:47,200 --> 00:34:52,480
quantum computing there's about five

845
00:34:49,359 --> 00:34:55,119
competing systems for the physics. One

846
00:34:52,480 --> 00:34:57,359
of them is exactly these same circuits I

847
00:34:55,119 --> 00:34:59,760
showed you. But here we're ignoring the

848
00:34:57,359 --> 00:35:01,760
quantum coherence part. Um, we're

849
00:34:59,760 --> 00:35:04,240
embracing all of the classical computing

850
00:35:01,760 --> 00:35:08,079
applications, but we're saving 10 to the

851
00:35:04,240 --> 00:35:09,920
five by using quantum mechanics, not to

852
00:35:08,079 --> 00:35:12,400
destroy the energy in a bit at every

853
00:35:09,920 --> 00:35:15,880
cycle, but to carefully manipulate it.

854
00:35:12,400 --> 00:35:15,880
>> Yeah, thanks.

855
00:35:16,480 --> 00:35:18,560
>> More questions?

856
00:35:17,119 --> 00:35:19,200
>> I think there's one back there.

857
00:35:18,560 --> 00:35:23,200
>> Yeah,

858
00:35:19,200 --> 00:35:26,160
>> thank you. Um, the Intel 404, I had to

859
00:35:23,200 --> 00:35:29,440
look it up. Um, it's a processor that

860
00:35:26,160 --> 00:35:33,119
came out in 1971 and had 2,300

861
00:35:29,440 --> 00:35:36,560
transistors. Um, what's the density of

862
00:35:33,119 --> 00:35:40,079
that processor that's currently um, you

863
00:35:36,560 --> 00:35:43,119
know, in testing? Is it the same size or

864
00:35:40,079 --> 00:35:47,680
is it bigger? So uh what's so again the

865
00:35:43,119 --> 00:35:51,359
the the reason yeah the the Intel 404 it

866
00:35:47,680 --> 00:35:53,839
it is in very short order became the

867
00:35:51,359 --> 00:35:56,800
8088 which became the IBM PC and then

868
00:35:53,839 --> 00:36:00,000
the world exploded and so it it it was

869
00:35:56,800 --> 00:36:02,000
the the the key complexity scale where

870
00:36:00,000 --> 00:36:04,079
the world diverged and Moors law really

871
00:36:02,000 --> 00:36:05,359
took off that's why it's such a revered

872
00:36:04,079 --> 00:36:08,480
processor and so we're at that

873
00:36:05,359 --> 00:36:11,359
complexity scale now the to the previous

874
00:36:08,480 --> 00:36:14,240
question um our feature size is on the

875
00:36:11,359 --> 00:36:16,560
order of micron scale, you know, maybe

876
00:36:14,240 --> 00:36:19,359
100 nanometers. The feature size we're

877
00:36:16,560 --> 00:36:23,520
using is ancient compared to

878
00:36:19,359 --> 00:36:26,320
state-of-the-art uh semiconductor fab uh

879
00:36:23,520 --> 00:36:28,240
uh but that's limited by the physics. So

880
00:36:26,320 --> 00:36:30,880
the these Josephson junctions

881
00:36:28,240 --> 00:36:33,119
manipulating quantum coherence, we can't

882
00:36:30,880 --> 00:36:34,640
shrink them arbitrarily smallly. There

883
00:36:33,119 --> 00:36:38,079
there's a natural scale length to the

884
00:36:34,640 --> 00:36:42,560
physics. Um but one of the really

885
00:36:38,079 --> 00:36:44,960
interesting things is uh moving data the

886
00:36:42,560 --> 00:36:47,040
two orders of magnitude that comes from

887
00:36:44,960 --> 00:36:49,200
the architectural inefficiency of the

888
00:36:47,040 --> 00:36:50,880
software and hardware disagreeing but it

889
00:36:49,200 --> 00:36:54,400
also just comes from the cost of pushing

890
00:36:50,880 --> 00:36:55,440
bits through wires. Um we heard earlier

891
00:36:54,400 --> 00:36:57,839
today somebody talking about

892
00:36:55,440 --> 00:37:00,160
superconducting power. That's true for

893
00:36:57,839 --> 00:37:02,000
us down at the bit level. There's no

894
00:37:00,160 --> 00:37:05,599
loss in moving our bits through these

895
00:37:02,000 --> 00:37:08,960
wires. So interconnect is um much much

896
00:37:05,599 --> 00:37:10,720
much cheaper in this world and in fact

897
00:37:08,960 --> 00:37:14,240
even in some of these architectures

898
00:37:10,720 --> 00:37:15,760
interconnect is a feature not a bit bug

899
00:37:14,240 --> 00:37:17,359
meaning we have flying bits in these

900
00:37:15,760 --> 00:37:20,640
wires is an interesting architectural

901
00:37:17,359 --> 00:37:22,800
implication. So it's possible there

902
00:37:20,640 --> 00:37:24,720
might be some scaling but for many

903
00:37:22,800 --> 00:37:26,400
reasons we don't need to push the

904
00:37:24,720 --> 00:37:30,079
feature size and we're not pushing the

905
00:37:26,400 --> 00:37:34,400
feature size um we're scaling out in the

906
00:37:30,079 --> 00:37:36,720
system size. Anything you'd add, Alex?

907
00:37:34,400 --> 00:37:40,079
>> I follow that. I mean, the other point I

908
00:37:36,720 --> 00:37:42,640
guess is scaling. One of the reasons COS

909
00:37:40,079 --> 00:37:44,560
had to shrink a lot related to charging

910
00:37:42,640 --> 00:37:46,640
wires. Everything gets smaller. It's

911
00:37:44,560 --> 00:37:48,560
lower power. Uh same thing with

912
00:37:46,640 --> 00:37:50,160
transistor sizes. None of that is true

913
00:37:48,560 --> 00:37:52,240
for us. So, we don't have the same power

914
00:37:50,160 --> 00:37:54,400
benefits. So it's that that's an

915
00:37:52,240 --> 00:37:56,640
economic no that's not

916
00:37:54,400 --> 00:38:01,280
>> what I'm trying to understand is how

917
00:37:56,640 --> 00:38:02,400
much bigger would a computer be um with

918
00:38:01,280 --> 00:38:05,280
the super

919
00:38:02,400 --> 00:38:08,000
>> physically physically it's likely it's

920
00:38:05,280 --> 00:38:10,960
it's quite plausibly going to be smaller

921
00:38:08,000 --> 00:38:13,440
>> because if you look at the megawatt uh

922
00:38:10,960 --> 00:38:15,440
Nvidia rack

923
00:38:13,440 --> 00:38:17,680
the work is actually happening in a tiny

924
00:38:15,440 --> 00:38:19,680
part of the rack and all the rest is

925
00:38:17,680 --> 00:38:24,160
packaging around that and heat transfer

926
00:38:19,680 --> 00:38:27,119
and you know wire interconnect um we can

927
00:38:24,160 --> 00:38:29,359
use space much more efficiently

928
00:38:27,119 --> 00:38:30,960
and so yeah I don't think we've done

929
00:38:29,359 --> 00:38:34,640
exactly that calculation but quite

930
00:38:30,960 --> 00:38:39,520
plausibly um uh

931
00:38:34,640 --> 00:38:41,280
>> so the heat transfer efficiency is it's

932
00:38:39,520 --> 00:38:45,000
something like 100 times better if you

933
00:38:41,280 --> 00:38:45,000
normalize because imagerating

934
00:38:49,599 --> 00:38:52,880
10 the five cheaper. So it's kind of

935
00:38:51,040 --> 00:38:54,320
like 10 the five watts per centimeter

936
00:38:52,880 --> 00:38:57,280
which is sort of a higher effective

937
00:38:54,320 --> 00:39:00,320
power density on the chip than you know

938
00:38:57,280 --> 00:39:02,079
this the surface of the cell. So that's

939
00:39:00,320 --> 00:39:03,440
what sets it at the rack scale. You know

940
00:39:02,079 --> 00:39:05,680
ultimately on the chip it might be super

941
00:39:03,440 --> 00:39:08,720
high power density but you have to

942
00:39:05,680 --> 00:39:11,119
dissipate it over some much larger area

943
00:39:08,720 --> 00:39:12,720
discharge to the ocean etc. And so from

944
00:39:11,119 --> 00:39:14,079
that perspective of how much heat do you

945
00:39:12,720 --> 00:39:16,560
have to get rid of that's what sets

946
00:39:14,079 --> 00:39:18,400
these really big scales like a size

947
00:39:16,560 --> 00:39:22,200
building.

948
00:39:18,400 --> 00:39:22,200
might be possibly smaller.

949
00:39:22,400 --> 00:39:27,920
>> Okay,

950
00:39:24,480 --> 00:39:32,119
any other questions?

951
00:39:27,920 --> 00:39:32,119
>> Okay, then let's hear it for Neil.

