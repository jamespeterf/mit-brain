1
00:00:05,520 --> 00:00:13,280
I'm going to introduce our next speaker.

2
00:00:10,000 --> 00:00:15,920
Our next speaker is from MIT Lincoln

3
00:00:13,280 --> 00:00:18,720
Lab, which actually isn't on the main

4
00:00:15,920 --> 00:00:23,279
campus here. Is about 25 miles west of

5
00:00:18,720 --> 00:00:26,880
here. Uh yeah. Um has an even uh bigger

6
00:00:23,279 --> 00:00:29,199
a larger uh R&D budget than the campus

7
00:00:26,880 --> 00:00:31,519
uh proper itself. uh doing a lot of work

8
00:00:29,199 --> 00:00:34,480
in homeland security. Uh so we thought

9
00:00:31,519 --> 00:00:37,920
it was be a great idea to uh bring in

10
00:00:34,480 --> 00:00:40,320
Dennis Ross who is a group leader of

11
00:00:37,920 --> 00:00:43,760
artificial intelligence technology and

12
00:00:40,320 --> 00:00:46,800
systems at MIT Lincoln Lab and he's

13
00:00:43,760 --> 00:00:48,480
going to talk to you about how AI and

14
00:00:46,800 --> 00:00:52,559
machine learning are affecting security

15
00:00:48,480 --> 00:00:54,879
and software. So let's hear for Dennis.

16
00:00:52,559 --> 00:00:56,640
All right. Uh good morning everybody. Uh

17
00:00:54,879 --> 00:00:58,320
my I'm glad everyone's in a good mood

18
00:00:56,640 --> 00:01:00,719
because you might not be after this. Uh

19
00:00:58,320 --> 00:01:02,160
a lot of this talk is going to be uh

20
00:01:00,719 --> 00:01:04,080
some of the challenges that we're seeing

21
00:01:02,160 --> 00:01:05,760
uh out in the world. Of course, as

22
00:01:04,080 --> 00:01:07,200
software engineers, you're kind of at

23
00:01:05,760 --> 00:01:09,040
the front lines of all this. So before I

24
00:01:07,200 --> 00:01:10,880
get started, just out of curiosity, how

25
00:01:09,040 --> 00:01:12,400
many of you work in cyber security in

26
00:01:10,880 --> 00:01:15,119
some way, shape, or form in your daily

27
00:01:12,400 --> 00:01:16,920
job? Okay, so a handful here. Uh how

28
00:01:15,119 --> 00:01:19,119
many of you work in AI

29
00:01:16,920 --> 00:01:20,320
security? All right. Now, everyone that

30
00:01:19,119 --> 00:01:21,680
didn't raise your hand, you're probably

31
00:01:20,320 --> 00:01:23,200
going to have to at some point very,

32
00:01:21,680 --> 00:01:24,880
very soon because you're on the front

33
00:01:23,200 --> 00:01:26,400
line of all the stuff you're about to

34
00:01:24,880 --> 00:01:28,880
see, uh, and we're going to talk through

35
00:01:26,400 --> 00:01:31,200
a little bit here. So, uh, my name is

36
00:01:28,880 --> 00:01:34,000
Dennis Ross. I lead a a group. We have

37
00:01:31,200 --> 00:01:36,240
about 60 or 70 folks, uh, working in,

38
00:01:34,000 --> 00:01:38,079
uh, building AI technologies, delivering

39
00:01:36,240 --> 00:01:39,600
them usually through commercial

40
00:01:38,079 --> 00:01:41,680
partnerships or direct to the government

41
00:01:39,600 --> 00:01:43,119
in lots of different areas. Uh, Lincoln

42
00:01:41,680 --> 00:01:44,880
Laboratory, we sit in Lexington,

43
00:01:43,119 --> 00:01:46,880
Massachusetts, although we do have a lab

44
00:01:44,880 --> 00:01:48,880
down on campus as well. uh and we're

45
00:01:46,880 --> 00:01:51,360
about a billion and a third a year of

46
00:01:48,880 --> 00:01:53,119
R&D and we do a lot of work across lots

47
00:01:51,360 --> 00:01:55,200
of different domains. I represent some

48
00:01:53,119 --> 00:01:56,479
of the AI and cyber pieces and then

49
00:01:55,200 --> 00:01:58,079
we'll jump around a little bit about

50
00:01:56,479 --> 00:02:00,479
some of the challenges and trends we're

51
00:01:58,079 --> 00:02:02,479
seeing. Uh personally, I am a

52
00:02:00,479 --> 00:02:03,840
mathematician by trade. I'm sorry, an AI

53
00:02:02,479 --> 00:02:06,560
researcher by trade because I like to

54
00:02:03,840 --> 00:02:08,160
get paid more. Um but generally

55
00:02:06,560 --> 00:02:10,959
speaking, I'm working with folks that

56
00:02:08,160 --> 00:02:12,640
are software engineers, uh front-end

57
00:02:10,959 --> 00:02:14,720
designers, psychologists,

58
00:02:12,640 --> 00:02:16,400
astrophysicists. It's this weird mix of

59
00:02:14,720 --> 00:02:18,000
all sorts of different skills that it

60
00:02:16,400 --> 00:02:19,360
takes to build really interesting

61
00:02:18,000 --> 00:02:21,440
software against some very hard

62
00:02:19,360 --> 00:02:22,879
problems. Uh so as we go through this,

63
00:02:21,440 --> 00:02:24,239
if you have a really burning question,

64
00:02:22,879 --> 00:02:25,920
let me know. But I do want to save time

65
00:02:24,239 --> 00:02:27,280
at the end to have some discussion. Uh

66
00:02:25,920 --> 00:02:28,879
and I think it might be a little bit of

67
00:02:27,280 --> 00:02:30,239
a different view than we've seen so far,

68
00:02:28,879 --> 00:02:31,920
which have been some fantastic talks

69
00:02:30,239 --> 00:02:34,080
this morning. Uh looking at some of the

70
00:02:31,920 --> 00:02:35,760
specific security implications. So

71
00:02:34,080 --> 00:02:37,040
before I get into much else, I just want

72
00:02:35,760 --> 00:02:39,280
to explain a little bit about our group,

73
00:02:37,040 --> 00:02:41,680
who we are, what we do. Uh so in my

74
00:02:39,280 --> 00:02:43,920
group we work across four major areas

75
00:02:41,680 --> 00:02:46,239
specifically in things like multimedia.

76
00:02:43,920 --> 00:02:48,480
In fact the first uh packet over IP came

77
00:02:46,239 --> 00:02:51,360
out of our group well before I was there

78
00:02:48,480 --> 00:02:53,200
uh sometime uh about 60 years ago. So

79
00:02:51,360 --> 00:02:54,879
this is a really longunning series of

80
00:02:53,200 --> 00:02:56,959
efforts which have looked like analytics

81
00:02:54,879 --> 00:02:58,800
signal processing which again we're just

82
00:02:56,959 --> 00:03:00,319
calling everything AI now. Uh but we

83
00:02:58,800 --> 00:03:02,080
have a long heritage in looking at what

84
00:03:00,319 --> 00:03:05,000
multimedia especially voice looks like

85
00:03:02,080 --> 00:03:07,519
over time. cyber security uh especially

86
00:03:05,000 --> 00:03:09,840
building things to be more secure and

87
00:03:07,519 --> 00:03:11,680
finding the uh ways to quickly automate

88
00:03:09,840 --> 00:03:12,879
uh threat hunting uh patching

89
00:03:11,680 --> 00:03:15,040
vulnerabilities all those different

90
00:03:12,879 --> 00:03:16,239
aspects that go into cyber uh fall into

91
00:03:15,040 --> 00:03:18,080
a group and we've been doing these two

92
00:03:16,239 --> 00:03:19,519
things for a very long time. What we're

93
00:03:18,080 --> 00:03:21,040
seeing is the emergence of some new

94
00:03:19,519 --> 00:03:23,200
areas which are based on those core

95
00:03:21,040 --> 00:03:24,319
technologies uh specifically in things

96
00:03:23,200 --> 00:03:26,480
like what we're calling information

97
00:03:24,319 --> 00:03:27,840
deterrence, information operations. Uh

98
00:03:26,480 --> 00:03:29,440
our group doesn't work in things like

99
00:03:27,840 --> 00:03:31,440
social media. uh we don't really work

100
00:03:29,440 --> 00:03:34,319
with sort of population level areas but

101
00:03:31,440 --> 00:03:35,680
more in how do we look at trends how do

102
00:03:34,319 --> 00:03:37,120
we look at what kind of messaging is

103
00:03:35,680 --> 00:03:38,640
going out think more like voice of

104
00:03:37,120 --> 00:03:41,280
America versus Facebook something like

105
00:03:38,640 --> 00:03:43,120
this uh and then the last part here is I

106
00:03:41,280 --> 00:03:45,120
actually operate the laboratory's AI red

107
00:03:43,120 --> 00:03:48,239
team and AI blue teams uh where we go

108
00:03:45,120 --> 00:03:51,519
out and try to find uh all sorts of ways

109
00:03:48,239 --> 00:03:53,599
to break and fix AI models uh the key

110
00:03:51,519 --> 00:03:55,280
here is almost all this is in software

111
00:03:53,599 --> 00:03:56,879
which of those interconnected systems do

112
00:03:55,280 --> 00:03:59,200
we find vulnerabilities this looks like

113
00:03:56,879 --> 00:04:00,319
cyber honestly from the most part. Uh,

114
00:03:59,200 --> 00:04:02,959
and what you're going to see throughout

115
00:04:00,319 --> 00:04:05,920
this is I'm going to talk very loosely

116
00:04:02,959 --> 00:04:07,599
between cyber AI software, they're kind

117
00:04:05,920 --> 00:04:09,599
of all the same thing to me, right? It's

118
00:04:07,599 --> 00:04:11,519
a big bucket of stuff that is all driven

119
00:04:09,599 --> 00:04:13,040
by code. So, the kind of vulnerabilities

120
00:04:11,519 --> 00:04:14,560
that we're seeing, the kind of ways in

121
00:04:13,040 --> 00:04:16,560
which we can exploit and then defend and

122
00:04:14,560 --> 00:04:18,079
patch these systems, it doesn't really

123
00:04:16,560 --> 00:04:19,519
matter if we're looking at an AI system,

124
00:04:18,079 --> 00:04:21,840
a cyber system, or some of these sort of

125
00:04:19,519 --> 00:04:23,919
run-of-the-mill um, vulnerabilities and

126
00:04:21,840 --> 00:04:26,160
software. We're going to go about it

127
00:04:23,919 --> 00:04:27,440
basically the same way.

128
00:04:26,160 --> 00:04:28,639
I'm not going to brief everything you're

129
00:04:27,440 --> 00:04:30,000
going to see on these slides. I actually

130
00:04:28,639 --> 00:04:31,280
wanted to make them as something you

131
00:04:30,000 --> 00:04:33,199
could sort of take away with you, some

132
00:04:31,280 --> 00:04:35,040
compendiums of interesting information.

133
00:04:33,199 --> 00:04:36,639
Uh you're going to see some companies on

134
00:04:35,040 --> 00:04:38,400
here, which is a no- win endorsements.

135
00:04:36,639 --> 00:04:39,520
It's just a by the way, here's some

136
00:04:38,400 --> 00:04:41,520
companies doing interesting things in

137
00:04:39,520 --> 00:04:43,600
the space. Uh but I've arranged this

138
00:04:41,520 --> 00:04:45,759
talk around some core areas specifically

139
00:04:43,600 --> 00:04:48,000
in what are we looking at for AI for

140
00:04:45,759 --> 00:04:50,000
cyber defense? How can we secure some of

141
00:04:48,000 --> 00:04:52,240
these AI systems? And what are some of

142
00:04:50,000 --> 00:04:53,759
those emerging threats? And again,

143
00:04:52,240 --> 00:04:55,120
software engineers, we're talking

144
00:04:53,759 --> 00:04:56,400
software. I'm just going to use one of

145
00:04:55,120 --> 00:04:58,280
these three words, mathematician, right?

146
00:04:56,400 --> 00:05:00,800
Up to isomeorphism, they're all the same

147
00:04:58,280 --> 00:05:02,560
thing. All right. What we've been seeing

148
00:05:00,800 --> 00:05:04,400
though is cyber security and AI

149
00:05:02,560 --> 00:05:06,479
software, all of this stuff is just

150
00:05:04,400 --> 00:05:08,320
coming together all at once. And if we

151
00:05:06,479 --> 00:05:10,320
actually want to get to a place where

152
00:05:08,320 --> 00:05:13,120
we're building systems that are secure,

153
00:05:10,320 --> 00:05:14,479
safe across all of our industries, we

154
00:05:13,120 --> 00:05:16,080
need to start thinking about this from

155
00:05:14,479 --> 00:05:18,080
the lowest levels of software design,

156
00:05:16,080 --> 00:05:19,199
data design, all the way up to the sort

157
00:05:18,080 --> 00:05:21,440
of finished systems and humans

158
00:05:19,199 --> 00:05:22,960
interacting with them. So hopefully by

159
00:05:21,440 --> 00:05:25,039
the end of this talk, everyone will be

160
00:05:22,960 --> 00:05:26,960
deputized to go be a cyber defender, be

161
00:05:25,039 --> 00:05:29,400
an AI defender, even if the language you

162
00:05:26,960 --> 00:05:32,160
speak is in software. All

163
00:05:29,400 --> 00:05:33,520
right. So this may be something you've

164
00:05:32,160 --> 00:05:35,759
thought about a little bit. Maybe you've

165
00:05:33,520 --> 00:05:38,479
seen uh other versions of what are these

166
00:05:35,759 --> 00:05:40,320
sort of attack defense patterns? Uh how

167
00:05:38,479 --> 00:05:42,639
might we actuate particular effects that

168
00:05:40,320 --> 00:05:45,120
we care about? What I want to lay out is

169
00:05:42,639 --> 00:05:47,600
similar to how we think about cyber, AI

170
00:05:45,120 --> 00:05:48,960
is itself an attack surface, right? And

171
00:05:47,600 --> 00:05:50,639
there's lots of ways to get after it. We

172
00:05:48,960 --> 00:05:51,919
use this very loosely and say, "Oh, of

173
00:05:50,639 --> 00:05:53,840
course these systems are vulnerable.

174
00:05:51,919 --> 00:05:56,080
There might be things that we can do to

175
00:05:53,840 --> 00:05:57,600
uh induce a bad effect, maybe release

176
00:05:56,080 --> 00:05:59,440
some bad code, whatever those different

177
00:05:57,600 --> 00:06:00,960
things are." But really, there's a whole

178
00:05:59,440 --> 00:06:03,039
bunch of different areas we can be

179
00:06:00,960 --> 00:06:05,280
talking about. Are we going after uh

180
00:06:03,039 --> 00:06:06,720
ways to attack a cyber system? Are we

181
00:06:05,280 --> 00:06:08,560
going after an information system? Are

182
00:06:06,720 --> 00:06:10,240
we going after the AI? Are we just

183
00:06:08,560 --> 00:06:11,759
looking at the downstream effects? So,

184
00:06:10,240 --> 00:06:13,600
the reason I want to lay out this way is

185
00:06:11,759 --> 00:06:16,160
you might be able to actuate via cyber

186
00:06:13,600 --> 00:06:18,319
or electronic warfare uh methods to

187
00:06:16,160 --> 00:06:20,639
induce some sort of effect in the AI

188
00:06:18,319 --> 00:06:22,639
space or maybe you're using AI to induce

189
00:06:20,639 --> 00:06:24,160
some sort of effect in cyerspace. Again,

190
00:06:22,639 --> 00:06:26,160
I don't really care which one of these

191
00:06:24,160 --> 00:06:28,000
ways we're going. Just useful to think

192
00:06:26,160 --> 00:06:30,240
about what is our actual attack surface.

193
00:06:28,000 --> 00:06:31,919
I feel like we generally speak and say,

194
00:06:30,240 --> 00:06:33,120
okay, well, we care about threats, we

195
00:06:31,919 --> 00:06:35,120
care about vulnerabilities, we care

196
00:06:33,120 --> 00:06:37,039
about software issues, but we don't tend

197
00:06:35,120 --> 00:06:38,400
to be very specific on no, no, but

198
00:06:37,039 --> 00:06:40,240
seriously, what is the way someone could

199
00:06:38,400 --> 00:06:41,520
actually do this? What are the ways that

200
00:06:40,240 --> 00:06:43,440
we should actually be defending against

201
00:06:41,520 --> 00:06:45,600
it? Uh, in the cyber world, you might

202
00:06:43,440 --> 00:06:47,840
have seen lots of the like common uh

203
00:06:45,600 --> 00:06:49,440
vulnerability uh like CVES, things like

204
00:06:47,840 --> 00:06:50,639
this where we're trying to rack and

205
00:06:49,440 --> 00:06:52,319
stack. We're trying to provide more

206
00:06:50,639 --> 00:06:54,000
information. Uh but what I'm trying to

207
00:06:52,319 --> 00:06:56,000
get at here is there are other ways to

208
00:06:54,000 --> 00:06:57,680
categorize these to look at what are the

209
00:06:56,000 --> 00:06:59,520
actual risks to you and the systems you

210
00:06:57,680 --> 00:07:01,360
care about. Now in your commercial

211
00:06:59,520 --> 00:07:02,800
companies you might have very different

212
00:07:01,360 --> 00:07:04,560
attack surfaces. You might care about

213
00:07:02,800 --> 00:07:06,400
very different things. Uh but try to

214
00:07:04,560 --> 00:07:08,000
think of this all of your problems and

215
00:07:06,400 --> 00:07:09,199
frame them in a way which is how might

216
00:07:08,000 --> 00:07:12,639
someone exploit them and what might

217
00:07:09,199 --> 00:07:14,160
those downstream effects be. All right.

218
00:07:12,639 --> 00:07:15,360
I'm going to build a lot of charts here.

219
00:07:14,160 --> 00:07:17,039
I'm not going to read a lot of charts,

220
00:07:15,360 --> 00:07:18,720
but these are uh things that you can

221
00:07:17,039 --> 00:07:21,199
take away uh use maybe in your

222
00:07:18,720 --> 00:07:23,319
day-to-day uh space for the cyber

223
00:07:21,199 --> 00:07:26,240
defense. Has anyone heard of DARPA's

224
00:07:23,319 --> 00:07:28,960
AIXCC challenge by chance? Actually

225
00:07:26,240 --> 00:07:30,880
lower than I was expecting. So, uh DARPA

226
00:07:28,960 --> 00:07:34,639
stood up this grand challenge very

227
00:07:30,880 --> 00:07:37,080
specifically to say, can we find and

228
00:07:34,639 --> 00:07:39,599
patch automatically in software

229
00:07:37,080 --> 00:07:41,360
vulnerabilities? So, there is a a set of

230
00:07:39,599 --> 00:07:43,599
open challenges. There'll be all sorts

231
00:07:41,360 --> 00:07:45,840
of links in here later you can see. But

232
00:07:43,599 --> 00:07:47,840
this is using lots of systems, LLMs

233
00:07:45,840 --> 00:07:50,479
especially to be able to look at code

234
00:07:47,840 --> 00:07:52,319
bases, common open source uh toolkits,

235
00:07:50,479 --> 00:07:54,560
libraries, and actually injecting

236
00:07:52,319 --> 00:07:56,560
vulnerabilities into them to then do

237
00:07:54,560 --> 00:07:58,639
things like capture the flag exercises.

238
00:07:56,560 --> 00:08:00,319
And then we're seeing the uh competitors

239
00:07:58,639 --> 00:08:01,759
do things like use an LLM that they're

240
00:08:00,319 --> 00:08:03,680
fine-tuning to find the vulnerabilities

241
00:08:01,759 --> 00:08:04,879
and LLM put in. But you know, that is

242
00:08:03,680 --> 00:08:07,199
sort of the way the world goes right

243
00:08:04,879 --> 00:08:09,039
now. Everything is an LLM. So what are

244
00:08:07,199 --> 00:08:11,360
we taking away from this? This is a

245
00:08:09,039 --> 00:08:13,280
system and a a set of exercises that are

246
00:08:11,360 --> 00:08:14,960
set up to be able to generate

247
00:08:13,280 --> 00:08:16,800
interesting vulnerabilities, to be able

248
00:08:14,960 --> 00:08:18,720
to generate capabilities for defending

249
00:08:16,800 --> 00:08:20,639
them and automatic patching. So the

250
00:08:18,720 --> 00:08:22,000
dream is that in a a world in the

251
00:08:20,639 --> 00:08:23,039
future, you're going to focus on the

252
00:08:22,000 --> 00:08:24,400
things you heard earlier. You're going

253
00:08:23,039 --> 00:08:25,840
to look at that modular design. You're

254
00:08:24,400 --> 00:08:27,199
going to be a software architect. You're

255
00:08:25,840 --> 00:08:28,720
not going to have to worry as much about

256
00:08:27,199 --> 00:08:30,479
what is the cyber security of your

257
00:08:28,720 --> 00:08:31,919
systems. I don't think these things are

258
00:08:30,479 --> 00:08:33,599
ever going to be foolproof, but these

259
00:08:31,919 --> 00:08:35,919
kind of challenges are ways in which you

260
00:08:33,599 --> 00:08:37,680
can start to see how are we actually

261
00:08:35,919 --> 00:08:38,959
measuring uh the safety of these

262
00:08:37,680 --> 00:08:40,719
systems. How are we actually looking at

263
00:08:38,959 --> 00:08:42,719
vulnerabilities and what are ways to

264
00:08:40,719 --> 00:08:44,560
automatically patch them? Uh I think the

265
00:08:42,719 --> 00:08:46,640
finals of this are coming up maybe this

266
00:08:44,560 --> 00:08:48,240
summer or so. Uh kind of a multi-million

267
00:08:46,640 --> 00:08:50,480
dollar prize for people to be able to do

268
00:08:48,240 --> 00:08:51,680
automatic capture the flag as well as

269
00:08:50,480 --> 00:08:52,800
creating some of these patches. So I

270
00:08:51,680 --> 00:08:54,320
encourage you to reach out and look at

271
00:08:52,800 --> 00:08:56,080
this. But what are some of the things

272
00:08:54,320 --> 00:08:58,880
that come up with this? If you're doing

273
00:08:56,080 --> 00:09:01,360
things like, "Hey, uh, chatbt, I have a

274
00:08:58,880 --> 00:09:03,040
common open source library here. Induce

275
00:09:01,360 --> 00:09:04,800
some sort of buffer overflow attack."

276
00:09:03,040 --> 00:09:06,000
Now, I could also just give that problem

277
00:09:04,800 --> 00:09:07,440
to a bunch of undergrads and they'll

278
00:09:06,000 --> 00:09:09,600
probably induce that for me, right? Like

279
00:09:07,440 --> 00:09:12,000
there's all sorts of ways to put bad uh

280
00:09:09,600 --> 00:09:13,680
code inside of these code bases. uh but

281
00:09:12,000 --> 00:09:15,680
trying to do this in a controlled way

282
00:09:13,680 --> 00:09:17,920
where you can automate it at scale using

283
00:09:15,680 --> 00:09:19,839
generative AI is really tough because

284
00:09:17,920 --> 00:09:21,680
what it tends to do is I can break

285
00:09:19,839 --> 00:09:23,839
anything but then it's very obvious came

286
00:09:21,680 --> 00:09:26,560
from a computer or maybe the effect

287
00:09:23,839 --> 00:09:28,080
never is u reachable inside of the code.

288
00:09:26,560 --> 00:09:30,480
How do you actually dial it in to be

289
00:09:28,080 --> 00:09:32,880
something that's interesting that is

290
00:09:30,480 --> 00:09:35,440
looks like the baseline code uh and is

291
00:09:32,880 --> 00:09:37,440
actually a challenge to find but also

292
00:09:35,440 --> 00:09:39,040
represents actual threats we see. So

293
00:09:37,440 --> 00:09:40,800
these are the kind of uh challenges

294
00:09:39,040 --> 00:09:43,519
we're seeing come out of this DARPA

295
00:09:40,800 --> 00:09:45,600
exercise uh and trying to build all the

296
00:09:43,519 --> 00:09:47,680
tools around that into existing software

297
00:09:45,600 --> 00:09:50,080
workflows to be able to do things like

298
00:09:47,680 --> 00:09:51,839
understand the risk, understand uh what

299
00:09:50,080 --> 00:09:53,440
these vulnerabilities are actuated by to

300
00:09:51,839 --> 00:09:55,440
be able to build patches or detectors

301
00:09:53,440 --> 00:09:56,640
against it is very difficult. So I'd

302
00:09:55,440 --> 00:09:58,000
encourage you all to look at some of the

303
00:09:56,640 --> 00:10:00,240
papers coming out of this, look at some

304
00:09:58,000 --> 00:10:01,440
of the software flows. But in general,

305
00:10:00,240 --> 00:10:03,040
this is the kind of things you're doing

306
00:10:01,440 --> 00:10:04,240
day-to-day if you're starting to use

307
00:10:03,040 --> 00:10:06,160
these LLMs, if you're starting to use

308
00:10:04,240 --> 00:10:08,560
generative AI, which I'm all for. These

309
00:10:06,160 --> 00:10:10,080
things are great. You are inducing

310
00:10:08,560 --> 00:10:12,880
potential vulnerabilities inside of your

311
00:10:10,080 --> 00:10:14,640
systems. Uh so from this at the bottom

312
00:10:12,880 --> 00:10:17,279
uh we're seeing and I'll try to move

313
00:10:14,640 --> 00:10:19,600
between both sides. What we're seeing is

314
00:10:17,279 --> 00:10:21,399
people are building these kind of tools

315
00:10:19,600 --> 00:10:24,000
uh to go provide these automatic

316
00:10:21,399 --> 00:10:26,640
patches. They tend to be LLM based but

317
00:10:24,000 --> 00:10:28,399
there are ways to do this. Now, I

318
00:10:26,640 --> 00:10:30,320
wouldn't trust this to fully fix the

319
00:10:28,399 --> 00:10:32,320
system right now, but I also wouldn't

320
00:10:30,320 --> 00:10:34,880
want my defenders or my software

321
00:10:32,320 --> 00:10:37,200
engineers not to be using these tools to

322
00:10:34,880 --> 00:10:38,800
try to do what they do faster. I'm

323
00:10:37,200 --> 00:10:40,320
actually okay if this doesn't find

324
00:10:38,800 --> 00:10:41,920
everything, and I still need to have a

325
00:10:40,320 --> 00:10:43,399
red team that is going to go for these

326
00:10:41,920 --> 00:10:45,360
really, really, really hard to find

327
00:10:43,399 --> 00:10:47,440
vulnerabilities. It's a defense and

328
00:10:45,360 --> 00:10:48,480
depth approach. So, this is just another

329
00:10:47,440 --> 00:10:50,000
tool you should start getting

330
00:10:48,480 --> 00:10:51,600
comfortable with. Again, I don't think

331
00:10:50,000 --> 00:10:53,120
software is any different than AI is any

332
00:10:51,600 --> 00:10:54,560
different than cyber. You should be

333
00:10:53,120 --> 00:10:56,240
thinking about this. What are some ways

334
00:10:54,560 --> 00:10:57,600
I might be introducing vulnerabilities?

335
00:10:56,240 --> 00:10:59,600
And at the same time, what are some ways

336
00:10:57,600 --> 00:11:00,800
that it can fix my crappy code? I

337
00:10:59,600 --> 00:11:02,240
mentioned a mathematician. I didn't

338
00:11:00,800 --> 00:11:03,920
learn how to code until I'm like, oh, I

339
00:11:02,240 --> 00:11:05,200
can do external graph theory with code

340
00:11:03,920 --> 00:11:06,800
instead of drawing them all in my

341
00:11:05,200 --> 00:11:08,480
notebook, which happened when I was

342
00:11:06,800 --> 00:11:10,800
doing my like actual dissertation,

343
00:11:08,480 --> 00:11:12,560
right? I learned very late uh how to

344
00:11:10,800 --> 00:11:14,320
code. So, my code is crappy, but that's

345
00:11:12,560 --> 00:11:16,279
okay because I'm able to uh make it

346
00:11:14,320 --> 00:11:18,480
better with some of these automated

347
00:11:16,279 --> 00:11:20,640
tools. Uh we've talked a little bit

348
00:11:18,480 --> 00:11:22,480
about Copilot today. I'm not sure if

349
00:11:20,640 --> 00:11:24,240
anyone has used it for the more sort of

350
00:11:22,480 --> 00:11:26,640
agentic type approaches. I know it's

351
00:11:24,240 --> 00:11:29,440
come up a few times. Uh but now instead

352
00:11:26,640 --> 00:11:31,279
of just saying generate me code, it's

353
00:11:29,440 --> 00:11:33,680
working in sort of broader ecosystems.

354
00:11:31,279 --> 00:11:35,839
It seems sore cyber security ecosystems

355
00:11:33,680 --> 00:11:37,600
where instead of having to produce code,

356
00:11:35,839 --> 00:11:39,839
you're trying to produce insights across

357
00:11:37,600 --> 00:11:41,519
lots of different data sets. Uh so now

358
00:11:39,839 --> 00:11:43,440
imagine that if you're actually looking

359
00:11:41,519 --> 00:11:45,920
at your entire industries your entire

360
00:11:43,440 --> 00:11:47,920
company's uh cyber security posture for

361
00:11:45,920 --> 00:11:50,079
example lots of alerts are coming in

362
00:11:47,920 --> 00:11:51,600
lots of data are coming in how can you

363
00:11:50,079 --> 00:11:53,200
actually understand this in a way that

364
00:11:51,600 --> 00:11:55,040
lets you take any action right you can

365
00:11:53,200 --> 00:11:56,800
go patch you can go do things that are

366
00:11:55,040 --> 00:11:58,399
general hygiene uh you might even

367
00:11:56,800 --> 00:11:59,680
automate some of those but what are the

368
00:11:58,399 --> 00:12:00,720
ones you should care about what are the

369
00:11:59,680 --> 00:12:02,000
risks that are actually happening to

370
00:12:00,720 --> 00:12:03,519
your system and how do you actually do

371
00:12:02,000 --> 00:12:06,000
this without uh burning out all of your

372
00:12:03,519 --> 00:12:07,440
IT and cyber defense people well when

373
00:12:06,000 --> 00:12:09,519
you start looking at these kind of

374
00:12:07,440 --> 00:12:11,680
challenges it's really tough because

375
00:12:09,519 --> 00:12:13,519
every system is different. Every network

376
00:12:11,680 --> 00:12:14,959
is different. Uh the logs might look

377
00:12:13,519 --> 00:12:16,880
slightly different. Even though these

378
00:12:14,959 --> 00:12:19,120
LLM like approaches are pretty good at

379
00:12:16,880 --> 00:12:20,560
learning context over time, your system

380
00:12:19,120 --> 00:12:21,760
might just not look like what's in the

381
00:12:20,560 --> 00:12:23,680
training data. And we see this all the

382
00:12:21,760 --> 00:12:25,839
time, especially in the government side.

383
00:12:23,680 --> 00:12:27,200
Uh how do you also make sure that we're

384
00:12:25,839 --> 00:12:28,800
not just doing what you heard a little

385
00:12:27,200 --> 00:12:30,720
bit earlier, which is I'm only seeing

386
00:12:28,800 --> 00:12:32,639
things I saw before, right? Novel

387
00:12:30,720 --> 00:12:34,560
attacks happen all the time. Well, my

388
00:12:32,639 --> 00:12:36,000
argument here is use these tools to find

389
00:12:34,560 --> 00:12:37,360
the things that have been seen before

390
00:12:36,000 --> 00:12:38,639
and then spend your security analyst

391
00:12:37,360 --> 00:12:40,000
time to find more interesting things

392
00:12:38,639 --> 00:12:42,959
they haven't seen, which is a really

393
00:12:40,000 --> 00:12:44,399
hard problem. Uh, and then if we look at

394
00:12:42,959 --> 00:12:46,480
some of the ways in which we can use

395
00:12:44,399 --> 00:12:48,480
this to do interesting things uh from

396
00:12:46,480 --> 00:12:50,800
sort of the software side is how do we

397
00:12:48,480 --> 00:12:52,560
actually use these AI assistive tools as

398
00:12:50,800 --> 00:12:54,320
an assistant, right? I don't need it to

399
00:12:52,560 --> 00:12:56,160
automate my job away. What I need it to

400
00:12:54,320 --> 00:12:57,760
do is help find interesting context for

401
00:12:56,160 --> 00:12:59,440
me, summarize some things, run some

402
00:12:57,760 --> 00:13:01,200
queries for me, pull the data before I

403
00:12:59,440 --> 00:13:03,760
need it. Think of this as that sort of

404
00:13:01,200 --> 00:13:05,440
assistive mode where instead of just

405
00:13:03,760 --> 00:13:06,399
saying, "I need the AI to know exactly

406
00:13:05,440 --> 00:13:08,240
what to do because I defined

407
00:13:06,399 --> 00:13:09,760
everything." Just let it be clippy,

408
00:13:08,240 --> 00:13:11,279
right? Looks like you're trying to do a

409
00:13:09,760 --> 00:13:12,560
hunt. Here's some interesting data.

410
00:13:11,279 --> 00:13:14,560
Looks like you're trying to scrape this

411
00:13:12,560 --> 00:13:16,480
pcap file. Here's a way to extract out

412
00:13:14,560 --> 00:13:17,920
that entity you care about. And it turns

413
00:13:16,480 --> 00:13:19,839
out these tools are pretty good at it

414
00:13:17,920 --> 00:13:21,440
already. So, the reason I want to talk

415
00:13:19,839 --> 00:13:24,399
through this one a little bit is we have

416
00:13:21,440 --> 00:13:26,639
a way to uh use these agentic like

417
00:13:24,399 --> 00:13:28,800
systems. Okay. Move around a little bit.

418
00:13:26,639 --> 00:13:30,480
Hope they don't fall on a chair. uh to

419
00:13:28,800 --> 00:13:32,560
help automate some of our workflow,

420
00:13:30,480 --> 00:13:33,839
answer offload some of this work, and

421
00:13:32,560 --> 00:13:35,440
actually make us better because we don't

422
00:13:33,839 --> 00:13:37,120
need to be cyber defenders on a whole

423
00:13:35,440 --> 00:13:38,480
stack. In fact, you're going to see a

424
00:13:37,120 --> 00:13:40,959
whole lot of this, especially at the

425
00:13:38,480 --> 00:13:43,200
bottom. How many of you that work and

426
00:13:40,959 --> 00:13:44,639
understand data flows, network flows,

427
00:13:43,200 --> 00:13:46,720
how things are actually connected inside

428
00:13:44,639 --> 00:13:48,639
of your apps are thinking through the

429
00:13:46,720 --> 00:13:50,000
what are the actual say nation state

430
00:13:48,639 --> 00:13:51,279
level attacks that my company might be

431
00:13:50,000 --> 00:13:53,760
facing for someone trying to steal my

432
00:13:51,279 --> 00:13:55,120
IP? That intersection is really small.

433
00:13:53,760 --> 00:13:56,880
So, what are some ways to actually

434
00:13:55,120 --> 00:13:58,639
bridge those gaps through these assisted

435
00:13:56,880 --> 00:13:59,760
technologies? Turns out copilot works

436
00:13:58,639 --> 00:14:02,160
pretty well for this. Many of the other

437
00:13:59,760 --> 00:14:03,440
generative tools already do. Uh so we

438
00:14:02,160 --> 00:14:05,199
have to start thinking about how are we

439
00:14:03,440 --> 00:14:07,199
training our folks to actually look at

440
00:14:05,199 --> 00:14:08,480
these different tools, offload and

441
00:14:07,199 --> 00:14:10,399
define the kind of work that they should

442
00:14:08,480 --> 00:14:12,120
be doing and then hopefully push forward

443
00:14:10,399 --> 00:14:14,880
our defensive

444
00:14:12,120 --> 00:14:16,160
posture. Uh again I have a couple

445
00:14:14,880 --> 00:14:18,399
companies in here. This is an

446
00:14:16,160 --> 00:14:20,320
interesting one. Uh but it's not

447
00:14:18,399 --> 00:14:22,320
endorsing this particular one. Uh but

448
00:14:20,320 --> 00:14:25,440
for Resilient, how do you actually look

449
00:14:22,320 --> 00:14:26,880
at ways to understand what is your risk

450
00:14:25,440 --> 00:14:29,440
inside of your system? This is sort of

451
00:14:26,880 --> 00:14:31,360
the normal thing we talk about uh for

452
00:14:29,440 --> 00:14:33,199
cyber security which is what do I

453
00:14:31,360 --> 00:14:34,880
actually need to care about? What is

454
00:14:33,199 --> 00:14:37,279
going to attack me? What is going to

455
00:14:34,880 --> 00:14:38,480
lead to exploitation? So these kind of

456
00:14:37,279 --> 00:14:41,040
uh there's a ton of companies in the

457
00:14:38,480 --> 00:14:42,399
cottage we'll say security industry that

458
00:14:41,040 --> 00:14:44,160
have done this for cyber for a long

459
00:14:42,399 --> 00:14:46,560
time. What we're starting to see is

460
00:14:44,160 --> 00:14:48,480
people really relying on generative AI

461
00:14:46,560 --> 00:14:50,240
uh to be able to do things like rapid

462
00:14:48,480 --> 00:14:51,760
exploit development. If you have a

463
00:14:50,240 --> 00:14:53,360
vulnerability, can they show proof a

464
00:14:51,760 --> 00:14:54,720
concept right away that says yeah this

465
00:14:53,360 --> 00:14:56,959
is something you need to worry about.

466
00:14:54,720 --> 00:14:58,639
Can you generate network artifacts that

467
00:14:56,959 --> 00:15:00,720
your uh defenders can look at and say,

468
00:14:58,639 --> 00:15:02,320
"Yes, I can see this on my network or

469
00:15:00,720 --> 00:15:04,079
something similar to it. Maybe this is

470
00:15:02,320 --> 00:15:05,600
actively being attacked because the fact

471
00:15:04,079 --> 00:15:06,959
of the matter is we're never going to

472
00:15:05,600 --> 00:15:08,560
fix everything." So, if you're not going

473
00:15:06,959 --> 00:15:10,720
to fix everything, what are some ways in

474
00:15:08,560 --> 00:15:12,079
which you can prioritize those defenses?

475
00:15:10,720 --> 00:15:14,079
All right, I recognize these are really

476
00:15:12,079 --> 00:15:15,839
slide dense, it's not going to get less

477
00:15:14,079 --> 00:15:17,040
dense. Uh, and I'm not going to brief

478
00:15:15,839 --> 00:15:18,320
all of these, but I want you to be able

479
00:15:17,040 --> 00:15:21,279
to take them away if they're interesting

480
00:15:18,320 --> 00:15:23,920
to you. Uh, on this is some different

481
00:15:21,279 --> 00:15:26,079
initiatives across industry and the DoD.

482
00:15:23,920 --> 00:15:28,160
uh as an FFRDC, fedally funded research

483
00:15:26,079 --> 00:15:30,399
development center at MIT, uh we sit at

484
00:15:28,160 --> 00:15:32,639
the simplex between industry,

485
00:15:30,399 --> 00:15:34,079
government, and academia. Uh we like to

486
00:15:32,639 --> 00:15:35,920
say we're the best at all of them. The

487
00:15:34,079 --> 00:15:37,120
secret is we're just really do a little

488
00:15:35,920 --> 00:15:39,519
bit of all of them at the same time,

489
00:15:37,120 --> 00:15:42,079
which is the special sauce. Uh so we sit

490
00:15:39,519 --> 00:15:43,440
in a lot of these different areas. Take

491
00:15:42,079 --> 00:15:44,959
a look at these. You can read them

492
00:15:43,440 --> 00:15:47,040
after. Uh but there's many different

493
00:15:44,959 --> 00:15:49,040
initiatives that are playing across both

494
00:15:47,040 --> 00:15:51,120
industry and government right now. I do

495
00:15:49,040 --> 00:15:53,040
want to highlight, especially on the far

496
00:15:51,120 --> 00:15:55,839
left, which you've heard a lot today,

497
00:15:53,040 --> 00:15:58,240
industry is already using LLMs for a lot

498
00:15:55,839 --> 00:16:00,000
of code generation. I think the 47% was

499
00:15:58,240 --> 00:16:01,360
thrown around earlier. Uh my guess is

500
00:16:00,000 --> 00:16:03,360
some industries that's even higher

501
00:16:01,360 --> 00:16:04,800
depending on what you're actually doing.

502
00:16:03,360 --> 00:16:06,320
This isn't going away. The number of

503
00:16:04,800 --> 00:16:07,440
people we've hired in our group, the

504
00:16:06,320 --> 00:16:08,480
first question they ask is you have

505
00:16:07,440 --> 00:16:09,440
co-pilot, right? Even though we're

506
00:16:08,480 --> 00:16:11,240
working on these weird government

507
00:16:09,440 --> 00:16:13,839
things, like I can use that,

508
00:16:11,240 --> 00:16:15,839
right? It's it's not going away. Be

509
00:16:13,839 --> 00:16:17,600
aware of that. Tailor to your use cases.

510
00:16:15,839 --> 00:16:19,360
see how this can make you faster and

511
00:16:17,600 --> 00:16:21,360
better while uh not ignoring some of the

512
00:16:19,360 --> 00:16:23,440
risks. I did want to point out one thing

513
00:16:21,360 --> 00:16:25,759
on the uh government side specifically

514
00:16:23,440 --> 00:16:27,360
is red teams have been around for a long

515
00:16:25,759 --> 00:16:29,040
time, right? We've looked at systems, we

516
00:16:27,360 --> 00:16:30,480
probe for vulnerabilities. Many

517
00:16:29,040 --> 00:16:32,639
companies have their own red teams for

518
00:16:30,480 --> 00:16:34,639
their business use cases. We're starting

519
00:16:32,639 --> 00:16:37,120
to see this being very explicitly

520
00:16:34,639 --> 00:16:38,880
tailored to breaking AI systems. How do

521
00:16:37,120 --> 00:16:40,480
we defend them because we can show what

522
00:16:38,880 --> 00:16:42,720
those actual vulnerabilities that we can

523
00:16:40,480 --> 00:16:44,240
actually are. Uh so this is a major

524
00:16:42,720 --> 00:16:45,839
major trend. And there's a ton of

525
00:16:44,240 --> 00:16:47,839
movement in the government space and the

526
00:16:45,839 --> 00:16:49,440
industry space to catch up to be able to

527
00:16:47,839 --> 00:16:51,600
make sure we're actually looking at real

528
00:16:49,440 --> 00:16:53,440
vulnerabilities. So do think about those

529
00:16:51,600 --> 00:16:55,199
as you're bringing in those tools. Talk

530
00:16:53,440 --> 00:16:56,720
to your security professionals. Think

531
00:16:55,199 --> 00:16:57,920
about what it actually means for your

532
00:16:56,720 --> 00:16:59,839
product. Think what it means for your

533
00:16:57,920 --> 00:17:01,279
company and think about what those uh

534
00:16:59,839 --> 00:17:02,800
exploits might be. We'll talk about that

535
00:17:01,279 --> 00:17:04,959
in a little more detail on the next

536
00:17:02,800 --> 00:17:07,400
slide. Uh again, I'm not going to brief.

537
00:17:04,959 --> 00:17:10,160
Oops. Sorry if I lazed somebody.

538
00:17:07,400 --> 00:17:11,360
Um same thing here. There's a collection

539
00:17:10,160 --> 00:17:13,439
of interesting things going on in

540
00:17:11,360 --> 00:17:15,199
academia, uh, as well as some links of

541
00:17:13,439 --> 00:17:16,319
some interesting papers, uh, blogs,

542
00:17:15,199 --> 00:17:17,439
other posts you might want to see.

543
00:17:16,319 --> 00:17:18,559
Again, I'm not going to brief all of

544
00:17:17,439 --> 00:17:19,600
these, and I'm happy to share all the

545
00:17:18,559 --> 00:17:21,360
slides. You're welcome to take pictures

546
00:17:19,600 --> 00:17:22,720
of them, but I can send you a copy, uh,

547
00:17:21,360 --> 00:17:24,480
if you want to click through all this,

548
00:17:22,720 --> 00:17:26,039
but just things I found over the years

549
00:17:24,480 --> 00:17:28,160
that I think are rather

550
00:17:26,039 --> 00:17:30,240
interesting. All right, switching gears

551
00:17:28,160 --> 00:17:33,080
a little bit. Let's talk about uh, the

552
00:17:30,240 --> 00:17:34,799
defense across some of these AI specific

553
00:17:33,080 --> 00:17:37,360
systems. How many of you have used

554
00:17:34,799 --> 00:17:38,720
something on hugging face? Right. I

555
00:17:37,360 --> 00:17:40,559
mean, I use it all the time. It's a

556
00:17:38,720 --> 00:17:41,640
really interesting repository of a bunch

557
00:17:40,559 --> 00:17:44,240
of AI

558
00:17:41,640 --> 00:17:46,679
models. How many of you used a pickle

559
00:17:44,240 --> 00:17:49,559
from hugging face? A pickle file in your

560
00:17:46,679 --> 00:17:52,320
Python. Good. Right. Keep it

561
00:17:49,559 --> 00:17:54,240
low. It's not great. There's all sorts

562
00:17:52,320 --> 00:17:56,320
of ways in which uh exploits have been

563
00:17:54,240 --> 00:17:57,760
known to propagate through things like

564
00:17:56,320 --> 00:17:59,120
pickle files in Python or of course

565
00:17:57,760 --> 00:18:01,520
there's other methods in which uh

566
00:17:59,120 --> 00:18:03,280
exploits can come through. How many

567
00:18:01,520 --> 00:18:04,640
companies do you think are using models

568
00:18:03,280 --> 00:18:05,679
on HuggingFace right now? How many

569
00:18:04,640 --> 00:18:07,039
governments do you think are using

570
00:18:05,679 --> 00:18:09,760
models in these open- source

571
00:18:07,039 --> 00:18:11,039
repositories right now? It's a lot. So,

572
00:18:09,760 --> 00:18:13,039
what are we doing to make sure we

573
00:18:11,039 --> 00:18:15,200
understand some of those risks? We

574
00:18:13,039 --> 00:18:16,960
shouldn't ignore them, right? This is an

575
00:18:15,200 --> 00:18:19,520
amazing thing that we can share openly

576
00:18:16,960 --> 00:18:21,120
across the entire world. Advances in AI.

577
00:18:19,520 --> 00:18:22,799
I'm a huge advocate for that. Let's

578
00:18:21,120 --> 00:18:24,720
share all of our model weights. Let's

579
00:18:22,799 --> 00:18:25,840
advance this as much as we can. But

580
00:18:24,720 --> 00:18:27,760
there are going to be malicious folks

581
00:18:25,840 --> 00:18:29,760
out there who are trying to either steal

582
00:18:27,760 --> 00:18:31,760
IP and do cyber effects or other things

583
00:18:29,760 --> 00:18:33,760
inside of your systems. what is the way

584
00:18:31,760 --> 00:18:35,200
in which we can robustly understand what

585
00:18:33,760 --> 00:18:37,360
some of these risks are as well as

586
00:18:35,200 --> 00:18:39,600
building guard rails to protect them. So

587
00:18:37,360 --> 00:18:41,919
as you see more and more reliance on AI

588
00:18:39,600 --> 00:18:43,600
uh code completion turns out some of the

589
00:18:41,919 --> 00:18:45,200
models uh you can buy right now work

590
00:18:43,600 --> 00:18:47,120
really well and there's really good

591
00:18:45,200 --> 00:18:49,200
fine-tuned ones on hugging face that do

592
00:18:47,120 --> 00:18:51,720
a really good job at exactly those very

593
00:18:49,200 --> 00:18:53,919
specific things you want to do and say

594
00:18:51,720 --> 00:18:54,840
crypto maybe worry about that a little

595
00:18:53,919 --> 00:18:57,360
bit

596
00:18:54,840 --> 00:18:59,039
right as a side note. So actually last

597
00:18:57,360 --> 00:19:01,919
talk great way to use crypto. Try that

598
00:18:59,039 --> 00:19:03,760
one instead. All right. Uh there's also

599
00:19:01,919 --> 00:19:05,440
an interesting industry popping up now

600
00:19:03,760 --> 00:19:07,280
that's looking at ways to actually

601
00:19:05,440 --> 00:19:08,799
analyze these systems. What are some of

602
00:19:07,280 --> 00:19:11,039
those risks and vulnerabilities inside

603
00:19:08,799 --> 00:19:12,960
of uh these models? How can we actually

604
00:19:11,039 --> 00:19:15,039
scan them? And we don't actually know

605
00:19:12,960 --> 00:19:17,280
what these things look like sort of

606
00:19:15,039 --> 00:19:19,200
broadly. Uh as software engineers, we

607
00:19:17,280 --> 00:19:20,799
might say, okay, we can think about this

608
00:19:19,200 --> 00:19:22,320
as a code base. We can validate it, but

609
00:19:20,799 --> 00:19:23,679
what if it's just much model weights?

610
00:19:22,320 --> 00:19:25,520
Like what are the things that we

611
00:19:23,679 --> 00:19:27,600
actually need to understand about this?

612
00:19:25,520 --> 00:19:29,520
How do we educate uh the software

613
00:19:27,600 --> 00:19:30,960
engineers in the room to be able to

614
00:19:29,520 --> 00:19:32,960
understand what these risks are? How do

615
00:19:30,960 --> 00:19:34,799
you even poke and prod to know what you

616
00:19:32,960 --> 00:19:35,840
might be exposing your systems to? And

617
00:19:34,799 --> 00:19:37,280
that's really tough. So there are

618
00:19:35,840 --> 00:19:38,880
companies who are doing this now. You

619
00:19:37,280 --> 00:19:41,039
likely have some inbuilt solutions in

620
00:19:38,880 --> 00:19:42,480
your own uh companies. But do think

621
00:19:41,039 --> 00:19:43,840
about how do we actually figure out

622
00:19:42,480 --> 00:19:45,280
what's going on inside those models and

623
00:19:43,840 --> 00:19:47,039
what some of those risks are. And I

624
00:19:45,280 --> 00:19:48,480
think there's a lot of ways that we can

625
00:19:47,039 --> 00:19:51,440
uh help defend things sort of from the

626
00:19:48,480 --> 00:19:53,039
ground up. And then lastly uh picked on

627
00:19:51,440 --> 00:19:55,039
meta there's a lot of other folks who

628
00:19:53,039 --> 00:19:56,720
were looking at AI safety security or

629
00:19:55,039 --> 00:19:58,480
other permutations these words depending

630
00:19:56,720 --> 00:20:01,200
on what's uh popular in a particular

631
00:19:58,480 --> 00:20:03,679
day. Uh but how are we looking at ways

632
00:20:01,200 --> 00:20:06,000
to defend these systems? Uh so it's

633
00:20:03,679 --> 00:20:08,400
definitely in something like OpenAI or

634
00:20:06,000 --> 00:20:11,039
Meta's business model to not release

635
00:20:08,400 --> 00:20:13,360
ways to build a bomb from an LLM. Uh to

636
00:20:11,039 --> 00:20:15,360
not induce self harm because you're uh

637
00:20:13,360 --> 00:20:16,880
you know idealizing suicide or something

638
00:20:15,360 --> 00:20:18,720
else dangerous psychology

639
00:20:16,880 --> 00:20:20,559
psychologically speaking. How do you

640
00:20:18,720 --> 00:20:21,919
actually make sure you can detect that?

641
00:20:20,559 --> 00:20:23,440
What are ways that you can build in

642
00:20:21,919 --> 00:20:25,600
guardrails? What are ways you can log

643
00:20:23,440 --> 00:20:27,120
it? And what are ways you can stop it uh

644
00:20:25,600 --> 00:20:29,039
maybe from propagating across other

645
00:20:27,120 --> 00:20:30,159
industries as well? So think about what

646
00:20:29,039 --> 00:20:32,240
that might look like, what those

647
00:20:30,159 --> 00:20:34,159
guardrails are. These right now are

648
00:20:32,240 --> 00:20:36,080
really done boutique. It is people who

649
00:20:34,159 --> 00:20:37,679
are trying to break the models, doing

650
00:20:36,080 --> 00:20:40,080
interesting test harnesses, trying to

651
00:20:37,679 --> 00:20:41,840
induce bad behavior. Uh but hopefully we

652
00:20:40,080 --> 00:20:43,760
can build those guardrails back in and

653
00:20:41,840 --> 00:20:47,039
as a way to defend these systems across

654
00:20:43,760 --> 00:20:49,039
everybody's um ecosystems. Again, not

655
00:20:47,039 --> 00:20:50,960
going to brief all these. Feel free to

656
00:20:49,039 --> 00:20:52,640
take a picture, but we'll send them. Uh,

657
00:20:50,960 --> 00:20:55,120
across all of this, there's a lot going

658
00:20:52,640 --> 00:20:57,120
on in this this realm. Some of the AI

659
00:20:55,120 --> 00:20:59,120
security uh work in the last couple

660
00:20:57,120 --> 00:21:01,840
months has dwindled. Uh, but I think

661
00:20:59,120 --> 00:21:03,679
it's in people's business use cases and

662
00:21:01,840 --> 00:21:05,280
uh business interests to make sure that

663
00:21:03,679 --> 00:21:06,880
their models are safe and secure and I

664
00:21:05,280 --> 00:21:08,240
don't think that is going away. So,

665
00:21:06,880 --> 00:21:10,000
think about some ways in which we can

666
00:21:08,240 --> 00:21:11,760
push that forward. Uh, there are some

667
00:21:10,000 --> 00:21:14,480
great government frameworks. Uh, NIST

668
00:21:11,760 --> 00:21:16,000
has some, NSA has some. you can look at

669
00:21:14,480 --> 00:21:18,080
how can you secure these systems and

670
00:21:16,000 --> 00:21:19,679
how's the government think about it. Uh

671
00:21:18,080 --> 00:21:21,120
similarly lots of interesting things

672
00:21:19,679 --> 00:21:23,360
going on. I do want to highlight the

673
00:21:21,120 --> 00:21:25,360
watermarking and fingerprinting work.

674
00:21:23,360 --> 00:21:27,679
This is a really critical way which you

675
00:21:25,360 --> 00:21:30,240
can own your IP, watch how it travels

676
00:21:27,679 --> 00:21:31,440
around, watch it for if it's stolen and

677
00:21:30,240 --> 00:21:33,280
ways to make sure that things are

678
00:21:31,440 --> 00:21:34,480
behaving as expected inside of systems.

679
00:21:33,280 --> 00:21:36,240
So I would encourage you to read more

680
00:21:34,480 --> 00:21:38,159
about that uh as you're using these AI

681
00:21:36,240 --> 00:21:39,760
systems even for code generation. Right?

682
00:21:38,159 --> 00:21:41,360
What are some ways to know that this was

683
00:21:39,760 --> 00:21:42,559
made by a trusted system versus

684
00:21:41,360 --> 00:21:44,400
something that someone just downloaded

685
00:21:42,559 --> 00:21:46,960
online?

686
00:21:44,400 --> 00:21:48,400
All right. Uh, the last bit I want to do

687
00:21:46,960 --> 00:21:50,640
is talk a little bit about some of the

688
00:21:48,400 --> 00:21:52,799
emerging threats. Uh, sorry, I missed a

689
00:21:50,640 --> 00:21:55,679
title there. How many of you have heard

690
00:21:52,799 --> 00:21:55,679
of shadow

691
00:21:56,679 --> 00:22:01,840
AI? It's a fancy word with a cool logo

692
00:21:59,840 --> 00:22:04,400
that I found from some random website.

693
00:22:01,840 --> 00:22:06,880
Uh, it's just people in your company

694
00:22:04,400 --> 00:22:08,720
using AI improperly. Maybe you're

695
00:22:06,880 --> 00:22:10,159
inducing vulnerabilities in code. Maybe

696
00:22:08,720 --> 00:22:11,760
you're an insider threat. Remember, an

697
00:22:10,159 --> 00:22:13,679
insider threat doesn't have to be

698
00:22:11,760 --> 00:22:15,120
witting. they might just be a bad coder

699
00:22:13,679 --> 00:22:17,600
or they might be doing things against

700
00:22:15,120 --> 00:22:19,679
company policy. There's a lot of ways we

701
00:22:17,600 --> 00:22:20,880
can tell people are misusing IT systems.

702
00:22:19,679 --> 00:22:23,200
There's not a lot of ways we can tell

703
00:22:20,880 --> 00:22:24,720
people are misusing AI systems. So, as

704
00:22:23,200 --> 00:22:26,320
you're enabling, say software

705
00:22:24,720 --> 00:22:27,679
development to become much more

706
00:22:26,320 --> 00:22:29,200
automated. What are some of those

707
00:22:27,679 --> 00:22:30,880
guardrails? What are some things you can

708
00:22:29,200 --> 00:22:32,480
do to make sure you're not losing IP,

709
00:22:30,880 --> 00:22:33,520
that someone didn't accidentally put in

710
00:22:32,480 --> 00:22:36,000
something that leaks your entire

711
00:22:33,520 --> 00:22:38,080
codebase or induces a vulnerability you

712
00:22:36,000 --> 00:22:40,799
don't know about?

713
00:22:38,080 --> 00:22:42,640
Uh there's also we've talked about this

714
00:22:40,799 --> 00:22:45,120
a little bit uh lots of work happening

715
00:22:42,640 --> 00:22:47,520
right now to try to evaluate how well

716
00:22:45,120 --> 00:22:49,600
these things are generating secure code.

717
00:22:47,520 --> 00:22:51,760
Uh in particular I take the approach

718
00:22:49,600 --> 00:22:53,760
that if I would let an undergrad build

719
00:22:51,760 --> 00:22:55,280
it and put it into my system what level

720
00:22:53,760 --> 00:22:57,200
of vetting would I have done to make

721
00:22:55,280 --> 00:22:59,039
sure I trust it. Do the same thing with

722
00:22:57,200 --> 00:23:00,480
LLMs. We already have good practices for

723
00:22:59,039 --> 00:23:03,360
working people that are getting up to

724
00:23:00,480 --> 00:23:05,840
speed in uh programming. So just don't

725
00:23:03,360 --> 00:23:07,280
trust it to be great. That's fine. Build

726
00:23:05,840 --> 00:23:08,880
some systems of verified. We heard about

727
00:23:07,280 --> 00:23:10,080
some of the formal methods earlier, but

728
00:23:08,880 --> 00:23:11,840
you can think about the normal unit

729
00:23:10,080 --> 00:23:13,280
tests, the normal things that we do as

730
00:23:11,840 --> 00:23:14,480
well as things like fuzzing to try to

731
00:23:13,280 --> 00:23:17,600
find some of these vulnerabilities

732
00:23:14,480 --> 00:23:19,520
before they hit your system. And then uh

733
00:23:17,600 --> 00:23:21,600
lastly, I do in particular want to call

734
00:23:19,520 --> 00:23:22,720
out open AI uh for sharing a lot of this

735
00:23:21,600 --> 00:23:24,480
thread information with the broader

736
00:23:22,720 --> 00:23:26,000
community, watching as things are

737
00:23:24,480 --> 00:23:27,679
emerging. Uh what are some of those risk

738
00:23:26,000 --> 00:23:30,159
and vulnerabilities and how do you

739
00:23:27,679 --> 00:23:32,559
actually provide fixes? If they said

740
00:23:30,159 --> 00:23:34,799
nation state X is attacking all of our

741
00:23:32,559 --> 00:23:36,720
systems with this capability that it's

742
00:23:34,799 --> 00:23:38,799
you know generating code for crypto,

743
00:23:36,720 --> 00:23:40,880
right? Pick your thing. How do we fix

744
00:23:38,799 --> 00:23:42,640
that? What are some ways to put that in

745
00:23:40,880 --> 00:23:44,799
disparit AI systems and disparit

746
00:23:42,640 --> 00:23:46,320
software stacks? That's really tough.

747
00:23:44,799 --> 00:23:47,679
So, uh, take a look at what some of the

748
00:23:46,320 --> 00:23:48,960
frontier models are doing now and how

749
00:23:47,679 --> 00:23:50,799
they're sharing information. And I

750
00:23:48,960 --> 00:23:52,480
encourage you, uh, to not make the same

751
00:23:50,799 --> 00:23:54,640
mistake we did in cyber, which is when

752
00:23:52,480 --> 00:23:55,840
you see risks, just hide it to yourself

753
00:23:54,640 --> 00:23:58,480
because you don't want to share all of

754
00:23:55,840 --> 00:24:00,320
your your sort of bad news. All right,

755
00:23:58,480 --> 00:24:01,679
same thing here. Lots of stuff going on

756
00:24:00,320 --> 00:24:03,640
in this space. I want you to think

757
00:24:01,679 --> 00:24:05,679
through them all. Go read them at your

758
00:24:03,640 --> 00:24:07,360
leisure. Because we have five minutes

759
00:24:05,679 --> 00:24:09,360
left, I want to take some questions. I

760
00:24:07,360 --> 00:24:11,919
know that was a lot. Uh, hopefully we're

761
00:24:09,360 --> 00:24:14,720
a mix of uh, seeing risk as well as

762
00:24:11,919 --> 00:24:17,279
opportunities. Hey, I was wondering

763
00:24:14,720 --> 00:24:20,000
where do you see the most attacks coming

764
00:24:17,279 --> 00:24:22,000
um aimed at AI systems? Is it actually

765
00:24:20,000 --> 00:24:24,480
where the processing is happening or is

766
00:24:22,000 --> 00:24:27,440
it where systems are connecting to AI to

767
00:24:24,480 --> 00:24:29,039
get answers? So right now uh many of the

768
00:24:27,440 --> 00:24:30,640
attacks are actually just like say for

769
00:24:29,039 --> 00:24:32,480
LLMs at the prompt level. What are some

770
00:24:30,640 --> 00:24:34,320
ways to jailbreak to extract information

771
00:24:32,480 --> 00:24:36,080
that we don't care about? Uh but we're

772
00:24:34,320 --> 00:24:38,000
seeing sophisticated thread actors start

773
00:24:36,080 --> 00:24:39,520
to go against software stacks, right?

774
00:24:38,000 --> 00:24:41,360
You take the low hanging fruit first. If

775
00:24:39,520 --> 00:24:43,520
I can make open AAI give me my training

776
00:24:41,360 --> 00:24:45,279
data that was in a very special uh

777
00:24:43,520 --> 00:24:47,120
protected data I care about just do

778
00:24:45,279 --> 00:24:48,480
that. But as those things become more

779
00:24:47,120 --> 00:24:49,679
defensive cat and mouse they're going to

780
00:24:48,480 --> 00:24:51,120
go after software stack. You're going to

781
00:24:49,679 --> 00:24:52,559
go all the way down the processor level

782
00:24:51,120 --> 00:24:55,120
extract the information you care about.

783
00:24:52,559 --> 00:24:57,039
So right now think about open APIs

784
00:24:55,120 --> 00:24:58,720
things that are touching AI systems

785
00:24:57,039 --> 00:25:00,320
defend those secure those write good

786
00:24:58,720 --> 00:25:02,880
code around them but the whole stack is

787
00:25:00,320 --> 00:25:05,120
going to be vulnerable. Um prompts can

788
00:25:02,880 --> 00:25:08,159
be very valuable to train the AI models.

789
00:25:05,120 --> 00:25:11,039
How do we protect our um prompts that we

790
00:25:08,159 --> 00:25:13,360
feed into the system to

791
00:25:11,039 --> 00:25:16,000
Yeah. Uh I that's a really good question

792
00:25:13,360 --> 00:25:17,520
because you might share more information

793
00:25:16,000 --> 00:25:19,360
than you expect by just what you're

794
00:25:17,520 --> 00:25:21,039
asking of these models. I'm a big fan of

795
00:25:19,360 --> 00:25:23,360
crypto. What are some ways that we can

796
00:25:21,039 --> 00:25:25,039
do things like multi-party computation

797
00:25:23,360 --> 00:25:25,919
uh share you know just encrypt

798
00:25:25,039 --> 00:25:28,400
everything. How do you have

799
00:25:25,919 --> 00:25:30,159
cryptographically secure channels that

800
00:25:28,400 --> 00:25:31,360
no one system knows all the pieces of

801
00:25:30,159 --> 00:25:32,960
the thing you ask but you can still

802
00:25:31,360 --> 00:25:35,039
mathematically show you can answer the

803
00:25:32,960 --> 00:25:37,760
question. uh there's some work in NPC

804
00:25:35,039 --> 00:25:39,919
probably adds 15 20% overhead which is a

805
00:25:37,760 --> 00:25:41,279
lot but if you really have a sensitive

806
00:25:39,919 --> 00:25:43,039
application that might be some way to

807
00:25:41,279 --> 00:25:45,279
help protect that information uh

808
00:25:43,039 --> 00:25:47,679
otherwise normal cyber security posture

809
00:25:45,279 --> 00:25:48,799
applies right keep those queries safe uh

810
00:25:47,679 --> 00:25:51,360
make sure they're stored inside your

811
00:25:48,799 --> 00:25:55,400
enclave have some pretty good uh normal

812
00:25:51,360 --> 00:25:55,400
practices for securing them

813
00:25:55,679 --> 00:25:59,200
is there any guideline around when we

814
00:25:57,760 --> 00:26:01,600
work with the third party but they are

815
00:25:59,200 --> 00:26:04,320
using AI and when we are interacting

816
00:26:01,600 --> 00:26:06,880
with them how do we ensure like you know

817
00:26:04,320 --> 00:26:08,960
what kind of you know safety practices

818
00:26:06,880 --> 00:26:12,000
there like is there any guideline these

819
00:26:08,960 --> 00:26:13,679
are the things we must do yeah I don't

820
00:26:12,000 --> 00:26:16,240
ever trust anyone maybe it's just a

821
00:26:13,679 --> 00:26:18,320
consequence of my job so um I'm okay

822
00:26:16,240 --> 00:26:20,880
with using things from untrusted sources

823
00:26:18,320 --> 00:26:22,159
so having some best practices for what

824
00:26:20,880 --> 00:26:23,840
are those unit tests what are the things

825
00:26:22,159 --> 00:26:26,720
that are critical for you what are the

826
00:26:23,840 --> 00:26:28,799
kind of uh outputs you expect put in

827
00:26:26,720 --> 00:26:31,200
those firm guard rails if you see things

828
00:26:28,799 --> 00:26:32,799
pop out that are unexpected don't run

829
00:26:31,200 --> 00:26:35,120
them for example if

830
00:26:32,799 --> 00:26:36,799
uh eliciting code from these models. Uh

831
00:26:35,120 --> 00:26:38,000
but generally if you just take a zero

832
00:26:36,799 --> 00:26:39,360
trust approach, I think that's probably

833
00:26:38,000 --> 00:26:41,039
the safest bet because we should work

834
00:26:39,360 --> 00:26:43,200
with untrusted partners. That's great,

835
00:26:41,039 --> 00:26:44,720
but let's just not even if it's not

836
00:26:43,200 --> 00:26:47,360
malicious, let's not assume that things

837
00:26:44,720 --> 00:26:51,679
won't have bad effects in our systems.

838
00:26:47,360 --> 00:26:53,679
One more question. Okay. Oh, maybe two.

839
00:26:51,679 --> 00:26:55,120
No. Can you talk about quickly about um

840
00:26:53,679 --> 00:26:57,120
containment? So, if there's an

841
00:26:55,120 --> 00:26:58,720
exploitation, how do you can you know

842
00:26:57,120 --> 00:27:00,320
have things change and how do you

843
00:26:58,720 --> 00:27:02,159
contain it and shut down systems in a

844
00:27:00,320 --> 00:27:04,880
way that's stable? Yeah, I'm a very big

845
00:27:02,159 --> 00:27:06,799
fan of maintaining uh fine-tuned models

846
00:27:04,880 --> 00:27:08,880
for your business use case that might be

847
00:27:06,799 --> 00:27:10,559
kind of crappy as in you train them on

848
00:27:08,880 --> 00:27:12,159
gold standard data that you trust, you

849
00:27:10,559 --> 00:27:13,760
believe is not exploited. Of course, it

850
00:27:12,159 --> 00:27:15,120
might be that you can roll back to. So

851
00:27:13,760 --> 00:27:17,200
maybe you're using the frontiers, maybe

852
00:27:15,120 --> 00:27:18,799
you're using other systems, but you have

853
00:27:17,200 --> 00:27:20,559
a way to say whoop something's happening

854
00:27:18,799 --> 00:27:22,159
here. Let's jump back to an old version.

855
00:27:20,559 --> 00:27:24,080
Let's jump back to a trusted uh version

856
00:27:22,159 --> 00:27:25,840
or a gold version uh to be able to push

857
00:27:24,080 --> 00:27:27,039
that forward. Uh in terms of putting in

858
00:27:25,840 --> 00:27:28,480
guardrails, that's really tough right

859
00:27:27,039 --> 00:27:30,640
now. I don't think there's a way to do

860
00:27:28,480 --> 00:27:31,600
it that definitely gets rid of the

861
00:27:30,640 --> 00:27:33,679
exploits because these are

862
00:27:31,600 --> 00:27:35,120
nondeterministic systems. Uh but usually

863
00:27:33,679 --> 00:27:36,640
if you have older models you can roll

864
00:27:35,120 --> 00:27:38,480
back to and it you might have to keep

865
00:27:36,640 --> 00:27:41,360
going if that risk persists but that's

866
00:27:38,480 --> 00:27:44,000
one good standard way.

867
00:27:41,360 --> 00:27:46,640
So we build a lot of u application by

868
00:27:44,000 --> 00:27:49,679
using open source and now we see the CVS

869
00:27:46,640 --> 00:27:52,720
also available and as as you mentioned

870
00:27:49,679 --> 00:27:55,360
now uh with the help of LLM uh people

871
00:27:52,720 --> 00:27:57,520
are probably easily can create the

872
00:27:55,360 --> 00:28:01,600
payloads. So what is the trend are you

873
00:27:57,520 --> 00:28:04,240
seeing in um uh what you say increasing

874
00:28:01,600 --> 00:28:07,279
amount of payload for CVS is there any

875
00:28:04,240 --> 00:28:09,919
trend which is like uh payload means it

876
00:28:07,279 --> 00:28:12,240
is becoming easier or yeah I do think

877
00:28:09,919 --> 00:28:14,559
it's becoming easier uh but it as you've

878
00:28:12,240 --> 00:28:17,520
heard earlier it's not necessarily the

879
00:28:14,559 --> 00:28:18,880
most novel ways of doing it. Uh so

880
00:28:17,520 --> 00:28:20,240
you're going to get the strip kitties

881
00:28:18,880 --> 00:28:22,159
you're going to get advanced actors who

882
00:28:20,240 --> 00:28:24,240
can do what they already could do faster

883
00:28:22,159 --> 00:28:25,679
which isn't great but defenders can also

884
00:28:24,240 --> 00:28:27,679
do the same thing. What you're not

885
00:28:25,679 --> 00:28:29,760
seeing is a lot of eliciting of really

886
00:28:27,679 --> 00:28:31,360
interesting, difficult zero days that no

887
00:28:29,760 --> 00:28:33,679
one else would have thought of before.

888
00:28:31,360 --> 00:28:35,279
Uh so I I think it's kind of the the

889
00:28:33,679 --> 00:28:36,640
normal cat and mouse. It's happening

890
00:28:35,279 --> 00:28:39,039
faster, but we can also find them and

891
00:28:36,640 --> 00:28:40,640
patch them faster, too. So maybe I'm

892
00:28:39,039 --> 00:28:42,520
slightly optimistic there, which good

893
00:28:40,640 --> 00:28:44,679
note to end

894
00:28:42,520 --> 00:28:48,919
on. All right. Okay. Let's hear it for

895
00:28:44,679 --> 00:28:48,919
Dennis. Thank you.

