1
00:00:20,800 --> 00:00:26,840
okay hello everyone thank you all for

2
00:00:22,720 --> 00:00:28,840
joining the last seminar of spring term

3
00:00:26,840 --> 00:00:31,920
uh it's a great pleasure to have

4
00:00:28,840 --> 00:00:34,040
Professor Peter MCM here with us today

5
00:00:31,920 --> 00:00:35,480
uh Peter is an assistant professor in

6
00:00:34,040 --> 00:00:37,360
the department of applied and

7
00:00:35,480 --> 00:00:39,559
Engineering physics at Cornell

8
00:00:37,360 --> 00:00:42,559
University and he's been there since

9
00:00:39,559 --> 00:00:45,199
2019 prior to that he completed his PhD

10
00:00:42,559 --> 00:00:47,640
and post-doctoral training at Stanford

11
00:00:45,199 --> 00:00:50,039
University uh his research is focused on

12
00:00:47,640 --> 00:00:52,800
Computing with physical systems both in

13
00:00:50,039 --> 00:00:54,520
classical and Quantum domain and uh he

14
00:00:52,800 --> 00:00:56,199
has been recipient of many awards

15
00:00:54,520 --> 00:00:58,760
including the Packard and Sloan

16
00:00:56,199 --> 00:01:00,960
fellowships Office of Naval Research

17
00:00:58,760 --> 00:01:04,119
young investigator program award Google

18
00:01:00,960 --> 00:01:05,920
Quantum research award he's also a sear

19
00:01:04,119 --> 00:01:08,320
Israeli Global scholar in Quantum

20
00:01:05,920 --> 00:01:10,360
information science uh so it's a great

21
00:01:08,320 --> 00:01:11,960
pleasure to have you here Peter today

22
00:01:10,360 --> 00:01:14,360
very excited to learn more about your

23
00:01:11,960 --> 00:01:16,560
work before that I just wanted to remind

24
00:01:14,360 --> 00:01:18,119
everyone if you're in the room feel free

25
00:01:16,560 --> 00:01:20,560
to ask your questions throughout the

26
00:01:18,119 --> 00:01:22,560
talk that's what Peter prefers H for

27
00:01:20,560 --> 00:01:24,320
whoever is listening to us on Zoom

28
00:01:22,560 --> 00:01:25,640
please type your questions and at the

29
00:01:24,320 --> 00:01:27,640
end of the talk I'm going to help

30
00:01:25,640 --> 00:01:28,960
moderate the questions for you so with

31
00:01:27,640 --> 00:01:33,880
that thank you for joining and we're

32
00:01:28,960 --> 00:01:33,880
excited to hear your talk awesome thank

33
00:01:35,799 --> 00:01:39,759
you uh so thank you very much for the

34
00:01:38,000 --> 00:01:41,320
kind introduction and thank you for the

35
00:01:39,759 --> 00:01:42,680
invitation to speak it's a it's a great

36
00:01:41,320 --> 00:01:45,200
pleasure to be here and it's been a

37
00:01:42,680 --> 00:01:47,320
while since I gave a talk uh and it's

38
00:01:45,200 --> 00:01:50,680
always wonderful to uh to come

39
00:01:47,320 --> 00:01:54,280
visit so uh let me let me start my timer

40
00:01:50,680 --> 00:01:57,320
to make sure that we uh end this on time

41
00:01:54,280 --> 00:02:00,560
so uh my grand plan is to tell you today

42
00:01:57,320 --> 00:02:03,360
about Computing with physical systems um

43
00:02:00,560 --> 00:02:05,159
so I'm going to start off by describing

44
00:02:03,360 --> 00:02:06,640
uh some challenges in modern Computing

45
00:02:05,159 --> 00:02:08,160
that I think could be addressed by

46
00:02:06,640 --> 00:02:09,920
building what we think of as Computing

47
00:02:08,160 --> 00:02:13,000
with physical systems or physics based

48
00:02:09,920 --> 00:02:14,800
or physics inspired Computing and I'll

49
00:02:13,000 --> 00:02:16,560
describe a bit about how that what we

50
00:02:14,800 --> 00:02:18,640
mean by that differs from conventional

51
00:02:16,560 --> 00:02:22,080
Computing and how we could benefit from

52
00:02:18,640 --> 00:02:23,760
uh Computing in this new way uh I'll

53
00:02:22,080 --> 00:02:26,080
then talk about what we call physical

54
00:02:23,760 --> 00:02:27,959
neural networks or how we can use almost

55
00:02:26,080 --> 00:02:30,959
arbitrary physical systems to perform

56
00:02:27,959 --> 00:02:33,920
neural network processing for us

57
00:02:30,959 --> 00:02:35,560
and uh then I will describe an onip

58
00:02:33,920 --> 00:02:36,760
photonic neural network platform that

59
00:02:35,560 --> 00:02:39,239
we've recently

60
00:02:36,760 --> 00:02:42,040
developed uh that is an example of a

61
00:02:39,239 --> 00:02:45,120
physical neural network and uh this will

62
00:02:42,040 --> 00:02:47,480
be towards sort of how can we scale uh

63
00:02:45,120 --> 00:02:50,120
these physics based machines to larger

64
00:02:47,480 --> 00:02:52,599
and larger neural network sizes and then

65
00:02:50,120 --> 00:02:55,159
finally I'll end with some commentary on

66
00:02:52,599 --> 00:02:58,360
uh prospects or speculations about uh

67
00:02:55,159 --> 00:03:00,239
where physics Computing might go um and

68
00:02:58,360 --> 00:03:02,000
especially since this was a nano seminar

69
00:03:00,239 --> 00:03:03,640
I felt almost obliged to include some

70
00:03:02,000 --> 00:03:05,319
Nano Electronics in this so we we

71
00:03:03,640 --> 00:03:06,959
haven't actually don't have any results

72
00:03:05,319 --> 00:03:09,480
to talk about in Nano Electronics but I

73
00:03:06,959 --> 00:03:10,720
thought I'd uh end with some description

74
00:03:09,480 --> 00:03:13,360
of sort of how some of the things I

75
00:03:10,720 --> 00:03:14,959
talked about might be applied in uh in

76
00:03:13,360 --> 00:03:17,040
the sort of either CS or other

77
00:03:14,959 --> 00:03:19,319
conventional Nano electronic

78
00:03:17,040 --> 00:03:21,319
Realms so first of all let's begin with

79
00:03:19,319 --> 00:03:23,319
uh why do we even want to build

80
00:03:21,319 --> 00:03:25,239
computers that are faster or more energy

81
00:03:23,319 --> 00:03:27,159
efficient uh maybe to this audience

82
00:03:25,239 --> 00:03:29,080
that's completely obvious so I'll I'll

83
00:03:27,159 --> 00:03:31,040
go relatively quickly through this but a

84
00:03:29,080 --> 00:03:33,360
large motivation for us has been the the

85
00:03:31,040 --> 00:03:35,439
growth in deep learning in neural

86
00:03:33,360 --> 00:03:39,040
networks you can see in this uh this

87
00:03:35,439 --> 00:03:40,799
chart here uh the size of uh uh or the

88
00:03:39,040 --> 00:03:42,239
compute cost required to train various

89
00:03:40,799 --> 00:03:44,480
machine learning models spanning the

90
00:03:42,239 --> 00:03:46,360
last several decades and kind of if you

91
00:03:44,480 --> 00:03:49,120
look in the last decade you can see this

92
00:03:46,360 --> 00:03:53,519
really massive increase over many orders

93
00:03:49,120 --> 00:03:55,200
of magnitude um and part of the the

94
00:03:53,519 --> 00:03:57,360
reason that this has happened is this

95
00:03:55,200 --> 00:03:59,000
discovery that as you scale uh certain

96
00:03:57,360 --> 00:04:01,400
neural network architectures the more

97
00:03:59,000 --> 00:04:03,319
parameters you the more compute you add

98
00:04:01,400 --> 00:04:05,000
the more capable these models become so

99
00:04:03,319 --> 00:04:06,360
there's a strong sort of desire to make

100
00:04:05,000 --> 00:04:09,439
the models bigger and

101
00:04:06,360 --> 00:04:11,040
bigger uh but it creates an a pretty

102
00:04:09,439 --> 00:04:13,319
obvious problem which is if we sort of

103
00:04:11,040 --> 00:04:16,479
zoom into the last decade and we look at

104
00:04:13,319 --> 00:04:19,199
this nice plot of in Blue uh these

105
00:04:16,479 --> 00:04:21,440
neural network models and how they've

106
00:04:19,199 --> 00:04:24,080
been their compute demands have been

107
00:04:21,440 --> 00:04:27,000
increasing with time versus the Moes law

108
00:04:24,080 --> 00:04:28,280
scaling in Orange of uh standard

109
00:04:27,000 --> 00:04:30,560
conventional

110
00:04:28,280 --> 00:04:33,400
processes uh there's clearly a major

111
00:04:30,560 --> 00:04:34,840
issue uh that the models are growing or

112
00:04:33,400 --> 00:04:36,639
the compute demands are growing much

113
00:04:34,840 --> 00:04:38,880
more rapidly than our computers have

114
00:04:36,639 --> 00:04:40,960
been growing and you can make

115
00:04:38,880 --> 00:04:42,600
specialized chips that are designed

116
00:04:40,960 --> 00:04:45,800
purely for neural network processing and

117
00:04:42,600 --> 00:04:48,120
get some benefit from doing that uh this

118
00:04:45,800 --> 00:04:49,320
essentially jumps the line up by a bit

119
00:04:48,120 --> 00:04:51,800
but it doesn't really change the

120
00:04:49,320 --> 00:04:54,680
fundamental Mo La scaling underneath it

121
00:04:51,800 --> 00:04:57,680
uh so that is good to have it's good to

122
00:04:54,680 --> 00:05:00,039
do uh but it's not

123
00:04:57,680 --> 00:05:02,880
sufficient another strong motivation for

124
00:05:00,039 --> 00:05:05,240
us has been uh thinking about sensing

125
00:05:02,880 --> 00:05:08,199
that there are sensors all around us and

126
00:05:05,240 --> 00:05:10,400
sensor bandwidths keep increasing um so

127
00:05:08,199 --> 00:05:14,000
we have even Now consumer electronic

128
00:05:10,400 --> 00:05:16,360
devices with 200 megapixel cameras um

129
00:05:14,000 --> 00:05:19,039
and in this uh this example shown on the

130
00:05:16,360 --> 00:05:22,520
on the left here if you think about uh

131
00:05:19,039 --> 00:05:24,840
some kind of Medi pixel sensor that is

132
00:05:22,520 --> 00:05:27,160
digitizing data at sort of maybe 50

133
00:05:24,840 --> 00:05:29,440
gigas samples a second uh in some

134
00:05:27,160 --> 00:05:31,840
defense application even if you already

135
00:05:29,440 --> 00:05:33,639
have 512 pixels of this the amount of

136
00:05:31,840 --> 00:05:37,440
data that's coming out of this kind of

137
00:05:33,639 --> 00:05:39,800
sensor is enormous 500 terabits a second

138
00:05:37,440 --> 00:05:41,360
uh so storing that data is clearly a

139
00:05:39,800 --> 00:05:43,880
challenge and not a great solution to

140
00:05:41,360 --> 00:05:45,440
the the Deluge problem uh but also

141
00:05:43,880 --> 00:05:46,960
processing it in real time is also

142
00:05:45,440 --> 00:05:48,880
really

143
00:05:46,960 --> 00:05:51,759
challenging and something that appeared

144
00:05:48,880 --> 00:05:53,319
in a relatively recent SRC report is

145
00:05:51,759 --> 00:05:56,319
this notion that maybe one of the ways

146
00:05:53,319 --> 00:05:58,720
to solve the sensing challenge will be

147
00:05:56,319 --> 00:06:00,560
to perform analog pre-processing so

148
00:05:58,720 --> 00:06:02,240
before we digitize

149
00:06:00,560 --> 00:06:04,880
this really high bandwidth signal that's

150
00:06:02,240 --> 00:06:06,240
coming into our sensor uh maybe we

151
00:06:04,880 --> 00:06:07,800
should perform some analog

152
00:06:06,240 --> 00:06:09,319
pre-processing that extracts the

153
00:06:07,800 --> 00:06:11,400
relevant features for us this is

154
00:06:09,319 --> 00:06:12,800
something that's really inspired by uh

155
00:06:11,400 --> 00:06:14,960
by Neuroscience and even the human

156
00:06:12,800 --> 00:06:17,080
visual system where there's really high

157
00:06:14,960 --> 00:06:19,240
bandwidth data coming into my eyes but

158
00:06:17,080 --> 00:06:21,479
in order for me to know that uh the sort

159
00:06:19,240 --> 00:06:23,080
of the remote control is here and my

160
00:06:21,479 --> 00:06:25,039
mouse is here and so on I don't need the

161
00:06:23,080 --> 00:06:27,720
full sort of super high megapixel

162
00:06:25,039 --> 00:06:29,120
resolution image uh and so our brains

163
00:06:27,720 --> 00:06:31,039
actually do a a whole bunch of

164
00:06:29,120 --> 00:06:32,560
processing before the information

165
00:06:31,039 --> 00:06:34,800
ultimately gets to the part where we

166
00:06:32,560 --> 00:06:34,800
make

167
00:06:35,520 --> 00:06:41,599
decisions so okay we would like uh more

168
00:06:39,880 --> 00:06:43,560
speed or Energy Efficiency for both

169
00:06:41,599 --> 00:06:46,479
these reasons uh what are some options

170
00:06:43,560 --> 00:06:48,599
we have to achieve that so very

171
00:06:46,479 --> 00:06:50,400
generically there's essentially four

172
00:06:48,599 --> 00:06:53,919
strategies that I'll talk about that you

173
00:06:50,400 --> 00:06:55,639
can try to use to make computers more uh

174
00:06:53,919 --> 00:06:59,199
have higher throughput or greater Energy

175
00:06:55,639 --> 00:07:01,639
Efficiency so the first is that you can

176
00:06:59,199 --> 00:07:05,599
elim abstractions so Computing is built

177
00:07:01,639 --> 00:07:06,960
on a a great system of abstractions uh

178
00:07:05,599 --> 00:07:08,440
which is something we all learn about in

179
00:07:06,960 --> 00:07:11,400
undergraduate electrical and computer

180
00:07:08,440 --> 00:07:13,879
engineering degree um but it has a major

181
00:07:11,400 --> 00:07:15,360
challenge that now there's uh so sort of

182
00:07:13,879 --> 00:07:17,919
so many layers of this that you've

183
00:07:15,360 --> 00:07:19,599
introduced inefficiency at each layer so

184
00:07:17,919 --> 00:07:21,400
now you could imagine stripping some of

185
00:07:19,599 --> 00:07:23,160
these away for example we rely an

186
00:07:21,400 --> 00:07:25,199
abstraction of digital logic even though

187
00:07:23,160 --> 00:07:28,080
it's built fundamentally on physical

188
00:07:25,199 --> 00:07:29,840
devices that are analog U we assume that

189
00:07:28,080 --> 00:07:31,919
signals are deterministic and always

190
00:07:29,840 --> 00:07:35,039
correct uh you could imagine going

191
00:07:31,919 --> 00:07:37,080
towards analog or probabilistic

192
00:07:35,039 --> 00:07:39,400
computation and really part of this

193
00:07:37,080 --> 00:07:41,479
philosophy is going to be designing

194
00:07:39,400 --> 00:07:43,120
behavior from the bottom up so you start

195
00:07:41,479 --> 00:07:45,319
with your physical substrate and what it

196
00:07:43,120 --> 00:07:47,440
naturally does and build up from there

197
00:07:45,319 --> 00:07:48,919
rather than the conventional approach of

198
00:07:47,440 --> 00:07:50,360
going from the top down and I'll have

199
00:07:48,919 --> 00:07:52,879
another slide on this later to give a

200
00:07:50,360 --> 00:07:55,360
sort of even crisper picture of what uh

201
00:07:52,879 --> 00:07:57,599
what this is really all about a second

202
00:07:55,360 --> 00:08:00,120
category is that a major challenge in

203
00:07:57,599 --> 00:08:01,919
compute systems is that your your memory

204
00:08:00,120 --> 00:08:04,240
is often separated from your processor

205
00:08:01,919 --> 00:08:05,960
and there's a bottleneck and bandwidth

206
00:08:04,240 --> 00:08:08,400
between the two of them sometimes called

207
00:08:05,960 --> 00:08:11,280
the vyan bottleneck and that you can try

208
00:08:08,400 --> 00:08:13,919
to address this by uh Computing in

209
00:08:11,280 --> 00:08:15,039
memory so merging the memory and compute

210
00:08:13,919 --> 00:08:17,080
this is something that people of course

211
00:08:15,039 --> 00:08:19,440
are ready do a lot of in conventional

212
00:08:17,080 --> 00:08:21,120
digital and analog electronic Computing

213
00:08:19,440 --> 00:08:23,479
uh but it's a general I think principle

214
00:08:21,120 --> 00:08:26,080
that you can also apply in uh in physics

215
00:08:23,479 --> 00:08:27,080
based Computing uh the third option you

216
00:08:26,080 --> 00:08:29,720
have is something that I think

217
00:08:27,080 --> 00:08:30,960
especially in the the DAT seminar series

218
00:08:29,720 --> 00:08:32,880
uh many people in the audience are very

219
00:08:30,960 --> 00:08:34,800
well familiar with is that you could try

220
00:08:32,880 --> 00:08:37,080
to go beyond seos you could use

221
00:08:34,800 --> 00:08:39,120
alternative Hardware Hardware physical

222
00:08:37,080 --> 00:08:41,080
substrates or even modalities for

223
00:08:39,120 --> 00:08:42,839
example Quantum Computing is something

224
00:08:41,080 --> 00:08:45,160
that you could try to apply in at least

225
00:08:42,839 --> 00:08:47,320
very specific

226
00:08:45,160 --> 00:08:50,240
applications then finally a fourth

227
00:08:47,320 --> 00:08:51,480
pillar which is has for a long time been

228
00:08:50,240 --> 00:08:53,480
something that sort of people have been

229
00:08:51,480 --> 00:08:55,600
really resistant to but is now maybe

230
00:08:53,480 --> 00:08:57,000
something we were forced to to think

231
00:08:55,600 --> 00:08:59,120
about taking seriously and whether we

232
00:08:57,000 --> 00:09:02,160
should really do it is being tolerant to

233
00:08:59,120 --> 00:09:03,880
VAR ations instead of insisting that

234
00:09:02,160 --> 00:09:05,519
every copy that comes out of our Fab

235
00:09:03,880 --> 00:09:07,240
Works in exactly the same way for the

236
00:09:05,519 --> 00:09:10,640
level of works the same way we mean in

237
00:09:07,240 --> 00:09:12,000
our abstraction hierarchy uh maybe we

238
00:09:10,640 --> 00:09:13,800
shouldn't insist on that anymore

239
00:09:12,000 --> 00:09:15,399
certainly it's an option you have if you

240
00:09:13,800 --> 00:09:17,839
would like to get the most out of the

241
00:09:15,399 --> 00:09:21,160
physical device you have if you allow

242
00:09:17,839 --> 00:09:23,600
each copy to vary uh you can uh you can

243
00:09:21,160 --> 00:09:25,920
push the performance and so this may

244
00:09:23,600 --> 00:09:28,079
require some tolerating some amount of

245
00:09:25,920 --> 00:09:30,560
train retraining or reconfiguration of

246
00:09:28,079 --> 00:09:32,519
the device after its fabric a and many

247
00:09:30,560 --> 00:09:37,079
people have been thinking about this uh

248
00:09:32,519 --> 00:09:38,440
Jeff Hinton had a a a talk at a Europe a

249
00:09:37,079 --> 00:09:40,079
year at a bit ago where he described

250
00:09:38,440 --> 00:09:41,480
this as mortal Computing that each piece

251
00:09:40,079 --> 00:09:43,360
of Hardware will sort of fundamentally

252
00:09:41,480 --> 00:09:46,560
live its own life as you have to as you

253
00:09:43,360 --> 00:09:49,800
have to retrain it U so those are those

254
00:09:46,560 --> 00:09:52,040
are some options you have let's now talk

255
00:09:49,800 --> 00:09:53,120
a little bit about this uh business of

256
00:09:52,040 --> 00:09:55,680
eliminating

257
00:09:53,120 --> 00:09:57,800
abstractions um so on the left here we

258
00:09:55,680 --> 00:09:59,399
see this uh standard stack of

259
00:09:57,800 --> 00:10:00,640
abstractions that you learn about

260
00:09:59,399 --> 00:10:03,839
throughout an e

261
00:10:00,640 --> 00:10:05,800
degree uh starting from uh depending on

262
00:10:03,839 --> 00:10:07,680
where you like the camos transistors

263
00:10:05,800 --> 00:10:09,360
building all the way up through

264
00:10:07,680 --> 00:10:11,040
insisting that they output digital logic

265
00:10:09,360 --> 00:10:12,839
digital signals you have logic gates you

266
00:10:11,040 --> 00:10:14,760
can construct adders and multipliers and

267
00:10:12,839 --> 00:10:16,720
so on from that building all the way up

268
00:10:14,760 --> 00:10:19,120
through to user

269
00:10:16,720 --> 00:10:21,360
applications and a beautiful thing about

270
00:10:19,120 --> 00:10:22,760
this abstraction hierarchy is that if

271
00:10:21,360 --> 00:10:25,320
you're working at any one of these

272
00:10:22,760 --> 00:10:27,040
levels you don't need to know in great

273
00:10:25,320 --> 00:10:29,680
detail exactly what's happening above or

274
00:10:27,040 --> 00:10:30,880
you below you but the price you pay for

275
00:10:29,680 --> 00:10:32,760
this is that now each of these

276
00:10:30,880 --> 00:10:35,639
abstractions is assumed to be extremely

277
00:10:32,760 --> 00:10:37,880
tight and you can't just decide oh well

278
00:10:35,639 --> 00:10:39,800
I'll make my my add only sort of add or

279
00:10:37,880 --> 00:10:41,440
add some of the time uh that's that's

280
00:10:39,800 --> 00:10:43,360
not an acceptable thing to do in this

281
00:10:41,440 --> 00:10:44,839
framework where people are are relying

282
00:10:43,360 --> 00:10:47,160
on you to do the thing you said

283
00:10:44,839 --> 00:10:49,800
according to the

284
00:10:47,160 --> 00:10:52,639
specification a completely uh different

285
00:10:49,800 --> 00:10:54,399
way of going about things is to do what

286
00:10:52,639 --> 00:10:55,839
one by term physics based Computing

287
00:10:54,399 --> 00:10:57,959
where we start with some physical

288
00:10:55,839 --> 00:11:00,399
Hardware it could be uh unconventional

289
00:10:57,959 --> 00:11:03,839
it could just be camos Hardware

290
00:11:00,399 --> 00:11:05,399
uh and we we treat it as a complex

291
00:11:03,839 --> 00:11:09,320
system that is naturally performing

292
00:11:05,399 --> 00:11:11,800
computations for us and we program it by

293
00:11:09,320 --> 00:11:13,320
deciding on the connectivity between the

294
00:11:11,800 --> 00:11:15,000
components of the hardware or the

295
00:11:13,320 --> 00:11:15,920
parameters of the physical system that

296
00:11:15,000 --> 00:11:18,839
we can

297
00:11:15,920 --> 00:11:20,720
change and so in this setting the

298
00:11:18,839 --> 00:11:23,000
hardware connectivity and the parameters

299
00:11:20,720 --> 00:11:24,720
induce certain dynamics of the hardware

300
00:11:23,000 --> 00:11:26,399
which in turn induce an algorithm so you

301
00:11:24,720 --> 00:11:29,320
don't start at the pop top and say I

302
00:11:26,399 --> 00:11:30,800
want this algorithm and compile down you

303
00:11:29,320 --> 00:11:33,079
start with what the hardware naturally

304
00:11:30,800 --> 00:11:35,560
does and you tweak it until it does the

305
00:11:33,079 --> 00:11:38,399
thing you desire and so programming in

306
00:11:35,560 --> 00:11:40,360
this kind of model is analogous or is

307
00:11:38,399 --> 00:11:42,320
done by really adjusting the tunable

308
00:11:40,360 --> 00:11:43,720
parameters of the system and so here

309
00:11:42,320 --> 00:11:46,399
there's no real separation between the

310
00:11:43,720 --> 00:11:49,760
notion of the program that you run and

311
00:11:46,399 --> 00:11:51,240
the hardware that it's running on and uh

312
00:11:49,760 --> 00:11:52,920
this makes programming more difficult

313
00:11:51,240 --> 00:11:55,399
but what you get in an exchange is that

314
00:11:52,920 --> 00:11:57,639
all the sort of inefficiencies that you

315
00:11:55,399 --> 00:11:58,800
had uh that you were by imposing

316
00:11:57,639 --> 00:12:01,600
constraints at each layer and an

317
00:11:58,800 --> 00:12:05,360
abstraction hierarchy are now all

318
00:12:01,600 --> 00:12:07,920
gone so I wanted to give one example uh

319
00:12:05,360 --> 00:12:09,959
of this kind of physics based Computing

320
00:12:07,920 --> 00:12:13,079
modality that's been explored over the

321
00:12:09,959 --> 00:12:16,240
last decade and that uh including by

322
00:12:13,079 --> 00:12:17,680
people uh at MIT so uh maybe some of you

323
00:12:16,240 --> 00:12:19,079
will have seen this before if you have

324
00:12:17,680 --> 00:12:20,240
great if you haven't you can ignore it

325
00:12:19,079 --> 00:12:21,519
we're not going to use it later in the

326
00:12:20,240 --> 00:12:23,279
talk but I wanted to give at least one

327
00:12:21,519 --> 00:12:24,560
example uh for those of you who have

328
00:12:23,279 --> 00:12:27,040
familiar with things called icing

329
00:12:24,560 --> 00:12:28,800
machines so icing machines are special

330
00:12:27,040 --> 00:12:31,199
purpose pieces of physical Hardware that

331
00:12:28,800 --> 00:12:34,880
have been designed to through their

332
00:12:31,199 --> 00:12:37,320
natural Dynamics heris optim perform uh

333
00:12:34,880 --> 00:12:38,519
combinatorial optimization and an

334
00:12:37,320 --> 00:12:40,199
example of this is something called

335
00:12:38,519 --> 00:12:42,279
oscillator icing machines where people

336
00:12:40,199 --> 00:12:44,880
have proposed connecting nonlinear

337
00:12:42,279 --> 00:12:47,199
oscillators for example could be couple

338
00:12:44,880 --> 00:12:49,399
like a nonlinear capacitor and an

339
00:12:47,199 --> 00:12:51,560
inductor that are coupled for example

340
00:12:49,399 --> 00:12:55,040
resistively shown in this example to

341
00:12:51,560 --> 00:12:56,480
another spin uh and the idea is that you

342
00:12:55,040 --> 00:12:59,720
would design these through their natural

343
00:12:56,480 --> 00:13:01,680
Dynamics will uh will naturally minimize

344
00:12:59,720 --> 00:13:03,560
the over an overall cost function given

345
00:13:01,680 --> 00:13:04,839
by an iing hamiltonian and this is

346
00:13:03,560 --> 00:13:06,720
something that's now been sort of even

347
00:13:04,839 --> 00:13:09,680
scaled up relatively large this is an

348
00:13:06,720 --> 00:13:13,199
example of 2,000 oscillators in a in a

349
00:13:09,680 --> 00:13:15,240
seos uh seos device and in this uh kind

350
00:13:13,199 --> 00:13:16,760
of setting you have essentially a

351
00:13:15,240 --> 00:13:18,440
network of coupled oscillators they have

352
00:13:16,760 --> 00:13:19,600
some Dynamics this is describing the

353
00:13:18,440 --> 00:13:21,279
evolution of the phase of each

354
00:13:19,600 --> 00:13:23,639
oscillator and really the thing that

355
00:13:21,279 --> 00:13:25,399
gets programmed here there's no program

356
00:13:23,639 --> 00:13:27,040
the thing that gets programmed is how is

357
00:13:25,399 --> 00:13:29,600
each oscillator coupled to each other

358
00:13:27,040 --> 00:13:30,680
one which is this ji J and the in this

359
00:13:29,600 --> 00:13:33,839
formula

360
00:13:30,680 --> 00:13:35,440
here another example of this is uh

361
00:13:33,839 --> 00:13:37,199
probabilistic bit machines where people

362
00:13:35,440 --> 00:13:39,519
have taken stochastic magnetic tunnel

363
00:13:37,199 --> 00:13:41,600
Junctions uh which behave as essentially

364
00:13:39,519 --> 00:13:43,240
probabilistic bits that you can bias

365
00:13:41,600 --> 00:13:45,279
connected them together in networks they

366
00:13:43,240 --> 00:13:47,320
have a slightly different Dynamics than

367
00:13:45,279 --> 00:13:49,040
than these oscillator versions but a

368
00:13:47,320 --> 00:13:51,399
very similar concept holds you

369
00:13:49,040 --> 00:13:54,360
essentially the M the system is doing

370
00:13:51,399 --> 00:13:57,079
what it naturally does these these These

371
00:13:54,360 --> 00:13:58,600
smjs are sort of flipping back and forth

372
00:13:57,079 --> 00:14:01,040
and the thing you control is how they

373
00:13:58,600 --> 00:14:03,800
are connect conected and there's been

374
00:14:01,040 --> 00:14:05,399
work at in MIT and and Durk and M's

375
00:14:03,800 --> 00:14:06,959
group and also luch child's group on

376
00:14:05,399 --> 00:14:08,560
this kind of thing so some of you might

377
00:14:06,959 --> 00:14:10,639
have seen this before so this is an

378
00:14:08,560 --> 00:14:12,240
example of what I mean of getting rid of

379
00:14:10,639 --> 00:14:14,759
the abstractions and really just working

380
00:14:12,240 --> 00:14:17,480
at the base level of the

381
00:14:14,759 --> 00:14:19,399
physics son now I want the first story I

382
00:14:17,480 --> 00:14:21,440
want to tell you about is work in our

383
00:14:19,399 --> 00:14:22,880
group on constructing neural networks in

384
00:14:21,440 --> 00:14:24,440
this kind of way of taking physical

385
00:14:22,880 --> 00:14:26,800
systems and operating at the level of

386
00:14:24,440 --> 00:14:30,279
the physics to construct uh or perform

387
00:14:26,800 --> 00:14:32,560
neural network inference

388
00:14:30,279 --> 00:14:34,000
so uh some sort of quick preliminaries

389
00:14:32,560 --> 00:14:35,880
and mostly so I can show you some

390
00:14:34,000 --> 00:14:39,079
diagrams uh so that you can sort of

391
00:14:35,880 --> 00:14:41,199
match them uh to diagrams later on uh is

392
00:14:39,079 --> 00:14:42,959
if we think about a a deep neural

393
00:14:41,199 --> 00:14:44,759
network really what we mean by deep is

394
00:14:42,959 --> 00:14:46,560
that there are multiple layers so here

395
00:14:44,759 --> 00:14:48,360
is an example where we have some image

396
00:14:46,560 --> 00:14:51,320
encoded in a vector let's say this dog

397
00:14:48,360 --> 00:14:52,800
here and this this data gets or this

398
00:14:51,320 --> 00:14:54,480
information gets propagated through

399
00:14:52,800 --> 00:14:55,639
multiple layers of a neural network

400
00:14:54,480 --> 00:14:57,519
where we can think of each of these sort

401
00:14:55,639 --> 00:15:01,120
of boxes here there's supposed to be a

402
00:14:57,519 --> 00:15:03,880
gray box around it but it's okay okay uh

403
00:15:01,120 --> 00:15:05,639
or uh or a single layer of the neural

404
00:15:03,880 --> 00:15:07,480
network that performs some function on

405
00:15:05,639 --> 00:15:09,440
the input vector and outputs another

406
00:15:07,480 --> 00:15:11,759
vector and this gets propagated through

407
00:15:09,440 --> 00:15:13,279
until uh out poops the answer that okay

408
00:15:11,759 --> 00:15:14,120
this thing that you sent in was an image

409
00:15:13,279 --> 00:15:16,480
of a

410
00:15:14,120 --> 00:15:18,519
dog really the way we like to think

411
00:15:16,480 --> 00:15:20,079
about this is that it's a network or a

412
00:15:18,519 --> 00:15:22,000
composition of functions you're you're

413
00:15:20,079 --> 00:15:24,240
taking an input of a dog and some

414
00:15:22,000 --> 00:15:26,519
parameters that control what function is

415
00:15:24,240 --> 00:15:28,040
implemented here uh you compute it and

416
00:15:26,519 --> 00:15:29,480
then you're passing it to the next one

417
00:15:28,040 --> 00:15:30,920
so you're now

418
00:15:29,480 --> 00:15:32,880
sending that as an input to the next

419
00:15:30,920 --> 00:15:36,000
function and so on and so on so really

420
00:15:32,880 --> 00:15:39,759
these uh this is a specific onsets for a

421
00:15:36,000 --> 00:15:42,120
function that takes a an input and a set

422
00:15:39,759 --> 00:15:44,680
of parameters and produces an

423
00:15:42,120 --> 00:15:46,480
output and of course okay if you put a

424
00:15:44,680 --> 00:15:47,880
put a cat in you should if your network

425
00:15:46,480 --> 00:15:49,680
is trained correctly and these

426
00:15:47,880 --> 00:15:51,759
parameters are set right it should

427
00:15:49,680 --> 00:15:53,800
predict that you have a that it's a cat

428
00:15:51,759 --> 00:15:55,240
soort of how does this training happen

429
00:15:53,800 --> 00:15:57,639
well if we start with a network where

430
00:15:55,240 --> 00:16:00,360
the parameters are just set randomly or

431
00:15:57,639 --> 00:16:02,120
untrained in some way then uh you'll

432
00:16:00,360 --> 00:16:04,199
just get a random answer out and

433
00:16:02,120 --> 00:16:07,079
training proceeds by updating the

434
00:16:04,199 --> 00:16:08,440
parameters uh so that when you when you

435
00:16:07,079 --> 00:16:09,800
uh when you put a dog in and it gives

436
00:16:08,440 --> 00:16:12,040
the wrong answer you maybe prod the

437
00:16:09,800 --> 00:16:14,519
parameters and move them in a different

438
00:16:12,040 --> 00:16:16,240
direction and then once you've uh once

439
00:16:14,519 --> 00:16:18,240
you fixed these parameters for all of

440
00:16:16,240 --> 00:16:19,880
these layers here now you can perform

441
00:16:18,240 --> 00:16:21,639
inference where you can put in a new

442
00:16:19,880 --> 00:16:23,759
image of unseen data and have it

443
00:16:21,639 --> 00:16:26,079
correctly make a prediction and so

444
00:16:23,759 --> 00:16:27,839
everything I'm going to tell you about

445
00:16:26,079 --> 00:16:30,639
uh in this talk today is really going to

446
00:16:27,839 --> 00:16:32,279
be about making Hardware that could

447
00:16:30,639 --> 00:16:35,399
potentially be more energy efficient or

448
00:16:32,279 --> 00:16:37,759
faster for the inference task but by

449
00:16:35,399 --> 00:16:39,120
necessity we have to have done training

450
00:16:37,759 --> 00:16:41,120
to get there so I'm not going to give

451
00:16:39,120 --> 00:16:43,360
you a method to improve the speed or

452
00:16:41,120 --> 00:16:44,880
Energy Efficiency of training but I have

453
00:16:43,360 --> 00:16:46,240
to cover it in some way because

454
00:16:44,880 --> 00:16:48,160
otherwise I couldn't have got to the

455
00:16:46,240 --> 00:16:53,079
inference

456
00:16:48,160 --> 00:16:54,560
phase and uh this is uh sort of not not

457
00:16:53,079 --> 00:16:56,680
a silly thing to be doing even if you

458
00:16:54,560 --> 00:16:59,079
only solve the inference problem already

459
00:16:56,680 --> 00:17:00,680
that's a very large fraction of cloud

460
00:16:59,079 --> 00:17:02,519
compute for machine learning because you

461
00:17:00,680 --> 00:17:03,800
you train a model once and then sort of

462
00:17:02,519 --> 00:17:05,480
at this point literally hundreds of

463
00:17:03,800 --> 00:17:06,919
millions of people may use that model in

464
00:17:05,480 --> 00:17:10,000
inference

465
00:17:06,919 --> 00:17:11,640
mode so how are neural networks

466
00:17:10,000 --> 00:17:13,839
implemented in conventional Hardware

467
00:17:11,640 --> 00:17:16,000
well if we think about a typical layer

468
00:17:13,839 --> 00:17:17,480
sort of most simple form of this will be

469
00:17:16,000 --> 00:17:18,760
this layer of a neural network will be

470
00:17:17,480 --> 00:17:20,799
performed by matrix Vector

471
00:17:18,760 --> 00:17:22,959
multiplication so we have a vector in we

472
00:17:20,799 --> 00:17:25,280
get a vector out and they're related by

473
00:17:22,959 --> 00:17:27,079
a matrix Vector multiplication of the

474
00:17:25,280 --> 00:17:28,480
what's often called the weight Matrix W

475
00:17:27,079 --> 00:17:31,280
describing the connections between the

476
00:17:28,480 --> 00:17:33,360
neurons and the input vector and then at

477
00:17:31,280 --> 00:17:34,880
each point or each neuron each element

478
00:17:33,360 --> 00:17:36,400
of the vector and element wise

479
00:17:34,880 --> 00:17:39,000
nonlinearity is typically applied

480
00:17:36,400 --> 00:17:42,360
something like this REO function and

481
00:17:39,000 --> 00:17:43,840
this has been uh has been implemented in

482
00:17:42,360 --> 00:17:45,960
Hardware accelerators and digital

483
00:17:43,840 --> 00:17:48,720
electronics where you have things like

484
00:17:45,960 --> 00:17:49,960
systolic arrays where you uh where you

485
00:17:48,720 --> 00:17:51,480
almost have you can almost see like the

486
00:17:49,960 --> 00:17:54,760
spatial structure of the Matrix in the

487
00:17:51,480 --> 00:17:56,400
hardware itself uh if you squint uh and

488
00:17:54,760 --> 00:17:58,320
people have also been working on analog

489
00:17:56,400 --> 00:18:00,919
processes in electronics for example

490
00:17:58,320 --> 00:18:03,240
this cross bar array here where you send

491
00:18:00,919 --> 00:18:06,640
in the vector as voltages on these blue

492
00:18:03,240 --> 00:18:09,240
lines and then they are connected to uh

493
00:18:06,640 --> 00:18:12,440
the red lines via uh programmable

494
00:18:09,240 --> 00:18:14,520
resistors uh so memoris of devices or

495
00:18:12,440 --> 00:18:16,679
resistive memory devices so the the

496
00:18:14,520 --> 00:18:18,720
weight Matrix is encoded in these

497
00:18:16,679 --> 00:18:20,200
elements that connect the the wires of

498
00:18:18,720 --> 00:18:22,400
the crossbar and then you read out your

499
00:18:20,200 --> 00:18:25,440
answer as currents on the on the on the

500
00:18:22,400 --> 00:18:27,600
red lines and this here you can really

501
00:18:25,440 --> 00:18:30,480
see that like uh the the hardware has

502
00:18:27,600 --> 00:18:32,880
been designed to perform Matrix Vector

503
00:18:30,480 --> 00:18:34,559
multiplication um and people work really

504
00:18:32,880 --> 00:18:37,039
hard at getting these kind of devices to

505
00:18:34,559 --> 00:18:39,679
exactly perform a matrix Vector

506
00:18:37,039 --> 00:18:41,320
multiplication so we we asked the

507
00:18:39,679 --> 00:18:43,400
question of does do you really need to

508
00:18:41,320 --> 00:18:45,280
do this do you really need to make

509
00:18:43,400 --> 00:18:47,640
Matrix Vector multipliers to do neural

510
00:18:45,280 --> 00:18:49,480
network processing uh I mean intuitively

511
00:18:47,640 --> 00:18:51,880
the answer should be no like I don't

512
00:18:49,480 --> 00:18:54,440
think what's going on inside here is a a

513
00:18:51,880 --> 00:18:55,400
high Precision metric Vector multiplier

514
00:18:54,440 --> 00:18:58,000
um

515
00:18:55,400 --> 00:18:59,679
but uh can we can we try show this with

516
00:18:58,000 --> 00:19:01,000
a sort of less exotic Hardware than the

517
00:18:59,679 --> 00:19:03,240
human

518
00:19:01,000 --> 00:19:05,600
brain so what we're really going to do

519
00:19:03,240 --> 00:19:07,400
is say okay currently the the mode

520
00:19:05,600 --> 00:19:10,000
people work in is they run some python

521
00:19:07,400 --> 00:19:11,520
where they call reu on a matrix Vector

522
00:19:10,000 --> 00:19:13,159
multiplication and that gets compiled

523
00:19:11,520 --> 00:19:14,799
all the way down we're going to see if

524
00:19:13,159 --> 00:19:17,760
we can instead start with some hardware

525
00:19:14,799 --> 00:19:19,720
and build up to it performing uh neural

526
00:19:17,760 --> 00:19:22,679
network

527
00:19:19,720 --> 00:19:25,000
inference and let's not take something

528
00:19:22,679 --> 00:19:27,200
that like almost does a matrix Vector

529
00:19:25,000 --> 00:19:28,840
multiplication just a little bit off

530
00:19:27,200 --> 00:19:30,240
let's really see if we sort of make no

531
00:19:28,840 --> 00:19:32,120
effort at all to make the physical

532
00:19:30,240 --> 00:19:33,960
Hardware perform something that we we

533
00:19:32,120 --> 00:19:35,960
recognize as being like a neural network

534
00:19:33,960 --> 00:19:40,960
computation and see if we can

535
00:19:35,960 --> 00:19:43,120
succeed so uh the the the training in uh

536
00:19:40,960 --> 00:19:44,679
in deep learning or deep learning is

537
00:19:43,120 --> 00:19:46,080
really just about training these

538
00:19:44,679 --> 00:19:48,320
controllable functions so if I'm trying

539
00:19:46,080 --> 00:19:50,039
to classify this mless handr digit 8

540
00:19:48,320 --> 00:19:53,120
here we're really just about trying to

541
00:19:50,039 --> 00:19:55,480
pick parameter values uh Theta 1 Theta 2

542
00:19:53,120 --> 00:19:57,480
theta 3 where each of these are vectors

543
00:19:55,480 --> 00:19:59,760
uh that such that when you put it at

544
00:19:57,480 --> 00:20:02,799
eight you get out the classification so

545
00:19:59,760 --> 00:20:04,600
it's just about tuning the tuning the

546
00:20:02,799 --> 00:20:06,679
parameters so that it makes the correct

547
00:20:04,600 --> 00:20:09,520
prediction another observation you can

548
00:20:06,679 --> 00:20:12,400
make is that every physical system or

549
00:20:09,520 --> 00:20:14,000
almost every physical system uh really

550
00:20:12,400 --> 00:20:16,320
is you can also think about as

551
00:20:14,000 --> 00:20:19,120
performing a controllable function so

552
00:20:16,320 --> 00:20:20,919
the quantum.tv is broken and over here

553
00:20:19,120 --> 00:20:22,600
there should be a gray box for those of

554
00:20:20,919 --> 00:20:23,880
you on Zoom you will see it but for

555
00:20:22,600 --> 00:20:25,640
those of you in the room the the gray

556
00:20:23,880 --> 00:20:29,240
box is invisible but imagine there's a

557
00:20:25,640 --> 00:20:31,480
gray box here that represents a physical

558
00:20:29,240 --> 00:20:33,440
system and if you have some control over

559
00:20:31,480 --> 00:20:34,640
what the how the physical system behaves

560
00:20:33,440 --> 00:20:36,799
you have some things you can control

561
00:20:34,640 --> 00:20:38,280
about it and then you can there's things

562
00:20:36,799 --> 00:20:40,840
you can measure about the system you can

563
00:20:38,280 --> 00:20:43,280
think about this the undergoing time

564
00:20:40,840 --> 00:20:45,080
dynamics of some physical system is just

565
00:20:43,280 --> 00:20:47,000
having the physical system perform a

566
00:20:45,080 --> 00:20:49,520
function computer function Fe you it

567
00:20:47,000 --> 00:20:51,799
takes in some input and it performs some

568
00:20:49,520 --> 00:20:54,159
function according to its Dynamics you

569
00:20:51,799 --> 00:20:56,919
can kind of arbitrarily split the things

570
00:20:54,159 --> 00:20:59,000
you can control about the system into

571
00:20:56,919 --> 00:21:00,640
what we might call the data input data

572
00:20:59,000 --> 00:21:02,159
and what we might call parameters this

573
00:21:00,640 --> 00:21:04,480
is a sort of arbitrary distinction that

574
00:21:02,159 --> 00:21:06,080
you're allowed to make uh and so you can

575
00:21:04,480 --> 00:21:08,039
then think about any physical system is

576
00:21:06,080 --> 00:21:10,080
really performing controllable or

577
00:21:08,039 --> 00:21:13,320
trainable functions for

578
00:21:10,080 --> 00:21:16,360
you so what's an example of a physical

579
00:21:13,320 --> 00:21:17,640
system that performs some uh some

580
00:21:16,360 --> 00:21:20,000
physical function for us and then

581
00:21:17,640 --> 00:21:22,679
controllable function well this is a

582
00:21:20,000 --> 00:21:25,520
somewhat exotic example uh constructed

583
00:21:22,679 --> 00:21:27,240
in Optics but maybe uh a strong benefit

584
00:21:25,520 --> 00:21:28,640
of it is it clearly doesn't look like

585
00:21:27,240 --> 00:21:30,440
sort of conventional neural network

586
00:21:28,640 --> 00:21:32,600
Hardware we have at the heart a

587
00:21:30,440 --> 00:21:34,799
nonlinear Crystal that will perform some

588
00:21:32,600 --> 00:21:37,640
frequency generation and since this

589
00:21:34,799 --> 00:21:39,159
isn't an optic seminar uh I won't go

590
00:21:37,640 --> 00:21:41,159
into great detail about the optical

591
00:21:39,159 --> 00:21:42,440
physics that happens here but suffice to

592
00:21:41,159 --> 00:21:45,039
say essentially you can think about this

593
00:21:42,440 --> 00:21:46,679
box as uh being able to take sort of two

594
00:21:45,039 --> 00:21:48,840
photons and add their wavelength and you

595
00:21:46,679 --> 00:21:50,640
get out a photon of a of a of a higher

596
00:21:48,840 --> 00:21:52,480
energy oh sorry add their energies and

597
00:21:50,640 --> 00:21:55,919
you get out a photon of a higher energy

598
00:21:52,480 --> 00:21:58,240
and we we're using this in the uh a

599
00:21:55,919 --> 00:22:00,360
setting where we we shape the spectrum

600
00:21:58,240 --> 00:22:02,679
of the pulse that that gets sent into

601
00:22:00,360 --> 00:22:04,559
this uh nonlinear Crystal with a pulse

602
00:22:02,679 --> 00:22:06,240
shaper and then we measure the Spectrum

603
00:22:04,559 --> 00:22:08,799
out with a a

604
00:22:06,240 --> 00:22:14,000
spectrometer so if I feed

605
00:22:08,799 --> 00:22:16,440
in uh a light with uh frequency Omega 1

606
00:22:14,000 --> 00:22:17,880
what comes out is light at two at two

607
00:22:16,440 --> 00:22:20,840
times Omega

608
00:22:17,880 --> 00:22:22,480
1 if I send in light at Omega 2 then we

609
00:22:20,840 --> 00:22:25,640
get light out at two Omega 2 because you

610
00:22:22,480 --> 00:22:27,640
just added two Omega 2 photons together

611
00:22:25,640 --> 00:22:29,440
and if you put in light of two different

612
00:22:27,640 --> 00:22:31,840
wavelengths you get this sum coming out

613
00:22:29,440 --> 00:22:36,720
of Omega 1 plus Omega

614
00:22:31,840 --> 00:22:37,880
2 now we can imagine uh putting in an

615
00:22:36,720 --> 00:22:39,400
input Spectrum that's a bit more

616
00:22:37,880 --> 00:22:41,400
complicated than just one or two single

617
00:22:39,400 --> 00:22:44,000
frequencies that it has this spectrum

618
00:22:41,400 --> 00:22:46,840
here and we can look at what we get

619
00:22:44,000 --> 00:22:49,039
out and uh you can see that what you get

620
00:22:46,840 --> 00:22:51,279
out is actually at least a non-trivial

621
00:22:49,039 --> 00:22:54,559
by ey mapping of the

622
00:22:51,279 --> 00:22:57,159
input and you could think about dividing

623
00:22:54,559 --> 00:22:59,840
some part of this this spectrum on the

624
00:22:57,159 --> 00:23:01,880
left that you input and being you can

625
00:22:59,840 --> 00:23:03,600
denote some part of it like this part

626
00:23:01,880 --> 00:23:05,559
that's stepping that's increasing in the

627
00:23:03,600 --> 00:23:09,120
animation as the part that's going to

628
00:23:05,559 --> 00:23:11,400
control what function this thing

629
00:23:09,120 --> 00:23:14,120
realizes uh so you could now really

630
00:23:11,400 --> 00:23:16,440
think about this Optical system as

631
00:23:14,120 --> 00:23:20,120
realizing a controllable function of

632
00:23:16,440 --> 00:23:20,120
your uh your input your input

633
00:23:20,799 --> 00:23:26,120
light so the next uh or the the kind of

634
00:23:24,240 --> 00:23:28,480
the key idea then is given that physical

635
00:23:26,120 --> 00:23:31,360
systems realize uh controllable function

636
00:23:28,480 --> 00:23:32,960
fun for you is you can then take uh your

637
00:23:31,360 --> 00:23:34,400
standard sort of neural network diagram

638
00:23:32,960 --> 00:23:35,799
and think well what happens if you take

639
00:23:34,400 --> 00:23:37,840
this block this layer of the neural

640
00:23:35,799 --> 00:23:39,679
network and replace it with a physical

641
00:23:37,840 --> 00:23:42,000
system that performs a that computes a

642
00:23:39,679 --> 00:23:44,279
physical function for

643
00:23:42,000 --> 00:23:45,880
you and in fact you could do the same

644
00:23:44,279 --> 00:23:47,799
thing or on a deep Network you could

645
00:23:45,880 --> 00:23:49,760
take sort of each layer and replace it

646
00:23:47,799 --> 00:23:53,559
with imagine there are gray boxes where

647
00:23:49,760 --> 00:23:54,919
there are holes here um with copies of

648
00:23:53,559 --> 00:23:57,360
this physical

649
00:23:54,919 --> 00:24:00,400
system uh and each one can have

650
00:23:57,360 --> 00:24:02,640
different uh trainable parameters and

651
00:24:00,400 --> 00:24:06,880
this thing will then realize some

652
00:24:02,640 --> 00:24:06,880
complex trainable physical

653
00:24:07,320 --> 00:24:13,159
calculation so we did this with a with

654
00:24:11,559 --> 00:24:15,640
this nonlinear Optical setup that I

655
00:24:13,159 --> 00:24:18,360
showed you in a few slides ago where

656
00:24:15,640 --> 00:24:20,400
well we didn't copy the entire setup

657
00:24:18,360 --> 00:24:23,240
five times but we used it five times in

658
00:24:20,400 --> 00:24:24,960
a in a in a sort of a Time multiplexing

659
00:24:23,240 --> 00:24:26,320
fashion where again the the gray boxes

660
00:24:24,960 --> 00:24:29,480
here are missing but imagine each of

661
00:24:26,320 --> 00:24:32,240
these gray boxes is a Cy of this

662
00:24:29,480 --> 00:24:33,320
conceptually a copy of this frequency

663
00:24:32,240 --> 00:24:36,200
conversion

664
00:24:33,320 --> 00:24:38,600
setup and then each layer is going to

665
00:24:36,200 --> 00:24:40,679
have different set of parameters and

666
00:24:38,600 --> 00:24:42,919
what we chose to be the parameters was

667
00:24:40,679 --> 00:24:45,159
the left hand part of the spectrum so we

668
00:24:42,919 --> 00:24:46,840
just declared we can arbitrarily do it

669
00:24:45,159 --> 00:24:47,760
declared that the left hand part of the

670
00:24:46,840 --> 00:24:49,799
spectrum those are going to be the

671
00:24:47,760 --> 00:24:51,840
tunable parameters and then the right

672
00:24:49,799 --> 00:24:54,279
hand part of the spectrum is going to be

673
00:24:51,840 --> 00:24:56,840
uh the the input data that of what we're

674
00:24:54,279 --> 00:24:59,279
trying to classify and we did a a

675
00:24:56,840 --> 00:25:00,840
demonstration of this for classifying

676
00:24:59,279 --> 00:25:02,600
spoken vowels that have been then

677
00:25:00,840 --> 00:25:04,679
encoded in what the people in this

678
00:25:02,600 --> 00:25:06,960
community called forant

679
00:25:04,679 --> 00:25:09,200
frequencies and the idea is that at the

680
00:25:06,960 --> 00:25:11,080
end of sending this uh this signal

681
00:25:09,200 --> 00:25:13,360
through the network if we look at the

682
00:25:11,080 --> 00:25:15,279
Spectrum the the part of the spectrum

683
00:25:13,360 --> 00:25:17,399
that is the highest intensity will that

684
00:25:15,279 --> 00:25:19,320
will encode which vow uh it's been

685
00:25:17,399 --> 00:25:23,559
classified

686
00:25:19,320 --> 00:25:26,039
as so the key part of this then is how

687
00:25:23,559 --> 00:25:28,640
do we train this how do we figure out

688
00:25:26,039 --> 00:25:30,520
what uh parameters so what Val values of

689
00:25:28,640 --> 00:25:31,600
the spectrum we should put in at each

690
00:25:30,520 --> 00:25:35,799
layer of the

691
00:25:31,600 --> 00:25:37,960
network and uh the sort of key two key

692
00:25:35,799 --> 00:25:40,440
ideas for how to do this was first let's

693
00:25:37,960 --> 00:25:42,279
use a differentiable digital model of

694
00:25:40,440 --> 00:25:43,919
the system in training so a laptop is

695
00:25:42,279 --> 00:25:46,840
going to be involved in training not an

696
00:25:43,919 --> 00:25:48,360
inference but in training uh so that you

697
00:25:46,840 --> 00:25:50,120
can perform gradient descent using

698
00:25:48,360 --> 00:25:52,200
backprop which is sort of a great

699
00:25:50,120 --> 00:25:54,679
algorithm for training these things for

700
00:25:52,200 --> 00:25:56,080
uh kind of we know all reasons we know

701
00:25:54,679 --> 00:25:58,360
in love especially the fact that in a

702
00:25:56,080 --> 00:26:00,480
single pass you get back a gradient so

703
00:25:58,360 --> 00:26:02,520
get back all of the the derivative with

704
00:26:00,480 --> 00:26:05,440
respect to every parameter in a single

705
00:26:02,520 --> 00:26:07,159
pass and where the the second key idea

706
00:26:05,440 --> 00:26:08,960
to make this work is really you don't

707
00:26:07,159 --> 00:26:10,440
want to do everything in the laptop you

708
00:26:08,960 --> 00:26:13,159
only want to do half the work in the

709
00:26:10,440 --> 00:26:14,960
laptop you want to do backward passes in

710
00:26:13,159 --> 00:26:17,640
uh the laptop and you want to actually

711
00:26:14,960 --> 00:26:19,840
use your physical Hardware to make uh

712
00:26:17,640 --> 00:26:21,799
predictions that are going to be used to

713
00:26:19,840 --> 00:26:23,600
compute your loss function so let's see

714
00:26:21,799 --> 00:26:28,600
if I can get out of the pointer mode and

715
00:26:23,600 --> 00:26:28,600
play a little video um

716
00:26:29,159 --> 00:26:33,399
and

717
00:26:30,520 --> 00:26:36,640
then move this where's my

718
00:26:33,399 --> 00:26:41,200
pointer there we go

719
00:26:36,640 --> 00:26:43,039
and so we'll see this now we have some

720
00:26:41,200 --> 00:26:45,080
choice of parameters and data we send it

721
00:26:43,039 --> 00:26:46,960
through the physical system we get an

722
00:26:45,080 --> 00:26:48,480
answer that's not quite correct so we

723
00:26:46,960 --> 00:26:50,919
were supposed to be classifying this as

724
00:26:48,480 --> 00:26:52,760
an eight and it was it was not uh we get

725
00:26:50,919 --> 00:26:54,640
an we compute an error so how far off

726
00:26:52,760 --> 00:26:57,159
was the prediction you send that error

727
00:26:54,640 --> 00:27:00,279
through the Digital model on your laptop

728
00:26:57,159 --> 00:27:01,919
and it computes gradient and now you can

729
00:27:00,279 --> 00:27:04,679
update your parameters and then sort of

730
00:27:01,919 --> 00:27:04,679
repeat this in a

731
00:27:04,960 --> 00:27:10,240
cycle all

732
00:27:07,360 --> 00:27:13,799
right so when we do this uh sorry let me

733
00:27:10,240 --> 00:27:13,799
get my get my mouse pointed

734
00:27:15,880 --> 00:27:20,600
back so when we do this uh as a function

735
00:27:18,960 --> 00:27:22,960
of epoch so how many times we've been

736
00:27:20,600 --> 00:27:24,720
through the training data set uh we can

737
00:27:22,960 --> 00:27:26,080
look at the accuracy of classifying the

738
00:27:24,720 --> 00:27:28,720
vowels which was the task we were trying

739
00:27:26,080 --> 00:27:31,159
to do and we can see in the blue line

740
00:27:28,720 --> 00:27:33,399
that as you as you go through the

741
00:27:31,159 --> 00:27:35,720
training St you manage to get up to 93%

742
00:27:33,399 --> 00:27:39,279
accuracy doing it in this way so this

743
00:27:35,720 --> 00:27:41,240
works uh the black line is what we got

744
00:27:39,279 --> 00:27:43,640
the first time we tried this uh when we

745
00:27:41,240 --> 00:27:45,399
didn't do it in this uh sort of hybrid

746
00:27:43,640 --> 00:27:46,760
manner of forward pass through physical

747
00:27:45,399 --> 00:27:48,440
system backward pass through Digital

748
00:27:46,760 --> 00:27:50,840
model where we only did the training in

749
00:27:48,440 --> 00:27:53,440
a laptop so forward pass and backward

750
00:27:50,840 --> 00:27:55,600
pass both through a digital model uh and

751
00:27:53,440 --> 00:27:57,600
it doesn't work well at all uh and the

752
00:27:55,600 --> 00:27:59,039
reason for this is that our physical our

753
00:27:57,600 --> 00:28:00,760
model of the dynamics of the physical

754
00:27:59,039 --> 00:28:04,240
Hardware is a little bit inaccurate

755
00:28:00,760 --> 00:28:05,880
because modeling analog systems with qu

756
00:28:04,240 --> 00:28:08,640
like great quantitative accuracy is

757
00:28:05,880 --> 00:28:10,440
really difficult uh so that that doesn't

758
00:28:08,640 --> 00:28:12,080
work well but once you once you

759
00:28:10,440 --> 00:28:14,000
introduce the sort of hybrid scheme then

760
00:28:12,080 --> 00:28:16,240
it works really well and the intuition

761
00:28:14,000 --> 00:28:17,919
there is so long as your differentiable

762
00:28:16,240 --> 00:28:19,559
Digital model is accurate enough that it

763
00:28:17,919 --> 00:28:21,080
gives you the sides of the gradient

764
00:28:19,559 --> 00:28:23,760
correctly then you at least make

765
00:28:21,080 --> 00:28:26,640
progress and you move in the correct

766
00:28:23,760 --> 00:28:28,559
direction so we also did this uh we also

767
00:28:26,640 --> 00:28:30,399
tested this algorithm for for training

768
00:28:28,559 --> 00:28:32,480
on a few different physical systems and

769
00:28:30,399 --> 00:28:33,919
with a the task of handwritten digit

770
00:28:32,480 --> 00:28:36,200
recognition which is a little bit more

771
00:28:33,919 --> 00:28:38,519
difficult than vowels um we managed to

772
00:28:36,200 --> 00:28:40,080
do it with a a mechanical setup of a a

773
00:28:38,519 --> 00:28:43,480
piece of metal that was shaken which is

774
00:28:40,080 --> 00:28:47,159
also kind of a weird physical setting

775
00:28:43,480 --> 00:28:48,760
question so I assume that uh going from

776
00:28:47,159 --> 00:28:51,200
the air to actually updating the

777
00:28:48,760 --> 00:28:54,279
parameters is not trivial is that

778
00:28:51,200 --> 00:28:55,679
correct um it's it wouldn't be trivial

779
00:28:54,279 --> 00:28:59,200
if somebody hadn't already written pie

780
00:28:55,679 --> 00:29:01,200
torch for us but uh we are literally

781
00:28:59,200 --> 00:29:03,480
building building or standing on the

782
00:29:01,200 --> 00:29:06,279
shoulders of giants and so we can

783
00:29:03,480 --> 00:29:09,600
literally uh use all the Machinery of

784
00:29:06,279 --> 00:29:12,000
back propop in something like P torch uh

785
00:29:09,600 --> 00:29:15,960
to do that for us so we compute the loss

786
00:29:12,000 --> 00:29:17,480
function so this error and then uh and

787
00:29:15,960 --> 00:29:20,039
then we need this different the thing

788
00:29:17,480 --> 00:29:21,559
that's not in P torch by default is what

789
00:29:20,039 --> 00:29:23,200
is the differentiable Digital model of

790
00:29:21,559 --> 00:29:25,799
our physical system that we had to come

791
00:29:23,200 --> 00:29:27,480
up with ourselves but once we have that

792
00:29:25,799 --> 00:29:28,840
uh then we can sort of chug through and

793
00:29:27,480 --> 00:29:30,960
I think this would indeed have been like

794
00:29:28,840 --> 00:29:33,120
a couple of years effort in the I don't

795
00:29:30,960 --> 00:29:36,320
know maybe early 90s but uh now it's now

796
00:29:33,120 --> 00:29:39,760
it's easy

797
00:29:36,320 --> 00:29:39,760
question the input

798
00:29:42,440 --> 00:29:47,240
parameters input parameters are they one

799
00:29:44,880 --> 00:29:49,240
for one looks like half of your spectrum

800
00:29:47,240 --> 00:29:52,679
or what it is is your weights and the

801
00:29:49,240 --> 00:29:54,279
other half of is your input parameters

802
00:29:52,679 --> 00:29:56,679
are you able to just set the input

803
00:29:54,279 --> 00:29:58,279
parameters one to one once you've

804
00:29:56,679 --> 00:29:59,799
completed the training or do they also

805
00:29:58,279 --> 00:30:03,440
need like a slight

806
00:29:59,799 --> 00:30:06,519
mapping right um essentially the answer

807
00:30:03,440 --> 00:30:07,840
is they map one to one I mean there's a

808
00:30:06,519 --> 00:30:09,399
there's like an implicit thing going on

809
00:30:07,840 --> 00:30:10,799
though of like how do we set that part

810
00:30:09,399 --> 00:30:13,519
of the spectrum it's like well we get to

811
00:30:10,799 --> 00:30:15,200
update uh values in a lookup table for a

812
00:30:13,519 --> 00:30:16,600
spatial light modulator and those are

813
00:30:15,200 --> 00:30:18,279
really the things we're changing and so

814
00:30:16,600 --> 00:30:20,559
it's like we're not directly changing

815
00:30:18,279 --> 00:30:22,000
the Spectrum we're we're we're changing

816
00:30:20,559 --> 00:30:23,600
the the thing we can control of the

817
00:30:22,000 --> 00:30:25,799
electronic device that changes the

818
00:30:23,600 --> 00:30:27,679
Spectrum uh but yeah there's no like

819
00:30:25,799 --> 00:30:29,480
extra trained thing to go from trained

820
00:30:27,679 --> 00:30:32,640
param the inference

821
00:30:29,480 --> 00:30:32,640
parameters thank

822
00:30:36,360 --> 00:30:41,799
you thank you uh do you also have uh

823
00:30:40,120 --> 00:30:43,880
many many different layers and each

824
00:30:41,799 --> 00:30:46,880
layer is the same crystal but with the

825
00:30:43,880 --> 00:30:48,600
different shaper and output ah great

826
00:30:46,880 --> 00:30:49,840
question yes so the short answer is yes

827
00:30:48,600 --> 00:30:51,480
we have many many layers both in

828
00:30:49,840 --> 00:30:52,679
inference and in training and in

829
00:30:51,480 --> 00:30:54,799
training just so that I didn't have to

830
00:30:52,679 --> 00:30:56,399
draw this thing with many layers uh I

831
00:30:54,799 --> 00:30:58,960
already showed one but it extends

832
00:30:56,399 --> 00:31:02,840
naturally uh also great great thanks to

833
00:30:58,960 --> 00:31:04,320
pytorch um so yeah we're do this uh this

834
00:31:02,840 --> 00:31:06,000
vows example we're doing with five

835
00:31:04,320 --> 00:31:07,120
layers and then some of the other

836
00:31:06,000 --> 00:31:08,440
examples I show we had slightly

837
00:31:07,120 --> 00:31:10,639
different architectures with varying

838
00:31:08,440 --> 00:31:13,679
numbers of layers but uh all of them

839
00:31:10,639 --> 00:31:16,600
with more than one I see so for each

840
00:31:13,679 --> 00:31:18,919
reference or inference then um for each

841
00:31:16,600 --> 00:31:22,760
layer when it propagates to the next

842
00:31:18,919 --> 00:31:25,480
layer um how do you I guess how do you

843
00:31:22,760 --> 00:31:28,120
implement it so that it's not blocked by

844
00:31:25,480 --> 00:31:32,519
you know the manual tuning speed of a

845
00:31:28,120 --> 00:31:34,279
grad student ah right so it's uh it's

846
00:31:32,519 --> 00:31:36,240
not locked by the manual tuning speed of

847
00:31:34,279 --> 00:31:37,880
a grad student but it is currently

848
00:31:36,240 --> 00:31:39,679
bottom neck by something else which is

849
00:31:37,880 --> 00:31:42,639
also not desirable we'd like to get

850
00:31:39,679 --> 00:31:46,120
Beyond it which is uh in this Optical

851
00:31:42,639 --> 00:31:47,600
setup for example we we measure at the

852
00:31:46,120 --> 00:31:49,440
spectrometer but then how does that get

853
00:31:47,600 --> 00:31:51,000
fed to the next layer it's like well

854
00:31:49,440 --> 00:31:53,200
it's now an Electronics it's in some

855
00:31:51,000 --> 00:31:54,799
laptop and then we feeded into the sort

856
00:31:53,200 --> 00:31:57,000
of data for the pulse shaper for the

857
00:31:54,799 --> 00:31:58,639
next round so there's some limit on the

858
00:31:57,000 --> 00:32:00,720
througho of the system of like how

859
00:31:58,639 --> 00:32:02,559
quickly can we take the output of a

860
00:32:00,720 --> 00:32:04,120
spectrometer and feed it into the pulse

861
00:32:02,559 --> 00:32:06,240
shape before encoding the data for the

862
00:32:04,120 --> 00:32:09,480
next Set uh which we'd like to get rid

863
00:32:06,240 --> 00:32:11,200
of but uh it's a yeah limitation of the

864
00:32:09,480 --> 00:32:13,600
current sort of Paradigm so I think as

865
00:32:11,200 --> 00:32:14,559
we we think about sort of incidentally

866
00:32:13,600 --> 00:32:15,960
none of the examples I'm going to show

867
00:32:14,559 --> 00:32:17,760
you what I'm saying you should build

868
00:32:15,960 --> 00:32:19,960
Ural Network like these were more sort

869
00:32:17,760 --> 00:32:23,760
of chosen as extreme examples of like

870
00:32:19,960 --> 00:32:25,039
you can do it this way um we we' like to

871
00:32:23,760 --> 00:32:27,399
think about sort of how do we get rid of

872
00:32:25,039 --> 00:32:29,159
these bottlenecks but fortunately it was

873
00:32:27,399 --> 00:32:32,559
not limited by how quickly somebody can

874
00:32:29,159 --> 00:32:32,559
type on a laptop or anything like

875
00:32:35,080 --> 00:32:45,039
that uh so the function fop M uh needs

876
00:32:43,080 --> 00:32:48,000
to have certain mathematical properties

877
00:32:45,039 --> 00:32:49,519
I'm not sure I you know they need to be

878
00:32:48,000 --> 00:32:52,120
they need to they need to do some kind

879
00:32:49,519 --> 00:32:54,320
of integration um and it looks like you

880
00:32:52,120 --> 00:32:55,320
do that here because you're converting

881
00:32:54,320 --> 00:32:58,760
things to

882
00:32:55,320 --> 00:33:02,799
Spectra um but can you perhaps go into

883
00:32:58,760 --> 00:33:05,679
just um have you characterized the kinds

884
00:33:02,799 --> 00:33:07,679
of functions that are even that that

885
00:33:05,679 --> 00:33:10,559
that are possible by some of the

886
00:33:07,679 --> 00:33:12,159
physical means because I I don't see how

887
00:33:10,559 --> 00:33:14,200
you know not everything's going to do

888
00:33:12,159 --> 00:33:16,240
this right not every every physical

889
00:33:14,200 --> 00:33:17,760
system is going to nicely behave like

890
00:33:16,240 --> 00:33:21,080
this right right yeah it's a great

891
00:33:17,760 --> 00:33:22,320
question so sort of P the answer is uh I

892
00:33:21,080 --> 00:33:24,039
told you that pretty much every

893
00:33:22,320 --> 00:33:26,080
controllable physical system can be used

894
00:33:24,039 --> 00:33:28,760
to do neural network inference I didn't

895
00:33:26,080 --> 00:33:32,960
promise you that it would be good uh or

896
00:33:28,760 --> 00:33:34,440
good at it so which is maybe a little

897
00:33:32,960 --> 00:33:36,720
glib but it's actually pretty close to

898
00:33:34,440 --> 00:33:38,279
the truth uh it's just like pretty much

899
00:33:36,720 --> 00:33:39,840
any neural network architecture you

900
00:33:38,279 --> 00:33:42,279
could come up with in a neural Network's

901
00:33:39,840 --> 00:33:43,679
101 class as a homework exercise will

902
00:33:42,279 --> 00:33:46,240
act as a neural network to some extent

903
00:33:43,679 --> 00:33:48,720
but whether it's good or not well that's

904
00:33:46,240 --> 00:33:51,480
that's an art um so that's part of it is

905
00:33:48,720 --> 00:33:53,840
figuring out what and it's and it's task

906
00:33:51,480 --> 00:33:56,240
dependent some some architectures are

907
00:33:53,840 --> 00:33:58,960
good for some tasks and not for others

908
00:33:56,240 --> 00:34:00,559
uh so that's part of the story is

909
00:33:58,960 --> 00:34:01,760
finding physical Hardware that matches

910
00:34:00,559 --> 00:34:03,799
the task well and does have the

911
00:34:01,760 --> 00:34:04,960
sufficient sort of complexity but I

912
00:34:03,799 --> 00:34:06,440
think there's a really sort of even

913
00:34:04,960 --> 00:34:08,639
deeper thing that's really interesting

914
00:34:06,440 --> 00:34:10,119
there which is physical systems we can

915
00:34:08,639 --> 00:34:11,200
think about what mathematical function

916
00:34:10,119 --> 00:34:13,440
they Implement just from sort of

917
00:34:11,200 --> 00:34:14,720
analyzing from a physics perspective and

918
00:34:13,440 --> 00:34:16,760
often when you're doing things you'll

919
00:34:14,720 --> 00:34:19,320
naturally see a convolution arises in

920
00:34:16,760 --> 00:34:21,240
how the inputs and outputs are connected

921
00:34:19,320 --> 00:34:22,760
um and you can have some intuition from

922
00:34:21,240 --> 00:34:25,119
the computer sign side that well

923
00:34:22,760 --> 00:34:27,440
convolutions get used in for example

924
00:34:25,119 --> 00:34:29,399
image processing in the earlier layers

925
00:34:27,440 --> 00:34:32,240
so A system that naturally performs

926
00:34:29,399 --> 00:34:34,560
convolutions might be good for that um

927
00:34:32,240 --> 00:34:36,000
but we also know that sort of Highly

928
00:34:34,560 --> 00:34:37,359
connected systems are good so if you

929
00:34:36,000 --> 00:34:39,760
give me some system that are very

930
00:34:37,359 --> 00:34:41,200
sparsely connected components that's

931
00:34:39,760 --> 00:34:43,359
probably not going to be very good for

932
00:34:41,200 --> 00:34:44,839
most tasks you want something that with

933
00:34:43,359 --> 00:34:47,000
with sort of all of the internal

934
00:34:44,839 --> 00:34:48,960
elements then neurons if you will sort

935
00:34:47,000 --> 00:34:50,720
of speak with each other a lot but also

936
00:34:48,960 --> 00:34:52,359
that that that Pro that connectivity is

937
00:34:50,720 --> 00:34:53,879
programmable and we're still just

938
00:34:52,359 --> 00:34:55,399
beginning to learn sort of what

939
00:34:53,879 --> 00:34:57,240
intuitions can we take and then how do

940
00:34:55,399 --> 00:34:59,640
we realize them in different physical

941
00:34:57,240 --> 00:34:59,640
systems

942
00:35:01,320 --> 00:35:08,920
all right great so uh I wanted to give

943
00:35:04,880 --> 00:35:10,839
another couple of examples of uh demos

944
00:35:08,920 --> 00:35:13,000
we did with this but I I also want to

945
00:35:10,839 --> 00:35:14,599
move on to talking about things that are

946
00:35:13,000 --> 00:35:16,720
sort of less toy examples than these

947
00:35:14,599 --> 00:35:18,480
ones so just going through relatively

948
00:35:16,720 --> 00:35:19,920
quickly we did an example with mechanics

949
00:35:18,480 --> 00:35:21,079
which was also picked not because I

950
00:35:19,920 --> 00:35:23,119
think you should make your neural

951
00:35:21,079 --> 00:35:25,760
networks and replace gpus with with

952
00:35:23,119 --> 00:35:27,200
mechanics but because it's an example of

953
00:35:25,760 --> 00:35:29,320
something that you would niely maybe not

954
00:35:27,200 --> 00:35:31,560
expect to able to do handwritten digit

955
00:35:29,320 --> 00:35:33,520
recognition and yet is able to succeed

956
00:35:31,560 --> 00:35:35,520
at least to the level of about 90%

957
00:35:33,520 --> 00:35:37,000
accuracy so this is literally we have a

958
00:35:35,520 --> 00:35:38,599
a piece of metal that we shake with an

959
00:35:37,000 --> 00:35:41,040
audio speaker we listen to it with a

960
00:35:38,599 --> 00:35:42,880
microphone that's sitting up there uh we

961
00:35:41,040 --> 00:35:44,520
also did one with the the world's

962
00:35:42,880 --> 00:35:47,160
simplest analog electronic neural

963
00:35:44,520 --> 00:35:49,640
network it's an RLC oscillator we added

964
00:35:47,160 --> 00:35:52,800
one transistor again for the intuition

965
00:35:49,640 --> 00:35:54,560
from CS that neural nonlinearity is very

966
00:35:52,800 --> 00:35:57,119
important in giving sort of power to

967
00:35:54,560 --> 00:35:58,960
neural networks U and this does a little

968
00:35:57,119 --> 00:36:00,560
bit better and then we also did a demo

969
00:35:58,960 --> 00:36:02,040
with the Optics setup that I already

970
00:36:00,560 --> 00:36:06,400
showed

971
00:36:02,040 --> 00:36:07,920
you so uh something that uh sort of

972
00:36:06,400 --> 00:36:10,040
naturally comes up with this is like

973
00:36:07,920 --> 00:36:13,160
well okay you first of all you did some

974
00:36:10,040 --> 00:36:14,960
toy demos uh can you do better than you

975
00:36:13,160 --> 00:36:17,880
did if you get more sophisticated

976
00:36:14,960 --> 00:36:19,599
hardware and then how tolerant is this

977
00:36:17,880 --> 00:36:21,319
whole thing to sort of variations from

978
00:36:19,599 --> 00:36:23,400
hardware and and mismatch between the

979
00:36:21,319 --> 00:36:25,240
model you use in the in the in the

980
00:36:23,400 --> 00:36:29,160
training and the actual the actual

981
00:36:25,240 --> 00:36:31,920
inference so we we did some simulations

982
00:36:29,160 --> 00:36:34,280
with a a prototypical model of coupled

983
00:36:31,920 --> 00:36:36,880
oscillators with a thousand oscillators

984
00:36:34,280 --> 00:36:38,040
which is larger than the hardware we had

985
00:36:36,880 --> 00:36:40,680
but it's something that sort of shows

986
00:36:38,040 --> 00:36:43,280
how does the scale and for an easy task

987
00:36:40,680 --> 00:36:44,960
of IM 10 digit classification we can

988
00:36:43,280 --> 00:36:46,480
then get to state-of-the-art accuracy

989
00:36:44,960 --> 00:36:49,000
with a a network of a thousand

990
00:36:46,480 --> 00:36:50,599
oscillators this is now solving the the

991
00:36:49,000 --> 00:36:52,200
the memus task a completely different

992
00:36:50,599 --> 00:36:53,960
way than a sort of neural networks 101

993
00:36:52,200 --> 00:36:55,960
would do it but it's also possible in

994
00:36:53,960 --> 00:36:58,440
this sort of weird Paradigm of like

995
00:36:55,960 --> 00:37:01,119
connected coupled oscillators

996
00:36:58,440 --> 00:37:03,440
um we also uh tried a more difficult

997
00:37:01,119 --> 00:37:05,359
task which by ml standards is not

998
00:37:03,440 --> 00:37:07,880
actually hard it's just hard for us in

999
00:37:05,359 --> 00:37:09,760
this this sort of nent community uh of

1000
00:37:07,880 --> 00:37:12,200
fashion imess which is a variation of

1001
00:37:09,760 --> 00:37:13,960
image recognition of of garments like is

1002
00:37:12,200 --> 00:37:16,560
it a t-shirt or is it trousers or

1003
00:37:13,960 --> 00:37:19,960
whatever um and we were able to show

1004
00:37:16,560 --> 00:37:21,440
that we can train these uh network of

1005
00:37:19,960 --> 00:37:24,040
thousand oscillators or thousand is

1006
00:37:21,440 --> 00:37:27,680
oscillators to perform mest handwritten

1007
00:37:24,040 --> 00:37:29,079
mest fashion uh recognition uh accurate

1008
00:37:27,680 --> 00:37:31,640
L but then we can also sort of see how

1009
00:37:29,079 --> 00:37:34,040
do we if we make our model and our

1010
00:37:31,640 --> 00:37:35,520
simulated Hardware mismatch how much can

1011
00:37:34,040 --> 00:37:37,280
we tolerate and it turns out that with

1012
00:37:35,520 --> 00:37:39,000
this sort of physics aware training type

1013
00:37:37,280 --> 00:37:41,480
approach you can tolerate a mismatch of

1014
00:37:39,000 --> 00:37:43,240
up to 70% difference in the parameters

1015
00:37:41,480 --> 00:37:44,960
of the model versus the actual hardware

1016
00:37:43,240 --> 00:37:47,760
and it's still have it perform inference

1017
00:37:44,960 --> 00:37:49,960
at a almost state-of-the-art level U and

1018
00:37:47,760 --> 00:37:51,599
similarly you can train it to be a in a

1019
00:37:49,960 --> 00:37:53,920
way that it's resilient to variations

1020
00:37:51,599 --> 00:37:55,920
from device to device so you get a new

1021
00:37:53,920 --> 00:37:57,560
device off the production line it's up

1022
00:37:55,920 --> 00:37:59,319
to let's say the parameters of the

1023
00:37:57,560 --> 00:38:01,520
components are 10% different from the

1024
00:37:59,319 --> 00:38:04,440
ones you trained on uh so long as you've

1025
00:38:01,520 --> 00:38:05,880
trained it in this style uh if you just

1026
00:38:04,440 --> 00:38:07,960
transfer the parameters to the new

1027
00:38:05,880 --> 00:38:11,560
device you'll get almost the same

1028
00:38:07,960 --> 00:38:13,960
accuracy so 87.5 versus

1029
00:38:11,560 --> 00:38:16,079
88.9 so the main kind of things we take

1030
00:38:13,960 --> 00:38:18,240
away from all this this work so far is

1031
00:38:16,079 --> 00:38:19,599
that we can make functioning neural

1032
00:38:18,240 --> 00:38:21,839
networks out of physical Hardware

1033
00:38:19,599 --> 00:38:24,760
without forcing the hardware to do sort

1034
00:38:21,839 --> 00:38:26,880
of very specific mathematical operations

1035
00:38:24,760 --> 00:38:28,520
we've given a training procedure I want

1036
00:38:26,880 --> 00:38:30,119
say that it's only training procedure or

1037
00:38:28,520 --> 00:38:32,520
the best training procedure but it's a

1038
00:38:30,119 --> 00:38:35,480
way uh that we were able to train these

1039
00:38:32,520 --> 00:38:37,079
things and the system doesn't need to be

1040
00:38:35,480 --> 00:38:38,440
Noise free to work well given that I

1041
00:38:37,079 --> 00:38:40,560
showed you experimental results that

1042
00:38:38,440 --> 00:38:42,560
took place in a real lab uh that's

1043
00:38:40,560 --> 00:38:44,000
hopefully convincing and we of course

1044
00:38:42,560 --> 00:38:45,880
also have the simulations that sort of

1045
00:38:44,000 --> 00:38:49,400
back up about how that works as it

1046
00:38:45,880 --> 00:38:51,640
scales U and yeah you can work with

1047
00:38:49,400 --> 00:38:53,839
pretty imperfectly calibrated hardware

1048
00:38:51,640 --> 00:38:55,720
and uh this transfer is possible so this

1049
00:38:53,839 --> 00:38:58,880
sort of covers well it's feasible to do

1050
00:38:55,720 --> 00:39:01,160
this it's General across a a fairly wide

1051
00:38:58,880 --> 00:39:04,680
variety of devices the training is

1052
00:39:01,160 --> 00:39:06,079
efficient because we use backr and uh

1053
00:39:04,680 --> 00:39:07,560
these things look like they could be

1054
00:39:06,079 --> 00:39:09,800
then manufacturable because you can

1055
00:39:07,560 --> 00:39:11,760
tolerate these

1056
00:39:09,800 --> 00:39:13,000
variations so now I want to tell you

1057
00:39:11,760 --> 00:39:15,160
about some work we've done over the last

1058
00:39:13,000 --> 00:39:18,000
year or two uh that recently came out as

1059
00:39:15,160 --> 00:39:21,040
a pre-print that is on a specific

1060
00:39:18,000 --> 00:39:23,319
photonic realization um where we look to

1061
00:39:21,040 --> 00:39:24,760
scale up uh photonic neural network

1062
00:39:23,319 --> 00:39:26,480
onchip photonic neural networks using

1063
00:39:24,760 --> 00:39:28,440
this sort of style of of thinking about

1064
00:39:26,480 --> 00:39:31,280
things

1065
00:39:28,440 --> 00:39:32,920
so uh many of you may know already uh

1066
00:39:31,280 --> 00:39:34,839
there's a large effort at onchip phonic

1067
00:39:32,920 --> 00:39:36,640
neural networks that was really

1068
00:39:34,839 --> 00:39:39,240
pioneered here this is this paper is

1069
00:39:36,640 --> 00:39:41,920
from Durk England and Marine solutes

1070
00:39:39,240 --> 00:39:43,960
groups uh from seven years ago now where

1071
00:39:41,920 --> 00:39:46,440
they performed a matrix Vector

1072
00:39:43,960 --> 00:39:48,760
multiplication on an in input Vector

1073
00:39:46,440 --> 00:39:50,359
with of Dimension four to an output D

1074
00:39:48,760 --> 00:39:53,280
Vector of Dimension four using an array

1075
00:39:50,359 --> 00:39:56,200
of Max Ender

1076
00:39:53,280 --> 00:39:58,000
interferometers and in the sort of

1077
00:39:56,200 --> 00:39:59,560
intervening seven years people have been

1078
00:39:58,000 --> 00:40:01,280
pushing the scale of this but they found

1079
00:39:59,560 --> 00:40:02,920
it really really difficult uh so if we

1080
00:40:01,280 --> 00:40:04,960
look at the state-ofthe-art and input

1081
00:40:02,920 --> 00:40:07,280
Vector Dimension and output Vector

1082
00:40:04,960 --> 00:40:08,560
Dimension this original work was there

1083
00:40:07,280 --> 00:40:10,839
and sort of some people have managed to

1084
00:40:08,560 --> 00:40:13,599
push up a little bit and push a little

1085
00:40:10,839 --> 00:40:15,960
bit on the side um but it's nowhere near

1086
00:40:13,599 --> 00:40:17,599
as big as we would like and no one ever

1087
00:40:15,960 --> 00:40:19,240
shows you a plot like that without you

1088
00:40:17,599 --> 00:40:20,280
know sooner or later in the presentation

1089
00:40:19,240 --> 00:40:21,800
we're going to fill up we're going to

1090
00:40:20,280 --> 00:40:23,319
fill in a new point in this in this

1091
00:40:21,800 --> 00:40:25,800
plane

1092
00:40:23,319 --> 00:40:26,839
somewhere uh and we really tried to be

1093
00:40:25,800 --> 00:40:28,960
fair in this plot though this is

1094
00:40:26,839 --> 00:40:30,760
including many of the sort of most or as

1095
00:40:28,960 --> 00:40:32,760
far as we know all of the M the main

1096
00:40:30,760 --> 00:40:35,119
recent results in this

1097
00:40:32,760 --> 00:40:37,000
field so we'd like to scale these but

1098
00:40:35,119 --> 00:40:38,440
it's proven really difficult and part of

1099
00:40:37,000 --> 00:40:40,000
part of there's many reasons for it part

1100
00:40:38,440 --> 00:40:42,400
of something to notice here is the scale

1101
00:40:40,000 --> 00:40:44,520
bar that the these these mzi devices are

1102
00:40:42,400 --> 00:40:47,319
not that small 60 microns in the scale

1103
00:40:44,520 --> 00:40:50,560
by there so a really interesting idea

1104
00:40:47,319 --> 00:40:52,440
was proposed in 2019 uh from sham FS

1105
00:40:50,560 --> 00:40:55,200
group at Stanford and one of his former

1106
00:40:52,440 --> 00:40:56,960
students Z for you at at Wisconsin they

1107
00:40:55,200 --> 00:40:59,560
both simultaneously proposed this idea

1108
00:40:56,960 --> 00:41:01,680
of If instead of we make a mesh of mzis

1109
00:40:59,560 --> 00:41:03,520
or a crossb phase change memories or one

1110
00:41:01,680 --> 00:41:05,480
of the many other approaches people have

1111
00:41:03,520 --> 00:41:06,960
taken to building onchip neural networks

1112
00:41:05,480 --> 00:41:08,720
what happens if you give me a slab of

1113
00:41:06,960 --> 00:41:11,319
material and you just let me program the

1114
00:41:08,720 --> 00:41:12,599
refractive index of it arbitrarily uh

1115
00:41:11,319 --> 00:41:14,319
you'll get these sort of very weird

1116
00:41:12,599 --> 00:41:16,640
patterns that control the light in weird

1117
00:41:14,319 --> 00:41:18,800
ways but you can train it to do

1118
00:41:16,640 --> 00:41:22,880
inference it was a really fascinating

1119
00:41:18,800 --> 00:41:24,280
idea but they left open a major question

1120
00:41:22,880 --> 00:41:26,200
which is they showed that in simulation

1121
00:41:24,280 --> 00:41:28,400
this will work but they didn't say how

1122
00:41:26,200 --> 00:41:30,000
you you could make it device who

1123
00:41:28,400 --> 00:41:31,000
refective in you can control is a

1124
00:41:30,000 --> 00:41:34,119
function of

1125
00:41:31,000 --> 00:41:35,760
space um so how can you do that how can

1126
00:41:34,119 --> 00:41:37,880
you make a device that's like a slab

1127
00:41:35,760 --> 00:41:40,640
wave guide shown here where you can

1128
00:41:37,880 --> 00:41:43,280
control uh this uh this map of

1129
00:41:40,640 --> 00:41:47,640
refractive index and that's what we said

1130
00:41:43,280 --> 00:41:49,000
out to do so uh we we recently yeah

1131
00:41:47,640 --> 00:41:50,720
worked on this and did an experimental

1132
00:41:49,000 --> 00:41:52,240
demonstration where we we at least

1133
00:41:50,720 --> 00:41:53,599
provide our solution to this problem

1134
00:41:52,240 --> 00:41:55,599
there are other people working on it as

1135
00:41:53,599 --> 00:41:58,000
well but our solution is as following so

1136
00:41:55,599 --> 00:41:59,960
we start with the Silicon substrate and

1137
00:41:58,000 --> 00:42:01,880
we add a slab wave guide oh we're going

1138
00:41:59,960 --> 00:42:03,119
to go back to like having missing boxes

1139
00:42:01,880 --> 00:42:04,560
again which should be interesting but

1140
00:42:03,119 --> 00:42:08,079
the most important part here is the red

1141
00:42:04,560 --> 00:42:10,359
bit uh which is a slab lithium Nate wave

1142
00:42:08,079 --> 00:42:12,000
guide surrounded by cladding so that's

1143
00:42:10,359 --> 00:42:14,000
where the light is going to that we're

1144
00:42:12,000 --> 00:42:15,640
we're trying to do computation on is

1145
00:42:14,000 --> 00:42:17,480
going to propagate and then we're going

1146
00:42:15,640 --> 00:42:20,440
to add a big chunk of photoconductor on

1147
00:42:17,480 --> 00:42:22,200
top of that and then an electrode and

1148
00:42:20,440 --> 00:42:24,440
we're want the electrode because we're

1149
00:42:22,200 --> 00:42:27,359
going to bias this with the

1150
00:42:24,440 --> 00:42:29,200
voltage and why does this let us control

1151
00:42:27,359 --> 00:42:30,760
refractive index of this red wave guide

1152
00:42:29,200 --> 00:42:33,559
as a function of

1153
00:42:30,760 --> 00:42:35,359
space uh essentially the answer is we're

1154
00:42:33,559 --> 00:42:37,680
going to use the electrooptic effect in

1155
00:42:35,359 --> 00:42:39,400
the lithium nabate but in a slightly

1156
00:42:37,680 --> 00:42:43,079
weird way that if we look at a patch of

1157
00:42:39,400 --> 00:42:44,640
this uh uh of this device and we sort of

1158
00:42:43,079 --> 00:42:46,240
yeah go down taking a cross-section of

1159
00:42:44,640 --> 00:42:49,240
it what we see is that effectively we

1160
00:42:46,240 --> 00:42:51,960
have a circuit model where we have some

1161
00:42:49,240 --> 00:42:53,720
uh resistance or really it's impedance

1162
00:42:51,960 --> 00:42:55,440
but you can think about it resistance of

1163
00:42:53,720 --> 00:42:57,319
the photoconductor and the resistance of

1164
00:42:55,440 --> 00:42:59,200
the uh the wave guide and so this a

1165
00:42:57,319 --> 00:43:01,200
resistive divider effect for this bias

1166
00:42:59,200 --> 00:43:02,400
voltage that's put on about half the

1167
00:43:01,200 --> 00:43:03,760
voltage is going to drop over the

1168
00:43:02,400 --> 00:43:06,400
photoconductor and half of it is going

1169
00:43:03,760 --> 00:43:08,960
to drop over the over the uh the lithium

1170
00:43:06,400 --> 00:43:10,720
dibate wave guide which we will Define

1171
00:43:08,960 --> 00:43:12,160
as that having no refractive index

1172
00:43:10,720 --> 00:43:14,200
contrast now you just sort of get this

1173
00:43:12,160 --> 00:43:15,960
extra phase shift everywhere in the in

1174
00:43:14,200 --> 00:43:19,319
the wave

1175
00:43:15,960 --> 00:43:21,160
guide now imagine you shine light onto

1176
00:43:19,319 --> 00:43:23,400
this wave guide from above with a

1177
00:43:21,160 --> 00:43:25,520
pattern and this pattern is going to be

1178
00:43:23,400 --> 00:43:27,800
directly transduced into a refractive

1179
00:43:25,520 --> 00:43:29,880
index pattern in the way it works is

1180
00:43:27,800 --> 00:43:31,280
that wherever you have like for example

1181
00:43:29,880 --> 00:43:34,040
this region here that we've sort of done

1182
00:43:31,280 --> 00:43:35,720
this inset to we wherever you have light

1183
00:43:34,040 --> 00:43:38,280
the photoconductor will absorb the light

1184
00:43:35,720 --> 00:43:39,920
by definition it will increase its

1185
00:43:38,280 --> 00:43:41,839
conductance or reduce its

1186
00:43:39,920 --> 00:43:43,559
resistance and now your resistive

1187
00:43:41,839 --> 00:43:45,400
divider will have a smaller resistor for

1188
00:43:43,559 --> 00:43:48,160
the photoconductor part and so you'll

1189
00:43:45,400 --> 00:43:51,119
drop more of the voltage over the uh the

1190
00:43:48,160 --> 00:43:53,319
wave guide and through the electrooptic

1191
00:43:51,119 --> 00:43:57,640
effect you will now induce a phase shift

1192
00:43:53,319 --> 00:43:59,079
at that part of the the wave guide um

1193
00:43:57,640 --> 00:44:00,480
in the in the work that we're presenting

1194
00:43:59,079 --> 00:44:02,640
we've achieved a refractive index

1195
00:44:00,480 --> 00:44:05,000
contrast of 10 Theus 3 we think it

1196
00:44:02,640 --> 00:44:07,119
should be possible to push this to 10us

1197
00:44:05,000 --> 00:44:09,079
2 this is enough that we can do an

1198
00:44:07,119 --> 00:44:10,920
experiment like this where we shine the

1199
00:44:09,079 --> 00:44:13,480
Pat this is literally a pattern that we

1200
00:44:10,920 --> 00:44:16,240
shine like an image onto the top of our

1201
00:44:13,480 --> 00:44:18,160
chip uh and we pattern we pattern this

1202
00:44:16,240 --> 00:44:20,079
sort of pattern of this Fork shape and

1203
00:44:18,160 --> 00:44:21,920
we send in a beam in the middle and what

1204
00:44:20,079 --> 00:44:24,440
we see comes out we come out this is

1205
00:44:21,920 --> 00:44:26,359
experimental data uh we see two beams

1206
00:44:24,440 --> 00:44:27,720
coming out so this is showing that you

1207
00:44:26,359 --> 00:44:30,520
can effectively make like a beam

1208
00:44:27,720 --> 00:44:32,599
splitter device uh in this way so this

1209
00:44:30,520 --> 00:44:35,640
is showing that the the the physical

1210
00:44:32,599 --> 00:44:37,599
mechanism is working uh this shows a a

1211
00:44:35,640 --> 00:44:39,240
picture of our our chip and we're

1212
00:44:37,599 --> 00:44:40,839
shining this pattern from above you can

1213
00:44:39,240 --> 00:44:43,319
see sort of the pattern here it's like

1214
00:44:40,839 --> 00:44:44,880
below ey resolution but you can see the

1215
00:44:43,319 --> 00:44:46,079
green light at least so the green the

1216
00:44:44,880 --> 00:44:48,160
green light is what we're sending under

1217
00:44:46,079 --> 00:44:49,760
the photoconductor then we have infrared

1218
00:44:48,160 --> 00:44:53,079
light coming in and out is the light

1219
00:44:49,760 --> 00:44:56,000
that the computation is being performed

1220
00:44:53,079 --> 00:44:57,760
on you can imagine using this platform

1221
00:44:56,000 --> 00:44:59,319
not just to make beam split

1222
00:44:57,760 --> 00:45:01,040
or other programmable circuits which is

1223
00:44:59,319 --> 00:45:02,599
indeed something you could do but you

1224
00:45:01,040 --> 00:45:04,800
could use it to perform machine learning

1225
00:45:02,599 --> 00:45:08,359
in the same sort of sense as the mzi

1226
00:45:04,800 --> 00:45:10,000
array chips where uh we're now going to

1227
00:45:08,359 --> 00:45:12,000
have this pattern that is going to

1228
00:45:10,000 --> 00:45:14,119
encode our parameters of our neural

1229
00:45:12,000 --> 00:45:15,480
network and we're going to perform a

1230
00:45:14,119 --> 00:45:17,040
gradient descent update of those

1231
00:45:15,480 --> 00:45:19,000
parameters using the physics aware

1232
00:45:17,040 --> 00:45:22,000
training sort of hybrid Loop that I

1233
00:45:19,000 --> 00:45:24,040
showed you in the last section uh so you

1234
00:45:22,000 --> 00:45:25,720
could imagine encoding an image into the

1235
00:45:24,040 --> 00:45:29,440
input Vector you have to do some sort of

1236
00:45:25,720 --> 00:45:31,480
unfolding to get it into one D and then

1237
00:45:29,440 --> 00:45:33,559
uh trade it so that when you look get it

1238
00:45:31,480 --> 00:45:35,720
get the output that it's sort of peaked

1239
00:45:33,559 --> 00:45:38,040
at uh at two because you put in an image

1240
00:45:35,720 --> 00:45:39,880
of two uh and this is really happening

1241
00:45:38,040 --> 00:45:42,680
through this physics of wave propagation

1242
00:45:39,880 --> 00:45:45,040
through the wave guide uh where the the

1243
00:45:42,680 --> 00:45:47,000
uh the the sort of refractive index

1244
00:45:45,040 --> 00:45:49,400
modulations are the things that are

1245
00:45:47,000 --> 00:45:51,480
changing how the the waves from

1246
00:45:49,400 --> 00:45:53,559
different uh input digits are being

1247
00:45:51,480 --> 00:45:55,040
changed uh so if you put in a different

1248
00:45:53,559 --> 00:45:58,400
digit you should get a different pattern

1249
00:45:55,040 --> 00:46:00,680
of of wave propagation coming out

1250
00:45:58,400 --> 00:46:03,040
so we we tested this on amest hand and

1251
00:46:00,680 --> 00:46:05,800
digits uh with a variation so not the

1252
00:46:03,040 --> 00:46:07,760
full size images but 7 by seven down

1253
00:46:05,800 --> 00:46:09,800
samplings which makes the task harder

1254
00:46:07,760 --> 00:46:12,880
it's more difficult to identify the

1255
00:46:09,800 --> 00:46:14,440
image when you when you shrink it um and

1256
00:46:12,880 --> 00:46:16,760
we feed it into the system by just sort

1257
00:46:14,440 --> 00:46:19,280
of reshaping this array to 1D so it's a

1258
00:46:16,760 --> 00:46:20,400
49 dimensional input and a 10

1259
00:46:19,280 --> 00:46:22,839
dimensional output because we're just

1260
00:46:20,400 --> 00:46:26,319
trying to classify a single layer uh

1261
00:46:22,839 --> 00:46:30,480
what we what image we we had and we

1262
00:46:26,319 --> 00:46:30,480
achieved an accuracy in experiment of

1263
00:46:30,559 --> 00:46:38,040
86% and that puts us over here so we're

1264
00:46:34,720 --> 00:46:41,240
not anywhere near what a GPU could do

1265
00:46:38,040 --> 00:46:44,000
not by many many orders of magnitude um

1266
00:46:41,240 --> 00:46:46,520
but we've really managed to push uh

1267
00:46:44,000 --> 00:46:48,040
where these how big the the computation

1268
00:46:46,520 --> 00:46:51,960
is that the onip platform can do in

1269
00:46:48,040 --> 00:46:54,400
these spatially multiplexed examples U

1270
00:46:51,960 --> 00:46:55,680
and this is really in large part enabled

1271
00:46:54,400 --> 00:46:57,960
by the fact that we sort of got rid of

1272
00:46:55,680 --> 00:47:00,599
the abstraction of these disc components

1273
00:46:57,960 --> 00:47:02,760
and just directly program the

1274
00:47:00,599 --> 00:47:04,559
physics so yeah so what what sort of

1275
00:47:02,760 --> 00:47:07,240
principles are this that we did we use

1276
00:47:04,559 --> 00:47:08,800
in this example well we didn't insist on

1277
00:47:07,240 --> 00:47:10,680
exactly performing a very specific

1278
00:47:08,800 --> 00:47:12,119
Matrix Vector multiplication we just

1279
00:47:10,680 --> 00:47:15,559
sort of trained on the parameters that

1280
00:47:12,119 --> 00:47:17,520
we could control um we are doing

1281
00:47:15,559 --> 00:47:19,440
inmemory compute the the parameters are

1282
00:47:17,520 --> 00:47:21,839
sort of sitting in the wave guide so to

1283
00:47:19,440 --> 00:47:23,640
speak um and it's it's using an

1284
00:47:21,839 --> 00:47:26,720
alternative hydrid it's using photonics

1285
00:47:23,640 --> 00:47:29,000
and we trained on a specific device so

1286
00:47:26,720 --> 00:47:30,440
uh we're tolerant to the fact that the

1287
00:47:29,000 --> 00:47:32,880
devices that come out of our key room

1288
00:47:30,440 --> 00:47:35,440
that the the student in postto who built

1289
00:47:32,880 --> 00:47:36,839
this make is a little different each

1290
00:47:35,440 --> 00:47:39,800
time but that's okay we have a training

1291
00:47:36,839 --> 00:47:42,520
procedure that can that can handle

1292
00:47:39,800 --> 00:47:45,200
that so now in the last let's say five

1293
00:47:42,520 --> 00:47:47,680
minutes or so I'd like to talk about

1294
00:47:45,200 --> 00:47:50,319
some prospects for where I see this uh

1295
00:47:47,680 --> 00:47:52,880
General field so to speak going um the

1296
00:47:50,319 --> 00:47:55,240
main things I'd like to touch on are uh

1297
00:47:52,880 --> 00:47:57,119
a near-term applications so clearly

1298
00:47:55,240 --> 00:47:59,880
Building A system that can beat a GP pu

1299
00:47:57,119 --> 00:48:01,200
at doing running chat GPT is extremely

1300
00:47:59,880 --> 00:48:03,800
difficult and we're not going to do that

1301
00:48:01,200 --> 00:48:05,440
anytime soon uh so what can we do in the

1302
00:48:03,800 --> 00:48:08,319
inters I think smart senses are a really

1303
00:48:05,440 --> 00:48:10,559
good answer to that I'll also talk a

1304
00:48:08,319 --> 00:48:12,200
little bit about uh how we might imagine

1305
00:48:10,559 --> 00:48:15,400
building large scale neural network

1306
00:48:12,200 --> 00:48:17,640
accelerators that uh connects with the

1307
00:48:15,400 --> 00:48:19,520
the Nano theme of this this seminar and

1308
00:48:17,640 --> 00:48:21,640
then I'll say a couple of words about

1309
00:48:19,520 --> 00:48:24,480
possible extensions to to Quantum neural

1310
00:48:21,640 --> 00:48:25,760
networks so smart sensors of this notion

1311
00:48:24,480 --> 00:48:27,200
of you can have a sensor that is

1312
00:48:25,760 --> 00:48:30,680
integrated with the neural nwor that

1313
00:48:27,200 --> 00:48:32,960
sort of pre-processes the data and this

1314
00:48:30,680 --> 00:48:34,440
kind of physics based or physical

1315
00:48:32,960 --> 00:48:35,760
Computing Paradigm is very well suited

1316
00:48:34,440 --> 00:48:36,720
to this because you may have many

1317
00:48:35,760 --> 00:48:38,240
different things that you're trying to

1318
00:48:36,720 --> 00:48:40,960
sense that are not electronic they could

1319
00:48:38,240 --> 00:48:42,920
be photons I.E Optics they could be sort

1320
00:48:40,960 --> 00:48:45,599
of acoustic waves or mechanical forces

1321
00:48:42,920 --> 00:48:47,480
Etc and a natural Paradigm for

1322
00:48:45,599 --> 00:48:49,720
pre-processing this is first make an

1323
00:48:47,480 --> 00:48:52,200
invisible gray box that has a physical

1324
00:48:49,720 --> 00:48:53,440
system that is the same like modality as

1325
00:48:52,200 --> 00:48:55,119
this so if it's photons and you should

1326
00:48:53,440 --> 00:48:57,240
have some Optical system that

1327
00:48:55,119 --> 00:48:59,680
pre-processes to extract the relevant

1328
00:48:57,240 --> 00:49:01,599
information before you transduce

1329
00:48:59,680 --> 00:49:04,319
digitize and then do further

1330
00:49:01,599 --> 00:49:06,280
processing and as one example of this we

1331
00:49:04,319 --> 00:49:09,000
recently well not recently I guess the

1332
00:49:06,280 --> 00:49:12,520
paper came out about a year ago now uh

1333
00:49:09,000 --> 00:49:15,760
did an example where we uh used a two-

1334
00:49:12,520 --> 00:49:17,559
layer Optical neural network to extract

1335
00:49:15,760 --> 00:49:19,040
the relevant features of a 3D printed

1336
00:49:17,559 --> 00:49:20,440
scene where we have a traffic sign and

1337
00:49:19,040 --> 00:49:22,119
we're trying to extract what is the

1338
00:49:20,440 --> 00:49:23,680
speed limit on that sign you don't care

1339
00:49:22,119 --> 00:49:26,079
about all the sort of great details of

1340
00:49:23,680 --> 00:49:27,640
this image you just care about what are

1341
00:49:26,079 --> 00:49:29,760
the numbers on this side and we were

1342
00:49:27,640 --> 00:49:32,240
able to achieve a compression RTI of 400

1343
00:49:29,760 --> 00:49:34,400
to1 where you can digitize just four

1344
00:49:32,240 --> 00:49:36,760
pixels and are then able to accurately

1345
00:49:34,400 --> 00:49:38,520
classify what the speed limit is uh so

1346
00:49:36,760 --> 00:49:39,960
this is an example of sort of doing this

1347
00:49:38,520 --> 00:49:42,240
pre-processing you're not doing the full

1348
00:49:39,960 --> 00:49:44,000
thing in the physical system but you're

1349
00:49:42,240 --> 00:49:47,640
greatly reducing the bandwidth that you

1350
00:49:44,000 --> 00:49:49,359
have to digitize uh and and postprocess

1351
00:49:47,640 --> 00:49:51,079
uh by doing this sort of pre-processing

1352
00:49:49,359 --> 00:49:53,319
in the analog

1353
00:49:51,079 --> 00:49:55,200
domain you could imagine playing the

1354
00:49:53,319 --> 00:49:56,720
same game with u with microwaves it's

1355
00:49:55,200 --> 00:49:58,559
not specific to Optics and that's

1356
00:49:56,720 --> 00:50:02,480
something we're we're working on with a

1357
00:49:58,559 --> 00:50:05,200
mixed signal CS group where sort of

1358
00:50:02,480 --> 00:50:07,240
heris you can think about like wideband

1359
00:50:05,200 --> 00:50:08,440
radar having 100 gz of bandwidth it's a

1360
00:50:07,240 --> 00:50:10,920
little bit weird that you have to

1361
00:50:08,440 --> 00:50:12,720
digitize 200 gigas samples a second in

1362
00:50:10,920 --> 00:50:14,440
order to tell a very few number of bits

1363
00:50:12,720 --> 00:50:17,200
so like What airplane is it and how fast

1364
00:50:14,440 --> 00:50:19,040
is it moving um it feels like a natural

1365
00:50:17,200 --> 00:50:21,880
place where you can sort of benefit from

1366
00:50:19,040 --> 00:50:26,040
from trying to pre-process in the analog

1367
00:50:21,880 --> 00:50:27,680
domain we're also really interested in

1368
00:50:26,040 --> 00:50:31,880
even though it's really difficult how

1369
00:50:27,680 --> 00:50:33,839
you can make uh analog electronic neural

1370
00:50:31,880 --> 00:50:36,599
network processes that might ultimately

1371
00:50:33,839 --> 00:50:39,079
actually be able to beat gpus at at at

1372
00:50:36,599 --> 00:50:41,359
running chat GPT it's really really

1373
00:50:39,079 --> 00:50:45,520
challenging but we're inspired by the

1374
00:50:41,359 --> 00:50:47,839
fact that the human brain has a vastly

1375
00:50:45,520 --> 00:50:50,280
larger number of neurons and parameters

1376
00:50:47,839 --> 00:50:51,920
parameters synapses than artificial

1377
00:50:50,280 --> 00:50:53,760
neural networks and yet only uses 12

1378
00:50:51,920 --> 00:50:54,880
wats of power so it feels like in

1379
00:50:53,760 --> 00:50:56,839
principle

1380
00:50:54,880 --> 00:50:59,079
there's we should be able to do much

1381
00:50:56,839 --> 00:51:00,559
better than we're currently doing now

1382
00:50:59,079 --> 00:51:03,440
and so there's a bunch of strategies you

1383
00:51:00,559 --> 00:51:04,760
can take um many of which are not kind

1384
00:51:03,440 --> 00:51:06,319
of unique to us people have been

1385
00:51:04,760 --> 00:51:08,720
thinking about these for for a long time

1386
00:51:06,319 --> 00:51:11,280
of like lower the voltage operated

1387
00:51:08,720 --> 00:51:13,200
analog U you can think about tolerating

1388
00:51:11,280 --> 00:51:16,000
noisy components maybe using exotic

1389
00:51:13,200 --> 00:51:19,559
Hardware Etc something that I think is

1390
00:51:16,000 --> 00:51:21,839
actually an idea that uh that is really

1391
00:51:19,559 --> 00:51:24,319
interesting uh but that is something

1392
00:51:21,839 --> 00:51:26,200
that people have kind of uh ignored

1393
00:51:24,319 --> 00:51:27,920
until ignored for a long time is the

1394
00:51:26,200 --> 00:51:30,280
notion that you can you don't need

1395
00:51:27,920 --> 00:51:31,880
updatable memory anymore and this is

1396
00:51:30,280 --> 00:51:34,400
something that's a bit new in The Last 5

1397
00:51:31,880 --> 00:51:36,040
Years uh five years ago if you wanted

1398
00:51:34,400 --> 00:51:37,240
neural network accelerator is almost by

1399
00:51:36,040 --> 00:51:38,880
definition that you must be able to

1400
00:51:37,240 --> 00:51:41,400
change what neural network it

1401
00:51:38,880 --> 00:51:43,240
runs um because you want your your chip

1402
00:51:41,400 --> 00:51:46,040
to be able to do many different things

1403
00:51:43,240 --> 00:51:47,640
but uh this has changed now that we have

1404
00:51:46,040 --> 00:51:49,839
what people are often calling Foundation

1405
00:51:47,640 --> 00:51:52,240
models things like GPT where it's

1406
00:51:49,839 --> 00:51:54,720
trained on the entirety of the internet

1407
00:51:52,240 --> 00:51:57,000
and open AI brings out a new GPT model

1408
00:51:54,720 --> 00:51:58,720
only every one to two years and that

1409
00:51:57,000 --> 00:52:00,240
model can do many different tasks and

1410
00:51:58,720 --> 00:52:02,799
you can sort of post tweak to do many

1411
00:52:00,240 --> 00:52:04,640
different tasks it's maybe not so stupid

1412
00:52:02,799 --> 00:52:06,119
to think about can you or should you

1413
00:52:04,640 --> 00:52:08,079
make a neural network accelerator that's

1414
00:52:06,119 --> 00:52:10,240
sort of printed it's like a CD R you

1415
00:52:08,079 --> 00:52:11,920
print it that has the latest model on it

1416
00:52:10,240 --> 00:52:13,200
and you use it for a year or two and

1417
00:52:11,920 --> 00:52:16,200
then you throw it out and you buy a new

1418
00:52:13,200 --> 00:52:19,880
one uh the next model comes out and if

1419
00:52:16,200 --> 00:52:22,000
you do that uh if you don't need uh

1420
00:52:19,880 --> 00:52:23,680
programmable memory anymore that is

1421
00:52:22,000 --> 00:52:25,799
greatly reducing a constraint on your

1422
00:52:23,680 --> 00:52:29,000
engineering so you can maybe uh get

1423
00:52:25,799 --> 00:52:31,440
large factors of of of benefited both

1424
00:52:29,000 --> 00:52:34,599
speeded energy by by having sort of

1425
00:52:31,440 --> 00:52:36,559
printed prefabricated neural networks so

1426
00:52:34,599 --> 00:52:38,359
a couple of ideas for sort of just

1427
00:52:36,559 --> 00:52:40,520
concepts of how one might benefit from

1428
00:52:38,359 --> 00:52:42,280
this idea one is people have built these

1429
00:52:40,520 --> 00:52:44,920
crossbar arrays where they use memorist

1430
00:52:42,280 --> 00:52:47,280
of devices uh but the transistors that

1431
00:52:44,920 --> 00:52:48,559
you use to drive the currents to uh to

1432
00:52:47,280 --> 00:52:50,240
update the weights are actually pretty

1433
00:52:48,559 --> 00:52:51,960
large and so the unit cell sizes are

1434
00:52:50,240 --> 00:52:53,920
pretty large and people are also

1435
00:52:51,960 --> 00:52:55,119
insisting that this thing do exactly a

1436
00:52:53,920 --> 00:52:57,319
matrix Vector

1437
00:52:55,119 --> 00:52:59,040
multiplication you can maybe relax both

1438
00:52:57,319 --> 00:53:01,599
those assumptions and then shrink the

1439
00:52:59,040 --> 00:53:03,160
unit cell size and uh there's probably

1440
00:53:01,599 --> 00:53:06,240
potential for sort of a few orders of

1441
00:53:03,160 --> 00:53:09,559
magnitude win from doing that another

1442
00:53:06,240 --> 00:53:12,880
kind of even more exotic idea is to

1443
00:53:09,559 --> 00:53:15,799
maybe try and make some uh some Nano

1444
00:53:12,880 --> 00:53:20,400
electronic material where you patn the

1445
00:53:15,799 --> 00:53:22,240
the material uh in in 3D or 2D space uh

1446
00:53:20,400 --> 00:53:24,520
and if you just sort of do a counting

1447
00:53:22,240 --> 00:53:26,760
argument of like if how many voxels you

1448
00:53:24,520 --> 00:53:29,400
have when you patent the conductivity of

1449
00:53:26,760 --> 00:53:33,160
a 2d 3D material versus some sort of

1450
00:53:29,400 --> 00:53:34,920
reasonable size of it um you can imagine

1451
00:53:33,160 --> 00:53:38,640
really getting a very very large number

1452
00:53:34,920 --> 00:53:40,799
of parameters uh embedded in your your

1453
00:53:38,640 --> 00:53:44,200
system but they're fixed uh but that may

1454
00:53:40,799 --> 00:53:45,720
be okay uh so one uh one concrete idea

1455
00:53:44,200 --> 00:53:47,160
for how to maybe try something like this

1456
00:53:45,720 --> 00:53:50,000
in the near term is Imagine taking a

1457
00:53:47,160 --> 00:53:52,839
slab of silicon and then patterning the

1458
00:53:50,000 --> 00:53:55,480
the P andn type dopet uh in a completely

1459
00:53:52,839 --> 00:53:57,040
programmable way and if this diagram

1460
00:53:55,480 --> 00:53:58,880
looks a little bit familiar

1461
00:53:57,040 --> 00:54:00,760
we were inspired to do this by the work

1462
00:53:58,880 --> 00:54:03,000
at the photonic side like can we take

1463
00:54:00,760 --> 00:54:04,720
the photonic reprogrammable refractive

1464
00:54:03,000 --> 00:54:06,839
index idea and do the same kind of thing

1465
00:54:04,720 --> 00:54:08,440
in Nano Electronics I think the answer

1466
00:54:06,839 --> 00:54:09,880
is yes I'm a new postto who will be

1467
00:54:08,440 --> 00:54:12,559
starting working on this in a couple of

1468
00:54:09,880 --> 00:54:13,880
months and uh so sort of check back in

1469
00:54:12,559 --> 00:54:15,760
this space in a couple of years and

1470
00:54:13,880 --> 00:54:17,280
we'll see if we see if we succeed it but

1471
00:54:15,760 --> 00:54:18,920
I'm really really excited about this

1472
00:54:17,280 --> 00:54:20,839
direction I think there's lots of

1473
00:54:18,920 --> 00:54:24,680
different ways you could try Do

1474
00:54:20,839 --> 00:54:26,640
It um I will say epsilon about Quantum

1475
00:54:24,680 --> 00:54:27,760
neural networks which is a very natural

1476
00:54:26,640 --> 00:54:29,000
thought for at least someone from my

1477
00:54:27,760 --> 00:54:31,000
background or I came from the quantum

1478
00:54:29,000 --> 00:54:32,440
Optics background is or Quantum

1479
00:54:31,000 --> 00:54:34,119
information background is can you play

1480
00:54:32,440 --> 00:54:35,520
the same game with Quantum physical

1481
00:54:34,119 --> 00:54:38,040
systems I've everything I've told you

1482
00:54:35,520 --> 00:54:39,720
about so far are classical but uh we

1483
00:54:38,040 --> 00:54:41,240
have the same story in quantumness of

1484
00:54:39,720 --> 00:54:43,119
people are thinking about sort of many

1485
00:54:41,240 --> 00:54:45,359
layers of abstraction to build up fa

1486
00:54:43,119 --> 00:54:47,880
tolerance and Universal Quantum

1487
00:54:45,359 --> 00:54:49,599
Computing but if you want to get sort of

1488
00:54:47,880 --> 00:54:53,319
wins in the near term maybe it makes

1489
00:54:49,599 --> 00:54:54,760
sense to uh play this game of try to do

1490
00:54:53,319 --> 00:54:56,799
the same thing we've done tastico like

1491
00:54:54,760 --> 00:54:59,520
replace the physical system with maybe

1492
00:54:56,799 --> 00:55:02,880
some dissipative maybe poorly calibrated

1493
00:54:59,520 --> 00:55:04,799
Quantum system and then uh and then try

1494
00:55:02,880 --> 00:55:06,160
train it in this way and maybe there's

1495
00:55:04,799 --> 00:55:08,079
an interesting question of like well

1496
00:55:06,160 --> 00:55:10,079
what should the equivalent of the laptop

1497
00:55:08,079 --> 00:55:12,040
be for training a Quantum system in this

1498
00:55:10,079 --> 00:55:14,319
way maybe it should be a standard

1499
00:55:12,040 --> 00:55:16,400
circuit model quantum computer uh maybe

1500
00:55:14,319 --> 00:55:18,079
it can be some sort of like

1501
00:55:16,400 --> 00:55:20,160
approximation of the Dynamics with like

1502
00:55:18,079 --> 00:55:22,000
some tenso Network approximation uh

1503
00:55:20,160 --> 00:55:24,039
we're still busy figuring this out this

1504
00:55:22,000 --> 00:55:25,280
is something we're actively working on

1505
00:55:24,039 --> 00:55:27,359
primarily in the super conducting

1506
00:55:25,280 --> 00:55:30,319
circuits domain

1507
00:55:27,359 --> 00:55:31,760
so that uh I should acknowledge the the

1508
00:55:30,319 --> 00:55:33,520
group and everybody who did the work as

1509
00:55:31,760 --> 00:55:35,760
well as the people who are funding uh

1510
00:55:33,520 --> 00:55:38,680
funding this work and I will leave you

1511
00:55:35,760 --> 00:55:40,079
with uh some thoughts of uh yeah we've

1512
00:55:38,680 --> 00:55:42,039
we've shown that you can make physical

1513
00:55:40,079 --> 00:55:44,559
neural networks and harness the natural

1514
00:55:42,039 --> 00:55:46,920
dynamics of many systems uh in our

1515
00:55:44,559 --> 00:55:48,760
onchip photonic neural network scheme

1516
00:55:46,920 --> 00:55:50,640
we've sort of proposed this new way to

1517
00:55:48,760 --> 00:55:52,760
make a programmable refractive index is

1518
00:55:50,640 --> 00:55:54,160
a function of space and this is seems to

1519
00:55:52,760 --> 00:55:57,480
be an interesting way to sort of push

1520
00:55:54,160 --> 00:55:59,760
the scaling of these uh onchip deves and

1521
00:55:57,480 --> 00:56:02,000
then uh sort of thinking towards the

1522
00:55:59,760 --> 00:56:03,280
future I think in sensor or smart sensor

1523
00:56:02,000 --> 00:56:05,559
processing is a really promising

1524
00:56:03,280 --> 00:56:06,880
near-term application but uh photonic

1525
00:56:05,559 --> 00:56:08,960
neural networks and Nano electronic

1526
00:56:06,880 --> 00:56:10,480
neural networks should be competitive if

1527
00:56:08,960 --> 00:56:12,079
you can make them large enough and so

1528
00:56:10,480 --> 00:56:15,079
big challeng is how are we going to

1529
00:56:12,079 --> 00:56:16,880
really be able to scale this um and uh

1530
00:56:15,079 --> 00:56:18,720
that's something I'm looking forward to

1531
00:56:16,880 --> 00:56:20,559
working on in the for the next the next

1532
00:56:18,720 --> 00:56:21,640
few years I think it should keep us busy

1533
00:56:20,559 --> 00:56:22,640
so thank you all so much for your

1534
00:56:21,640 --> 00:56:23,760
attention thank you for the questions

1535
00:56:22,640 --> 00:56:24,920
that we've already had and I look

1536
00:56:23,760 --> 00:56:27,079
forward to more questions and more

1537
00:56:24,920 --> 00:56:29,330
discussion after the uh uh after the

1538
00:56:27,079 --> 00:56:32,630
formal end of the

1539
00:56:29,330 --> 00:56:32,630
[Applause]

1540
00:56:33,000 --> 00:56:37,839
talk okay we'll take a few questions but

1541
00:56:36,160 --> 00:56:40,079
just a reminder there is going to be a

1542
00:56:37,839 --> 00:56:42,640
reception right after so uh for now

1543
00:56:40,079 --> 00:56:45,839
we'll take a couple questions hey Peter

1544
00:56:42,640 --> 00:56:47,880
thanks for the great talk so I I find

1545
00:56:45,839 --> 00:56:49,720
the need for the differential Digital

1546
00:56:47,880 --> 00:56:51,640
model to be like I don't know it's just

1547
00:56:49,720 --> 00:56:53,400
kind of unsatisfying so I'm wondering if

1548
00:56:51,640 --> 00:56:55,799
there's a way you can use the physics of

1549
00:56:53,400 --> 00:56:58,079
the system to get that differentiability

1550
00:56:55,799 --> 00:57:00,280
like using Loren's reciprocity in the

1551
00:56:58,079 --> 00:57:02,000
response to perturbation I don't know

1552
00:57:00,280 --> 00:57:03,599
just be interested in that it it

1553
00:57:02,000 --> 00:57:05,839
disturbs us too we would really like to

1554
00:57:03,599 --> 00:57:07,400
get rid of the laptop because for one

1555
00:57:05,839 --> 00:57:09,359
thing it means that we have no chance of

1556
00:57:07,400 --> 00:57:11,000
getting a like a speed or energy boost

1557
00:57:09,359 --> 00:57:12,720
in training and we would like to be able

1558
00:57:11,000 --> 00:57:15,440
to do that

1559
00:57:12,720 --> 00:57:17,520
um so there's a bunch of people thinking

1560
00:57:15,440 --> 00:57:20,440
about a bunch of different ways and in

1561
00:57:17,520 --> 00:57:22,039
in the Optics case Alex lovi's group at

1562
00:57:20,440 --> 00:57:24,839
Oxford for example has this interesting

1563
00:57:22,039 --> 00:57:26,319
proposal of for a very specific setting

1564
00:57:24,839 --> 00:57:27,960
if they send the light through in one

1565
00:57:26,319 --> 00:57:29,000
way it does the forward pass then they

1566
00:57:27,960 --> 00:57:32,559
can compute the eror and if they

1567
00:57:29,000 --> 00:57:34,839
literally send it backwards they can

1568
00:57:32,559 --> 00:57:37,240
engine excuse me they can engineer it so

1569
00:57:34,839 --> 00:57:38,799
that you get gradients but it's a very

1570
00:57:37,240 --> 00:57:40,200
specific setting and like the largest

1571
00:57:38,799 --> 00:57:41,799
demo they've managed to do is I think

1572
00:57:40,200 --> 00:57:43,319
like something like with three two

1573
00:57:41,799 --> 00:57:45,119
layers of three neurons or something

1574
00:57:43,319 --> 00:57:47,079
like this so it's really yeah it's

1575
00:57:45,119 --> 00:57:48,480
really hard the engineer is difficult

1576
00:57:47,079 --> 00:57:49,680
but it would be really nice if you could

1577
00:57:48,480 --> 00:57:52,480
make something like that but

1578
00:57:49,680 --> 00:57:55,160
generalizable and tolerant to to

1579
00:57:52,480 --> 00:57:56,799
variations uh Flor and marads had a few

1580
00:57:55,160 --> 00:57:59,400
really interesting ideas in this this

1581
00:57:56,799 --> 00:58:00,880
space as well uh yosu beno's group

1582
00:57:59,400 --> 00:58:02,680
brought out this notion of something

1583
00:58:00,880 --> 00:58:05,319
called equilibrium propagation for any

1584
00:58:02,680 --> 00:58:07,760
system that equilibri their their Theory

1585
00:58:05,319 --> 00:58:09,160
roughly holds um and that's also sort of

1586
00:58:07,760 --> 00:58:10,880
a local rule where you don't have to run

1587
00:58:09,160 --> 00:58:13,480
backrop on a laptop so there's a bunch

1588
00:58:10,880 --> 00:58:15,680
of ideas but nobody's yet sort of

1589
00:58:13,480 --> 00:58:18,480
stumbled upon like the one thing that's

1590
00:58:15,680 --> 00:58:20,480
clearly going to be the winner uh our P

1591
00:58:18,480 --> 00:58:22,319
method I I literally described it as

1592
00:58:20,480 --> 00:58:23,680
like it's a way to do it I'm certainly

1593
00:58:22,319 --> 00:58:28,119
wouldn't say it's the best or the end I

1594
00:58:23,680 --> 00:58:28,119
hope it's not the end great thank you

1595
00:58:31,680 --> 00:58:35,880
you thanks for the talk P um it's hard

1596
00:58:34,440 --> 00:58:38,640
not to listen and wonder about

1597
00:58:35,880 --> 00:58:41,359
applications in control so maybe you

1598
00:58:38,640 --> 00:58:43,799
could comment a little bit on that right

1599
00:58:41,359 --> 00:58:46,599
yeah uh I think uh there's this sort of

1600
00:58:43,799 --> 00:58:48,039
nice merger of uh sort of control

1601
00:58:46,599 --> 00:58:50,760
traditional control community and

1602
00:58:48,039 --> 00:58:52,760
reinforcement learning community and

1603
00:58:50,760 --> 00:58:54,280
this kind of stuff naturally plays in

1604
00:58:52,760 --> 00:58:57,000
with that because okay once you allow

1605
00:58:54,280 --> 00:58:58,880
yourself to do reinforcement learning

1606
00:58:57,000 --> 00:59:00,400
you can naturally imagine a physical

1607
00:58:58,880 --> 00:59:03,200
controller doing the reinforcement

1608
00:59:00,400 --> 00:59:06,079
learning um and this feels really

1609
00:59:03,200 --> 00:59:07,960
interesting for sort of very small scale

1610
00:59:06,079 --> 00:59:10,440
systems where you're very constrained in

1611
00:59:07,960 --> 00:59:13,039
the amount of power you have that said I

1612
00:59:10,440 --> 00:59:14,520
I dug into it a little bit in the case

1613
00:59:13,039 --> 00:59:15,760
of sort of micro robots because I had

1614
00:59:14,520 --> 00:59:17,039
the same thought I was like ah maybe

1615
00:59:15,760 --> 00:59:20,880
these people who are building sort of

1616
00:59:17,039 --> 00:59:22,880
really tiny robots could use this but it

1617
00:59:20,880 --> 00:59:25,440
seems really challenging because they

1618
00:59:22,880 --> 00:59:27,000
can already get the Coss processes that

1619
00:59:25,440 --> 00:59:29,319
do the control

1620
00:59:27,000 --> 00:59:32,559
down to like an amazingly small size and

1621
00:59:29,319 --> 00:59:34,599
power budget and so after speaking to a

1622
00:59:32,559 --> 00:59:36,319
few of them for a little bit I I I

1623
00:59:34,599 --> 00:59:38,680
became a little bit less optimistic that

1624
00:59:36,319 --> 00:59:41,319
there's like an easy win there but it

1625
00:59:38,680 --> 00:59:44,559
really does feel like ultimately you

1626
00:59:41,319 --> 00:59:46,880
should be able to benefit um it's just

1627
00:59:44,559 --> 00:59:48,839
yeah already the seos has already gone

1628
00:59:46,880 --> 00:59:51,559
there once that's happened it gets

1629
00:59:48,839 --> 00:59:51,559
really difficult to

1630
00:59:54,480 --> 01:00:01,960
win yeah and again very nice talk um

1631
00:59:58,400 --> 01:00:04,520
sort of coming from a similar space uh

1632
01:00:01,960 --> 01:00:05,720
with the sort of precompression

1633
01:00:04,520 --> 01:00:07,480
algorithms that you guys are

1634
01:00:05,720 --> 01:00:09,760
implementing

1635
01:00:07,480 --> 01:00:12,720
physically uh do you guys foresee those

1636
01:00:09,760 --> 01:00:14,280
needing to be reprogrammable I mean how

1637
01:00:12,720 --> 01:00:16,200
do you decide and especially like an

1638
01:00:14,280 --> 01:00:19,599
autonomous system if you're if you're

1639
01:00:16,200 --> 01:00:21,720
trying to pre-c compress sensor data uh

1640
01:00:19,599 --> 01:00:23,640
along which Dimensions to compress and

1641
01:00:21,720 --> 01:00:26,960
when those scenarios are appropriate and

1642
01:00:23,640 --> 01:00:29,039
then how do you then in turn you know

1643
01:00:26,960 --> 01:00:31,559
keep your system from being too

1644
01:00:29,039 --> 01:00:34,440
generalized you know how does this sort

1645
01:00:31,559 --> 01:00:36,839
of adaptability between very compressed

1646
01:00:34,440 --> 01:00:38,599
and not compressed at all come in right

1647
01:00:36,839 --> 01:00:41,839
right yeah so I think the the way we

1648
01:00:38,599 --> 01:00:43,559
think about it is largely like take any

1649
01:00:41,839 --> 01:00:45,480
like for example Machine Vision system

1650
01:00:43,559 --> 01:00:47,760
that is already using deep learning to

1651
01:00:45,480 --> 01:00:50,079
make inferences from the images this is

1652
01:00:47,760 --> 01:00:51,559
a natural setting to immediately do that

1653
01:00:50,079 --> 01:00:52,880
how generalized it should be it's like

1654
01:00:51,559 --> 01:00:54,720
well how how generalized are they

1655
01:00:52,880 --> 01:00:56,200
currently doing it what is they how wide

1656
01:00:54,720 --> 01:00:58,319
is their training data set so so to

1657
01:00:56,200 --> 01:00:59,400
speak you can just do literally exactly

1658
01:00:58,319 --> 01:01:01,799
the same thing but now you're going to

1659
01:00:59,400 --> 01:01:03,319
reduce a bottleneck in the middle if you

1660
01:01:01,799 --> 01:01:04,760
want to go to new applications where

1661
01:01:03,319 --> 01:01:06,720
people aren't already doing Machine

1662
01:01:04,760 --> 01:01:08,480
Vision with deep neural networks then

1663
01:01:06,720 --> 01:01:09,880
you have much more thinking to do but

1664
01:01:08,480 --> 01:01:11,760
it's not really thinking that's specific

1665
01:01:09,880 --> 01:01:13,799
to our way of doing stuff it's the same

1666
01:01:11,760 --> 01:01:16,799
questions you always ask when you say is

1667
01:01:13,799 --> 01:01:18,280
it okay to use machine learning to do x

1668
01:01:16,799 --> 01:01:19,640
uh and when should how like how much

1669
01:01:18,280 --> 01:01:22,079
training data you need how much do you

1670
01:01:19,640 --> 01:01:25,480
need to update it how like how often you

1671
01:01:22,079 --> 01:01:27,119
need to refresh it it's that's true of

1672
01:01:25,480 --> 01:01:30,599
anyone even doing it the conventional

1673
01:01:27,119 --> 01:01:30,599
way a just gets rid of the

1674
01:01:33,280 --> 01:01:39,440
bottleneck uh thanks for a nice talk um

1675
01:01:36,920 --> 01:01:42,240
so uh for restricted baltim machines

1676
01:01:39,440 --> 01:01:44,720
there is this kind of uh uh bipar type

1677
01:01:42,240 --> 01:01:46,839
connectivity that my understanding is

1678
01:01:44,720 --> 01:01:48,920
they use to be able to use back

1679
01:01:46,839 --> 01:01:50,440
propagation uh and kind of training do

1680
01:01:48,920 --> 01:01:52,319
you have similar restrictions on both

1681
01:01:50,440 --> 01:01:54,599
kind of the physical systems again I

1682
01:01:52,319 --> 01:01:55,720
guess this is another way of asking the

1683
01:01:54,599 --> 01:02:00,640
same question that's been asked a couple

1684
01:01:55,720 --> 01:02:03,640
times already or uh uh kind of uh yeah

1685
01:02:00,640 --> 01:02:06,599
physical systems or training um

1686
01:02:03,640 --> 01:02:08,279
mechanisms that you would use to uh to

1687
01:02:06,599 --> 01:02:10,279
enable this back propagation training

1688
01:02:08,279 --> 01:02:13,079
for your system like are classes of

1689
01:02:10,279 --> 01:02:15,720
systems that work better or worse than

1690
01:02:13,079 --> 01:02:18,359
others the key thing for us for

1691
01:02:15,720 --> 01:02:19,880
trainability is really how good is our

1692
01:02:18,359 --> 01:02:21,640
differentiable Digital model so you

1693
01:02:19,880 --> 01:02:24,039
don't need to like necessarily restrict

1694
01:02:21,640 --> 01:02:29,039
the system to be in yeah like taking on

1695
01:02:24,039 --> 01:02:32,480
some bipar at graph like an RB M but

1696
01:02:29,039 --> 01:02:34,119
uh you it's a pretty general statement

1697
01:02:32,480 --> 01:02:35,839
that like you want your model to be good

1698
01:02:34,119 --> 01:02:38,559
enough that the gradients have the

1699
01:02:35,839 --> 01:02:40,359
correct sign and certainly you can

1700
01:02:38,559 --> 01:02:42,920
imagine some constraints on the hardware

1701
01:02:40,359 --> 01:02:44,400
to let it not go into regimes that are

1702
01:02:42,920 --> 01:02:46,520
too hard to model so that's one

1703
01:02:44,400 --> 01:02:48,599
constraint like for example I'll sort of

1704
01:02:46,520 --> 01:02:50,000
thumb suck exam thumb suck an example

1705
01:02:48,599 --> 01:02:53,240
it's not like a real example but like

1706
01:02:50,000 --> 01:02:54,839
imagine in our 2D wave guide stuff that

1707
01:02:53,240 --> 01:02:56,240
if we pumped so hard that the lithium

1708
01:02:54,839 --> 01:02:58,640
nabate started doing frequency

1709
01:02:56,240 --> 01:03:01,160
conversion now our model of the wave

1710
01:02:58,640 --> 01:03:02,559
propagation if we didn't model that part

1711
01:03:01,160 --> 01:03:03,720
would would be off and so things

1712
01:03:02,559 --> 01:03:05,400
wouldn't work well so You' be like well

1713
01:03:03,720 --> 01:03:08,440
you should restrict yourself to lower

1714
01:03:05,400 --> 01:03:12,480
enough powers that that doesn't happen

1715
01:03:08,440 --> 01:03:14,079
um is yeah an example but uh it's

1716
01:03:12,480 --> 01:03:15,240
there's many settings in which it helps

1717
01:03:14,079 --> 01:03:18,400
you at least to be a little bit

1718
01:03:15,240 --> 01:03:20,599
constrained so that your model is good

1719
01:03:18,400 --> 01:03:20,599
thank

1720
01:03:22,880 --> 01:03:30,720
you thanks for the talk um I I had a

1721
01:03:26,680 --> 01:03:32,720
question about um kind of how many

1722
01:03:30,720 --> 01:03:35,279
layers you can embed Within These

1723
01:03:32,720 --> 01:03:36,880
physical systems I think most of the

1724
01:03:35,279 --> 01:03:39,240
examples you showed there's kind of only

1725
01:03:36,880 --> 01:03:42,920
one set of weights and one physical

1726
01:03:39,240 --> 01:03:44,680
system that you're tuning have you tried

1727
01:03:42,920 --> 01:03:48,760
sending

1728
01:03:44,680 --> 01:03:51,000
um uh your model through like multiple

1729
01:03:48,760 --> 01:03:53,440
sets of physical systems and do you see

1730
01:03:51,000 --> 01:03:56,640
any change to your output there right

1731
01:03:53,440 --> 01:03:58,880
right so this one the first set of

1732
01:03:56,640 --> 01:04:00,400
examples I showed of uh vowel

1733
01:03:58,880 --> 01:04:04,119
classification was actually with I think

1734
01:04:00,400 --> 01:04:05,839
a five layer neural network um so we had

1735
01:04:04,119 --> 01:04:07,359
five conceptually copies of the

1736
01:04:05,839 --> 01:04:09,760
nonlinear Optical Crystal like in

1737
01:04:07,359 --> 01:04:12,119
practice we actually the lasers are

1738
01:04:09,760 --> 01:04:13,559
expensive so we only had one and we send

1739
01:04:12,119 --> 01:04:15,160
it through we measure the data then we

1740
01:04:13,559 --> 01:04:18,039
send through another and another we like

1741
01:04:15,160 --> 01:04:19,240
Multiplex it in time but uh certainly

1742
01:04:18,039 --> 01:04:22,079
what's happening from a computation

1743
01:04:19,240 --> 01:04:23,680
perspective is a deep a deep computation

1744
01:04:22,079 --> 01:04:28,359
as you go through these five

1745
01:04:23,680 --> 01:04:30,359
layers um and yeah in some it depends on

1746
01:04:28,359 --> 01:04:31,960
the example like in the sensing example

1747
01:04:30,359 --> 01:04:34,000
that I I mean I told you only for about

1748
01:04:31,960 --> 01:04:37,079
30 seconds but there were two uh there

1749
01:04:34,000 --> 01:04:38,640
were two layers uh in the uh

1750
01:04:37,079 --> 01:04:40,359
reprogrammable wave gu we actually only

1751
01:04:38,640 --> 01:04:44,680
did one layer not because we couldn't do

1752
01:04:40,359 --> 01:04:45,960
two or five or more layers um but uh

1753
01:04:44,680 --> 01:04:47,599
because we wanted to demonstrate that

1754
01:04:45,960 --> 01:04:49,960
even with just one layer we can actually

1755
01:04:47,599 --> 01:04:54,440
get close to the sort of theoretical

1756
01:04:49,960 --> 01:04:57,640
maximum for a for a linear a linear

1757
01:04:54,440 --> 01:04:57,640
classification thank

1758
01:04:58,440 --> 01:05:02,279
maybe one last question and then we'll

1759
01:05:00,079 --> 01:05:03,960
open it to the

1760
01:05:02,279 --> 01:05:06,720
reception hi thank you for the

1761
01:05:03,960 --> 01:05:08,559
interesting talk um I'm curious in the

1762
01:05:06,720 --> 01:05:10,640
physical neural network examples you

1763
01:05:08,559 --> 01:05:13,680
give uh you still kind of keep this

1764
01:05:10,640 --> 01:05:15,920
layerwise structure whereas I guess if

1765
01:05:13,680 --> 01:05:18,680
you have some parameterizable physical

1766
01:05:15,920 --> 01:05:20,880
system um you should be able to just

1767
01:05:18,680 --> 01:05:22,400
tune those parameters themselves without

1768
01:05:20,880 --> 01:05:25,200
the use for layers like layers doesn't

1769
01:05:22,400 --> 01:05:27,319
feel like a fundamental design Choice uh

1770
01:05:25,200 --> 01:05:29,079
I'm curious what are your thoughts on I

1771
01:05:27,319 --> 01:05:32,160
guess maybe that's like an optimization

1772
01:05:29,079 --> 01:05:33,839
problem or generally like how learning

1773
01:05:32,160 --> 01:05:35,480
could or should work alternatives to

1774
01:05:33,839 --> 01:05:37,960
back propop that sort of thing right

1775
01:05:35,480 --> 01:05:41,200
right yeah that's a great question so

1776
01:05:37,960 --> 01:05:43,359
uh one sufficiently complex physical

1777
01:05:41,200 --> 01:05:45,599
system should be able to realize

1778
01:05:43,359 --> 01:05:47,680
something that is effectively a deep

1779
01:05:45,599 --> 01:05:49,200
computation so in principle I should

1780
01:05:47,680 --> 01:05:51,079
have been able to give all my examples

1781
01:05:49,200 --> 01:05:52,400
with just one physical system I

1782
01:05:51,079 --> 01:05:56,359
shouldn't need

1783
01:05:52,400 --> 01:06:00,240
multiple but uh

1784
01:05:56,359 --> 01:06:01,880
it it yeah engineering one system that

1785
01:06:00,240 --> 01:06:03,359
does the sort of the whole thing you

1786
01:06:01,880 --> 01:06:04,440
want is actually kind of challenging it

1787
01:06:03,359 --> 01:06:06,240
goes back to the question that we were

1788
01:06:04,440 --> 01:06:07,720
talking about a little earlier of like

1789
01:06:06,240 --> 01:06:09,680
how what does the physical system need

1790
01:06:07,720 --> 01:06:11,279
to be able to do if you want it to do I

1791
01:06:09,680 --> 01:06:13,480
don't know maybe it's a getting a little

1792
01:06:11,279 --> 01:06:16,160
too close to like following exactly what

1793
01:06:13,480 --> 01:06:17,559
CS people do but uh like a convolutional

1794
01:06:16,160 --> 01:06:20,000
neural lay with convolutional layers

1795
01:06:17,559 --> 01:06:21,880
followed by fully connected layers if

1796
01:06:20,000 --> 01:06:23,720
you want if you want one system that

1797
01:06:21,880 --> 01:06:25,640
does sort of everything you can imagine

1798
01:06:23,720 --> 01:06:28,079
that it's pretty natural to actually

1799
01:06:25,640 --> 01:06:30,119
have two different ones that each sort

1800
01:06:28,079 --> 01:06:32,200
of do the different part better rather

1801
01:06:30,119 --> 01:06:34,799
than trying to engineer this one that in

1802
01:06:32,200 --> 01:06:37,760
principle you could make but what

1803
01:06:34,799 --> 01:06:41,400
physical system is that uh that said

1804
01:06:37,760 --> 01:06:42,720
this uh this kind of idea uh here I'm

1805
01:06:41,400 --> 01:06:44,319
kind of optimistic that actually if you

1806
01:06:42,720 --> 01:06:46,359
give me a big enough piece of silicon

1807
01:06:44,319 --> 01:06:48,880
like you can actually get this thing to

1808
01:06:46,359 --> 01:06:49,839
learn the whole thing all at once it's

1809
01:06:48,880 --> 01:06:53,640
probably going to be a really hot

1810
01:06:49,839 --> 01:06:53,640
trading problem though

1811
01:06:57,079 --> 01:07:03,440
okay great so let's thank Peter again

1812
01:06:59,559 --> 01:07:03,440
for a great talk thank you

1813
01:07:04,880 --> 01:07:09,400
very and thank you all for joining

1814
01:07:07,279 --> 01:07:11,400
there's going to be a reception so

1815
01:07:09,400 --> 01:07:14,880
please stay back Peter will also be here

1816
01:07:11,400 --> 01:07:14,880
so you can chat

