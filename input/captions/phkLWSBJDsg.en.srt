1
00:00:00,919 --> 00:00:06,040
I'm thrilled to invite Jennifer on stage

2
00:00:03,800 --> 00:00:07,640
Jennifer is our keynote today and she's

3
00:00:06,040 --> 00:00:09,440
the professor in the department of

4
00:00:07,640 --> 00:00:12,160
electrical engineering and computer

5
00:00:09,440 --> 00:00:14,679
science at the center for computational

6
00:00:12,160 --> 00:00:17,160
biology and the bioengineering program

7
00:00:14,679 --> 00:00:19,680
at the University of California Berkeley

8
00:00:17,160 --> 00:00:21,880
I think we all know a lot about Jennifer

9
00:00:19,680 --> 00:00:24,680
already so I'll just give her the stage

10
00:00:21,880 --> 00:00:26,400
and please thank you oh I feel

11
00:00:24,680 --> 00:00:29,320
privileged I don't have to knock on the

12
00:00:26,400 --> 00:00:31,039
door this is great um so it's a pleasure

13
00:00:29,320 --> 00:00:33,000
to be here and I think I only have a

14
00:00:31,039 --> 00:00:36,520
small part of my talk that will piss Pat

15
00:00:33,000 --> 00:00:38,840
off so um we'll see where is Pat very

16
00:00:36,520 --> 00:00:41,239
good very good it's a little like nursy

17
00:00:38,840 --> 00:00:42,879
Indulgence uh it's not in nurs but okay

18
00:00:41,239 --> 00:00:44,719
so I'm going to yeah just kind of have

19
00:00:42,879 --> 00:00:47,160
four parts s of a general overview of

20
00:00:44,719 --> 00:00:49,440
what I think people are thinking about

21
00:00:47,160 --> 00:00:52,120
and doing in the space of uh machine

22
00:00:49,440 --> 00:00:53,960
learning or now ai and uh protein

23
00:00:52,120 --> 00:00:55,280
engineering then I give an overview

24
00:00:53,960 --> 00:00:57,239
where I'm just just going to tell you a

25
00:00:55,280 --> 00:00:59,199
flavor of some of the things we think

26
00:00:57,239 --> 00:01:00,440
about and the challenges and then the

27
00:00:59,199 --> 00:01:02,519
third one is the one that will'll piss

28
00:01:00,440 --> 00:01:04,680
Pat off which is just a little bit about

29
00:01:02,519 --> 00:01:07,280
uh a particular nugget of a problem in

30
00:01:04,680 --> 00:01:09,360
diffusing on discret Spaces uh that I'll

31
00:01:07,280 --> 00:01:12,640
explain and then I'll go more detail

32
00:01:09,360 --> 00:01:14,360
into one um on re-engineering aav for

33
00:01:12,640 --> 00:01:15,680
therapeutic use and I'll probably at the

34
00:01:14,360 --> 00:01:17,960
end skip a few slides because I've

35
00:01:15,680 --> 00:01:20,640
injected some new stuff but uh but it'll

36
00:01:17,960 --> 00:01:22,119
it'll all work out uh is this oh yeah

37
00:01:20,640 --> 00:01:24,400
there we go okay so I don't think I need

38
00:01:22,119 --> 00:01:26,280
to argue to this crowd why it's

39
00:01:24,400 --> 00:01:28,000
important to be able to make further

40
00:01:26,280 --> 00:01:30,000
advances in protein engineering it

41
00:01:28,000 --> 00:01:31,720
touches not only on drug Therapeutics

42
00:01:30,000 --> 00:01:35,280
but also on agriculture and

43
00:01:31,720 --> 00:01:37,280
environmental um kinds of problems uh

44
00:01:35,280 --> 00:01:39,320
and so if you just put on your kind of

45
00:01:37,280 --> 00:01:41,240
abstract computational thinking hat as a

46
00:01:39,320 --> 00:01:43,960
computer scientist uh you realize this

47
00:01:41,240 --> 00:01:45,479
just a huge combinatorial space so even

48
00:01:43,960 --> 00:01:48,399
if I have a protein that's of length

49
00:01:45,479 --> 00:01:49,560
only around 50 uh amino acids long the

50
00:01:48,399 --> 00:01:51,040
number of combinations is already

51
00:01:49,560 --> 00:01:52,640
getting near the number of atoms in the

52
00:01:51,040 --> 00:01:54,159
universe and so forget the fact that I

53
00:01:52,640 --> 00:01:55,799
don't even know how to measure things

54
00:01:54,159 --> 00:01:57,680
that I like at a high scale and so forth

55
00:01:55,799 --> 00:02:00,000
just like at some very fundamental level

56
00:01:57,680 --> 00:02:02,079
this is a huge combinatorial space that

57
00:02:00,000 --> 00:02:04,200
need to search through and find what we

58
00:02:02,079 --> 00:02:05,920
want and so of course there's a

59
00:02:04,200 --> 00:02:08,360
precedent for doing this it's called

60
00:02:05,920 --> 00:02:09,920
Natural Evolution has given us you know

61
00:02:08,360 --> 00:02:12,040
it's why we're all sitting here with all

62
00:02:09,920 --> 00:02:13,599
these lovely proteins in our bodies that

63
00:02:12,040 --> 00:02:16,280
uh let me give this talk and breathe the

64
00:02:13,599 --> 00:02:17,879
air but it's it takes so long that the

65
00:02:16,280 --> 00:02:20,360
purpose of protein engineering is of

66
00:02:17,879 --> 00:02:22,800
course to accelerate that into something

67
00:02:20,360 --> 00:02:25,480
finite in our in our lives and so there

68
00:02:22,800 --> 00:02:27,200
have been these two main um ways that

69
00:02:25,480 --> 00:02:28,879
people in doing protein engineering and

70
00:02:27,200 --> 00:02:30,400
only until recently one of them had a

71
00:02:28,879 --> 00:02:33,360
Nobel Prize now both sections get a

72
00:02:30,400 --> 00:02:36,400
Nobel Prize uh and so this sort of data

73
00:02:33,360 --> 00:02:38,160
free um approach is this sort of Rosetta

74
00:02:36,400 --> 00:02:40,560
biophysics based approach that Baker

75
00:02:38,160 --> 00:02:42,599
drove forward for many years and it says

76
00:02:40,560 --> 00:02:44,599
almost rest in peace so there are a few

77
00:02:42,599 --> 00:02:46,680
Corners where people claim that this is

78
00:02:44,599 --> 00:02:47,959
still quite useful above and beyond the

79
00:02:46,680 --> 00:02:50,239
machine learning approaches that have

80
00:02:47,959 --> 00:02:52,000
taken off um and then there's of course

81
00:02:50,239 --> 00:02:54,000
the wet lab of directed Evolution and

82
00:02:52,000 --> 00:02:55,720
the reason we're all here together and

83
00:02:54,000 --> 00:02:57,800
I'm talking to is because ma machine

84
00:02:55,720 --> 00:02:59,280
learning is interleaving with all of

85
00:02:57,800 --> 00:03:01,879
this sometimes people pretend it's

86
00:02:59,280 --> 00:03:04,680
taking it over but of course um we still

87
00:03:01,879 --> 00:03:06,840
actually rely on on combining many ideas

88
00:03:04,680 --> 00:03:08,760
from biophysics from data from the wet

89
00:03:06,840 --> 00:03:10,159
lab and machine learning in order to

90
00:03:08,760 --> 00:03:12,599
have some

91
00:03:10,159 --> 00:03:14,400
successes so I when I tell people that I

92
00:03:12,599 --> 00:03:16,239
work on protein engineering which I

93
00:03:14,400 --> 00:03:18,519
started to do about seven years ago

94
00:03:16,239 --> 00:03:21,239
people say oh didn't Alpha fold 2 solve

95
00:03:18,519 --> 00:03:22,920
your problem and so I and I just want to

96
00:03:21,239 --> 00:03:24,920
emphasize that Alpha fold 2 was a

97
00:03:22,920 --> 00:03:27,360
tremendous success is incredibly

98
00:03:24,920 --> 00:03:29,519
powerful tool but it goes from sequence

99
00:03:27,360 --> 00:03:32,000
to structure and if I were to want to

100
00:03:29,519 --> 00:03:33,439
use it for protein design if anything I

101
00:03:32,000 --> 00:03:35,280
would need it to go from structure to

102
00:03:33,439 --> 00:03:37,280
sequence because protein design is about

103
00:03:35,280 --> 00:03:39,080
I tell you what sequence to make in the

104
00:03:37,280 --> 00:03:40,599
laboratory right nothing else works I

105
00:03:39,080 --> 00:03:42,519
can't tell you the structure can't tell

106
00:03:40,599 --> 00:03:44,319
you the function I need fundamentally

107
00:03:42,519 --> 00:03:46,400
the sequence and so there are these

108
00:03:44,319 --> 00:03:48,080
inverse folding models so my student K

109
00:03:46,400 --> 00:03:50,400
Su actually at Facebook um basically

110
00:03:48,080 --> 00:03:52,159
trained Alpha fold in Reverse um at

111
00:03:50,400 --> 00:03:54,079
scale um to get one of these inverse

112
00:03:52,159 --> 00:03:55,959
folding models and those are a useful

113
00:03:54,079 --> 00:03:57,680
component of machine learning based

114
00:03:55,959 --> 00:03:58,879
protein engineering but they don't solve

115
00:03:57,680 --> 00:04:00,560
the problem and something that's

116
00:03:58,879 --> 00:04:02,720
overlooked in a lot lot of the fancy

117
00:04:00,560 --> 00:04:04,720
Machinery of generative models and so

118
00:04:02,720 --> 00:04:06,959
forth is that we don't actually know the

119
00:04:04,720 --> 00:04:08,640
link typically between the structure or

120
00:04:06,959 --> 00:04:11,120
the sequence and the function unless it

121
00:04:08,640 --> 00:04:13,000
is structurally mediated um it's like a

122
00:04:11,120 --> 00:04:14,920
lot of binding may be but for example

123
00:04:13,000 --> 00:04:17,400
catalysis we don't know how to change

124
00:04:14,920 --> 00:04:21,120
like an active site to get an enzyme to

125
00:04:17,400 --> 00:04:22,720
do what we want and so when I first

126
00:04:21,120 --> 00:04:24,440
started working in this area I was very

127
00:04:22,720 --> 00:04:26,280
confused there would be a new paper out

128
00:04:24,440 --> 00:04:28,280
every week claiming to have solved uh

129
00:04:26,280 --> 00:04:30,120
protein engineering and and I and it was

130
00:04:28,280 --> 00:04:32,360
all sliced and diced and strange ways

131
00:04:30,120 --> 00:04:34,320
and so um one of my students want um

132
00:04:32,360 --> 00:04:36,479
kind of put this uh idea together you

133
00:04:34,320 --> 00:04:37,880
could divide the AI task into some

134
00:04:36,479 --> 00:04:39,199
prediction tasks this is a little

135
00:04:37,880 --> 00:04:40,800
arbitrary but I think it's helpful to

136
00:04:39,199 --> 00:04:42,280
think of this way so machine learning

137
00:04:40,800 --> 00:04:44,639
prediction tasks and then the next slide

138
00:04:42,280 --> 00:04:46,600
we'll have what I call Design tasks so

139
00:04:44,639 --> 00:04:48,560
the prediction tasks would be I give you

140
00:04:46,600 --> 00:04:50,320
the sequence you predict the structure

141
00:04:48,560 --> 00:04:52,600
that would be Alpha fold or I give you

142
00:04:50,320 --> 00:04:53,919
the sequence you predict the function uh

143
00:04:52,600 --> 00:04:55,840
or I give you the structure and you

144
00:04:53,919 --> 00:04:58,680
predict the function and so people are

145
00:04:55,840 --> 00:05:00,000
working on all three of those problems

146
00:04:58,680 --> 00:05:01,840
and then there's going to be three more

147
00:05:00,000 --> 00:05:04,440
on the next slide and Al together these

148
00:05:01,840 --> 00:05:06,320
are kind of little chunks of the overall

149
00:05:04,440 --> 00:05:08,199
problem of protein engineering that many

150
00:05:06,320 --> 00:05:10,120
people work on and then we sometimes

151
00:05:08,199 --> 00:05:12,880
assemble them together into some

152
00:05:10,120 --> 00:05:14,360
pipeline so in terms of design tasks

153
00:05:12,880 --> 00:05:16,120
fundamentally we want to design the

154
00:05:14,360 --> 00:05:18,800
sequence and it'd be great if we could

155
00:05:16,120 --> 00:05:20,680
just spe specify the function and design

156
00:05:18,800 --> 00:05:23,080
a sequence and be done we can't

157
00:05:20,680 --> 00:05:25,479
generally um do that very well uh

158
00:05:23,080 --> 00:05:28,000
another task we might set ourselves is

159
00:05:25,479 --> 00:05:29,880
to say I I condition on a function and I

160
00:05:28,000 --> 00:05:31,440
you tell me the backbone structure and

161
00:05:29,880 --> 00:05:32,880
then given the backbone structure then

162
00:05:31,440 --> 00:05:35,160
you generate the sequence and then I'm

163
00:05:32,880 --> 00:05:36,680
piping together two different models and

164
00:05:35,160 --> 00:05:38,880
this is actually one of the ways that

165
00:05:36,680 --> 00:05:40,400
people uh do things right now but they

166
00:05:38,880 --> 00:05:42,319
can kind of as I said only do

167
00:05:40,400 --> 00:05:44,600
structure-based function they can't say

168
00:05:42,319 --> 00:05:47,400
you tell catalyze you know that reaction

169
00:05:44,600 --> 00:05:49,880
go um this is just not possible at the

170
00:05:47,400 --> 00:05:51,600
moment so okay so that's like an

171
00:05:49,880 --> 00:05:53,960
overview of some of the types of

172
00:05:51,600 --> 00:05:56,240
problems both prediction and design that

173
00:05:53,960 --> 00:05:58,280
all get used in a basket together to try

174
00:05:56,240 --> 00:06:00,199
to do the overall uh and of course

175
00:05:58,280 --> 00:06:01,880
protein engineering means very different

176
00:06:00,199 --> 00:06:03,800
things depending where you're starting

177
00:06:01,880 --> 00:06:05,560
from what um throughput of data you can

178
00:06:03,800 --> 00:06:07,759
measure what's already known about the

179
00:06:05,560 --> 00:06:09,479
protein and so forth but U generally

180
00:06:07,759 --> 00:06:11,400
speaking there's a few Trends in how

181
00:06:09,479 --> 00:06:13,000
people use machine learning uh to

182
00:06:11,400 --> 00:06:15,000
advance the field of protein engineering

183
00:06:13,000 --> 00:06:16,720
and a really big one for example um

184
00:06:15,000 --> 00:06:18,759
Facebook and now the the sort of

185
00:06:16,720 --> 00:06:20,240
spin-off company that Alex Rivas has

186
00:06:18,759 --> 00:06:22,599
been heading uh doing a lot of

187
00:06:20,240 --> 00:06:23,960
representation learning and so for those

188
00:06:22,599 --> 00:06:25,880
of like and that's a sort of buzz

189
00:06:23,960 --> 00:06:27,880
buzzword but a real world in machine

190
00:06:25,880 --> 00:06:29,440
learning in all kinds of areas not only

191
00:06:27,880 --> 00:06:31,360
in the sciences and really what it means

192
00:06:29,440 --> 00:06:33,720
is is that you to give data to a

193
00:06:31,360 --> 00:06:35,199
computer it has to be in numeric form

194
00:06:33,720 --> 00:06:37,360
and if it's not in numeric form if it's

195
00:06:35,199 --> 00:06:39,560
in a graph form or it's in a sequence I

196
00:06:37,360 --> 00:06:41,680
have to get it into bits uh and you can

197
00:06:39,560 --> 00:06:43,440
do that with sequences of amino acids

198
00:06:41,680 --> 00:06:45,599
very trivially um through some just

199
00:06:43,440 --> 00:06:46,960
deterministic um encoding but these

200
00:06:45,599 --> 00:06:48,479
encodings are not sort of the most

201
00:06:46,960 --> 00:06:50,440
useful and the idea of representation

202
00:06:48,479 --> 00:06:53,560
learning is to take a huge set of say

203
00:06:50,440 --> 00:06:55,520
protein sequences and learn a numeric

204
00:06:53,560 --> 00:06:57,440
coding that understands something about

205
00:06:55,520 --> 00:06:59,840
sequences such that I can use that

206
00:06:57,440 --> 00:07:01,639
encoding for other problems that have

207
00:06:59,840 --> 00:07:05,599
data and therefore get better

208
00:07:01,639 --> 00:07:07,280
statistical efficiency is the Hope um

209
00:07:05,599 --> 00:07:11,039
okay so that's representation learning

210
00:07:07,280 --> 00:07:12,680
and you may do that um yeah uh and then

211
00:07:11,039 --> 00:07:14,960
a second really this is like sort of a

212
00:07:12,680 --> 00:07:17,039
Hot Topic is conditional or generative

213
00:07:14,960 --> 00:07:19,319
models and conditional generative models

214
00:07:17,039 --> 00:07:21,039
we all know that uh chat gvt is a

215
00:07:19,319 --> 00:07:22,680
generative model the next to does next

216
00:07:21,039 --> 00:07:24,240
token prediction similarly you can have

217
00:07:22,680 --> 00:07:27,840
a generative model over protein

218
00:07:24,240 --> 00:07:29,720
sequences or um uh and you may condition

219
00:07:27,840 --> 00:07:31,360
on the structure so you may say tell

220
00:07:29,720 --> 00:07:33,800
give me this is the backbone structure I

221
00:07:31,360 --> 00:07:36,160
desire please tell me the amino acids I

222
00:07:33,800 --> 00:07:37,639
should use or a distribution over them

223
00:07:36,160 --> 00:07:39,639
uh or sometimes you may condition on the

224
00:07:37,639 --> 00:07:41,520
family in a perfect world you could just

225
00:07:39,639 --> 00:07:43,759
condition on the function uh but again

226
00:07:41,520 --> 00:07:45,759
that like to do that it's not sufficient

227
00:07:43,759 --> 00:07:47,680
to have fancy machine learning Machinery

228
00:07:45,759 --> 00:07:50,960
you need to have the information that

229
00:07:47,680 --> 00:07:52,879
actually links these somewhere in there

230
00:07:50,960 --> 00:07:54,400
and then another um thing that people

231
00:07:52,879 --> 00:07:56,319
have spent a lot of time is are these

232
00:07:54,400 --> 00:07:57,560
generative models for the backbone

233
00:07:56,319 --> 00:07:59,240
structure and they're kind of there's

234
00:07:57,560 --> 00:08:01,560
one from David uh David Baker's group

235
00:07:59,240 --> 00:08:03,440
and then there's John Ingram at all um

236
00:08:01,560 --> 00:08:05,159
that sort of did it at the same time and

237
00:08:03,440 --> 00:08:07,840
these are using these sort of um

238
00:08:05,159 --> 00:08:09,840
well-known diffusion models that Pat Uh

239
00:08:07,840 --> 00:08:13,199
alluded to and again there's these are

240
00:08:09,840 --> 00:08:14,960
like really elegant and and they can be

241
00:08:13,199 --> 00:08:17,080
very powerful but you have to

242
00:08:14,960 --> 00:08:19,240
fundamentally know something about the

243
00:08:17,080 --> 00:08:21,120
function in order to use them properly

244
00:08:19,240 --> 00:08:22,599
and again if you don't have that missing

245
00:08:21,120 --> 00:08:24,440
if you don't have that information that

246
00:08:22,599 --> 00:08:26,440
links the structure to the desired

247
00:08:24,440 --> 00:08:28,360
function then all that fancy Machinery

248
00:08:26,440 --> 00:08:29,919
is not going to buy you anything I'm not

249
00:08:28,360 --> 00:08:31,759
a downer on this I just think this is

250
00:08:29,919 --> 00:08:34,120
overlooked I think these are powerful

251
00:08:31,759 --> 00:08:36,479
interesting models that are um are

252
00:08:34,120 --> 00:08:39,760
already probably maybe

253
00:08:36,479 --> 00:08:41,839
useful um so and if you if you want an

254
00:08:39,760 --> 00:08:43,279
overview sort of primer of some of these

255
00:08:41,839 --> 00:08:45,120
ideas of generative models you can check

256
00:08:43,279 --> 00:08:48,600
out a recent uh primer that my students

257
00:08:45,120 --> 00:08:51,320
and I wrote uh posted here uh and then

258
00:08:48,600 --> 00:08:53,440
maybe this one is also this is one of

259
00:08:51,320 --> 00:08:55,640
the least sexy problems but very

260
00:08:53,440 --> 00:08:57,880
important problems is how do I estimate

261
00:08:55,640 --> 00:09:00,640
some link between the sequence and the

262
00:08:57,880 --> 00:09:02,880
function so that I can uh know about

263
00:09:00,640 --> 00:09:04,959
that that that how to engineer for the

264
00:09:02,880 --> 00:09:07,560
function I desire when it's not just

265
00:09:04,959 --> 00:09:09,079
structurally mediated in particular and

266
00:09:07,560 --> 00:09:11,120
for many problems when we start an

267
00:09:09,079 --> 00:09:14,040
engineering campaign we have few or no

268
00:09:11,120 --> 00:09:15,959
labeled data and so we rely often on the

269
00:09:14,040 --> 00:09:18,000
kind of information that Alpha fold also

270
00:09:15,959 --> 00:09:20,480
relies on to do structure prediction

271
00:09:18,000 --> 00:09:22,200
which is evolutionary data so you just

272
00:09:20,480 --> 00:09:24,160
as Alpha fold doesn't work well if you

273
00:09:22,200 --> 00:09:26,680
don't have a set of homologous proteins

274
00:09:24,160 --> 00:09:28,120
to help inform the structure prediction

275
00:09:26,680 --> 00:09:30,120
uh similarly if you don't have

276
00:09:28,120 --> 00:09:31,680
homologues to start off an engineering

277
00:09:30,120 --> 00:09:34,399
campaign then you're starting off kind

278
00:09:31,680 --> 00:09:36,240
of with zero data and so we we really do

279
00:09:34,399 --> 00:09:38,040
leverage this evolutionary data to help

280
00:09:36,240 --> 00:09:40,279
us start a campaign that will be

281
00:09:38,040 --> 00:09:42,240
iterative and we gather more information

282
00:09:40,279 --> 00:09:43,210
but helps direct us um to start the

283
00:09:42,240 --> 00:09:44,640
first round of

284
00:09:43,210 --> 00:09:47,040
[Music]

285
00:09:44,640 --> 00:09:49,320
experiments and then of course uh like

286
00:09:47,040 --> 00:09:50,720
alphaf 2 was amazing and alphafold 3

287
00:09:49,320 --> 00:09:52,640
might be but as Pat said we don't

288
00:09:50,720 --> 00:09:53,920
completely know because we can't access

289
00:09:52,640 --> 00:09:55,880
it but I think some of the really

290
00:09:53,920 --> 00:09:57,600
important problems in any case whether

291
00:09:55,880 --> 00:09:59,079
we had access fully to Alpha volt 3 or

292
00:09:57,600 --> 00:10:00,800
not are this idea that actually we

293
00:09:59,079 --> 00:10:02,839
really kind of want to know the main

294
00:10:00,800 --> 00:10:04,680
confirmational states that a protein

295
00:10:02,839 --> 00:10:06,800
takes on maybe we sometimes want to know

296
00:10:04,680 --> 00:10:09,240
the Dynamics as well and fundamentally

297
00:10:06,800 --> 00:10:10,839
we just can't do that right now um like

298
00:10:09,240 --> 00:10:12,720
the closest way to do that is basically

299
00:10:10,839 --> 00:10:14,680
mcro Dynamics and that doesn't work um

300
00:10:12,720 --> 00:10:16,240
particularly well and is very expensive

301
00:10:14,680 --> 00:10:18,600
and so this is one of the wide open

302
00:10:16,240 --> 00:10:21,920
things that people can't really tackle

303
00:10:18,600 --> 00:10:25,480
that is felt would help uh improve the

304
00:10:21,920 --> 00:10:27,720
successes in this general area and so

305
00:10:25,480 --> 00:10:29,079
sometimes uh I've I've gotten into a

306
00:10:27,720 --> 00:10:30,760
surprising number of arguments with

307
00:10:29,079 --> 00:10:33,040
people that now that we have generative

308
00:10:30,760 --> 00:10:35,560
models we can just generate more data so

309
00:10:33,040 --> 00:10:36,800
that we can get the models that we want

310
00:10:35,560 --> 00:10:38,519
because one of the problems here is we

311
00:10:36,800 --> 00:10:40,680
just don't have the data to get the

312
00:10:38,519 --> 00:10:42,240
Dynamics and so I woke up one Sunday a

313
00:10:40,680 --> 00:10:44,079
little annoyed with telling people over

314
00:10:42,240 --> 00:10:46,760
and over we can't just fill the data Gap

315
00:10:44,079 --> 00:10:49,639
by using generative models um which if

316
00:10:46,760 --> 00:10:52,600
you is to some people strikingly obvious

317
00:10:49,639 --> 00:10:53,920
and to others um strikingly not obvious

318
00:10:52,600 --> 00:10:55,519
anyhow you can check this out if you

319
00:10:53,920 --> 00:10:57,440
want to hear some of my thoughts about

320
00:10:55,519 --> 00:10:59,519
the hype in Ai and Science and why

321
00:10:57,440 --> 00:11:01,600
structure prediction in particular was a

322
00:10:59,519 --> 00:11:04,519
task that could be solved well static

323
00:11:01,600 --> 00:11:06,720
structure prediction uh okay so that's

324
00:11:04,519 --> 00:11:08,760
sort of loading you up on on what's

325
00:11:06,720 --> 00:11:10,440
happening in this field in general and

326
00:11:08,760 --> 00:11:12,120
so now let me give you an overview I try

327
00:11:10,440 --> 00:11:15,000
to find the areas that aren't too

328
00:11:12,120 --> 00:11:17,839
crowded that I think are impactful and

329
00:11:15,000 --> 00:11:19,440
so on the one hand uh we do some

330
00:11:17,839 --> 00:11:20,959
methodological development just on our

331
00:11:19,440 --> 00:11:22,519
own I don't run a wet lab and then we

332
00:11:20,959 --> 00:11:24,360
also collaborate with wet lab

333
00:11:22,519 --> 00:11:26,560
investigators to keep ourselves kind of

334
00:11:24,360 --> 00:11:27,800
honest and working on real things but

335
00:11:26,560 --> 00:11:29,720
this was this what I'm going to tell you

336
00:11:27,800 --> 00:11:31,600
now is like a little vignette about my

337
00:11:29,720 --> 00:11:34,440
entry Waypoint to the field of protein

338
00:11:31,600 --> 00:11:36,600
engineering which I got into by accident

339
00:11:34,440 --> 00:11:38,399
um by thinking about a certain kind of

340
00:11:36,600 --> 00:11:40,839
problem that ended up looking a lot like

341
00:11:38,399 --> 00:11:43,959
this so in standard supervised machine

342
00:11:40,839 --> 00:11:45,680
learning you take a set of um X's uh

343
00:11:43,959 --> 00:11:47,399
like a protein sequence that is paired

344
00:11:45,680 --> 00:11:49,000
with a known label this is multiple

345
00:11:47,399 --> 00:11:51,240
labels you can just think of one label

346
00:11:49,000 --> 00:11:53,120
like this is sequences maybe this is gfp

347
00:11:51,240 --> 00:11:54,880
fluorescent something like that and then

348
00:11:53,120 --> 00:11:56,360
because they're paired your goal as a

349
00:11:54,880 --> 00:11:58,399
machine learning person is to set the

350
00:11:56,360 --> 00:11:59,959
knobs of your predictive model here a

351
00:11:58,399 --> 00:12:02,480
neural network such that that on the

352
00:11:59,959 --> 00:12:03,880
training data you map the X's to the Y

353
00:12:02,480 --> 00:12:06,160
correctly and you have to be a bit

354
00:12:03,880 --> 00:12:07,839
careful so that it works on unseen data

355
00:12:06,160 --> 00:12:09,839
and doesn't just memorize your data and

356
00:12:07,839 --> 00:12:12,120
that's basically the game of supervised

357
00:12:09,839 --> 00:12:14,079
prediction but when we do design what

358
00:12:12,120 --> 00:12:16,079
we're fundamentally doing is saying if

359
00:12:14,079 --> 00:12:17,920
we do machine learning based design then

360
00:12:16,079 --> 00:12:20,160
we assume we've sort of encoded some

361
00:12:17,920 --> 00:12:21,480
knowledge in a network that Maps

362
00:12:20,160 --> 00:12:23,399
something about the sequence to the

363
00:12:21,480 --> 00:12:24,880
function and then we want to invert the

364
00:12:23,399 --> 00:12:26,800
model so we don't want to adjust the

365
00:12:24,880 --> 00:12:28,639
parameters anym anymore we assume we've

366
00:12:26,800 --> 00:12:30,440
done the best we can at learning this

367
00:12:28,639 --> 00:12:32,360
functional mapping what we want to do

368
00:12:30,440 --> 00:12:34,720
now is specify a property like higher

369
00:12:32,360 --> 00:12:37,360
fluorescence than I ever saw in the data

370
00:12:34,720 --> 00:12:38,959
set or better um catalytic efficiency

371
00:12:37,360 --> 00:12:41,240
that I saw in the training data set

372
00:12:38,959 --> 00:12:43,360
leave these knobs fixed and actually

373
00:12:41,240 --> 00:12:45,760
optimize for the sequence that will give

374
00:12:43,360 --> 00:12:47,399
me the most um highest value according

375
00:12:45,760 --> 00:12:49,360
to this neural network because that's

376
00:12:47,399 --> 00:12:52,240
kind of the best I can do if this is the

377
00:12:49,360 --> 00:12:55,040
approach I'm going to take and so this

378
00:12:52,240 --> 00:12:57,560
is actually much much much harder than

379
00:12:55,040 --> 00:13:00,480
supervised learning for several reasons

380
00:12:57,560 --> 00:13:04,120
um and so let me just give you one sort

381
00:13:00,480 --> 00:13:05,680
of it's my banana analogy so it turns

382
00:13:04,120 --> 00:13:08,120
out in computer vision people have

383
00:13:05,680 --> 00:13:09,800
something that smells a lot like um

384
00:13:08,120 --> 00:13:12,120
machine learning based design in the way

385
00:13:09,800 --> 00:13:14,120
I just presented it and I should say I

386
00:13:12,120 --> 00:13:15,600
you may think oh well generative models

387
00:13:14,120 --> 00:13:17,399
don't behave like this but under the

388
00:13:15,600 --> 00:13:19,199
hood everything has this problem that

389
00:13:17,399 --> 00:13:21,160
I'm about to show you whether it looks

390
00:13:19,199 --> 00:13:23,320
like this or it's an actual um

391
00:13:21,160 --> 00:13:24,920
generative model but you specify so the

392
00:13:23,320 --> 00:13:26,440
machine learning people take this neural

393
00:13:24,920 --> 00:13:28,839
network and they train it on many many

394
00:13:26,440 --> 00:13:30,320
different classes of objects banana kiwi

395
00:13:28,839 --> 00:13:32,240
car or whatever and then it's a

396
00:13:30,320 --> 00:13:34,079
state-of-the-art classifier and they say

397
00:13:32,240 --> 00:13:36,160
I want to understand what my neural

398
00:13:34,079 --> 00:13:38,399
network thinks a quintessential banana

399
00:13:36,160 --> 00:13:40,399
is so similarly to how I might say give

400
00:13:38,399 --> 00:13:42,720
me the highest catalytic efficiency they

401
00:13:40,399 --> 00:13:44,920
say turn the banana node to one and turn

402
00:13:42,720 --> 00:13:46,639
the other class nodes to zero leave the

403
00:13:44,920 --> 00:13:49,160
parameters fixed start with a random

404
00:13:46,639 --> 00:13:51,959
image and iter iteratively optimize it

405
00:13:49,160 --> 00:13:54,800
till I have the most banana like image

406
00:13:51,959 --> 00:13:56,399
and so when you do that you get abstract

407
00:13:54,800 --> 00:13:58,440
art and so you know if you were to try

408
00:13:56,399 --> 00:14:00,199
to go print this and put it on the table

409
00:13:58,440 --> 00:14:02,199
you couldn't right it could just be a 2d

410
00:14:00,199 --> 00:14:03,639
thing you couldn't make a 3D object of

411
00:14:02,199 --> 00:14:05,639
this even though it's plausible it has

412
00:14:03,639 --> 00:14:07,519
the right curvature the right colors and

413
00:14:05,639 --> 00:14:09,120
so by analogy this is actually what

414
00:14:07,519 --> 00:14:11,079
happens if you try to do machine

415
00:14:09,120 --> 00:14:13,320
learning based design and you try to

416
00:14:11,079 --> 00:14:15,079
squeeze the network for all its worth

417
00:14:13,320 --> 00:14:17,000
and you're squeezing it outside of its

418
00:14:15,079 --> 00:14:18,959
comfort zone is really what's happening

419
00:14:17,000 --> 00:14:20,759
is it only understands things near the

420
00:14:18,959 --> 00:14:22,880
training data and when you try to do

421
00:14:20,759 --> 00:14:25,320
what I call extrapolative Design it's

422
00:14:22,880 --> 00:14:28,000
almost certainly going to fail and just

423
00:14:25,320 --> 00:14:29,800
give you nonsense and so this is the

424
00:14:28,000 --> 00:14:31,399
sort of the initial this set the stage

425
00:14:29,800 --> 00:14:33,320
for the initial set of work that my

426
00:14:31,399 --> 00:14:35,320
group did when I started to work in this

427
00:14:33,320 --> 00:14:37,199
area which is fundamentally navigating

428
00:14:35,320 --> 00:14:39,759
this tension between how can I

429
00:14:37,199 --> 00:14:41,839
extrapolate from a learned function that

430
00:14:39,759 --> 00:14:44,000
tells me about the the property I care

431
00:14:41,839 --> 00:14:45,880
about um but I have to understand that

432
00:14:44,000 --> 00:14:47,920
that model doesn't I can't trust it

433
00:14:45,880 --> 00:14:50,160
everywhere and there's fundamentally

434
00:14:47,920 --> 00:14:51,480
always going to be such a tension if

435
00:14:50,160 --> 00:14:53,440
you're trying to do this sort of

436
00:14:51,480 --> 00:14:55,120
extrapolative design if you're trying to

437
00:14:53,440 --> 00:14:57,160
design things within the scope of

438
00:14:55,120 --> 00:14:58,560
properties you've already seen then I

439
00:14:57,160 --> 00:15:02,440
think you have a much better chance but

440
00:14:58,560 --> 00:15:04,079
often that's not what we're doing um so

441
00:15:02,440 --> 00:15:05,560
right so there I'm not going to go into

442
00:15:04,079 --> 00:15:07,600
details but some of our earliest papers

443
00:15:05,560 --> 00:15:09,600
are trying to basically get at this

444
00:15:07,600 --> 00:15:12,120
either by

445
00:15:09,600 --> 00:15:13,880
um but well by just pointing out the

446
00:15:12,120 --> 00:15:15,480
problem because people were just doing

447
00:15:13,880 --> 00:15:17,639
optimization saying they were doing

448
00:15:15,480 --> 00:15:20,880
protein engineering we trying to really

449
00:15:17,639 --> 00:15:23,480
get at technical ways to help understand

450
00:15:20,880 --> 00:15:25,320
where can I trust the model and how far

451
00:15:23,480 --> 00:15:27,600
can I go before I should collect a new

452
00:15:25,320 --> 00:15:29,319
round of data which is a bit of a risk

453
00:15:27,600 --> 00:15:30,959
assessment and you may um have different

454
00:15:29,319 --> 00:15:32,519
risk for different problems but and

455
00:15:30,959 --> 00:15:35,639
maybe I'll just highlight this one paper

456
00:15:32,519 --> 00:15:37,480
that Hunter nissanoff um LED and came up

457
00:15:35,639 --> 00:15:39,040
with the idea for which is to say we

458
00:15:37,480 --> 00:15:41,199
know that machine learning models

459
00:15:39,040 --> 00:15:43,160
typically outperform biophysics model

460
00:15:41,199 --> 00:15:45,399
based models um that may exist for the

461
00:15:43,160 --> 00:15:47,240
same problem near the training data but

462
00:15:45,399 --> 00:15:49,160
as I go further from the training data

463
00:15:47,240 --> 00:15:51,199
like all bets are off for the machine

464
00:15:49,160 --> 00:15:53,560
learning model whereas since biophysics

465
00:15:51,199 --> 00:15:55,759
based models typically only at most tune

466
00:15:53,560 --> 00:15:57,639
maybe a couple of scalar parameters on

467
00:15:55,759 --> 00:15:59,720
real data they should they should be

468
00:15:57,639 --> 00:16:01,399
kind of equally good across the whole

469
00:15:59,720 --> 00:16:03,040
space but worse than the machine

470
00:16:01,399 --> 00:16:05,399
learning models near the training data

471
00:16:03,040 --> 00:16:07,240
because machine learning is powerful uh

472
00:16:05,399 --> 00:16:09,040
if you use it properly and so in this

473
00:16:07,240 --> 00:16:12,000
paper he says Let's Get The Best of Both

474
00:16:09,040 --> 00:16:13,399
Worlds where we understand as the

475
00:16:12,000 --> 00:16:15,399
machine as the machine learning model

476
00:16:13,399 --> 00:16:18,440
kind of degrades and becomes more and

477
00:16:15,399 --> 00:16:20,160
more uncertain let me seamlessly blend

478
00:16:18,440 --> 00:16:22,279
the the predictions from the biophysics

479
00:16:20,160 --> 00:16:25,600
model sort of gracefully as I go further

480
00:16:22,279 --> 00:16:27,199
away from the data um I'm not I'm not

481
00:16:25,600 --> 00:16:29,680
going to say much about the other ones

482
00:16:27,199 --> 00:16:31,040
in the interest of time

483
00:16:29,680 --> 00:16:33,639
um but if you're interested in this sort

484
00:16:31,040 --> 00:16:36,160
of whole space Also Clara and I wrote a

485
00:16:33,639 --> 00:16:38,639
sort of perspective slash I don't know

486
00:16:36,160 --> 00:16:40,279
tutorial weird piece that is basically

487
00:16:38,639 --> 00:16:42,720
talking about all of these problems of

488
00:16:40,279 --> 00:16:44,480
how do I do extrapolative design can I

489
00:16:42,720 --> 00:16:46,079
actually design for novel properties

490
00:16:44,480 --> 00:16:48,639
using machine learning like how do I

491
00:16:46,079 --> 00:16:50,040
think about that and it's it's a general

492
00:16:48,639 --> 00:16:52,720
framework of a discussion but it's

493
00:16:50,040 --> 00:16:56,040
anchored in protein

494
00:16:52,720 --> 00:16:57,880
engineering okay and then so some of the

495
00:16:56,040 --> 00:17:00,120
other topics that we've touched on

496
00:16:57,880 --> 00:17:02,639
include the fact that often in The

497
00:17:00,120 --> 00:17:04,559
Sciences I mean it's less so now but

498
00:17:02,639 --> 00:17:06,439
people will just take existing neural

499
00:17:04,559 --> 00:17:09,720
network architectures like convolutional

500
00:17:06,439 --> 00:17:11,319
neural networks Transformers that were

501
00:17:09,720 --> 00:17:12,760
in particular let's say convolution was

502
00:17:11,319 --> 00:17:15,679
designed for images like there's

503
00:17:12,760 --> 00:17:17,520
something about spatial smoothness and

504
00:17:15,679 --> 00:17:19,439
so forth and so this is what we call an

505
00:17:17,520 --> 00:17:20,720
inductive bias in machine learning it's

506
00:17:19,439 --> 00:17:22,280
it's putting in some of your own

507
00:17:20,720 --> 00:17:24,559
knowledge into how you design the

508
00:17:22,280 --> 00:17:26,919
architecture of the neural network and

509
00:17:24,559 --> 00:17:28,520
it's not so much the case now um people

510
00:17:26,919 --> 00:17:30,480
are thinking about graphs and other

511
00:17:28,520 --> 00:17:32,440
things but often we just grab whatever

512
00:17:30,480 --> 00:17:34,640
worked in computer vision or natural

513
00:17:32,440 --> 00:17:36,200
language processing and directly apply

514
00:17:34,640 --> 00:17:37,559
it to our scientific problem without

515
00:17:36,200 --> 00:17:39,720
thinking about what those assumptions

516
00:17:37,559 --> 00:17:42,320
were and if they make sense and so we

517
00:17:39,720 --> 00:17:44,080
have uh one is a collaborative paper

518
00:17:42,320 --> 00:17:45,400
with um amorality and and information

519
00:17:44,080 --> 00:17:47,320
Theory group and we have a follow-up

520
00:17:45,400 --> 00:17:49,600
that's not at out but the idea here is

521
00:17:47,320 --> 00:17:50,960
to say if we're making functions uh

522
00:17:49,600 --> 00:17:53,000
trying to estimate functions that go

523
00:17:50,960 --> 00:17:54,919
from sequence space to a property of

524
00:17:53,000 --> 00:17:57,080
Interest then I should think about

525
00:17:54,919 --> 00:17:58,919
inductive biases in the epistatic

526
00:17:57,080 --> 00:18:00,640
landscape which is sort of the comple

527
00:17:58,919 --> 00:18:02,799
lexity of the interactions between the

528
00:18:00,640 --> 00:18:04,840
sequence positions and we know that most

529
00:18:02,799 --> 00:18:07,120
biology is very kind of low complexity

530
00:18:04,840 --> 00:18:09,679
on that scale it's often linear addative

531
00:18:07,120 --> 00:18:11,320
in positions sometimes there's pairwise

532
00:18:09,679 --> 00:18:13,840
and the higher I go in the order of the

533
00:18:11,320 --> 00:18:15,600
epistasis probably the less likely it is

534
00:18:13,840 --> 00:18:17,440
and so the idea is can we encode that

535
00:18:15,600 --> 00:18:21,960
into our neural

536
00:18:17,440 --> 00:18:24,799
networks um and so now uh and then this

537
00:18:21,960 --> 00:18:26,200
this is also an accidental paper uh so

538
00:18:24,799 --> 00:18:28,240
what we set out to do here is some of

539
00:18:26,200 --> 00:18:29,480
the papers that were coming out uh in

540
00:18:28,240 --> 00:18:32,159
the early

541
00:18:29,480 --> 00:18:33,840
days of saying how can I do low-end

542
00:18:32,159 --> 00:18:36,600
protein engineering I have just a few

543
00:18:33,840 --> 00:18:38,039
supervised examples can I design for a

544
00:18:36,600 --> 00:18:40,600
property when I have only a few

545
00:18:38,039 --> 00:18:43,520
supervised examples to train my model

546
00:18:40,600 --> 00:18:45,880
and and can I use large unsupervised

547
00:18:43,520 --> 00:18:47,640
data sets to combine with them and do

548
00:18:45,880 --> 00:18:49,760
better and they these were creative

549
00:18:47,640 --> 00:18:51,679
interesting papers but we couldn't tell

550
00:18:49,760 --> 00:18:53,480
if they were just interesting stories or

551
00:18:51,679 --> 00:18:55,440
they were real results and so we started

552
00:18:53,480 --> 00:18:58,440
to compare and this is a little bit

553
00:18:55,440 --> 00:19:00,799
outdated now but um so we ended up with

554
00:18:58,440 --> 00:19:02,400
this big comparison and the and when

555
00:19:00,799 --> 00:19:04,360
they when we published this they wanted

556
00:19:02,400 --> 00:19:05,919
to highlight our new method but the

557
00:19:04,360 --> 00:19:07,679
point of our new method was that it was

558
00:19:05,919 --> 00:19:10,000
really dumb and simple and beating the

559
00:19:07,679 --> 00:19:12,039
really fancy big machine learning models

560
00:19:10,000 --> 00:19:14,520
and so I think that was a lesson like a

561
00:19:12,039 --> 00:19:16,280
lesson that is often um hidden in a lot

562
00:19:14,520 --> 00:19:18,280
of these papers that simple simple

563
00:19:16,280 --> 00:19:20,520
things without these big models actually

564
00:19:18,280 --> 00:19:21,880
work really well and also in fact

565
00:19:20,520 --> 00:19:23,440
there's something fundamentally wrong

566
00:19:21,880 --> 00:19:25,320
with this paper and many of these papers

567
00:19:23,440 --> 00:19:26,679
is they all work on a bunch of data sets

568
00:19:25,320 --> 00:19:28,799
that are looking only at one and two

569
00:19:26,679 --> 00:19:30,080
mutations away from a wild type but but

570
00:19:28,799 --> 00:19:31,919
the whole point of machine learning is

571
00:19:30,080 --> 00:19:33,520
to do more than that and then we

572
00:19:31,919 --> 00:19:35,120
sometimes Benchmark on these data sets

573
00:19:33,520 --> 00:19:36,720
and we did because that's all we could

574
00:19:35,120 --> 00:19:38,760
do at the time it's becoming a little

575
00:19:36,720 --> 00:19:40,200
bit better now um and these two I'm

576
00:19:38,760 --> 00:19:42,159
going to tell you about in more detail

577
00:19:40,200 --> 00:19:44,159
so I'm going to write write um in a

578
00:19:42,159 --> 00:19:45,640
moment but right so sometimes we have

579
00:19:44,159 --> 00:19:47,720
these kind of you can see maybe Pie in

580
00:19:45,640 --> 00:19:49,919
the Sky maybe a bit more theoretically

581
00:19:47,720 --> 00:19:51,760
oriented projects that uh we think are

582
00:19:49,919 --> 00:19:53,960
useful to help clean up some of the

583
00:19:51,760 --> 00:19:56,600
thinking and that will lead to hopefully

584
00:19:53,960 --> 00:19:58,720
practically use useful algorithms and

585
00:19:56,600 --> 00:20:00,679
models as well but we we like to work

586
00:19:58,720 --> 00:20:02,799
very closely with people who have a wet

587
00:20:00,679 --> 00:20:04,799
lab to make sure we're working on real

588
00:20:02,799 --> 00:20:07,520
problems and maybe pivoting or tweaking

589
00:20:04,799 --> 00:20:09,480
where we were in our computational lone

590
00:20:07,520 --> 00:20:11,320
heads and so uh so there's a number of

591
00:20:09,480 --> 00:20:12,799
them and I the one I'll talk about Mo uh

592
00:20:11,320 --> 00:20:14,840
in a second is is the first one with

593
00:20:12,799 --> 00:20:17,080
David Schaefer which is the most mature

594
00:20:14,840 --> 00:20:18,880
of these and is already published but I

595
00:20:17,080 --> 00:20:20,280
I I like to put my acknowledgements to

596
00:20:18,880 --> 00:20:22,120
my group in the middle so I don't get

597
00:20:20,280 --> 00:20:23,799
stuck without them in the end so I'll

598
00:20:22,120 --> 00:20:25,799
just say these are the people of the

599
00:20:23,799 --> 00:20:28,280
work I'm talking about today that made

600
00:20:25,799 --> 00:20:30,159
it happen or are making it happen um so

601
00:20:28,280 --> 00:20:32,080
in the next two projects I'm going to um

602
00:20:30,159 --> 00:20:33,960
talk about David and aosa who are now at

603
00:20:32,080 --> 00:20:36,840
Dino although which is here in Boston I

604
00:20:33,960 --> 00:20:40,360
think neither is in Boston uh and they

605
00:20:36,840 --> 00:20:43,919
LED this aav project and then um

606
00:20:40,360 --> 00:20:45,320
Stefan uh Hunter and bear led the the

607
00:20:43,919 --> 00:20:47,559
I'm going to tell you a little nugget of

608
00:20:45,320 --> 00:20:50,880
a project that's a bit about some

609
00:20:47,559 --> 00:20:52,720
diffusion models uh and then Khloe is

610
00:20:50,880 --> 00:20:54,080
the one that did that kind of big

611
00:20:52,720 --> 00:20:56,760
comparison and showed that a simple

612
00:20:54,080 --> 00:20:59,640
model worked uh I think that's those are

613
00:20:56,760 --> 00:21:01,799
the main acknowledgements for this talk

614
00:20:59,640 --> 00:21:05,320
okay so right so this is I just sort of

615
00:21:01,799 --> 00:21:09,120
inserted this in today uh uh pat pat you

616
00:21:05,320 --> 00:21:10,360
can close your ears now so um so because

617
00:21:09,120 --> 00:21:11,960
this is going to end up talking a little

618
00:21:10,360 --> 00:21:13,600
bit about diffusion but I'd like to set

619
00:21:11,960 --> 00:21:15,120
the stage here which is here's something

620
00:21:13,600 --> 00:21:17,240
that happens a lot in machine learning

621
00:21:15,120 --> 00:21:19,520
based design is I have a generative

622
00:21:17,240 --> 00:21:21,760
model for something just like sequences

623
00:21:19,520 --> 00:21:23,679
all sequences and I can build such a

624
00:21:21,760 --> 00:21:25,600
model kind of Fairly easily because if

625
00:21:23,679 --> 00:21:27,679
all I want to do is generate a plausible

626
00:21:25,600 --> 00:21:29,799
protein sequence that's likely to fold

627
00:21:27,679 --> 00:21:32,320
and be stable and I can I have tons and

628
00:21:29,799 --> 00:21:34,679
tons of proteins in unir and I can train

629
00:21:32,320 --> 00:21:36,360
a big unsupervised model on these and

630
00:21:34,679 --> 00:21:38,360
that's pretty easy of course no one

631
00:21:36,360 --> 00:21:40,320
cares about just randomly generating a

632
00:21:38,360 --> 00:21:42,559
protein sequence right what I want is I

633
00:21:40,320 --> 00:21:44,600
want to generate a protein sequence X

634
00:21:42,559 --> 00:21:46,159
conditioned on some uh property that I

635
00:21:44,600 --> 00:21:48,559
care about so I want to do conditional

636
00:21:46,159 --> 00:21:50,279
design and so what we usually do not

637
00:21:48,559 --> 00:21:53,120
only in The Sciences but this is even

638
00:21:50,279 --> 00:21:55,080
what chat GPT does they take a big model

639
00:21:53,120 --> 00:21:57,039
and then they kind of re refine it in

640
00:21:55,080 --> 00:21:58,799
some way to get it to do what they want

641
00:21:57,039 --> 00:22:00,919
and this can be people use different

642
00:21:58,799 --> 00:22:02,039
words uh I technically what you're

643
00:22:00,919 --> 00:22:04,240
really trying to do is take an

644
00:22:02,039 --> 00:22:06,159
unconditional model and make it into a

645
00:22:04,240 --> 00:22:08,520
conditional model for some property you

646
00:22:06,159 --> 00:22:11,000
care about so the question is like how

647
00:22:08,520 --> 00:22:12,760
can I do that if I have this P of X I've

648
00:22:11,000 --> 00:22:14,720
trained it and I have either a

649
00:22:12,760 --> 00:22:17,720
predictive model for the property I care

650
00:22:14,720 --> 00:22:19,520
about or a set of data how can I adjust

651
00:22:17,720 --> 00:22:22,080
like given that I put all this work into

652
00:22:19,520 --> 00:22:25,039
P of X with a ton of data how can I kind

653
00:22:22,080 --> 00:22:27,039
of Squish P of X around to get the

654
00:22:25,039 --> 00:22:28,640
correct conditional distribution and

655
00:22:27,039 --> 00:22:30,640
this is you can think of this both for

656
00:22:28,640 --> 00:22:32,760
proteins there's sort of an analog in

657
00:22:30,640 --> 00:22:35,600
the small molecule spaces is a problem

658
00:22:32,760 --> 00:22:37,600
that kind of appears everywhere um and

659
00:22:35,600 --> 00:22:39,240
so there's and and also people like

660
00:22:37,600 --> 00:22:40,840
don't really I don't know why they don't

661
00:22:39,240 --> 00:22:43,039
talk about this they just present one of

662
00:22:40,840 --> 00:22:44,799
these as this is obviously the right way

663
00:22:43,039 --> 00:22:46,679
to do it and I think it's interesting to

664
00:22:44,799 --> 00:22:48,000
ask which way provides where what are

665
00:22:46,679 --> 00:22:49,600
the pros and cons of these different

666
00:22:48,000 --> 00:22:51,000
methods and I'll I'll talk a little bit

667
00:22:49,600 --> 00:22:53,080
about that but the first thing you can

668
00:22:51,000 --> 00:22:54,559
do is just kind of actually bake it in

669
00:22:53,080 --> 00:22:56,200
which is to say you don't start with an

670
00:22:54,559 --> 00:22:58,960
unconditional model you ju you just

671
00:22:56,200 --> 00:23:01,039
start with um all the x's and Y you have

672
00:22:58,960 --> 00:23:02,679
access to and you just directly build

673
00:23:01,039 --> 00:23:05,559
this conditional model and then you

674
00:23:02,679 --> 00:23:08,559
sample X conditioned on y this is not

675
00:23:05,559 --> 00:23:10,159
very commonly done another thing you do

676
00:23:08,559 --> 00:23:12,400
is you start with that P of X this

677
00:23:10,159 --> 00:23:14,640
unconditional model and then you use the

678
00:23:12,400 --> 00:23:16,600
predictive model or the data and you

679
00:23:14,640 --> 00:23:18,720
update it and you have to retrain the

680
00:23:16,600 --> 00:23:20,840
model it has a new set of parameters and

681
00:23:18,720 --> 00:23:22,799
now you have this one static model that

682
00:23:20,840 --> 00:23:24,679
you can sample from for the condition

683
00:23:22,799 --> 00:23:26,279
you care about and the third one and I

684
00:23:24,679 --> 00:23:28,520
feel like John Ingram in his chroma

685
00:23:26,279 --> 00:23:31,320
paper highlighted this um particularly

686
00:23:28,520 --> 00:23:34,640
nicely which is that you can also take

687
00:23:31,320 --> 00:23:38,320
this P ofx model and not change it at

688
00:23:34,640 --> 00:23:40,400
all but play like Lego blocks with your

689
00:23:38,320 --> 00:23:42,600
like a classifier over the property you

690
00:23:40,400 --> 00:23:43,960
care about and in the in the parlance of

691
00:23:42,600 --> 00:23:45,679
machine learning and these diffusion

692
00:23:43,960 --> 00:23:47,640
flow models we talk about guiding the

693
00:23:45,679 --> 00:23:49,400
model and so what's nice about this it

694
00:23:47,640 --> 00:23:51,960
is very plug andplay you can put all

695
00:23:49,400 --> 00:23:54,159
this energy and and all this data into

696
00:23:51,960 --> 00:23:56,000
an unconditional model get it to be as

697
00:23:54,159 --> 00:23:58,240
good as possible and instead of having

698
00:23:56,000 --> 00:24:00,480
to like retrain it for each new

699
00:23:58,240 --> 00:24:02,320
condition you can take your classifier

700
00:24:00,480 --> 00:24:04,200
and as you're generating a new sample

701
00:24:02,320 --> 00:24:07,080
you they kind of play together to

702
00:24:04,200 --> 00:24:08,600
generate the right kind of sample it's a

703
00:24:07,080 --> 00:24:10,799
very technical area and I'm just trying

704
00:24:08,600 --> 00:24:12,600
to give you a flavor here of how it

705
00:24:10,799 --> 00:24:14,480
works and so this is I would say one of

706
00:24:12,600 --> 00:24:16,200
the most popular ways in particular

707
00:24:14,480 --> 00:24:18,320
because in the Sciences we do often have

708
00:24:16,200 --> 00:24:20,159
a lot of unsupervised data and we want

709
00:24:18,320 --> 00:24:23,440
to put a lot of these domain specific

710
00:24:20,159 --> 00:24:25,200
inductive biases into a giant pfx model

711
00:24:23,440 --> 00:24:27,080
and then we want to um be able to reuse

712
00:24:25,200 --> 00:24:30,760
it to do different things for different

713
00:24:27,080 --> 00:24:32,520
conditioning exercises is and so uh who

714
00:24:30,760 --> 00:24:36,360
who knows B rule I I get to read on the

715
00:24:32,520 --> 00:24:37,960
room whoa all right A lot of people know

716
00:24:36,360 --> 00:24:39,559
base rule if you don't don't worry all

717
00:24:37,960 --> 00:24:42,159
all I want to make a point here is that

718
00:24:39,559 --> 00:24:43,960
if you have an unconditional model and

719
00:24:42,159 --> 00:24:46,440
you have a classifier for the property

720
00:24:43,960 --> 00:24:48,120
you care about then in order to invert

721
00:24:46,440 --> 00:24:50,399
the order what you really want is you

722
00:24:48,120 --> 00:24:51,799
want to generate sequences based uh

723
00:24:50,399 --> 00:24:53,399
conditioned on a property you have to

724
00:24:51,799 --> 00:24:55,120
use this base rule which is just this

725
00:24:53,399 --> 00:24:57,960
very simple but very powerful widely

726
00:24:55,120 --> 00:24:59,720
used equation and so I actually don't

727
00:24:57,960 --> 00:25:01,320
understand this reference my my student

728
00:24:59,720 --> 00:25:02,960
did it does somebody get it it's some

729
00:25:01,320 --> 00:25:04,120
popular culture that I they always have

730
00:25:02,960 --> 00:25:06,880
these jokes I don't know what they mean

731
00:25:04,120 --> 00:25:09,039
but people laugh so I left it um but the

732
00:25:06,880 --> 00:25:10,960
problem is like so I I understand it's

733
00:25:09,039 --> 00:25:13,159
bad this is bad and that says down bad

734
00:25:10,960 --> 00:25:14,679
so I'm happy to keep it on the slide um

735
00:25:13,159 --> 00:25:16,760
so this is basically generally

736
00:25:14,679 --> 00:25:18,080
intractable like we cannot deal with

737
00:25:16,760 --> 00:25:19,640
this and if you don't know why don't

738
00:25:18,080 --> 00:25:21,080
worry I'll just tell you and so what

739
00:25:19,640 --> 00:25:22,720
that means is if I have a generative

740
00:25:21,080 --> 00:25:24,320
model that's unconditional and when I

741
00:25:22,720 --> 00:25:26,679
combine it with a classifier I have this

742
00:25:24,320 --> 00:25:28,000
huge problem and so diffusion models are

743
00:25:26,679 --> 00:25:29,279
very nice again don't worry about all

744
00:25:28,000 --> 00:25:31,120
the math here I'm just going to tell the

745
00:25:29,279 --> 00:25:33,200
story very simply but what happens in

746
00:25:31,120 --> 00:25:34,440
diffusion models is you you do this

747
00:25:33,200 --> 00:25:36,840
thing where you actually use the

748
00:25:34,440 --> 00:25:38,600
gradient of the of the density that you

749
00:25:36,840 --> 00:25:39,840
care about instead of getting just this

750
00:25:38,600 --> 00:25:41,520
thing you want to take the gradient of

751
00:25:39,840 --> 00:25:43,279
it and once you have that in your hands

752
00:25:41,520 --> 00:25:44,720
you can generate samples by following a

753
00:25:43,279 --> 00:25:46,240
procedure and what's nice is when you

754
00:25:44,720 --> 00:25:48,120
take the gradient of this whole thing

755
00:25:46,240 --> 00:25:49,159
don't worry what happens is that term

756
00:25:48,120 --> 00:25:50,919
that we don't know how to deal with it

757
00:25:49,159 --> 00:25:54,120
just falls out and this is one of the

758
00:25:50,919 --> 00:25:57,000
powers of using guidance in in uh in

759
00:25:54,120 --> 00:25:59,399
diffusion models is we can um we can

760
00:25:57,000 --> 00:26:02,799
just get rid of that nasty that nasty

761
00:25:59,399 --> 00:26:04,600
term so however that all the work in

762
00:26:02,799 --> 00:26:07,039
diffusion and flow models for a long

763
00:26:04,600 --> 00:26:09,480
time was just in real valued spaces so

764
00:26:07,039 --> 00:26:11,159
on um like things like images which are

765
00:26:09,480 --> 00:26:14,880
actually discreet but we think of and

766
00:26:11,159 --> 00:26:17,120
treat as continuous uh and so but in we

767
00:26:14,880 --> 00:26:20,559
care a lot about designing sequences or

768
00:26:17,120 --> 00:26:21,919
you may care about designing uh uh um

769
00:26:20,559 --> 00:26:24,440
small molecules and these are

770
00:26:21,919 --> 00:26:26,360
fundamentally discreet objects right

771
00:26:24,440 --> 00:26:28,919
like it's very different to generate a

772
00:26:26,360 --> 00:26:31,120
real valued say 3D structure structure

773
00:26:28,919 --> 00:26:33,720
or image than it is to say generate a

774
00:26:31,120 --> 00:26:35,640
sequence of things and we know for

775
00:26:33,720 --> 00:26:36,880
example chat GPT generates a sequence of

776
00:26:35,640 --> 00:26:39,520
things but it doesn't use these very

777
00:26:36,880 --> 00:26:40,880
powerful models at the moment these

778
00:26:39,520 --> 00:26:43,279
diffusion and flow models which have

779
00:26:40,880 --> 00:26:46,480
really shown to be incredibly useful in

780
00:26:43,279 --> 00:26:48,919
this in um especially in computer vision

781
00:26:46,480 --> 00:26:50,640
and now I was actually quite skeptical

782
00:26:48,919 --> 00:26:52,039
that they were useful on discrete States

783
00:26:50,640 --> 00:26:53,320
spases and I kept telling my students so

784
00:26:52,039 --> 00:26:54,840
it's totally different why do you want

785
00:26:53,320 --> 00:26:56,360
to use these models here but they

786
00:26:54,840 --> 00:26:58,919
eventually convinced me and that's

787
00:26:56,360 --> 00:27:01,399
another topic of discussion is like why

788
00:26:58,919 --> 00:27:03,960
are these useful CU it's very different

789
00:27:01,399 --> 00:27:05,399
um but what what happens is it's so

790
00:27:03,960 --> 00:27:07,080
different in fact that you literally

791
00:27:05,399 --> 00:27:09,919
can't even use the same underlying

792
00:27:07,080 --> 00:27:11,880
algorithms models probability anything

793
00:27:09,919 --> 00:27:13,679
everything falls apart and the reason it

794
00:27:11,880 --> 00:27:16,559
falls apart is when you go to a discrete

795
00:27:13,679 --> 00:27:18,600
space all these gradients don't exist

796
00:27:16,559 --> 00:27:19,640
basically like you can get no you can't

797
00:27:18,600 --> 00:27:21,360
compute them or you can't get

798
00:27:19,640 --> 00:27:24,320
information from these gradients and so

799
00:27:21,360 --> 00:27:25,919
all the underlying Machinery is broken

800
00:27:24,320 --> 00:27:27,840
and so there's a fantastic set of work

801
00:27:25,919 --> 00:27:29,399
by kampbell at all in particular that

802
00:27:27,840 --> 00:27:30,960
gets a around this by using a very

803
00:27:29,399 --> 00:27:33,279
technical Machinery that I'm not going

804
00:27:30,960 --> 00:27:34,880
to go into called continuous time markof

805
00:27:33,279 --> 00:27:37,559
processes and I think that's the most

806
00:27:34,880 --> 00:27:40,360
elegant solution to do diffusion over

807
00:27:37,559 --> 00:27:42,039
discrete State spaces and so what my but

808
00:27:40,360 --> 00:27:44,440
what they didn't do what was missing was

809
00:27:42,039 --> 00:27:46,440
how can I generate in a conditional

810
00:27:44,440 --> 00:27:48,320
manner how can I condition on the

811
00:27:46,440 --> 00:27:49,880
property I care about and there's no

812
00:27:48,320 --> 00:27:52,679
point having a fantastic generative

813
00:27:49,880 --> 00:27:54,399
model if you can't condition um and on

814
00:27:52,679 --> 00:27:56,159
the property you care about and so

815
00:27:54,399 --> 00:27:58,519
basically my students have filled in

816
00:27:56,159 --> 00:28:00,240
this Gap so Hunter bear and Stefan this

817
00:27:58,519 --> 00:28:01,640
project and and again it's quite

818
00:28:00,240 --> 00:28:03,799
technical so I'm just trying to give you

819
00:28:01,640 --> 00:28:05,559
an overview of the main idea here but

820
00:28:03,799 --> 00:28:07,279
fundamentally you replace these

821
00:28:05,559 --> 00:28:09,440
gradients with something called a rate

822
00:28:07,279 --> 00:28:11,760
Matrix here uh you don't really need to

823
00:28:09,440 --> 00:28:14,519
worry about the math and and just simply

824
00:28:11,760 --> 00:28:17,320
by going into this continuous time uh

825
00:28:14,519 --> 00:28:18,640
what happens is you I know I'm not

826
00:28:17,320 --> 00:28:20,159
you're not going to get it here but you

827
00:28:18,640 --> 00:28:21,640
could look in the paper if you want but

828
00:28:20,159 --> 00:28:25,200
you get around the problem of the

829
00:28:21,640 --> 00:28:27,080
normalizing constant um uh at the same

830
00:28:25,200 --> 00:28:29,440
time as being able to kind of correctly

831
00:28:27,080 --> 00:28:30,640
do diffusion in these spaces and so this

832
00:28:29,440 --> 00:28:32,240
was something people actually have been

833
00:28:30,640 --> 00:28:34,519
trying to do saying I don't know how to

834
00:28:32,240 --> 00:28:36,919
do guidance in on on these kinds of

835
00:28:34,519 --> 00:28:38,880
models and so now you can do it in a

836
00:28:36,919 --> 00:28:40,799
like a like there's there's a lot of

837
00:28:38,880 --> 00:28:42,720
derivations here so it's like it's a

838
00:28:40,799 --> 00:28:44,440
very clean uh system that they've

839
00:28:42,720 --> 00:28:47,519
derived where you really can do this

840
00:28:44,440 --> 00:28:51,519
with the correct formalism

841
00:28:47,519 --> 00:28:53,720
now um and I I mean I don't know who let

842
00:28:51,519 --> 00:28:55,320
me just take a poll who in the audience

843
00:28:53,720 --> 00:28:56,559
does this line mean something to and if

844
00:28:55,320 --> 00:28:58,720
it's too few I'm not even going to talk

845
00:28:56,559 --> 00:29:00,240
about this slide okay

846
00:28:58,720 --> 00:29:01,880
like a few very briefly again the rest

847
00:29:00,240 --> 00:29:03,760
of you just tune out for 10 seconds so

848
00:29:01,880 --> 00:29:05,640
these are with the in in regular

849
00:29:03,760 --> 00:29:07,720
continuous space you get the

850
00:29:05,640 --> 00:29:08,880
conditioning um distribution gradient

851
00:29:07,720 --> 00:29:11,880
that you want by taking the

852
00:29:08,880 --> 00:29:13,399
unconditional one and and adding uh the

853
00:29:11,880 --> 00:29:16,880
gradient of the classifier this is

854
00:29:13,399 --> 00:29:18,760
called classifier um uh guided uh

855
00:29:16,880 --> 00:29:20,279
generation and so basically what my

856
00:29:18,760 --> 00:29:21,840
students show it looks very simple here

857
00:29:20,279 --> 00:29:23,519
but again it took a lot of cleverness to

858
00:29:21,840 --> 00:29:25,399
actually figure this out is you get a

859
00:29:23,519 --> 00:29:27,519
very analogous equation and basically

860
00:29:25,399 --> 00:29:29,880
the core entity in these models on

861
00:29:27,519 --> 00:29:31,519
discret St bases becomes this rate

862
00:29:29,880 --> 00:29:33,320
Matrix instead of the gradient and they

863
00:29:31,519 --> 00:29:35,159
show that you get a very analogous

864
00:29:33,320 --> 00:29:37,600
update and so you get a very similar

865
00:29:35,159 --> 00:29:43,399
ability to Plug and Play very nicely and

866
00:29:37,600 --> 00:29:43,399
and I'll leave it at that um

867
00:29:46,480 --> 00:29:51,919
just and this was one of these um sort

868
00:29:49,399 --> 00:29:54,000
of toy nups papers just to bring an idea

869
00:29:51,919 --> 00:29:56,200
into the world so I actually think I

870
00:29:54,000 --> 00:29:59,880
love nurs in the sense that I take nurs

871
00:29:56,200 --> 00:30:01,960
as a like a publish creative interesting

872
00:29:59,880 --> 00:30:03,799
ideas and don't believe the results of

873
00:30:01,960 --> 00:30:05,039
venue but we have to put the results in

874
00:30:03,799 --> 00:30:06,360
there so we you know and I think it's

875
00:30:05,039 --> 00:30:08,159
not a bad idea you just shouldn't take

876
00:30:06,360 --> 00:30:09,840
them too literally right so not claiming

877
00:30:08,159 --> 00:30:10,960
we've like solved some new problem here

878
00:30:09,840 --> 00:30:12,919
although I think the third one actually

879
00:30:10,960 --> 00:30:14,679
is quite interesting and if we pushed it

880
00:30:12,919 --> 00:30:16,480
further it would be potentially solving

881
00:30:14,679 --> 00:30:17,960
a real problem but we basically applied

882
00:30:16,480 --> 00:30:19,360
this diffusion over a discret state

883
00:30:17,960 --> 00:30:21,279
spases for small molecules on a

884
00:30:19,360 --> 00:30:22,720
particular toy problem we don't really

885
00:30:21,279 --> 00:30:24,640
believe in but just demonstrates the

886
00:30:22,720 --> 00:30:26,519
Machinery works one for um some

887
00:30:24,640 --> 00:30:27,960
enhancers and DNA sequence and in this

888
00:30:26,519 --> 00:30:29,760
case Hunter actually trained his own

889
00:30:27,960 --> 00:30:31,960
inverse folding model and showed how to

890
00:30:29,760 --> 00:30:33,919
use this Machinery to generate things

891
00:30:31,960 --> 00:30:35,360
that um would be sequences that fold

892
00:30:33,919 --> 00:30:37,320
just as well as a regular inverse

893
00:30:35,360 --> 00:30:39,679
folding model but be more likely to

894
00:30:37,320 --> 00:30:42,960
additionally be predicted to be stable

895
00:30:39,679 --> 00:30:46,200
by guiding it um to be more

896
00:30:42,960 --> 00:30:48,039
stable okay so now I'm going to uh go

897
00:30:46,200 --> 00:30:49,840
into the like for those of you who found

898
00:30:48,039 --> 00:30:51,279
that too Technical and weird hopefully

899
00:30:49,840 --> 00:30:53,840
you can enjoy this part a little bit

900
00:30:51,279 --> 00:30:56,240
more this is a a long this this project

901
00:30:53,840 --> 00:30:58,279
took many many years it finally came out

902
00:30:56,240 --> 00:30:59,360
um earlier this year which blows my mind

903
00:30:58,279 --> 00:31:01,360
it feels like it should have come out a

904
00:30:59,360 --> 00:31:03,279
long time ago but um so David Baker is a

905
00:31:01,360 --> 00:31:05,559
colleague of mine who's a world expert

906
00:31:03,279 --> 00:31:06,440
in um directed Evolution and in

907
00:31:05,559 --> 00:31:08,559
particular one of the things he's

908
00:31:06,440 --> 00:31:11,039
focused on for a long time is using uh

909
00:31:08,559 --> 00:31:13,120
this Adeno Associated virus as a a

910
00:31:11,039 --> 00:31:15,240
delivery mechanism for uh for

911
00:31:13,120 --> 00:31:17,600
Therapeutics and so and this is this is

912
00:31:15,240 --> 00:31:20,039
the team of folks that have worked on it

913
00:31:17,600 --> 00:31:22,360
and so this actually has been very

914
00:31:20,039 --> 00:31:23,679
promising there are clinical trials that

915
00:31:22,360 --> 00:31:25,039
have shown this to be useful and you

916
00:31:23,679 --> 00:31:26,960
probably have read about it in the news

917
00:31:25,039 --> 00:31:29,200
now there's actually like this really

918
00:31:26,960 --> 00:31:30,919
happening where people are are getting a

919
00:31:29,200 --> 00:31:33,880
real win from delivering something with

920
00:31:30,919 --> 00:31:35,120
an aav but there the problem is that all

921
00:31:33,880 --> 00:31:36,760
of them basically have to be behind the

922
00:31:35,120 --> 00:31:38,840
bloodb brain barrier so the antibodies

923
00:31:36,760 --> 00:31:40,440
don't shut it down um there's a bunch of

924
00:31:38,840 --> 00:31:42,399
cells we don't know how to Target we

925
00:31:40,440 --> 00:31:44,679
don't know how to change the aav virus

926
00:31:42,399 --> 00:31:46,880
to Target the cells we want and not to

927
00:31:44,679 --> 00:31:48,240
Target the cells we don't want um so

928
00:31:46,880 --> 00:31:50,320
there's a still although it is like

929
00:31:48,240 --> 00:31:52,159
still a big it has had some good

930
00:31:50,320 --> 00:31:54,399
successes there's a long way to go if we

931
00:31:52,159 --> 00:31:56,399
want to use it more generally and so

932
00:31:54,399 --> 00:31:59,200
what the the this project was about was

933
00:31:56,399 --> 00:32:01,559
what I call uh from my perspective I

934
00:31:59,200 --> 00:32:03,080
think an understudied and really lovely

935
00:32:01,559 --> 00:32:05,039
problem for machine learning people to

936
00:32:03,080 --> 00:32:07,159
think about is Library design when I'm

937
00:32:05,039 --> 00:32:09,320
doing either like just from the get-go

938
00:32:07,159 --> 00:32:10,639
what's the first library of proteins I

939
00:32:09,320 --> 00:32:12,440
should measure in the lab and then

940
00:32:10,639 --> 00:32:14,000
iteratively how should I go about doing

941
00:32:12,440 --> 00:32:15,880
that and you could think of that as

942
00:32:14,000 --> 00:32:17,120
classical design of experiments but I

943
00:32:15,880 --> 00:32:18,919
think in the era of machine learning

944
00:32:17,120 --> 00:32:21,799
there's a lot of kind of interesting

945
00:32:18,919 --> 00:32:24,000
problems that get folded into there so

946
00:32:21,799 --> 00:32:25,760
the goal was to say that Dave does all

947
00:32:24,000 --> 00:32:27,679
these uh different campaigns to try to

948
00:32:25,760 --> 00:32:28,639
change AV like this or like this or like

949
00:32:27,679 --> 00:32:30,440
this

950
00:32:28,639 --> 00:32:32,039
uh to achieve different things and every

951
00:32:30,440 --> 00:32:34,120
time he does it he has to kind of start

952
00:32:32,039 --> 00:32:36,799
from scratch um with what's essentially

953
00:32:34,120 --> 00:32:39,000
a uniform library of proteins and so let

954
00:32:36,799 --> 00:32:41,679
me clarify what I mean by uniform

955
00:32:39,000 --> 00:32:43,200
library of proteins so in this case one

956
00:32:41,679 --> 00:32:45,919
of the things that's got him a lot of

957
00:32:43,200 --> 00:32:47,600
wins is to in his directed Evolution you

958
00:32:45,919 --> 00:32:50,600
need a form of um to you need some

959
00:32:47,600 --> 00:32:52,240
mechanism to induce variety to get to do

960
00:32:50,600 --> 00:32:54,639
the evolution and one of the ones that's

961
00:32:52,240 --> 00:32:56,799
very powerful in a is to insert a 21

962
00:32:54,639 --> 00:32:58,519
nucleotide uh insertion into a very

963
00:32:56,799 --> 00:33:00,720
specific part of the caps and this

964
00:32:58,519 --> 00:33:03,559
produces a tremendous amount of variety

965
00:33:00,720 --> 00:33:05,200
in phen many phenotypes um and so this

966
00:33:03,559 --> 00:33:06,919
is something that they often start a

967
00:33:05,200 --> 00:33:09,240
campaign with is they just say you know

968
00:33:06,919 --> 00:33:13,600
what for this 21 Mr nucleotide let's

969
00:33:09,240 --> 00:33:15,639
just U make random 21 Ms uniformly and

970
00:33:13,600 --> 00:33:17,559
insert them um at scale like at

971
00:33:15,639 --> 00:33:20,000
something like 10 to the six and then do

972
00:33:17,559 --> 00:33:21,799
directed Evolution but when you do that

973
00:33:20,000 --> 00:33:23,440
although this is a powerful mechanism

974
00:33:21,799 --> 00:33:25,240
what happens with those libraries it

975
00:33:23,440 --> 00:33:27,919
starts sort of at uniform it's not quite

976
00:33:25,240 --> 00:33:29,639
uniform it's technically called nnk to

977
00:33:27,919 --> 00:33:30,679
reduce stop codons but for if you don't

978
00:33:29,639 --> 00:33:32,120
know what that means you can just think

979
00:33:30,679 --> 00:33:35,039
of it as uniform over the space of

980
00:33:32,120 --> 00:33:37,080
nucleotides with a small tweak and when

981
00:33:35,039 --> 00:33:39,039
you do that you waste half the library

982
00:33:37,080 --> 00:33:40,600
from the get-go and the reason you waste

983
00:33:39,039 --> 00:33:42,360
half of it is half those viruses just

984
00:33:40,600 --> 00:33:44,360
don't package like they're basically not

985
00:33:42,360 --> 00:33:46,039
functional and the game of directed

986
00:33:44,360 --> 00:33:48,360
evolution is a game of getting yourself

987
00:33:46,039 --> 00:33:49,840
the best lottery tickets possible and so

988
00:33:48,360 --> 00:33:52,000
here we know that half our lottery

989
00:33:49,840 --> 00:33:54,279
tickets basically are failing from the

990
00:33:52,000 --> 00:33:56,399
get-go and they've no hope of helping us

991
00:33:54,279 --> 00:33:57,799
in our directed Evolution experiment and

992
00:33:56,399 --> 00:33:59,320
you can filter them out but you're

993
00:33:57,799 --> 00:34:01,519
losing those tickets so you're losing

994
00:33:59,320 --> 00:34:03,320
like your shots on goal and so the idea

995
00:34:01,519 --> 00:34:06,320
was can we create a better starting

996
00:34:03,320 --> 00:34:08,119
library for any Downstream task that

997
00:34:06,320 --> 00:34:10,200
Dave may have in his lab any um

998
00:34:08,119 --> 00:34:11,919
engineering goal for aav and so what

999
00:34:10,200 --> 00:34:14,760
does it mean that it's better it means

1000
00:34:11,919 --> 00:34:17,480
we have a just as much ideally just as

1001
00:34:14,760 --> 00:34:19,399
much diversity as uniform um or close to

1002
00:34:17,480 --> 00:34:21,399
that but have better packaging

1003
00:34:19,399 --> 00:34:23,440
efficiency on the whole for the whole

1004
00:34:21,399 --> 00:34:24,720
Library so that we get to keep all our

1005
00:34:23,440 --> 00:34:27,320
lottery

1006
00:34:24,720 --> 00:34:28,720
tickets and so so this is what we set

1007
00:34:27,320 --> 00:34:30,440
out to do and this is what we have done

1008
00:34:28,720 --> 00:34:31,879
and this is now one of the libraries

1009
00:34:30,440 --> 00:34:34,839
that he uses so the first thing we did

1010
00:34:31,879 --> 00:34:37,119
is we took his nnk Library which came

1011
00:34:34,839 --> 00:34:38,760
with sequences and some with log and

1012
00:34:37,119 --> 00:34:41,000
Richmond um I could tell you there's a

1013
00:34:38,760 --> 00:34:42,359
whole paper about like not how we did it

1014
00:34:41,000 --> 00:34:44,240
in this paper but I'll I'll give you a

1015
00:34:42,359 --> 00:34:46,040
teaser if there's time but we built a

1016
00:34:44,240 --> 00:34:47,800
supervised model from sequence base of

1017
00:34:46,040 --> 00:34:49,639
21 nucleotides actually I think we did

1018
00:34:47,800 --> 00:34:52,079
it at the amino acid level to the

1019
00:34:49,639 --> 00:34:54,399
packaging efficiency for the library he

1020
00:34:52,079 --> 00:34:56,159
was already using this uniform or nnk

1021
00:34:54,399 --> 00:34:57,720
Library don't worry about the details

1022
00:34:56,159 --> 00:34:59,880
here but basically we pick out a neural

1023
00:34:57,720 --> 00:35:01,839
Network architecture very simple one

1024
00:34:59,880 --> 00:35:03,920
quite small that just does best at

1025
00:35:01,839 --> 00:35:07,680
predicting packaging efficiency from

1026
00:35:03,920 --> 00:35:09,160
sequence on the nnk library and then we

1027
00:35:07,680 --> 00:35:11,839
wanted to check how well this model

1028
00:35:09,160 --> 00:35:14,040
worked so we picked four uh variants uh

1029
00:35:11,839 --> 00:35:15,800
that we one five that we had not seen in

1030
00:35:14,040 --> 00:35:17,839
the training data including the global

1031
00:35:15,800 --> 00:35:19,359
predicted maximum and then sort of like

1032
00:35:17,839 --> 00:35:21,560
a dilution series and show that this

1033
00:35:19,359 --> 00:35:23,800
model actually works really well across

1034
00:35:21,560 --> 00:35:26,079
the whole range so this model actually

1035
00:35:23,800 --> 00:35:27,240
understands the whole scope of the of

1036
00:35:26,079 --> 00:35:29,440
this problem we don't have a lot of

1037
00:35:27,240 --> 00:35:31,280
extrap ation issues here we just want

1038
00:35:29,440 --> 00:35:33,119
good packaging and we have seen some

1039
00:35:31,280 --> 00:35:35,320
good packaging and the model works in

1040
00:35:33,119 --> 00:35:38,200
the whole range and so now what I this

1041
00:35:35,320 --> 00:35:39,680
is sort of a simple uh idea here but one

1042
00:35:38,200 --> 00:35:42,040
that's missing from a lot of machine

1043
00:35:39,680 --> 00:35:44,079
learning library design so what you

1044
00:35:42,040 --> 00:35:45,599
really you almost always have some sort

1045
00:35:44,079 --> 00:35:47,480
of trade-off when you're designing and

1046
00:35:45,599 --> 00:35:49,119
what the trade-off we have here is each

1047
00:35:47,480 --> 00:35:51,119
point here is a library this is a

1048
00:35:49,119 --> 00:35:53,240
machine learning library design D1

1049
00:35:51,119 --> 00:35:55,960
machine learning library design D2 this

1050
00:35:53,240 --> 00:35:58,760
x here is the nnk library so every point

1051
00:35:55,960 --> 00:36:00,359
is a library and each Library specifies

1052
00:35:58,760 --> 00:36:02,800
I think I forgot to mention this it

1053
00:36:00,359 --> 00:36:05,119
specifies the probability of actg at

1054
00:36:02,800 --> 00:36:07,599
each of the 21 positions so instead of

1055
00:36:05,119 --> 00:36:09,760
uniform I can make it non-uniform but I

1056
00:36:07,599 --> 00:36:11,839
can only control it at each position

1057
00:36:09,760 --> 00:36:13,800
separately um but that's still quite a

1058
00:36:11,839 --> 00:36:15,680
bit of control and all the Machinery I'm

1059
00:36:13,800 --> 00:36:17,119
telling you about can be extended when

1060
00:36:15,680 --> 00:36:19,119
you have more control if you could do

1061
00:36:17,119 --> 00:36:21,000
full synthesis you can still do what I'm

1062
00:36:19,119 --> 00:36:23,359
going to tell you about here but so in

1063
00:36:21,000 --> 00:36:26,280
our case this nnk Library it has the

1064
00:36:23,359 --> 00:36:28,359
most um of all the libraries we computed

1065
00:36:26,280 --> 00:36:30,480
it has the most pairwise diversity and

1066
00:36:28,359 --> 00:36:32,119
that's cuz it's very close to uniform

1067
00:36:30,480 --> 00:36:33,760
and on this side over here if you take

1068
00:36:32,119 --> 00:36:35,640
this Library don't don't worry about the

1069
00:36:33,760 --> 00:36:37,680
spread this should just be a clean line

1070
00:36:35,640 --> 00:36:39,400
is an optimization issue but if you take

1071
00:36:37,680 --> 00:36:41,920
um one of Let's just say this Library

1072
00:36:39,400 --> 00:36:44,400
here this library has really high

1073
00:36:41,920 --> 00:36:46,839
expected packaging efficiency which is

1074
00:36:44,400 --> 00:36:48,599
this axis but it has really low

1075
00:36:46,839 --> 00:36:51,119
diversity and that's because there's a

1076
00:36:48,599 --> 00:36:53,920
tradeoff and so the nnk library has

1077
00:36:51,119 --> 00:36:55,240
really great diversity but it has crap

1078
00:36:53,920 --> 00:36:57,599
packaging efficiency and that's where

1079
00:36:55,240 --> 00:36:59,280
I'm losing most of my library over here

1080
00:36:57,599 --> 00:37:01,400
have a library that has fantastic

1081
00:36:59,280 --> 00:37:02,960
packaging efficiency but it has no

1082
00:37:01,400 --> 00:37:04,839
diversity because what is it doing it's

1083
00:37:02,960 --> 00:37:06,520
taking the one sequence the package is

1084
00:37:04,839 --> 00:37:08,599
best and it's making the whole library

1085
00:37:06,520 --> 00:37:10,119
look just like that sequence basically

1086
00:37:08,599 --> 00:37:11,839
and so we know we probably want to

1087
00:37:10,119 --> 00:37:13,720
operate somewhere in between those two

1088
00:37:11,839 --> 00:37:15,880
extremes and in this case we were pretty

1089
00:37:13,720 --> 00:37:17,720
lucky so in these trade-off curves if

1090
00:37:15,880 --> 00:37:19,880
you have a vertical position so this

1091
00:37:17,720 --> 00:37:22,119
machine learning D3 is essentially

1092
00:37:19,880 --> 00:37:24,040
vertically above the nnk library what

1093
00:37:22,119 --> 00:37:26,280
that means is I don't take any hit in

1094
00:37:24,040 --> 00:37:28,680
the diversity but I have vastly better

1095
00:37:26,280 --> 00:37:31,119
packaging efficiency so this is a Nob

1096
00:37:28,680 --> 00:37:32,640
brainer wind over the other Library what

1097
00:37:31,119 --> 00:37:34,040
we don't know is it possible this

1098
00:37:32,640 --> 00:37:35,920
library is better like we don't

1099
00:37:34,040 --> 00:37:37,520
generally know where we want to operate

1100
00:37:35,920 --> 00:37:40,119
on such a curve all we know is in

1101
00:37:37,520 --> 00:37:42,880
vertical lines the top point is better

1102
00:37:40,119 --> 00:37:45,440
so we decided uh on heris of sort of the

1103
00:37:42,880 --> 00:37:47,079
Kink here and then uh some other herti

1104
00:37:45,440 --> 00:37:48,440
we just picked these three libraries and

1105
00:37:47,079 --> 00:37:49,800
we measured the full tighter of these

1106
00:37:48,440 --> 00:37:52,119
libraries in the lab and checked that

1107
00:37:49,800 --> 00:37:55,640
indeed they have the kind of behavior we

1108
00:37:52,119 --> 00:37:57,839
expect uh to see and then um through an

1109
00:37:55,640 --> 00:38:00,240
experiment and sort of a decision making

1110
00:37:57,839 --> 00:38:02,240
process we picked this D2 to go forward

1111
00:38:00,240 --> 00:38:04,680
with and that's because after packaging

1112
00:38:02,240 --> 00:38:07,160
it and checking uh the variants it had

1113
00:38:04,680 --> 00:38:09,680
the most uh diversity variants post

1114
00:38:07,160 --> 00:38:11,680
packaging and so the idea was now we've

1115
00:38:09,680 --> 00:38:14,359
designed a new library that's meant to

1116
00:38:11,680 --> 00:38:16,680
replace this uniform nnk library that

1117
00:38:14,359 --> 00:38:18,920
will be better for any Downstream

1118
00:38:16,680 --> 00:38:21,280
engineering campaign for a property I've

1119
00:38:18,920 --> 00:38:23,440
not even thought about in designing this

1120
00:38:21,280 --> 00:38:25,400
Library just because I'm keeping more of

1121
00:38:23,440 --> 00:38:27,720
my lottery tickets uh in the sense that

1122
00:38:25,400 --> 00:38:29,760
I have a diverse library but almost

1123
00:38:27,720 --> 00:38:31,880
things are are more likely to package

1124
00:38:29,760 --> 00:38:33,200
and I'm less Lo likely to lose those

1125
00:38:31,880 --> 00:38:35,920
tickets and if I have more lottery

1126
00:38:33,200 --> 00:38:38,319
tickets I'm more likely to win the game

1127
00:38:35,920 --> 00:38:40,240
and so as a proof of principle that in

1128
00:38:38,319 --> 00:38:42,960
fact this was true we took a protein

1129
00:38:40,240 --> 00:38:44,640
engineering um task that the the model

1130
00:38:42,960 --> 00:38:46,880
and our procedure in our library knew

1131
00:38:44,640 --> 00:38:50,079
nothing about and the task was to infect

1132
00:38:46,880 --> 00:38:52,040
um non-human primate uh brain cells many

1133
00:38:50,079 --> 00:38:53,400
of which are very hard to infect and

1134
00:38:52,040 --> 00:38:55,480
what we showed is that even though our

1135
00:38:53,400 --> 00:38:57,160
model knew nothing about the brain or

1136
00:38:55,480 --> 00:38:58,680
these brain cells or anything that just

1137
00:38:57,160 --> 00:39:00,640
because we had sort of a better set of

1138
00:38:58,680 --> 00:39:02,720
lottery tickets that we had a much much

1139
00:39:00,640 --> 00:39:04,400
higher probability of infecting those

1140
00:39:02,720 --> 00:39:05,440
brain cells than the library that they

1141
00:39:04,400 --> 00:39:08,079
would have

1142
00:39:05,440 --> 00:39:10,359
used um and we're and now there's a

1143
00:39:08,079 --> 00:39:11,880
bunch of of follow-ups uh but it's not

1144
00:39:10,359 --> 00:39:13,680
on pre-print or anything yet but we're

1145
00:39:11,880 --> 00:39:15,520
using this type of thinking to keep

1146
00:39:13,680 --> 00:39:17,599
going further and further to make these

1147
00:39:15,520 --> 00:39:20,359
aavs more suitable for Therapeutics in

1148
00:39:17,599 --> 00:39:23,000
particular in the brain and on micral

1149
00:39:20,359 --> 00:39:24,520
cells um so I think in the interest of

1150
00:39:23,000 --> 00:39:26,440
time I'm going to skip over this but I'm

1151
00:39:24,520 --> 00:39:28,319
going to just make it a teaser uh and so

1152
00:39:26,440 --> 00:39:30,400
what I'm going to say is I said that um

1153
00:39:28,319 --> 00:39:34,040
I the whole thing started by building a

1154
00:39:30,400 --> 00:39:36,560
supervised model um over um the seven

1155
00:39:34,040 --> 00:39:39,079
amino acids uh which represent the 21

1156
00:39:36,560 --> 00:39:41,440
nucleotide insert to the what this is

1157
00:39:39,079 --> 00:39:44,240
technically is this log enrichment so

1158
00:39:41,440 --> 00:39:46,520
okay who who's computed a log

1159
00:39:44,240 --> 00:39:48,319
enrichment all right fewer people than

1160
00:39:46,520 --> 00:39:51,359
who knew Bas rule I'm very surprised by

1161
00:39:48,319 --> 00:39:53,960
you guys so this is a standard way to

1162
00:39:51,359 --> 00:39:55,880
deal with sequencing data is you um have

1163
00:39:53,960 --> 00:39:58,240
some sequence of interest and you put it

1164
00:39:55,880 --> 00:39:59,720
through a select X and then you and you

1165
00:39:58,240 --> 00:40:01,400
it has some amount in the starting

1166
00:39:59,720 --> 00:40:03,520
library and you count how many times it

1167
00:40:01,400 --> 00:40:05,560
happened you put it through a selection

1168
00:40:03,520 --> 00:40:07,359
um for example packaging selection and

1169
00:40:05,560 --> 00:40:09,319
then you get another account of how many

1170
00:40:07,359 --> 00:40:11,440
times it happened and then you compute

1171
00:40:09,319 --> 00:40:13,000
this like very simple log enrichment

1172
00:40:11,440 --> 00:40:14,720
which is really just the ratio of how

1173
00:40:13,000 --> 00:40:16,319
many times it was in the after the

1174
00:40:14,720 --> 00:40:17,800
selection before the selection just

1175
00:40:16,319 --> 00:40:19,960
normalized by total read count so this

1176
00:40:17,800 --> 00:40:22,440
is like a very widely used um

1177
00:40:19,960 --> 00:40:25,000
statistical estimate to understand which

1178
00:40:22,440 --> 00:40:27,000
sequences are more likely to package or

1179
00:40:25,000 --> 00:40:28,760
pass any selection at all outside of

1180
00:40:27,000 --> 00:40:31,079
protein engineering is a very very kind

1181
00:40:28,760 --> 00:40:32,560
of widely used idea and so this is

1182
00:40:31,079 --> 00:40:34,400
actually what we did to build these

1183
00:40:32,560 --> 00:40:36,400
supervised models but in some other

1184
00:40:34,400 --> 00:40:38,160
projects we realized this didn't work

1185
00:40:36,400 --> 00:40:40,960
and so I'm not going to go into it but

1186
00:40:38,160 --> 00:40:42,680
basically in our PL in our the AV these

1187
00:40:40,960 --> 00:40:44,640
were 21 nucleotides and we could just

1188
00:40:42,680 --> 00:40:46,119
count how many times they happened but

1189
00:40:44,640 --> 00:40:48,200
imagine that this was a really big

1190
00:40:46,119 --> 00:40:49,920
region and all you had were short reads

1191
00:40:48,200 --> 00:40:51,720
now you can't count and you can't just

1192
00:40:49,920 --> 00:40:52,960
get a log enrichment and you have a

1193
00:40:51,720 --> 00:40:55,079
problem and so basically if you're

1194
00:40:52,960 --> 00:40:56,599
interested go check out this paper which

1195
00:40:55,079 --> 00:40:59,000
shows you how to handle a variety of

1196
00:40:56,599 --> 00:41:01,280
situations can't natively handle but in

1197
00:40:59,000 --> 00:41:04,119
addition even in the ones you can handle

1198
00:41:01,280 --> 00:41:06,520
just has better statistical power so I'm

1199
00:41:04,119 --> 00:41:09,359
going to stop there and leave some time

1200
00:41:06,520 --> 00:41:11,079
for questions but maybe I'll just um

1201
00:41:09,359 --> 00:41:12,359
yeah and I'll leave this up if then if

1202
00:41:11,079 --> 00:41:13,800
if you want you can ask questions about

1203
00:41:12,359 --> 00:41:18,319
it or read it and and otherwise I'll

1204
00:41:13,800 --> 00:41:18,319
just take some questions now thank

1205
00:41:21,800 --> 00:41:28,520
you thank you Jennifer uh so I requested

1206
00:41:25,319 --> 00:41:30,800
you to come um down at the microphone if

1207
00:41:28,520 --> 00:41:34,040
you have questions so we have 10 minutes

1208
00:41:30,800 --> 00:41:36,240
for questions please come down and ask

1209
00:41:34,040 --> 00:41:38,440
and we have also online moderator for

1210
00:41:36,240 --> 00:41:38,440
the

1211
00:41:51,720 --> 00:41:56,200
questions hello I just wanted to ask uh

1212
00:41:54,480 --> 00:41:58,160
how do you measure the edge of data the

1213
00:41:56,200 --> 00:42:01,280
slide you showed with measure the what

1214
00:41:58,160 --> 00:42:02,839
the edge of data Edge Edge yeah you you

1215
00:42:01,280 --> 00:42:05,480
you mentioned that you use machine

1216
00:42:02,839 --> 00:42:08,400
learning and physics based approach in a

1217
00:42:05,480 --> 00:42:11,560
slide to combine them where the physics

1218
00:42:08,400 --> 00:42:13,800
based uh oh I see yeah so that's I think

1219
00:42:11,560 --> 00:42:15,079
which let me rephrase the question I

1220
00:42:13,800 --> 00:42:17,119
think you're saying I mentioned in one

1221
00:42:15,079 --> 00:42:18,760
of the like things I didn't have a whole

1222
00:42:17,119 --> 00:42:21,520
set of slides for that Hunter had a

1223
00:42:18,760 --> 00:42:23,760
project in blend in cohes coherently

1224
00:42:21,520 --> 00:42:26,119
blending biophysics based bottles with

1225
00:42:23,760 --> 00:42:27,760
machine learning models where the idea

1226
00:42:26,119 --> 00:42:30,040
is you want the machine learning model

1227
00:42:27,760 --> 00:42:31,599
to gracefully back gracefully back off

1228
00:42:30,040 --> 00:42:33,839
to the biophysics based model and you

1229
00:42:31,599 --> 00:42:36,599
ask the exactly the correct question is

1230
00:42:33,839 --> 00:42:39,119
how do you know where to start trusting

1231
00:42:36,599 --> 00:42:41,160
one and not trusting the other more like

1232
00:42:39,119 --> 00:42:43,240
relatively speaking and that is sort of

1233
00:42:41,160 --> 00:42:45,760
the Crux of the problem and that problem

1234
00:42:43,240 --> 00:42:48,559
requires that you have a decent handle

1235
00:42:45,760 --> 00:42:51,240
on uncertainty estimates and which is Al

1236
00:42:48,559 --> 00:42:53,240
like which is also quite a technical

1237
00:42:51,240 --> 00:42:55,440
area and I would say this is not solved

1238
00:42:53,240 --> 00:42:58,160
but there's reasonable things you can do

1239
00:42:55,440 --> 00:42:59,960
and the and so uncertainty in a lot of

1240
00:42:58,160 --> 00:43:01,280
classical statistical modeling you

1241
00:42:59,960 --> 00:43:02,559
always just think of sensor noise you

1242
00:43:01,280 --> 00:43:03,960
build a linear aggression and there's

1243
00:43:02,559 --> 00:43:05,480
like an error term actually which most

1244
00:43:03,960 --> 00:43:06,599
people forget about but there is and

1245
00:43:05,480 --> 00:43:08,280
that's you think of that as just like

1246
00:43:06,599 --> 00:43:09,800
the error of the machine from which you

1247
00:43:08,280 --> 00:43:12,240
got the data and that's like the

1248
00:43:09,800 --> 00:43:14,640
standard um type of noise we think about

1249
00:43:12,240 --> 00:43:15,839
in statistical modeling but but in these

1250
00:43:14,640 --> 00:43:17,319
kinds of problems where you have the

1251
00:43:15,839 --> 00:43:19,040
issue of extrapolation there's something

1252
00:43:17,319 --> 00:43:21,000
that's sometimes called epistemic

1253
00:43:19,040 --> 00:43:23,119
uncertainty and that's not uncertainty

1254
00:43:21,000 --> 00:43:24,920
from sensor noise in your data it's

1255
00:43:23,119 --> 00:43:27,240
uncertainty because you're farther away

1256
00:43:24,920 --> 00:43:29,240
from the training data it's like like

1257
00:43:27,240 --> 00:43:31,040
absence of knowledge about that part of

1258
00:43:29,240 --> 00:43:33,680
the space and so there are certain

1259
00:43:31,040 --> 00:43:35,760
classes of modeling techniques um like

1260
00:43:33,680 --> 00:43:38,079
Gan process regression which can be

1261
00:43:35,760 --> 00:43:40,839
quite good at that in many cases in the

1262
00:43:38,079 --> 00:43:43,280
modern era we often take an ensemble of

1263
00:43:40,839 --> 00:43:44,960
neural networks um and there's arguments

1264
00:43:43,280 --> 00:43:47,040
about why that works or not these are

1265
00:43:44,960 --> 00:43:49,640
are imperfect and that is the key thing

1266
00:43:47,040 --> 00:43:52,520
you need in order to do this as well as

1267
00:43:49,640 --> 00:43:54,559
an uncertainty estimate of some kind for

1268
00:43:52,520 --> 00:43:56,480
the biophysics based model although that

1269
00:43:54,559 --> 00:43:58,280
one you can handle if you don't have it

1270
00:43:56,480 --> 00:44:03,400
through some kind kind of slightly

1271
00:43:58,280 --> 00:44:03,400
clever but simple thinking um

1272
00:44:06,440 --> 00:44:10,720
than hey Jennifer thank you for the

1273
00:44:09,040 --> 00:44:13,800
really really amazing and interesting

1274
00:44:10,720 --> 00:44:16,119
talk thank you um I was curious so you

1275
00:44:13,800 --> 00:44:19,160
presented a slide

1276
00:44:16,119 --> 00:44:21,200
on basically three different ways of

1277
00:44:19,160 --> 00:44:25,400
looking at this problem of going from P

1278
00:44:21,200 --> 00:44:27,839
of x to or P of maybe P of Y conditioned

1279
00:44:25,400 --> 00:44:30,480
on x m and

1280
00:44:27,839 --> 00:44:33,000
you boxed like the basically the

1281
00:44:30,480 --> 00:44:35,160
freezing approach of taking a freezing

1282
00:44:33,000 --> 00:44:38,599
your model and then training something

1283
00:44:35,160 --> 00:44:39,880
on top of it um which was interesting so

1284
00:44:38,599 --> 00:44:42,000
just to be clear we're not training

1285
00:44:39,880 --> 00:44:45,400
really something on top of it we're

1286
00:44:42,000 --> 00:44:46,440
training actually quite separately like

1287
00:44:45,400 --> 00:44:48,839
they're not

1288
00:44:46,440 --> 00:44:51,079
trained uh well they're I guess they're

1289
00:44:48,839 --> 00:44:52,599
a little bit trained together through a

1290
00:44:51,079 --> 00:44:55,800
noise schedule but

1291
00:44:52,599 --> 00:44:58,079
yeah um sorry to interrupt yeah yeah

1292
00:44:55,800 --> 00:45:00,079
yeah okay I mean maybe idea pushes away

1293
00:44:58,079 --> 00:45:03,400
from my question then so my question

1294
00:45:00,079 --> 00:45:06,200
would have been if there was a reason to

1295
00:45:03,400 --> 00:45:08,160
take the same model uh or to have the

1296
00:45:06,200 --> 00:45:12,040
same initial model that's frozen that

1297
00:45:08,160 --> 00:45:13,720
we're training off of so yeah now I'm

1298
00:45:12,040 --> 00:45:16,440
like are you saying what's the advantage

1299
00:45:13,720 --> 00:45:18,319
in doing that so the advantages yeah so

1300
00:45:16,440 --> 00:45:20,319
like the advantages I could think of are

1301
00:45:18,319 --> 00:45:24,839
that I mean we could save memory and

1302
00:45:20,319 --> 00:45:27,280
training cost by reusing the model

1303
00:45:24,839 --> 00:45:29,680
versus what the fine tuning model like

1304
00:45:27,280 --> 00:45:31,599
I've seen some better performances with

1305
00:45:29,680 --> 00:45:32,720
some of the fine tuning approach well

1306
00:45:31,599 --> 00:45:34,760
first of all I wouldn't believe anything

1307
00:45:32,720 --> 00:45:36,760
you see almost anywhere until you do it

1308
00:45:34,760 --> 00:45:38,800
for your problem yourself in a way that

1309
00:45:36,760 --> 00:45:41,079
you are honest with yourself but I'm

1310
00:45:38,800 --> 00:45:42,720
sure I have no doubt that each of those

1311
00:45:41,079 --> 00:45:45,200
methods has a place where it is

1312
00:45:42,720 --> 00:45:46,920
genuinely best and I don't think and I

1313
00:45:45,200 --> 00:45:48,640
think it like I've I keep thinking I

1314
00:45:46,920 --> 00:45:50,520
want to write something a paper about

1315
00:45:48,640 --> 00:45:52,160
this but I have not done that because I

1316
00:45:50,520 --> 00:45:53,920
haven't done the work to do it but I

1317
00:45:52,160 --> 00:45:55,599
think one of and I again I think John

1318
00:45:53,920 --> 00:45:57,960
Ingraham in the chroma paper makes a

1319
00:45:55,599 --> 00:46:01,200
really nice point about this so he puts

1320
00:45:57,960 --> 00:46:04,079
this like he has this crazy in a sense

1321
00:46:01,200 --> 00:46:06,920
super wellth thought out model to do uh

1322
00:46:04,079 --> 00:46:09,160
to generate backbone structures so the

1323
00:46:06,920 --> 00:46:11,599
3D coordinates of each C Alpha carbon

1324
00:46:09,160 --> 00:46:13,880
and he puts all this domain knowledge in

1325
00:46:11,599 --> 00:46:16,400
about geometry and like a prior of

1326
00:46:13,880 --> 00:46:18,440
collapsed polymers and like like and

1327
00:46:16,400 --> 00:46:19,960
like all and again doesn't matter if you

1328
00:46:18,440 --> 00:46:22,000
understand but he puts all this domain

1329
00:46:19,960 --> 00:46:24,200
specific knowledge into that model with

1330
00:46:22,000 --> 00:46:26,359
some understanding of what he's modeling

1331
00:46:24,200 --> 00:46:28,440
which is um protein structures that live

1332
00:46:26,359 --> 00:46:30,319
in a 3D physical space that represent

1333
00:46:28,440 --> 00:46:33,400
proteins and so he can put all this

1334
00:46:30,319 --> 00:46:35,440
amount of thinking into there uh and and

1335
00:46:33,400 --> 00:46:36,880
now he has this model and when he's

1336
00:46:35,440 --> 00:46:39,240
doing that he may not know what

1337
00:46:36,880 --> 00:46:40,720
Downstream use cases he wants so but he

1338
00:46:39,240 --> 00:46:42,280
because he puts so much energy and

1339
00:46:40,720 --> 00:46:44,359
thought into that model it's very nice

1340
00:46:42,280 --> 00:46:46,079
if he can repurpose that model for all

1341
00:46:44,359 --> 00:46:47,440
these kind of other Downstream tasks

1342
00:46:46,079 --> 00:46:49,119
where he wants to condition on different

1343
00:46:47,440 --> 00:46:51,200
things and he can just now he has this

1344
00:46:49,119 --> 00:46:53,400
extremely powerful thing that he can

1345
00:46:51,200 --> 00:46:55,280
play with Lego blocks by kind of plug

1346
00:46:53,400 --> 00:46:57,319
and play and so I think like it's true

1347
00:46:55,280 --> 00:46:58,720
like he could now just take take all

1348
00:46:57,319 --> 00:47:01,760
that machinery and thinking and like

1349
00:46:58,720 --> 00:47:03,640
train it kind of all together as well

1350
00:47:01,760 --> 00:47:06,559
but I think in many cases these models

1351
00:47:03,640 --> 00:47:09,400
are kind of expensive and and even in

1352
00:47:06,559 --> 00:47:11,640
Industry people are taxed for um compute

1353
00:47:09,400 --> 00:47:14,359
and so forth so I think it is like a

1354
00:47:11,640 --> 00:47:16,119
practical issue you could in fact just

1355
00:47:14,359 --> 00:47:18,760
redo everything but I think it's nice

1356
00:47:16,119 --> 00:47:20,319
not to have to retrain giant models even

1357
00:47:18,760 --> 00:47:23,000
if you're just doing some low rank

1358
00:47:20,319 --> 00:47:26,000
approximation blah blah blah yeah thank

1359
00:47:23,000 --> 00:47:26,000
you

1360
00:47:28,319 --> 00:47:34,079
hi Jennifer um yeah so actually sort of

1361
00:47:30,760 --> 00:47:36,240
related to that uh uh that question so

1362
00:47:34,079 --> 00:47:37,319
one thing oh I was looking at that slide

1363
00:47:36,240 --> 00:47:39,200
where you're showing like this

1364
00:47:37,319 --> 00:47:41,319
classifier gu and one thing that was

1365
00:47:39,200 --> 00:47:43,200
confusing to me is it that like your

1366
00:47:41,319 --> 00:47:45,040
unconditional model has to be a

1367
00:47:43,200 --> 00:47:47,040
diffusion model or could it be like is

1368
00:47:45,040 --> 00:47:49,160
this kind of a general that's a that's a

1369
00:47:47,040 --> 00:47:51,520
good question as well so that paper is

1370
00:47:49,160 --> 00:47:53,960
focused on Flow matching and diffusion

1371
00:47:51,520 --> 00:47:55,720
models and I don't want I don't want to

1372
00:47:53,960 --> 00:47:57,400
my but my let's say my students are now

1373
00:47:55,720 --> 00:47:58,800
working in more General

1374
00:47:57,400 --> 00:47:59,920
versions of that and I don't want to say

1375
00:47:58,800 --> 00:48:03,400
it because I don't want I don't want

1376
00:47:59,920 --> 00:48:05,440
them to get scooped um but I there are

1377
00:48:03,400 --> 00:48:07,520
some more general concepts there and

1378
00:48:05,440 --> 00:48:10,880
some of the seeds of that exist in some

1379
00:48:07,520 --> 00:48:13,480
literature um but that paper is specific

1380
00:48:10,880 --> 00:48:15,920
to it's well I would say it's anchored

1381
00:48:13,480 --> 00:48:18,160
in diffusion and flow matching models

1382
00:48:15,920 --> 00:48:21,000
and uh but any basically if you have

1383
00:48:18,160 --> 00:48:23,000
under any under any model for which the

1384
00:48:21,000 --> 00:48:25,400
underlying process is a continuous time

1385
00:48:23,000 --> 00:48:27,680
markof process which has a which is

1386
00:48:25,400 --> 00:48:30,000
defined essentially by the rate Matrix

1387
00:48:27,680 --> 00:48:31,720
then all the math they did applies to

1388
00:48:30,000 --> 00:48:33,680
updating the rate Matrix from an

1389
00:48:31,720 --> 00:48:35,599
unconditional rate Matrix to a

1390
00:48:33,680 --> 00:48:37,240
conditioned rate Matrix and so in

1391
00:48:35,599 --> 00:48:39,319
principle there could be a broader set

1392
00:48:37,240 --> 00:48:41,200
of things based on continuous time

1393
00:48:39,319 --> 00:48:43,240
markof processes for which the exact

1394
00:48:41,200 --> 00:48:45,839
same thing would hold and then so that's

1395
00:48:43,240 --> 00:48:47,760
like a direct generalization of turning

1396
00:48:45,839 --> 00:48:50,599
the same machinery and then there's just

1397
00:48:47,760 --> 00:48:52,799
a more general idea of not exactly using

1398
00:48:50,599 --> 00:48:55,960
that Machinery but this kind of idea

1399
00:48:52,799 --> 00:48:58,040
let's say so so just uh clarify so then

1400
00:48:55,960 --> 00:49:00,240
the classifi or whatever predictive

1401
00:48:58,040 --> 00:49:03,520
model you're creating has to be in the

1402
00:49:00,240 --> 00:49:05,680
same it has to be a model which is like

1403
00:49:03,520 --> 00:49:10,119
kind of it's a diffusion model

1404
00:49:05,680 --> 00:49:11,799
prediction task or or you're Oh you mean

1405
00:49:10,119 --> 00:49:13,880
the two parts you're plugging and

1406
00:49:11,799 --> 00:49:15,119
playing there yeah so in that work

1407
00:49:13,880 --> 00:49:17,040
that's right they're both it's it's

1408
00:49:15,119 --> 00:49:18,920
basically saying I assume the overall

1409
00:49:17,040 --> 00:49:20,760
model is a continuous time markup

1410
00:49:18,920 --> 00:49:22,839
process defined by a rate Matrix now

1411
00:49:20,760 --> 00:49:24,040
tell me how to go from a a one M rate

1412
00:49:22,839 --> 00:49:25,880
Matrix which represents the

1413
00:49:24,040 --> 00:49:28,400
unconditional model to another rate

1414
00:49:25,880 --> 00:49:30,839
Matrix which represents the conditioned

1415
00:49:28,400 --> 00:49:33,200
model and it's so all the Machinery is

1416
00:49:30,839 --> 00:49:34,839
is converting rate matrices basically

1417
00:49:33,200 --> 00:49:38,559
and that's this that's the requirement

1418
00:49:34,839 --> 00:49:38,559
for that piece of work okay

1419
00:49:41,559 --> 00:49:47,079
thanks uh more questions and I can bring

1420
00:49:45,280 --> 00:49:49,160
up we can bring up the microphone if

1421
00:49:47,079 --> 00:49:52,520
you're sitting in the middle and cannot

1422
00:49:49,160 --> 00:49:52,520
walk down

1423
00:49:57,440 --> 00:50:03,480
okay then

1424
00:49:59,240 --> 00:50:05,559
um I have a more conceptual question

1425
00:50:03,480 --> 00:50:07,640
like I just want to understand from you

1426
00:50:05,559 --> 00:50:09,920
so you mentioned how powerful the

1427
00:50:07,640 --> 00:50:12,079
diffusion models are so before the

1428
00:50:09,920 --> 00:50:13,599
diffusion models existed the generative

1429
00:50:12,079 --> 00:50:15,720
model to be clear I said they're very

1430
00:50:13,599 --> 00:50:17,920
powerful for real valued spaces and I

1431
00:50:15,720 --> 00:50:19,960
don't think there's clear evidence yet

1432
00:50:17,920 --> 00:50:22,920
that they're powerful on discrete State

1433
00:50:19,960 --> 00:50:26,799
spaces and I have been skeptical but I I

1434
00:50:22,920 --> 00:50:30,359
now think they are um but it's a very

1435
00:50:26,799 --> 00:50:34,480
nuanced topic yeah so would you be uh

1436
00:50:30,359 --> 00:50:37,160
even on the continuous space um could

1437
00:50:34,480 --> 00:50:39,640
you give us an idea some conceptual or

1438
00:50:37,160 --> 00:50:41,640
philosophical idea of why diffusion

1439
00:50:39,640 --> 00:50:43,680
model is better compared to the that's a

1440
00:50:41,640 --> 00:50:47,119
question I have been asking a lot of

1441
00:50:43,680 --> 00:50:49,839
people like I have a vague intuition um

1442
00:50:47,119 --> 00:50:51,760
but I don't I don't have a great answer

1443
00:50:49,839 --> 00:50:53,319
and also I don't work in those spaces

1444
00:50:51,760 --> 00:50:55,200
and like I'd say the big wins are in

1445
00:50:53,319 --> 00:50:57,000
computer vision and so the people who

1446
00:50:55,200 --> 00:50:58,920
maybe deeply understand it are in

1447
00:50:57,000 --> 00:51:00,760
computer vision which I'm not but I have

1448
00:50:58,920 --> 00:51:03,240
the sense that there's something about

1449
00:51:00,760 --> 00:51:05,240
when you do in a standard gener in most

1450
00:51:03,240 --> 00:51:07,880
standard generative models you either

1451
00:51:05,240 --> 00:51:12,160
somehow just generate all like the whole

1452
00:51:07,880 --> 00:51:14,559
image at once in images or uh

1453
00:51:12,160 --> 00:51:17,000
and but something about in diffusion

1454
00:51:14,559 --> 00:51:18,680
you're you're taking like a random image

1455
00:51:17,000 --> 00:51:21,040
and then you're perturbing it a tiny

1456
00:51:18,680 --> 00:51:22,680
amount over and over and over and so

1457
00:51:21,040 --> 00:51:24,480
there's basically it's doing something

1458
00:51:22,680 --> 00:51:27,559
that no other models do which is is

1459
00:51:24,480 --> 00:51:31,799
touching all the pixels at once but just

1460
00:51:27,559 --> 00:51:33,880
a tiny little bit uh and and so that I

1461
00:51:31,799 --> 00:51:35,799
don't think that's a great answer but

1462
00:51:33,880 --> 00:51:37,640
that is like something that those models

1463
00:51:35,799 --> 00:51:39,640
do that other models don't is they're

1464
00:51:37,640 --> 00:51:41,680
just they're touching everything but

1465
00:51:39,640 --> 00:51:44,240
just a little bit and doing it over and

1466
00:51:41,680 --> 00:51:47,280
over I don't think that's a great answer

1467
00:51:44,240 --> 00:51:50,799
though yeah it's like uh incremental

1468
00:51:47,280 --> 00:51:52,920
learning somehow like from a very random

1469
00:51:50,799 --> 00:51:55,839
like if I want to put it in a

1470
00:51:52,920 --> 00:51:57,559
philosophical concept like think about

1471
00:51:55,839 --> 00:51:59,839
if I want to think think about something

1472
00:51:57,559 --> 00:52:02,119
a model that doesn't know anything and

1473
00:51:59,839 --> 00:52:05,000
has some noise and starts shaping the

1474
00:52:02,119 --> 00:52:08,079
data Maybe um and I also think a lot

1475
00:52:05,000 --> 00:52:10,040
about what's the philosophy behind that

1476
00:52:08,079 --> 00:52:11,640
makes those models so powerful that was

1477
00:52:10,040 --> 00:52:14,280
the question but I think that is true

1478
00:52:11,640 --> 00:52:17,040
it's incremental learning and

1479
00:52:14,280 --> 00:52:20,680
and yeah maybe we'll talk off offline

1480
00:52:17,040 --> 00:52:22,520
more thank you for that question um okay

1481
00:52:20,680 --> 00:52:27,160
so if there is no more question then

1482
00:52:22,520 --> 00:52:27,160
let's thank Jennifer one more time

