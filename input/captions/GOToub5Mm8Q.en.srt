1
00:00:00,000 --> 00:00:01,040

2
00:00:01,040 --> 00:00:01,540
All right.

3
00:00:01,540 --> 00:00:04,920
Our next session-- our
last session before lunch--

4
00:00:04,920 --> 00:00:08,720
is focused on two
projects involved with AI.

5
00:00:08,720 --> 00:00:13,160
We have Esther Duflo from
Economics and Graham Jones

6
00:00:13,160 --> 00:00:14,245
from Anthropology.

7
00:00:14,245 --> 00:00:15,620
They'll give their
presentations.

8
00:00:15,620 --> 00:00:17,495
Then they'll be joined
for a panel discussion

9
00:00:17,495 --> 00:00:18,680
with their collaborators--

10
00:00:18,680 --> 00:00:22,840
Marzyeh Ghassemi from EECS,
and Arvind Satyanarayan, also

11
00:00:22,840 --> 00:00:24,080
from ECS.

12
00:00:24,080 --> 00:00:25,955
And then Daniel
Huttenlocher, the Dean

13
00:00:25,955 --> 00:00:27,580
of the Schwarzman
College of Computing,

14
00:00:27,580 --> 00:00:28,740
will join to moderate.

15
00:00:28,740 --> 00:00:31,268
So, Esther, thank you.

16
00:00:31,268 --> 00:00:34,656
[APPLAUSE]

17
00:00:34,656 --> 00:00:37,560

18
00:00:37,560 --> 00:00:39,240
Thank you very much.

19
00:00:39,240 --> 00:00:42,640
Thank you for being here
in such large number,

20
00:00:42,640 --> 00:00:48,240
and for just the very thing
that it's possible to do this.

21
00:00:48,240 --> 00:00:50,720
Lily and I serve
on the task force

22
00:00:50,720 --> 00:00:53,240
of the undergraduate education.

23
00:00:53,240 --> 00:00:58,760
So these are the idea that
MIT is not just an engineering

24
00:00:58,760 --> 00:01:03,030
school, but has a very
vibrant [INAUDIBLE] program,

25
00:01:03,030 --> 00:01:07,610
and that it's integrated
deeply into the education

26
00:01:07,610 --> 00:01:09,530
experience of the
students, and also

27
00:01:09,530 --> 00:01:13,030
in the research of
various faculties,

28
00:01:13,030 --> 00:01:14,150
of course, very critical.

29
00:01:14,150 --> 00:01:16,400
I'm looking at my phone
because I want to time myself.

30
00:01:16,400 --> 00:01:19,130

31
00:01:19,130 --> 00:01:22,770
So this is an example,
perhaps, of one of the things

32
00:01:22,770 --> 00:01:27,730
that this incredible
place makes possible.

33
00:01:27,730 --> 00:01:31,170
So this is a project
about using--

34
00:01:31,170 --> 00:01:36,130
so most of my work in the
context of J-PAL the Jameel

35
00:01:36,130 --> 00:01:37,450
Poverty Action Lab--

36
00:01:37,450 --> 00:01:41,090
is about designing
and evaluating

37
00:01:41,090 --> 00:01:44,970
policies that improve the lives
of poor people around the world.

38
00:01:44,970 --> 00:01:49,650
J-PAL is actually a network
of faculty across the world,

39
00:01:49,650 --> 00:01:52,850
of about 1,000 faculties
across the world.

40
00:01:52,850 --> 00:01:57,300
It works in various sectors from
health, education, environment,

41
00:01:57,300 --> 00:01:59,580
et cetera.

42
00:01:59,580 --> 00:02:02,900
And our work is really kind of
at the intersection of research

43
00:02:02,900 --> 00:02:04,740
and policy.

44
00:02:04,740 --> 00:02:09,180
In this context, of course,
a very active area right

45
00:02:09,180 --> 00:02:13,460
now is the combination
of AI and [INAUDIBLE].

46
00:02:13,460 --> 00:02:17,020
And we have actually,
we're able to raise money

47
00:02:17,020 --> 00:02:20,660
to do AI and [INAUDIBLE],
which we call AI for Good.

48
00:02:20,660 --> 00:02:23,000
And one of this
is AI for Health,

49
00:02:23,000 --> 00:02:26,020
which has been very
active at MIT way

50
00:02:26,020 --> 00:02:29,020
before we got into it,
in particular with--

51
00:02:29,020 --> 00:02:32,980
but not only-- with the help
of, again, the Jameel family

52
00:02:32,980 --> 00:02:35,220
through the [INAUDIBLE] clinic.

53
00:02:35,220 --> 00:02:37,260
So this project
is a collaboration

54
00:02:37,260 --> 00:02:38,998
of many people with
different hats.

55
00:02:38,998 --> 00:02:40,540
And I think that's
kind of what makes

56
00:02:40,540 --> 00:02:43,060
this wonderful and possible.

57
00:02:43,060 --> 00:02:46,920
One, of course, is Marzyeh
from [INAUDIBLE] and myself.

58
00:02:46,920 --> 00:02:49,300
Marzyeh's here
and join us later.

59
00:02:49,300 --> 00:02:53,820
Ziad Obermeyer is a medical
doctor, a emergency room

60
00:02:53,820 --> 00:02:58,840
physician, as well as
being a researcher in AI

61
00:02:58,840 --> 00:03:01,040
for the longest time.

62
00:03:01,040 --> 00:03:02,620
Frank is also from economics.

63
00:03:02,620 --> 00:03:06,260
Girija Vaidyanathan is both
a civil servant, very high.

64
00:03:06,260 --> 00:03:08,200
She retired as the
chief secretary

65
00:03:08,200 --> 00:03:12,800
for state of Tamil Nadu--
so, the biggest person

66
00:03:12,800 --> 00:03:14,620
in the state in the bureaucracy.

67
00:03:14,620 --> 00:03:16,880
But she is also a medical
doctor and a researcher,

68
00:03:16,880 --> 00:03:19,080
and she's an OB/GYN by training.

69
00:03:19,080 --> 00:03:22,180
And then two students, one
in economics, one in the ECS.

70
00:03:22,180 --> 00:03:25,660
So you can see how it takes
many cooks to do this project,

71
00:03:25,660 --> 00:03:29,640
but this is what makes
it particularly great.

72
00:03:29,640 --> 00:03:32,000
So what's the policy challenge?

73
00:03:32,000 --> 00:03:36,920
The policy challenge is
that countries like India--

74
00:03:36,920 --> 00:03:39,280
but not just India, but
certainly middle-income

75
00:03:39,280 --> 00:03:41,120
countries who are growing fast--

76
00:03:41,120 --> 00:03:46,440
are now kind of trapped into a
double burden of disease, one

77
00:03:46,440 --> 00:03:49,720
being the traditional
infectious diseases

78
00:03:49,720 --> 00:03:52,480
and the other being
non-communicable diseases that

79
00:03:52,480 --> 00:03:54,870
are more and more important
in people's lives.

80
00:03:54,870 --> 00:03:57,330
And of course, a third
thing is coming along,

81
00:03:57,330 --> 00:04:00,530
which is the direct
impact of climate change.

82
00:04:00,530 --> 00:04:05,370
But for the noncommunicable
diseases, there is many--

83
00:04:05,370 --> 00:04:09,170
there are large treatment
gaps which often

84
00:04:09,170 --> 00:04:12,010
stems from a diagnosis gap.

85
00:04:12,010 --> 00:04:16,529
So for example, in a
survey we have of people--

86
00:04:16,529 --> 00:04:19,589
it's a panel study we
are doing with people.

87
00:04:19,589 --> 00:04:24,710
They started 50 and
above about 10 years ago.

88
00:04:24,710 --> 00:04:26,690
So they are now 60 and above.

89
00:04:26,690 --> 00:04:28,650
Many of them have
various conditions

90
00:04:28,650 --> 00:04:32,310
that are non-communicable and
manageable, if not treatable.

91
00:04:32,310 --> 00:04:36,210
But most of them do not
that they have them.

92
00:04:36,210 --> 00:04:40,750
In many cases, this diagnosis
exists in high-income setting.

93
00:04:40,750 --> 00:04:45,170
It's an even--
people in India who

94
00:04:45,170 --> 00:04:49,270
are high income themselves can
easily get diagnosed for this.

95
00:04:49,270 --> 00:04:50,890
For example, one
of the condition

96
00:04:50,890 --> 00:04:55,540
that's going to be really
the focus for today is--

97
00:04:55,540 --> 00:04:57,540
more like as a test
case-- is whether or not

98
00:04:57,540 --> 00:05:00,180
you've had a heart
attack in the past.

99
00:05:00,180 --> 00:05:02,520
So if you had a what's called
a silent heart attack--

100
00:05:02,520 --> 00:05:05,040
I'm not a doctor, so I hope
I'm not saying anything wrong--

101
00:05:05,040 --> 00:05:06,940
but if you had a
silent heart attack had

102
00:05:06,940 --> 00:05:09,725
a heart attack in the past,
you didn't die, that's good.

103
00:05:09,725 --> 00:05:10,600
That's the good news.

104
00:05:10,600 --> 00:05:13,060
The bad news is it makes
you weaker and more

105
00:05:13,060 --> 00:05:14,980
susceptible to the next one.

106
00:05:14,980 --> 00:05:17,380
And the good news again,
is that that's actually

107
00:05:17,380 --> 00:05:18,880
a manageable condition.

108
00:05:18,880 --> 00:05:22,400
You can put people on very cheap
things-- beta blocker, statins,

109
00:05:22,400 --> 00:05:24,300
baby aspirin, and they'll get--

110
00:05:24,300 --> 00:05:26,800
that's really help
managing the condition.

111
00:05:26,800 --> 00:05:29,660
But for that, they have to
know that that has happened.

112
00:05:29,660 --> 00:05:32,540
And many of the
silent heart attack

113
00:05:32,540 --> 00:05:33,880
never made it to the hospital.

114
00:05:33,880 --> 00:05:37,700
So there is no
sign they have it.

115
00:05:37,700 --> 00:05:39,260
How do you diagnose it?

116
00:05:39,260 --> 00:05:42,300
The gold standard
test is an ultrasound.

117
00:05:42,300 --> 00:05:44,060
An ultrasound is
expensive and can only

118
00:05:44,060 --> 00:05:45,740
be done in the hospital.

119
00:05:45,740 --> 00:05:49,620
So our idea was to use--

120
00:05:49,620 --> 00:05:51,230
to see whether it
was, really, it

121
00:05:51,230 --> 00:05:54,110
was to test out whether it
was possible to diagnose

122
00:05:54,110 --> 00:05:55,750
silent heart attack--

123
00:05:55,750 --> 00:05:58,110
not diagnose, but
predict whether someone

124
00:05:58,110 --> 00:06:01,350
might have had a silent
heart attack in the past--

125
00:06:01,350 --> 00:06:07,810
using a simple one lead ECG
device produced by Alivecor.

126
00:06:07,810 --> 00:06:09,310
These are the type
that you might

127
00:06:09,310 --> 00:06:11,790
be given if you have elderly
parents to check that there

128
00:06:11,790 --> 00:06:14,510
are arrhythmia is in check.

129
00:06:14,510 --> 00:06:15,490
So that's what it does.

130
00:06:15,490 --> 00:06:18,790
It's not designed to test
whether there was a heart

131
00:06:18,790 --> 00:06:19,830
attack.

132
00:06:19,830 --> 00:06:23,910
But [INAUDIBLE]
hunch was that it

133
00:06:23,910 --> 00:06:28,270
might be possible to go from
there to a form, a prediction,

134
00:06:28,270 --> 00:06:30,715
that there might have been
a heart attack in the past.

135
00:06:30,715 --> 00:06:33,090
So we are predicting the past,
which is a little strange,

136
00:06:33,090 --> 00:06:35,990
but you see what I mean.

137
00:06:35,990 --> 00:06:38,310
So what we are.

138
00:06:38,310 --> 00:06:41,130
So if that worked, that's
really worth testing,

139
00:06:41,130 --> 00:06:44,310
that there was a good chance
that it wasn't going to work.

140
00:06:44,310 --> 00:06:48,650
But if it does work, then,
because the government

141
00:06:48,650 --> 00:06:50,250
of Tamil Nadu,
where we are doing

142
00:06:50,250 --> 00:06:51,870
this work, in the
south of India,

143
00:06:51,870 --> 00:06:54,430
like many other governments
in the United States,

144
00:06:54,430 --> 00:06:58,530
have lots of people going to
people's doorsteps anyways

145
00:06:58,530 --> 00:07:01,690
for various things, like giving
people diabetes medicines

146
00:07:01,690 --> 00:07:02,690
and so on.

147
00:07:02,690 --> 00:07:05,010
It's very easy to outfit
them with this device, which

148
00:07:05,010 --> 00:07:06,170
is super cheap.

149
00:07:06,170 --> 00:07:08,610
And then they can do
the test, and then we

150
00:07:08,610 --> 00:07:11,770
can run the algorithm,
see whether someone

151
00:07:11,770 --> 00:07:16,090
is above a certain threshold,
and above a certain threshold

152
00:07:16,090 --> 00:07:18,710
of likelihood that that event
has occurred in the past,

153
00:07:18,710 --> 00:07:22,630
we can send them to
get tested in hospital,

154
00:07:22,630 --> 00:07:24,830
get the ultrasound that
confirmed the diagnosis.

155
00:07:24,830 --> 00:07:26,250
So that was the idea.

156
00:07:26,250 --> 00:07:29,770
And so what we're
requiring from the test

157
00:07:29,770 --> 00:07:31,250
is that it gives--
whether or not

158
00:07:31,250 --> 00:07:35,850
it gives us any information that
is better than noise, or better

159
00:07:35,850 --> 00:07:41,910
than age, or better than the
scores that people use today.

160
00:07:41,910 --> 00:07:47,180
So what we did is that we, with
the help of [INAUDIBLE] South

161
00:07:47,180 --> 00:07:50,900
Asia and the government,
we organized these camps

162
00:07:50,900 --> 00:07:55,020
where people were able to come.

163
00:07:55,020 --> 00:07:58,340
And then we gave them
a batteries of tests.

164
00:07:58,340 --> 00:08:02,380
So today, I'm presenting to
you this one, but the hope

165
00:08:02,380 --> 00:08:05,460
in working with Marzyeh's
group is that there'll

166
00:08:05,460 --> 00:08:08,020
be many others, because we
collected all of the tests

167
00:08:08,020 --> 00:08:10,620
under the sun, as long as
we had people available,

168
00:08:10,620 --> 00:08:13,260
both cheap and expensive.

169
00:08:13,260 --> 00:08:15,000
So we have things
that are cheap to do,

170
00:08:15,000 --> 00:08:20,420
like a retina photo, blood
pressure, [INAUDIBLE] of course.

171
00:08:20,420 --> 00:08:23,900
And then we have also ultrasound
and many other things.

172
00:08:23,900 --> 00:08:28,520
And then what we did is,
so our students, Jenny,

173
00:08:28,520 --> 00:08:31,940
who is supported by
this grant, started

174
00:08:31,940 --> 00:08:35,460
working with a lot of people
in the department and EECS,

175
00:08:35,460 --> 00:08:41,380
and to teach herself how you
apply machine learning algorithm

176
00:08:41,380 --> 00:08:42,980
to these things.

177
00:08:42,980 --> 00:08:46,670
We predicted.

178
00:08:46,670 --> 00:08:47,930
We used the data.

179
00:08:47,930 --> 00:08:50,410
So we have the
gold standard test.

180
00:08:50,410 --> 00:08:51,830
That's our ground truth.

181
00:08:51,830 --> 00:08:54,250
We have the cheaper test--

182
00:08:54,250 --> 00:08:56,510
anything that someone
could easily, cheaply

183
00:08:56,510 --> 00:08:57,290
do on the ground.

184
00:08:57,290 --> 00:08:59,870
But in particular, this
one is [INAUDIBLE].

185
00:08:59,870 --> 00:09:01,390
And then the
objective is to form

186
00:09:01,390 --> 00:09:05,030
a model that predicts that
the person is likely to have

187
00:09:05,030 --> 00:09:07,310
had a heart attack.

188
00:09:07,310 --> 00:09:10,170
So that's how we are
building the algorithm.

189
00:09:10,170 --> 00:09:13,367
And we are also using, in order
to be more efficient given

190
00:09:13,367 --> 00:09:14,950
the amount of data
we are using, we're

191
00:09:14,950 --> 00:09:17,230
also bringing in
data from Sweden

192
00:09:17,230 --> 00:09:18,890
to pre-train the algorithm.

193
00:09:18,890 --> 00:09:23,150
And we then fine tune
within our existing data.

194
00:09:23,150 --> 00:09:24,570
So that work happened.

195
00:09:24,570 --> 00:09:28,270
It went pretty well, because--

196
00:09:28,270 --> 00:09:30,390
so this is how the
government is-- how

197
00:09:30,390 --> 00:09:32,190
the collaboration is working.

198
00:09:32,190 --> 00:09:33,290
This work happened.

199
00:09:33,290 --> 00:09:36,430
It worked pretty well
because at the high end,

200
00:09:36,430 --> 00:09:38,790
our algorithm is
quite predictive,

201
00:09:38,790 --> 00:09:44,600
that someone would have had
this, a silent heart attack.

202
00:09:44,600 --> 00:09:48,400
In our high risk
group, 95% of above.

203
00:09:48,400 --> 00:09:51,380
The chance that they've
had one is about 9%,

204
00:09:51,380 --> 00:09:56,580
which is nine times greater
than in any of the other groups.

205
00:09:56,580 --> 00:09:59,000
So what we did-- this
is our economist hat--

206
00:09:59,000 --> 00:10:02,680
is we do a cost-benefit
analysis, which tells us

207
00:10:02,680 --> 00:10:05,440
whether this would be
worth it to deploy this,

208
00:10:05,440 --> 00:10:09,200
identify the people,
send them to be tested

209
00:10:09,200 --> 00:10:12,040
if they are at high risk.

210
00:10:12,040 --> 00:10:15,680
And that's compare the number
of lives you could save.

211
00:10:15,680 --> 00:10:17,540
In this way, with
the cost of testing,

212
00:10:17,540 --> 00:10:21,280
a bunch of people who probably
haven't had that event happened.

213
00:10:21,280 --> 00:10:23,760
And it's super, super,
super cost effective.

214
00:10:23,760 --> 00:10:27,560
And what is remarkable
compared to existing tests

215
00:10:27,560 --> 00:10:31,640
is that it catches
young people who

216
00:10:31,640 --> 00:10:34,140
are less likely to have
had a silent heart attack,

217
00:10:34,140 --> 00:10:38,360
but have many years of life
saved if you can catch them.

218
00:10:38,360 --> 00:10:40,450
And those young
people are completely

219
00:10:40,450 --> 00:10:43,650
excluded from the
current screening

220
00:10:43,650 --> 00:10:46,770
because the screening is
basically based on age.

221
00:10:46,770 --> 00:10:49,310
So something like that
kind of makes it young,

222
00:10:49,310 --> 00:10:53,930
meaning not super young,
but like the 40-plus.

223
00:10:53,930 --> 00:10:57,330
So where the last things we--

224
00:10:57,330 --> 00:10:59,730
this brings me to
one of the thing

225
00:10:59,730 --> 00:11:05,130
we want to do in the future,
which is the student that ECS is

226
00:11:05,130 --> 00:11:08,250
a financing with this grant,
is already starting to do,

227
00:11:08,250 --> 00:11:11,650
is to look at fairness
property of this algorithm.

228
00:11:11,650 --> 00:11:13,510
Like, yes, we
catch young people.

229
00:11:13,510 --> 00:11:15,930
Do we miss people
that we shouldn't be,

230
00:11:15,930 --> 00:11:17,590
that we shouldn't be missing?

231
00:11:17,590 --> 00:11:20,930
And a preliminary result
suggests that this algorithm

232
00:11:20,930 --> 00:11:24,990
actually works better in
urban areas for older people,

233
00:11:24,990 --> 00:11:28,050
even though it does work for
younger people, and for men

234
00:11:28,050 --> 00:11:29,170
than for women.

235
00:11:29,170 --> 00:11:31,490
So one of the things
we can do in the future

236
00:11:31,490 --> 00:11:35,890
is to improve it, to make
it to solve these issues.

237
00:11:35,890 --> 00:11:37,430
And of course, this
is, [INAUDIBLE]

238
00:11:37,430 --> 00:11:40,470
scratching the surface of
what we can do with this data.

239
00:11:40,470 --> 00:11:43,090
What we can do in the world is
also scratching the surface.

240
00:11:43,090 --> 00:11:49,230
We already raised more money
to the AI for Good initiative

241
00:11:49,230 --> 00:11:52,430
at J-PAL to run a randomized
control trial where

242
00:11:52,430 --> 00:11:56,060
these things will actually
be, first, refined,

243
00:11:56,060 --> 00:11:59,050
like one more round of data
collection to make it better.

244
00:11:59,050 --> 00:12:01,630
And then will be
sent to people--

245
00:12:01,630 --> 00:12:03,875
well, people will actually
be outfitted with this,

246
00:12:03,875 --> 00:12:05,250
and we'll see
whether they do it.

247
00:12:05,250 --> 00:12:07,190
And we can compare
whether or not

248
00:12:07,190 --> 00:12:09,010
people get diagnosed with this.

249
00:12:09,010 --> 00:12:13,490
So as my son was
just at Model UN,

250
00:12:13,490 --> 00:12:16,370
I yield my time to the
Chair, but I don't have any.

251
00:12:16,370 --> 00:12:18,412
So thank you so much.

252
00:12:18,412 --> 00:12:30,470
[APPLAUSE]

253
00:12:30,470 --> 00:12:31,310
Oh.

254
00:12:31,310 --> 00:12:32,990
Thank you, Esther.

255
00:12:32,990 --> 00:12:36,560
Let me make sure this works.

256
00:12:36,560 --> 00:12:37,300
Couple more.

257
00:12:37,300 --> 00:12:39,940

258
00:12:39,940 --> 00:12:41,525
Oh, that's me.

259
00:12:41,525 --> 00:12:42,680
[LAUGHTER]

260
00:12:42,680 --> 00:12:45,400
Hi, I'm Graham Jones.

261
00:12:45,400 --> 00:12:48,940
So thank you so much, Esther.

262
00:12:48,940 --> 00:12:53,000
Thank you, [INAUDIBLE] and the
MITHIC team, Jen and Elena.

263
00:12:53,000 --> 00:12:56,560
Thanks to AgustÃ­n
and Anantha, Sally

264
00:12:56,560 --> 00:13:00,440
for making this all possible,
and to the generous funders

265
00:13:00,440 --> 00:13:03,950
of this program, and to
all of you for being here.

266
00:13:03,950 --> 00:13:07,880

267
00:13:07,880 --> 00:13:10,480
So my name is Graham Jones.

268
00:13:10,480 --> 00:13:13,660
I am a linguistic
anthropologist,

269
00:13:13,660 --> 00:13:16,040
which means that
I study how people

270
00:13:16,040 --> 00:13:19,080
communicate with each other.

271
00:13:19,080 --> 00:13:24,640
My MITHIC collaborator, Arvind
Sathyanarayan, who's over there,

272
00:13:24,640 --> 00:13:26,820
studies human-computer
interaction,

273
00:13:26,820 --> 00:13:29,080
which means that he
studies how people

274
00:13:29,080 --> 00:13:31,480
communicate with computers.

275
00:13:31,480 --> 00:13:35,090
Our MITHIC projects, spanning
[INAUDIBLE] and the School

276
00:13:35,090 --> 00:13:40,430
of Engineering, focuses on an
unprecedented moment in history,

277
00:13:40,430 --> 00:13:43,570
where large language
models have allowed people

278
00:13:43,570 --> 00:13:46,490
to communicate with
computers as if they

279
00:13:46,490 --> 00:13:48,410
were talking to other people.

280
00:13:48,410 --> 00:13:50,810
So you can see the
kind of Venn diagram

281
00:13:50,810 --> 00:13:53,610
of our areas of expertise.

282
00:13:53,610 --> 00:13:57,850
We wanted to draw on
these respective fields

283
00:13:57,850 --> 00:14:02,410
to understand the opportunities
that LLMs represented, but also

284
00:14:02,410 --> 00:14:07,210
to mitigate harms and
improve societal impacts.

285
00:14:07,210 --> 00:14:10,590
But our collaboration actually
began several years earlier.

286
00:14:10,590 --> 00:14:15,130
And so I want to try to
contextualize it for you today.

287
00:14:15,130 --> 00:14:18,510
So as I said, I study
how people communicate.

288
00:14:18,510 --> 00:14:21,010
And in particular, I'm
interested in how people

289
00:14:21,010 --> 00:14:22,930
communicate information--

290
00:14:22,930 --> 00:14:24,690
valuable information.

291
00:14:24,690 --> 00:14:27,890
Information like secrets.

292
00:14:27,890 --> 00:14:32,010
Imagine someone with a
secret to share and someone

293
00:14:32,010 --> 00:14:33,860
else who wants it.

294
00:14:33,860 --> 00:14:36,740
The secret holder could
just tell the secret

295
00:14:36,740 --> 00:14:39,820
straight out-- here you
go, and spill the beans.

296
00:14:39,820 --> 00:14:43,860
But consider what happens
in the following example.

297
00:14:43,860 --> 00:14:46,100
This is a magician
who I recorded

298
00:14:46,100 --> 00:14:50,380
sharing a secret during my field
work among French magicians.

299
00:14:50,380 --> 00:14:52,620
This is just a small
snippet, a snippet

300
00:14:52,620 --> 00:14:54,800
of a much longer revelation.

301
00:14:54,800 --> 00:14:57,660
And I was one of the people
receiving the secret.

302
00:14:57,660 --> 00:15:01,320
So he says, to you, I'll
give it to you, all right.

303
00:15:01,320 --> 00:15:03,566
But well, um, I'm--

304
00:15:03,566 --> 00:15:07,260
we're not going to make
it official or whatever.

305
00:15:07,260 --> 00:15:08,360
That's it.

306
00:15:08,360 --> 00:15:10,960
It's-- it's his.

307
00:15:10,960 --> 00:15:12,360
It's-- no.

308
00:15:12,360 --> 00:15:13,660
But--

309
00:15:13,660 --> 00:15:17,540
So you'll notice that far
from just spilling the beans

310
00:15:17,540 --> 00:15:21,400
or giving the secret,
this person is stammering.

311
00:15:21,400 --> 00:15:23,600
He's hedging, he's
interrupting himself.

312
00:15:23,600 --> 00:15:25,880
He's imposing conditions.

313
00:15:25,880 --> 00:15:29,040
He's anxious about
sharing the secret.

314
00:15:29,040 --> 00:15:33,120
And his anxiety is not only
palpable but contagious,

315
00:15:33,120 --> 00:15:36,440
affecting the people who
are receiving the secret.

316
00:15:36,440 --> 00:15:39,080
And that shared
anxiety is part of what

317
00:15:39,080 --> 00:15:41,480
makes magicians magicians.

318
00:15:41,480 --> 00:15:45,160
Now from this, I want you
to retain two lessons.

319
00:15:45,160 --> 00:15:48,800
One, the form of talk
shapes its meaning,

320
00:15:48,800 --> 00:15:53,640
and two, meaning is social.

321
00:15:53,640 --> 00:15:56,800
Now here's an example from
Arvind's research, a data

322
00:15:56,800 --> 00:16:00,520
visualization, like one
of many that most of you

323
00:16:00,520 --> 00:16:02,800
probably see every day.

324
00:16:02,800 --> 00:16:06,760
This is also intended to
convey information-- probably

325
00:16:06,760 --> 00:16:08,660
very important information.

326
00:16:08,660 --> 00:16:12,440
And most people think that
conveying information is all

327
00:16:12,440 --> 00:16:14,840
that data visualizations do.

328
00:16:14,840 --> 00:16:18,600
But compare it to this
data visualization.

329
00:16:18,600 --> 00:16:20,600
You'll notice some differences.

330
00:16:20,600 --> 00:16:23,040
The color palette is different.

331
00:16:23,040 --> 00:16:26,420
And in this new chart,
instead of bland color fields,

332
00:16:26,420 --> 00:16:30,010
we have these
colorful green plants.

333
00:16:30,010 --> 00:16:32,930
You'll also notice that
the information has

334
00:16:32,930 --> 00:16:35,290
been removed from these graphs.

335
00:16:35,290 --> 00:16:37,230
We don't what they're about.

336
00:16:37,230 --> 00:16:41,990
But if I asked you where you
thought these graphs came from,

337
00:16:41,990 --> 00:16:45,690
I bet many of you
would have some ideas.

338
00:16:45,690 --> 00:16:48,170
Arvind and I
recently ran a series

339
00:16:48,170 --> 00:16:50,930
of experiments using
charts like these,

340
00:16:50,930 --> 00:16:55,130
and we found that even when
the content of visualizations

341
00:16:55,130 --> 00:16:59,890
is obscured, people have very
strong ideas about who made them

342
00:16:59,890 --> 00:17:01,250
and why.

343
00:17:01,250 --> 00:17:04,130
And this has profound
implications for whether or not

344
00:17:04,130 --> 00:17:06,690
people trust that information--

345
00:17:06,690 --> 00:17:09,650
a discovery with
deep implications

346
00:17:09,650 --> 00:17:14,089
for fighting myths
and disinformation.

347
00:17:14,089 --> 00:17:18,170
But I want you to retain this.

348
00:17:18,170 --> 00:17:21,349
This discovery means that
even for data visualization,

349
00:17:21,349 --> 00:17:24,679
form shapes meaning, and
that meaning is social.

350
00:17:24,679 --> 00:17:27,460

351
00:17:27,460 --> 00:17:30,060
Just a few weeks ago, there
was an article in The MIT News

352
00:17:30,060 --> 00:17:33,260
about this discovery, if
any of you are interested.

353
00:17:33,260 --> 00:17:36,820
But that's not really
why you're here today.

354
00:17:36,820 --> 00:17:40,020
What does this
have to do with AI?

355
00:17:40,020 --> 00:17:43,100
It turns out quite a lot.

356
00:17:43,100 --> 00:17:47,500
LLMs now allow us to
communicate with computers

357
00:17:47,500 --> 00:17:51,380
as if we were communicating
with other people.

358
00:17:51,380 --> 00:17:54,980
Old-fashioned,
rule-governed AI could not

359
00:17:54,980 --> 00:17:58,860
handle many of the things
that humans do with language.

360
00:17:58,860 --> 00:18:02,100
Generative AI has
changed all that.

361
00:18:02,100 --> 00:18:05,580
Consider ambiguity.

362
00:18:05,580 --> 00:18:08,340
In the sentence, "The
peasants are revolting,"

363
00:18:08,340 --> 00:18:11,620
is revolting a verb
or an adjective?

364
00:18:11,620 --> 00:18:15,260
Well, it depends on the
context, which humans

365
00:18:15,260 --> 00:18:16,820
are very good at parsing--

366
00:18:16,820 --> 00:18:20,060
or laughing about
when they can't.

367
00:18:20,060 --> 00:18:24,180
Probabilistic AI can
parse such ambiguities,

368
00:18:24,180 --> 00:18:26,790
and in interactions
with human users

369
00:18:26,790 --> 00:18:29,230
it reflects our key tenants.

370
00:18:29,230 --> 00:18:34,070
Form shapes meaning
and meaning is social.

371
00:18:34,070 --> 00:18:37,450
This is a deep
technological breakthrough.

372
00:18:37,450 --> 00:18:42,310
But like many breakthroughs, it
is far from an unalloyed good.

373
00:18:42,310 --> 00:18:44,870
Consider this transcript.

374
00:18:44,870 --> 00:18:48,910
This is a snippet of many
thousands of messages shared

375
00:18:48,910 --> 00:18:52,290
between a British man
and an AI girlfriend,

376
00:18:52,290 --> 00:18:54,950
that he created on
the Replika app.

377
00:18:54,950 --> 00:18:57,870
Over the course of
months, this AI girlfriend

378
00:18:57,870 --> 00:19:00,030
helped the man
plan to assassinate

379
00:19:00,030 --> 00:19:01,710
the Queen of England.

380
00:19:01,710 --> 00:19:06,790
He was arrested with a crossbow
breaking into Buckingham Palace.

381
00:19:06,790 --> 00:19:11,550
Old-fashioned rule-governed
AI could never have done this.

382
00:19:11,550 --> 00:19:14,810
It used to be the case that
in building an AI system,

383
00:19:14,810 --> 00:19:17,270
a designer would
pre-configure settings

384
00:19:17,270 --> 00:19:21,110
for everything that a human
might do with the system.

385
00:19:21,110 --> 00:19:23,350
This could involve
giving more agency

386
00:19:23,350 --> 00:19:26,960
to the human user for
more manual tasks, or more

387
00:19:26,960 --> 00:19:30,140
agency to the system for
more automated tasks.

388
00:19:30,140 --> 00:19:33,880
But all of this would
be decided in advance.

389
00:19:33,880 --> 00:19:36,880
But as we have shown,
interactions with gen AI

390
00:19:36,880 --> 00:19:41,440
are open ended, and as such,
designing for generative

391
00:19:41,440 --> 00:19:44,600
AI takes a different
form, because what

392
00:19:44,600 --> 00:19:48,760
the system and the user will do
together is not predetermined.

393
00:19:48,760 --> 00:19:53,840
Designers configure
open-ended possibility spaces.

394
00:19:53,840 --> 00:19:59,000
Our MITHIC project focuses
on how best to configure

395
00:19:59,000 --> 00:20:02,440
those possibility spaces.

396
00:20:02,440 --> 00:20:07,080
We approach this issue in terms
of sociocultural awareness

397
00:20:07,080 --> 00:20:11,340
because AI agents communicating
like people with people

398
00:20:11,340 --> 00:20:15,360
are entering into
sociocultural relationships.

399
00:20:15,360 --> 00:20:18,720
We think that making
designers aware of this

400
00:20:18,720 --> 00:20:22,260
will lead to not only
safer systems, but ones

401
00:20:22,260 --> 00:20:24,340
that are better performing.

402
00:20:24,340 --> 00:20:26,960
This is what we proposed
in our MITHIC project.

403
00:20:26,960 --> 00:20:31,380
But at the time of the proposal,
we really didn't how to do it.

404
00:20:31,380 --> 00:20:34,120
And that's one of the really
great things about this grant,

405
00:20:34,120 --> 00:20:38,060
is that it gave us the freedom
to explore an open-ended problem

406
00:20:38,060 --> 00:20:40,700
space of our own.

407
00:20:40,700 --> 00:20:42,380
Working on this
over the summer, we

408
00:20:42,380 --> 00:20:44,780
were captivated by
an emerging trend

409
00:20:44,780 --> 00:20:47,420
in the design of AI agents.

410
00:20:47,420 --> 00:20:51,740
We began experimenting with
building systems in which

411
00:20:51,740 --> 00:20:56,340
each possibility space was its
own independent agent, offering

412
00:20:56,340 --> 00:21:00,420
a much more granular control
over how the overall system

413
00:21:00,420 --> 00:21:02,100
might behave.

414
00:21:02,100 --> 00:21:05,180
This is called
multi-agent AI, and we

415
00:21:05,180 --> 00:21:08,060
decided that the best way
to explore the topic was

416
00:21:08,060 --> 00:21:09,980
by teaching about it.

417
00:21:09,980 --> 00:21:12,340
We quickly put together
a special subject

418
00:21:12,340 --> 00:21:16,140
called Humane User Experience
Design, which we're currently

419
00:21:16,140 --> 00:21:20,270
teaching, in which students are
using a multi-agent approach

420
00:21:20,270 --> 00:21:23,190
to designing AI systems.

421
00:21:23,190 --> 00:21:26,990
The crux of our collaboration
is the term "humane."

422
00:21:26,990 --> 00:21:30,550
Like "revolting," it's a
bit of a double entendre,

423
00:21:30,550 --> 00:21:36,670
meaning both human serving,
or benevolent, and human-like.

424
00:21:36,670 --> 00:21:40,990
What's interesting is that
our students are discovering

425
00:21:40,990 --> 00:21:43,710
that there's a fundamental
tension between building

426
00:21:43,710 --> 00:21:47,610
a system that's good for people,
and one that seems human,

427
00:21:47,610 --> 00:21:52,350
because real people can
develop dangerous dependencies.

428
00:21:52,350 --> 00:21:54,370
We have 40 students
in the class,

429
00:21:54,370 --> 00:21:57,470
and they're currently
working on final projects,

430
00:21:57,470 --> 00:22:00,550
ranging from domains
such as education, health

431
00:22:00,550 --> 00:22:04,110
and wellness, personal
growth, and entertainment.

432
00:22:04,110 --> 00:22:06,950
We're hoping to
continue with a smaller

433
00:22:06,950 --> 00:22:11,430
subset of current seniors
as MEng students next year,

434
00:22:11,430 --> 00:22:13,470
and really, to
see how far we can

435
00:22:13,470 --> 00:22:16,070
push the application
of anthropology

436
00:22:16,070 --> 00:22:19,780
to computer science and computer
science to anthropology.

437
00:22:19,780 --> 00:22:22,080
And incidentally, on
my way out of class

438
00:22:22,080 --> 00:22:25,520
today, I spoke with one
of our graduating seniors

439
00:22:25,520 --> 00:22:29,360
who just had an internship
in New York working

440
00:22:29,360 --> 00:22:34,960
with a startup that's using
multi-agent AI to develop a tool

441
00:22:34,960 --> 00:22:38,220
to help senior citizens
navigate social services.

442
00:22:38,220 --> 00:22:40,880
And he said, it's
just our class.

443
00:22:40,880 --> 00:22:44,680
So that was very
validating for us.

444
00:22:44,680 --> 00:22:49,920
This collaboration reflects the
best of MIT, of mens et manus.

445
00:22:49,920 --> 00:22:52,160
And we are very grateful
for the opportunity

446
00:22:52,160 --> 00:22:55,880
that MITHIC has given us
to pursue it in cooperation

447
00:22:55,880 --> 00:22:57,740
with our wonderful students.

448
00:22:57,740 --> 00:22:58,871
Thank you.

449
00:22:58,871 --> 00:23:02,308
[APPLAUSE]

450
00:23:02,308 --> 00:23:11,160

451
00:23:11,160 --> 00:23:13,860
[INAUDIBLE]

452
00:23:13,860 --> 00:23:17,090

453
00:23:17,090 --> 00:23:19,610
Get myself organized.

454
00:23:19,610 --> 00:23:22,990
I was wondering,
who's sitting where?

455
00:23:22,990 --> 00:23:23,490
Great.

456
00:23:23,490 --> 00:23:28,010
Thank you for these
really fantastic projects.

457
00:23:28,010 --> 00:23:35,250
Maybe I'll start with a
question for Esther and Marzyeh,

458
00:23:35,250 --> 00:23:38,890
just sort of in the order
of presentation, I guess.

459
00:23:38,890 --> 00:23:42,490
So this is an extremely
important problem domain,

460
00:23:42,490 --> 00:23:45,810
of creating new, low-cost
screening methods

461
00:23:45,810 --> 00:23:47,850
to refer people
for further testing

462
00:23:47,850 --> 00:23:50,690
that they might not
otherwise be getting

463
00:23:50,690 --> 00:23:54,490
for very serious but
hidden health conditions.

464
00:23:54,490 --> 00:23:59,650
So my first question is, how
did your multidisciplinary team

465
00:23:59,650 --> 00:24:02,850
arrive at this particular
problem of silent heart attack,

466
00:24:02,850 --> 00:24:05,510
and the approach
that you pursued?

467
00:24:05,510 --> 00:24:07,382
And maybe just as a--

468
00:24:07,382 --> 00:24:10,330
what I find really remarkable
about this approach

469
00:24:10,330 --> 00:24:12,920
is it's technologically
feasible.

470
00:24:12,920 --> 00:24:16,260

471
00:24:16,260 --> 00:24:19,360
There's no human-AI
collaboration in it,

472
00:24:19,360 --> 00:24:23,500
which becomes a gigantic
rat's nest of trying

473
00:24:23,500 --> 00:24:26,980
to get better outcomes.

474
00:24:26,980 --> 00:24:32,620
And then, of course,
it's aimed at treating

475
00:24:32,620 --> 00:24:34,620
a very important issue.

476
00:24:34,620 --> 00:24:37,300
So how did you get there?

477
00:24:37,300 --> 00:24:40,620
Yeah, maybe I'll start with,
because that was before Marzyeh

478
00:24:40,620 --> 00:24:42,060
came onto the project.

479
00:24:42,060 --> 00:24:45,380
And then she can say what--

480
00:24:45,380 --> 00:24:48,000
the various places we
could go from there.

481
00:24:48,000 --> 00:24:51,180

482
00:24:51,180 --> 00:24:55,140
The genesis of this project is--

483
00:24:55,140 --> 00:25:03,900
I was in a sadly, now defunct
NIH PO1, like a center grant,

484
00:25:03,900 --> 00:25:06,120
on health and aging.

485
00:25:06,120 --> 00:25:08,940
[INAUDIBLE] a lot
of different people

486
00:25:08,940 --> 00:25:10,800
were working on
different things.

487
00:25:10,800 --> 00:25:13,680
And Ziad and
Sendhil Mullainathan

488
00:25:13,680 --> 00:25:18,560
were presenting about
overtesting and under-testing.

489
00:25:18,560 --> 00:25:20,840
In the US, we are more
talking about the fact

490
00:25:20,840 --> 00:25:22,180
that they are overtesting.

491
00:25:22,180 --> 00:25:25,440
We're sending people,
referring people to tests.

492
00:25:25,440 --> 00:25:28,640
But they did point out
that even in the US,

493
00:25:28,640 --> 00:25:30,300
there is also under-testing.

494
00:25:30,300 --> 00:25:31,720
And then there were--

495
00:25:31,720 --> 00:25:34,520
Ziad was-- and then they were
presenting the possibility

496
00:25:34,520 --> 00:25:38,960
that something like that
could help pre-screen

497
00:25:38,960 --> 00:25:40,923
who should be tested for what.

498
00:25:40,923 --> 00:25:42,840
And I thought, oh, this
is really interesting,

499
00:25:42,840 --> 00:25:45,460
because I'm sure there is
progress to be made in the US.

500
00:25:45,460 --> 00:25:48,200
But I know for a
fact that this is,

501
00:25:48,200 --> 00:25:51,280
if it's, like, second-order
improvement in the US,

502
00:25:51,280 --> 00:25:55,440
it's first-first-order
improvement in the places where

503
00:25:55,440 --> 00:26:00,260
I work, where people don't
get tested for anything.

504
00:26:00,260 --> 00:26:04,200
In fact, they get drugs before
they get tested for anything.

505
00:26:04,200 --> 00:26:07,920
And so that's how I
approach Ziad to see

506
00:26:07,920 --> 00:26:09,320
if he wanted to work on this.

507
00:26:09,320 --> 00:26:12,490
And the idea of heart
attack came from him.

508
00:26:12,490 --> 00:26:14,190
Again, he is a
practicing physician.

509
00:26:14,190 --> 00:26:15,190
He works in the ER.

510
00:26:15,190 --> 00:26:17,650
He sees them.

511
00:26:17,650 --> 00:26:21,310
And he also has been working in
this space for a very long time.

512
00:26:21,310 --> 00:26:25,590
And he thought it had all of
this virtue that you describe,

513
00:26:25,590 --> 00:26:28,370
which is, there
is no-- we are not

514
00:26:28,370 --> 00:26:30,010
trying to mimic
how a human would

515
00:26:30,010 --> 00:26:33,570
have made a decision, which is
a lot of how the problems are

516
00:26:33,570 --> 00:26:34,110
done.

517
00:26:34,110 --> 00:26:36,850
In fact, we are trying to
go straight to the ground

518
00:26:36,850 --> 00:26:41,290
truth, which is what
would the ultrasound say,

519
00:26:41,290 --> 00:26:42,890
where there is any--

520
00:26:42,890 --> 00:26:45,010
not really ambiguity.

521
00:26:45,010 --> 00:26:47,690
So that would be a great
advantage, something

522
00:26:47,690 --> 00:26:49,190
you can actually, very easily--

523
00:26:49,190 --> 00:26:53,250
maybe more easily do in
India, because people haven't

524
00:26:53,250 --> 00:26:55,690
gotten the ultrasound already.

525
00:26:55,690 --> 00:26:58,550
And it can be done cheaply.

526
00:26:58,550 --> 00:27:00,010
That can be deployed easily.

527
00:27:00,010 --> 00:27:03,790
And that have potential
life-saving virtue.

528
00:27:03,790 --> 00:27:07,770
So that's kind of why we wanted
to try this as a test case.

529
00:27:07,770 --> 00:27:10,500
And our proof of
concept with the idea

530
00:27:10,500 --> 00:27:12,480
that there's no
reason to stop there.

531
00:27:12,480 --> 00:27:13,940
The data is public.

532
00:27:13,940 --> 00:27:20,180
It's on the public use platform.

533
00:27:20,180 --> 00:27:24,660
So anybody can try to
do better, or to do it,

534
00:27:24,660 --> 00:27:29,260
work on other diseases, and
can also go to Tamil Nadu

535
00:27:29,260 --> 00:27:32,540
and work with us
to deploy whatever

536
00:27:32,540 --> 00:27:35,260
their idea is in the field.

537
00:27:35,260 --> 00:27:37,920
And when I heard of
MITHIC, I was like, oh,

538
00:27:37,920 --> 00:27:46,260
that's an occasion to grab the
attention of my ECS friends.

539
00:27:46,260 --> 00:27:49,500
And I had seen
Marzyeh in the context

540
00:27:49,500 --> 00:27:51,620
of talking about these
grants and a future grant

541
00:27:51,620 --> 00:27:53,800
where we were trying
to get at the PO1.

542
00:27:53,800 --> 00:27:56,620
So I thought she will be
really the right person

543
00:27:56,620 --> 00:27:59,380
to lead that effort.

544
00:27:59,380 --> 00:28:01,880
I'm also, just for
people who don't know me,

545
00:28:01,880 --> 00:28:03,820
I'm a noted hater
of AI in health,

546
00:28:03,820 --> 00:28:06,710
because that's my
research topic.

547
00:28:06,710 --> 00:28:08,930
I have seen so many
bad deployments--

548
00:28:08,930 --> 00:28:10,210
so many bad deployments.

549
00:28:10,210 --> 00:28:11,150
It's very bad.

550
00:28:11,150 --> 00:28:12,750
But a majority of
the bad deployments

551
00:28:12,750 --> 00:28:16,510
are actually when
you're trying to not--

552
00:28:16,510 --> 00:28:19,130
your goal is to make
something more efficient,

553
00:28:19,130 --> 00:28:21,070
and your idea for
efficiency is, I'll just

554
00:28:21,070 --> 00:28:25,430
colonize this human-to-human
interaction with an AI system,

555
00:28:25,430 --> 00:28:28,790
thereby taking out the
expensive part of it, which

556
00:28:28,790 --> 00:28:32,170
is the interaction with another
human, which I'm very against.

557
00:28:32,170 --> 00:28:35,190
I don't want AI to colonize all
human interactions in a health

558
00:28:35,190 --> 00:28:35,770
setting.

559
00:28:35,770 --> 00:28:40,150
And I loved this example,
because when Esther approached

560
00:28:40,150 --> 00:28:43,370
me, again, as a noted hater
of specific deployments,

561
00:28:43,370 --> 00:28:44,770
this is such a good deployment.

562
00:28:44,770 --> 00:28:47,110
It's one of the few times
somebody has approached me,

563
00:28:47,110 --> 00:28:50,370
and I have said, this makes
so much sense in every way.

564
00:28:50,370 --> 00:28:54,950
It's technically the right
data to try to use-- low cost--

565
00:28:54,950 --> 00:28:58,110
to project or predict
what high-cost data

566
00:28:58,110 --> 00:28:59,290
would have given you.

567
00:28:59,290 --> 00:29:01,472
It's attacking a problem
that actually matters,

568
00:29:01,472 --> 00:29:02,930
and that we can do
something about.

569
00:29:02,930 --> 00:29:05,060
It's not predicting
some hypothetical thing

570
00:29:05,060 --> 00:29:06,310
that there's no treatment for.

571
00:29:06,310 --> 00:29:09,770
We actually have something
we can do with it.

572
00:29:09,770 --> 00:29:11,930
And it's also
extremely scalable.

573
00:29:11,930 --> 00:29:13,730
So I think this is--

574
00:29:13,730 --> 00:29:19,130
it was one of the best
designed uses of AI

575
00:29:19,130 --> 00:29:21,310
in a healthcare
setting that I've seen,

576
00:29:21,310 --> 00:29:24,210
and I'm very excited
to be part of it.

577
00:29:24,210 --> 00:29:26,230
I have a million follow
up questions from that,

578
00:29:26,230 --> 00:29:29,090
but I'm going to
ping-pong back and forth.

579
00:29:29,090 --> 00:29:32,820
Otherwise, I'll go down the
rabbit hole of one project.

580
00:29:32,820 --> 00:29:37,770
So Arvind and Graham, I think
the question of understanding

581
00:29:37,770 --> 00:29:40,490
what happens when people
communicate with computers

582
00:29:40,490 --> 00:29:45,330
as if they were human has become
incredibly important with AI

583
00:29:45,330 --> 00:29:48,610
advances, as you noted,
and really urgent,

584
00:29:48,610 --> 00:29:55,330
and one that requires insight
from multiple disciplines.

585
00:29:55,330 --> 00:29:58,330
I guess maybe I'll start with
a somewhat irreverent kind

586
00:29:58,330 --> 00:29:59,310
of question.

587
00:29:59,310 --> 00:30:04,780
So people already talk to
their pets like they're human.

588
00:30:04,780 --> 00:30:07,780
So does that mean it's
inherent that people

589
00:30:07,780 --> 00:30:10,740
are going to communicate
with AI as if it's human?

590
00:30:10,740 --> 00:30:13,260
It certainly presents itself
as much more human-like

591
00:30:13,260 --> 00:30:16,140
than most of our pets.

592
00:30:16,140 --> 00:30:18,740
Even if it's made clear
that it's not human.

593
00:30:18,740 --> 00:30:21,540
And currently, of
course, the direction

594
00:30:21,540 --> 00:30:24,220
is it's pretending
to be human rather

595
00:30:24,220 --> 00:30:26,260
than-- but certainly
some people are

596
00:30:26,260 --> 00:30:28,640
advocating for not doing that.

597
00:30:28,640 --> 00:30:32,340
So how do you think about
how we address this,

598
00:30:32,340 --> 00:30:34,220
both with better
human understanding

599
00:30:34,220 --> 00:30:36,200
and with better
AI systems design?

600
00:30:36,200 --> 00:30:40,240
I think either one of those
alone, probably not enough.

601
00:30:40,240 --> 00:30:43,380
And maybe I'll start
with Arvind, since--

602
00:30:43,380 --> 00:30:45,280
He's looking at me
as if he wants--

603
00:30:45,280 --> 00:30:45,940
OK, well then--

604
00:30:45,940 --> 00:30:46,280
--he wants to start with me.

605
00:30:46,280 --> 00:30:46,780
All right.

606
00:30:46,780 --> 00:30:49,020
You guys figure out who starts.

607
00:30:49,020 --> 00:30:51,360
Well, I think, as
an anthropologist,

608
00:30:51,360 --> 00:30:57,300
we know, cross-culturally,
that people

609
00:30:57,300 --> 00:31:00,260
around the world
in various cultures

610
00:31:00,260 --> 00:31:04,070
have the propensity
and the capacity

611
00:31:04,070 --> 00:31:09,170
to personify all types of
entities in the world--

612
00:31:09,170 --> 00:31:13,590
not just animals, but entities
that we in Western culture

613
00:31:13,590 --> 00:31:17,110
might consider inanimate.

614
00:31:17,110 --> 00:31:20,510
And I think that this
traces back fundamentally

615
00:31:20,510 --> 00:31:25,510
to a fundamental mechanism
of human psychology, which

616
00:31:25,510 --> 00:31:26,910
is pareidolia--

617
00:31:26,910 --> 00:31:33,030
our tendency to perceive
animacy in the world,

618
00:31:33,030 --> 00:31:35,750
or to seek out
agency in the world.

619
00:31:35,750 --> 00:31:41,850
It's a capacity that's
necessary for human survival.

620
00:31:41,850 --> 00:31:44,430
And it gets organized
in various cultures

621
00:31:44,430 --> 00:31:53,150
into refined capacities or
refined cosmologies that

622
00:31:53,150 --> 00:31:56,990
allow us to experience
the world as animated

623
00:31:56,990 --> 00:32:00,380
by agents, non-human agents.

624
00:32:00,380 --> 00:32:04,300
So we know that, in a
sense, as you say, Dan,

625
00:32:04,300 --> 00:32:09,760
there's nothing new
about our tendency

626
00:32:09,760 --> 00:32:15,580
to personify or to
personify AI agents.

627
00:32:15,580 --> 00:32:22,430
I think, nevertheless,
given partially

628
00:32:22,430 --> 00:32:27,000
the institutional and
sociological conditions

629
00:32:27,000 --> 00:32:31,480
in which AI is currently
being deployed--

630
00:32:31,480 --> 00:32:35,100
if we consider, for instance,
the crisis of loneliness,

631
00:32:35,100 --> 00:32:40,400
the crisis of isolation in
modern Western democracies,

632
00:32:40,400 --> 00:32:44,440
I think it creates a
very volatile situation

633
00:32:44,440 --> 00:32:47,000
in which the
personification of AI

634
00:32:47,000 --> 00:32:49,600
can have perverse consequences.

635
00:32:49,600 --> 00:32:55,160
And so we need to be very
mindful of the cultural context

636
00:32:55,160 --> 00:33:00,890
in which AI is
activating, summoning,

637
00:33:00,890 --> 00:33:04,568
mobilizing that human
propensity for personification.

638
00:33:04,568 --> 00:33:07,110
All right, so now I'm really
going to put Arvind on the spot.

639
00:33:07,110 --> 00:33:07,510
Yeah, no.

640
00:33:07,510 --> 00:33:08,010
So--

641
00:33:08,010 --> 00:33:09,030
How do we do that?

642
00:33:09,030 --> 00:33:11,130
So I mean, that's
sort of the thing

643
00:33:11,130 --> 00:33:13,130
that we're exploring
in our class.

644
00:33:13,130 --> 00:33:17,035
So just building off of Graham's
response, we even taught this--

645
00:33:17,035 --> 00:33:18,410
I think Graham
taught this in one

646
00:33:18,410 --> 00:33:21,050
of our first lectures,
this human propensity

647
00:33:21,050 --> 00:33:27,250
to personify, anthropomorphize
even, non-human inanimate

648
00:33:27,250 --> 00:33:28,250
objects.

649
00:33:28,250 --> 00:33:31,210
And I think the difference
for me with chatbots

650
00:33:31,210 --> 00:33:33,370
is that this is now happening--

651
00:33:33,370 --> 00:33:37,210
the chatbot can respond back
to you and respond back to you

652
00:33:37,210 --> 00:33:39,090
with language.

653
00:33:39,090 --> 00:33:44,250
And language can be-- it is this
incredibly seductive medium.

654
00:33:44,250 --> 00:33:48,890
We, I mean, even going back to
the early experiments with Eliza

655
00:33:48,890 --> 00:33:50,410
that happened here
at MIT, you don't

656
00:33:50,410 --> 00:33:53,290
need a very sophisticated
algorithm for folks

657
00:33:53,290 --> 00:33:55,290
to suddenly read
tremendous amounts

658
00:33:55,290 --> 00:33:58,190
of meaning into the interaction
that they're seeing.

659
00:33:58,190 --> 00:34:04,070
And now we've got AI
models that can respond

660
00:34:04,070 --> 00:34:06,950
far more contextually,
responsively

661
00:34:06,950 --> 00:34:09,312
to what the user is saying.

662
00:34:09,312 --> 00:34:11,270
And so as a result, I
mean, I'm sure many of us

663
00:34:11,270 --> 00:34:15,750
have been following the news and
seeing all the adverse outcomes

664
00:34:15,750 --> 00:34:18,510
of that, all the way through
to really horrific things

665
00:34:18,510 --> 00:34:20,989
like suicide.

666
00:34:20,989 --> 00:34:25,949
And our goal with our class
is to give our students

667
00:34:25,949 --> 00:34:29,989
various handles into the
design process for how

668
00:34:29,989 --> 00:34:31,090
to contend with that.

669
00:34:31,090 --> 00:34:34,270
So we cover things from
the conversational design--

670
00:34:34,270 --> 00:34:37,989
how these different
multi-agent systems

671
00:34:37,989 --> 00:34:41,030
can take on different
frames, either intentionally

672
00:34:41,030 --> 00:34:45,070
or unintentionally, through
to the visual and interaction

673
00:34:45,070 --> 00:34:47,550
design pieces, because
that also contributes

674
00:34:47,550 --> 00:34:52,030
a lot of the phenomenon that
users are experiencing as they

675
00:34:52,030 --> 00:34:53,550
converse with the chatbot.

676
00:34:53,550 --> 00:34:56,760
And what I think is really
amazing for us to see

677
00:34:56,760 --> 00:35:01,960
is the students came in
already reacting to, or perhaps

678
00:35:01,960 --> 00:35:04,340
against, this phenomenon
that we're seeing.

679
00:35:04,340 --> 00:35:07,800
Many of them-- we just finished
around of presentations today--

680
00:35:07,800 --> 00:35:10,320
are designing
explicitly to make sure

681
00:35:10,320 --> 00:35:13,200
these agents aren't
sycophantic, aren't

682
00:35:13,200 --> 00:35:16,220
telling people exactly what
it is they wish to hear.

683
00:35:16,220 --> 00:35:19,120
Even going back to the very
first reading we set them,

684
00:35:19,120 --> 00:35:20,800
many of them picked
up-- you know what?

685
00:35:20,800 --> 00:35:24,520
We want from our really good
friends, a true AI companion,

686
00:35:24,520 --> 00:35:28,960
is for occasionally that
companion to push back on us.

687
00:35:28,960 --> 00:35:32,440
And what does it mean to design
for that sort of interaction

688
00:35:32,440 --> 00:35:37,520
is really at the heart of our
class and our MITHIC project.

689
00:35:37,520 --> 00:35:40,160
Great.

690
00:35:40,160 --> 00:35:40,660
All right.

691
00:35:40,660 --> 00:35:43,620
Switching context
back to health.

692
00:35:43,620 --> 00:35:49,520
[CHUCKLES] So this one's
maybe a little more

693
00:35:49,520 --> 00:35:50,580
on the technical side.

694
00:35:50,580 --> 00:35:52,930
So the training set
that you had for this,

695
00:35:52,930 --> 00:35:57,970
of about 6,000 individuals,
where you had ground truth data

696
00:35:57,970 --> 00:36:03,050
that you were trying to predict,
is actually quite small compared

697
00:36:03,050 --> 00:36:06,050
to a lot of
machine-learning systems.

698
00:36:06,050 --> 00:36:10,010
So are you developing larger
training sets, especially

699
00:36:10,010 --> 00:36:13,210
as you're looking
at subpopulations,

700
00:36:13,210 --> 00:36:15,610
which is a very important
part of your work.

701
00:36:15,610 --> 00:36:18,530
Very small training set doesn't
represent any subpopulation

702
00:36:18,530 --> 00:36:21,250
well, or might exclude
some completely.

703
00:36:21,250 --> 00:36:24,650
But also, I think one of the
big lessons from, at least one

704
00:36:24,650 --> 00:36:28,010
of my big takeaways from
modern machine learning

705
00:36:28,010 --> 00:36:31,210
is in some sense, the more
data, even that isn't obviously

706
00:36:31,210 --> 00:36:33,030
directly relevant
that you train with,

707
00:36:33,030 --> 00:36:34,190
the better these things do.

708
00:36:34,190 --> 00:36:35,930
So how are you
thinking about that

709
00:36:35,930 --> 00:36:38,768
in the context of this project?

710
00:36:38,768 --> 00:36:40,310
Yeah, that's what
the physicists say.

711
00:36:40,310 --> 00:36:41,350
No more data.

712
00:36:41,350 --> 00:36:43,010
[INTERPOSING VOICES]

713
00:36:43,010 --> 00:36:44,450
The rest doesn't matter.

714
00:36:44,450 --> 00:36:48,490
We are about to go to the field,
or maybe in the field in--

715
00:36:48,490 --> 00:36:53,040
with at least doubling
the size of the data set.

716
00:36:53,040 --> 00:36:54,880
Even that is not
going to be gigantic.

717
00:36:54,880 --> 00:36:57,220
But that's why the
Swedish came in.

718
00:36:57,220 --> 00:36:58,740
That helped.

719
00:36:58,740 --> 00:37:01,160
And then the new data
we are collecting,

720
00:37:01,160 --> 00:37:05,840
we were talking about
also being smarter,

721
00:37:05,840 --> 00:37:07,580
how we collect the data.

722
00:37:07,580 --> 00:37:10,980
In particular, we can start by
deploying people in the field

723
00:37:10,980 --> 00:37:14,900
nearby, and see whether
they look like--

724
00:37:14,900 --> 00:37:18,780
they already have our
imperfect algorithm,

725
00:37:18,780 --> 00:37:21,300
already predict where they are
likely to have had the heart

726
00:37:21,300 --> 00:37:23,780
attack, and then, if they
have, say, please, please

727
00:37:23,780 --> 00:37:25,000
go get tested.

728
00:37:25,000 --> 00:37:28,180
So we'll have more
people with instances.

729
00:37:28,180 --> 00:37:31,040
Because it's not just
the size of the data set.

730
00:37:31,040 --> 00:37:35,335
It is how many are
positive, which is an issue.

731
00:37:35,335 --> 00:37:37,460
Yeah, we have a real issue
with silent heart attack

732
00:37:37,460 --> 00:37:39,060
where the rate is very low.

733
00:37:39,060 --> 00:37:43,200
And so if you have anything that
has a very low prevalence rate,

734
00:37:43,200 --> 00:37:45,980
it's really important that
you capture as many examples

735
00:37:45,980 --> 00:37:47,640
of the positives as possible.

736
00:37:47,640 --> 00:37:51,040
And so by doing more, like,
active learning style selection

737
00:37:51,040 --> 00:37:53,440
of more participants, you
can really squeeze as much

738
00:37:53,440 --> 00:37:55,260
as possible out of
the data you have,

739
00:37:55,260 --> 00:37:57,180
because 6,000 samples
is not enough.

740
00:37:57,180 --> 00:37:59,620
But if you select the next
few thousand intelligently,

741
00:37:59,620 --> 00:38:02,040
it can do a lot better.

742
00:38:02,040 --> 00:38:04,982
And maybe, since
switching context again

743
00:38:04,982 --> 00:38:07,440
is going to be hard, because
I see this thing counting down

744
00:38:07,440 --> 00:38:08,140
in front of me.

745
00:38:08,140 --> 00:38:09,840
[LAUGHTER]

746
00:38:09,840 --> 00:38:14,640
So say you're really
successful with this project.

747
00:38:14,640 --> 00:38:18,560
Does that start to overload the
next stage of the health care

748
00:38:18,560 --> 00:38:19,180
system?

749
00:38:19,180 --> 00:38:21,600
Because these people
are not seeking

750
00:38:21,600 --> 00:38:28,220
these more expensive and
extensive forms of testing.

751
00:38:28,220 --> 00:38:29,720
Or how have you
thought about that?

752
00:38:29,720 --> 00:38:33,160
No, I think screening almost
never, never does that.

753
00:38:33,160 --> 00:38:34,860
In any of the
applications I've seen--

754
00:38:34,860 --> 00:38:37,300
and this is a very good
example, because as Esther said,

755
00:38:37,300 --> 00:38:39,680
we do actually have
treatments that are very cheap

756
00:38:39,680 --> 00:38:42,860
and can be used to maintain
the condition longer term.

757
00:38:42,860 --> 00:38:45,900
But even if we think about if
you have a silent heart attack,

758
00:38:45,900 --> 00:38:48,810
then your conditional
probability

759
00:38:48,810 --> 00:38:51,370
of having a second heart
attack is much higher.

760
00:38:51,370 --> 00:38:54,450
And so even just having
more screening opportunities

761
00:38:54,450 --> 00:38:57,230
for those individuals
is a huge opportunity.

762
00:38:57,230 --> 00:39:01,250
And so again, I hate a lot
of possible deployments

763
00:39:01,250 --> 00:39:02,070
of AI in health.

764
00:39:02,070 --> 00:39:03,470
But this is a really good one.

765
00:39:03,470 --> 00:39:06,450
It has a real impact
to do some good.

766
00:39:06,450 --> 00:39:07,730
Super.

767
00:39:07,730 --> 00:39:08,498
All right.

768
00:39:08,498 --> 00:39:10,290
We're going to turn it
to the audience now.

769
00:39:10,290 --> 00:39:14,530
And we'll let Carol
emcee that part.

770
00:39:14,530 --> 00:39:17,550
Unfortunately, [INAUDIBLE]
keeping the trains running,

771
00:39:17,550 --> 00:39:19,990
we don't have time for
any audience questions.

772
00:39:19,990 --> 00:39:21,875
[LAUGHTER]

773
00:39:21,875 --> 00:39:22,750
But we do have time--

774
00:39:22,750 --> 00:39:23,230
Wait a minute.

775
00:39:23,230 --> 00:39:24,313
Then I'm going to go over.

776
00:39:24,313 --> 00:39:26,130
[LAUGHTER]

777
00:39:26,130 --> 00:39:26,770
Sorry.

778
00:39:26,770 --> 00:39:29,890
We have a lunch
actually planned where

779
00:39:29,890 --> 00:39:31,530
we can continue
this conversation

780
00:39:31,530 --> 00:39:34,110
and the other conversations
from this morning.

781
00:39:34,110 --> 00:39:36,030
So I wanted to
thank, first of all,

782
00:39:36,030 --> 00:39:39,010
this panel for their
presentations and conversation.

783
00:39:39,010 --> 00:39:42,360
[APPLAUSE]

784
00:39:42,360 --> 00:39:45,000

