1
00:00:01,501 --> 00:00:04,260
(upbeat music)

2
00:00:04,260 --> 00:00:06,600
- The type of data robot learning needs

3
00:00:06,600 --> 00:00:08,520
has to be a lot more flexible

4
00:00:08,520 --> 00:00:11,160
and has to contain real
physics so a robot knows

5
00:00:11,160 --> 00:00:12,570
how to interact with the world.

6
00:00:12,570 --> 00:00:14,220
As it turned out, there are these things

7
00:00:14,220 --> 00:00:18,180
called generated models
that can produce realistic

8
00:00:18,180 --> 00:00:21,540
and very diverse images on
demand if you tell the model

9
00:00:21,540 --> 00:00:24,870
what you want in the language we speak.

10
00:00:24,870 --> 00:00:29,070
So our key insight is that we
can use these generated models

11
00:00:29,070 --> 00:00:31,560
as internet scale data on steroids

12
00:00:31,560 --> 00:00:33,900
or internet scale data on demand,

13
00:00:33,900 --> 00:00:37,440
and use these models to generate the data

14
00:00:37,440 --> 00:00:39,000
for the robot to learn from.

15
00:00:39,000 --> 00:00:40,590
This is what the robot sees.

16
00:00:40,590 --> 00:00:43,620
Each frame is rendered
with a generative model.

17
00:00:43,620 --> 00:00:46,350
By flashing between
different visual experiences,

18
00:00:46,350 --> 00:00:48,060
the robot gains an understanding

19
00:00:48,060 --> 00:00:51,240
of the many ways the world might look.

20
00:00:51,240 --> 00:00:53,640
So our research program
is called Lucid Sim.

21
00:00:53,640 --> 00:00:56,220
It's a simulated learning
environment for like robots

22
00:00:56,220 --> 00:00:59,760
to learn how to do visual
parkour from generated images.

23
00:00:59,760 --> 00:01:01,440
If you really want household robots

24
00:01:01,440 --> 00:01:03,990
that can live in the same room as we do,

25
00:01:03,990 --> 00:01:06,060
you need robots that have legs.

26
00:01:06,060 --> 00:01:09,510
The impact of our research is
to give robots the mobility

27
00:01:09,510 --> 00:01:13,953
and needs to operate in the
places like our humans live in.

28
00:01:14,790 --> 00:01:17,640
- Typically, people use image
generative models to make art,

29
00:01:17,640 --> 00:01:19,950
but here what we want to
do is teach the robot how

30
00:01:19,950 --> 00:01:21,510
to master its environment.

31
00:01:21,510 --> 00:01:23,760
What we really want is to have
the physics from the physics

32
00:01:23,760 --> 00:01:26,070
engine drive the image generation process,

33
00:01:26,070 --> 00:01:27,360
and the complication here is

34
00:01:27,360 --> 00:01:29,700
that you typically don't
have any control over the

35
00:01:29,700 --> 00:01:32,280
composition of the image that you produce.

36
00:01:32,280 --> 00:01:33,690
Fortunately, turns out

37
00:01:33,690 --> 00:01:35,850
that this technique called
control net, which allows you

38
00:01:35,850 --> 00:01:37,530
to control the composition of the image

39
00:01:37,530 --> 00:01:40,050
through means like depth or semantics.

40
00:01:40,050 --> 00:01:43,110
So that's how we make these
images realistic physics wise,

41
00:01:43,110 --> 00:01:45,180
the second problem that we
face is that we want the robot

42
00:01:45,180 --> 00:01:48,660
to take in a series of images
rather than a single frame.

43
00:01:48,660 --> 00:01:50,670
So we develop dreams in motion,

44
00:01:50,670 --> 00:01:53,280
which helps us generate
subsequent frames from a single

45
00:01:53,280 --> 00:01:54,990
generated image that aligns

46
00:01:54,990 --> 00:01:58,170
with the robot's movement and experience.

47
00:01:58,170 --> 00:02:00,300
So there's a technique called
image warping, which amounts

48
00:02:00,300 --> 00:02:02,670
to moving around the pixels
in an image in accordance

49
00:02:02,670 --> 00:02:05,310
to changes in a robot's
camera perspective.

50
00:02:05,310 --> 00:02:08,550
- If you just teach the robot
using data from another robot

51
00:02:08,550 --> 00:02:10,140
actually wouldn't work very well.

52
00:02:10,140 --> 00:02:12,210
So it's really, really
important for the robot

53
00:02:12,210 --> 00:02:14,490
to learn from its own
data that's generated

54
00:02:14,490 --> 00:02:15,750
by its own actions.

55
00:02:15,750 --> 00:02:18,510
And this type of data is
called on-policy data.

56
00:02:18,510 --> 00:02:19,890
So with this paper, we show

57
00:02:19,890 --> 00:02:22,410
that on-policy data is
responsible for most

58
00:02:22,410 --> 00:02:25,162
of the performance of the robot
that's learning in the end.

59
00:02:25,162 --> 00:02:27,900
(upbeat music)

60
00:02:27,900 --> 00:02:29,970
If we look at robotics as a whole,

61
00:02:29,970 --> 00:02:32,880
there are really three
problems that we have to solve.

62
00:02:32,880 --> 00:02:35,340
The first problem is how to produce robots

63
00:02:35,340 --> 00:02:38,490
and performing policies,
and that's Lucid Sim.

64
00:02:38,490 --> 00:02:40,470
Once you master individual policies,

65
00:02:40,470 --> 00:02:43,800
then the next question is
how you can get the number

66
00:02:43,800 --> 00:02:45,150
of skills you need so

67
00:02:45,150 --> 00:02:47,490
that when the robot enters the
real world, it doesn't have

68
00:02:47,490 --> 00:02:50,193
to learn new skills on
the fly from scratch.

69
00:02:51,690 --> 00:02:54,150
Now the third problem is
a little bit more subtle,

70
00:02:54,150 --> 00:02:57,180
and I believe the hardest
problems in robotics are not

71
00:02:57,180 --> 00:03:00,843
problems in robotics, but
problems with intelligence itself.

72
00:03:01,770 --> 00:03:04,530
So the final part of
this program is to expand

73
00:03:04,530 --> 00:03:08,550
what Lucid Sim does on individual
skills to an entire quest

74
00:03:08,550 --> 00:03:10,680
that involve many, many steps.

75
00:03:10,680 --> 00:03:13,620
And the goal of the third
step is to give robots ability

76
00:03:13,620 --> 00:03:17,460
to handle these long horizon
tasks and to reason about

77
00:03:17,460 --> 00:03:20,640
and to explore the open
world at its own will.

78
00:03:20,640 --> 00:03:23,550
We're so excited for you to see Lucid Sim

79
00:03:23,550 --> 00:03:25,020
and what these robots are capable

80
00:03:25,020 --> 00:03:26,730
of doing in the real world.

81
00:03:26,730 --> 00:03:28,230
I truly believe that this is

82
00:03:28,230 --> 00:03:30,630
how you'll be training
your robot in the future.

83
00:03:30,630 --> 00:03:33,333
That is to learn from generated data.

