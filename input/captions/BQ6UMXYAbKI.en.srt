1
00:00:00,000 --> 00:00:03,381
[MUSIC PLAYING]

2
00:00:03,381 --> 00:00:14,010

3
00:00:14,010 --> 00:00:15,490
MEGAN MITCHELL: Good morning.

4
00:00:15,490 --> 00:00:16,750
Good afternoon.

5
00:00:16,750 --> 00:00:21,300
Good evening, whatever your
time zone, wherever you are.

6
00:00:21,300 --> 00:00:22,570
Welcome.

7
00:00:22,570 --> 00:00:26,310
As always, we'd love to know
where people are, who you are,

8
00:00:26,310 --> 00:00:29,400
where you're joining us from.

9
00:00:29,400 --> 00:00:30,870
To add a little--

10
00:00:30,870 --> 00:00:35,700
I see some familiar faces,
some familiar names,

11
00:00:35,700 --> 00:00:38,260
a few people who I know have
been on vacation and are back.

12
00:00:38,260 --> 00:00:39,720
This is exciting.

13
00:00:39,720 --> 00:00:43,560
But we'd love to hear from
you and share who you are

14
00:00:43,560 --> 00:00:46,530
and where you're
joining us from.

15
00:00:46,530 --> 00:00:49,330
But I suppose I should start
with myself in that case.

16
00:00:49,330 --> 00:00:50,380
How's everyone doing?

17
00:00:50,380 --> 00:00:51,700
I am Megan Mitchell.

18
00:00:51,700 --> 00:00:55,320
I am currently the
acting director

19
00:00:55,320 --> 00:01:00,630
for strategy and operations for
the Jameel World Education Lab.

20
00:01:00,630 --> 00:01:02,700
And many of you are
familiar with J-WEL.

21
00:01:02,700 --> 00:01:07,320
But, actually, this is an event
that we open to the public

22
00:01:07,320 --> 00:01:09,820
and open more broadly
beyond just our members.

23
00:01:09,820 --> 00:01:12,360
So let me just ground us there.

24
00:01:12,360 --> 00:01:18,810
The Jameel World Education Lab
at MIT is a center designed--

25
00:01:18,810 --> 00:01:22,920
our mission is education
designed for everyone to thrive.

26
00:01:22,920 --> 00:01:26,880
And educators everywhere, as
I think everyone on this call

27
00:01:26,880 --> 00:01:29,730
knows and feels, are
being asked to do

28
00:01:29,730 --> 00:01:33,120
more, to reach more students,
to upgrade their learning,

29
00:01:33,120 --> 00:01:35,580
to fuel the economy,
and serve the community

30
00:01:35,580 --> 00:01:39,100
in new and unique ways.

31
00:01:39,100 --> 00:01:43,140
And integrating technology
and considering technology,

32
00:01:43,140 --> 00:01:45,210
I'm sure, is particularly
relevant to those

33
00:01:45,210 --> 00:01:47,080
who chose to join this call.

34
00:01:47,080 --> 00:01:49,500
So we work to meet
this imperative

35
00:01:49,500 --> 00:01:54,270
by working with educational
innovators around the world.

36
00:01:54,270 --> 00:01:58,050
And, together, we look
to disrupt the status

37
00:01:58,050 --> 00:02:03,130
quo in education by creating
practical, actionable new ideas

38
00:02:03,130 --> 00:02:06,100
that can transform
learning at scale.

39
00:02:06,100 --> 00:02:08,389
We do this through
ambitious projects.

40
00:02:08,389 --> 00:02:11,740
We couldn't do this without
our member organizations

41
00:02:11,740 --> 00:02:13,820
and our staff and faculty.

42
00:02:13,820 --> 00:02:18,950
Together, we explore new
ideas, share thinking,

43
00:02:18,950 --> 00:02:22,330
what's the global context, local
context for the type of work

44
00:02:22,330 --> 00:02:23,450
we're all doing.

45
00:02:23,450 --> 00:02:25,720
And we do this through
workshops and campus visits

46
00:02:25,720 --> 00:02:30,790
to MIT, sessions like
this one and really hope

47
00:02:30,790 --> 00:02:34,480
to bridge the transition
from education

48
00:02:34,480 --> 00:02:38,230
to the workforce in cultivating
innovations to better connect

49
00:02:38,230 --> 00:02:43,510
learners to understand
what's happening particularly

50
00:02:43,510 --> 00:02:48,100
in the technology space and
how that impacts all of us.

51
00:02:48,100 --> 00:02:52,150
And so, today, you're joining
us for our first event

52
00:02:52,150 --> 00:02:54,680
in our AI Innovation Series.

53
00:02:54,680 --> 00:02:55,820
So welcome.

54
00:02:55,820 --> 00:02:57,520
Welcome to all of you.

55
00:02:57,520 --> 00:02:59,980
There are five events
in this series.

56
00:02:59,980 --> 00:03:03,140
And some are public
forums like this one.

57
00:03:03,140 --> 00:03:06,490
Others are designed as
interactive discussions,

58
00:03:06,490 --> 00:03:09,880
just for our members so that
they can reflect on and react

59
00:03:09,880 --> 00:03:14,830
to maybe the previous sessions
and share their own experiences

60
00:03:14,830 --> 00:03:19,150
and what this means to them, as
they reflect on what was said

61
00:03:19,150 --> 00:03:20,270
and react.

62
00:03:20,270 --> 00:03:24,610
But also think about what
it means for them and share

63
00:03:24,610 --> 00:03:26,710
with one another,
because this really isn't

64
00:03:26,710 --> 00:03:28,340
meant to be a one-way dialogue.

65
00:03:28,340 --> 00:03:33,640
This is meant to be a
space to learn together.

66
00:03:33,640 --> 00:03:36,490
So for the members
on this call, I just

67
00:03:36,490 --> 00:03:38,830
want to call out
that next week we'll

68
00:03:38,830 --> 00:03:41,500
have a follow-on
to this discussion

69
00:03:41,500 --> 00:03:46,360
where Hae Won Park, who's a
colleague and a researcher

70
00:03:46,360 --> 00:03:49,630
in Cynthia Breazeal's
lab, will lead

71
00:03:49,630 --> 00:03:55,180
a discussion with our members
around reflections and reactions

72
00:03:55,180 --> 00:03:57,760
and really sharing.

73
00:03:57,760 --> 00:04:02,280
And then for those of you who
aren't members, or members,

74
00:04:02,280 --> 00:04:05,760
our August 22 event will
also be open to the public.

75
00:04:05,760 --> 00:04:08,860
And that will be focused on
some entrepreneurial ventures

76
00:04:08,860 --> 00:04:12,090
and people who are taking this
really interesting technology

77
00:04:12,090 --> 00:04:15,540
and these innovations and
applying them in ways,

78
00:04:15,540 --> 00:04:18,940
maybe in, we'll say, a more
commercial application.

79
00:04:18,940 --> 00:04:21,720
So what's the building
blocks from the research?

80
00:04:21,720 --> 00:04:24,150
And what's being done
at places like MIT

81
00:04:24,150 --> 00:04:28,250
to the implementation
in the field?

82
00:04:28,250 --> 00:04:32,720
And so, I think the potential
and the promise and even

83
00:04:32,720 --> 00:04:36,200
the perils, whether they're real
or imagined of generative AI,

84
00:04:36,200 --> 00:04:37,580
I think, have been--

85
00:04:37,580 --> 00:04:41,180
I mean, I know they've been
a core theme of J-WEL's work

86
00:04:41,180 --> 00:04:42,030
this year.

87
00:04:42,030 --> 00:04:44,180
And I think even--

88
00:04:44,180 --> 00:04:49,443
it touches on every conversation
we have, every opportunity.

89
00:04:49,443 --> 00:04:51,860
And I think for a lot of people
in the world, this is new.

90
00:04:51,860 --> 00:04:55,710
And this is something
that is just everywhere.

91
00:04:55,710 --> 00:04:58,100
In fact, I was having dinner
with my nine-year-old nephew

92
00:04:58,100 --> 00:04:58,610
last night.

93
00:04:58,610 --> 00:05:02,470
And he said, Auntie
Meg, generative AI--

94
00:05:02,470 --> 00:05:04,190
AI is everywhere.

95
00:05:04,190 --> 00:05:05,750
It's exploded.

96
00:05:05,750 --> 00:05:06,660
Yes and no.

97
00:05:06,660 --> 00:05:07,380
It's been around.

98
00:05:07,380 --> 00:05:09,088
We tried to talk about
that a little bit.

99
00:05:09,088 --> 00:05:14,150
But when you're eating pizza
after a Little League Baseball

100
00:05:14,150 --> 00:05:17,510
game and the topic
is AI, I think

101
00:05:17,510 --> 00:05:21,500
it just shows how it's
crossed so many dimensions.

102
00:05:21,500 --> 00:05:23,630
But I think it's been
real for all of us

103
00:05:23,630 --> 00:05:25,550
on this call for a long time.

104
00:05:25,550 --> 00:05:29,370
And so generative AI has
really been a core theme

105
00:05:29,370 --> 00:05:31,030
of J-WEL's work this year.

106
00:05:31,030 --> 00:05:34,080
And, in fact, in May,
about 30 representatives,

107
00:05:34,080 --> 00:05:36,150
from across our
member institutions,

108
00:05:36,150 --> 00:05:40,230
joined us in Cambridge for a
week of shared conversations,

109
00:05:40,230 --> 00:05:42,870
collaborations, and workshops.

110
00:05:42,870 --> 00:05:46,283
We brought in what we think were
very insightful presentations,

111
00:05:46,283 --> 00:05:47,700
whether they were
from our members

112
00:05:47,700 --> 00:05:51,630
or from folks within the
MIT community and even

113
00:05:51,630 --> 00:05:54,060
some in-person demonstrations,
to really deepen

114
00:05:54,060 --> 00:05:57,240
the understanding of what's
happening in the space, feel

115
00:05:57,240 --> 00:06:01,390
collaboration, and the like.

116
00:06:01,390 --> 00:06:05,960
And some of the key themes
that we felt that emerged--

117
00:06:05,960 --> 00:06:08,350
and I think the pain
points we felt is--

118
00:06:08,350 --> 00:06:12,070
governance in AI is
taking place in real time.

119
00:06:12,070 --> 00:06:14,800
And for us, it was in higher ed.

120
00:06:14,800 --> 00:06:18,580
I think we're feeling that
in every aspect of society,

121
00:06:18,580 --> 00:06:20,950
whether that's university,
government, governance.

122
00:06:20,950 --> 00:06:22,960
That's geopolitical.

123
00:06:22,960 --> 00:06:28,330
That is what our political
leaders are doing.

124
00:06:28,330 --> 00:06:30,970
And, really, baseline
literacy in AI

125
00:06:30,970 --> 00:06:35,920
is needed for educators
across the spectrum.

126
00:06:35,920 --> 00:06:38,650
And then, I think
some core tenets

127
00:06:38,650 --> 00:06:43,270
that undergird all of this are
trust, governance, transparency,

128
00:06:43,270 --> 00:06:44,440
and inclusion.

129
00:06:44,440 --> 00:06:48,070
Those are building blocks that
organizations and institutions

130
00:06:48,070 --> 00:06:51,520
need to think about and
consider and address

131
00:06:51,520 --> 00:06:54,940
as they embark on these
types of integration

132
00:06:54,940 --> 00:06:57,790
and thinking about
AI in their space.

133
00:06:57,790 --> 00:07:02,340
And so, I think those are some
things that you'll hear from.

134
00:07:02,340 --> 00:07:06,920
And we're excited for
today's conversation.

135
00:07:06,920 --> 00:07:10,310
So, today, as I mentioned, this
is our grounding conversation

136
00:07:10,310 --> 00:07:13,530
for our AI Innovators Series.

137
00:07:13,530 --> 00:07:19,280
And so today you will hear from
MIT Dean for Digital Learning,

138
00:07:19,280 --> 00:07:21,200
Professor Cynthia Breazeal.

139
00:07:21,200 --> 00:07:23,210
And she'll give an
introduction to how

140
00:07:23,210 --> 00:07:25,580
these types of
technologies work,

141
00:07:25,580 --> 00:07:28,070
how students and teachers
might deploy these tools

142
00:07:28,070 --> 00:07:30,350
and approaches,
and how schools can

143
00:07:30,350 --> 00:07:32,420
take advantage of
new opportunities,

144
00:07:32,420 --> 00:07:35,100
and manage the many
challenges we know we'll face.

145
00:07:35,100 --> 00:07:36,920
But a challenge
is an opportunity

146
00:07:36,920 --> 00:07:39,320
if you turn it
around when promoting

147
00:07:39,320 --> 00:07:41,420
student-independent learning.

148
00:07:41,420 --> 00:07:47,390
So before I turn it over
to Professor Breazeal--

149
00:07:47,390 --> 00:07:51,950
she is a leader in this space
globally and a leader at MIT.

150
00:07:51,950 --> 00:07:56,330
She is the director
of MIT RAISE, which

151
00:07:56,330 --> 00:07:59,870
is Responsible AI for
Social Empowerment,

152
00:07:59,870 --> 00:08:03,590
is what RAISE stands for, which
is across MIT research effort

153
00:08:03,590 --> 00:08:06,320
on advancing and
democratizing AI

154
00:08:06,320 --> 00:08:10,850
education, particularly through
K to 12 and adult education.

155
00:08:10,850 --> 00:08:13,860
It's a collaboration
between MIT Open Learning--

156
00:08:13,860 --> 00:08:17,810
Open Learning is where
J-WEL sits and lives within

157
00:08:17,810 --> 00:08:19,460
the institution--

158
00:08:19,460 --> 00:08:22,820
the Media Lab, and the
Schwarzman College of Computing.

159
00:08:22,820 --> 00:08:28,460
RAISE offers annual K
to 12 outreach programs,

160
00:08:28,460 --> 00:08:32,870
including a six-week summer
program called MIT Future

161
00:08:32,870 --> 00:08:33,600
Makers.

162
00:08:33,600 --> 00:08:37,159
They do the Day of AI,
which is a great opportunity

163
00:08:37,159 --> 00:08:39,830
for thousands of teachers
across the world to bring AI

164
00:08:39,830 --> 00:08:42,080
literacy into their classrooms.

165
00:08:42,080 --> 00:08:46,310
And under her leadership,
she's launched these programs

166
00:08:46,310 --> 00:08:50,180
to introduce students of
all backgrounds and teachers

167
00:08:50,180 --> 00:08:53,730
to the foundational concepts of
AI and its role in their lives.

168
00:08:53,730 --> 00:08:55,550
And so, there's where
you start to-- you

169
00:08:55,550 --> 00:09:00,290
build in that inclusion that's
so important in all of the work

170
00:09:00,290 --> 00:09:04,070
that we do and the
considerations that we do.

171
00:09:04,070 --> 00:09:06,980
Dean Breazeal also heads the
Personal Robotics Research Group

172
00:09:06,980 --> 00:09:08,870
at the MIT Media Lab.

173
00:09:08,870 --> 00:09:11,480
And that's where she's a
professor of media, arts,

174
00:09:11,480 --> 00:09:12,330
and sciences.

175
00:09:12,330 --> 00:09:16,250
And her research focus includes
technical innovation in AI

176
00:09:16,250 --> 00:09:20,000
and user-experience design and
understanding the psychology

177
00:09:20,000 --> 00:09:24,590
of engagement to design
personified-AI technologies that

178
00:09:24,590 --> 00:09:29,070
can promote human flourishing
and personal growth.

179
00:09:29,070 --> 00:09:34,290
Professor Dean Breazeal is
also the faculty director head

180
00:09:34,290 --> 00:09:38,470
that we work closely with at
the Jameel World Education Lab.

181
00:09:38,470 --> 00:09:41,610
So, her work intersects
in so many places

182
00:09:41,610 --> 00:09:46,260
and is so important to all that
we do at J-WEL but then also

183
00:09:46,260 --> 00:09:48,870
the work that is
happening across MIT,

184
00:09:48,870 --> 00:09:52,600
in this transformational space.

185
00:09:52,600 --> 00:09:55,120
So now what I'd like
to do is-- actually,

186
00:09:55,120 --> 00:09:59,740
before we turn it
over to Dean Breazeal,

187
00:09:59,740 --> 00:10:05,320
I'm going to actually introduce
Bill Bonvillian, who is the--

188
00:10:05,320 --> 00:10:10,390
he has the-- we'll call it
the "distinguished moniker"

189
00:10:10,390 --> 00:10:12,890
to be the head of
our Thinking Series.

190
00:10:12,890 --> 00:10:16,720
And I think if you spent
any time with his bio,

191
00:10:16,720 --> 00:10:20,200
you'd understand why that's a
perfect Thinking Series for him

192
00:10:20,200 --> 00:10:21,710
to be leading for us.

193
00:10:21,710 --> 00:10:26,170
He's authored many books
and articles and chapters

194
00:10:26,170 --> 00:10:28,930
on science, technology, and
science-technology policy

195
00:10:28,930 --> 00:10:30,740
and workforce education.

196
00:10:30,740 --> 00:10:36,920
And he's been teaching science
and policy at MIT since 2007.

197
00:10:36,920 --> 00:10:39,400
So he's really been
integral in helping

198
00:10:39,400 --> 00:10:42,850
us think about what this
series would look like

199
00:10:42,850 --> 00:10:48,520
and has worked, really, over
time to help us and help MIT

200
00:10:48,520 --> 00:10:51,910
support their relationships
with research and development

201
00:10:51,910 --> 00:10:54,450
operations actions
at the government

202
00:10:54,450 --> 00:10:56,580
and institutional level there.

203
00:10:56,580 --> 00:11:00,240
And he's just a wonderful
collaborator and member

204
00:11:00,240 --> 00:11:02,200
of our team and our community.

205
00:11:02,200 --> 00:11:03,810
So what I'd like to
do is-- he's going

206
00:11:03,810 --> 00:11:06,840
to share a little
bit of his thoughts

207
00:11:06,840 --> 00:11:11,850
and the origins of
this series and then

208
00:11:11,850 --> 00:11:14,260
turn it over to Dean Breazeal.

209
00:11:14,260 --> 00:11:15,648
So, Bill, can I turn it to you?

210
00:11:15,648 --> 00:11:17,440
WILLIAM B. BONVILLAIN:
Sure, thanks, Megan.

211
00:11:17,440 --> 00:11:18,982
And I'll just be
very brief because I

212
00:11:18,982 --> 00:11:22,470
want to get to Cynthia too.

213
00:11:22,470 --> 00:11:26,100
AI offers, as I think we
know, a whole new suite

214
00:11:26,100 --> 00:11:27,070
of opportunities.

215
00:11:27,070 --> 00:11:28,710
So it's the second
big revolution

216
00:11:28,710 --> 00:11:33,090
that's been coming along in
education in the last decade.

217
00:11:33,090 --> 00:11:36,270
The first really was the
advent of online education

218
00:11:36,270 --> 00:11:40,510
at real scale and the technical
mechanisms to do that.

219
00:11:40,510 --> 00:11:41,790
This is the latest.

220
00:11:41,790 --> 00:11:44,340
And it offers an
opportunity for--

221
00:11:44,340 --> 00:11:46,440
and I'll just hit
some highlights--

222
00:11:46,440 --> 00:11:49,410
a new personalization
of education,

223
00:11:49,410 --> 00:11:53,500
where teachers can really
adapt education, really,

224
00:11:53,500 --> 00:11:55,450
to the needs of
particular students.

225
00:11:55,450 --> 00:11:57,730
Part of that is that
it opens an opportunity

226
00:11:57,730 --> 00:12:00,550
for digital tutoring.

227
00:12:00,550 --> 00:12:03,490
AI can help in grading, at
least on simple exams that

228
00:12:03,490 --> 00:12:07,900
use answer keys, but over time,
potentially more than that.

229
00:12:07,900 --> 00:12:11,050
It can offer
feedback to students

230
00:12:11,050 --> 00:12:12,310
on what they're learning.

231
00:12:12,310 --> 00:12:14,230
In other words, it can
alert them to things

232
00:12:14,230 --> 00:12:16,810
that they're missing and
put them into feedback loops

233
00:12:16,810 --> 00:12:19,390
to catch up.

234
00:12:19,390 --> 00:12:23,500
And it can reduce time
on administrative tasks

235
00:12:23,500 --> 00:12:25,780
for teachers and
for administrators.

236
00:12:25,780 --> 00:12:27,760
Obviously, there's
drawbacks to AI.

237
00:12:27,760 --> 00:12:31,840
Like all new technologies, it's
always a double-edged sword.

238
00:12:31,840 --> 00:12:35,590
Some of the issues include
reduced human interaction

239
00:12:35,590 --> 00:12:41,710
for students, overdependence
on technology,

240
00:12:41,710 --> 00:12:47,080
and reduced interaction
with real, live teachers.

241
00:12:47,080 --> 00:12:49,330
And there are,
obviously, data-privacy

242
00:12:49,330 --> 00:12:50,290
and security issues.

243
00:12:50,290 --> 00:12:51,770
And there's ethical issues.

244
00:12:51,770 --> 00:12:55,030
Does this open big doors
to larger-scale cheating.

245
00:12:55,030 --> 00:12:56,690
And how do we
manage that process?

246
00:12:56,690 --> 00:13:00,510
So that's just a quick snapshot
on some of the background

247
00:13:00,510 --> 00:13:01,010
issues.

248
00:13:01,010 --> 00:13:05,750
But let's turn to Cynthia, who
is going to really educate us

249
00:13:05,750 --> 00:13:07,440
on these kinds of issues.

250
00:13:07,440 --> 00:13:09,870
Cynthia.

251
00:13:09,870 --> 00:13:12,170
CYNTHIA BREAZEAL: Thank
you, Bill and Megan.

252
00:13:12,170 --> 00:13:15,080
It's a pleasure to
be joining you here.

253
00:13:15,080 --> 00:13:19,520
I'm going to be touching on
a lot of different topics,

254
00:13:19,520 --> 00:13:23,360
as it relates to the
intersection of AI

255
00:13:23,360 --> 00:13:26,510
and education.

256
00:13:26,510 --> 00:13:31,280
My goal is to tell you a little
bit about the research that's

257
00:13:31,280 --> 00:13:33,860
happening in academic
settings as AI

258
00:13:33,860 --> 00:13:38,020
is advancing different roles
it can play in education.

259
00:13:38,020 --> 00:13:40,520
I'm sure a lot of you are very
aware of the personalized AI

260
00:13:40,520 --> 00:13:41,040
tutors.

261
00:13:41,040 --> 00:13:43,743
I'm going to present our
particular take on that, which

262
00:13:43,743 --> 00:13:46,160
is different than probably
what you've heard about Cognigo

263
00:13:46,160 --> 00:13:47,030
and things like that.

264
00:13:47,030 --> 00:13:48,697
Very different age
group, very different

265
00:13:48,697 --> 00:13:51,260
focus on the kinds
of intelligence.

266
00:13:51,260 --> 00:13:55,140
I am going to talk about
helping people learn about AI.

267
00:13:55,140 --> 00:13:57,620
So learning with AI, you
could argue that's the "AI

268
00:13:57,620 --> 00:14:01,830
personalized learning companion"
work, learning about AI,

269
00:14:01,830 --> 00:14:03,540
so AI literacy.

270
00:14:03,540 --> 00:14:07,790
And we at MIT now are actually
using a term, "AI fluency,"

271
00:14:07,790 --> 00:14:10,440
which is a much deeper
engagement with this material

272
00:14:10,440 --> 00:14:13,740
that empowers people to
create solutions with these

273
00:14:13,740 --> 00:14:14,500
technologies.

274
00:14:14,500 --> 00:14:17,160
And how do we empower a much
more diverse group of people

275
00:14:17,160 --> 00:14:18,720
to do that?

276
00:14:18,720 --> 00:14:20,700
And then at the
end of the talk, I

277
00:14:20,700 --> 00:14:23,250
want to just share
a bunch of resources

278
00:14:23,250 --> 00:14:26,370
that we at MIT and
RAISE have developed

279
00:14:26,370 --> 00:14:30,120
on a broad set of topics
that you may find helpful

280
00:14:30,120 --> 00:14:33,870
and interesting from curriculum
materials and policy documents

281
00:14:33,870 --> 00:14:34,630
and so forth.

282
00:14:34,630 --> 00:14:36,797
And, hopefully, that can
maybe inform the discussion

283
00:14:36,797 --> 00:14:38,740
you have with Hae Won later on.

284
00:14:38,740 --> 00:14:41,340
So, again, to talk
about learning with AI,

285
00:14:41,340 --> 00:14:44,070
learning about AI, and
then some resources

286
00:14:44,070 --> 00:14:46,907
that hopefully you'll
find useful to you--

287
00:14:46,907 --> 00:14:48,990
a lot of the work that I
am going to be presenting

288
00:14:48,990 --> 00:14:51,910
is going to be through
the lens of K-12.

289
00:14:51,910 --> 00:14:54,150
But I can assure you
many of the technologies

290
00:14:54,150 --> 00:14:59,040
and insights and lessons
apply to people of all ages

291
00:14:59,040 --> 00:15:00,880
and learning stages.

292
00:15:00,880 --> 00:15:03,160
So let me get started.

293
00:15:03,160 --> 00:15:06,960
So I first, again, want to
talk about my research group

294
00:15:06,960 --> 00:15:11,220
to orient you towards the
lens upon which we approach

295
00:15:11,220 --> 00:15:14,040
this challenge or
this opportunity of,

296
00:15:14,040 --> 00:15:19,420
how do you design interactive AI
systems that help people learn?

297
00:15:19,420 --> 00:15:23,730
So as Megan mentioned, I
am the founder and director

298
00:15:23,730 --> 00:15:27,730
in the MIT Media Lab of
the Personal Robots Group.

299
00:15:27,730 --> 00:15:31,290
And as that title suggests,
we actually design and develop

300
00:15:31,290 --> 00:15:33,640
physical robots.

301
00:15:33,640 --> 00:15:34,960
And so what kind of robots?

302
00:15:34,960 --> 00:15:36,668
Well, if you think
about science fiction,

303
00:15:36,668 --> 00:15:39,510
like Star Wars and
R2-D2 and C-3PO,

304
00:15:39,510 --> 00:15:43,570
that is actually the kind of
robot that we try to make,

305
00:15:43,570 --> 00:15:49,110
so robots that can deeply
interact with people in a very

306
00:15:49,110 --> 00:15:53,820
socially and "emotionally
intelligent human centered" way,

307
00:15:53,820 --> 00:15:58,950
very much how we have dreamed
about them in the movies.

308
00:15:58,950 --> 00:16:02,190
And our goal then is to--
obviously, we're not there yet.

309
00:16:02,190 --> 00:16:06,540
So we are trying to
responsibly advance

310
00:16:06,540 --> 00:16:10,350
the science, the computational
methods to endow machines,

311
00:16:10,350 --> 00:16:14,070
in this case robots, with
these capabilities that

312
00:16:14,070 --> 00:16:16,380
allow them to much deeply
understand and engage

313
00:16:16,380 --> 00:16:18,210
and collaborate with us.

314
00:16:18,210 --> 00:16:21,510
And it's really oriented
towards helping people

315
00:16:21,510 --> 00:16:24,200
achieve our goals, right?

316
00:16:24,200 --> 00:16:28,920
So how through that interaction,
can these social robots

317
00:16:28,920 --> 00:16:32,190
help unlock human potential,
through education,

318
00:16:32,190 --> 00:16:36,310
say, better health care, and
so forth, maximize our success?

319
00:16:36,310 --> 00:16:39,888
How do we design to address what
humans need to be successful

320
00:16:39,888 --> 00:16:42,180
so that we are actually more
effective with these tools

321
00:16:42,180 --> 00:16:43,510
and technologies?

322
00:16:43,510 --> 00:16:46,740
And a lot of this is about, how
do we promote human flourishing?

323
00:16:46,740 --> 00:16:50,080
How do we help people become
who they aspire to be?

324
00:16:50,080 --> 00:16:52,800
So that's the mission of
the Personal Robots Group.

325
00:16:52,800 --> 00:16:55,650
And that's the lens upon,
which a lot of this work,

326
00:16:55,650 --> 00:16:58,680
this first part of the talk,
I'm going to touch upon.

327
00:16:58,680 --> 00:17:04,599
So we're in the age now of
this era of generative AI.

328
00:17:04,599 --> 00:17:09,750
I can tell you as a roboticist,
we are extremely practical.

329
00:17:09,750 --> 00:17:12,720
All of these cool new
tools and technologies--

330
00:17:12,720 --> 00:17:15,599
obviously, before
generative AI, deep learning

331
00:17:15,599 --> 00:17:19,140
was the latest hype cycle,
various hype cycles before that.

332
00:17:19,140 --> 00:17:23,310
We basically just view these
as cool tools in a toolbox

333
00:17:23,310 --> 00:17:25,710
that we actually
have to leverage

334
00:17:25,710 --> 00:17:29,370
to design and create solutions.

335
00:17:29,370 --> 00:17:31,680
Many of these techniques,
many of these methods

336
00:17:31,680 --> 00:17:33,750
with human-centered
design to get a robot

337
00:17:33,750 --> 00:17:37,060
to do anything interesting
or useful for people, right?

338
00:17:37,060 --> 00:17:40,860
So we, basically, view each of
these new tools and technologies

339
00:17:40,860 --> 00:17:42,690
as just another
thing we can leverage

340
00:17:42,690 --> 00:17:46,380
to help us advance our mission
faster, more flexibly, more

341
00:17:46,380 --> 00:17:48,430
robustly, et cetera.

342
00:17:48,430 --> 00:17:54,870
So I will say, we have
been on this trajectory

343
00:17:54,870 --> 00:17:59,260
certainly since I think
Siri came out on the iPhone,

344
00:17:59,260 --> 00:18:03,850
where now a dominant mode of
which everyday people interact

345
00:18:03,850 --> 00:18:08,840
with artificial intelligence
is as a personified entity.

346
00:18:08,840 --> 00:18:11,920
Now, before that, a lot of that
was decision-support tools,

347
00:18:11,920 --> 00:18:13,600
graphs, whatever,
more like a tool

348
00:18:13,600 --> 00:18:15,440
that you would use
to make decisions.

349
00:18:15,440 --> 00:18:17,890
The interfaces are
becoming more and more

350
00:18:17,890 --> 00:18:19,745
social and interpersonal, right?

351
00:18:19,745 --> 00:18:21,370
So there's a long
tradition, of course,

352
00:18:21,370 --> 00:18:25,880
of AI for graphical
agents, virtual characters.

353
00:18:25,880 --> 00:18:26,920
We mentioned Siri.

354
00:18:26,920 --> 00:18:28,240
Then Alexa came along.

355
00:18:28,240 --> 00:18:30,437
And now you had
far-field speech, right?

356
00:18:30,437 --> 00:18:32,020
And the whole question
was, are people

357
00:18:32,020 --> 00:18:33,880
actually going to talk to AI?

358
00:18:33,880 --> 00:18:35,770
And it turned out
this idea of AI

359
00:18:35,770 --> 00:18:38,710
that could be evoked
just with a wake word.

360
00:18:38,710 --> 00:18:40,630
Suddenly, it allowed
people of all ages,

361
00:18:40,630 --> 00:18:42,220
from young children
to older adults,

362
00:18:42,220 --> 00:18:45,260
to access AI in a
much more easy way.

363
00:18:45,260 --> 00:18:48,490
So that was another
big jump ahead.

364
00:18:48,490 --> 00:18:50,320
And now, of course,
we're in this era

365
00:18:50,320 --> 00:18:51,970
of generative AI with ChatGPT.

366
00:18:51,970 --> 00:18:55,630
And now, ironically, we're back
to text on screen and typing.

367
00:18:55,630 --> 00:18:59,050
But the language and the
multi-turn interaction

368
00:18:59,050 --> 00:19:02,920
is now much more
advanced, flexible,

369
00:19:02,920 --> 00:19:04,960
and robust with these
generative methods

370
00:19:04,960 --> 00:19:08,000
than they had been
done previously.

371
00:19:08,000 --> 00:19:10,030
And now we're
seeing even more so,

372
00:19:10,030 --> 00:19:17,310
people are very willing to have
long, intimate conversations

373
00:19:17,310 --> 00:19:18,670
with AI.

374
00:19:18,670 --> 00:19:21,270
So just to say,
we as human beings

375
00:19:21,270 --> 00:19:25,967
are so willing to engage AIs
in a social and emotional way.

376
00:19:25,967 --> 00:19:28,050
And I have seen this from
my own work for decades.

377
00:19:28,050 --> 00:19:32,640
But now with the proliferation
of this personified-AI

378
00:19:32,640 --> 00:19:35,670
technologies, I think now
everybody appreciates--

379
00:19:35,670 --> 00:19:39,540
this is a dominant mode upon
which everyday people interact

380
00:19:39,540 --> 00:19:40,680
with AIs.

381
00:19:40,680 --> 00:19:42,210
And there are
important implications

382
00:19:42,210 --> 00:19:45,350
of that, important
implications of that.

383
00:19:45,350 --> 00:19:49,100
So, first of all, when
you look at human behavior

384
00:19:49,100 --> 00:19:51,770
and intelligence, it's just
important to acknowledge

385
00:19:51,770 --> 00:19:52,980
that we have evolved.

386
00:19:52,980 --> 00:19:56,930
We have very specialized
regions in our brain

387
00:19:56,930 --> 00:20:01,080
to understand and interact
with the social world.

388
00:20:01,080 --> 00:20:03,030
Now, what do I mean
by the social world?

389
00:20:03,030 --> 00:20:06,950
We have the inanimate world,
where objects are governed

390
00:20:06,950 --> 00:20:08,162
by the laws of physics.

391
00:20:08,162 --> 00:20:09,620
And we have parts
of our brain that

392
00:20:09,620 --> 00:20:12,770
are good at understanding
those entities.

393
00:20:12,770 --> 00:20:15,530
And then there
are social others.

394
00:20:15,530 --> 00:20:17,960
And social others'
behavior is governed

395
00:20:17,960 --> 00:20:23,300
by having a mind, so mental
states, things like thoughts,

396
00:20:23,300 --> 00:20:27,500
beliefs, intents, desires,
inferring them and understanding

397
00:20:27,500 --> 00:20:31,190
them, having your own and being
able to infer and recognize

398
00:20:31,190 --> 00:20:34,130
that in others, in order to
understand and coordinate

399
00:20:34,130 --> 00:20:36,110
behavior, is like--

400
00:20:36,110 --> 00:20:40,740
we are like the super species
of that on the planet.

401
00:20:40,740 --> 00:20:43,250
If you look at everything around
us, everything we've built,

402
00:20:43,250 --> 00:20:45,630
it's all because we can
communicate and collaborate

403
00:20:45,630 --> 00:20:48,860
in these extremely sophisticated
ways with one another.

404
00:20:48,860 --> 00:20:51,690
This is an incredible
competence that we have.

405
00:20:51,690 --> 00:20:53,700
And I can tell you,
it is very difficult

406
00:20:53,700 --> 00:20:57,310
to endow machines with
this kind of capability.

407
00:20:57,310 --> 00:20:59,560
But a list of these
things are here.

408
00:20:59,560 --> 00:21:03,750
So how do you have a theory of
other minds, again, inferring

409
00:21:03,750 --> 00:21:04,720
these mental states?

410
00:21:04,720 --> 00:21:06,330
How do you empathize with other?

411
00:21:06,330 --> 00:21:09,460
How can you connect,
communicate, harmonize?

412
00:21:09,460 --> 00:21:11,680
All of these things are
about social intelligence.

413
00:21:11,680 --> 00:21:13,200
And so, again, an
opportunity and challenge

414
00:21:13,200 --> 00:21:14,880
that my group has
looked at for decades

415
00:21:14,880 --> 00:21:18,127
is, what does that mean when
you talk about a machine that

416
00:21:18,127 --> 00:21:19,710
can collaborate with
us in a much more

417
00:21:19,710 --> 00:21:21,710
human-centered, natural way?

418
00:21:21,710 --> 00:21:24,540
So a lot of my work
has looked at, again,

419
00:21:24,540 --> 00:21:27,030
advancing the social and
emotional intelligence

420
00:21:27,030 --> 00:21:28,890
of AI systems.

421
00:21:28,890 --> 00:21:31,500
And particularly around
emotional engagement,

422
00:21:31,500 --> 00:21:34,590
how do you design to be a
more richly collaborative

423
00:21:34,590 --> 00:21:37,380
ally, real, bidirectional
give and take?

424
00:21:37,380 --> 00:21:39,928
And thinking about
long-term interaction, when

425
00:21:39,928 --> 00:21:41,970
you look at Star Wars and
you think about robots,

426
00:21:41,970 --> 00:21:45,060
it's not like it's a device
that you turn on and turn

427
00:21:45,060 --> 00:21:46,510
off and walk away from it.

428
00:21:46,510 --> 00:21:48,900
But robots-- they live with you.

429
00:21:48,900 --> 00:21:50,530
They are part of your world.

430
00:21:50,530 --> 00:21:52,330
They are beyond the screen.

431
00:21:52,330 --> 00:21:54,780
And there are different
challenges and opportunities

432
00:21:54,780 --> 00:21:56,880
in designing something
that is of our world,

433
00:21:56,880 --> 00:21:59,945
in our world, shares our
world, and over long periods

434
00:21:59,945 --> 00:22:01,320
of time where it
goes beyond just

435
00:22:01,320 --> 00:22:04,800
session-to-session interaction,
to this idea of building

436
00:22:04,800 --> 00:22:06,152
a relationship.

437
00:22:06,152 --> 00:22:07,860
And you're starting
to see the beginnings

438
00:22:07,860 --> 00:22:10,830
of people's willingness to
build a relationship with AIs.

439
00:22:10,830 --> 00:22:14,100
And so then what are
the appropriate roles

440
00:22:14,100 --> 00:22:16,860
between people and
increasingly-intelligent

441
00:22:16,860 --> 00:22:17,380
machines?

442
00:22:17,380 --> 00:22:19,890
So when we think about
the foundational building

443
00:22:19,890 --> 00:22:23,820
blocks of these capabilities,
the field of social robotics,

444
00:22:23,820 --> 00:22:24,450
increasingly--

445
00:22:24,450 --> 00:22:27,630
I would say even fields like
natural language processing

446
00:22:27,630 --> 00:22:31,150
is broadening out to think about
social intelligence questions.

447
00:22:31,150 --> 00:22:34,290
But how do you recognize,
again, all the richness

448
00:22:34,290 --> 00:22:38,070
and sophistication and subtlety
of human communication?

449
00:22:38,070 --> 00:22:40,110
Again, we're talking
about ChatGPT, which

450
00:22:40,110 --> 00:22:41,530
is largely text and language.

451
00:22:41,530 --> 00:22:44,890
Still, they're just starting to
release multimodal capabilities.

452
00:22:44,890 --> 00:22:47,730
But if you look at the more
human side of literature,

453
00:22:47,730 --> 00:22:50,280
I mean, people will say human
communication is anywhere

454
00:22:50,280 --> 00:22:52,823
from 60% to 80% non-verbal.

455
00:22:52,823 --> 00:22:53,323
Non-verbal.

456
00:22:53,323 --> 00:22:56,640
[LAUGHS] There is
a lot of richness

457
00:22:56,640 --> 00:22:59,470
in how we communicate that goes
beyond the words that we speak.

458
00:22:59,470 --> 00:23:02,850
And, in fact, if given a person
an option between believing

459
00:23:02,850 --> 00:23:05,580
what person a person
says, via their words,

460
00:23:05,580 --> 00:23:08,010
versus how they say it with
their prosody or their body,

461
00:23:08,010 --> 00:23:09,930
they tend to believe
how you say it

462
00:23:09,930 --> 00:23:12,610
as your true meaning
and your true intention.

463
00:23:12,610 --> 00:23:15,340
So non-verbal
communication is huge.

464
00:23:15,340 --> 00:23:18,157
It's still a largely uncracked
nut, I would say, in AI.

465
00:23:18,157 --> 00:23:20,490
But this is where things are
starting to try to go more,

466
00:23:20,490 --> 00:23:22,290
not just on "one
to one individual"

467
00:23:22,290 --> 00:23:24,968
interaction but to a group.

468
00:23:24,968 --> 00:23:27,010
We can collaborate with
groups of people as well.

469
00:23:27,010 --> 00:23:29,615
So, again, lots of room
to grow in the perception

470
00:23:29,615 --> 00:23:30,990
of human social
cues-- and that's

471
00:23:30,990 --> 00:23:33,480
giving you a sense of where
the new challenges are.

472
00:23:33,480 --> 00:23:35,822
You've heard a lot
about machine learning.

473
00:23:35,822 --> 00:23:38,280
I'm sure you heard a lot about
supervised machine learning,

474
00:23:38,280 --> 00:23:40,020
with these large data
sets that you label

475
00:23:40,020 --> 00:23:43,530
and you feed in to recognize and
classify and predict categories

476
00:23:43,530 --> 00:23:45,650
and classes and patterns.

477
00:23:45,650 --> 00:23:47,280
There's reinforcement learning.

478
00:23:47,280 --> 00:23:48,530
You're going to see
some examples of that

479
00:23:48,530 --> 00:23:49,988
in my talk, where
a system actually

480
00:23:49,988 --> 00:23:52,180
learns through its own
firsthand experience.

481
00:23:52,180 --> 00:23:54,717
You're hearing about generative
AI, which is a combination.

482
00:23:54,717 --> 00:23:56,300
It's an unsupervised
learning approach

483
00:23:56,300 --> 00:23:58,258
that combines aspects of
reinforcement learning

484
00:23:58,258 --> 00:24:00,820
and supervised big neural
networks and so forth.

485
00:24:00,820 --> 00:24:02,150
It's a mash up.

486
00:24:02,150 --> 00:24:05,120
But the point is, when you look
at social-emotional learning,

487
00:24:05,120 --> 00:24:06,990
it's not a
one-directional stream.

488
00:24:06,990 --> 00:24:10,790
It's often a collaboration
between the teacher

489
00:24:10,790 --> 00:24:12,110
versus the learner.

490
00:24:12,110 --> 00:24:13,610
And often we think
about the machine

491
00:24:13,610 --> 00:24:16,272
as being the learner and the
human being, the instructor,

492
00:24:16,272 --> 00:24:18,480
whether it's through data
or interaction or whatever.

493
00:24:18,480 --> 00:24:20,090
But the reality is--

494
00:24:20,090 --> 00:24:21,950
and human interaction
is very fluid.

495
00:24:21,950 --> 00:24:26,280
We can learn and collaborate
from each other as peers,

496
00:24:26,280 --> 00:24:27,390
et cetera, et cetera.

497
00:24:27,390 --> 00:24:29,980
So when you think about
this interaction of teaching

498
00:24:29,980 --> 00:24:32,480
and learning, especially for a
robot, which is now something

499
00:24:32,480 --> 00:24:34,688
facing you and is having to
learn from your real time

500
00:24:34,688 --> 00:24:36,990
interaction, how do you view
that as a collaboration?

501
00:24:36,990 --> 00:24:38,490
So there's a lot
of interesting work

502
00:24:38,490 --> 00:24:40,100
in social forms of
learning that is

503
00:24:40,100 --> 00:24:41,550
informing that work and beyond.

504
00:24:41,550 --> 00:24:44,120
So, again, how do you interact
with and teach with something

505
00:24:44,120 --> 00:24:46,290
that is physically there,
learning from you in the moment

506
00:24:46,290 --> 00:24:47,707
to do something
that's customized,

507
00:24:47,707 --> 00:24:49,650
personalized, context relevant.

508
00:24:49,650 --> 00:24:51,210
That's a different
learning problem

509
00:24:51,210 --> 00:24:52,585
than just feeding
a bunch of data

510
00:24:52,585 --> 00:24:55,400
and trying to learn
classifiers and so forth.

511
00:24:55,400 --> 00:24:59,390
When we talked about a system
that can coordinate with you,

512
00:24:59,390 --> 00:25:01,190
it's not only important
that the robot

513
00:25:01,190 --> 00:25:03,020
be able to understand
your social cues.

514
00:25:03,020 --> 00:25:07,530
But you have to understand and
infer its states of mind, right?

515
00:25:07,530 --> 00:25:10,790
And so, how the robot expresses
both verbally and non-verbally

516
00:25:10,790 --> 00:25:13,370
is actually quite important,
because as human beings,

517
00:25:13,370 --> 00:25:16,710
we are trying to read
both signals all the time,

518
00:25:16,710 --> 00:25:20,390
whether you meant to design
something for that or not.

519
00:25:20,390 --> 00:25:23,390
So the point is, we are
inferring all kinds of things

520
00:25:23,390 --> 00:25:24,720
from a robot's behavior.

521
00:25:24,720 --> 00:25:26,750
So you really have to
think holistically about,

522
00:25:26,750 --> 00:25:29,040
how is it expressing
itself to the person

523
00:25:29,040 --> 00:25:32,150
so that they have appropriate
mental models of what

524
00:25:32,150 --> 00:25:36,180
the robot is actually
cognitively processing, right?

525
00:25:36,180 --> 00:25:38,580
So that expression
becomes really important.

526
00:25:38,580 --> 00:25:40,017
And then, in the
big picture, now

527
00:25:40,017 --> 00:25:42,600
what happens when you're in this
interaction is collaboration.

528
00:25:42,600 --> 00:25:44,960
Well, now you're in the realm
of human-social psychology

529
00:25:44,960 --> 00:25:46,050
and social perception.

530
00:25:46,050 --> 00:25:48,720
And, again, people
can't turn this off.

531
00:25:48,720 --> 00:25:50,630
They will ascribe these
things to the nature

532
00:25:50,630 --> 00:25:52,800
of the interaction,
even with the machine.

533
00:25:52,800 --> 00:25:54,710
So when you're in
collaboration, things

534
00:25:54,710 --> 00:25:58,830
like building rapport, building
trust and trustworthiness,

535
00:25:58,830 --> 00:26:00,330
a sense of liking--

536
00:26:00,330 --> 00:26:02,220
all of these things
come into play

537
00:26:02,220 --> 00:26:05,760
when you start talking
about real interaction,

538
00:26:05,760 --> 00:26:07,080
even with an AI system.

539
00:26:07,080 --> 00:26:09,870
So how do you design robots
that can do that over repeated

540
00:26:09,870 --> 00:26:11,620
"encountered over long
term" interactions.

541
00:26:11,620 --> 00:26:13,037
So, again, this
is just giving you

542
00:26:13,037 --> 00:26:16,980
a sense of the broad field
of play and advancements

543
00:26:16,980 --> 00:26:19,250
we're trying to make
in the big picture.

544
00:26:19,250 --> 00:26:24,390
OK, so given all of that, again,
I teed up-- what our group is--

545
00:26:24,390 --> 00:26:28,080
our vision of how can we design
these technologies to help

546
00:26:28,080 --> 00:26:30,590
people to succeed,
grow, and flourish.

547
00:26:30,590 --> 00:26:34,930
So a lot of our work is
inspired by Positive Psychology.

548
00:26:34,930 --> 00:26:37,140
This is a diagram
from Martin Siegelman.

549
00:26:37,140 --> 00:26:40,860
And it's just to say, when you
talk about human flourishing,

550
00:26:40,860 --> 00:26:43,890
these are the kinds of
qualities you have to address.

551
00:26:43,890 --> 00:26:46,322
And these are profoundly
social and emotional.

552
00:26:46,322 --> 00:26:48,780
They're not just cognitive
decision-making processes, which

553
00:26:48,780 --> 00:26:51,190
has dominated the
conversation around AI, right?

554
00:26:51,190 --> 00:26:54,360
So it's about engagement,
relationships, well-being,

555
00:26:54,360 --> 00:26:57,803
meaning, positive emotion,
a sense of achievement.

556
00:26:57,803 --> 00:26:59,220
That is not the
words we typically

557
00:26:59,220 --> 00:27:01,020
talk about when we
talk about AI systems

558
00:27:01,020 --> 00:27:03,570
or typically talking about
efficiency and saving money

559
00:27:03,570 --> 00:27:05,127
and making money.

560
00:27:05,127 --> 00:27:06,960
These are the things
that we need to address

561
00:27:06,960 --> 00:27:08,723
to promote human flourishing.

562
00:27:08,723 --> 00:27:10,890
So, again, this just to say
this is a different lens

563
00:27:10,890 --> 00:27:12,480
upon which we look
at our work and why

564
00:27:12,480 --> 00:27:15,320
we build the systems we do.

565
00:27:15,320 --> 00:27:18,530
So, again, in the
big picture then,

566
00:27:18,530 --> 00:27:20,870
this humanized
engagement is not just

567
00:27:20,870 --> 00:27:24,500
about having a really good
experience with the AI

568
00:27:24,500 --> 00:27:26,840
technology, say, it's a robot.

569
00:27:26,840 --> 00:27:29,450
But if you can achieve this
deeper human engagement,

570
00:27:29,450 --> 00:27:31,550
you can actually enable
transformative change

571
00:27:31,550 --> 00:27:32,340
in people's lives.

572
00:27:32,340 --> 00:27:34,170
So what do I mean by
transformative change?

573
00:27:34,170 --> 00:27:36,830
Well, if somebody
can actually learn

574
00:27:36,830 --> 00:27:39,740
new skills or mindsets
or capabilities,

575
00:27:39,740 --> 00:27:40,860
they are transformed.

576
00:27:40,860 --> 00:27:42,900
They are able to do things
they couldn't before.

577
00:27:42,900 --> 00:27:44,540
They now have access
to opportunities

578
00:27:44,540 --> 00:27:45,540
they didn't have before.

579
00:27:45,540 --> 00:27:48,440
That's an example for me
of transformative change

580
00:27:48,440 --> 00:27:49,500
in health care.

581
00:27:49,500 --> 00:27:52,880
If you're better able to manage
a chronic condition-- if you're

582
00:27:52,880 --> 00:27:56,240
better able to be emotionally
resilient in times of stress,

583
00:27:56,240 --> 00:27:57,410
you are transformed.

584
00:27:57,410 --> 00:27:58,700
And you are going to
be able to do things

585
00:27:58,700 --> 00:28:01,367
you were not going to be able to
do before you were transformed.

586
00:28:01,367 --> 00:28:02,990
So these are the
things that we try

587
00:28:02,990 --> 00:28:04,940
to look at in our
group as case studies

588
00:28:04,940 --> 00:28:08,480
and as contexts to
try to demonstrate,

589
00:28:08,480 --> 00:28:13,360
in advance, our work with actual
people in real-world situations.

590
00:28:13,360 --> 00:28:16,580
So we do a lot of work
across different domains.

591
00:28:16,580 --> 00:28:20,643
I'm going to focus on our
particular work on education.

592
00:28:20,643 --> 00:28:22,060
And so, in this
section, I'm going

593
00:28:22,060 --> 00:28:25,600
to talk about our
personalized-learning-companion

594
00:28:25,600 --> 00:28:26,530
work.

595
00:28:26,530 --> 00:28:29,080
A lot of that work
is actually focused

596
00:28:29,080 --> 00:28:33,130
on early childhood
education, so young children,

597
00:28:33,130 --> 00:28:36,880
kindergarten-type age,
often in school settings.

598
00:28:36,880 --> 00:28:38,510
So why does this matter?

599
00:28:38,510 --> 00:28:42,520
So in the United States, we
do not have a "universal early

600
00:28:42,520 --> 00:28:43,703
childhood education" system.

601
00:28:43,703 --> 00:28:44,870
We're trying to change that.

602
00:28:44,870 --> 00:28:47,150
But, historically,
we have not had one.

603
00:28:47,150 --> 00:28:49,360
And because of
that, there's a lot

604
00:28:49,360 --> 00:28:53,660
of inequity in children's
readiness, even in kindergarten.

605
00:28:53,660 --> 00:28:56,020
And there's a lot of data
and studies that have shown

606
00:28:56,020 --> 00:28:58,510
if children cannot even start
kindergarten ready to learn,

607
00:28:58,510 --> 00:29:00,800
it is very difficult
to catch up.

608
00:29:00,800 --> 00:29:03,430
And there's a lot of
down-the-road consequences

609
00:29:03,430 --> 00:29:04,040
of that.

610
00:29:04,040 --> 00:29:07,490
It can be extremely expensive
to try to catch children up.

611
00:29:07,490 --> 00:29:09,400
So just to say, in
the United States,

612
00:29:09,400 --> 00:29:14,830
there is a big societal
need to improve equity

613
00:29:14,830 --> 00:29:17,990
in the quality of
early-childhood education.

614
00:29:17,990 --> 00:29:20,120
So it's a serious challenge.

615
00:29:20,120 --> 00:29:23,360
It's a serious problem that we
face here in the United States.

616
00:29:23,360 --> 00:29:26,440
But I'm sure worldwide,
post pandemic,

617
00:29:26,440 --> 00:29:30,610
we're all seeing the impacts
of this on children's education

618
00:29:30,610 --> 00:29:31,700
of all grades.

619
00:29:31,700 --> 00:29:34,650
So just to say,
this question of how

620
00:29:34,650 --> 00:29:36,330
we can create
technologies that help

621
00:29:36,330 --> 00:29:42,090
to enhance, to help children
learn in ways that allow them

622
00:29:42,090 --> 00:29:44,588
to be at grade-appropriate
level, right now

623
00:29:44,588 --> 00:29:46,380
is of particular
importance because there's

624
00:29:46,380 --> 00:29:48,930
a lot of kids that are not
where they would have been

625
00:29:48,930 --> 00:29:50,680
if we didn't have the pandemic.

626
00:29:50,680 --> 00:29:54,360
So, anyway, there's just a lot
of global societal need, which

627
00:29:54,360 --> 00:29:58,600
I think is why, again,
this idea of AI tutors

628
00:29:58,600 --> 00:29:59,870
has been around for decades.

629
00:29:59,870 --> 00:30:02,080
It's been around a long time.

630
00:30:02,080 --> 00:30:04,970
And with every new technology,
we try to advance them.

631
00:30:04,970 --> 00:30:06,850
This is just the
context upon which

632
00:30:06,850 --> 00:30:09,910
I think we are really
seeing the next wave of why

633
00:30:09,910 --> 00:30:12,250
having these personalized
learning technologies

634
00:30:12,250 --> 00:30:14,710
are important for
kids today, beyond all

635
00:30:14,710 --> 00:30:16,160
of the other
traditional reasons.

636
00:30:16,160 --> 00:30:20,620
So when we talk about
AI-supported learning now,

637
00:30:20,620 --> 00:30:23,560
we're talking about
physically-embodied,

638
00:30:23,560 --> 00:30:26,300
socially-embodied robots.

639
00:30:26,300 --> 00:30:28,220
Now, what are the
advantages of that?

640
00:30:28,220 --> 00:30:29,710
We just heard about Cognigo.

641
00:30:29,710 --> 00:30:32,260
And Bill was talking
about this concern

642
00:30:32,260 --> 00:30:35,920
of social isolation
between students

643
00:30:35,920 --> 00:30:38,380
to other students,
students to teachers.

644
00:30:38,380 --> 00:30:39,550
That is a real risk.

645
00:30:39,550 --> 00:30:41,890
A lot of these systems are
really designed like Cognigo

646
00:30:41,890 --> 00:30:44,440
for one-to-one, text-based
interaction between a student

647
00:30:44,440 --> 00:30:45,430
and the screen.

648
00:30:45,430 --> 00:30:46,110
Right?

649
00:30:46,110 --> 00:30:48,700
What we're trying
to do is create

650
00:30:48,700 --> 00:30:50,320
learning companion
technologies that

651
00:30:50,320 --> 00:30:52,720
are integrated into the
human-social environment

652
00:30:52,720 --> 00:30:53,680
of learning.

653
00:30:53,680 --> 00:30:55,510
Learning is profoundly social.

654
00:30:55,510 --> 00:30:57,010
That is one thing
we have definitely

655
00:30:57,010 --> 00:30:59,930
learned from all of our great
educational technologies.

656
00:30:59,930 --> 00:31:01,655
Learning is profoundly social.

657
00:31:01,655 --> 00:31:04,280
It's important that people learn
in the context of other people

658
00:31:04,280 --> 00:31:05,820
and human teachers.

659
00:31:05,820 --> 00:31:09,330
So that is table stakes for
what we're trying to design.

660
00:31:09,330 --> 00:31:12,110
So in this photo here,
you see a social robot

661
00:31:12,110 --> 00:31:13,940
that because it's
physically embodied,

662
00:31:13,940 --> 00:31:16,517
it can actually interact
with a group of people.

663
00:31:16,517 --> 00:31:18,350
It can interact with
the main child learner.

664
00:31:18,350 --> 00:31:19,920
It can turn and
talk to the teacher.

665
00:31:19,920 --> 00:31:21,860
There's a little
girl friend looking--

666
00:31:21,860 --> 00:31:24,350
imagine that fluid
interaction in

667
00:31:24,350 --> 00:31:28,560
a physically-socially-situated
environment, like a classroom.

668
00:31:28,560 --> 00:31:30,270
That is what we're trying to do.

669
00:31:30,270 --> 00:31:32,820
The robot then is
designed to try

670
00:31:32,820 --> 00:31:34,710
to help facilitate
this interaction,

671
00:31:34,710 --> 00:31:37,030
to promote, obviously,
the children's learning.

672
00:31:37,030 --> 00:31:40,000
A lot of our context is around
early language and literacy

673
00:31:40,000 --> 00:31:42,000
skills because we know
that's critical for a lot

674
00:31:42,000 --> 00:31:44,100
of future academic success.

675
00:31:44,100 --> 00:31:47,887
It is designed not to be a tutor
but to be a practice partner.

676
00:31:47,887 --> 00:31:49,720
That's what we call it
a learning companion.

677
00:31:49,720 --> 00:31:51,990
It's like a really
fun playmate that

678
00:31:51,990 --> 00:31:53,790
plays games,
educational-learning games

679
00:31:53,790 --> 00:31:55,860
with kids that
reinforces and gives them

680
00:31:55,860 --> 00:31:58,670
additional practice to what the
teacher is actually teaching.

681
00:31:58,670 --> 00:32:00,670
So we're not trying to
compete with the teacher.

682
00:32:00,670 --> 00:32:02,830
We're trying to supplement
what the teacher does.

683
00:32:02,830 --> 00:32:05,400
And, again, why did we
choose early childhood

684
00:32:05,400 --> 00:32:07,210
beyond the societal challenge?

685
00:32:07,210 --> 00:32:10,020
It is a real litmus test
because young children

686
00:32:10,020 --> 00:32:12,360
aren't going to learn by
typing text and so forth.

687
00:32:12,360 --> 00:32:15,150
They interact through play and
"real time social emotional"

688
00:32:15,150 --> 00:32:15,730
interaction.

689
00:32:15,730 --> 00:32:17,760
If you can design a
technology that can actually

690
00:32:17,760 --> 00:32:20,700
engage a young child
in an authentic way,

691
00:32:20,700 --> 00:32:23,190
your algorithms are doing
something right, right?

692
00:32:23,190 --> 00:32:26,040
So it is a really acid
test, I would say,

693
00:32:26,040 --> 00:32:28,560
as if our algorithms
and our advancements

694
00:32:28,560 --> 00:32:31,030
are actually doing
something meaningful

695
00:32:31,030 --> 00:32:34,380
and advancing the
ball, so to speak.

696
00:32:34,380 --> 00:32:37,190
We are modeling
peer-to-peer interactions.

697
00:32:37,190 --> 00:32:40,770
So the robot interacts more
like a peer-like, maybe slightly

698
00:32:40,770 --> 00:32:45,060
more advanced learner than,
again, a human teacher.

699
00:32:45,060 --> 00:32:48,630
And we do a lot of
personalization under the hood.

700
00:32:48,630 --> 00:32:51,223
So we use a lot of reinforcement
learning to do that.

701
00:32:51,223 --> 00:32:52,890
I'm sure we've heard
about generative AI

702
00:32:52,890 --> 00:32:54,630
and how you can customize pumps.

703
00:32:54,630 --> 00:32:56,370
A lot of our work
is about the robot

704
00:32:56,370 --> 00:32:58,770
through real-time
interaction through how

705
00:32:58,770 --> 00:33:01,690
the child expresses what they
say, what they do in the game.

706
00:33:01,690 --> 00:33:04,620
The robot is trying to learn
what we call "a policy of what

707
00:33:04,620 --> 00:33:06,600
the most optimal
next action is,"

708
00:33:06,600 --> 00:33:09,060
to help achieve the goals of
keeping that child engaged

709
00:33:09,060 --> 00:33:10,630
and promoting that
child's learning.

710
00:33:10,630 --> 00:33:12,005
So that's
reinforcement learning.

711
00:33:12,005 --> 00:33:14,320
We do a lot of reinforcement
learning in this.

712
00:33:14,320 --> 00:33:16,280
We don't just do
this in a vacuum.

713
00:33:16,280 --> 00:33:20,750
We engage teachers and students
and stakeholders in co-design.

714
00:33:20,750 --> 00:33:22,270
And this is really,
really important

715
00:33:22,270 --> 00:33:24,370
because whatever we
design has to fit, again,

716
00:33:24,370 --> 00:33:26,090
in the human environment.

717
00:33:26,090 --> 00:33:28,870
If it's not solving a
problem that the teachers

718
00:33:28,870 --> 00:33:30,490
or the students
are really wanting,

719
00:33:30,490 --> 00:33:32,140
they're not going to use it.

720
00:33:32,140 --> 00:33:34,640
And we've seen that story in
technology again and again.

721
00:33:34,640 --> 00:33:36,560
So we do a lot of co-design.

722
00:33:36,560 --> 00:33:40,660
We are innovating at this
intersection of psychology

723
00:33:40,660 --> 00:33:43,420
and engagement, so
understanding real people,

724
00:33:43,420 --> 00:33:45,050
what they do in this context.

725
00:33:45,050 --> 00:33:47,830
And with a technology
like a social robot,

726
00:33:47,830 --> 00:33:50,020
we do a lot of
human-experience design

727
00:33:50,020 --> 00:33:52,150
to make sure that these
games of these interactions

728
00:33:52,150 --> 00:33:55,690
are not only effective but
delightful and engaging,

729
00:33:55,690 --> 00:33:57,920
emotionally, socially
engaging for children.

730
00:33:57,920 --> 00:34:00,138
And that is what is
defining and informing

731
00:34:00,138 --> 00:34:01,930
what are the computational
challenges we're

732
00:34:01,930 --> 00:34:03,020
trying to design.

733
00:34:03,020 --> 00:34:04,930
We don't just say we want to
design a new machine learning

734
00:34:04,930 --> 00:34:05,540
algorithm.

735
00:34:05,540 --> 00:34:07,471
We ask the question,
what do people need?

736
00:34:07,471 --> 00:34:08,929
What does it have
to be able to do?

737
00:34:08,929 --> 00:34:10,221
What advancements are required?

738
00:34:10,221 --> 00:34:12,489
And that is how we frame
the technical challenges

739
00:34:12,489 --> 00:34:13,310
of the algorithms.

740
00:34:13,310 --> 00:34:15,020
We try to innovate.

741
00:34:15,020 --> 00:34:19,350
When you think about
AI-enabled personalization,

742
00:34:19,350 --> 00:34:22,159
in some sense, that becomes part
of the design cycle as well,

743
00:34:22,159 --> 00:34:24,110
because whatever
we design is going

744
00:34:24,110 --> 00:34:26,239
to be adapted based on
this real-time interaction

745
00:34:26,239 --> 00:34:27,139
with learners.

746
00:34:27,139 --> 00:34:29,900
And then, again, that
is a constant monitoring

747
00:34:29,900 --> 00:34:32,639
and redesigning with the
stakeholders as well.

748
00:34:32,639 --> 00:34:34,429
So this is just
to say, co-design.

749
00:34:34,429 --> 00:34:37,843
With AI, in this context,
is part of the process

750
00:34:37,843 --> 00:34:38,760
we have to go through.

751
00:34:38,760 --> 00:34:40,628
There's really no
cutting corners here.

752
00:34:40,628 --> 00:34:42,920
There's no just throwing in
data and get something out.

753
00:34:42,920 --> 00:34:45,745
It is a constant process of
trying to keep yourself honest

754
00:34:45,745 --> 00:34:47,120
and making sure
you're delivering

755
00:34:47,120 --> 00:34:49,100
something of value
that is working

756
00:34:49,100 --> 00:34:50,400
in the authentic context.

757
00:34:50,400 --> 00:34:52,340
And we learn a lot
through this process.

758
00:34:52,340 --> 00:34:55,940
And we're able to make really
compelling advances to AI

759
00:34:55,940 --> 00:34:57,750
when we do this process.

760
00:34:57,750 --> 00:35:00,410
So I'm going to show
a series of videos

761
00:35:00,410 --> 00:35:05,190
just so that you can see a
little bit of this in action.

762
00:35:05,190 --> 00:35:08,550
So, again, we're talking
about peer-to-peer learning.

763
00:35:08,550 --> 00:35:09,968
So how do we model a robot?

764
00:35:09,968 --> 00:35:12,260
How do we create these models
that a robot can actually

765
00:35:12,260 --> 00:35:14,220
engage in this interaction?

766
00:35:14,220 --> 00:35:16,410
So one of the first
steps that we did--

767
00:35:16,410 --> 00:35:19,470
and this was a few years ago--
was we brought in a lot of kids.

768
00:35:19,470 --> 00:35:23,695
And we recorded how they engage
in a story-sharing interaction,

769
00:35:23,695 --> 00:35:25,320
because a lot of what
we have the robot

770
00:35:25,320 --> 00:35:29,010
do with the child is a
dialogic-story-sharing

771
00:35:29,010 --> 00:35:30,220
interaction.

772
00:35:30,220 --> 00:35:32,770
So we record these things.

773
00:35:32,770 --> 00:35:35,850
We then start to train new
models based on the capability.

774
00:35:35,850 --> 00:35:37,920
And then we
basically put a robot

775
00:35:37,920 --> 00:35:39,750
in the context of
one of these children

776
00:35:39,750 --> 00:35:42,960
and compare what does that
behavior look like when now it's

777
00:35:42,960 --> 00:35:44,460
a child interacting
with a robot,

778
00:35:44,460 --> 00:35:47,010
versus a child interacting
with another child.

779
00:35:47,010 --> 00:35:49,343
How close are we to
capturing that dynamic?

780
00:35:49,343 --> 00:35:50,010
[VIDEO PLAYBACK]

781
00:35:50,010 --> 00:35:54,750
- So the cat broke the window.

782
00:35:54,750 --> 00:35:58,943
And then it ran to
school and peeked inside.

783
00:35:58,943 --> 00:36:01,493

784
00:36:01,493 --> 00:36:02,910
- And then they
saw a caterpillar.

785
00:36:02,910 --> 00:36:04,586
- And then some classmates--

786
00:36:04,586 --> 00:36:07,756
[INTERPOSING VOICES]

787
00:36:07,756 --> 00:36:10,243

788
00:36:10,243 --> 00:36:12,660
CYNTHIA BREAZEAL: So a lot of
affective computing-- again,

789
00:36:12,660 --> 00:36:14,820
this is other
machine-learning models

790
00:36:14,820 --> 00:36:18,610
to track states and sequences
of interaction patterns.

791
00:36:18,610 --> 00:36:19,830
- And they wanted to go in--

792
00:36:19,830 --> 00:36:22,205
CYNTHIA BREAZEAL: And then we
put the model in the robot.

793
00:36:22,205 --> 00:36:24,810
And we bring in-- and, again,
a whole other group of children

794
00:36:24,810 --> 00:36:27,510
are now engaging
in this interaction

795
00:36:27,510 --> 00:36:29,910
and looking at the
social cues of the robot,

796
00:36:29,910 --> 00:36:32,430
looking at the child's
non-verbal and verbal behavior,

797
00:36:32,430 --> 00:36:36,420
to try to have it engage as an
interested, supportive-learning

798
00:36:36,420 --> 00:36:38,340
companion.

799
00:36:38,340 --> 00:36:42,180
So that gives you a sense of
how we endow these social skills

800
00:36:42,180 --> 00:36:43,030
into the robot.

801
00:36:43,030 --> 00:36:45,780
This next video is
the bigger picture

802
00:36:45,780 --> 00:36:49,260
of starting to try to learn
policies, adaptive policies,

803
00:36:49,260 --> 00:36:52,800
to personalize
interactions with children.

804
00:36:52,800 --> 00:36:54,480
So, again,
reinforcement learning,

805
00:36:54,480 --> 00:36:57,330
a different kind of machine
learning to learn, again,

806
00:36:57,330 --> 00:36:58,780
personalized policies.

807
00:36:58,780 --> 00:37:02,070
- Hi, my name is Tega.

808
00:37:02,070 --> 00:37:03,720
Nice to meet you.

809
00:37:03,720 --> 00:37:06,353
What is your name?

810
00:37:06,353 --> 00:37:06,853
- Aliana.

811
00:37:06,853 --> 00:37:07,830

812
00:37:07,830 --> 00:37:09,900
- Early literacy
skills form the basis

813
00:37:09,900 --> 00:37:12,720
of a large part of
children's future learning.

814
00:37:12,720 --> 00:37:15,390
But children entering
preschool and kindergarten

815
00:37:15,390 --> 00:37:18,090
come from linguistically
diverse backgrounds, affected

816
00:37:18,090 --> 00:37:21,150
by many factors, including
their family's cultural origin

817
00:37:21,150 --> 00:37:23,310
and socioeconomic status.

818
00:37:23,310 --> 00:37:26,700
Therefore, it's important to
create personalized solutions

819
00:37:26,700 --> 00:37:29,190
for promoting early literacy.

820
00:37:29,190 --> 00:37:32,760
- What was your
favorite story so far?

821
00:37:32,760 --> 00:37:34,620
- My favorite story?

822
00:37:34,620 --> 00:37:39,450
It was the bunny one.

823
00:37:39,450 --> 00:37:42,360
- Using a social robot
as an embodied AI agent

824
00:37:42,360 --> 00:37:44,190
provides several advantages.

825
00:37:44,190 --> 00:37:47,700
Studies show that early literacy
development requires exposure

826
00:37:47,700 --> 00:37:50,460
to a rich variety of spoken
language and vocabulary

827
00:37:50,460 --> 00:37:52,670
during social
interaction with others.

828
00:37:52,670 --> 00:37:54,550
Children engage
with social robots

829
00:37:54,550 --> 00:37:56,560
in an emotive and
relational way.

830
00:37:56,560 --> 00:37:58,630
And the affective cues
that social robots

831
00:37:58,630 --> 00:38:01,510
can elicit from children
provide a better opportunity

832
00:38:01,510 --> 00:38:04,210
to assess their
engagement states.

833
00:38:04,210 --> 00:38:07,090
Our reinforcement learning
personalization policy

834
00:38:07,090 --> 00:38:09,300
takes full advantage of
these engagement cues.

835
00:38:09,300 --> 00:38:12,720

836
00:38:12,720 --> 00:38:15,630
The robot's goal is to
learn an action policy that

837
00:38:15,630 --> 00:38:17,580
maximizes the chance
for each child

838
00:38:17,580 --> 00:38:20,370
to learn new linguistic
knowledge while also promoting

839
00:38:20,370 --> 00:38:21,750
engagement.

840
00:38:21,750 --> 00:38:24,150
During the robot's
storytelling, the robot

841
00:38:24,150 --> 00:38:25,800
models the child's
listening behavior

842
00:38:25,800 --> 00:38:28,320
based on several
features, including

843
00:38:28,320 --> 00:38:31,380
whether the child responds
to the robot's question--

844
00:38:31,380 --> 00:38:36,510
- What would you see if you
went to a rainforest someday?

845
00:38:36,510 --> 00:38:37,590
- Lions.

846
00:38:37,590 --> 00:38:40,440
- --the length of the response,
and also the child's facial

847
00:38:40,440 --> 00:38:43,350
affect cues during
the robot's story.

848
00:38:43,350 --> 00:38:47,160
The robot's action space is the
lexical and syntactic complexity

849
00:38:47,160 --> 00:38:49,230
of a sentence in the storybook.

850
00:38:49,230 --> 00:38:51,630
The reward function
is a weighted sum

851
00:38:51,630 --> 00:38:53,370
of engagement and learning.

852
00:38:53,370 --> 00:38:57,150
- The frog went out of his pond.

853
00:38:57,150 --> 00:38:58,680
- The reward
function was designed

854
00:38:58,680 --> 00:39:02,160
to reward new lexical and
syntactic learning but balancing

855
00:39:02,160 --> 00:39:04,890
this with engagement so that
the algorithm doesn't always

856
00:39:04,890 --> 00:39:07,920
select a story with the
highest linguistic complexity.

857
00:39:07,920 --> 00:39:09,900
Over the course of
three months, we

858
00:39:09,900 --> 00:39:12,820
studied 67 bilingual and
English-language-learning

859
00:39:12,820 --> 00:39:15,940
students from 12 local
kindergarten classrooms.

860
00:39:15,940 --> 00:39:18,810
Students were divided into
three balanced groups.

861
00:39:18,810 --> 00:39:21,100
The personalized and
non-personalized groups

862
00:39:21,100 --> 00:39:23,110
interacted with our
social robot, Tega,

863
00:39:23,110 --> 00:39:25,060
in a fully autonomous
interaction,

864
00:39:25,060 --> 00:39:28,070
in which the robot and child
took turns telling stories.

865
00:39:28,070 --> 00:39:30,640
The robot either
personalized its story choice

866
00:39:30,640 --> 00:39:32,870
or selected from a
fixed curriculum.

867
00:39:32,870 --> 00:39:35,020
Children in the
baseline group only

868
00:39:35,020 --> 00:39:39,250
participated in pre and post
tests without robot interaction.

869
00:39:39,250 --> 00:39:41,770
We found that the
story-selection policy did

870
00:39:41,770 --> 00:39:43,780
personalize to each
child over time,

871
00:39:43,780 --> 00:39:46,990
shown by the root-mean-square
distance from the initial policy

872
00:39:46,990 --> 00:39:50,470
and the rate of "matching
maximum reward state action"

873
00:39:50,470 --> 00:39:52,910
pairs in the Q table
between two policies.

874
00:39:52,910 --> 00:39:55,480
Children who interacted
with a personalized robot

875
00:39:55,480 --> 00:39:58,210
learned and used significantly
more target words

876
00:39:58,210 --> 00:40:01,480
compared to the non-personalized
and baseline groups.

877
00:40:01,480 --> 00:40:04,300
We also found that children's
engagement cues are

878
00:40:04,300 --> 00:40:06,430
significantly correlated
and that this trend

879
00:40:06,430 --> 00:40:09,040
was driven by the
personalization condition.

880
00:40:09,040 --> 00:40:13,570
- At the crest of the hill,
stood a bear blinking blearily

881
00:40:13,570 --> 00:40:16,640
outside his cave.

882
00:40:16,640 --> 00:40:23,330
- They went to the crest of the
hill and found a bear standing.

883
00:40:23,330 --> 00:40:24,630
- And he saw a bear.

884
00:40:24,630 --> 00:40:25,985
And he was so cared.

885
00:40:25,985 --> 00:40:26,485
So--

886
00:40:26,485 --> 00:40:29,900

887
00:40:29,900 --> 00:40:31,970
- Children in the
personalized condition

888
00:40:31,970 --> 00:40:34,820
attempted to answer the
robot's questions more often.

889
00:40:34,820 --> 00:40:36,590
And this tendency
was also reflected

890
00:40:36,590 --> 00:40:38,140
in their affective arousal cues.

891
00:40:38,140 --> 00:40:40,700

892
00:40:40,700 --> 00:40:43,310
Analyzing children's
electrodermal activity and body

893
00:40:43,310 --> 00:40:46,190
pose revealed that children
in the personalized condition

894
00:40:46,190 --> 00:40:48,770
showed significantly higher
attention and engagement

895
00:40:48,770 --> 00:40:50,075
levels throughout the sessions.

896
00:40:50,075 --> 00:40:53,312
[MUSIC PLAYING]

897
00:40:53,312 --> 00:40:56,150

898
00:40:56,150 --> 00:40:58,760
With a relatable-social-robot
companion,

899
00:40:58,760 --> 00:41:01,010
we have demonstrated that
when we match young students

900
00:41:01,010 --> 00:41:03,680
with a learning partner that
can grow and engage with them,

901
00:41:03,680 --> 00:41:06,117
those students flourish.

902
00:41:06,117 --> 00:41:06,950
[END VIDEO PLAYBACK]

903
00:41:06,950 --> 00:41:08,510
CYNTHIA BREAZEAL:
OK, so punch line

904
00:41:08,510 --> 00:41:11,270
is, the personalized
robot seemed

905
00:41:11,270 --> 00:41:14,060
to be even more engaging
and able to help

906
00:41:14,060 --> 00:41:16,740
support children learning than
the non-personalized robot.

907
00:41:16,740 --> 00:41:19,220
But I can tell you, even
both of those robots

908
00:41:19,220 --> 00:41:21,838
gave children's gain over
baseline classroom instruction.

909
00:41:21,838 --> 00:41:23,630
So even just having
the presence of a robot

910
00:41:23,630 --> 00:41:26,420
to help practice and
reinforce these behaviors

911
00:41:26,420 --> 00:41:28,240
was beneficial to children.

912
00:41:28,240 --> 00:41:31,567
So this was a result that
was shown on the slide.

913
00:41:31,567 --> 00:41:33,150
Some other things
that we've looked at

914
00:41:33,150 --> 00:41:35,400
is, what about the
quality of relationship

915
00:41:35,400 --> 00:41:37,830
that children experience
with the robot?

916
00:41:37,830 --> 00:41:39,760
And how does that
affect their learning?

917
00:41:39,760 --> 00:41:42,150
So we know that when children
have a better relationship

918
00:41:42,150 --> 00:41:44,370
with their teacher, it is often
correlated with higher learning

919
00:41:44,370 --> 00:41:45,170
outcomes.

920
00:41:45,170 --> 00:41:48,280
We are starting to see that
also with these AI systems.

921
00:41:48,280 --> 00:41:50,040
So we looked at
measures of measuring

922
00:41:50,040 --> 00:41:52,830
interpersonal relationship
in human-human interactions.

923
00:41:52,830 --> 00:41:55,060
We adapted that to a
human-robot context.

924
00:41:55,060 --> 00:41:56,853
We are seeing the same trends.

925
00:41:56,853 --> 00:41:59,520
So children who report having a
closer relationship to the robot

926
00:41:59,520 --> 00:42:01,930
tends to be correlated
with higher learning gains.

927
00:42:01,930 --> 00:42:03,420
And when we add in
personalization,

928
00:42:03,420 --> 00:42:05,798
that correlation
is even stronger.

929
00:42:05,798 --> 00:42:07,590
So we're starting to
see these interactions

930
00:42:07,590 --> 00:42:12,680
between personalization,
relationship, and learning.

931
00:42:12,680 --> 00:42:15,900

932
00:42:15,900 --> 00:42:21,780
You saw the video of benefits
of the practice partner.

933
00:42:21,780 --> 00:42:25,260
We have also been bringing these
technologies into home settings

934
00:42:25,260 --> 00:42:27,780
to try to foster
parent-child-robot triadic

935
00:42:27,780 --> 00:42:30,270
interactions to enrich the
dialogue between parents

936
00:42:30,270 --> 00:42:31,178
and children.

937
00:42:31,178 --> 00:42:32,970
So, again, bringing
that social interaction

938
00:42:32,970 --> 00:42:35,400
beyond the classroom
to the home--

939
00:42:35,400 --> 00:42:39,180
and so the punch line here is,
there's a lot of assumption

940
00:42:39,180 --> 00:42:42,670
that a human-human relationship
is what we should be going for.

941
00:42:42,670 --> 00:42:45,880
And this work is showing it's
not necessarily the case.

942
00:42:45,880 --> 00:42:47,230
It really just depends.

943
00:42:47,230 --> 00:42:51,720
So one of the things that's so
appealing and interesting about

944
00:42:51,720 --> 00:42:54,210
the social robot we've
designed is, yes,

945
00:42:54,210 --> 00:42:57,540
there are absolutely attributes
of a "human motivating

946
00:42:57,540 --> 00:43:00,460
supportive" ally,
learning with a friend.

947
00:43:00,460 --> 00:43:04,170
It's also a cool
high-tech tool that

948
00:43:04,170 --> 00:43:06,010
can access all
kinds of information

949
00:43:06,010 --> 00:43:08,898
and so forth from the
internet, et cetera, et cetera.

950
00:43:08,898 --> 00:43:10,440
But there are these
qualities, as I'm

951
00:43:10,440 --> 00:43:15,600
sure you saw in these videos, of
this non-judgmental, attentive--

952
00:43:15,600 --> 00:43:17,710
almost companion animal.

953
00:43:17,710 --> 00:43:20,397
And that creates a safe
context for learning,

954
00:43:20,397 --> 00:43:22,230
whereas a child might
be embarrassed to make

955
00:43:22,230 --> 00:43:24,900
a mistake in front of
a friend or a teacher.

956
00:43:24,900 --> 00:43:28,900
They don't seem to feel that
way in front of a social robot.

957
00:43:28,900 --> 00:43:32,310
So they're able to try new
things, take safe learning

958
00:43:32,310 --> 00:43:35,770
risks, which, of course,
benefits their learning.

959
00:43:35,770 --> 00:43:38,780
So, again, there's a different
relationship that's possible,

960
00:43:38,780 --> 00:43:41,910
where they can have actual
different kinds of benefits that

961
00:43:41,910 --> 00:43:44,563
enhance and supplement
the peer-to-peer learning

962
00:43:44,563 --> 00:43:46,230
and the teacher-student
learning that we

963
00:43:46,230 --> 00:43:47,890
have in our educational system.

964
00:43:47,890 --> 00:43:50,140
So I've talked a lot about
reinforcement learning.

965
00:43:50,140 --> 00:43:51,723
You're like, what
about generative AI?

966
00:43:51,723 --> 00:43:53,850
So, yes, of course we're
bringing generative AI

967
00:43:53,850 --> 00:43:55,710
into our systems as well.

968
00:43:55,710 --> 00:43:58,960
You can see we do a lot
of dialogic storytelling.

969
00:43:58,960 --> 00:44:00,990
The generative AI
models are allowing

970
00:44:00,990 --> 00:44:04,620
us to be much more flexible
and expansive in the kinds

971
00:44:04,620 --> 00:44:08,130
of conversations and
questions asked and answered

972
00:44:08,130 --> 00:44:11,210
that the robot is capable
of engaging children in.

973
00:44:11,210 --> 00:44:13,800
We were much more
limited and constrained

974
00:44:13,800 --> 00:44:15,150
before generative AI.

975
00:44:15,150 --> 00:44:16,567
So I'm just going
to show one more

976
00:44:16,567 --> 00:44:21,840
video of this example of now
having a dialogue around one

977
00:44:21,840 --> 00:44:24,927
of the stories in the
library of storybooks that's

978
00:44:24,927 --> 00:44:26,260
in our educational intervention.

979
00:44:26,260 --> 00:44:26,940
[VIDEO PLAYBACK]

980
00:44:26,940 --> 00:44:28,290
- What was the doll made out of?

981
00:44:28,290 --> 00:44:29,915
CYNTHIA BREAZEAL:
There's a story here.

982
00:44:29,915 --> 00:44:32,530

983
00:44:32,530 --> 00:44:33,560
- I'm not sure.

984
00:44:33,560 --> 00:44:35,768
I think I remember
something with beads.

985
00:44:35,768 --> 00:44:36,810
Can you tell me the rest?

986
00:44:36,810 --> 00:44:39,980

987
00:44:39,980 --> 00:44:41,440
- You're right about the beads.

988
00:44:41,440 --> 00:44:42,650
They are part of the
doll's decorations.

989
00:44:42,650 --> 00:44:44,330
CYNTHIA BREAZEAL:
So giving hints.

990
00:44:44,330 --> 00:44:46,702
- But let's focus on what
the whole doll is made from.

991
00:44:46,702 --> 00:44:48,785
CYNTHIA BREAZEAL: Being
able to adjust the story--

992
00:44:48,785 --> 00:44:52,600
- It's a type of soft material
that comes from an animal.

993
00:44:52,600 --> 00:44:54,020
Can you guess which one?

994
00:44:54,020 --> 00:44:55,240
CYNTHIA BREAZEAL: Again,
this is a generated hint.

995
00:44:55,240 --> 00:44:56,320
This is pretty good.

996
00:44:56,320 --> 00:44:57,480
- It said buckskin.

997
00:44:57,480 --> 00:44:58,980
But what animal
does that come from?

998
00:44:58,980 --> 00:45:01,520

999
00:45:01,520 --> 00:45:04,640
- Buckskin comes from a deer,
which is a kind of animal that

1000
00:45:04,640 --> 00:45:06,800
lives in the forest.

1001
00:45:06,800 --> 00:45:10,820
The doll was made from
the soft skin of a deer.

1002
00:45:10,820 --> 00:45:12,660
Let's start with the first pig.

1003
00:45:12,660 --> 00:45:16,910
Can you tell me what he
built his house out of?

1004
00:45:16,910 --> 00:45:19,550
- I think it's
made out of cakes.

1005
00:45:19,550 --> 00:45:21,330
- That sounds delicious.

1006
00:45:21,330 --> 00:45:23,780
Unfortunately, I don't
think that's right.

1007
00:45:23,780 --> 00:45:25,930
The first pig built
his house out of straw.

1008
00:45:25,930 --> 00:45:28,580

1009
00:45:28,580 --> 00:45:31,770
- What would you use
to build your house?

1010
00:45:31,770 --> 00:45:33,145
- That's an
interesting question.

1011
00:45:33,145 --> 00:45:34,212
CYNTHIA BREAZEAL: We're seeing
a mixed [INAUDIBLE] interaction.

1012
00:45:34,212 --> 00:45:36,212
- I think I'd build my
house out of solar panels

1013
00:45:36,212 --> 00:45:38,870
so that I can power my
home with renewable energy.

1014
00:45:38,870 --> 00:45:39,200
CYNTHIA BREAZEAL: And he's
like, renewable energy.

1015
00:45:39,200 --> 00:45:39,930
- Wow, that's very smart.

1016
00:45:39,930 --> 00:45:40,630
CYNTHIA BREAZEAL:
That's a great answer.

1017
00:45:40,630 --> 00:45:41,515
[LAUGHS]

1018
00:45:41,515 --> 00:45:42,390
SPEAKER 2: Thank you.

1019
00:45:42,390 --> 00:45:42,830
[END VIDEO PLAYBACK]

1020
00:45:42,830 --> 00:45:44,413
CYNTHIA BREAZEAL:
So, again, just much

1021
00:45:44,413 --> 00:45:47,660
more flexible, responsive,
open-ended bidirectional

1022
00:45:47,660 --> 00:45:49,560
interactions that's
powered by generative AI.

1023
00:45:49,560 --> 00:45:51,110
So, obviously,
this is allowing us

1024
00:45:51,110 --> 00:45:54,560
to approach this using new
tools and technologies that we

1025
00:45:54,560 --> 00:45:56,190
didn't have access beforehand.

1026
00:45:56,190 --> 00:45:58,040
And just before I
leave this section,

1027
00:45:58,040 --> 00:46:02,600
just to acknowledge
that education underlies

1028
00:46:02,600 --> 00:46:05,330
a lot of real world
applications across a lot

1029
00:46:05,330 --> 00:46:08,210
of different disciplines--
so when you talk about health

1030
00:46:08,210 --> 00:46:11,430
literacy, financial literacy,
coaching and training, behavior

1031
00:46:11,430 --> 00:46:15,540
change for health, at the crux
of a lot of these interventions

1032
00:46:15,540 --> 00:46:19,313
is helping people develop
new skills, new mindsets,

1033
00:46:19,313 --> 00:46:20,730
and new knowledge
so that they can

1034
00:46:20,730 --> 00:46:22,320
affect positive
change for themselves,

1035
00:46:22,320 --> 00:46:23,830
make better decisions.

1036
00:46:23,830 --> 00:46:27,450
So, again, learning is
so foundational to a lot

1037
00:46:27,450 --> 00:46:29,910
of these other domains,
where these agents could

1038
00:46:29,910 --> 00:46:32,550
have potentially a lot of use.

1039
00:46:32,550 --> 00:46:37,450
All right, so I'm going to end
this section on learning with AI

1040
00:46:37,450 --> 00:46:41,400
in this
social-emotional-human context

1041
00:46:41,400 --> 00:46:46,140
and now talk about how do we
help people become more AI

1042
00:46:46,140 --> 00:46:50,130
literate or even AI fluent
so that we are empowered

1043
00:46:50,130 --> 00:46:53,986
to create solutions to address
challenges in our communities?

1044
00:46:53,986 --> 00:46:56,670
So there's been a lot
of fear and concern

1045
00:46:56,670 --> 00:47:00,390
about AI replacing
people in jobs,

1046
00:47:00,390 --> 00:47:03,310
changing the landscape
of the job market,

1047
00:47:03,310 --> 00:47:04,420
et cetera, et cetera.

1048
00:47:04,420 --> 00:47:07,300
That is absolutely going
to happen to some extent.

1049
00:47:07,300 --> 00:47:09,240
But Erik Brynjolfsson
at Stanford

1050
00:47:09,240 --> 00:47:12,240
makes this point
that way more value

1051
00:47:12,240 --> 00:47:14,070
is going to be
unlocked when AI is

1052
00:47:14,070 --> 00:47:16,840
designed to help people create
something fundamentally new.

1053
00:47:16,840 --> 00:47:20,340
So he is basically
speaking to this deeper,

1054
00:47:20,340 --> 00:47:23,618
creative collaboration
between people and AI.

1055
00:47:23,618 --> 00:47:25,410
And this is what we
really need to focus on

1056
00:47:25,410 --> 00:47:28,050
is the creative, collaborative
interaction of people

1057
00:47:28,050 --> 00:47:31,390
with these new
empowering technologies.

1058
00:47:31,390 --> 00:47:34,170
And so, basically,
the argument is

1059
00:47:34,170 --> 00:47:36,312
AI is not going to replace
people for those jobs.

1060
00:47:36,312 --> 00:47:37,770
It's people who
know how to harness

1061
00:47:37,770 --> 00:47:39,780
AIs-- are going to be the ones
who get those jobs over people

1062
00:47:39,780 --> 00:47:41,980
who don't know how to
use these technologies.

1063
00:47:41,980 --> 00:47:44,940
So there's a lot of reason when
we think about the workforce,

1064
00:47:44,940 --> 00:47:46,920
as it is today, and how
it's going to continue

1065
00:47:46,920 --> 00:47:49,920
and helping people develop the
skills and abilities so they can

1066
00:47:49,920 --> 00:47:52,680
be competitive and effective
and creative problem

1067
00:47:52,680 --> 00:47:54,550
solvers in the workforce.

1068
00:47:54,550 --> 00:47:59,310
So we created this initiative,
RAISE, a couple of years ago.

1069
00:47:59,310 --> 00:48:02,370
RAISE, again, stands for
Responsible AI for Social

1070
00:48:02,370 --> 00:48:04,330
Empowerment and Education.

1071
00:48:04,330 --> 00:48:07,680
And the mission is to advance
equity and learning education

1072
00:48:07,680 --> 00:48:11,010
and empowerment through
artificial intelligence,

1073
00:48:11,010 --> 00:48:14,580
by rethinking and innovating
how to holistically

1074
00:48:14,580 --> 00:48:17,760
and equitably prepare
diverse students from K-12

1075
00:48:17,760 --> 00:48:20,430
to workforce, in
these technologies,

1076
00:48:20,430 --> 00:48:23,745
so that we can be happy,
engaged, empowered, successful,

1077
00:48:23,745 --> 00:48:26,470
in an increasingly
AI-powered society.

1078
00:48:26,470 --> 00:48:28,570
So that is our
mission for RAISE.

1079
00:48:28,570 --> 00:48:31,200
And now I'm going to talk
about a number of the programs

1080
00:48:31,200 --> 00:48:33,480
that we've been
developing and innovating

1081
00:48:33,480 --> 00:48:36,990
to deliver on this mission.

1082
00:48:36,990 --> 00:48:39,420
So another way to say
that mission statement

1083
00:48:39,420 --> 00:48:41,310
might be, we're
trying to advance

1084
00:48:41,310 --> 00:48:46,200
an empowered and wise
human-with-AI world,

1085
00:48:46,200 --> 00:48:49,020
by with and for all.

1086
00:48:49,020 --> 00:48:52,930
We want everyone to participate
and be engaged in this mission.

1087
00:48:52,930 --> 00:48:56,640
So there are three
main buckets that we

1088
00:48:56,640 --> 00:49:00,000
try to address-- in all of
our curriculum materials,

1089
00:49:00,000 --> 00:49:02,230
professional develop
materials, you name it,

1090
00:49:02,230 --> 00:49:04,723
one is to demystify
how AI actually works.

1091
00:49:04,723 --> 00:49:06,640
There's a lot of confusion
and hype out there.

1092
00:49:06,640 --> 00:49:08,848
We want people to have a
practical working knowledge,

1093
00:49:08,848 --> 00:49:11,940
grade-appropriate knowledge
of how AI actually works

1094
00:49:11,940 --> 00:49:14,400
and how to design and
use it responsibly.

1095
00:49:14,400 --> 00:49:19,110
We want people to become
informed AI digital citizens

1096
00:49:19,110 --> 00:49:22,260
so that we have a voice in
how we want these technologies

1097
00:49:22,260 --> 00:49:24,520
integrated and used in society.

1098
00:49:24,520 --> 00:49:27,420
We've heard many stories of
potential bias and inequities

1099
00:49:27,420 --> 00:49:29,970
that has arisen because of AI.

1100
00:49:29,970 --> 00:49:31,890
People need to
understand this and have

1101
00:49:31,890 --> 00:49:33,160
a voice in how they're used.

1102
00:49:33,160 --> 00:49:36,870
And the third bucket is
self-efficacy and empowerment,

1103
00:49:36,870 --> 00:49:39,300
being creative
makers and problem

1104
00:49:39,300 --> 00:49:41,190
solvers with these
technologies and making

1105
00:49:41,190 --> 00:49:44,880
that much more inclusive and
much greater access to that

1106
00:49:44,880 --> 00:49:46,420
as well.

1107
00:49:46,420 --> 00:49:49,750
So a lot of our
philosophy here at MIT

1108
00:49:49,750 --> 00:49:52,090
goes all the way back
to Seymour Papert.

1109
00:49:52,090 --> 00:49:56,150
He is the founding father
of constructionist learning.

1110
00:49:56,150 --> 00:50:00,670
He himself studied with Piaget
[LAUGHS] on constructionism,

1111
00:50:00,670 --> 00:50:01,570
structuralism.

1112
00:50:01,570 --> 00:50:04,720
Anyway, so Piaget's
constructionism,

1113
00:50:04,720 --> 00:50:08,080
where, basically, it's this
idea that when you actually

1114
00:50:08,080 --> 00:50:11,470
make something, learn to make
something, create something,

1115
00:50:11,470 --> 00:50:16,270
share it with others, you are
engaging in a much deeper sense

1116
00:50:16,270 --> 00:50:20,110
of knowledge and
empowerment than just

1117
00:50:20,110 --> 00:50:23,480
absorbing in this
lecture-style information.

1118
00:50:23,480 --> 00:50:26,110
So the act of creating
something meaningful,

1119
00:50:26,110 --> 00:50:29,140
integrating what you
learn in order to do that,

1120
00:50:29,140 --> 00:50:30,700
is really what
we're going to need

1121
00:50:30,700 --> 00:50:33,610
to empower a future
of children who

1122
00:50:33,610 --> 00:50:37,870
are going to be creative problem
solvers, creator makers, who

1123
00:50:37,870 --> 00:50:40,720
can, again, make
the world a better

1124
00:50:40,720 --> 00:50:43,010
place with these technologies.

1125
00:50:43,010 --> 00:50:44,710
I think the thing
that's really exciting

1126
00:50:44,710 --> 00:50:46,880
now is before it was
like, yeah, they're

1127
00:50:46,880 --> 00:50:48,630
going to get to do
that after college.

1128
00:50:48,630 --> 00:50:52,123
But the reality is these
tools and technologies

1129
00:50:52,123 --> 00:50:53,540
have gotten to the
point that they

1130
00:50:53,540 --> 00:50:55,670
are so accessible
that kids can actually

1131
00:50:55,670 --> 00:50:57,720
start to do this right now.

1132
00:50:57,720 --> 00:51:01,760
So I love this notion
of kid power with AI

1133
00:51:01,760 --> 00:51:05,532
because kids have this sense
of their inner superhero.

1134
00:51:05,532 --> 00:51:07,618
And what if their
superpower was AI?

1135
00:51:07,618 --> 00:51:09,410
And what if they could
combine those things

1136
00:51:09,410 --> 00:51:12,050
and go out and affect positive
change in their communities

1137
00:51:12,050 --> 00:51:15,060
and even the world, based on
what they're learning and doing?

1138
00:51:15,060 --> 00:51:17,000
I think a lot of
young people today

1139
00:51:17,000 --> 00:51:18,770
are feeling a sense
of hopelessness

1140
00:51:18,770 --> 00:51:21,120
because they feel like
they can't do anything.

1141
00:51:21,120 --> 00:51:23,550
AI and all this stuff
is happening to them.

1142
00:51:23,550 --> 00:51:26,390
We want to flip that
dialogue so they understand,

1143
00:51:26,390 --> 00:51:31,040
"you can create positive
change right now with these

1144
00:51:31,040 --> 00:51:31,820
technologies."

1145
00:51:31,820 --> 00:51:34,530
And we're going to
help them to do that.

1146
00:51:34,530 --> 00:51:37,250
So we are invading
new curriculum,

1147
00:51:37,250 --> 00:51:38,580
new pedagogical methods.

1148
00:51:38,580 --> 00:51:42,890
We have this framing we
call computational action.

1149
00:51:42,890 --> 00:51:45,200
So not just learning
through making, but learning

1150
00:51:45,200 --> 00:51:47,900
by solving, learning through
this action of children

1151
00:51:47,900 --> 00:51:50,820
can actually create things
with these innovative tools,

1152
00:51:50,820 --> 00:51:53,630
taking things like MIT
App Inventor, which

1153
00:51:53,630 --> 00:51:57,120
allow kids to make working
mobile apps on iOS and Android.

1154
00:51:57,120 --> 00:52:00,500
We've been infusing App Inventor
with state-of-the-art AI tools,

1155
00:52:00,500 --> 00:52:02,960
generative-AI tools,
deep-learning tools,

1156
00:52:02,960 --> 00:52:06,173
so they can apply them to
make solutions that can affect

1157
00:52:06,173 --> 00:52:06,840
their community.

1158
00:52:06,840 --> 00:52:08,580
And how do they do
that responsibly?

1159
00:52:08,580 --> 00:52:10,022
How do they do that ethically?

1160
00:52:10,022 --> 00:52:11,730
How do they do that
with their community?

1161
00:52:11,730 --> 00:52:14,450
That is what we're trying to
help teach kids what to do,

1162
00:52:14,450 --> 00:52:15,710
how to do that?

1163
00:52:15,710 --> 00:52:18,840
I'm going to give a quick
example with App Inventor.

1164
00:52:18,840 --> 00:52:21,320
So I'm sure many of you
are aware of App Inventor.

1165
00:52:21,320 --> 00:52:25,190
But this is one of the key
flagship platforms at MIT.

1166
00:52:25,190 --> 00:52:26,910
It is open source.

1167
00:52:26,910 --> 00:52:29,000
A lot of everything we
do basically in RAISE

1168
00:52:29,000 --> 00:52:30,030
is open source.

1169
00:52:30,030 --> 00:52:31,670
People are free to
use these materials,

1170
00:52:31,670 --> 00:52:33,470
adopt them, et
cetera, et cetera.

1171
00:52:33,470 --> 00:52:36,630
Again, "allows anyone to create
mobile apps in Android or iOS,"

1172
00:52:36,630 --> 00:52:39,140
this easy, intuitive block
based coding interface.

1173
00:52:39,140 --> 00:52:42,620
So lowers the barrier
to entry of students

1174
00:52:42,620 --> 00:52:45,470
being able to code
things with these, again,

1175
00:52:45,470 --> 00:52:47,920
block-based coding environments.

1176
00:52:47,920 --> 00:52:52,300
There's over 18 million
users in over 195 countries

1177
00:52:52,300 --> 00:52:54,230
who use App Inventor.

1178
00:52:54,230 --> 00:52:57,800
Almost 50% are from
developing economies.

1179
00:52:57,800 --> 00:53:02,050
So, again, widespread
uses, widespread usage.

1180
00:53:02,050 --> 00:53:04,780
I'm going to highlight one
example of computational action

1181
00:53:04,780 --> 00:53:06,260
that's particularly inspiring.

1182
00:53:06,260 --> 00:53:10,010
So we collaborate with a
non-profit called Technovation.

1183
00:53:10,010 --> 00:53:14,620
Technovation is a STEM program
for girls, particularly

1184
00:53:14,620 --> 00:53:18,220
high-school-aged girls,
around computational thinking

1185
00:53:18,220 --> 00:53:19,570
and innovation.

1186
00:53:19,570 --> 00:53:24,040
So this is one of the teams that
participated in Technovation.

1187
00:53:24,040 --> 00:53:26,200
And they're girls
who are living in one

1188
00:53:26,200 --> 00:53:28,870
of the largest slums in India.

1189
00:53:28,870 --> 00:53:31,120
And they decided
that they wanted

1190
00:53:31,120 --> 00:53:35,380
to create an app that basically
helped to schedule time

1191
00:53:35,380 --> 00:53:37,720
at when they would go--
the community could

1192
00:53:37,720 --> 00:53:40,670
go to the watering community
area to gather water.

1193
00:53:40,670 --> 00:53:43,430
Now, who tends to go to
gather water for the family?

1194
00:53:43,430 --> 00:53:45,410
It tends to be girls
about this age.

1195
00:53:45,410 --> 00:53:47,350
It could take hours to do this.

1196
00:53:47,350 --> 00:53:49,100
There could be fights
that would break out

1197
00:53:49,100 --> 00:53:50,490
because of a lot of tension.

1198
00:53:50,490 --> 00:53:52,490
So they wanted to
create this app

1199
00:53:52,490 --> 00:53:57,020
to help it be faster,
easier, and a better

1200
00:53:57,020 --> 00:54:00,110
experience for everyone
who went to those community

1201
00:54:00,110 --> 00:54:01,200
watering areas.

1202
00:54:01,200 --> 00:54:04,070
So, again, something that
actually really empowered

1203
00:54:04,070 --> 00:54:06,480
their community with
middle-school-aged girls--

1204
00:54:06,480 --> 00:54:10,280
so, again, anyone, anyone
can create solutions

1205
00:54:10,280 --> 00:54:12,380
with these tools
and technologies.

1206
00:54:12,380 --> 00:54:15,140
We have been creating
a lot of curriculum,

1207
00:54:15,140 --> 00:54:16,850
professional
development programs

1208
00:54:16,850 --> 00:54:21,140
for educators and teachers, to
bring these tools, technologies,

1209
00:54:21,140 --> 00:54:25,050
insights, concepts, mindsets,
and skill sets to students.

1210
00:54:25,050 --> 00:54:28,115
So one of our flagship
programs is called Day of AI.

1211
00:54:28,115 --> 00:54:32,660
Day of AI is short-format
curriculum for any teacher

1212
00:54:32,660 --> 00:54:35,270
to teach for all
students because it's not

1213
00:54:35,270 --> 00:54:37,100
just for computer
science students anymore.

1214
00:54:37,100 --> 00:54:39,140
The AI Genie's
out of the bottle.

1215
00:54:39,140 --> 00:54:41,510
Day of AI is really
about the intersection

1216
00:54:41,510 --> 00:54:44,090
of AI, digital citizenship,
and empowerment.

1217
00:54:44,090 --> 00:54:46,140
So it is truly for all students.

1218
00:54:46,140 --> 00:54:49,300
We have curriculum literally
from kindergarten to 12th grade,

1219
00:54:49,300 --> 00:54:53,110
including 12th grade
computer-science curriculum.

1220
00:54:53,110 --> 00:54:54,380
Its short format.

1221
00:54:54,380 --> 00:54:56,260
We know teachers
are overwhelmed.

1222
00:54:56,260 --> 00:54:57,910
So instead of asking
them to commit

1223
00:54:57,910 --> 00:55:01,270
to a whole semester-long
curriculum, these are short,

1224
00:55:01,270 --> 00:55:04,480
four to six-hour curriculum that
we're finding a lot of teachers

1225
00:55:04,480 --> 00:55:07,750
are like, that I can try out.

1226
00:55:07,750 --> 00:55:09,640
It introduces AI
literacy the way

1227
00:55:09,640 --> 00:55:12,190
I talked about and
in the way that it's

1228
00:55:12,190 --> 00:55:14,890
multidisciplinary,
highly-student-relatable

1229
00:55:14,890 --> 00:55:17,890
context, hands-on learning
and making, creating.

1230
00:55:17,890 --> 00:55:20,470
Lots of critical
decision making,

1231
00:55:20,470 --> 00:55:22,450
critical thinking,
creative decision making,

1232
00:55:22,450 --> 00:55:24,500
et cetera, et
cetera, is important.

1233
00:55:24,500 --> 00:55:26,390
It's free and open
for the materials.

1234
00:55:26,390 --> 00:55:28,270
And we have a lot of
online free asynchronous

1235
00:55:28,270 --> 00:55:30,260
professional development.

1236
00:55:30,260 --> 00:55:32,450
This is an example of
a lot of the topics

1237
00:55:32,450 --> 00:55:35,640
that we've been creating
over time, expanding that.

1238
00:55:35,640 --> 00:55:38,360
This year we just released one
in data science and climate.

1239
00:55:38,360 --> 00:55:40,617
This year, we might
do one on space.

1240
00:55:40,617 --> 00:55:42,950
We're seeing a lot of launches
to the moon and so forth.

1241
00:55:42,950 --> 00:55:45,170
So we think about
and look at how

1242
00:55:45,170 --> 00:55:48,050
AI is shaping topics that are
really interesting in the moment

1243
00:55:48,050 --> 00:55:49,268
to students.

1244
00:55:49,268 --> 00:55:51,060
And how can we create
materials to do that?

1245
00:55:51,060 --> 00:55:53,280
Last year, it was
all about ChatGPT.

1246
00:55:53,280 --> 00:55:54,480
We created that curriculum.

1247
00:55:54,480 --> 00:55:55,880
Last year it was the most
downloaded curriculum

1248
00:55:55,880 --> 00:55:56,730
of all of them.

1249
00:55:56,730 --> 00:55:59,150
So we try to be super
agile and responsive.

1250
00:55:59,150 --> 00:56:02,870
I'm going to show you a video of
what kids do once they've been

1251
00:56:02,870 --> 00:56:07,140
exposed to these materials.

1252
00:56:07,140 --> 00:56:09,433
This was the AI and
data science curriculum.

1253
00:56:09,433 --> 00:56:10,100
[VIDEO PLAYBACK]

1254
00:56:10,100 --> 00:56:11,850
- My name is Ileana Fournier.

1255
00:56:11,850 --> 00:56:13,623
- My name is Jessie Magenyi.

1256
00:56:13,623 --> 00:56:14,790
- My name is Victoria Leeth.

1257
00:56:14,790 --> 00:56:17,310
And my project is Tree Saver.

1258
00:56:17,310 --> 00:56:20,100
- And we were basically
getting information

1259
00:56:20,100 --> 00:56:22,890
about deforestation
in Massachusetts.

1260
00:56:22,890 --> 00:56:26,160
And we used that
information to make an app--

1261
00:56:26,160 --> 00:56:29,190
- --where people can click
it and ChatGPT will give them

1262
00:56:29,190 --> 00:56:32,190
information on how the can
help save trees and help

1263
00:56:32,190 --> 00:56:33,025
their community.

1264
00:56:33,025 --> 00:56:34,650
- We found that it
was really important

1265
00:56:34,650 --> 00:56:38,430
because it's such a
stable part to human life.

1266
00:56:38,430 --> 00:56:41,230
In the backyard of our
school, we're filled by trees.

1267
00:56:41,230 --> 00:56:42,370
Trees are everywhere.

1268
00:56:42,370 --> 00:56:44,610
And then not even 30
minutes out in Boston,

1269
00:56:44,610 --> 00:56:46,270
there's really no trees at all.

1270
00:56:46,270 --> 00:56:50,730
So that duality and that drastic
change was really interesting

1271
00:56:50,730 --> 00:56:51,880
and intrigued all of us.

1272
00:56:51,880 --> 00:56:56,220
- The future is kind of scary to
think about because with the way

1273
00:56:56,220 --> 00:57:00,210
the climate is changing
and all that's happening,

1274
00:57:00,210 --> 00:57:04,320
I feel like if there
is no change done fast,

1275
00:57:04,320 --> 00:57:08,140
I feel like the future
would not be a good one.

1276
00:57:08,140 --> 00:57:09,850
- The whole point
of our project is

1277
00:57:09,850 --> 00:57:12,670
to help people find ways
to help their community

1278
00:57:12,670 --> 00:57:15,760
and help the environment by
improving the amount of trees

1279
00:57:15,760 --> 00:57:16,545
that are cut down.

1280
00:57:16,545 --> 00:57:18,640
AI can make us a
more powerful person

1281
00:57:18,640 --> 00:57:21,970
because it can help us find
ways to connect with others

1282
00:57:21,970 --> 00:57:25,360
and help us connect and
gain more information

1283
00:57:25,360 --> 00:57:27,300
that we can get ourselves.

1284
00:57:27,300 --> 00:57:29,770
- It could still be mastered
because it's so new.

1285
00:57:29,770 --> 00:57:32,740
And I feel like getting
that in schools everywhere,

1286
00:57:32,740 --> 00:57:35,890
you could really find the
new greatest minds that

1287
00:57:35,890 --> 00:57:39,680
will be able to use AI, for
the better of the world.

1288
00:57:39,680 --> 00:57:41,010
- And, yes, we did--

1289
00:57:41,010 --> 00:57:43,400
I felt powerful like,
"I can do this."

1290
00:57:43,400 --> 00:57:46,480
And I feel like I
can make an app now

1291
00:57:46,480 --> 00:57:47,940
that can change the world.

1292
00:57:47,940 --> 00:57:50,222
[MUSIC PLAYING]

1293
00:57:50,222 --> 00:57:51,160
[END VIDEO PLAYBACK]

1294
00:57:51,160 --> 00:57:53,510
CYNTHIA BREAZEAL: So
this is what we're after.

1295
00:57:53,510 --> 00:57:55,270
I can tell you this
Day of AI program,

1296
00:57:55,270 --> 00:57:57,250
we're already all
over the world.

1297
00:57:57,250 --> 00:58:00,430
Over 110 countries,
over 10,000 educators,

1298
00:58:00,430 --> 00:58:03,370
K-12 teachers are bringing
Day of AI to their classroom

1299
00:58:03,370 --> 00:58:06,550
and growing through our
international partnerships.

1300
00:58:06,550 --> 00:58:08,327
We have other partners
who are bringing it

1301
00:58:08,327 --> 00:58:10,660
to other regions of the world,
like South America, Latin

1302
00:58:10,660 --> 00:58:11,180
America.

1303
00:58:11,180 --> 00:58:15,020
So, again, this is very much
a community-oriented endeavor.

1304
00:58:15,020 --> 00:58:17,035
"So what do kids solve with AI?"

1305
00:58:17,035 --> 00:58:19,000
I mean, we see this
again and again.

1306
00:58:19,000 --> 00:58:21,230
They want to create
a more just society.

1307
00:58:21,230 --> 00:58:24,650
They want to help
our planet to thrive.

1308
00:58:24,650 --> 00:58:26,480
They want to help
people to flourish.

1309
00:58:26,480 --> 00:58:28,090
I mean, the kids
are truly inspiring,

1310
00:58:28,090 --> 00:58:30,460
as you could see, in that video.

1311
00:58:30,460 --> 00:58:34,690
So just kind of wrap
up with some resources

1312
00:58:34,690 --> 00:58:39,010
that I can point you to, we have
a lot of different curriculum

1313
00:58:39,010 --> 00:58:39,560
on RAISE.

1314
00:58:39,560 --> 00:58:41,560
You can check it
out the Day of AI.

1315
00:58:41,560 --> 00:58:43,465
Dayofai.org has all
those curriculum

1316
00:58:43,465 --> 00:58:45,340
I mentioned, teacher
professional development

1317
00:58:45,340 --> 00:58:46,090
serials.

1318
00:58:46,090 --> 00:58:48,160
We have other programs
and Data Activism,

1319
00:58:48,160 --> 00:58:50,660
so Data Science for Social Good.

1320
00:58:50,660 --> 00:58:53,000
Future Makers is a
technical-skills-building

1321
00:58:53,000 --> 00:58:53,760
program.

1322
00:58:53,760 --> 00:58:57,200
All these programs
are available.

1323
00:58:57,200 --> 00:58:59,810
We know that AI has to support
teachers and supporting

1324
00:58:59,810 --> 00:59:00,330
students.

1325
00:59:00,330 --> 00:59:02,247
All of our curriculum
professional development

1326
00:59:02,247 --> 00:59:03,680
is also oriented
to help teachers

1327
00:59:03,680 --> 00:59:08,250
feel informed and comfortable
in teaching their kids about AI.

1328
00:59:08,250 --> 00:59:10,117
We have written
a position paper.

1329
00:59:10,117 --> 00:59:12,450
I'm going to leave this screen
up just for a little bit,

1330
00:59:12,450 --> 00:59:14,867
for those of you who want to
get the QR code on Generative

1331
00:59:14,867 --> 00:59:18,140
AI and K-12 Education.

1332
00:59:18,140 --> 00:59:20,630
It gives a lot of
opportunities, challenges, et

1333
00:59:20,630 --> 00:59:25,480
cetera, et cetera, that
might be of interest to you.

1334
00:59:25,480 --> 00:59:27,850
We know that
teachers need to also

1335
00:59:27,850 --> 00:59:29,920
be up to speed on how
they can use generative

1336
00:59:29,920 --> 00:59:31,550
AI in their practice.

1337
00:59:31,550 --> 00:59:35,117
This is a free online course we
developed with Grow with Google.

1338
00:59:35,117 --> 00:59:36,700
It's a two-hour "go
at your own pace,"

1339
00:59:36,700 --> 00:59:39,370
all about how you can use
generative AI in your teaching

1340
00:59:39,370 --> 00:59:40,330
practice.

1341
00:59:40,330 --> 00:59:43,300
As Bill mentioned, things
like customizing lessons,

1342
00:59:43,300 --> 00:59:46,390
creating rubrics, et
cetera, et cetera,

1343
00:59:46,390 --> 00:59:48,047
these are actual
activities that you

1344
00:59:48,047 --> 00:59:49,630
go through as part
of the course to be

1345
00:59:49,630 --> 00:59:52,600
a very practical hands-on
thing for how generative AI can

1346
00:59:52,600 --> 00:59:53,840
be used to empower teachers.

1347
00:59:53,840 --> 00:59:57,050
Teachers are an important AI
workforce for this country.

1348
00:59:57,050 --> 00:59:59,900
So you can check that out.

1349
00:59:59,900 --> 01:00:03,040
Teachers and educators need to
be part of the conversation.

1350
01:00:03,040 --> 01:00:06,800
We just released "AI
Policy Guidance."

1351
01:00:06,800 --> 01:00:09,950
Thank you, Bill, for
helping preview earlier

1352
01:00:09,950 --> 01:00:11,000
versions of this paper.

1353
01:00:11,000 --> 01:00:12,300
But it's available now.

1354
01:00:12,300 --> 01:00:14,180
So when you think
about AI policy--

1355
01:00:14,180 --> 01:00:14,935
this is for K-12.

1356
01:00:14,935 --> 01:00:16,310
But, again, I
think this is going

1357
01:00:16,310 --> 01:00:19,550
to be relevant for other
age groups as well.

1358
01:00:19,550 --> 01:00:22,430
But, hopefully, this will be
informative and interesting

1359
01:00:22,430 --> 01:00:28,207
for you also, so around
AI policy guidance.

1360
01:00:28,207 --> 01:00:30,790
All right, and you're all here
joining the call because you're

1361
01:00:30,790 --> 01:00:31,880
interested in this stuff.

1362
01:00:31,880 --> 01:00:34,550
It's still early days,
so you're the pioneers.

1363
01:00:34,550 --> 01:00:36,560
You should experiment and share.

1364
01:00:36,560 --> 01:00:38,690
And I know J-WEL helps
to facilitate that.

1365
01:00:38,690 --> 01:00:42,760
So looking forward to see what
you are innovating and sharing

1366
01:00:42,760 --> 01:00:44,410
as well.

1367
01:00:44,410 --> 01:00:47,350
And with that, thank you.

1368
01:00:47,350 --> 01:00:50,950
MEGAN MITCHELL: Cynthia,
thank you so much,

1369
01:00:50,950 --> 01:00:54,400
amazing information,
knowledge, I think, to absorb.

1370
01:00:54,400 --> 01:00:57,805
As you were speaking,
I was getting messages

1371
01:00:57,805 --> 01:01:01,300
from teachers locally
and globally about,

1372
01:01:01,300 --> 01:01:02,780
how do I get involved?

1373
01:01:02,780 --> 01:01:05,110
And I think you
really shared so many

1374
01:01:05,110 --> 01:01:07,750
incredible, actionable
resources, which

1375
01:01:07,750 --> 01:01:12,640
is what makes, I think, the
work you're doing so fantastic,

1376
01:01:12,640 --> 01:01:15,160
to take it from the
lab, to give educators

1377
01:01:15,160 --> 01:01:18,010
a way to implement this.

1378
01:01:18,010 --> 01:01:20,560
You talked a little
bit about this.

1379
01:01:20,560 --> 01:01:23,320
And as I asked this question, I
mentioned in the chat, "Please,

1380
01:01:23,320 --> 01:01:26,260
folks, can we put questions in
the chat so, in the time we have

1381
01:01:26,260 --> 01:01:28,660
left-- you are able to ask--

1382
01:01:28,660 --> 01:01:31,300
we can get a few questions
from you all in too?

1383
01:01:31,300 --> 01:01:38,830
But how are these folks in
emerging economies globally

1384
01:01:38,830 --> 01:01:40,520
finding these resources?

1385
01:01:40,520 --> 01:01:43,990
So often when we talk even
to some of our members,

1386
01:01:43,990 --> 01:01:47,960
they'll say we just
feel like we're behind

1387
01:01:47,960 --> 01:01:51,470
because our young children don't
have the same access to just

1388
01:01:51,470 --> 01:01:56,030
core technology that
kids have in the US

1389
01:01:56,030 --> 01:01:57,860
that they might have in Europe.

1390
01:01:57,860 --> 01:02:00,080
And I think you've
developed tools

1391
01:02:00,080 --> 01:02:01,710
that these young
people can access.

1392
01:02:01,710 --> 01:02:03,360
But how are people finding them?

1393
01:02:03,360 --> 01:02:07,010
How do you see folks
overcoming, in some

1394
01:02:07,010 --> 01:02:11,000
of these markets, the
connectivity issues or even

1395
01:02:11,000 --> 01:02:13,910
their mindset that they don't
have the type of resources

1396
01:02:13,910 --> 01:02:14,842
that they need?

1397
01:02:14,842 --> 01:02:16,550
CYNTHIA BREAZEAL:
Yeah, so I'll tell you,

1398
01:02:16,550 --> 01:02:20,120
I mean, at least from what
we're doing in RAISE--

1399
01:02:20,120 --> 01:02:23,420
so one of the
advantages of being MIT

1400
01:02:23,420 --> 01:02:26,960
is, we are a place that people
tend to just Google and find,

1401
01:02:26,960 --> 01:02:28,620
what is MIT doing in this space?

1402
01:02:28,620 --> 01:02:32,330
And they do tend to find
MIT RAISE or Day of AI

1403
01:02:32,330 --> 01:02:34,850
from that search.

1404
01:02:34,850 --> 01:02:37,430
With the Day of AI
program, in particular,

1405
01:02:37,430 --> 01:02:39,440
that's because I
feel it's like-- it

1406
01:02:39,440 --> 01:02:41,100
is for all students
and all teachers.

1407
01:02:41,100 --> 01:02:42,390
It's designed for that.

1408
01:02:42,390 --> 01:02:44,780
It is like the
entry-level point, where

1409
01:02:44,780 --> 01:02:46,520
it's like, if
you're just curious,

1410
01:02:46,520 --> 01:02:49,250
and you want to try to
see what it's about,

1411
01:02:49,250 --> 01:02:51,907
and you're not able to take
a semester-long curriculum--

1412
01:02:51,907 --> 01:02:53,990
because there's a lot of
curriculum that are being

1413
01:02:53,990 --> 01:02:55,240
developed out there right now.

1414
01:02:55,240 --> 01:02:58,200
But it's mostly for
computer-science teachers.

1415
01:02:58,200 --> 01:03:00,210
This is positioned
very differently.

1416
01:03:00,210 --> 01:03:03,050
This is really AI and
digital citizenship.

1417
01:03:03,050 --> 01:03:05,580
And we've designed it
with access in mind.

1418
01:03:05,580 --> 01:03:08,750
So we have materials that
span the whole gamut from,

1419
01:03:08,750 --> 01:03:11,360
yeah, if you want to be coding
apps through App Inventor

1420
01:03:11,360 --> 01:03:13,170
and things like that,
you can do that.

1421
01:03:13,170 --> 01:03:16,650
We have curriculum that doesn't
involve technology at all.

1422
01:03:16,650 --> 01:03:19,700
So, for instance, when the
White House, last year,

1423
01:03:19,700 --> 01:03:22,190
released their blueprint
for an AI Bill of Rights,

1424
01:03:22,190 --> 01:03:25,370
we created a whole curriculum in
AI and human rights and policy

1425
01:03:25,370 --> 01:03:26,570
that kids debate.

1426
01:03:26,570 --> 01:03:28,950
Doesn't require any
technology whatsoever.

1427
01:03:28,950 --> 01:03:31,550
So we have tried
to create materials

1428
01:03:31,550 --> 01:03:34,670
that no matter how
technologically comfortable

1429
01:03:34,670 --> 01:03:36,920
teachers are or
resourced they are,

1430
01:03:36,920 --> 01:03:40,070
there is something there that
they should be able to access

1431
01:03:40,070 --> 01:03:41,730
and bring to their students.

1432
01:03:41,730 --> 01:03:45,590
We are now working more
systematically and strategically

1433
01:03:45,590 --> 01:03:48,180
with organizations
around the world.

1434
01:03:48,180 --> 01:03:50,600
These are usually ministries
of education, increasingly,

1435
01:03:50,600 --> 01:03:53,600
or non-profits, where
they want to take--

1436
01:03:53,600 --> 01:03:54,980
and I call them reference--

1437
01:03:54,980 --> 01:03:58,910
[INAUDIBLE] reference materials
and then localize them

1438
01:03:58,910 --> 01:04:02,030
so that they're relevant
to their local region

1439
01:04:02,030 --> 01:04:04,920
and adapt them for that context.

1440
01:04:04,920 --> 01:04:09,300
And then they help recruit
and train teachers.

1441
01:04:09,300 --> 01:04:12,110
So, again, it's
becoming a very network,

1442
01:04:12,110 --> 01:04:16,250
community-oriented
approach, where we've done,

1443
01:04:16,250 --> 01:04:17,820
again, the reference design.

1444
01:04:17,820 --> 01:04:19,070
But then we are-- it's open.

1445
01:04:19,070 --> 01:04:21,615
It's Open Creative Commons.

1446
01:04:21,615 --> 01:04:23,370
It's the expectation
that people will

1447
01:04:23,370 --> 01:04:27,335
take them and adapt them
to be relevant and on point

1448
01:04:27,335 --> 01:04:28,360
for their students.

1449
01:04:28,360 --> 01:04:31,150
And so, we're starting to
see that happen as well.

1450
01:04:31,150 --> 01:04:33,300
So, for instance, we have
a particular foundation

1451
01:04:33,300 --> 01:04:34,720
that we've been
collaborating with in Chile.

1452
01:04:34,720 --> 01:04:36,180
If you to the Day
of AI website--

1453
01:04:36,180 --> 01:04:39,360
they have been bringing it to
many, many different countries

1454
01:04:39,360 --> 01:04:40,810
in Latin and South America.

1455
01:04:40,810 --> 01:04:44,130
So we often rely again on
our local collaborators

1456
01:04:44,130 --> 01:04:45,880
to help get that scale.

1457
01:04:45,880 --> 01:04:49,950
So there are
materials out there.

1458
01:04:49,950 --> 01:04:53,430
And I know we've done research
around the Day of AI materials.

1459
01:04:53,430 --> 01:04:55,090
There's a whole
evaluation study,

1460
01:04:55,090 --> 01:04:57,450
if you go to the Day of
AI website, with teachers

1461
01:04:57,450 --> 01:05:01,440
from like 40 countries.

1462
01:05:01,440 --> 01:05:04,080
We're being successful, I
think, in teachers' ability

1463
01:05:04,080 --> 01:05:06,840
to take these materials and
adapt them to their context

1464
01:05:06,840 --> 01:05:08,910
and feel that they're
having a good experience,

1465
01:05:08,910 --> 01:05:11,680
both for themselves as teachers
and for their students.

1466
01:05:11,680 --> 01:05:14,620
So I would definitely encourage
people to check those out.

1467
01:05:14,620 --> 01:05:16,470
We have recommended grade bands.

1468
01:05:16,470 --> 01:05:18,030
But I can tell you
this material--

1469
01:05:18,030 --> 01:05:21,340
I think it's also super
relevant for post-secondary.

1470
01:05:21,340 --> 01:05:23,940
If you're really talking
about, what the heck is AI?

1471
01:05:23,940 --> 01:05:26,220
And what's the responsible use?

1472
01:05:26,220 --> 01:05:29,550
And how do I think about the
ethical implications of--

1473
01:05:29,550 --> 01:05:31,300
that's what this curriculum is.

1474
01:05:31,300 --> 01:05:35,340
You can age up the dialogue
and the projects or down,

1475
01:05:35,340 --> 01:05:36,100
accordingly.

1476
01:05:36,100 --> 01:05:37,770
We just have
recommended grade bands.

1477
01:05:37,770 --> 01:05:40,030
But I think this
is so foundational.

1478
01:05:40,030 --> 01:05:43,560
It's actually, again,
just a good foundation

1479
01:05:43,560 --> 01:05:46,170
course for all kinds of people.

1480
01:05:46,170 --> 01:05:52,110
So I would definitely point
people there first and foremost.

1481
01:05:52,110 --> 01:05:55,170
And it is different, I think,
because most other curriculum

1482
01:05:55,170 --> 01:05:57,720
that I know of has largely
been designed, again,

1483
01:05:57,720 --> 01:06:01,570
for computer-science-oriented
instructors.

1484
01:06:01,570 --> 01:06:03,370
So, anyway, this is different.

1485
01:06:03,370 --> 01:06:05,615
This is really, "AI
is for everyone."

1486
01:06:05,615 --> 01:06:06,457
[LAUGHS]

1487
01:06:06,457 --> 01:06:08,980
MEGAN MITCHELL: [LAUGHS]
That's fantastic.

1488
01:06:08,980 --> 01:06:10,590
That's fantastic.

1489
01:06:10,590 --> 01:06:14,590
We have our first question in
the chat-- that I'm going to--

1490
01:06:14,590 --> 01:06:16,470
from Marta Aymerich.

1491
01:06:16,470 --> 01:06:19,560
And she is at the eHealth
Center at our Open

1492
01:06:19,560 --> 01:06:24,100
University of Catalonia,
so one of J-WEL members.

1493
01:06:24,100 --> 01:06:26,530
And there's the age old--

1494
01:06:26,530 --> 01:06:28,480
there's a lot of resistance
from many teachers

1495
01:06:28,480 --> 01:06:31,190
in introducing AI in education.

1496
01:06:31,190 --> 01:06:34,690
And I was even thinking
about these building blocks

1497
01:06:34,690 --> 01:06:38,110
that they're learning
by doing, not

1498
01:06:38,110 --> 01:06:41,650
by the lecture, which is also
such a traditional mode in so

1499
01:06:41,650 --> 01:06:44,350
many parts of the world.

1500
01:06:44,350 --> 01:06:49,300
How do you actively
try and get people

1501
01:06:49,300 --> 01:06:54,940
past that those mindset biases?

1502
01:06:54,940 --> 01:06:59,920
CYNTHIA BREAZEAL: So we're in a
couple intriguingly-interesting

1503
01:06:59,920 --> 01:07:02,330
conversations with
ministries of education.

1504
01:07:02,330 --> 01:07:07,450
I think we're just at a time
now where the world acknowledges

1505
01:07:07,450 --> 01:07:11,170
it's just really important
to build a foundational AI

1506
01:07:11,170 --> 01:07:14,440
literacy, if not AI
fluency, in general.

1507
01:07:14,440 --> 01:07:17,643
So you're always going to
have your early movers,

1508
01:07:17,643 --> 01:07:18,310
your innovators.

1509
01:07:18,310 --> 01:07:22,450
And that's who's coming to
us right now, understandably.

1510
01:07:22,450 --> 01:07:24,970
I think we're starting to
move into this next phase,

1511
01:07:24,970 --> 01:07:27,068
where ministries of
education are like,

1512
01:07:27,068 --> 01:07:28,360
no, this is actually important.

1513
01:07:28,360 --> 01:07:32,350
And we need to figure out how
to get it in systematically

1514
01:07:32,350 --> 01:07:35,380
into the formal education system
in a way that makes sense,

1515
01:07:35,380 --> 01:07:38,460
given that teachers already have
to teach a lot of other stuff.

1516
01:07:38,460 --> 01:07:40,810
But to make room for that--

1517
01:07:40,810 --> 01:07:44,790
so part of this is we have
to incentivize teachers

1518
01:07:44,790 --> 01:07:48,960
through the mechanisms
that allow them to advocate

1519
01:07:48,960 --> 01:07:51,750
or convince whoever they
need to that they can do so,

1520
01:07:51,750 --> 01:07:54,300
whether that's professional
development credits that they

1521
01:07:54,300 --> 01:07:58,717
need to do anyway or
advocate to the principal

1522
01:07:58,717 --> 01:08:01,300
or to the administrators of the
school that this is important.

1523
01:08:01,300 --> 01:08:03,750
Or if in different parts
of the world-- again,

1524
01:08:03,750 --> 01:08:06,600
if ministries of education
also help to signal,

1525
01:08:06,600 --> 01:08:09,030
this is actually really
important and is important that

1526
01:08:09,030 --> 01:08:09,613
you teach it--

1527
01:08:09,613 --> 01:08:14,310
I think we're trying to create
the conditions upon which

1528
01:08:14,310 --> 01:08:19,470
teachers feel they are able
to actually set the time aside

1529
01:08:19,470 --> 01:08:21,069
to actually do this.

1530
01:08:21,069 --> 01:08:24,240
And we're trying to be very
mindful of the amount of time

1531
01:08:24,240 --> 01:08:26,200
we're asking of
teachers to do this.

1532
01:08:26,200 --> 01:08:28,720
So, again, that's why we're
starting with short format?

1533
01:08:28,720 --> 01:08:31,612
But we're trying to do both
the top down and bottom up.

1534
01:08:31,612 --> 01:08:33,029
And we're running
some experiments

1535
01:08:33,029 --> 01:08:34,290
in different parts of
the world right now

1536
01:08:34,290 --> 01:08:35,470
to see what that looks like.

1537
01:08:35,470 --> 01:08:37,529
But I'm just
sharing, in general,

1538
01:08:37,529 --> 01:08:41,229
there seems to be
acknowledgment at the top level.

1539
01:08:41,229 --> 01:08:43,090
This is important.

1540
01:08:43,090 --> 01:08:47,109
And growing grassroots
and quality materials

1541
01:08:47,109 --> 01:08:48,918
that people can look
at and go, oh, people,

1542
01:08:48,918 --> 01:08:50,960
are having a good experience,
I should try that--

1543
01:08:50,960 --> 01:08:52,810
and it's not a super heavy lift.

1544
01:08:52,810 --> 01:08:57,080
That's how we're trying
to help bring those two

1545
01:08:57,080 --> 01:08:59,399
ends of the spectrum together.

1546
01:08:59,399 --> 01:09:02,720
The other thing that I think
is just worth people knowing is

1547
01:09:02,720 --> 01:09:05,060
an exercise that we're
actively involved in

1548
01:09:05,060 --> 01:09:08,270
is-- because we have, again,
a wide range of curriculum,

1549
01:09:08,270 --> 01:09:12,319
we're actually starting
to think about packaged

1550
01:09:12,319 --> 01:09:15,180
subject-relevant curriculum.

1551
01:09:15,180 --> 01:09:17,600
So if you're a math teacher
or a science teacher

1552
01:09:17,600 --> 01:09:20,479
or an arts teacher or a
social studies teacher

1553
01:09:20,479 --> 01:09:22,819
or an English or
language arts teacher,

1554
01:09:22,819 --> 01:09:25,939
what might be a short form
AI literacy curriculum

1555
01:09:25,939 --> 01:09:29,120
that both matches to what
you have to teach anyway,

1556
01:09:29,120 --> 01:09:31,350
but then also helps you
learn about AI, right?

1557
01:09:31,350 --> 01:09:34,870
So we're in that
exercise right now

1558
01:09:34,870 --> 01:09:39,670
as another tactic to try to
help teachers bring AI education

1559
01:09:39,670 --> 01:09:41,090
into their classrooms.

1560
01:09:41,090 --> 01:09:45,130
So we're trying to
figure out how to make

1561
01:09:45,130 --> 01:09:46,850
all of these incentives align.

1562
01:09:46,850 --> 01:09:51,670
But I think the broader
ecosystem is also starting

1563
01:09:51,670 --> 01:09:54,820
to align around this as well.

1564
01:09:54,820 --> 01:09:57,325
MEGAN MITCHELL: I think, for
the teachers on this call

1565
01:09:57,325 --> 01:09:59,200
and the educators who
might feel like they're

1566
01:09:59,200 --> 01:10:01,120
those early adopters,
I imagine it's

1567
01:10:01,120 --> 01:10:04,580
encouraging to hear that the
ministries are seeing this.

1568
01:10:04,580 --> 01:10:06,910
There are people
that they might not

1569
01:10:06,910 --> 01:10:08,350
get to hear from and talk to.

1570
01:10:08,350 --> 01:10:11,020
But you are seeing
this, at least

1571
01:10:11,020 --> 01:10:12,400
at either end of the spectrum.

1572
01:10:12,400 --> 01:10:15,490
And it'll come together,
which is, I think,

1573
01:10:15,490 --> 01:10:17,650
important and exciting.

1574
01:10:17,650 --> 01:10:22,480
One last question, also from
a member, Melchor Snchez

1575
01:10:22,480 --> 01:10:26,260
who's with UNAM in Mexico--

1576
01:10:26,260 --> 01:10:29,020
and he's interested just in
your thoughts and comments

1577
01:10:29,020 --> 01:10:32,980
on the challenges of
excessively anthropomorphizing

1578
01:10:32,980 --> 01:10:36,590
the different personal
characteristics when

1579
01:10:36,590 --> 01:10:40,580
you use these GenAI tools.

1580
01:10:40,580 --> 01:10:44,480
and what you're seeing-- you're
able to combat people's concerns

1581
01:10:44,480 --> 01:10:48,170
or respond to and
ameliorate their concerns--

1582
01:10:48,170 --> 01:10:49,910
would be interesting
to understand.

1583
01:10:49,910 --> 01:10:50,868
CYNTHIA BREAZEAL: Yeah.

1584
01:10:50,868 --> 01:10:52,820
So, I mean, I would
say, this is very much

1585
01:10:52,820 --> 01:10:56,840
why we started to go
deep into AI literacy

1586
01:10:56,840 --> 01:10:58,200
was exactly this point.

1587
01:10:58,200 --> 01:11:02,450
So we are designing these
personified AI technologies,

1588
01:11:02,450 --> 01:11:05,000
have done it for
decades, to try to help

1589
01:11:05,000 --> 01:11:06,600
promote human flourishing.

1590
01:11:06,600 --> 01:11:09,890
But we have seen examples now--
social media is like the poster

1591
01:11:09,890 --> 01:11:13,700
child for where a lot
of harm is being done

1592
01:11:13,700 --> 01:11:15,690
through these technologies.

1593
01:11:15,690 --> 01:11:17,720
And then with ChatGPT--

1594
01:11:17,720 --> 01:11:21,978
again, it started with Siri
and then Alexa, now ChatGPT.

1595
01:11:21,978 --> 01:11:24,020
We have seen it for decades,
people's willingness

1596
01:11:24,020 --> 01:11:26,060
to engage socially and
emotionally with these AI

1597
01:11:26,060 --> 01:11:26,650
technologies.

1598
01:11:26,650 --> 01:11:28,440
Now, you're seeing it at scale.

1599
01:11:28,440 --> 01:11:31,610
And then sometimes, the
media will present you

1600
01:11:31,610 --> 01:11:34,700
again stories and dialogues
where it's like, whoa,

1601
01:11:34,700 --> 01:11:37,070
maybe not so healthy, right?

1602
01:11:37,070 --> 01:11:40,860
So this is why the AI
literacy piece is so critical.

1603
01:11:40,860 --> 01:11:44,960
And I think the big thing people
have to understand right now--

1604
01:11:44,960 --> 01:11:46,971
and I have other
slides around this.

1605
01:11:46,971 --> 01:11:49,980

1606
01:11:49,980 --> 01:11:52,980
The paradox, if I'll
call it a paradox,

1607
01:11:52,980 --> 01:11:59,370
is that the generative-AI
technologies present themselves

1608
01:11:59,370 --> 01:12:04,410
as being very capable,
in a human-like way,

1609
01:12:04,410 --> 01:12:07,366
through language.

1610
01:12:07,366 --> 01:12:08,960
But they're not human.

1611
01:12:08,960 --> 01:12:11,230
The way they think, the way
they generate the outputs,

1612
01:12:11,230 --> 01:12:13,490
is not human at all.

1613
01:12:13,490 --> 01:12:15,210
It's completely
different, right?

1614
01:12:15,210 --> 01:12:16,210
I mean, a lot of these--

1615
01:12:16,210 --> 01:12:18,740
the large language models
are language models.

1616
01:12:18,740 --> 01:12:21,790
They're trying to predict
and generate the next word.

1617
01:12:21,790 --> 01:12:24,040
So we just have
to all understand

1618
01:12:24,040 --> 01:12:27,790
that when it's
generating an output that

1619
01:12:27,790 --> 01:12:30,940
could be really intriguing
and on point and exciting

1620
01:12:30,940 --> 01:12:35,200
or completely off, the
algorithm does not understand

1621
01:12:35,200 --> 01:12:36,407
what it's generating.

1622
01:12:36,407 --> 01:12:37,240
MEGAN MITCHELL: Yes.

1623
01:12:37,240 --> 01:12:38,615
CYNTHIA BREAZEAL:
All the meaning

1624
01:12:38,615 --> 01:12:41,380
is ascribed to the human
being who is looking at it

1625
01:12:41,380 --> 01:12:44,770
and making sense of it
to say, wow, that was

1626
01:12:44,770 --> 01:12:46,000
really useful or creative.

1627
01:12:46,000 --> 01:12:48,460
Or what the heck?

1628
01:12:48,460 --> 01:12:51,020
It's like, oh,
completely wrong, right?

1629
01:12:51,020 --> 01:12:54,390
The AI do not understand
what they're generating.

1630
01:12:54,390 --> 01:12:56,720
They are generating,
and human beings

1631
01:12:56,720 --> 01:13:01,540
are ascribing the meaning, which
is why we have to be critical,

1632
01:13:01,540 --> 01:13:06,630
in the right sense, of is that
output valuable, useful to me,

1633
01:13:06,630 --> 01:13:07,750
or not?

1634
01:13:07,750 --> 01:13:11,070
Because, again, when used
responsibly, appropriately,

1635
01:13:11,070 --> 01:13:12,360
it can help accelerate.

1636
01:13:12,360 --> 01:13:15,300
And it can be, again, terrific.

1637
01:13:15,300 --> 01:13:17,400
But it can also be
incredibly misleading.

1638
01:13:17,400 --> 01:13:21,540
We see examples of like, "It
seems so reasonable in its

1639
01:13:21,540 --> 01:13:22,080
response."

1640
01:13:22,080 --> 01:13:23,580
And if you're an
expert-- and you're

1641
01:13:23,580 --> 01:13:26,580
like, but that is completely
made up [LAUGHS] right?

1642
01:13:26,580 --> 01:13:29,383
So you just have to be
a critical thinker when

1643
01:13:29,383 --> 01:13:31,300
you use these kinds of
tools and technologies.

1644
01:13:31,300 --> 01:13:34,110
So the responsible use
piece of the AI literacy

1645
01:13:34,110 --> 01:13:36,330
and understanding what these
algorithms are actually

1646
01:13:36,330 --> 01:13:40,260
doing so you understand how
much you can rely on them

1647
01:13:40,260 --> 01:13:46,270
and not to over rely on
them is really important.

1648
01:13:46,270 --> 01:13:49,420
So whether it's the personified
aspects or the information--

1649
01:13:49,420 --> 01:13:52,450
no matter what the aspect,
it's just important, I think,

1650
01:13:52,450 --> 01:13:54,850
that people understand
what these algorithms are

1651
01:13:54,850 --> 01:14:01,000
doing so that you understand,
this is how much I can rely

1652
01:14:01,000 --> 01:14:03,520
on them to do something, right?

1653
01:14:03,520 --> 01:14:07,473
I always have to think
critically about the output.

1654
01:14:07,473 --> 01:14:08,390
MEGAN MITCHELL: Great.

1655
01:14:08,390 --> 01:14:11,860
Well, on that note, I hope
everyone can join me in thanking

1656
01:14:11,860 --> 01:14:16,720
Cynthia very much for an
incredibly interesting

1657
01:14:16,720 --> 01:14:21,010
and informative "packed with
good resources and information"

1658
01:14:21,010 --> 01:14:21,700
session.

1659
01:14:21,700 --> 01:14:25,180
For those of you who are here,
Amy has already put in the chat,

1660
01:14:25,180 --> 01:14:29,170
members will be doing a
roundtable discussion next week

1661
01:14:29,170 --> 01:14:33,640
with Hae Won Park, who is a very
valuable member of Cynthia's

1662
01:14:33,640 --> 01:14:37,280
team and lab, to do
further discussions.

1663
01:14:37,280 --> 01:14:39,400
And there's another
event on August 22

1664
01:14:39,400 --> 01:14:43,750
coming up, for the broader
public on emerging AI

1665
01:14:43,750 --> 01:14:46,210
innovations.

1666
01:14:46,210 --> 01:14:47,720
We have video recorded this.

1667
01:14:47,720 --> 01:14:50,200
And so, we'll make that video
available to the folks who

1668
01:14:50,200 --> 01:14:51,890
are here and publicly.

1669
01:14:51,890 --> 01:14:53,420
And we will be sending a survey.

1670
01:14:53,420 --> 01:14:56,320
So we really would appreciate
folks' feedback and input

1671
01:14:56,320 --> 01:14:58,543
because this is a give and take.

1672
01:14:58,543 --> 01:15:01,010
And I think that came
through loud and clear

1673
01:15:01,010 --> 01:15:02,610
even from what
Cynthia was sharing.

1674
01:15:02,610 --> 01:15:05,570
We don't do any of
our work alone at MIT.

1675
01:15:05,570 --> 01:15:08,885
We understand that there are
local contexts and information

1676
01:15:08,885 --> 01:15:11,280
and knowledge we can be
getting at every stage.

1677
01:15:11,280 --> 01:15:13,512
So thank you, everyone,
for being here.

1678
01:15:13,512 --> 01:15:15,470
Looking forward to seeing
some of you next week

1679
01:15:15,470 --> 01:15:17,040
and others later in August.

1680
01:15:17,040 --> 01:15:17,930
Take care.

1681
01:15:17,930 --> 01:15:19,790
Bye, bye.

1682
01:15:19,790 --> 01:15:23,140
[MUSIC PLAYING]

1683
01:15:23,140 --> 01:15:30,000

