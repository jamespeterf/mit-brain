1
00:00:00,000 --> 00:00:01,190

2
00:00:01,190 --> 00:00:02,610
Good morning, everyone.

3
00:00:02,610 --> 00:00:03,720
I'm Mark Gorenberg.

4
00:00:03,720 --> 00:00:05,720
I'm Chair of the MIT
Corporation and Founder

5
00:00:05,720 --> 00:00:07,410
of Zetta Venture Partners.

6
00:00:07,410 --> 00:00:10,220
It's a pleasure to be with
all of you here today.

7
00:00:10,220 --> 00:00:12,470
I'm especially delighted
to have the honor

8
00:00:12,470 --> 00:00:15,320
to welcome Yann LeCun to MIT.

9
00:00:15,320 --> 00:00:20,070
Yann is the Chief AI Scientist
at Meta, professor at NYU,

10
00:00:20,070 --> 00:00:24,390
and one of the true pioneers of
modern artificial intelligence.

11
00:00:24,390 --> 00:00:28,190
His work on convolutional
networks and deep learning

12
00:00:28,190 --> 00:00:32,810
has transformed how
machines see and learn,

13
00:00:32,810 --> 00:00:36,080
and how they listen and
understand the world.

14
00:00:36,080 --> 00:00:38,520
And that earned him
the Turing Award,

15
00:00:38,520 --> 00:00:41,180
which is essentially the
Nobel Prize for Computing,

16
00:00:41,180 --> 00:00:46,610
in 2018 alongside Geoffrey
Hinton and Yoshua Bengio.

17
00:00:46,610 --> 00:00:48,900
By the way, if you
go to Wikipedia,

18
00:00:48,900 --> 00:00:53,180
you'll see Yann has received
so many honors and awards

19
00:00:53,180 --> 00:00:55,687
that it would take our
entire session if I

20
00:00:55,687 --> 00:00:56,520
walked through them.

21
00:00:56,520 --> 00:00:58,580
So I'm going to send you there.

22
00:00:58,580 --> 00:01:01,450
So instead, over
the next half hour,

23
00:01:01,450 --> 00:01:03,060
we'll explore some
of the foundations

24
00:01:03,060 --> 00:01:05,340
of this field and
future directions

25
00:01:05,340 --> 00:01:08,290
in what promises to be a
fascinating discussion.

26
00:01:08,290 --> 00:01:10,140
So Yann, thank
you for being here

27
00:01:10,140 --> 00:01:14,410
to open the MIT Generative AI
Impact Consortium Symposium.

28
00:01:14,410 --> 00:01:15,600
A real pleasure.

29
00:01:15,600 --> 00:01:19,560
So in your life, frankly,
you've always been ahead

30
00:01:19,560 --> 00:01:21,340
of the conventional wisdom.

31
00:01:21,340 --> 00:01:24,600
As far as I can tell,
you've always been right.

32
00:01:24,600 --> 00:01:28,120
And let's talk about that a
few times in this half hour.

33
00:01:28,120 --> 00:01:30,880
So, if we can go back and
start at the beginning.

34
00:01:30,880 --> 00:01:37,110
So you did your PhD back in 1987
at what now is the Sorbonne.

35
00:01:37,110 --> 00:01:41,820
I don't French, but in English,
it translates to "connectionist

36
00:01:41,820 --> 00:01:43,030
learning models."

37
00:01:43,030 --> 00:01:47,070
It essentially established
the backpropagation learning

38
00:01:47,070 --> 00:01:50,070
algorithm for neural nets.

39
00:01:50,070 --> 00:01:53,040
And frankly, at the
time, most people

40
00:01:53,040 --> 00:01:55,180
were chasing things
like expert systems.

41
00:01:55,180 --> 00:01:58,170
So what inspired
you to have the idea

42
00:01:58,170 --> 00:02:02,110
and how did this
propel your career?

43
00:02:02,110 --> 00:02:06,610
So maybe it's naivetÃ©
or ignorance when I was

44
00:02:06,610 --> 00:02:12,190
an undergrad, when I discovered
a little by accident that people

45
00:02:12,190 --> 00:02:15,770
in the '50s and
'60s, including MIT,

46
00:02:15,770 --> 00:02:20,120
actually had thought about the
problem of self-organization,

47
00:02:20,120 --> 00:02:25,570
which eventually gave rise to
early ideas that machines can

48
00:02:25,570 --> 00:02:26,240
learn.

49
00:02:26,240 --> 00:02:28,600
And I found this
idea fascinating

50
00:02:28,600 --> 00:02:31,030
because I think
biology is really

51
00:02:31,030 --> 00:02:35,120
an inspiration for a lot of
engineering feats that we do.

52
00:02:35,120 --> 00:02:37,390
And certainly in
nature, everything

53
00:02:37,390 --> 00:02:39,670
that lives is capable of
adaptation and everything

54
00:02:39,670 --> 00:02:41,600
with a nervous system can learn.

55
00:02:41,600 --> 00:02:46,790
And so maybe I thought I was
not smart enough to conceive--

56
00:02:46,790 --> 00:02:48,550
or that humans in
general were not

57
00:02:48,550 --> 00:02:51,122
smart enough to conceive
an intelligent system,

58
00:02:51,122 --> 00:02:53,330
that an intelligent system
will have to build itself,

59
00:02:53,330 --> 00:02:55,700
and that's kind of what
directed me to machine learning.

60
00:02:55,700 --> 00:03:00,360
And being naive and ignorant,
I didn't know that at the time,

61
00:03:00,360 --> 00:03:05,330
the main approaches to AI
were not at all interested

62
00:03:05,330 --> 00:03:06,570
in machine learning.

63
00:03:06,570 --> 00:03:08,250
It was, as you said,
expert systems.

64
00:03:08,250 --> 00:03:13,250
People basically transcribing
the knowledge of experts

65
00:03:13,250 --> 00:03:18,395
into rules and facts and then
hoping this would be useful.

66
00:03:18,395 --> 00:03:20,270
We're facing a little
bit of the same problem

67
00:03:20,270 --> 00:03:25,640
today with LLMs where the
transcription of human knowledge

68
00:03:25,640 --> 00:03:30,620
into a machine that we can
interact with or talk to now

69
00:03:30,620 --> 00:03:33,140
is done through learning,
but it's still very much

70
00:03:33,140 --> 00:03:35,900
a bottleneck of
transferring knowledge

71
00:03:35,900 --> 00:03:37,760
from humans to machines.

72
00:03:37,760 --> 00:03:42,208
So I discovered that this
was a very non-popular idea,

73
00:03:42,208 --> 00:03:44,750
but I felt very strongly that
this was the right thing to do.

74
00:03:44,750 --> 00:03:53,060
I had a very hard time finding
a PhD advisor because nobody

75
00:03:53,060 --> 00:03:54,060
was working on this.

76
00:03:54,060 --> 00:03:59,320
So I found a very nice
gentlemen, Maurice Milgram,

77
00:03:59,320 --> 00:04:00,960
who said, well, you
look smart enough.

78
00:04:00,960 --> 00:04:02,670
You don't need any
funding because you

79
00:04:02,670 --> 00:04:06,160
have some in the country,
from my engineering school,

80
00:04:06,160 --> 00:04:09,370
and he said, I can't
help you technically,

81
00:04:09,370 --> 00:04:10,980
but I'll sign the papers.

82
00:04:10,980 --> 00:04:11,760
Wow.

83
00:04:11,760 --> 00:04:12,490
That's awesome.

84
00:04:12,490 --> 00:04:15,400
So you went on to Toronto to
work with Geoffrey Hinton.

85
00:04:15,400 --> 00:04:20,100
You went on to Bell
Labs, you went on to NYU.

86
00:04:20,100 --> 00:04:22,620
The industry went
through the AI winter

87
00:04:22,620 --> 00:04:27,375
in the '90s and into the
2000s, and then in 2013,

88
00:04:27,375 --> 00:04:31,030
you joined Facebook,
created FAIR--

89
00:04:31,030 --> 00:04:32,940
basically Facebook AI Research.

90
00:04:32,940 --> 00:04:38,750
And changed the name
from neural nets,

91
00:04:38,750 --> 00:04:41,250
effectively it became-- the
industry started to recognize it

92
00:04:41,250 --> 00:04:42,460
as deep learning.

93
00:04:42,460 --> 00:04:48,330
So I went back and I
watched this seminal talk

94
00:04:48,330 --> 00:04:53,170
that you did at NeurIPS back
in 2016 on predictive learning,

95
00:04:53,170 --> 00:04:57,270
and you moved the industry again
because the industry was really

96
00:04:57,270 --> 00:04:59,570
very focused on
reinforcement learning.

97
00:04:59,570 --> 00:05:01,330
And you used the
analogy of the cake,

98
00:05:01,330 --> 00:05:05,260
and you move the industry to
what you term self-supervised

99
00:05:05,260 --> 00:05:05,780
learning.

100
00:05:05,780 --> 00:05:07,190
Can you tell us a
little bit about that?

101
00:05:07,190 --> 00:05:07,690
Yeah.

102
00:05:07,690 --> 00:05:11,210
Well, I think at the time,
we're talking 2016, '15,

103
00:05:11,210 --> 00:05:15,070
this was just when it was pretty
clear that deep learning had

104
00:05:15,070 --> 00:05:18,170
revolutionized computer
vision, speech recognition,

105
00:05:18,170 --> 00:05:21,010
and it was just about to
revolutionize natural language

106
00:05:21,010 --> 00:05:23,380
processing, but it was
still pretty early.

107
00:05:23,380 --> 00:05:27,700
And what the industry was
using was supervised learning.

108
00:05:27,700 --> 00:05:30,550
At the research level, a
lot of people believed--

109
00:05:30,550 --> 00:05:34,730
for example, DeepMind was really
completely engaged in this.

110
00:05:34,730 --> 00:05:37,990
They believed that the path
towards a more powerful AI

111
00:05:37,990 --> 00:05:40,160
system was through
reinforcement learning.

112
00:05:40,160 --> 00:05:43,630
And I never believed
in this because--

113
00:05:43,630 --> 00:05:46,190
and reinforcement learning
is the main component of this

114
00:05:46,190 --> 00:05:48,550
because it's incredibly
inefficient in terms

115
00:05:48,550 --> 00:05:51,320
of the number of trials
that a machine has to do.

116
00:05:51,320 --> 00:05:55,390
So I showed the analogy of
the cake, which actually

117
00:05:55,390 --> 00:05:57,170
is a little interior to this.

118
00:05:57,170 --> 00:06:02,600
This was in a symposium at
NYU that I organized in 2015

119
00:06:02,600 --> 00:06:06,890
that I first showed this
thing, but I tried to brainwash

120
00:06:06,890 --> 00:06:09,620
the community in 2016 with a--

121
00:06:09,620 --> 00:06:10,630
Everybody loves cake.

122
00:06:10,630 --> 00:06:11,130
Right.

123
00:06:11,130 --> 00:06:15,350
So the analogy is that if
you think of AI intelligence

124
00:06:15,350 --> 00:06:17,840
as a cake, the bulk
of the cake would

125
00:06:17,840 --> 00:06:20,640
have to be self-supervised,
unsupervised learning,

126
00:06:20,640 --> 00:06:22,950
predictive learning, as
I called it at the time.

127
00:06:22,950 --> 00:06:25,813
And then the icing on the cake
would be supervised learning,

128
00:06:25,813 --> 00:06:28,230
and then the cherry on the
cake is reinforcement learning.

129
00:06:28,230 --> 00:06:31,460
You want to use reinforcement
learning as little as possible

130
00:06:31,460 --> 00:06:32,720
because it's so inefficient.

131
00:06:32,720 --> 00:06:33,720
You don't have a choice.

132
00:06:33,720 --> 00:06:35,450
In the end, need to
have some sort of way

133
00:06:35,450 --> 00:06:38,920
of correcting yourself,
but really, it

134
00:06:38,920 --> 00:06:40,110
should be a last resort.

135
00:06:40,110 --> 00:06:43,410
And the thing I
advertised at the time--

136
00:06:43,410 --> 00:06:48,440
so this is 10 years ago, is
we should train a machine

137
00:06:48,440 --> 00:06:51,500
to capture the internal
dependency of the data

138
00:06:51,500 --> 00:06:53,780
without training it
for any task so that it

139
00:06:53,780 --> 00:06:57,060
can represent the world.

140
00:06:57,060 --> 00:06:59,430
And then on top
of this-- and that

141
00:06:59,430 --> 00:07:02,017
only requires observation
from unlabeled data.

142
00:07:02,017 --> 00:07:03,600
And then on top of
this, you can train

143
00:07:03,600 --> 00:07:06,870
a system using the
representation learned

144
00:07:06,870 --> 00:07:08,010
by the system.

145
00:07:08,010 --> 00:07:10,590
You can train a system to
solve a particular task

146
00:07:10,590 --> 00:07:11,710
or any particular task.

147
00:07:11,710 --> 00:07:16,950
So that whole idea of
self-supervised learning,

148
00:07:16,950 --> 00:07:20,410
some of us started working
on it in the 2000s,

149
00:07:20,410 --> 00:07:23,010
but it was under the radar, and
the techniques we were using

150
00:07:23,010 --> 00:07:25,110
were not that great at the time.

151
00:07:25,110 --> 00:07:26,500
We tried to apply this--

152
00:07:26,500 --> 00:07:31,210
I tried to apply this in the
late 2000s on video prediction.

153
00:07:31,210 --> 00:07:33,570
So just take a video and
try to train a system

154
00:07:33,570 --> 00:07:35,830
to predict what's going
to happen in the video.

155
00:07:35,830 --> 00:07:38,220
And it basically didn't work.

156
00:07:38,220 --> 00:07:41,640
But where it did work, beyond
our wildest expectations,

157
00:07:41,640 --> 00:07:43,660
was for natural
language understanding.

158
00:07:43,660 --> 00:07:45,810
So take a sequence of
symbols and then try

159
00:07:45,810 --> 00:07:48,910
to predict the next symbols,
and that works really well.

160
00:07:48,910 --> 00:07:54,460
Now why does it work for
text or symbol sequences

161
00:07:54,460 --> 00:07:55,930
and it doesn't work for video?

162
00:07:55,930 --> 00:08:00,320
And the answer is you can
never predict the word that

163
00:08:00,320 --> 00:08:02,840
comes after a sequence
of words, but you

164
00:08:02,840 --> 00:08:05,360
can predict a distribution
over all possible words

165
00:08:05,360 --> 00:08:10,400
in a dictionary, or tokens, if
you want to call them this way.

166
00:08:10,400 --> 00:08:12,620
Because there's only a
finite number of them,

167
00:08:12,620 --> 00:08:14,550
so it's easy to
represent a distribution.

168
00:08:14,550 --> 00:08:19,800
But when it comes to predicting
the future of a video,

169
00:08:19,800 --> 00:08:24,080
there's so many plausible
futures in a video

170
00:08:24,080 --> 00:08:26,330
that it's basically
impossible to represent

171
00:08:26,330 --> 00:08:28,410
all of those possibilities.

172
00:08:28,410 --> 00:08:31,850
If I take a video of this
room and I pan the camera

173
00:08:31,850 --> 00:08:35,335
and I stop here, and I ask the
system to complete the video,

174
00:08:35,335 --> 00:08:36,710
there's no way
you can figure out

175
00:08:36,710 --> 00:08:40,400
what everybody looks like here,
or how many people are sitting

176
00:08:40,400 --> 00:08:41,850
and what the size
of the room is.

177
00:08:41,850 --> 00:08:47,270
It can certainly not predict
the texture of the ground

178
00:08:47,270 --> 00:08:48,540
and things of that type.

179
00:08:48,540 --> 00:08:51,600
So there are things that are
just completely unpredictable.

180
00:08:51,600 --> 00:08:54,890
And if you train a system to try
to predict all those details,

181
00:08:54,890 --> 00:08:56,360
you're basically killing it.

182
00:08:56,360 --> 00:08:58,010
You're not-- it's
not going anywhere.

183
00:08:58,010 --> 00:09:04,370
So it took us a number of years,
going back maybe five years,

184
00:09:04,370 --> 00:09:08,000
five years ago, to realize
this was never going to work

185
00:09:08,000 --> 00:09:09,560
and we had to invent
new techniques.

186
00:09:09,560 --> 00:09:12,670
Well, so-- and self-supervised
learning, the work

187
00:09:12,670 --> 00:09:15,730
that you did there, the seminal
work there with the transformer

188
00:09:15,730 --> 00:09:20,020
technology paper, I mean, that's
the basis for almost all LLMs

189
00:09:20,020 --> 00:09:20,810
today.

190
00:09:20,810 --> 00:09:22,760
And so it's really changed
the whole industry.

191
00:09:22,760 --> 00:09:24,710
And in fact, if we
fast forward to today--

192
00:09:24,710 --> 00:09:30,650
so the world changed with
ChatGPT at the end of 2022,

193
00:09:30,650 --> 00:09:33,700
and then you came out
with Llama in early 2023.

194
00:09:33,700 --> 00:09:38,680
Now you have something like
a billion monthly users

195
00:09:38,680 --> 00:09:41,630
for Meta AI based
on Llama technology,

196
00:09:41,630 --> 00:09:44,300
well over a billion
downloads of Llama.

197
00:09:44,300 --> 00:09:47,057
So it has really
democratized AI.

198
00:09:47,057 --> 00:09:48,640
And I have a confession
to make, which

199
00:09:48,640 --> 00:09:52,870
is that I was not involved in
Llama at a technical level.

200
00:09:52,870 --> 00:09:55,070
This was-- the first
Llama was actually

201
00:09:55,070 --> 00:09:59,213
a bit of a pirate project.

202
00:09:59,213 --> 00:09:59,880
Oh, interesting.

203
00:09:59,880 --> 00:10:03,710
In parallel with a more official
LLM project at Meta in late

204
00:10:03,710 --> 00:10:04,760
2022--

205
00:10:04,760 --> 00:10:06,180
or mid-2022.

206
00:10:06,180 --> 00:10:09,080
And this was a small group of
about a dozen people in Paris

207
00:10:09,080 --> 00:10:10,970
who just decided
that they wanted

208
00:10:10,970 --> 00:10:13,920
to have a lightweight, efficient
LLM, and they built it.

209
00:10:13,920 --> 00:10:18,780
And that became the workhorse
eventually early 2023,

210
00:10:18,780 --> 00:10:22,680
and led Mark Zuckerberg to
create the GenAI organization,

211
00:10:22,680 --> 00:10:26,780
which is now called Meta
Superintelligence Lab,

212
00:10:26,780 --> 00:10:29,730
to basically productize it.

213
00:10:29,730 --> 00:10:32,600
But at a technical
level, I personally

214
00:10:32,600 --> 00:10:33,900
had very little to do with it.

215
00:10:33,900 --> 00:10:37,115
But it is always the skunk
works that emerges anyway.

216
00:10:37,115 --> 00:10:38,060
Oh, absolutely.

217
00:10:38,060 --> 00:10:40,520
And the CapEx that's going
in by these big companies now

218
00:10:40,520 --> 00:10:44,330
is 323 billion I think this
year, about the top four

219
00:10:44,330 --> 00:10:46,490
companies, including Meta.

220
00:10:46,490 --> 00:10:49,550
Yet even with all
that success, you've

221
00:10:49,550 --> 00:10:51,380
said that LLMs are effectively--

222
00:10:51,380 --> 00:10:56,250
I'll paraphrase, a deep end
for human-level intelligence.

223
00:10:56,250 --> 00:11:00,520
Can you clarify why all this
scaling can't solve the problem?

224
00:11:00,520 --> 00:11:03,960
Well, so that connects with
what I just talked about.

225
00:11:03,960 --> 00:11:08,400
Here is an interesting little
calculation you can make.

226
00:11:08,400 --> 00:11:11,640
A typical LLM-- so
something like Llama 3--

227
00:11:11,640 --> 00:11:17,430
is trained on the order of 30
trillion tokens, 310 to the 13.

228
00:11:17,430 --> 00:11:20,430
A token is typically
3 bytes, so that--

229
00:11:20,430 --> 00:11:25,450
about 10 to the 14 bytes
to train a typical LLM.

230
00:11:25,450 --> 00:11:28,543
It's more now because people use
synthetic data and everything.

231
00:11:28,543 --> 00:11:29,960
That's for the
pre-training phase.

232
00:11:29,960 --> 00:11:32,880

233
00:11:32,880 --> 00:11:36,962
It would take any of us on the
order of 400,000 years or half

234
00:11:36,962 --> 00:11:38,920
a million years, to read
through that material.

235
00:11:38,920 --> 00:11:42,660
It's all the publicly
available text on the internet.

236
00:11:42,660 --> 00:11:45,900
Now, compare this with
the amount of information

237
00:11:45,900 --> 00:11:48,880
that gets to a visual cortex in
the first four years of life.

238
00:11:48,880 --> 00:11:54,520
A four-year-old has been awake a
total of 16,000 hours, roughly.

239
00:11:54,520 --> 00:11:57,850
And there's about
1 byte per second

240
00:11:57,850 --> 00:12:00,010
going through our visual
cortex, through each fiber

241
00:12:00,010 --> 00:12:03,760
of our optical nerves, and
we have 2 million fibers.

242
00:12:03,760 --> 00:12:07,280
So that's about 2 megabytes
per second, times 16,000 hours,

243
00:12:07,280 --> 00:12:08,780
it's about 10 to the 14 bytes.

244
00:12:08,780 --> 00:12:11,080
A four-year-old has
seen as much data

245
00:12:11,080 --> 00:12:13,360
through vision as the
biggest LLMs trained

246
00:12:13,360 --> 00:12:16,150
on all the publicly
available text.

247
00:12:16,150 --> 00:12:19,280
And that tells you
that, first of all,

248
00:12:19,280 --> 00:12:23,680
we're missing
something big, that we

249
00:12:23,680 --> 00:12:29,860
need AI systems to learn from
natural, high-bandwidth sensory

250
00:12:29,860 --> 00:12:31,863
data like video.

251
00:12:31,863 --> 00:12:34,030
We're never going to get
to human-level intelligence

252
00:12:34,030 --> 00:12:35,210
by just training on text.

253
00:12:35,210 --> 00:12:36,920
This is not happening.

254
00:12:36,920 --> 00:12:39,700
Despite what you might
hear, for some people who

255
00:12:39,700 --> 00:12:41,830
are in the cult in
Silicon Valley who

256
00:12:41,830 --> 00:12:43,910
are going to tell
you, by next year,

257
00:12:43,910 --> 00:12:46,450
we're going to have
a data center--

258
00:12:46,450 --> 00:12:50,890
a country of geniuses
in a data center--

259
00:12:50,890 --> 00:12:53,120
I'm quoting, OK?

260
00:12:53,120 --> 00:12:53,880
I won't say who.

261
00:12:53,880 --> 00:12:57,260

262
00:12:57,260 --> 00:12:59,580
I mean, this is
just not happening.

263
00:12:59,580 --> 00:13:04,280
Yeah, you will have
useful artifacts that

264
00:13:04,280 --> 00:13:07,040
can help people in
their daily lives

265
00:13:07,040 --> 00:13:10,950
and maybe feel like they have
the intelligence of a PhD,

266
00:13:10,950 --> 00:13:14,270
but it's because they would
be regurgitating things that

267
00:13:14,270 --> 00:13:17,660
have been trained
on, and those systems

268
00:13:17,660 --> 00:13:21,390
won't have actual intelligence
of the type that we expect.

269
00:13:21,390 --> 00:13:23,990
Not just from humans, but
even from your house cat.

270
00:13:23,990 --> 00:13:28,220
So, I mean, house cats
have an understanding

271
00:13:28,220 --> 00:13:30,540
of the physical world that
is completely amazing,

272
00:13:30,540 --> 00:13:33,470
and they only have 2 billion--

273
00:13:33,470 --> 00:13:37,520
800 million neurons in their
brain, it's not that big.

274
00:13:37,520 --> 00:13:39,703
And they certainly have
a very good understanding

275
00:13:39,703 --> 00:13:40,620
of the physical world.

276
00:13:40,620 --> 00:13:44,250
They can do complex
planning of complex actions.

277
00:13:44,250 --> 00:13:47,310
And we are nowhere
near matching this.

278
00:13:47,310 --> 00:13:50,040
So that's what
I'm interested in.

279
00:13:50,040 --> 00:13:51,510
How do we bridge that gap?

280
00:13:51,510 --> 00:13:55,540
How do we get systems to learn
models of the physical world?

281
00:13:55,540 --> 00:13:57,330
And that will require
new architectures

282
00:13:57,330 --> 00:13:58,810
that are not generative.

283
00:13:58,810 --> 00:14:01,500
So I'm telling people that
don't work on generative models,

284
00:14:01,500 --> 00:14:05,670
so they will think I'm crazy,
but I really believe this.

285
00:14:05,670 --> 00:14:08,110
And as you say, I'm trying
to be ahead of the game.

286
00:14:08,110 --> 00:14:08,255
Right.

287
00:14:08,255 --> 00:14:10,030
So you're ahead of the
conventional wisdom again.

288
00:14:10,030 --> 00:14:11,110
You've already
started working on it.

289
00:14:11,110 --> 00:14:12,100
You call it JEPA.

290
00:14:12,100 --> 00:14:12,790
JEPA, yeah.

291
00:14:12,790 --> 00:14:14,850
So tell us, how is
JEPA fundamentally

292
00:14:14,850 --> 00:14:16,270
different from LLMs?

293
00:14:16,270 --> 00:14:20,623
OK, so JEPA stands for
Joint Embedding Predictive

294
00:14:20,623 --> 00:14:23,040
Architecture, and we've been
working on this kind of stuff

295
00:14:23,040 --> 00:14:24,940
for five years or so.

296
00:14:24,940 --> 00:14:27,240
I published a long
paper, which is

297
00:14:27,240 --> 00:14:30,750
kind of a vision paper for
where I think AI research should

298
00:14:30,750 --> 00:14:32,760
go over the next 10 years.

299
00:14:32,760 --> 00:14:35,380
I published this in
2022, it's on OpenReview.

300
00:14:35,380 --> 00:14:41,400
And it's called "A Path Towards
Autonomous Machine Intelligence"

301
00:14:41,400 --> 00:14:44,830
where I basically lay the
groundwork for all of this.

302
00:14:44,830 --> 00:14:46,620
And since then, we've
been making progress

303
00:14:46,620 --> 00:14:49,620
towards that plan with
many of my colleagues

304
00:14:49,620 --> 00:14:51,130
at Meta and at NYU.

305
00:14:51,130 --> 00:14:53,620
And if you type "joint
embedding architectures"

306
00:14:53,620 --> 00:14:55,370
between quotes on
Google Scholar,

307
00:14:55,370 --> 00:14:57,860
you'll get on the
order of 750 hits.

308
00:14:57,860 --> 00:15:00,460
So there's a lot of people
working on this, mostly

309
00:15:00,460 --> 00:15:01,580
in academia.

310
00:15:01,580 --> 00:15:05,170
People have been very quick
to dismiss the contributions

311
00:15:05,170 --> 00:15:07,750
of academia because
all of AI research

312
00:15:07,750 --> 00:15:09,160
is in the hands of industry now.

313
00:15:09,160 --> 00:15:10,190
That's false.

314
00:15:10,190 --> 00:15:11,290
We don't do that here.

315
00:15:11,290 --> 00:15:13,870
No.

316
00:15:13,870 --> 00:15:17,770
But academia basically
tends to work

317
00:15:17,770 --> 00:15:20,372
on the next-generation
thing that industry does not

318
00:15:20,372 --> 00:15:22,330
realize it's going to
have a big impact on what

319
00:15:22,330 --> 00:15:24,700
they do in five to 10 years.

320
00:15:24,700 --> 00:15:28,720
So, OK, so what is the
difference between JEPA and LLMs

321
00:15:28,720 --> 00:15:30,110
or generative architecture?

322
00:15:30,110 --> 00:15:33,310
Generative architecture,
you give it-- you

323
00:15:33,310 --> 00:15:36,740
take a piece of data, let's say
a sequence of words of text,

324
00:15:36,740 --> 00:15:41,150
you corrupt it in some way
by removing some words,

325
00:15:41,150 --> 00:15:41,990
for example.

326
00:15:41,990 --> 00:15:45,280
And then you train a
neural net, big neural net,

327
00:15:45,280 --> 00:15:47,030
to predict the words
that are missing.

328
00:15:47,030 --> 00:15:50,483
In the case of LLMs, this is
a-- or the GPT architecture

329
00:15:50,483 --> 00:15:53,150
in particular, there's a trick,
which is that you don't actually

330
00:15:53,150 --> 00:15:55,380
need to corrupt the text.

331
00:15:55,380 --> 00:15:57,690
The architecture is
such that it is causal,

332
00:15:57,690 --> 00:16:01,030
so to predict a particular word,
because of the architecture,

333
00:16:01,030 --> 00:16:02,780
the system can only
look at the words that

334
00:16:02,780 --> 00:16:04,020
are to the left of it.

335
00:16:04,020 --> 00:16:05,600
And so implicitly,
when you train

336
00:16:05,600 --> 00:16:08,060
the system to just
reconstruct the input

337
00:16:08,060 --> 00:16:10,460
sequence on its
output, you implicitly

338
00:16:10,460 --> 00:16:13,220
train it to predict
the next token.

339
00:16:13,220 --> 00:16:16,740
And it's very efficient, it's
parallelizable, and everything.

340
00:16:16,740 --> 00:16:18,660
OK, so that's
generative architecture.

341
00:16:18,660 --> 00:16:22,188
It works because the
tokens are discrete,

342
00:16:22,188 --> 00:16:23,730
there's only a finite
number of them,

343
00:16:23,730 --> 00:16:26,360
and you can train the system
to produce a distribution

344
00:16:26,360 --> 00:16:27,950
over all possible tokens.

345
00:16:27,950 --> 00:16:29,730
That's how LLMs are done.

346
00:16:29,730 --> 00:16:32,350
And then you can use it to
do autoregressive prediction.

347
00:16:32,350 --> 00:16:34,310
Like, you have it
predict the next token,

348
00:16:34,310 --> 00:16:36,560
shift that in into
the input, and now

349
00:16:36,560 --> 00:16:39,180
you can predict the second
token, shift that in, et cetera.

350
00:16:39,180 --> 00:16:40,910
That's autoregressive
prediction.

351
00:16:40,910 --> 00:16:44,720
OK, now what I'm arguing for
is you can't do this for video

352
00:16:44,720 --> 00:16:47,630
because if you tokenize video,
they're still going to be a lot

353
00:16:47,630 --> 00:16:49,890
of things that are
just not predictable,

354
00:16:49,890 --> 00:16:53,850
details like what
everyone looks like here,

355
00:16:53,850 --> 00:16:56,460
that you just cannot predict.

356
00:16:56,460 --> 00:17:00,460
So the idea of JEPA is
you take your video,

357
00:17:00,460 --> 00:17:03,630
but you encode it into
a representation space

358
00:17:03,630 --> 00:17:05,980
where a lot of details
are eliminated,

359
00:17:05,980 --> 00:17:07,890
and then this
autoregressive prediction

360
00:17:07,890 --> 00:17:10,210
that we were previously
doing in input space,

361
00:17:10,210 --> 00:17:12,730
now you're doing it in
this representation space.

362
00:17:12,730 --> 00:17:14,319
Now the trick is--

363
00:17:14,319 --> 00:17:16,170
and the reason why it
didn't pop up earlier

364
00:17:16,170 --> 00:17:19,770
is that to train the
encoder and the predictor

365
00:17:19,770 --> 00:17:21,819
simultaneously is very tricky.

366
00:17:21,819 --> 00:17:29,850
The reason being that it's
very easy for the predictor

367
00:17:29,850 --> 00:17:32,590
to basically force the
encoder to not do anything,

368
00:17:32,590 --> 00:17:36,540
to ignore the input and
produce a constant output

369
00:17:36,540 --> 00:17:38,490
representation, another
prediction problem

370
00:17:38,490 --> 00:17:41,770
becomes trivial, but
it's not a good solution.

371
00:17:41,770 --> 00:17:46,110
And so you have to find ways to
trick the system into carrying

372
00:17:46,110 --> 00:17:47,820
as much information
about the input as

373
00:17:47,820 --> 00:17:51,910
possible in the representation,
but at the same time,

374
00:17:51,910 --> 00:17:54,820
eliminating some of the details
that are not predictable.

375
00:17:54,820 --> 00:17:59,160
So the system finds a
trade-off between carrying

376
00:17:59,160 --> 00:18:01,160
as much information as
possible about the input,

377
00:18:01,160 --> 00:18:03,500
but only the stuff
that it can predict.

378
00:18:03,500 --> 00:18:05,690
And that's the basic
concept of JEPA.

379
00:18:05,690 --> 00:18:09,327
It's not-- in terms
of architecture,

380
00:18:09,327 --> 00:18:11,410
there is the encoder, which
is different from what

381
00:18:11,410 --> 00:18:12,730
you see in LLMs.

382
00:18:12,730 --> 00:18:17,690
And the trick is in finding good
training algorithms, basically,

383
00:18:17,690 --> 00:18:21,430
or procedures or regularizers
to get the thing to learn

384
00:18:21,430 --> 00:18:23,060
interesting representations.

385
00:18:23,060 --> 00:18:28,390
Now, it wasn't clear
until fairly recently

386
00:18:28,390 --> 00:18:31,780
whether this type of
joint embedding method

387
00:18:31,780 --> 00:18:34,100
to learn representations
of natural data,

388
00:18:34,100 --> 00:18:37,600
like images and video,
would ultimately

389
00:18:37,600 --> 00:18:40,270
be better than techniques that
are trained to reconstruct

390
00:18:40,270 --> 00:18:41,590
at the pixel level.

391
00:18:41,590 --> 00:18:45,340
But at FAIR, we had
basically an A-B comparison

392
00:18:45,340 --> 00:18:50,000
where a big group was working
on a project called MAE, Masked

393
00:18:50,000 --> 00:18:55,140
Autoencoder, and a
video version of it,

394
00:18:55,140 --> 00:18:57,210
which was basically
take an image,

395
00:18:57,210 --> 00:18:59,450
corrupt it, and then train
some gigantic neural net

396
00:18:59,450 --> 00:19:03,600
to reconstruct the full
image, or the full video.

397
00:19:03,600 --> 00:19:06,420
And it didn't quite work.

398
00:19:06,420 --> 00:19:11,600
And in fact, at MIT, you
can ask someone about this.

399
00:19:11,600 --> 00:19:12,260
Kaiming He.

400
00:19:12,260 --> 00:19:14,970
He was one of the
principals in this project.

401
00:19:14,970 --> 00:19:20,070
And he was a little disappointed
by the results, and in the end,

402
00:19:20,070 --> 00:19:23,210
he reoriented his research,
left FAIR, and joined MIT

403
00:19:23,210 --> 00:19:25,710
on the faculty,
associate professor,

404
00:19:25,710 --> 00:19:28,490
and now he's at MIT at CSAIL.

405
00:19:28,490 --> 00:19:30,242
So in parallel, there
were other projects

406
00:19:30,242 --> 00:19:32,450
that tried to train the
joint embedding architectures

407
00:19:32,450 --> 00:19:35,270
without attempting
to reconstruct-- so

408
00:19:35,270 --> 00:19:37,080
non-generative architectures.

409
00:19:37,080 --> 00:19:39,275
And they turned out
to work much better.

410
00:19:39,275 --> 00:19:39,775
OK.

411
00:19:39,775 --> 00:19:42,230
And so it was clear
empirical evidence

412
00:19:42,230 --> 00:19:44,510
that, for natural
sensory data, you just

413
00:19:44,510 --> 00:19:47,220
don't want to use
generative architectures.

414
00:19:47,220 --> 00:19:51,510
And now we have also data that
shows that those systems surpass

415
00:19:51,510 --> 00:19:54,820
in performance, even
supervised models in images,

416
00:19:54,820 --> 00:19:56,740
which wasn't the case
until about a year ago.

417
00:19:56,740 --> 00:19:57,240
Wow.

418
00:19:57,240 --> 00:20:00,040
Which applications are starting
to show that early promise?

419
00:20:00,040 --> 00:20:03,000
Well, there is an
open-source system

420
00:20:03,000 --> 00:20:06,280
put out by some of my colleagues
at FAIR in Paris called DINO.

421
00:20:06,280 --> 00:20:07,830
I mean, they
pronounce it "deano."

422
00:20:07,830 --> 00:20:13,140
It's DINO, but they're
French, so they say "dino."

423
00:20:13,140 --> 00:20:17,160
And this is DINOv3,
the third version just

424
00:20:17,160 --> 00:20:19,630
came out a month or two ago.

425
00:20:19,630 --> 00:20:24,900
And so this is basically a
generic self-supervised vision

426
00:20:24,900 --> 00:20:28,590
encoder, image
encoder, which you

427
00:20:28,590 --> 00:20:30,760
can use for all kinds of
downstream applications,

428
00:20:30,760 --> 00:20:32,730
and there are basically
hundreds of papers

429
00:20:32,730 --> 00:20:36,210
of using the DINO
system, previous versions

430
00:20:36,210 --> 00:20:38,010
and current ones, for
all kinds of stuff.

431
00:20:38,010 --> 00:20:41,290
Medical image analysis,
biological image analysis,

432
00:20:41,290 --> 00:20:45,460
astronomy, and then just
everyday computer vision.

433
00:20:45,460 --> 00:20:50,770
So I think this is really the
self-supervised learning model.

434
00:20:50,770 --> 00:20:54,680
Took a long time, but it
finally won the battle

435
00:20:54,680 --> 00:20:58,190
if you want for image
and video representation.

436
00:20:58,190 --> 00:21:00,400
Another project that I was
more directly involved in

437
00:21:00,400 --> 00:21:03,280
is called V-JEPA, that
stands for Video JEPA.

438
00:21:03,280 --> 00:21:05,830
And it's done by
a group of people

439
00:21:05,830 --> 00:21:09,950
in Montreal, Paris, and
New York that I work with.

440
00:21:09,950 --> 00:21:13,040
And this system is
trained from video.

441
00:21:13,040 --> 00:21:16,630
So you take a video, corrupted
by masking a big chunk of it,

442
00:21:16,630 --> 00:21:20,170
and then train an architecture--
so run the full video

443
00:21:20,170 --> 00:21:22,480
and it partially maps
one through two encoders

444
00:21:22,480 --> 00:21:24,470
that are essentially identical.

445
00:21:24,470 --> 00:21:26,590
And then simultaneously
train a predictor

446
00:21:26,590 --> 00:21:28,840
to predict the representation
of the full video

447
00:21:28,840 --> 00:21:31,760
from the partially
masked, corrupted one.

448
00:21:31,760 --> 00:21:33,620
OK, that's the first
phase of training.

449
00:21:33,620 --> 00:21:37,000
We've trained this
on an amount of video

450
00:21:37,000 --> 00:21:40,237
that corresponds to
about a century of video.

451
00:21:40,237 --> 00:21:41,320
This is an insane amount--

452
00:21:41,320 --> 00:21:42,160
Century of video.

453
00:21:42,160 --> 00:21:42,660
Wow.

454
00:21:42,660 --> 00:21:46,310
Yeah, So it's not as efficient
as a four-year-old, clearly,

455
00:21:46,310 --> 00:21:51,050
but those systems
basically can show

456
00:21:51,050 --> 00:21:53,160
that they've learned a
little bit of common sense.

457
00:21:53,160 --> 00:21:57,270
If you show them a video where
something impossible occurs,

458
00:21:57,270 --> 00:22:00,050
like an object spontaneously
disappears or changes

459
00:22:00,050 --> 00:22:02,293
shape or something,
the prediction error

460
00:22:02,293 --> 00:22:03,210
goes through the roof.

461
00:22:03,210 --> 00:22:05,300
And so they can tell you
something really unusual

462
00:22:05,300 --> 00:22:08,910
occurred that I
don't understand.

463
00:22:08,910 --> 00:22:13,340
And so that's the first sign
of a self-supervised learning

464
00:22:13,340 --> 00:22:16,380
system, having acquired
a bit of common sense.

465
00:22:16,380 --> 00:22:19,020
And you're already seeing some
early success in robotics?

466
00:22:19,020 --> 00:22:20,040
Yeah, that's right.

467
00:22:20,040 --> 00:22:22,880
And so you can have a
second phase of training

468
00:22:22,880 --> 00:22:28,110
where you fine-tune a predictor
which is action-conditioned.

469
00:22:28,110 --> 00:22:30,930
And now what you have out
of this is a world model.

470
00:22:30,930 --> 00:22:32,510
So what is a world model?

471
00:22:32,510 --> 00:22:36,260
Given a representation of the
state of the world at time T,

472
00:22:36,260 --> 00:22:40,080
and given an action that an
agent would imagine taking,

473
00:22:40,080 --> 00:22:42,650
can you predict the state
of the world resulting

474
00:22:42,650 --> 00:22:44,515
from taking this action?

475
00:22:44,515 --> 00:22:45,390
That's a world model.

476
00:22:45,390 --> 00:22:48,540
If you have a system
with such a world model,

477
00:22:48,540 --> 00:22:50,520
you can use it for planning.

478
00:22:50,520 --> 00:22:54,120
You can imagine taking
a sequence of actions,

479
00:22:54,120 --> 00:22:56,160
and then using the
world model, predict

480
00:22:56,160 --> 00:22:58,590
what the outcome of
this sequence of actions

481
00:22:58,590 --> 00:22:59,620
is going to be.

482
00:22:59,620 --> 00:23:01,320
And then you can
have a cost function

483
00:23:01,320 --> 00:23:04,500
that measures to what
extent a particular task has

484
00:23:04,500 --> 00:23:06,150
been achieved.

485
00:23:06,150 --> 00:23:07,720
Have you made
coffee or something?

486
00:23:07,720 --> 00:23:13,150
And then using basically what
amounts to optimization methods,

487
00:23:13,150 --> 00:23:17,250
search for a sequence of actions
that optimize this objective,

488
00:23:17,250 --> 00:23:18,400
minimizes this objective.

489
00:23:18,400 --> 00:23:20,910
This is classical planning
and optimal control,

490
00:23:20,910 --> 00:23:24,450
except that the model,
the dynamical model

491
00:23:24,450 --> 00:23:27,027
of the environment
that we use, is learned

492
00:23:27,027 --> 00:23:28,860
through self-supervised
learning, as opposed

493
00:23:28,860 --> 00:23:31,600
to written down as a
bunch of equations,

494
00:23:31,600 --> 00:23:34,840
as is done in robotics or
classical optimal control.

495
00:23:34,840 --> 00:23:36,610
So this is really
what we're after,

496
00:23:36,610 --> 00:23:41,670
and we've shown that we can
do this with representations

497
00:23:41,670 --> 00:23:44,400
of the state of the world either
derived from things like DINO,

498
00:23:44,400 --> 00:23:49,360
or learn from scratch, or
on top of V-JEPA V-JEPA 2.

499
00:23:49,360 --> 00:23:53,260
And you can show that you
can use this to get a robot

500
00:23:53,260 --> 00:23:55,285
to accomplish a task zero shot.

501
00:23:55,285 --> 00:23:57,410
You don't have to train it
to accomplish this task.

502
00:23:57,410 --> 00:23:58,940
There's no training whatsoever.

503
00:23:58,940 --> 00:24:00,380
No RL.

504
00:24:00,380 --> 00:24:03,100
The training is completely
self-supervised.

505
00:24:03,100 --> 00:24:06,730
And in the end, the system
has a good enough world model

506
00:24:06,730 --> 00:24:10,180
that it can imagine how to
accomplish a task without ever

507
00:24:10,180 --> 00:24:11,960
being trained to
accomplish this task.

508
00:24:11,960 --> 00:24:12,460
Wow.

509
00:24:12,460 --> 00:24:14,290
So I saw that you
had one robot that I

510
00:24:14,290 --> 00:24:17,720
think it trained on 62
hours of tasks on its own,

511
00:24:17,720 --> 00:24:19,690
and then it did
self-supervised work.

512
00:24:19,690 --> 00:24:23,960
So that training of 62 hours
was not for a particular task.

513
00:24:23,960 --> 00:24:26,830
It was basically, here is the
state of the world at time T,

514
00:24:26,830 --> 00:24:28,450
here is an action,
and here is what

515
00:24:28,450 --> 00:24:31,640
the world is going to look like
as a result of this action.

516
00:24:31,640 --> 00:24:35,200
You can do this with simulated
data, with a robot simulator,

517
00:24:35,200 --> 00:24:39,130
or with real data where you
have a robot arm moving around

518
00:24:39,130 --> 00:24:41,290
and you know what
action was taken.

519
00:24:41,290 --> 00:24:45,140
So this notion of
world model, I think,

520
00:24:45,140 --> 00:24:48,960
which I already talked about
in my 2016 Keynote at NeurIPS,

521
00:24:48,960 --> 00:24:52,380
I think is going to be a key
component of future AI systems.

522
00:24:52,380 --> 00:24:54,500
And my prediction has been--

523
00:24:54,500 --> 00:24:58,370
I've been not making friends
in various corners of Silicon

524
00:24:58,370 --> 00:25:00,710
Valley, including
at Meta, saying

525
00:25:00,710 --> 00:25:02,600
that within three
to five years, this

526
00:25:02,600 --> 00:25:06,480
will be the dominant model
for AI architectures,

527
00:25:06,480 --> 00:25:09,168
and nobody in their right mind
would use LLMs of the type

528
00:25:09,168 --> 00:25:09,960
that we have today.

529
00:25:09,960 --> 00:25:10,460
Right.

530
00:25:10,460 --> 00:25:13,160
So that'll propel this to
be the decade of robotics

531
00:25:13,160 --> 00:25:14,490
basically coming up.

532
00:25:14,490 --> 00:25:14,990
Yeah.

533
00:25:14,990 --> 00:25:18,110
But on that point,
there is a large number

534
00:25:18,110 --> 00:25:19,670
of robotics companies
that have been

535
00:25:19,670 --> 00:25:23,060
created over the last few
years building humanoid robots.

536
00:25:23,060 --> 00:25:26,510
The big secret of the industry
is that none of those companies

537
00:25:26,510 --> 00:25:29,570
has any idea how to make
those robots smart enough

538
00:25:29,570 --> 00:25:30,980
to be useful--

539
00:25:30,980 --> 00:25:33,870
or I should say, smart enough
to be generally useful.

540
00:25:33,870 --> 00:25:37,010
We can train those robots
for particular tasks,

541
00:25:37,010 --> 00:25:39,180
maybe in manufacturing
and things like this,

542
00:25:39,180 --> 00:25:41,570
but your domestic
robot, there is a bunch

543
00:25:41,570 --> 00:25:43,070
of breakthroughs
that need to arrive

544
00:25:43,070 --> 00:25:44,670
in AI before that's possible.

545
00:25:44,670 --> 00:25:46,770
So the future of a
lot of those companies

546
00:25:46,770 --> 00:25:48,720
essentially depends
on whether we're

547
00:25:48,720 --> 00:25:50,615
going to make progress,
significant progress,

548
00:25:50,615 --> 00:25:51,990
towards those kind
of world model

549
00:25:51,990 --> 00:25:53,738
planning-type architectures.

550
00:25:53,738 --> 00:25:55,780
It's OK, we're not worried
about those companies.

551
00:25:55,780 --> 00:25:57,270
We're a huge believer
in entrepreneurship

552
00:25:57,270 --> 00:25:57,978
here, by the way.

553
00:25:57,978 --> 00:26:00,720
We're going to have a whole new
set of companies that are going

554
00:26:00,720 --> 00:26:03,330
to come out of these new ideas.

555
00:26:03,330 --> 00:26:06,250
The other thing is that
you've been a huge optimist.

556
00:26:06,250 --> 00:26:08,580
There's a lot of
fearful people about AI

557
00:26:08,580 --> 00:26:10,620
today, the ethics
of AI, et cetera,

558
00:26:10,620 --> 00:26:12,990
and you've been a huge optimist.

559
00:26:12,990 --> 00:26:14,820
Why do you think
these systems are not

560
00:26:14,820 --> 00:26:16,840
going to escape our control?

561
00:26:16,840 --> 00:26:20,230
What's your gestalt
on all of this?

562
00:26:20,230 --> 00:26:24,520
OK, so the overall
architecture of AI systems

563
00:26:24,520 --> 00:26:30,550
I've been advocating is one
that I call objective-driven.

564
00:26:30,550 --> 00:26:33,390
So this idea that you're
going to have a system that

565
00:26:33,390 --> 00:26:35,230
has some mental
model of the world,

566
00:26:35,230 --> 00:26:38,140
it's going to plan a sequence
of actions to arrive--

567
00:26:38,140 --> 00:26:43,440
to satisfy an objective,
to accomplish a task.

568
00:26:43,440 --> 00:26:45,300
And by construction,
that system can

569
00:26:45,300 --> 00:26:48,000
do nothing else but producing
a sequence of actions

570
00:26:48,000 --> 00:26:49,930
that optimizes that objective.

571
00:26:49,930 --> 00:26:54,540
Now in that objective, you
can hardwire guardrails.

572
00:26:54,540 --> 00:26:56,790
So let's say you have
a new domestic robot

573
00:26:56,790 --> 00:27:00,610
and you ask the domestic
robot, get me a coffee.

574
00:27:00,610 --> 00:27:02,520
And it gets to the
coffee machine,

575
00:27:02,520 --> 00:27:04,450
and there is someone
standing in front of it,

576
00:27:04,450 --> 00:27:07,590
you don't want that robot
to slash and kill the person

577
00:27:07,590 --> 00:27:11,460
in front of a coffee machine
because it's only objective will

578
00:27:11,460 --> 00:27:12,610
be to fetch you coffee.

579
00:27:12,610 --> 00:27:19,890
And so-- and by the
way, this is an example

580
00:27:19,890 --> 00:27:25,950
that people like Stuart Russell
use as an example of how you

581
00:27:25,950 --> 00:27:27,280
could build dangerous machines.

582
00:27:27,280 --> 00:27:29,520
And I've always dismissed
this, and he always

583
00:27:29,520 --> 00:27:31,110
thought I was stupid.

584
00:27:31,110 --> 00:27:34,120
He actually called me stupid
publicly in some interviews,

585
00:27:34,120 --> 00:27:36,622
so I'm used to it.

586
00:27:36,622 --> 00:27:38,620
A lot of people call
me stupid, that's fine.

587
00:27:38,620 --> 00:27:40,620
We'll have to have the
two of you back together.

588
00:27:40,620 --> 00:27:42,120
That's OK, but go ahead.

589
00:27:42,120 --> 00:27:43,540
At some point, perhaps.

590
00:27:43,540 --> 00:27:45,640
But the point is, you can--

591
00:27:45,640 --> 00:27:48,880
if you put guardrails in
the objective functions

592
00:27:48,880 --> 00:27:52,050
of the system,
hardware guardrails,

593
00:27:52,050 --> 00:27:53,175
that can be very low-level.

594
00:27:53,175 --> 00:27:54,232
Like you can have a--

595
00:27:54,232 --> 00:27:55,750
I don't know, a
domestic robot that

596
00:27:55,750 --> 00:27:58,780
can cook, you can have a very
low-level guardrail that says

597
00:27:58,780 --> 00:28:01,060
don't flail your arms if
there are people around

598
00:28:01,060 --> 00:28:04,310
and you have a knife in your
hands, things like that.

599
00:28:04,310 --> 00:28:07,360
So, we're going to have to
design those guardrails,

600
00:28:07,360 --> 00:28:10,000
but by construction,
the system will not

601
00:28:10,000 --> 00:28:11,510
be able to escape
those guardrails,

602
00:28:11,510 --> 00:28:14,810
it will have to satisfy them.

603
00:28:14,810 --> 00:28:17,530
And so I'm not saying that
designing those guardrails

604
00:28:17,530 --> 00:28:21,560
is going to be an easy task, but
we are used to this with humans.

605
00:28:21,560 --> 00:28:22,550
We design laws.

606
00:28:22,550 --> 00:28:24,640
Laws are basically
objective functions

607
00:28:24,640 --> 00:28:28,490
that change the landscape of
what actions you can take.

608
00:28:28,490 --> 00:28:31,960
The cost of each
action we're taking,

609
00:28:31,960 --> 00:28:36,970
we're making-- we make laws to
basically align human behavior

610
00:28:36,970 --> 00:28:38,150
with the common good.

611
00:28:38,150 --> 00:28:40,390
We even do this with
superhuman entities

612
00:28:40,390 --> 00:28:43,820
called corporations with
limited success, I admit.

613
00:28:43,820 --> 00:28:45,780
So, I mean, we're used to this.

614
00:28:45,780 --> 00:28:48,510
We've been used to this
problem for millennia.

615
00:28:48,510 --> 00:28:52,940
And I would argue, it's not a
more complex and challenging

616
00:28:52,940 --> 00:28:57,230
problem than, I don't
know, designing turbojets

617
00:28:57,230 --> 00:29:01,290
that can fly you halfway around
the world in complete safety.

618
00:29:01,290 --> 00:29:03,240
I mean, we can do amazing
feats of this type,

619
00:29:03,240 --> 00:29:04,620
so I'm really not worried.

620
00:29:04,620 --> 00:29:06,120
I'm not saying it's
an easy problem.

621
00:29:06,120 --> 00:29:06,740
I'm just not--

622
00:29:06,740 --> 00:29:07,670
I don't think--

623
00:29:07,670 --> 00:29:09,940
But you're not worried it'll get
out and be out of our control?

624
00:29:09,940 --> 00:29:10,440
Right.

625
00:29:10,440 --> 00:29:14,370
So I have like a hundred
more questions to ask you,

626
00:29:14,370 --> 00:29:16,200
but we're almost out of time.

627
00:29:16,200 --> 00:29:19,850
I do want to say, I know
that you should definitely

628
00:29:19,850 --> 00:29:22,080
come back to MIT and
spend a lot more time.

629
00:29:22,080 --> 00:29:24,570
I know that you love sailing.

630
00:29:24,570 --> 00:29:26,595
We're here, we have
a great crew team.

631
00:29:26,595 --> 00:29:28,470
I'm sure there are some
here in the audience,

632
00:29:28,470 --> 00:29:30,980
and we're here on the Charles.

633
00:29:30,980 --> 00:29:32,910
You and your brother
build airplanes.

634
00:29:32,910 --> 00:29:35,100
We have the number one
Aero & Astro Department,

635
00:29:35,100 --> 00:29:36,230
so please come back.

636
00:29:36,230 --> 00:29:38,040
And I know you're
a huge fan of jazz.

637
00:29:38,040 --> 00:29:38,540
Yes.

638
00:29:38,540 --> 00:29:39,090
Right?

639
00:29:39,090 --> 00:29:42,205
And we just built a whole new
Music and Theater Arts Building.

640
00:29:42,205 --> 00:29:43,500
It's one of the best--

641
00:29:43,500 --> 00:29:45,630
It's hard to beat New
York on that dimension.

642
00:29:45,630 --> 00:29:49,990
Ah, we're-- we're willing
to sit and compete.

643
00:29:49,990 --> 00:29:51,690
But let me ask you
the last question

644
00:29:51,690 --> 00:29:53,710
and just go full circle.

645
00:29:53,710 --> 00:29:57,180
So we started with your
PhD you were working on

646
00:29:57,180 --> 00:29:59,010
roughly 40 years ago.

647
00:29:59,010 --> 00:30:04,290
If you were an MIT PhD student
today, what would you work on?

648
00:30:04,290 --> 00:30:06,570
This is a question I get a lot.

649
00:30:06,570 --> 00:30:08,830
What would you study if
you were an undergrad?

650
00:30:08,830 --> 00:30:11,070
What would you work on?

651
00:30:11,070 --> 00:30:15,400
I mean, I think for
the last 40, 50 years,

652
00:30:15,400 --> 00:30:21,130
I think discovering the
mysteries of human intelligence,

653
00:30:21,130 --> 00:30:25,470
and MIT is very
engineering-focused.

654
00:30:25,470 --> 00:30:29,460
As an engineer myself, I think
the best way to understand

655
00:30:29,460 --> 00:30:31,240
something is to build it.

656
00:30:31,240 --> 00:30:35,310
Richard Feynman said
that, actually, as well.

657
00:30:35,310 --> 00:30:37,290
Well, he didn't mean build it--

658
00:30:37,290 --> 00:30:39,090
build a physical
artifact, but he

659
00:30:39,090 --> 00:30:44,780
means deriving the ideas is
yourself and appropriating them.

660
00:30:44,780 --> 00:30:50,440
And so I think that if you are
a ambitious young scientist

661
00:30:50,440 --> 00:30:54,590
or engineer, there are three
big questions to work on.

662
00:30:54,590 --> 00:30:56,780
One is, what's the
universe made of?

663
00:30:56,780 --> 00:30:57,923
What's life all about?

664
00:30:57,923 --> 00:30:59,090
And how does the brain work?

665
00:30:59,090 --> 00:31:01,720
So three scientific questions.

666
00:31:01,720 --> 00:31:03,680
And those on the
engineering side of it,

667
00:31:03,680 --> 00:31:06,755
at least for the last
one, is how do we

668
00:31:06,755 --> 00:31:07,880
build intelligent machines?

669
00:31:07,880 --> 00:31:10,690
What are the
essential components

670
00:31:10,690 --> 00:31:17,320
that constitute intelligence
and the minimal set of things

671
00:31:17,320 --> 00:31:21,250
that-- people were working also
on similar fields in biology

672
00:31:21,250 --> 00:31:23,450
and synthetic biology
and things like that.

673
00:31:23,450 --> 00:31:30,790
So certainly I would
probably work in this domain,

674
00:31:30,790 --> 00:31:34,370
as I decided to do 45 years ago.

675
00:31:34,370 --> 00:31:35,590
That's right.

676
00:31:35,590 --> 00:31:42,510
But if I was an undergrad,
people are asking this question.

677
00:31:42,510 --> 00:31:46,280
AI is going to come up and it's
going to do all kinds of stuff

678
00:31:46,280 --> 00:31:49,520
at a low level that we may
not need to learn anymore.

679
00:31:49,520 --> 00:31:53,990
I think there are things that we
shouldn't learn as engineering

680
00:31:53,990 --> 00:31:57,885
or science students anymore.

681
00:31:57,885 --> 00:31:59,510
And there are things--
those are things

682
00:31:59,510 --> 00:32:01,470
that have a short shelf life.

683
00:32:01,470 --> 00:32:05,420
So the joke I say is that if
you're studying computer science

684
00:32:05,420 --> 00:32:07,790
or engineering of
some kind, and you

685
00:32:07,790 --> 00:32:10,820
have a choice between a
course that teaches you

686
00:32:10,820 --> 00:32:13,430
about a piece of technology
that is currently fashionable

687
00:32:13,430 --> 00:32:17,030
like, I don't know, mobile app
programming or LLM prompting

688
00:32:17,030 --> 00:32:18,872
or whatever it is--

689
00:32:18,872 --> 00:32:23,570
I'm sure there are equivalent in
various engineering disciplines,

690
00:32:23,570 --> 00:32:25,830
don't take those classes.

691
00:32:25,830 --> 00:32:27,258
If you have a choice between--

692
00:32:27,258 --> 00:32:27,800
There you go.

693
00:32:27,800 --> 00:32:30,278
--between mobile programming
and quantum mechanics,

694
00:32:30,278 --> 00:32:32,820
take quantum mechanics, even if
you are a computer scientist,

695
00:32:32,820 --> 00:32:35,112
because you will learn about
things like, I don't know,

696
00:32:35,112 --> 00:32:36,090
path integrals?

697
00:32:36,090 --> 00:32:37,520
I mean, it's a
general method that

698
00:32:37,520 --> 00:32:39,450
is applicable to all
kinds of situations.

699
00:32:39,450 --> 00:32:42,010
It's a concept that you can
connect with other things.

700
00:32:42,010 --> 00:32:46,140
So it turns out, how to best
decode the sequence of words

701
00:32:46,140 --> 00:32:50,830
that you most likely in a
speech recognition system,

702
00:32:50,830 --> 00:32:53,920
for example, it's
actually a path integral.

703
00:32:53,920 --> 00:32:56,590
It's discrete, but it's
basically the same concept.

704
00:32:56,590 --> 00:32:58,830
I mean, there's basic
theoretical concepts

705
00:32:58,830 --> 00:33:00,930
like this that turn
out to be abstractions

706
00:33:00,930 --> 00:33:03,940
that are applicable to a
wide variety of things.

707
00:33:03,940 --> 00:33:08,100
So take those challenging
courses, and that

708
00:33:08,100 --> 00:33:10,630
will put you on a good path.

709
00:33:10,630 --> 00:33:16,480
And maybe future AI assistants
will take care of the low level.

710
00:33:16,480 --> 00:33:17,623
And so think of yourself.

711
00:33:17,623 --> 00:33:20,130

712
00:33:20,130 --> 00:33:23,820
Think of the situation
currently of a PhD advisor

713
00:33:23,820 --> 00:33:26,610
with a pool of PhD
students, and the big secret

714
00:33:26,610 --> 00:33:28,570
is that the students
teach the advisors,

715
00:33:28,570 --> 00:33:31,590
not the other way around.

716
00:33:31,590 --> 00:33:34,560
You will be in the same
situation as a student

717
00:33:34,560 --> 00:33:39,300
where during your PhD,
you will have a staff

718
00:33:39,300 --> 00:33:43,150
life of virtual people working
for you, AI assistants working

719
00:33:43,150 --> 00:33:43,790
for you.

720
00:33:43,790 --> 00:33:50,050
It will move your own level
of abstraction a couple levels

721
00:33:50,050 --> 00:33:52,090
up so that there
are a lot of things

722
00:33:52,090 --> 00:33:53,630
you won't have to take care of.

723
00:33:53,630 --> 00:33:57,080
It used to be that you could
do a PhD sequencing DNA.

724
00:33:57,080 --> 00:34:00,010
Not necessary anymore, we
have machines to do this.

725
00:34:00,010 --> 00:34:01,840
It used to be that
you could have

726
00:34:01,840 --> 00:34:06,370
a career as a mathematician
calculating logarithm tables

727
00:34:06,370 --> 00:34:08,540
and trigonometry tables.

728
00:34:08,540 --> 00:34:12,494
Not necessary anymore, we have
calculators and computers.

729
00:34:12,494 --> 00:34:17,440
Or solving differential
equations symbolically by hand,

730
00:34:17,440 --> 00:34:18,889
we solve them numerically.

731
00:34:18,889 --> 00:34:21,010
So there's a lot of--

732
00:34:21,010 --> 00:34:23,320
I mean, it's just a
natural continuation

733
00:34:23,320 --> 00:34:26,860
of technological progress
that humanity moves up

734
00:34:26,860 --> 00:34:30,580
the hierarchical ladder and
leaves the low-level stuff

735
00:34:30,580 --> 00:34:31,940
to machines.

736
00:34:31,940 --> 00:34:33,949
Well, the future is going
to be very exciting.

737
00:34:33,949 --> 00:34:37,159
Yann, thank you for such an
inspiring set of comments today.

738
00:34:37,159 --> 00:34:38,020
Thank you so much.

739
00:34:38,020 --> 00:34:38,920
Thank you.

740
00:34:38,920 --> 00:34:40,770
[APPLAUSE]

