1
00:00:00,160 --> 00:00:03,360
We're moving on to our next startup,

2
00:00:02,080 --> 00:00:05,680
Tinfoil.

3
00:00:03,360 --> 00:00:08,160
>> So, hi everyone. So, I'm going to talk

4
00:00:05,680 --> 00:00:10,080
about Tinfoil. Uh, my name is Jules Din.

5
00:00:08,160 --> 00:00:13,440
I'm one of the co-founders. Uh, I did my

6
00:00:10,080 --> 00:00:15,200
PhD at MIT in security and hardware

7
00:00:13,440 --> 00:00:17,279
security specifically. So, we're going

8
00:00:15,200 --> 00:00:19,840
to talk about verifiable private AI

9
00:00:17,279 --> 00:00:21,359
today.

10
00:00:19,840 --> 00:00:24,000
Oh, so the challenge that we're

11
00:00:21,359 --> 00:00:26,080
addressing is that your private data is

12
00:00:24,000 --> 00:00:29,439
now more than ever your most valuable

13
00:00:26,080 --> 00:00:32,079
asset. Um and when you use AI on private

14
00:00:29,439 --> 00:00:34,399
data, you risk to expose it to third

15
00:00:32,079 --> 00:00:36,239
parties. So short-term solution that

16
00:00:34,399 --> 00:00:39,040
we've seen uh in industry for instance

17
00:00:36,239 --> 00:00:41,760
is to restrict the usage of AI on

18
00:00:39,040 --> 00:00:44,800
sensitive data. So our goal at Tinfoil

19
00:00:41,760 --> 00:00:47,280
is to unlock the use of data on personal

20
00:00:44,800 --> 00:00:50,480
uh proprietary and regulated data for

21
00:00:47,280 --> 00:00:52,719
instance. So current solution have like

22
00:00:50,480 --> 00:00:55,120
strong limitation. You could do all your

23
00:00:52,719 --> 00:00:56,800
deployment on prem or even like on

24
00:00:55,120 --> 00:00:58,480
device like really in your basement but

25
00:00:56,800 --> 00:01:01,840
this has a like that's difficult to

26
00:00:58,480 --> 00:01:04,000
scale and has a lot of um limitation. Uh

27
00:01:01,840 --> 00:01:05,600
you can have legal enforcement. Uh but

28
00:01:04,000 --> 00:01:07,280
the problem here is that there's no

29
00:01:05,600 --> 00:01:08,720
technical way to enforce this contract

30
00:01:07,280 --> 00:01:10,479
and this could be like prone to data

31
00:01:08,720 --> 00:01:13,040
breaches and other things. And finally

32
00:01:10,479 --> 00:01:16,000
you can do pi reduction. But the problem

33
00:01:13,040 --> 00:01:18,400
here is that uh if you remove the um

34
00:01:16,000 --> 00:01:20,240
like very important data um or important

35
00:01:18,400 --> 00:01:22,400
detail on your data like you will lose

36
00:01:20,240 --> 00:01:24,960
accuracy or you will lose utility out of

37
00:01:22,400 --> 00:01:26,640
your AI system. So at info we're

38
00:01:24,960 --> 00:01:29,119
building using a different technologies

39
00:01:26,640 --> 00:01:32,159
and specifically uh hardware based

40
00:01:29,119 --> 00:01:36,000
security or hardware backed u enclave.

41
00:01:32,159 --> 00:01:38,640
So the latest Nvidia GPUs actually

42
00:01:36,000 --> 00:01:40,799
include already hardware mechanism to

43
00:01:38,640 --> 00:01:42,240
protect your data. So they've been

44
00:01:40,799 --> 00:01:44,400
mentioned several time in the webinar,

45
00:01:42,240 --> 00:01:46,159
but this type of technology is called uh

46
00:01:44,400 --> 00:01:49,520
secure hardware encloses or trusted

47
00:01:46,159 --> 00:01:51,119
execution environment. Um so the concept

48
00:01:49,520 --> 00:01:53,600
here is that all your data is going to

49
00:01:51,119 --> 00:01:56,399
be encrypted directly to inside an

50
00:01:53,600 --> 00:01:58,320
enclave that lives in the GPU. Because

51
00:01:56,399 --> 00:02:00,240
all these mechanisms are implemented in

52
00:01:58,320 --> 00:02:02,960
hardware, you actually do not have

53
00:02:00,240 --> 00:02:06,159
performance overhead or very negligible.

54
00:02:02,960 --> 00:02:07,840
And you will also be able to prove which

55
00:02:06,159 --> 00:02:09,360
code exactly is running inside of that

56
00:02:07,840 --> 00:02:11,760
secure environment. So you get a

57
00:02:09,360 --> 00:02:14,160
cryptographic proof that uh it's the

58
00:02:11,760 --> 00:02:16,879
correct uh secure uh environment that's

59
00:02:14,160 --> 00:02:18,560
running there. And so what's interesting

60
00:02:16,879 --> 00:02:20,239
once you have set up this environment is

61
00:02:18,560 --> 00:02:21,920
that even someone who controls the

62
00:02:20,239 --> 00:02:24,720
computer who has access to the computer

63
00:02:21,920 --> 00:02:26,720
cannot extract the sensitive data.

64
00:02:24,720 --> 00:02:28,560
And so you get several very interesting

65
00:02:26,720 --> 00:02:30,800
security properties out of uh this

66
00:02:28,560 --> 00:02:33,680
system. You have data privacy which mean

67
00:02:30,800 --> 00:02:35,680
that no one not even us cloud provider

68
00:02:33,680 --> 00:02:38,080
can see your data. You have

69
00:02:35,680 --> 00:02:40,000
verifiability and integrity which means

70
00:02:38,080 --> 00:02:41,920
that you can get a proof that you're

71
00:02:40,000 --> 00:02:44,239
running exactly the code that's open

72
00:02:41,920 --> 00:02:46,800
source and expected. So this address all

73
00:02:44,239 --> 00:02:49,120
supply chain worries or this kind of

74
00:02:46,800 --> 00:02:50,560
attack vectors. And you can also have

75
00:02:49,120 --> 00:02:52,480
model privacy. So for instance, if

76
00:02:50,560 --> 00:02:54,080
you're fine-tuning a model or like

77
00:02:52,480 --> 00:02:55,760
creating a model from scratch with very

78
00:02:54,080 --> 00:02:57,599
sensitive data, you probably want to

79
00:02:55,760 --> 00:02:59,840
keep the model private to some extent.

80
00:02:57,599 --> 00:03:02,879
And so that also achieved these kind of

81
00:02:59,840 --> 00:03:04,800
properties. But just to give you a

82
00:03:02,879 --> 00:03:06,560
concrete example of how this works and

83
00:03:04,800 --> 00:03:08,239
how this materialized real life, I'm

84
00:03:06,560 --> 00:03:10,800
going to show you a demo on a website uh

85
00:03:08,239 --> 00:03:13,920
of a chat implementation for instance.

86
00:03:10,800 --> 00:03:15,360
So uh this is our chat and so uh as you

87
00:03:13,920 --> 00:03:18,159
can see we have like a wide selection of

88
00:03:15,360 --> 00:03:20,959
open source model and so you can uh ask

89
00:03:18,159 --> 00:03:24,720
it uh simple question like you would do

90
00:03:20,959 --> 00:03:28,000
with chatg and so uh the difference here

91
00:03:24,720 --> 00:03:29,200
is going to be the security. So here as

92
00:03:28,000 --> 00:03:30,879
you can see we have like a bunch of

93
00:03:29,200 --> 00:03:33,599
security check that are happening in the

94
00:03:30,879 --> 00:03:36,159
browser to actually guarantee that your

95
00:03:33,599 --> 00:03:38,959
data is end to end protected that no one

96
00:03:36,159 --> 00:03:40,480
can access it. So I'm going to skip over

97
00:03:38,959 --> 00:03:42,720
the technical details. Happy to answer

98
00:03:40,480 --> 00:03:45,760
any question later. But basically every

99
00:03:42,720 --> 00:03:47,840
time we publish code uh it's open source

100
00:03:45,760 --> 00:03:49,440
automatically compiled measured and then

101
00:03:47,840 --> 00:03:50,879
the hardware is going to prove that it's

102
00:03:49,440 --> 00:03:54,200
exactly that code that running in the

103
00:03:50,879 --> 00:03:54,200
secure environment.

104
00:03:54,400 --> 00:03:59,599
Okay, back to my slides. So uh to take

105
00:03:56,720 --> 00:04:02,400
this kind of very um low-level

106
00:03:59,599 --> 00:04:03,920
technology and build something uh very

107
00:04:02,400 --> 00:04:05,680
integrated that works like this like you

108
00:04:03,920 --> 00:04:08,400
need a bunch of expertise from very

109
00:04:05,680 --> 00:04:10,159
different angles of security. And so we

110
00:04:08,400 --> 00:04:12,799
have a team that span very different

111
00:04:10,159 --> 00:04:15,200
aspect of security practice. So as I

112
00:04:12,799 --> 00:04:16,639
mentioned uh I did my PhD at MIT in

113
00:04:15,200 --> 00:04:19,759
secure hardware cryptography. I also

114
00:04:16,639 --> 00:04:21,680
worked at Nvidia on um like helping uh

115
00:04:19,759 --> 00:04:24,240
implement and use the type of technology

116
00:04:21,680 --> 00:04:26,080
that we're using. Uh I also worked at

117
00:04:24,240 --> 00:04:28,240
Microsoft research and also with Intel

118
00:04:26,080 --> 00:04:31,360
Labs during my PhD on this kind of

119
00:04:28,240 --> 00:04:34,080
commercial um computing technologies. Uh

120
00:04:31,360 --> 00:04:36,800
Tanya uh was at Cloudflare before

121
00:04:34,080 --> 00:04:39,120
Tinfoil doing also cryptography. She has

122
00:04:36,800 --> 00:04:41,919
deployed internet security protocol um

123
00:04:39,120 --> 00:04:45,199
for two millions of people. Uh Sasha was

124
00:04:41,919 --> 00:04:50,639
my labmate at MIT in cryptography and

125
00:04:45,199 --> 00:04:52,960
privacy system and Nate uh is um uh my

126
00:04:50,639 --> 00:04:54,880
fourth co-ounder and he has been working

127
00:04:52,960 --> 00:04:57,759
on a lot of privacy infrastructure

128
00:04:54,880 --> 00:05:00,759
including um tour and other like privacy

129
00:04:57,759 --> 00:05:00,759
systems.

130
00:05:01,040 --> 00:05:04,800
So just to show a case study for

131
00:05:02,960 --> 00:05:07,280
instance uh we've been working with a

132
00:05:04,800 --> 00:05:09,759
government contractor u to help deploy

133
00:05:07,280 --> 00:05:11,120
AI in mission on mission critical data.

134
00:05:09,759 --> 00:05:13,280
So the advantage here is that with

135
00:05:11,120 --> 00:05:15,360
tinfoil uh they have complete isolation

136
00:05:13,280 --> 00:05:17,840
of data across team. There's no operator

137
00:05:15,360 --> 00:05:19,919
access to all uh this sensitive data and

138
00:05:17,840 --> 00:05:22,720
you also have end to end uh supply chain

139
00:05:19,919 --> 00:05:25,039
transparency. uh in term of where we are

140
00:05:22,720 --> 00:05:28,240
at as a startup. So we're in production

141
00:05:25,039 --> 00:05:30,639
with a lot of consumer and startups and

142
00:05:28,240 --> 00:05:32,880
we're also doing a proof of concept with

143
00:05:30,639 --> 00:05:35,840
several entropas client. Uh we also went

144
00:05:32,880 --> 00:05:37,520
through y combinators a few months ago

145
00:05:35,840 --> 00:05:39,440
and so the ask is that we're

146
00:05:37,520 --> 00:05:41,280
specifically looking for uh customers in

147
00:05:39,440 --> 00:05:43,440
regulated industries. So finance,

148
00:05:41,280 --> 00:05:45,440
healthcare, insurance, government. So if

149
00:05:43,440 --> 00:05:47,520
you want to roll out AI in sensitive use

150
00:05:45,440 --> 00:05:50,400
cases where current security options are

151
00:05:47,520 --> 00:05:51,840
not enough, uh please come talk to us.

152
00:05:50,400 --> 00:05:55,400
And these are my contact and feel free

153
00:05:51,840 --> 00:05:55,400
to add me on LinkedIn.

154
00:05:55,440 --> 00:05:59,600
>> Thank you so much Jules. Uh we're

155
00:05:57,759 --> 00:06:01,280
running out of time so if anyone is

156
00:05:59,600 --> 00:06:04,240
interested in asking the questions

157
00:06:01,280 --> 00:06:06,160
please um reach out to Jules directly or

158
00:06:04,240 --> 00:06:08,000
connect with us. Thank you both for

159
00:06:06,160 --> 00:06:10,240
startups and to all of you for joining

160
00:06:08,000 --> 00:06:11,840
us today and please reach out again for

161
00:06:10,240 --> 00:06:14,080
if you need help connecting with these

162
00:06:11,840 --> 00:06:14,080
startups.

