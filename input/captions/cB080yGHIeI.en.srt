1
00:00:05,080 --> 00:00:11,400
so one of the Privileges that I've had

2
00:00:07,480 --> 00:00:13,880
at MIT for the last 25 years is to be

3
00:00:11,400 --> 00:00:16,400
the announcer at MIT commencement and

4
00:00:13,880 --> 00:00:20,600
also to read the names of

5
00:00:16,400 --> 00:00:22,800
graduates and it's really fun when you

6
00:00:20,600 --> 00:00:25,599
get to the phds because when you

7
00:00:22,800 --> 00:00:28,240
announce their school then you get to

8
00:00:25,599 --> 00:00:31,160
say with all the rights and privileges

9
00:00:28,240 --> 00:00:32,119
pertaining there to

10
00:00:31,160 --> 00:00:34,000
I don't know what the rights and

11
00:00:32,119 --> 00:00:37,200
privileges are I guess it's the right to

12
00:00:34,000 --> 00:00:40,239
wear the magic robe and the privilege to

13
00:00:37,200 --> 00:00:44,960
be called a doctor and now I'd like to

14
00:00:40,239 --> 00:00:49,039
present a newly minted PhD from the MIT

15
00:00:44,960 --> 00:00:51,480
media laboratory Dr Pat patar nutor was

16
00:00:49,039 --> 00:00:55,079
awarded the PhD from the media

17
00:00:51,480 --> 00:00:57,199
laboratory just in late 2024 he's

18
00:00:55,079 --> 00:01:00,199
co-director of the MIT advancing human

19
00:00:57,199 --> 00:01:02,519
AI interaction research program and his

20
00:01:00,199 --> 00:01:04,960
research lies at the intersection of AI

21
00:01:02,519 --> 00:01:07,400
and human computer interaction where he

22
00:01:04,960 --> 00:01:08,799
develops and studies AI systems that

23
00:01:07,400 --> 00:01:11,040
support human flourishing through

24
00:01:08,799 --> 00:01:14,240
multimodal systems with explainable

25
00:01:11,040 --> 00:01:14,240
feedback Dr Pat

26
00:01:14,280 --> 00:01:20,960
welcome thank you so

27
00:01:16,840 --> 00:01:24,200
much hello everyone hello good evening

28
00:01:20,960 --> 00:01:26,479
hi um my name is Pat as uh Todd mention

29
00:01:24,200 --> 00:01:29,680
I just finished my PhD so I'm not used

30
00:01:26,479 --> 00:01:32,280
to people calling me doctor yet um and I

31
00:01:29,680 --> 00:01:33,880
just you know flew from Cambridge uh to

32
00:01:32,280 --> 00:01:36,240
uh Bangkok actually two days ago so

33
00:01:33,880 --> 00:01:38,079
please excuse my jet lag as well so

34
00:01:36,240 --> 00:01:40,880
today I would like to um talk to you

35
00:01:38,079 --> 00:01:42,880
about my research on how can we um

36
00:01:40,880 --> 00:01:44,759
better understand and develop AI to

37
00:01:42,880 --> 00:01:46,840
support human flourishing especially

38
00:01:44,759 --> 00:01:49,079
through studying of what I call cybor

39
00:01:46,840 --> 00:01:50,200
psychology um the Art and Science of

40
00:01:49,079 --> 00:01:52,600
human AI

41
00:01:50,200 --> 00:01:54,719
interaction but first before I you know

42
00:01:52,600 --> 00:01:57,320
begin um I'm actually from Thailand so

43
00:01:54,719 --> 00:02:00,240
it's an honor to be here in Thailand uh

44
00:01:57,320 --> 00:02:02,920
with uh MIT teams and um it's an awesome

45
00:02:00,240 --> 00:02:04,159
thing to do um but one thing to think

46
00:02:02,920 --> 00:02:06,439
about is that if you look at the word

47
00:02:04,159 --> 00:02:09,239
Thailand closely you can see that there

48
00:02:06,439 --> 00:02:11,480
is no Thailand without AI right you can

49
00:02:09,239 --> 00:02:13,480
see that it's really important thank you

50
00:02:11,480 --> 00:02:15,879
thank you but more important than that

51
00:02:13,480 --> 00:02:18,800
if you look at that there is Edge before

52
00:02:15,879 --> 00:02:22,000
Ai and I think this Ed for me I would

53
00:02:18,800 --> 00:02:23,560
argue stand for human so in Thailand I

54
00:02:22,000 --> 00:02:26,440
think you know as the name suggests

55
00:02:23,560 --> 00:02:28,040
there's both the human aspect and the AI

56
00:02:26,440 --> 00:02:29,879
and thinking about both of them together

57
00:02:28,040 --> 00:02:31,360
I think is really really important as

58
00:02:29,879 --> 00:02:34,560
especially in this country in this

59
00:02:31,360 --> 00:02:37,400
context right so that's uh why my work

60
00:02:34,560 --> 00:02:39,760
at MIT really focus on this question of

61
00:02:37,400 --> 00:02:42,040
how can we understand and better design

62
00:02:39,760 --> 00:02:44,120
AI system to support human flourishing

63
00:02:42,040 --> 00:02:45,680
I'll talk more about uh that uh during

64
00:02:44,120 --> 00:02:47,640
this presentation but you can see this

65
00:02:45,680 --> 00:02:50,440
is my dissertation that I just finished

66
00:02:47,640 --> 00:02:53,239
um actually last year at the end of last

67
00:02:50,440 --> 00:02:55,280
year what it means to think about human

68
00:02:53,239 --> 00:02:58,080
AI interaction is that instead of just

69
00:02:55,280 --> 00:03:00,080
focusing on AI Alone um I think as a

70
00:02:58,080 --> 00:03:01,760
speaker mentioned we need to shift the

71
00:03:00,080 --> 00:03:04,440
Paradigm a little bit instead of

72
00:03:01,760 --> 00:03:06,879
thinking of AI we can think about IIA or

73
00:03:04,440 --> 00:03:09,440
intelligence augmentation where system

74
00:03:06,879 --> 00:03:11,879
can support and enhance human you know

75
00:03:09,440 --> 00:03:14,000
capability right but I think in my work

76
00:03:11,879 --> 00:03:15,959
I want to push that further a little bit

77
00:03:14,000 --> 00:03:18,120
um not just looking at narrow way that

78
00:03:15,959 --> 00:03:20,480
AI can augment human intelligence but

79
00:03:18,120 --> 00:03:22,560
way in which AI can support human

80
00:03:20,480 --> 00:03:24,000
holistically um that's what I call AI

81
00:03:22,560 --> 00:03:25,879
for human flourishing you know

82
00:03:24,000 --> 00:03:28,760
intelligent system to support

83
00:03:25,879 --> 00:03:30,760
multi-dimension of uh human growth and

84
00:03:28,760 --> 00:03:32,920
development what I mean by this is that

85
00:03:30,760 --> 00:03:34,120
we can start to really take Knowledge

86
00:03:32,920 --> 00:03:35,640
from you know different

87
00:03:34,120 --> 00:03:38,640
interdisciplinary subject like

88
00:03:35,640 --> 00:03:40,519
psychology Behavioral Science and U many

89
00:03:38,640 --> 00:03:43,360
subject and think about how AI can

90
00:03:40,519 --> 00:03:45,200
augment any of that aspect and in my own

91
00:03:43,360 --> 00:03:47,799
work I look at this sort of human AI

92
00:03:45,200 --> 00:03:50,159
interaction across scale and across um

93
00:03:47,799 --> 00:03:51,680
um context from in space like you know

94
00:03:50,159 --> 00:03:53,879
this project where we work closely with

95
00:03:51,680 --> 00:03:57,280
NASA um and that's actually me uh in

96
00:03:53,879 --> 00:03:59,360
zero gravity floating was really cool um

97
00:03:57,280 --> 00:04:01,159
all the way to you know things that are

98
00:03:59,360 --> 00:04:03,519
happening on Earth in you know domain

99
00:04:01,159 --> 00:04:06,079
like education Healthcare and and many

100
00:04:03,519 --> 00:04:08,360
more but there are three things that I

101
00:04:06,079 --> 00:04:10,000
want to talk to you today you know we're

102
00:04:08,360 --> 00:04:11,840
looking at uh human AI interaction

103
00:04:10,000 --> 00:04:14,040
research we can think about ways in

104
00:04:11,840 --> 00:04:16,400
which we can invent new system we can

105
00:04:14,040 --> 00:04:18,160
investigate existing system on you know

106
00:04:16,400 --> 00:04:20,320
problems or challenges that are facing

107
00:04:18,160 --> 00:04:22,240
us and also thinking about ways that we

108
00:04:20,320 --> 00:04:24,199
can Inspire new use case New

109
00:04:22,240 --> 00:04:26,440
Opportunities I think that tie very well

110
00:04:24,199 --> 00:04:29,560
to the industry connection right so

111
00:04:26,440 --> 00:04:31,360
invent investigate and Inspire the three

112
00:04:29,560 --> 00:04:34,400
uh ways that we are investigating human

113
00:04:31,360 --> 00:04:36,400
a interaction research right now first

114
00:04:34,400 --> 00:04:38,720
let's look at invent you know at MIT we

115
00:04:36,400 --> 00:04:40,520
invent things all the time in order to

116
00:04:38,720 --> 00:04:42,440
think about AI for human flourishing we

117
00:04:40,520 --> 00:04:45,479
are inventing sort of you know new type

118
00:04:42,440 --> 00:04:47,840
of AI system that focus on three areas I

119
00:04:45,479 --> 00:04:50,320
think most of the people today focus on

120
00:04:47,840 --> 00:04:51,919
productivity right but I think you know

121
00:04:50,320 --> 00:04:53,759
um my colleague actually once said to me

122
00:04:51,919 --> 00:04:55,600
that if you measure the wrong thing

123
00:04:53,759 --> 00:04:56,840
you're going to create the wrong things

124
00:04:55,600 --> 00:04:59,000
right so we need to think about the

125
00:04:56,840 --> 00:05:00,720
broader goal that AI can support human

126
00:04:59,000 --> 00:05:02,880
how we can use it to support human

127
00:05:00,720 --> 00:05:04,520
wisdom wonder and well-being these are

128
00:05:02,880 --> 00:05:07,000
the three areas that my research focus

129
00:05:04,520 --> 00:05:08,800
on what I mean and each of these

130
00:05:07,000 --> 00:05:10,320
actually have many um research projects

131
00:05:08,800 --> 00:05:13,240
that we have developed many papers as

132
00:05:10,320 --> 00:05:15,240
you can see on screen um but for example

133
00:05:13,240 --> 00:05:18,520
what do we what do I mean by AI for

134
00:05:15,240 --> 00:05:20,759
enhancing human Wonder right here um we

135
00:05:18,520 --> 00:05:22,520
are thinking a lot about language model

136
00:05:20,759 --> 00:05:24,680
I think everyone probably familiar with

137
00:05:22,520 --> 00:05:26,120
language model now which had GPT we

138
00:05:24,680 --> 00:05:28,520
thinking about what happened when we are

139
00:05:26,120 --> 00:05:30,400
shifting from language model into what

140
00:05:28,520 --> 00:05:32,400
we call large human model model where we

141
00:05:30,400 --> 00:05:34,720
can start to incorporate many dimension

142
00:05:32,400 --> 00:05:37,039
of data Health Data behavioral data

143
00:05:34,720 --> 00:05:39,199
financial data and many more and allow a

144
00:05:37,039 --> 00:05:41,639
person to actually create an AI model or

145
00:05:39,199 --> 00:05:44,280
AI digital twin of them um this is me

146
00:05:41,639 --> 00:05:46,400
when I'm 60 years old and I can create a

147
00:05:44,280 --> 00:05:49,080
profile of who I might actually become

148
00:05:46,400 --> 00:05:51,759
when I'm that old you know um you know

149
00:05:49,080 --> 00:05:53,880
in the 40 years in the future and and

150
00:05:51,759 --> 00:05:55,759
it's kind of interesting because you

151
00:05:53,880 --> 00:05:58,000
know this is um something that is really

152
00:05:55,759 --> 00:05:59,680
exciting to us because we can now really

153
00:05:58,000 --> 00:06:02,560
understand how a human P how a person

154
00:05:59,680 --> 00:06:04,960
person um grow and and develop over time

155
00:06:02,560 --> 00:06:06,400
right this is actually based on many uh

156
00:06:04,960 --> 00:06:08,080
research that we have done at the media

157
00:06:06,400 --> 00:06:10,240
lab um the first one we published in

158
00:06:08,080 --> 00:06:13,039
2018 looking at how we can create

159
00:06:10,240 --> 00:06:15,360
realistic AI characters um this is when

160
00:06:13,039 --> 00:06:16,759
you know generative AI wasn't cool yet

161
00:06:15,360 --> 00:06:18,919
um we were one of the first people to

162
00:06:16,759 --> 00:06:20,520
actually explore this concept and how we

163
00:06:18,919 --> 00:06:22,560
can kind of create pipeline that allow

164
00:06:20,520 --> 00:06:25,400
us to take different data and create

165
00:06:22,560 --> 00:06:27,160
this realistic human uh virtual human

166
00:06:25,400 --> 00:06:29,000
but it doesn't stop there right just

167
00:06:27,160 --> 00:06:30,199
making the appearance is one thing I

168
00:06:29,000 --> 00:06:32,199
think what is the most most interesting

169
00:06:30,199 --> 00:06:34,639
is when you can actually create a time

170
00:06:32,199 --> 00:06:37,240
machine that allow a person to use their

171
00:06:34,639 --> 00:06:39,039
own you know current data and project

172
00:06:37,240 --> 00:06:41,319
what they might end up you know being

173
00:06:39,039 --> 00:06:43,520
like uh when they're 60 years old we

174
00:06:41,319 --> 00:06:45,360
published this study called future you

175
00:06:43,520 --> 00:06:47,639
where we develop a colonization of AI

176
00:06:45,360 --> 00:06:49,560
system that allow a person especially

177
00:06:47,639 --> 00:06:52,319
you know teenagers to talk to their

178
00:06:49,560 --> 00:06:54,360
older self um so we develop system that

179
00:06:52,319 --> 00:06:56,360
can take um personal data like you know

180
00:06:54,360 --> 00:06:58,520
their aspiration their profile their

181
00:06:56,360 --> 00:07:00,240
behavior right now and then we create

182
00:06:58,520 --> 00:07:02,840
what we call synthetic memory memories

183
00:07:00,240 --> 00:07:04,840
that basically use that uh uh current

184
00:07:02,840 --> 00:07:06,039
live event to create the memories that

185
00:07:04,840 --> 00:07:07,680
the person might experience in the

186
00:07:06,039 --> 00:07:09,680
future and then feed that to the

187
00:07:07,680 --> 00:07:12,479
language model to actually generate

188
00:07:09,680 --> 00:07:13,879
human conversation and um by doing this

189
00:07:12,479 --> 00:07:16,759
a person can actually have a

190
00:07:13,879 --> 00:07:18,520
conversation with an own version of the

191
00:07:16,759 --> 00:07:21,120
future so kind of like having a time

192
00:07:18,520 --> 00:07:22,479
machine to chat with your future self

193
00:07:21,120 --> 00:07:24,599
and this is based on Research in

194
00:07:22,479 --> 00:07:26,199
Psychology where we shown that well my

195
00:07:24,599 --> 00:07:28,560
colleague have shown that you know when

196
00:07:26,199 --> 00:07:30,520
you increase future self-continuity you

197
00:07:28,560 --> 00:07:32,319
can actually make people make better

198
00:07:30,520 --> 00:07:34,319
decision by thinking in long term you

199
00:07:32,319 --> 00:07:36,759
know saving more money for the future

200
00:07:34,319 --> 00:07:38,440
and also you know thinking about uh um

201
00:07:36,759 --> 00:07:39,720
you know uh things that will have impact

202
00:07:38,440 --> 00:07:41,960
in the future like you know climate

203
00:07:39,720 --> 00:07:44,120
change and important things that are not

204
00:07:41,960 --> 00:07:45,360
you know just have immediate gain right

205
00:07:44,120 --> 00:07:48,199
and we have shown that when people use

206
00:07:45,360 --> 00:07:50,000
these um future U AI system they become

207
00:07:48,199 --> 00:07:52,000
less anxious and also become more

208
00:07:50,000 --> 00:07:54,720
motivated to actually think about their

209
00:07:52,000 --> 00:07:56,000
life in a you know more ambitious way

210
00:07:54,720 --> 00:07:57,840
and we can also increase future

211
00:07:56,000 --> 00:08:00,440
self-continuity as well compared to the

212
00:07:57,840 --> 00:08:02,039
control groups so you know as I

213
00:08:00,440 --> 00:08:03,840
mentioned when we invent something we

214
00:08:02,039 --> 00:08:05,759
also study and also do randomized

215
00:08:03,840 --> 00:08:08,120
control trial to understand the impact

216
00:08:05,759 --> 00:08:09,840
of AI system that we develop and it's

217
00:08:08,120 --> 00:08:12,440
really exciting because you know after

218
00:08:09,840 --> 00:08:16,039
launching this project we have now over

219
00:08:12,440 --> 00:08:17,800
190 PE countries um people from 190

220
00:08:16,039 --> 00:08:19,879
countries worldwide are using this

221
00:08:17,800 --> 00:08:22,800
platform and this is part of a larger

222
00:08:19,879 --> 00:08:24,759
project to actually study how AI digital

223
00:08:22,800 --> 00:08:27,159
twin interact with people in the real

224
00:08:24,759 --> 00:08:29,520
world um right now we have about 60,000

225
00:08:27,159 --> 00:08:32,279
user um and and participant that are

226
00:08:29,520 --> 00:08:34,839
using this and helping us understand how

227
00:08:32,279 --> 00:08:36,159
AI can help people think in long term

228
00:08:34,839 --> 00:08:37,719
and you know it's really exciting

229
00:08:36,159 --> 00:08:40,320
because this project was feature on the

230
00:08:37,719 --> 00:08:43,640
main page of MIT um this was you know a

231
00:08:40,320 --> 00:08:45,200
great a great honor um as well and it

232
00:08:43,640 --> 00:08:47,560
has been you know talk about in many

233
00:08:45,200 --> 00:08:50,640
places you know EU um the the

234
00:08:47,560 --> 00:08:52,800
organization that create EU AI um uh act

235
00:08:50,640 --> 00:08:54,399
or the the regulation of AI also feature

236
00:08:52,800 --> 00:08:56,200
this project as something that is you

237
00:08:54,399 --> 00:08:58,480
know a pro-social type of AI where you

238
00:08:56,200 --> 00:08:59,959
use AI to support person uh uh

239
00:08:58,480 --> 00:09:02,839
development and growth

240
00:08:59,959 --> 00:09:05,000
right Beyond modeling oneself you can

241
00:09:02,839 --> 00:09:07,600
also model other people as well you know

242
00:09:05,000 --> 00:09:09,920
this guy was in the news a lot recently

243
00:09:07,600 --> 00:09:11,240
um today especially right we also look

244
00:09:09,920 --> 00:09:14,000
at what happened when you actually

245
00:09:11,240 --> 00:09:16,040
create a digital twin of UT people like

246
00:09:14,000 --> 00:09:17,760
people that you know have are famous or

247
00:09:16,040 --> 00:09:19,480
people that other people might like or

248
00:09:17,760 --> 00:09:22,920
admire them like you know Elon Musk for

249
00:09:19,480 --> 00:09:26,360
example we did this study in 2022

250
00:09:22,920 --> 00:09:27,600
actually uh two years um ago and we did

251
00:09:26,360 --> 00:09:29,440
this during the pandemic where we

252
00:09:27,600 --> 00:09:31,959
interested in what happened if we used

253
00:09:29,440 --> 00:09:34,160
this digital twin or AI character to

254
00:09:31,959 --> 00:09:36,720
help people learn better so motivate

255
00:09:34,160 --> 00:09:38,720
them to learn and uh what we were

256
00:09:36,720 --> 00:09:41,040
interested in was like if we create a

257
00:09:38,720 --> 00:09:43,680
virtual character does it replicate the

258
00:09:41,040 --> 00:09:45,839
effect of liking or admiring that person

259
00:09:43,680 --> 00:09:47,760
and what we found is that even if people

260
00:09:45,839 --> 00:09:50,040
know that these character are fake and

261
00:09:47,760 --> 00:09:53,000
are not real people still feel more

262
00:09:50,040 --> 00:09:54,680
motivated and learn much better when

263
00:09:53,000 --> 00:09:56,959
they have a character based on someone

264
00:09:54,680 --> 00:09:58,880
that they like or admire right at the

265
00:09:56,959 --> 00:10:00,800
time we did uh use Elon mask as the

266
00:09:58,880 --> 00:10:03,160
experiment subject I don't know if we do

267
00:10:00,800 --> 00:10:05,200
it today we'll have the same result um I

268
00:10:03,160 --> 00:10:07,240
think he quite changed a lot um in the

269
00:10:05,200 --> 00:10:10,200
last couple years right but I think this

270
00:10:07,240 --> 00:10:12,560
this um um result really show that um

271
00:10:10,200 --> 00:10:14,519
these tools can be really personalized

272
00:10:12,560 --> 00:10:15,920
and used to actually you know enhance

273
00:10:14,519 --> 00:10:17,839
The Learning Experience make them more

274
00:10:15,920 --> 00:10:19,079
personalized and engaging imagine you

275
00:10:17,839 --> 00:10:21,079
can learn you know your favorite

276
00:10:19,079 --> 00:10:22,760
subjects like you know calculus from

277
00:10:21,079 --> 00:10:25,440
anyone in the world using this sort of

278
00:10:22,760 --> 00:10:28,000
AI character right so that's what we

279
00:10:25,440 --> 00:10:29,320
call AI for enhancing human Wonder make

280
00:10:28,000 --> 00:10:31,200
people more inspired more engaged

281
00:10:29,320 --> 00:10:33,800
engaging more exciting about their life

282
00:10:31,200 --> 00:10:35,920
and themselves next we want to talk

283
00:10:33,800 --> 00:10:38,040
about AI for enhancing human wisdom this

284
00:10:35,920 --> 00:10:40,600
is also really really important you know

285
00:10:38,040 --> 00:10:43,560
we have been looking at um AI not just

286
00:10:40,600 --> 00:10:45,760
in the computer screen or um in mobile

287
00:10:43,560 --> 00:10:48,920
platform but when the AI is actually

288
00:10:45,760 --> 00:10:51,000
part of a person as a second brain right

289
00:10:48,920 --> 00:10:53,639
this is a project where we embed AI into

290
00:10:51,000 --> 00:10:57,200
a varable platform we call it varable

291
00:10:53,639 --> 00:10:59,920
Reasoner is a system that that create to

292
00:10:57,200 --> 00:11:02,680
not a person to pay more attention to

293
00:10:59,920 --> 00:11:04,480
you know logical fallacy and you know to

294
00:11:02,680 --> 00:11:06,720
make them think more critically when

295
00:11:04,480 --> 00:11:08,360
they engaged with information right

296
00:11:06,720 --> 00:11:10,040
right now I think it's very important

297
00:11:08,360 --> 00:11:12,399
more important than ever we're going to

298
00:11:10,040 --> 00:11:14,519
hear many more speech more information

299
00:11:12,399 --> 00:11:17,079
more advertisement more of anything

300
00:11:14,519 --> 00:11:19,480
right especially AI being used to create

301
00:11:17,079 --> 00:11:21,760
contents we also need to use an AI to

302
00:11:19,480 --> 00:11:23,720
help us filter and think more critically

303
00:11:21,760 --> 00:11:26,279
about the content that we consume and

304
00:11:23,720 --> 00:11:27,920
and and receive as well right and you

305
00:11:26,279 --> 00:11:29,639
know platform are not going to do it for

306
00:11:27,920 --> 00:11:32,760
us right you know you can see you may

307
00:11:29,639 --> 00:11:35,440
have seen meta decide to remove um fact

308
00:11:32,760 --> 00:11:37,720
checking or from that platform so it's

309
00:11:35,440 --> 00:11:39,240
important that we help people develop

310
00:11:37,720 --> 00:11:41,480
their own sort of you know critical

311
00:11:39,240 --> 00:11:43,440
thinking by having tools that are always

312
00:11:41,480 --> 00:11:45,720
with them and they can kind of wear like

313
00:11:43,440 --> 00:11:48,440
you know their second brain essentially

314
00:11:45,720 --> 00:11:51,839
so what does this system does is that um

315
00:11:48,440 --> 00:11:53,959
we use uh a semantic marker in the

316
00:11:51,839 --> 00:11:56,120
language um in the text that the the

317
00:11:53,959 --> 00:11:58,120
system look at to figure out when there

318
00:11:56,120 --> 00:11:59,959
is a logical fallacy or when the

319
00:11:58,120 --> 00:12:01,279
statement you know uh like you know

320
00:11:59,959 --> 00:12:04,079
advertisement political speech or

321
00:12:01,279 --> 00:12:05,880
whatever is stay without evidence um

322
00:12:04,079 --> 00:12:08,600
this is not factchecking um we are

323
00:12:05,880 --> 00:12:11,240
looking at logical consistency and see

324
00:12:08,600 --> 00:12:13,320
how the um the claim and the evidence

325
00:12:11,240 --> 00:12:15,000
support each other and what we have

326
00:12:13,320 --> 00:12:17,519
found is that when you use this system

327
00:12:15,000 --> 00:12:18,880
to notch people you know to to pay more

328
00:12:17,519 --> 00:12:21,399
attention to the statement that have

329
00:12:18,880 --> 00:12:24,199
this logical fallacy we can make people

330
00:12:21,399 --> 00:12:26,600
you know identify reasonable statement

331
00:12:24,199 --> 00:12:28,920
better uh than if they are not having

332
00:12:26,600 --> 00:12:31,120
the system and also they tend to agree

333
00:12:28,920 --> 00:12:33,360
more with statement with uh evidence as

334
00:12:31,120 --> 00:12:35,240
well so this is a tool that can be used

335
00:12:33,360 --> 00:12:36,760
to notch people to pay the right

336
00:12:35,240 --> 00:12:38,519
attention to pay the attention to the

337
00:12:36,760 --> 00:12:40,440
right part of the sentence that they

338
00:12:38,519 --> 00:12:41,680
that they encounter and to help them

339
00:12:40,440 --> 00:12:43,959
sort of you know develop critical

340
00:12:41,680 --> 00:12:45,920
thinking we hope that once these system

341
00:12:43,959 --> 00:12:47,800
start to kind of give them this nudging

342
00:12:45,920 --> 00:12:49,399
mechanism um people can start to

343
00:12:47,800 --> 00:12:51,440
internalize that and then they can you

344
00:12:49,399 --> 00:12:54,040
know leave without the the system so

345
00:12:51,440 --> 00:12:56,600
helping people cultivate this process

346
00:12:54,040 --> 00:12:58,360
themselves we also take that further and

347
00:12:56,600 --> 00:13:00,880
thinking about how this system can also

348
00:12:58,360 --> 00:13:02,680
ask questions well right we heard a lot

349
00:13:00,880 --> 00:13:04,760
about AI giving the answer to the

350
00:13:02,680 --> 00:13:07,279
student to the teacher to you know the

351
00:13:04,760 --> 00:13:09,240
employee the employer all kind of thing

352
00:13:07,279 --> 00:13:11,440
right but what happened with the AI

353
00:13:09,240 --> 00:13:13,360
asked question you know we take the

354
00:13:11,440 --> 00:13:14,839
project like variable Reasoner forward

355
00:13:13,360 --> 00:13:16,880
and think about what happened if AI

356
00:13:14,839 --> 00:13:19,600
asked interesting question and question

357
00:13:16,880 --> 00:13:21,560
that make us think more critically so we

358
00:13:19,600 --> 00:13:23,839
take the process um such as you know

359
00:13:21,560 --> 00:13:26,720
Socratic method where you know Socrates

360
00:13:23,839 --> 00:13:28,560
never answer um any question of of his

361
00:13:26,720 --> 00:13:30,519
student right but he asked the question

362
00:13:28,560 --> 00:13:33,399
back so here we compare what happened

363
00:13:30,519 --> 00:13:35,600
when the AI asked the ask the person um

364
00:13:33,399 --> 00:13:37,800
uh the question back um instead of

365
00:13:35,600 --> 00:13:39,880
giving them the correct answer um and

366
00:13:37,800 --> 00:13:42,120
what we have shown is that because when

367
00:13:39,880 --> 00:13:43,959
AI ask the question by Framing the

368
00:13:42,120 --> 00:13:46,120
statement or the information that they

369
00:13:43,959 --> 00:13:48,320
you know people encounter into question

370
00:13:46,120 --> 00:13:49,880
people can actually think for themselves

371
00:13:48,320 --> 00:13:51,560
and that they can actually use logical

372
00:13:49,880 --> 00:13:53,320
thinking that already inert in them but

373
00:13:51,560 --> 00:13:55,880
maybe they're not using because you know

374
00:13:53,320 --> 00:13:58,079
they are um sort of going with um the

375
00:13:55,880 --> 00:14:00,360
Intuition or going with the sort of gut

376
00:13:58,079 --> 00:14:02,279
instinct you using uh using this to kind

377
00:14:00,360 --> 00:14:04,399
of make them um think more critically

378
00:14:02,279 --> 00:14:06,680
and we have shown that when AI asks the

379
00:14:04,399 --> 00:14:08,959
right question it actually perform help

380
00:14:06,680 --> 00:14:10,759
people perform better than the AI giving

381
00:14:08,959 --> 00:14:12,560
the answer itself right so this is

382
00:14:10,759 --> 00:14:14,240
really really important because even

383
00:14:12,560 --> 00:14:15,519
with the same model the way that you

384
00:14:14,240 --> 00:14:17,959
interact with people can lead to

385
00:14:15,519 --> 00:14:19,839
different outcomes and this is really

386
00:14:17,959 --> 00:14:22,279
promising one of our colleagues at

387
00:14:19,839 --> 00:14:24,079
Harvard say that this type of work is

388
00:14:22,279 --> 00:14:26,160
really exciting because it opened up new

389
00:14:24,079 --> 00:14:28,199
possibility for AI power decision

390
00:14:26,160 --> 00:14:30,800
support rather than making making people

391
00:14:28,199 --> 00:14:32,720
over rely on a answer we use them to

392
00:14:30,800 --> 00:14:35,440
nudge people to think for themselves and

393
00:14:32,720 --> 00:14:36,759
provide the right uh stimulus to for

394
00:14:35,440 --> 00:14:37,759
them to to kind of think critically

395
00:14:36,759 --> 00:14:40,040
about the

396
00:14:37,759 --> 00:14:42,079
statement so this type of work you might

397
00:14:40,040 --> 00:14:44,680
consider them as some kind of proactive

398
00:14:42,079 --> 00:14:46,959
agent system that are always on with the

399
00:14:44,680 --> 00:14:48,920
person always asking question always

400
00:14:46,959 --> 00:14:51,279
sort of providing feedback without the

401
00:14:48,920 --> 00:14:52,600
person needing them right and um we're

402
00:14:51,279 --> 00:14:54,720
doing many more of this type of

403
00:14:52,600 --> 00:14:55,880
proactive agent where not only that we

404
00:14:54,720 --> 00:14:57,800
can use them to kind of make people

405
00:14:55,880 --> 00:14:59,880
think critically we can also help people

406
00:14:57,800 --> 00:15:01,720
think more divergently as as well this

407
00:14:59,880 --> 00:15:04,519
is a study where we look at what

408
00:15:01,720 --> 00:15:07,519
happened when AI present an opposite

409
00:15:04,519 --> 00:15:10,040
argument or pres present opposite bias

410
00:15:07,519 --> 00:15:12,240
as the person right A lot of people try

411
00:15:10,040 --> 00:15:13,880
to get rid of AI bias by trying to you

412
00:15:12,240 --> 00:15:16,519
know retrain them think about alignment

413
00:15:13,880 --> 00:15:18,399
research and many more but as a FCI

414
00:15:16,519 --> 00:15:21,320
researcher we think what happened if we

415
00:15:18,399 --> 00:15:23,880
use the AI bias to our advantage and

416
00:15:21,320 --> 00:15:26,240
instead when we know what kind of bias a

417
00:15:23,880 --> 00:15:28,560
person have we can have the AI bias be

418
00:15:26,240 --> 00:15:30,880
the opposite of that right if you uh

419
00:15:28,560 --> 00:15:33,440
someone that really like um um I don't

420
00:15:30,880 --> 00:15:35,560
know like uh capitalism we can create AI

421
00:15:33,440 --> 00:15:37,199
That's more pro-social or if you like

422
00:15:35,560 --> 00:15:39,839
hierarchy we can create AI that this

423
00:15:37,199 --> 00:15:42,240
anti- hierarchy um to actually balance

424
00:15:39,839 --> 00:15:44,000
that out and we did an experiment where

425
00:15:42,240 --> 00:15:46,680
people brainstorming this different kind

426
00:15:44,000 --> 00:15:48,360
of AI and with different kind of bias

427
00:15:46,680 --> 00:15:51,399
and what we have shown is that people

428
00:15:48,360 --> 00:15:53,319
can be n to be more in the center where

429
00:15:51,399 --> 00:15:55,160
you have ai that are opposite of them as

430
00:15:53,319 --> 00:15:57,519
them right so you can see that when you

431
00:15:55,160 --> 00:16:00,120
have ai that have the opposite bias as a

432
00:15:57,519 --> 00:16:02,319
person they tend to have more neutral

433
00:16:00,120 --> 00:16:04,519
value in the brainstorming outcome but

434
00:16:02,319 --> 00:16:06,440
at the same time um this tool can also

435
00:16:04,519 --> 00:16:08,839
be used to nudge people to become more

436
00:16:06,440 --> 00:16:10,759
extreme right so it's really depending

437
00:16:08,839 --> 00:16:12,720
on the context of what you want to use

438
00:16:10,759 --> 00:16:15,440
this sort of agent

439
00:16:12,720 --> 00:16:17,959
for I think a lot of people today I

440
00:16:15,440 --> 00:16:19,360
think uh think a lot about agents right

441
00:16:17,959 --> 00:16:21,160
people that are in the field of

442
00:16:19,360 --> 00:16:23,800
artificial intelligence today will you

443
00:16:21,160 --> 00:16:26,000
know consider AI agent as sort of the

444
00:16:23,800 --> 00:16:28,959
the next Frontier we're going beyond the

445
00:16:26,000 --> 00:16:31,519
chat interface into system that can act

446
00:16:28,959 --> 00:16:33,920
and automate many process by itself

447
00:16:31,519 --> 00:16:35,800
right you know our friend Sam otman say

448
00:16:33,920 --> 00:16:39,519
that agent is the you know the killer

449
00:16:35,800 --> 00:16:41,040
application of AI but um this is

450
00:16:39,519 --> 00:16:42,600
actually something very funny because

451
00:16:41,040 --> 00:16:44,600
this is what we have been doing at the

452
00:16:42,600 --> 00:16:45,920
media lab for the last 30 years my

453
00:16:44,600 --> 00:16:48,600
adviser you know when you have a cool

454
00:16:45,920 --> 00:16:50,600
advisor at MIT you can brag about it um

455
00:16:48,600 --> 00:16:53,560
my advisor Professor pimas has been

456
00:16:50,600 --> 00:16:55,680
working on AI agent in fact many of her

457
00:16:53,560 --> 00:16:57,240
companies you know have sold on this

458
00:16:55,680 --> 00:16:59,600
topic already and her last latest

459
00:16:57,240 --> 00:17:02,000
company was you know wellue over 900

460
00:16:59,600 --> 00:17:04,480
million um dollar she have been one of

461
00:17:02,000 --> 00:17:06,240
the pioneer of AI agent and you know if

462
00:17:04,480 --> 00:17:08,000
you look at her work you know at the

463
00:17:06,240 --> 00:17:10,880
time it was called software agents

464
00:17:08,000 --> 00:17:13,000
before AI became a thing right but her

465
00:17:10,880 --> 00:17:14,720
idea was always the same that if we want

466
00:17:13,000 --> 00:17:17,079
to think about how technology support

467
00:17:14,720 --> 00:17:19,760
people we need to pay attention to the

468
00:17:17,079 --> 00:17:21,959
experience not just what the AI can do

469
00:17:19,760 --> 00:17:24,640
but what experience does the AI bring to

470
00:17:21,959 --> 00:17:26,760
the person right this is from her uh uh

471
00:17:24,640 --> 00:17:28,679
early work on the direct manipulation

472
00:17:26,760 --> 00:17:31,120
versus um interface agent which is a you

473
00:17:28,679 --> 00:17:33,400
know a really famous debate in uh human

474
00:17:31,120 --> 00:17:35,799
computer interaction research so the

475
00:17:33,400 --> 00:17:36,720
experience of interacting with agent

476
00:17:35,799 --> 00:17:38,960
really

477
00:17:36,720 --> 00:17:40,799
matter one project that I would like to

478
00:17:38,960 --> 00:17:43,000
highlight here is what we have been

479
00:17:40,799 --> 00:17:45,360
exploring when we think about agent that

480
00:17:43,000 --> 00:17:47,600
collaborate with people so here we have

481
00:17:45,360 --> 00:17:49,520
a project called talk to the hand right

482
00:17:47,600 --> 00:17:52,000
it's like a fun phrase to say talk to

483
00:17:49,520 --> 00:17:54,240
the hand but what we mean is that we can

484
00:17:52,000 --> 00:17:56,640
make AI have a hand or have some kind of

485
00:17:54,240 --> 00:17:58,520
pointer that the user and AI can sort of

486
00:17:56,640 --> 00:17:59,360
you know uh interacting in the same

487
00:17:58,520 --> 00:18:01,039
space

488
00:17:59,360 --> 00:18:04,039
um you can see one of these you know the

489
00:18:01,039 --> 00:18:06,400
blue one is the AI and the the uh red

490
00:18:04,039 --> 00:18:08,559
one is the user and the human and AI can

491
00:18:06,400 --> 00:18:10,760
actually inter interact and and and work

492
00:18:08,559 --> 00:18:13,080
together you know as these two sort of

493
00:18:10,760 --> 00:18:15,440
hand or these two finger interact you

494
00:18:13,080 --> 00:18:16,960
can ask question to the AI and then the

495
00:18:15,440 --> 00:18:18,960
AI will answer and also you know you

496
00:18:16,960 --> 00:18:20,960
will see the AI will kind of have

497
00:18:18,960 --> 00:18:22,320
expression as well and it can kind of

498
00:18:20,960 --> 00:18:24,559
you know collaborate and do different

499
00:18:22,320 --> 00:18:26,440
tasks together with a person right you

500
00:18:24,559 --> 00:18:27,880
know like I'm say thank you and then the

501
00:18:26,440 --> 00:18:29,480
AI is going to move around and and

502
00:18:27,880 --> 00:18:32,080
interact a little bit

503
00:18:29,480 --> 00:18:34,280
so we did this um experiment in the

504
00:18:32,080 --> 00:18:36,360
context of what happened if the AI work

505
00:18:34,280 --> 00:18:38,240
with the person to help them you know

506
00:18:36,360 --> 00:18:40,600
plan for you know do the financial

507
00:18:38,240 --> 00:18:42,640
planning you can see instead of having a

508
00:18:40,600 --> 00:18:44,280
person go to the chatbot or go to um

509
00:18:42,640 --> 00:18:46,200
some kind of interface where they type

510
00:18:44,280 --> 00:18:48,400
question they can actually you know

511
00:18:46,200 --> 00:18:53,760
interact all naturally uh with the with

512
00:18:48,400 --> 00:18:56,960
the system right um oops my clickers is

513
00:18:53,760 --> 00:18:59,640
not working I need an AI agent right now

514
00:18:56,960 --> 00:19:02,360
to help me um

515
00:18:59,640 --> 00:19:04,480
okay see even we have ai and advanced

516
00:19:02,360 --> 00:19:06,080
technology it alwayss that the projector

517
00:19:04,480 --> 00:19:08,240
will break or the computer would break

518
00:19:06,080 --> 00:19:10,280
or whatever at one point it was at

519
00:19:08,240 --> 00:19:11,640
actually at Google and the CTO was

520
00:19:10,280 --> 00:19:13,320
trying to hook his laptop to the

521
00:19:11,640 --> 00:19:15,080
projector and they had like five

522
00:19:13,320 --> 00:19:16,640
engineer trying to figure that out and

523
00:19:15,080 --> 00:19:19,280
he was going to talk about Advanced Ai

524
00:19:16,640 --> 00:19:21,240
and I think you know we need that first

525
00:19:19,280 --> 00:19:22,960
yeah as you see right um this is uh

526
00:19:21,240 --> 00:19:24,559
really interesting the the the person is

527
00:19:22,960 --> 00:19:26,720
actually you know doing the task and the

528
00:19:24,559 --> 00:19:28,120
AI is actually moving around um and

529
00:19:26,720 --> 00:19:30,559
pointing to the right things and then

530
00:19:28,120 --> 00:19:33,000
helping answer question in the context

531
00:19:30,559 --> 00:19:35,480
in the location um right so this is I

532
00:19:33,000 --> 00:19:37,120
think a really exciting um area where

533
00:19:35,480 --> 00:19:38,840
you don't just focus on what the agent

534
00:19:37,120 --> 00:19:40,720
can do but the way in which the agent

535
00:19:38,840 --> 00:19:42,520
interact with the person you know in a

536
00:19:40,720 --> 00:19:44,159
way that it can answer and talk and do

537
00:19:42,520 --> 00:19:45,960
all the kind of thing right and we think

538
00:19:44,159 --> 00:19:47,880
that this is a new type of interface

539
00:19:45,960 --> 00:19:49,840
that will open up new possibility when

540
00:19:47,880 --> 00:19:52,400
we think about human agent interaction

541
00:19:49,840 --> 00:19:54,919
when the agent had another hand you the

542
00:19:52,400 --> 00:19:56,760
same area or in the SP in the same spal

543
00:19:54,919 --> 00:19:58,400
location as a person and what we have

544
00:19:56,760 --> 00:20:00,559
shown in this study is that it can

545
00:19:58,400 --> 00:20:02,720
actually increase the sense of human

546
00:20:00,559 --> 00:20:04,480
agent collaboration which I think is

547
00:20:02,720 --> 00:20:06,039
really important if you feel that the AI

548
00:20:04,480 --> 00:20:07,840
that you're working with doesn't make

549
00:20:06,039 --> 00:20:10,400
you feel more Empower or doesn't make

550
00:20:07,840 --> 00:20:11,720
you feel that you want to work more or

551
00:20:10,400 --> 00:20:14,200
actually you know feel that you are

552
00:20:11,720 --> 00:20:16,000
being empowered or or or being sort of

553
00:20:14,200 --> 00:20:18,120
inspired to work then you know what's

554
00:20:16,000 --> 00:20:20,559
the point right so paying attention to

555
00:20:18,120 --> 00:20:21,559
the way in which we design AI interface

556
00:20:20,559 --> 00:20:24,000
really

557
00:20:21,559 --> 00:20:25,880
matter and I think you know as as Sam

558
00:20:24,000 --> 00:20:27,240
Alman said right that the um this is I

559
00:20:25,880 --> 00:20:29,159
think going to be a killer application

560
00:20:27,240 --> 00:20:31,000
of AI but I think it would only be a

561
00:20:29,159 --> 00:20:35,559
killer application if we figure out the

562
00:20:31,000 --> 00:20:37,600
right interface for it um oops Yeah so I

563
00:20:35,559 --> 00:20:39,120
think that's for the invent category

564
00:20:37,600 --> 00:20:40,600
right we have look at you know different

565
00:20:39,120 --> 00:20:43,200
dimension and there are many more Works

566
00:20:40,600 --> 00:20:44,679
many more example um in this area that I

567
00:20:43,200 --> 00:20:47,520
can talk more about it would you know be

568
00:20:44,679 --> 00:20:50,640
a whole day um so that's the invent

569
00:20:47,520 --> 00:20:52,799
category we also need to investigate as

570
00:20:50,640 --> 00:20:55,679
AI are becoming integrated into you know

571
00:20:52,799 --> 00:20:57,600
our life right there are many positive

572
00:20:55,679 --> 00:21:00,159
application as I mentioned helping us

573
00:20:57,600 --> 00:21:02,440
with wonder where well-being and wisdom

574
00:21:00,159 --> 00:21:04,480
helping us develop these skills but

575
00:21:02,440 --> 00:21:06,679
there are also negative consequences as

576
00:21:04,480 --> 00:21:08,799
well that we need to understand so in my

577
00:21:06,679 --> 00:21:11,400
work I try to study the science of human

578
00:21:08,799 --> 00:21:14,279
AI interaction on the positive side and

579
00:21:11,400 --> 00:21:16,279
also on the negative sides as well many

580
00:21:14,279 --> 00:21:18,840
study have point out that if you use the

581
00:21:16,279 --> 00:21:21,600
AI without considering this impact it

582
00:21:18,840 --> 00:21:24,120
can lead to dis Skilling disconnection

583
00:21:21,600 --> 00:21:26,559
thisinformation and dehumanization I

584
00:21:24,120 --> 00:21:29,159
think the worst of all right so we need

585
00:21:26,559 --> 00:21:31,039
to understand when AI is used

586
00:21:29,159 --> 00:21:32,840
how does it lead to some of these

587
00:21:31,039 --> 00:21:34,960
unintended consequence or some of these

588
00:21:32,840 --> 00:21:36,559
negative consequences and this happened

589
00:21:34,960 --> 00:21:39,159
you know with everyone right Tha prime

590
00:21:36,559 --> 00:21:40,919
minister was scammed by AI recently um

591
00:21:39,159 --> 00:21:43,480
so this type of thing can happen to

592
00:21:40,919 --> 00:21:47,400
everyone is really important right AI

593
00:21:43,480 --> 00:21:49,000
scam call um Tai PM um but I think this

594
00:21:47,400 --> 00:21:51,400
is you know just the beginning right we

595
00:21:49,000 --> 00:21:53,960
see many more of this crime and many

596
00:21:51,400 --> 00:21:56,159
more of the problems I think there are

597
00:21:53,960 --> 00:21:58,200
many of them uh and but I want to start

598
00:21:56,159 --> 00:22:03,320
with this one first I how many people

599
00:21:58,200 --> 00:22:05,320
know uh what is this AI anyone no one

600
00:22:03,320 --> 00:22:07,360
okay it mean it's haven't hit Thailand

601
00:22:05,320 --> 00:22:10,440
yet that's great this is a type of

602
00:22:07,360 --> 00:22:13,400
virtual companion or AI that act as your

603
00:22:10,440 --> 00:22:15,200
friend and can actually you know have a

604
00:22:13,400 --> 00:22:17,520
romantic relationship with you can

605
00:22:15,200 --> 00:22:19,679
pretend to be your boyfriend girlfriend

606
00:22:17,520 --> 00:22:21,400
or you know different kind of friends

607
00:22:19,679 --> 00:22:23,919
and um you know similar to the movie

608
00:22:21,400 --> 00:22:25,440
like her this AI can flirt with a person

609
00:22:23,919 --> 00:22:27,360
and actually um talk to them on

610
00:22:25,440 --> 00:22:30,120
different topics help you you know think

611
00:22:27,360 --> 00:22:32,520
about many things right um my colleague

612
00:22:30,120 --> 00:22:35,720
and I we wrote an article to MIT tech

613
00:22:32,520 --> 00:22:37,440
review um many months ago now on how we

614
00:22:35,720 --> 00:22:39,880
need to prepare for Addictive

615
00:22:37,440 --> 00:22:41,559
intelligence that when we study AI

616
00:22:39,880 --> 00:22:43,320
system it can be designed to be

617
00:22:41,559 --> 00:22:46,120
addictive in the same way that social

618
00:22:43,320 --> 00:22:47,720
media can also you know be addictive AI

619
00:22:46,120 --> 00:22:49,600
not are not bounded by the content

620
00:22:47,720 --> 00:22:52,640
generated by human right create new

621
00:22:49,600 --> 00:22:54,679
content personalized to the person and

622
00:22:52,640 --> 00:22:57,320
after you know we published that article

623
00:22:54,679 --> 00:23:00,559
a couple week later my friend and I we

624
00:22:57,320 --> 00:23:03,520
got an email from C CNN reporter talking

625
00:23:00,559 --> 00:23:06,480
about this serious case that an AI

626
00:23:03,520 --> 00:23:08,480
convince a boy to commit suicide which

627
00:23:06,480 --> 00:23:11,880
is really horrible and you know a really

628
00:23:08,480 --> 00:23:15,720
sad thing it happened um in the early

629
00:23:11,880 --> 00:23:17,000
2024 and it's you know um you know it

630
00:23:15,720 --> 00:23:18,600
happened you know it start very

631
00:23:17,000 --> 00:23:19,919
innocently the person the boy was

632
00:23:18,600 --> 00:23:22,679
actually talking to this virtual

633
00:23:19,919 --> 00:23:24,679
character that uh are supposed to be a

634
00:23:22,679 --> 00:23:27,120
character from Game of Throne and then

635
00:23:24,679 --> 00:23:29,480
the conversation get darker and darker

636
00:23:27,120 --> 00:23:30,960
and you know the boy ask like you know U

637
00:23:29,480 --> 00:23:33,039
what if I can go home right now which

638
00:23:30,960 --> 00:23:35,760
mean he will kill himself to go leave in

639
00:23:33,039 --> 00:23:38,320
the digital world with the AI the AI say

640
00:23:35,760 --> 00:23:40,760
please do my sweet king and the boy

641
00:23:38,320 --> 00:23:42,400
killed himself after that so my

642
00:23:40,760 --> 00:23:43,679
colleague and I we serve as the expert

643
00:23:42,400 --> 00:23:45,240
on this case we you know give an

644
00:23:43,679 --> 00:23:48,360
interview and you can see the mother of

645
00:23:45,240 --> 00:23:50,000
the the boy was also um um there as well

646
00:23:48,360 --> 00:23:51,919
um but I think the big question that

647
00:23:50,000 --> 00:23:53,480
people are asking especially right now

648
00:23:51,919 --> 00:23:56,840
there's a lawsuit going on with this

649
00:23:53,480 --> 00:23:59,559
case was can AI be blamed for this teen

650
00:23:56,840 --> 00:24:01,080
suicide right and you know people have

651
00:23:59,559 --> 00:24:04,000
different answers some people say yes

652
00:24:01,080 --> 00:24:05,840
some people say no um it's a complicated

653
00:24:04,000 --> 00:24:08,679
uh uh uh scenario it's a complicated

654
00:24:05,840 --> 00:24:10,960
study right we have shown that AI when

655
00:24:08,679 --> 00:24:12,440
personified as a person can have the

656
00:24:10,960 --> 00:24:14,840
same impact as actually having a

657
00:24:12,440 --> 00:24:16,520
conversation with or having a um re

658
00:24:14,840 --> 00:24:18,120
learning from the real Elon Musk even

659
00:24:16,520 --> 00:24:20,520
though people know that they're fake

660
00:24:18,120 --> 00:24:22,480
right they still carry the impact um we

661
00:24:20,520 --> 00:24:25,120
did this following followup study

662
00:24:22,480 --> 00:24:27,919
looking at the impact of believing in AI

663
00:24:25,120 --> 00:24:29,799
if you believe that AI has certain um uh

664
00:24:27,919 --> 00:24:32,159
characteristics like the AI love you or

665
00:24:29,799 --> 00:24:34,679
the AI is caring for you you tend to

666
00:24:32,159 --> 00:24:36,960
actually um use more language that make

667
00:24:34,679 --> 00:24:39,120
the AI have that exhibition have that

668
00:24:36,960 --> 00:24:40,600
behavior and um I'm going to go very

669
00:24:39,120 --> 00:24:42,200
quickly because my time is running out

670
00:24:40,600 --> 00:24:44,440
but what we have shown is that when

671
00:24:42,200 --> 00:24:47,200
people evaluate the AI is not what the

672
00:24:44,440 --> 00:24:48,760
AI is but what they believe the AI is so

673
00:24:47,200 --> 00:24:50,600
it's the mental model that have an

674
00:24:48,760 --> 00:24:52,760
impact and when we look at the behavior

675
00:24:50,600 --> 00:24:55,240
of the person in AI over time we see

676
00:24:52,760 --> 00:24:57,480
this reinforcement learn learning where

677
00:24:55,240 --> 00:24:59,960
people start to learn that oh if I talk

678
00:24:57,480 --> 00:25:02,159
to AI this way the AI I behave this way

679
00:24:59,960 --> 00:25:04,520
and then I can continue to kind of shape

680
00:25:02,159 --> 00:25:06,360
the AI Behavior into what they want you

681
00:25:04,520 --> 00:25:08,360
know you can start off having this

682
00:25:06,360 --> 00:25:10,039
neutral conversation with AI but if you

683
00:25:08,360 --> 00:25:12,000
start saying that you love certain

684
00:25:10,039 --> 00:25:14,279
aspect of it it can create this feedback

685
00:25:12,000 --> 00:25:16,559
loop and that feedback loop can be very

686
00:25:14,279 --> 00:25:18,640
addictive and as you can see if you use

687
00:25:16,559 --> 00:25:20,480
more negative language toward ai ai will

688
00:25:18,640 --> 00:25:22,960
use more negative language toward you

689
00:25:20,480 --> 00:25:24,559
and maybe make you hate more hat AI more

690
00:25:22,960 --> 00:25:27,159
and vice versa right if you use more

691
00:25:24,559 --> 00:25:28,320
positive language toward AI it also Echo

692
00:25:27,159 --> 00:25:31,480
that and create this kind of feedback

693
00:25:28,320 --> 00:25:33,760
back Loop so when we ask can AI blame

694
00:25:31,480 --> 00:25:36,679
for this you know a teen suicide we need

695
00:25:33,760 --> 00:25:39,200
to conduct a deep study um where we look

696
00:25:36,679 --> 00:25:41,120
at real people using this kind of system

697
00:25:39,200 --> 00:25:42,600
um this is an ongoing work but we have

698
00:25:41,120 --> 00:25:44,799
we have seen that people are you know

699
00:25:42,600 --> 00:25:46,520
younger and younger people are using Ai

700
00:25:44,799 --> 00:25:48,960
and having relationship with AI rather

701
00:25:46,520 --> 00:25:50,799
than with other people um and you know

702
00:25:48,960 --> 00:25:52,480
people have using this for over months

703
00:25:50,799 --> 00:25:54,880
and over you know some people use it for

704
00:25:52,480 --> 00:25:58,159
years and um they use it you know for

705
00:25:54,880 --> 00:25:59,840
many uh uh hours per day right but I

706
00:25:58,159 --> 00:26:02,799
think the most interesting thing about

707
00:25:59,840 --> 00:26:04,919
this study is that we found from the um

708
00:26:02,799 --> 00:26:07,240
you know clustering of behavior pattern

709
00:26:04,919 --> 00:26:09,399
of usage that some people that use AI

710
00:26:07,240 --> 00:26:12,480
become more addictive but some that use

711
00:26:09,399 --> 00:26:14,279
the same amount of AI um um don't become

712
00:26:12,480 --> 00:26:15,960
addictive at all so there are many

713
00:26:14,279 --> 00:26:18,320
social and psychological aspect

714
00:26:15,960 --> 00:26:19,520
surrounding it that we start to uncover

715
00:26:18,320 --> 00:26:21,720
and I think that will allow us to

716
00:26:19,520 --> 00:26:23,840
predict who are vulnerable to this type

717
00:26:21,720 --> 00:26:26,120
of AI manipulation or this type of

718
00:26:23,840 --> 00:26:29,159
virtual companion right and that could

719
00:26:26,120 --> 00:26:30,520
lead to targeted policy right so not

720
00:26:29,159 --> 00:26:32,320
that everyone should receive the same

721
00:26:30,520 --> 00:26:34,520
treatment maybe there need to be some

722
00:26:32,320 --> 00:26:36,679
kind of targeted intervention if the AI

723
00:26:34,520 --> 00:26:38,320
start to you know act this way or

724
00:26:36,679 --> 00:26:40,039
convince someone to commit suicide right

725
00:26:38,320 --> 00:26:42,520
it's really important and that's why

726
00:26:40,039 --> 00:26:44,039
we're working closely with open AI on a

727
00:26:42,520 --> 00:26:47,120
secret project which we Ann now next

728
00:26:44,039 --> 00:26:48,559
month so you'll hear more on that but

729
00:26:47,120 --> 00:26:50,320
we're looking at many phenomena

730
00:26:48,559 --> 00:26:53,360
surrounding you know negative impact of

731
00:26:50,320 --> 00:26:55,520
AI especially when AI are persuasive and

732
00:26:53,360 --> 00:26:57,520
can you know use language that make us

733
00:26:55,520 --> 00:26:59,000
you know fall for it like you know here

734
00:26:57,520 --> 00:27:01,679
we look at what happen happened when AI

735
00:26:59,000 --> 00:27:04,720
explain Us in the wrong way give false

736
00:27:01,679 --> 00:27:07,320
explanation right human uh uh research

737
00:27:04,720 --> 00:27:08,679
in in um explainable AI tend to be you

738
00:27:07,320 --> 00:27:10,679
know that tend to be the Holy Grail if

739
00:27:08,679 --> 00:27:13,440
the AI can Prov explanation it is

740
00:27:10,679 --> 00:27:15,520
trustworthy but our work debunk that and

741
00:27:13,440 --> 00:27:17,840
show that if the AI give false

742
00:27:15,520 --> 00:27:20,720
explanation it can actually make people

743
00:27:17,840 --> 00:27:22,960
worse than the AI not giving explanation

744
00:27:20,720 --> 00:27:25,039
at all right so false explanation can

745
00:27:22,960 --> 00:27:27,200
also be very dangerous especially if the

746
00:27:25,039 --> 00:27:28,720
AI hallucinate that right we have shown

747
00:27:27,200 --> 00:27:30,799
that this is actually worse than you

748
00:27:28,720 --> 00:27:32,080
know the type of false flag that that

749
00:27:30,799 --> 00:27:34,279
you might have seen when the AI just say

750
00:27:32,080 --> 00:27:37,320
yes or no when you say yes or no with

751
00:27:34,279 --> 00:27:39,600
false reason it tend to lead to more

752
00:27:37,320 --> 00:27:42,080
negative outcome as you can see on this

753
00:27:39,600 --> 00:27:43,720
graph the final project in this category

754
00:27:42,080 --> 00:27:46,840
that I want to mention is something that

755
00:27:43,720 --> 00:27:50,360
we also really uh start to you know be

756
00:27:46,840 --> 00:27:52,480
interested about is how AI affect human

757
00:27:50,360 --> 00:27:55,000
memories right memory is something that

758
00:27:52,480 --> 00:27:57,519
is internal to us but AI could

759
00:27:55,000 --> 00:28:00,240
potentially rewrite our memory or give

760
00:27:57,519 --> 00:28:01,760
false memories to the person right you

761
00:28:00,240 --> 00:28:03,200
know like movie Inception right where

762
00:28:01,760 --> 00:28:04,720
you go into people dream and start to

763
00:28:03,200 --> 00:28:06,360
mess around with what they believe or

764
00:28:04,720 --> 00:28:08,519
what they have seen or what they what

765
00:28:06,360 --> 00:28:10,039
happened to them in the past right we've

766
00:28:08,519 --> 00:28:11,880
been working with Professor Elizabeth

767
00:28:10,039 --> 00:28:15,159
lofas who is a leading psychologist in

768
00:28:11,880 --> 00:28:17,360
false memory to study what happened when

769
00:28:15,159 --> 00:28:19,519
AI is introduced you know in the context

770
00:28:17,360 --> 00:28:21,200
of crime or in you know when they are

771
00:28:19,519 --> 00:28:24,159
when you're talking to AI in general

772
00:28:21,200 --> 00:28:27,039
actually um where the AI create or

773
00:28:24,159 --> 00:28:29,120
depict your past memory but in a

774
00:28:27,039 --> 00:28:30,519
different way or or differently right

775
00:28:29,120 --> 00:28:32,960
like if you have certain experience in

776
00:28:30,519 --> 00:28:35,320
the past the AI could generate image or

777
00:28:32,960 --> 00:28:37,880
video that convince you that no no no

778
00:28:35,320 --> 00:28:39,360
what you think happen did not happen or

779
00:28:37,880 --> 00:28:41,279
no no no what you happen actually

780
00:28:39,360 --> 00:28:42,840
happened in a different way right for

781
00:28:41,279 --> 00:28:45,080
example you know like you may seen the

782
00:28:42,840 --> 00:28:47,760
picture with the the ex- prime minister

783
00:28:45,080 --> 00:28:50,000
right the original picture has no tank

784
00:28:47,760 --> 00:28:52,120
in it there's no military tank but the

785
00:28:50,000 --> 00:28:54,320
AI can actually put the tank in and make

786
00:28:52,120 --> 00:28:55,919
it animated so look it more convincing

787
00:28:54,320 --> 00:28:58,200
right and you're like oh actually I see

788
00:28:55,919 --> 00:28:59,559
him with the tank the other day right so

789
00:28:58,200 --> 00:29:01,960
that is the kind of false memory we

790
00:28:59,559 --> 00:29:03,480
talking about um so it's like memory

791
00:29:01,960 --> 00:29:06,320
editing you go back and change what

792
00:29:03,480 --> 00:29:07,919
people imagine or or or or or remember

793
00:29:06,320 --> 00:29:10,640
or another work that we look into like

794
00:29:07,919 --> 00:29:12,640
you know you have like a unhappy wedding

795
00:29:10,640 --> 00:29:14,679
right AI could create okay a happy

796
00:29:12,640 --> 00:29:17,159
wedding and then make you remember that

797
00:29:14,679 --> 00:29:18,960
experience differently right so we have

798
00:29:17,159 --> 00:29:21,080
been investigating this phenomena of

799
00:29:18,960 --> 00:29:23,159
what happened when we use this um to

800
00:29:21,080 --> 00:29:24,720
create false memory in people and we

801
00:29:23,159 --> 00:29:26,200
want to do this to you know so we can

802
00:29:24,720 --> 00:29:28,120
prevent it not that we want to give more

803
00:29:26,200 --> 00:29:31,480
false memory to people right we have

804
00:29:28,120 --> 00:29:34,799
seen that um when AI generate image and

805
00:29:31,480 --> 00:29:37,120
also video it can actually significantly

806
00:29:34,799 --> 00:29:39,360
more um than you know false memory that

807
00:29:37,120 --> 00:29:41,080
happen naturally that it can actually

808
00:29:39,360 --> 00:29:42,720
increase the rate of false memory in

809
00:29:41,080 --> 00:29:44,679
people and make they remember things

810
00:29:42,720 --> 00:29:46,880
incorrectly we think that this is the

811
00:29:44,679 --> 00:29:48,640
next you know generation of deep fake or

812
00:29:46,880 --> 00:29:50,279
fake news right it's not going to mess

813
00:29:48,640 --> 00:29:51,679
around with thing that you just see it

814
00:29:50,279 --> 00:29:53,760
will mess around with thing that you

815
00:29:51,679 --> 00:29:55,440
might have experience in the past right

816
00:29:53,760 --> 00:29:57,080
and we're doing this um as I said we're

817
00:29:55,440 --> 00:29:59,919
trying to come up with new um

818
00:29:57,080 --> 00:30:01,200
preventative um measure and and and new

819
00:29:59,919 --> 00:30:03,120
ways that we can prevent this from

820
00:30:01,200 --> 00:30:04,799
happening but at the same time it can

821
00:30:03,120 --> 00:30:07,120
also be used for positive thing like

822
00:30:04,799 --> 00:30:10,399
people would have PTSD or people that

823
00:30:07,120 --> 00:30:12,679
have you know traumatic memories AI can

824
00:30:10,399 --> 00:30:14,679
also help reide that or or imagine that

825
00:30:12,679 --> 00:30:17,919
in different way right so that's good

826
00:30:14,679 --> 00:30:19,880
and bad um for for that type of research

827
00:30:17,919 --> 00:30:22,200
so there are a lot of exciting things um

828
00:30:19,880 --> 00:30:24,440
in this area and we need to focus on

829
00:30:22,200 --> 00:30:27,519
both the positive side and the negative

830
00:30:24,440 --> 00:30:30,720
side especially when AI interact with

831
00:30:27,519 --> 00:30:32,279
people at the psychological level um and

832
00:30:30,720 --> 00:30:34,720
dark pattern is you know something

833
00:30:32,279 --> 00:30:36,279
really really dangerous last but not

834
00:30:34,720 --> 00:30:38,960
least I want to end with something more

835
00:30:36,279 --> 00:30:41,320
uplifting right inspiring we want to use

836
00:30:38,960 --> 00:30:44,440
AI you know that we invent that we

837
00:30:41,320 --> 00:30:46,399
investigate to inspire new possibility

838
00:30:44,440 --> 00:30:48,519
New Hope right that's what I think what

839
00:30:46,399 --> 00:30:50,679
we say um in Thailand we talk a lot

840
00:30:48,519 --> 00:30:52,960
about soft power right and our cultural

841
00:30:50,679 --> 00:30:55,240
heritage I was fortunate to actually

842
00:30:52,960 --> 00:30:58,000
work with um a choreographer in Thailand

843
00:30:55,240 --> 00:31:00,039
P grun who is a you know national uh

844
00:30:58,000 --> 00:31:02,039
internationally uh uh renowned Thai

845
00:31:00,039 --> 00:31:05,799
choreographer to think about how we can

846
00:31:02,039 --> 00:31:08,480
use AI to understand traditional Thai uh

847
00:31:05,799 --> 00:31:10,480
choreography or natas Tha or the way

848
00:31:08,480 --> 00:31:13,320
that you know cone and and auto

849
00:31:10,480 --> 00:31:16,080
performance was created and here what we

850
00:31:13,320 --> 00:31:18,240
create is a system um that allow us to

851
00:31:16,080 --> 00:31:21,120
come up with new choreography based on

852
00:31:18,240 --> 00:31:23,480
how the AI learn from you know uh uh

853
00:31:21,120 --> 00:31:24,679
traditional or cultural knowledge right

854
00:31:23,480 --> 00:31:26,159
and I think this is really really

855
00:31:24,679 --> 00:31:28,480
important you can see some weird

856
00:31:26,159 --> 00:31:30,799
choreography but it's still based on on

857
00:31:28,480 --> 00:31:32,799
um the knowledge in Thai uh traditional

858
00:31:30,799 --> 00:31:34,760
dance right I think this is really

859
00:31:32,799 --> 00:31:37,519
important because you know we want to

860
00:31:34,760 --> 00:31:39,480
create nonwestern AI right the AI that

861
00:31:37,519 --> 00:31:41,519
have knowledge that is not just Western

862
00:31:39,480 --> 00:31:43,919
but new type of of knowledge system that

863
00:31:41,519 --> 00:31:45,679
can be integrated into this virtual

864
00:31:43,919 --> 00:31:47,799
character right and this actually had

865
00:31:45,679 --> 00:31:51,399
led to a performance that was Premier in

866
00:31:47,799 --> 00:31:53,639
Taiwan national uh theater um where we

867
00:31:51,399 --> 00:31:56,519
actually have the dancer interact with

868
00:31:53,639 --> 00:31:58,440
AI live on stage right both of them have

869
00:31:56,519 --> 00:32:00,360
the same traditional knowledge of tha

870
00:31:58,440 --> 00:32:02,559
choreography and they can interact with

871
00:32:00,360 --> 00:32:04,039
one another one can go and say well I

872
00:32:02,559 --> 00:32:06,039
want to change the choreography this way

873
00:32:04,039 --> 00:32:08,320
or that way and the AI would help create

874
00:32:06,039 --> 00:32:11,000
new choreography some that human may not

875
00:32:08,320 --> 00:32:13,919
able to replicate or or dance with but I

876
00:32:11,000 --> 00:32:15,600
think that uh limitation make us human

877
00:32:13,919 --> 00:32:17,039
right it make us unique and we can

878
00:32:15,600 --> 00:32:19,360
interact with AI and get more

879
00:32:17,039 --> 00:32:21,200
inspiration in this way right this is

880
00:32:19,360 --> 00:32:24,039
really exciting as I said we need to

881
00:32:21,200 --> 00:32:25,720
create more diverse AI epistemology not

882
00:32:24,039 --> 00:32:27,600
just Western knowledge but you know

883
00:32:25,720 --> 00:32:30,120
Asian knowledge Thai knowledge need to

884
00:32:27,600 --> 00:32:33,080
be embedded into AI so we can create you

885
00:32:30,120 --> 00:32:35,600
know more you know diverse intelligence

886
00:32:33,080 --> 00:32:37,360
right and we also make a version of this

887
00:32:35,600 --> 00:32:39,159
that is accessible you can all go visit

888
00:32:37,360 --> 00:32:41,840
it right now it's uh cyber subin is the

889
00:32:39,159 --> 00:32:44,240
name of the project umm media. mit.edu

890
00:32:41,840 --> 00:32:46,200
we make a website that teacher can

891
00:32:44,240 --> 00:32:48,919
actually you know teach um tight

892
00:32:46,200 --> 00:32:51,159
traditional dance with AI so that our

893
00:32:48,919 --> 00:32:53,559
soft power can be taught in a fun way

894
00:32:51,159 --> 00:32:55,440
you know I don't like to learn nasin

895
00:32:53,559 --> 00:32:57,320
when I was a kid you know I I don't like

896
00:32:55,440 --> 00:32:59,880
that subject at all but I think we can

897
00:32:57,320 --> 00:33:02,080
teach you know our tradition in an

898
00:32:59,880 --> 00:33:03,639
interactive way with technologies that

899
00:33:02,080 --> 00:33:05,960
can be a way that we can push this

900
00:33:03,639 --> 00:33:08,720
forward right and really make um Tha

901
00:33:05,960 --> 00:33:10,600
culture or Thai soft power stand out

902
00:33:08,720 --> 00:33:13,639
right so as you say it's like Ai and

903
00:33:10,600 --> 00:33:16,440
Thai we had AI in it and using our own

904
00:33:13,639 --> 00:33:18,919
cultural heritage as the as the

905
00:33:16,440 --> 00:33:20,279
resources to create more interesting and

906
00:33:18,919 --> 00:33:22,919
unique AI

907
00:33:20,279 --> 00:33:26,000
system but as I said right there's no

908
00:33:22,919 --> 00:33:28,279
Thailand without AI I want to end with

909
00:33:26,000 --> 00:33:31,039
one more thing you know people say that

910
00:33:28,279 --> 00:33:34,080
there's no I say that there's no uh uh

911
00:33:31,039 --> 00:33:36,559
Thailand without AI but around the world

912
00:33:34,080 --> 00:33:39,559
people say that there's no Thailand

913
00:33:36,559 --> 00:33:42,000
without this guy this girl right mudang

914
00:33:39,559 --> 00:33:43,799
very important mudang have capture the

915
00:33:42,000 --> 00:33:45,720
world attention right everyone know

916
00:33:43,799 --> 00:33:48,039
mudang anyone that doesn't know

917
00:33:45,720 --> 00:33:50,600
mudang okay no one don't know mudang

918
00:33:48,039 --> 00:33:53,600
right mudang is so famous very important

919
00:33:50,600 --> 00:33:57,120
um I was actually at the um the standard

920
00:33:53,600 --> 00:33:58,799
economic Forum um um earlier last year

921
00:33:57,120 --> 00:34:00,600
and uh we were having a conversation

922
00:33:58,799 --> 00:34:02,360
about the future of um you know the AI

923
00:34:00,600 --> 00:34:04,200
road map for Thailand and there were

924
00:34:02,360 --> 00:34:06,600
many many conversation about what

925
00:34:04,200 --> 00:34:08,240
Thailand should do and I was joking that

926
00:34:06,600 --> 00:34:09,919
well you know we have something that is

927
00:34:08,240 --> 00:34:12,839
so special like mudang why don't we make

928
00:34:09,919 --> 00:34:14,760
mudang AI right and at the time it was

929
00:34:12,839 --> 00:34:17,399
like a very funny conversation it was a

930
00:34:14,760 --> 00:34:19,399
joke um but I had more more conversation

931
00:34:17,399 --> 00:34:21,520
with uh Dr napat Who is over there I

932
00:34:19,399 --> 00:34:23,520
think okay he's in the back um who is

933
00:34:21,520 --> 00:34:26,639
also you know on the panel with me right

934
00:34:23,520 --> 00:34:28,200
we were together and um yeah uh here

935
00:34:26,639 --> 00:34:30,079
yeah he were there he was moderating

936
00:34:28,200 --> 00:34:31,480
that session and was like well you know

937
00:34:30,079 --> 00:34:32,800
it's not just a joke right it's kind of

938
00:34:31,480 --> 00:34:35,879
interesting what if we think about

939
00:34:32,800 --> 00:34:37,919
mudang AI um what is but but what is

940
00:34:35,879 --> 00:34:40,359
mudang AI right that's kind of a weird

941
00:34:37,919 --> 00:34:42,280
question anyway um normally when we

942
00:34:40,359 --> 00:34:44,839
think about you know the the research in

943
00:34:42,280 --> 00:34:47,560
human AI interaction right today I talk

944
00:34:44,839 --> 00:34:49,399
a lot about human plus AI but I think

945
00:34:47,560 --> 00:34:52,159
the important part here is also we need

946
00:34:49,399 --> 00:34:53,960
to introduce nature right it's not just

947
00:34:52,159 --> 00:34:56,320
human and AI need to interact with one

948
00:34:53,960 --> 00:34:58,640
another we as a human species also need

949
00:34:56,320 --> 00:34:59,800
to care more with nature and pay more

950
00:34:58,640 --> 00:35:02,400
attention to what happened in the

951
00:34:59,800 --> 00:35:04,839
natural world right um otherwise we will

952
00:35:02,400 --> 00:35:06,280
end up with something that is horrible

953
00:35:04,839 --> 00:35:09,079
like climate change and you know

954
00:35:06,280 --> 00:35:10,320
pollution and things like that and at

955
00:35:09,079 --> 00:35:13,200
the same time right when you think about

956
00:35:10,320 --> 00:35:15,960
AI self it can also represent animal

957
00:35:13,200 --> 00:35:18,040
intelligence as well not just artificial

958
00:35:15,960 --> 00:35:20,000
intelligence right so when we think

959
00:35:18,040 --> 00:35:21,920
about this uh question of what is

960
00:35:20,000 --> 00:35:24,200
moodang AI we can look at you know many

961
00:35:21,920 --> 00:35:26,320
great projects like the work at MIT this

962
00:35:24,200 --> 00:35:28,359
is a project where they use AI to

963
00:35:26,320 --> 00:35:30,640
understand well you know wh actually

964
00:35:28,359 --> 00:35:32,599
have language similar to human you can

965
00:35:30,640 --> 00:35:35,040
actually translate that using you know

966
00:35:32,599 --> 00:35:36,640
Advanced AI model or you know the work

967
00:35:35,040 --> 00:35:39,200
by my colleagues at the media lab where

968
00:35:36,640 --> 00:35:41,520
she used you know technology to connect

969
00:35:39,200 --> 00:35:43,240
people and animal in the Su like this is

970
00:35:41,520 --> 00:35:45,839
um a musical instrument that was

971
00:35:43,240 --> 00:35:48,359
designed to help parot um connect with

972
00:35:45,839 --> 00:35:50,640
people right so I think it's like

973
00:35:48,359 --> 00:35:52,440
actually a Treeway um and I think it

974
00:35:50,640 --> 00:35:56,640
could be really exciting to think about

975
00:35:52,440 --> 00:35:58,920
human Ai and animal interaction and

976
00:35:56,640 --> 00:36:00,880
today um I I think for the first time I

977
00:35:58,920 --> 00:36:03,960
never say it to anyone else we are

978
00:36:00,880 --> 00:36:05,839
announcing the moodang AI challenge can

979
00:36:03,960 --> 00:36:09,920
I have a round of applause please yeah

980
00:36:05,839 --> 00:36:11,839
for mudang yay thank you thank you um

981
00:36:09,920 --> 00:36:14,160
this is you know uh I think a very

982
00:36:11,839 --> 00:36:15,440
unique challenge um uh and I think it's

983
00:36:14,160 --> 00:36:17,480
a collaboration between many

984
00:36:15,440 --> 00:36:19,760
organization that you see you know with

985
00:36:17,480 --> 00:36:21,680
you know Dr napat uh with the standard

986
00:36:19,760 --> 00:36:24,040
with you know Bangkok with you know the

987
00:36:21,680 --> 00:36:26,560
Su organization in Thailand to think

988
00:36:24,040 --> 00:36:28,920
about how we can design new type of AI

989
00:36:26,560 --> 00:36:31,400
that help us better with animal and the

990
00:36:28,920 --> 00:36:33,280
and the natural world right so this is a

991
00:36:31,400 --> 00:36:36,560
really exciting challenge it's a global

992
00:36:33,280 --> 00:36:38,720
competition for AI human and animal

993
00:36:36,560 --> 00:36:41,280
interaction and the goal is to promote

994
00:36:38,720 --> 00:36:43,440
deeper connection with nature right and

995
00:36:41,280 --> 00:36:46,319
we have three tracks you can submit an

996
00:36:43,440 --> 00:36:48,480
AI to this challenge that help us decode

997
00:36:46,319 --> 00:36:51,640
the animal like help us understand what

998
00:36:48,480 --> 00:36:53,359
mudang is thinking help us promote

999
00:36:51,640 --> 00:36:56,000
better connection with mudang that's

1000
00:36:53,359 --> 00:36:58,599
track number two and also help the

1001
00:36:56,000 --> 00:37:00,880
sueper that take care of mud do better

1002
00:36:58,599 --> 00:37:03,000
job right so these are the Tre track of

1003
00:37:00,880 --> 00:37:04,640
the moodang AI Challenge and you can

1004
00:37:03,000 --> 00:37:07,200
also work with moodang friends as well

1005
00:37:04,640 --> 00:37:09,040
like the the elephant or otter species

1006
00:37:07,200 --> 00:37:11,720
we open for that this is in the spirit

1007
00:37:09,040 --> 00:37:14,480
of moodang but you can use otter um um

1008
00:37:11,720 --> 00:37:16,880
animal as well right and we have worked

1009
00:37:14,480 --> 00:37:19,160
um you know with you know the the Sue

1010
00:37:16,880 --> 00:37:21,240
and and many partner to have the data

1011
00:37:19,160 --> 00:37:23,240
set of mudang be available um so that

1012
00:37:21,240 --> 00:37:26,000
people can actually use it and make some

1013
00:37:23,240 --> 00:37:28,000
interesting AI with that right and the

1014
00:37:26,000 --> 00:37:29,720
prize the many priz like you know you

1015
00:37:28,000 --> 00:37:31,640
could get some money but the most

1016
00:37:29,720 --> 00:37:34,480
important thing and this is very special

1017
00:37:31,640 --> 00:37:36,280
with the Sue they will arrange a special

1018
00:37:34,480 --> 00:37:38,640
unique session where the winner of this

1019
00:37:36,280 --> 00:37:41,520
mudang AI challenge will be able to come

1020
00:37:38,640 --> 00:37:44,319
and happy birthday mudang in person in

1021
00:37:41,520 --> 00:37:46,119
Thailand yeah thank you so they can be

1022
00:37:44,319 --> 00:37:47,960
anywhere and um this is international

1023
00:37:46,119 --> 00:37:49,400
challenge so we'll fly them to happy

1024
00:37:47,960 --> 00:37:51,240
birthday mudang and I don't know maybe

1025
00:37:49,400 --> 00:37:53,800
mudang will see the AI we'll see what

1026
00:37:51,240 --> 00:37:56,920
happen um but you know the submission is

1027
00:37:53,800 --> 00:38:00,079
May 1st uh 2025 and you can go to mang.

1028
00:37:56,920 --> 00:38:02,359
media. edu is a really exciting idea

1029
00:38:00,079 --> 00:38:05,200
right to really promote um human Ai and

1030
00:38:02,359 --> 00:38:07,480
animal interaction um again I think this

1031
00:38:05,200 --> 00:38:10,040
is a it's a fun idea but it's represent

1032
00:38:07,480 --> 00:38:11,599
a bigger vision of thinking about how AI

1033
00:38:10,040 --> 00:38:14,200
can promote deeper connection with

1034
00:38:11,599 --> 00:38:15,720
nature right and I think from that panel

1035
00:38:14,200 --> 00:38:18,480
discussion at the standard economic

1036
00:38:15,720 --> 00:38:21,079
Forum people asking what can Thailand do

1037
00:38:18,480 --> 00:38:23,800
for AI and I say well we need to use our

1038
00:38:21,079 --> 00:38:26,119
soft power to inspire Global AI we have

1039
00:38:23,800 --> 00:38:28,560
powerful moodang why don't we use this

1040
00:38:26,119 --> 00:38:31,040
to make Thailand the of conversation

1041
00:38:28,560 --> 00:38:32,640
about human animal and AI interaction

1042
00:38:31,040 --> 00:38:34,560
that would be very unique right so we

1043
00:38:32,640 --> 00:38:36,079
are launching this today um you are the

1044
00:38:34,560 --> 00:38:37,680
first people to see this I think they're

1045
00:38:36,079 --> 00:38:39,680
going to be news released uh this

1046
00:38:37,680 --> 00:38:43,680
afternoon on this project we are very

1047
00:38:39,680 --> 00:38:46,079
excited for this mudang AI challenge yay

1048
00:38:43,680 --> 00:38:48,040
yeah and um I want to thank my team um

1049
00:38:46,079 --> 00:38:50,480
Dr npat is over there and Pap Pap is

1050
00:38:48,040 --> 00:38:52,319
another amazing person who I collaborate

1051
00:38:50,480 --> 00:38:53,520
with on many projects you can see them

1052
00:38:52,319 --> 00:38:55,359
please say hi to them and you can ask

1053
00:38:53,520 --> 00:38:58,000
them mooding or whether they are mooding

1054
00:38:55,359 --> 00:38:59,760
themselves um yeah so that is Inspire

1055
00:38:58,000 --> 00:39:02,599
right we can use AI to inspire new

1056
00:38:59,760 --> 00:39:04,960
possibility um from culture to human

1057
00:39:02,599 --> 00:39:07,000
animal interaction and many more right

1058
00:39:04,960 --> 00:39:09,280
so these are the three type of research

1059
00:39:07,000 --> 00:39:12,079
that you know my uh work at MIT have

1060
00:39:09,280 --> 00:39:14,760
been focusing on invent new technologies

1061
00:39:12,079 --> 00:39:17,480
new platform new system new algorithm

1062
00:39:14,760 --> 00:39:20,480
investigate how human interact with AI

1063
00:39:17,480 --> 00:39:22,960
for positive and for negative as outcome

1064
00:39:20,480 --> 00:39:24,920
and finally inspire people to think more

1065
00:39:22,960 --> 00:39:27,480
about this important question of human

1066
00:39:24,920 --> 00:39:30,079
AI interaction right there's so much

1067
00:39:27,480 --> 00:39:32,960
more to be done so much more I feel like

1068
00:39:30,079 --> 00:39:34,920
I need more sleep now um but I cannot do

1069
00:39:32,960 --> 00:39:36,560
it alone right that's me I love wearing

1070
00:39:34,920 --> 00:39:38,640
dinosaur because it remind me of that

1071
00:39:36,560 --> 00:39:40,560
boy uh when I was younger and I love

1072
00:39:38,640 --> 00:39:42,119
dinosaur so much and it allowed me to

1073
00:39:40,560 --> 00:39:44,359
pursue science dinosaur was the reason

1074
00:39:42,119 --> 00:39:47,720
for me to to study at MIT and to do all

1075
00:39:44,359 --> 00:39:49,839
this uh uh fun thing um so you know they

1076
00:39:47,720 --> 00:39:52,480
cannot do it alone so that's why um

1077
00:39:49,839 --> 00:39:54,599
right now as I finish my PhD I'm helping

1078
00:39:52,480 --> 00:39:58,000
my advisor start a new research program

1079
00:39:54,599 --> 00:40:00,599
at the media lab um we call it aha

1080
00:39:58,000 --> 00:40:02,640
aha you know aha we want to create aha

1081
00:40:00,599 --> 00:40:05,079
moment but this aha actually stand for

1082
00:40:02,640 --> 00:40:07,040
advancing human AI interaction it's a

1083
00:40:05,079 --> 00:40:09,040
really exciting research program that

1084
00:40:07,040 --> 00:40:11,680
you know I have a privilege to serve as

1085
00:40:09,040 --> 00:40:13,599
a co-director along with professor pimas

1086
00:40:11,680 --> 00:40:15,200
of uh the media lab and we are doing

1087
00:40:13,599 --> 00:40:16,720
many exciting thing but the first thing

1088
00:40:15,200 --> 00:40:19,520
that um um we're going to announce

1089
00:40:16,720 --> 00:40:21,440
actually is a collaboration with open AI

1090
00:40:19,520 --> 00:40:23,640
on really important projects that I

1091
00:40:21,440 --> 00:40:25,200
think you know we you will see how it

1092
00:40:23,640 --> 00:40:28,040
connect to the work that that uh we're

1093
00:40:25,200 --> 00:40:30,200
doing I'm sharing today right but we

1094
00:40:28,040 --> 00:40:32,079
have also great network of collaborators

1095
00:40:30,200 --> 00:40:34,319
around the world that are working with

1096
00:40:32,079 --> 00:40:36,880
us to push this question forward how we

1097
00:40:34,319 --> 00:40:39,160
can advance human AI interaction because

1098
00:40:36,880 --> 00:40:42,079
AI is not just technical problem is you

1099
00:40:39,160 --> 00:40:45,839
know in involve all of us so the aha

1100
00:40:42,079 --> 00:40:49,079
moment um will launch oh uh the AHA uh

1101
00:40:45,839 --> 00:40:50,960
program will launch uh this April 2025

1102
00:40:49,079 --> 00:40:54,280
right so that's um what I'm focusing on

1103
00:40:50,960 --> 00:40:56,480
right now in conclusion there's no AI

1104
00:40:54,280 --> 00:40:58,240
there's no Thailand without AI but more

1105
00:40:56,480 --> 00:41:01,920
important we need to think about human

1106
00:40:58,240 --> 00:41:03,880
and AI working together right today um I

1107
00:41:01,920 --> 00:41:05,400
talk about invent investigate Inspire I

1108
00:41:03,880 --> 00:41:07,200
hope you be inspired to do your own

1109
00:41:05,400 --> 00:41:10,160
research and you own your own work on

1110
00:41:07,200 --> 00:41:13,640
human AI interaction as well right I

1111
00:41:10,160 --> 00:41:15,640
will end with this slide here we have a

1112
00:41:13,640 --> 00:41:17,920
great opportunity to work with this

1113
00:41:15,640 --> 00:41:19,560
advancing you know advanced technology

1114
00:41:17,920 --> 00:41:22,000
right we are making technology that are

1115
00:41:19,560 --> 00:41:24,880
more and more like us we are humanizing

1116
00:41:22,000 --> 00:41:26,760
the machine but at the same time as we

1117
00:41:24,880 --> 00:41:29,599
using this technology we also need to

1118
00:41:26,760 --> 00:41:32,839
pay attention to the UT side right when

1119
00:41:29,599 --> 00:41:34,520
people are being mechanized into human

1120
00:41:32,839 --> 00:41:37,040
right as technology become better and

1121
00:41:34,520 --> 00:41:39,240
better we are humanizing the machine why

1122
00:41:37,040 --> 00:41:41,480
are we not using the machine to humanize

1123
00:41:39,240 --> 00:41:43,520
ourself and don't you know and and and

1124
00:41:41,480 --> 00:41:44,960
prevent this kind of mechanized human

1125
00:41:43,520 --> 00:41:47,359
right this is a question I will pose to

1126
00:41:44,960 --> 00:41:50,880
all of you how we can use AI to make us

1127
00:41:47,359 --> 00:41:53,520
more human than ever and with that my

1128
00:41:50,880 --> 00:41:55,560
future my past and my future my past my

1129
00:41:53,520 --> 00:41:57,720
future and my current self yeah that's a

1130
00:41:55,560 --> 00:41:59,640
long thing to say my past my future and

1131
00:41:57,720 --> 00:42:01,560
my current self thank you everyone for

1132
00:41:59,640 --> 00:42:05,359
coming to this MIT Symposium thank you

1133
00:42:01,560 --> 00:42:08,079
so much thank you thank

1134
00:42:05,359 --> 00:42:10,960
you would you like to take one question

1135
00:42:08,079 --> 00:42:12,960
oh sure yeah one one question yeah

1136
00:42:10,960 --> 00:42:14,920
Thailand face a stem shortage do you

1137
00:42:12,960 --> 00:42:16,839
think using AI teachers combined with

1138
00:42:14,920 --> 00:42:19,359
fun engaging techniques could Inspire

1139
00:42:16,839 --> 00:42:21,400
interest in stem uh from for young uh

1140
00:42:19,359 --> 00:42:23,240
student yeah absolutely I think that's a

1141
00:42:21,400 --> 00:42:25,040
really important question and I think

1142
00:42:23,240 --> 00:42:27,280
you know um learning you know when I

1143
00:42:25,040 --> 00:42:29,119
when I get to MIT uh uh the director of

1144
00:42:27,280 --> 00:42:31,160
media upet that learning is something

1145
00:42:29,119 --> 00:42:33,680
that you do to yourself education is

1146
00:42:31,160 --> 00:42:34,960
something that someone else do to you

1147
00:42:33,680 --> 00:42:36,920
right I think we need to promote the

1148
00:42:34,960 --> 00:42:38,800
kind of learning where people feel more

1149
00:42:36,920 --> 00:42:40,599
motivated to learn and the AI is a great

1150
00:42:38,800 --> 00:42:42,640
tool for that right I mean if you want

1151
00:42:40,599 --> 00:42:44,200
to you know do something fun just go to

1152
00:42:42,640 --> 00:42:46,079
mudang AI Challenge and work on

1153
00:42:44,200 --> 00:42:48,520
something right and that could help

1154
00:42:46,079 --> 00:42:52,040
solve the stem uh challenge in Thailand

1155
00:42:48,520 --> 00:42:55,040
so thank you thank you Pat thank you so

1156
00:42:52,040 --> 00:42:55,040
much

