1
00:00:05,520 --> 00:00:09,519
i guess then

2
00:00:06,879 --> 00:00:12,080
we'll start maybe i'll just kick off so

3
00:00:09,519 --> 00:00:13,599
i'm dan huttenlocker

4
00:00:12,080 --> 00:00:14,719
i'm the dean of the schwartzman college

5
00:00:13,599 --> 00:00:17,119
of computing

6
00:00:14,719 --> 00:00:19,359
uh

7
00:00:17,119 --> 00:00:21,359
came back to mit about three decades

8
00:00:19,359 --> 00:00:23,680
after getting my phd here

9
00:00:21,359 --> 00:00:26,640
uh and then a few months later kovitz

10
00:00:23,680 --> 00:00:28,000
started so it's been an interesting uh

11
00:00:26,640 --> 00:00:29,599
interesting trip

12
00:00:28,000 --> 00:00:31,439
uh i've been working i've had the

13
00:00:29,599 --> 00:00:35,520
privilege and pleasure of working with

14
00:00:31,439 --> 00:00:37,280
eric uh for gosh a decade or more now

15
00:00:35,520 --> 00:00:39,120
uh eric was uh

16
00:00:37,280 --> 00:00:40,719
ceo of google at the time and was very

17
00:00:39,120 --> 00:00:42,399
and google was very involved in the

18
00:00:40,719 --> 00:00:43,920
cornell tech campus that we built in new

19
00:00:42,399 --> 00:00:45,520
york city that i was leading that

20
00:00:43,920 --> 00:00:47,039
project

21
00:00:45,520 --> 00:00:48,640
and then

22
00:00:47,039 --> 00:00:51,199
starting in new york and continuing

23
00:00:48,640 --> 00:00:53,280
after i moved up here

24
00:00:51,199 --> 00:00:55,920
eric and i worked on a book together

25
00:00:53,280 --> 00:00:58,480
with dr henry kissinger

26
00:00:55,920 --> 00:00:59,920
called the age of ai in our human future

27
00:00:58,480 --> 00:01:02,559
which uh

28
00:00:59,920 --> 00:01:05,280
came out in the fall and uh well i guess

29
00:01:02,559 --> 00:01:07,840
november so the very late fall

30
00:01:05,280 --> 00:01:10,799
and we thought it would be fun to uh in

31
00:01:07,840 --> 00:01:13,119
the mit context just have uh so as

32
00:01:10,799 --> 00:01:15,040
doglar the eecs uh

33
00:01:13,119 --> 00:01:16,560
department head and my deputy dean in

34
00:01:15,040 --> 00:01:19,200
the college of computing

35
00:01:16,560 --> 00:01:20,640
uh ask us some questions related to some

36
00:01:19,200 --> 00:01:22,080
of the kinds of topics we talked about

37
00:01:20,640 --> 00:01:23,680
in the book and then turn it over for

38
00:01:22,080 --> 00:01:25,280
audience questions

39
00:01:23,680 --> 00:01:26,880
so eric thanks so much for joining us

40
00:01:25,280 --> 00:01:29,520
here today

41
00:01:26,880 --> 00:01:33,759
thank you and um dana you actually

42
00:01:29,520 --> 00:01:36,320
worked for me under my command i did um

43
00:01:33,759 --> 00:01:38,479
for a couple of years as a visiting

44
00:01:36,320 --> 00:01:41,119
faculty member at google

45
00:01:38,479 --> 00:01:42,799
where we first met yep and he became the

46
00:01:41,119 --> 00:01:44,159
dean of carnegie

47
00:01:42,799 --> 00:01:46,320
cornell tech

48
00:01:44,159 --> 00:01:48,320
and uh which was really transformative

49
00:01:46,320 --> 00:01:49,920
and i think that convinced mit that he

50
00:01:48,320 --> 00:01:51,439
could actually manage which he really is

51
00:01:49,920 --> 00:01:52,960
very good at so

52
00:01:51,439 --> 00:01:54,479
your transformation here has been

53
00:01:52,960 --> 00:01:57,520
fantastic and

54
00:01:54,479 --> 00:01:59,280
and that was actually 2007 it was uh

55
00:01:57,520 --> 00:02:01,520
getting to be a while ago so it's more

56
00:01:59,280 --> 00:02:03,360
than a decade 50 50 you should be good

57
00:02:01,520 --> 00:02:04,399
at math 15 years

58
00:02:03,360 --> 00:02:06,960
and

59
00:02:04,399 --> 00:02:09,280
but what is mit is doing now

60
00:02:06,960 --> 00:02:11,039
i've gotten to know mit well enough

61
00:02:09,280 --> 00:02:13,200
to know that mit is

62
00:02:11,039 --> 00:02:16,000
making the transition

63
00:02:13,200 --> 00:02:18,560
from a culture which is sort of

64
00:02:16,000 --> 00:02:21,200
obsessive about strange details is my

65
00:02:18,560 --> 00:02:24,959
description of mit

66
00:02:21,200 --> 00:02:26,800
into some of these broader themes um

67
00:02:24,959 --> 00:02:28,959
and uh my first meeting here i don't

68
00:02:26,800 --> 00:02:31,440
think i ever told you this when when uh

69
00:02:28,959 --> 00:02:32,800
raphael is a fantastic president

70
00:02:31,440 --> 00:02:34,400
said i should come by and visit they

71
00:02:32,800 --> 00:02:35,280
presented me a project which was still

72
00:02:34,400 --> 00:02:37,280
being

73
00:02:35,280 --> 00:02:39,040
developed in lisp

74
00:02:37,280 --> 00:02:40,959
and i thought okay we need to

75
00:02:39,040 --> 00:02:42,640
update that a bit so i think we've made

76
00:02:40,959 --> 00:02:43,680
some progress

77
00:02:42,640 --> 00:02:45,440
right

78
00:02:43,680 --> 00:02:46,959
we still love to obsess on the details

79
00:02:45,440 --> 00:02:48,560
but the big picture is also super

80
00:02:46,959 --> 00:02:51,599
important

81
00:02:48,560 --> 00:02:53,200
well uh i'm very delighted to welcome

82
00:02:51,599 --> 00:02:55,680
eric to mit today and have this

83
00:02:53,200 --> 00:02:57,840
conversation with dan and eric

84
00:02:55,680 --> 00:03:01,599
on a very important topic about you know

85
00:02:57,840 --> 00:03:03,200
how ai transforms the human society and

86
00:03:01,599 --> 00:03:04,879
i think this is a great conversation to

87
00:03:03,200 --> 00:03:05,840
have with two leading thinkers in this

88
00:03:04,879 --> 00:03:07,360
area

89
00:03:05,840 --> 00:03:10,640
who both bring

90
00:03:07,360 --> 00:03:12,720
a wealth of expertise and experience and

91
00:03:10,640 --> 00:03:14,480
as dan mentioned they just recently

92
00:03:12,720 --> 00:03:16,720
wrote a book on the topic

93
00:03:14,480 --> 00:03:19,120
which contains a very deep analysis of

94
00:03:16,720 --> 00:03:21,840
these important issues but what i found

95
00:03:19,120 --> 00:03:23,440
most important about the book is it

96
00:03:21,840 --> 00:03:25,680
provides a

97
00:03:23,440 --> 00:03:27,599
framing and a language to really

98
00:03:25,680 --> 00:03:29,840
systematically think about these issues

99
00:03:27,599 --> 00:03:31,680
and ask the right questions which is the

100
00:03:29,840 --> 00:03:34,480
most important thing to start with on

101
00:03:31,680 --> 00:03:36,799
such complicated challenges but also

102
00:03:34,480 --> 00:03:38,159
suggest paths forward so we have a lot

103
00:03:36,799 --> 00:03:39,920
to discuss

104
00:03:38,159 --> 00:03:42,239
but before we dive in i just want to

105
00:03:39,920 --> 00:03:44,400
make sure to remind the audience that we

106
00:03:42,239 --> 00:03:46,319
would like to invite you to participate

107
00:03:44,400 --> 00:03:48,400
in this discussion so if you're joining

108
00:03:46,319 --> 00:03:51,040
us virtually you can submit your

109
00:03:48,400 --> 00:03:52,799
questions using the box

110
00:03:51,040 --> 00:03:54,400
at the bottom of your screen please do

111
00:03:52,799 --> 00:03:58,000
so i will try to get to as many

112
00:03:54,400 --> 00:03:59,680
questions as we can during the q a later

113
00:03:58,000 --> 00:04:02,560
but i'll start with some questions just

114
00:03:59,680 --> 00:04:04,080
to set the stage and uh you know start

115
00:04:02,560 --> 00:04:07,120
our conversation get our thinking

116
00:04:04,080 --> 00:04:08,640
together uh the first one is a very

117
00:04:07,120 --> 00:04:11,040
general question but

118
00:04:08,640 --> 00:04:13,200
there's so many answers i i'm just very

119
00:04:11,040 --> 00:04:15,599
curious about your perspective on this

120
00:04:13,200 --> 00:04:19,040
and that is basically the difference

121
00:04:15,599 --> 00:04:19,919
between the public conception of a.i

122
00:04:19,040 --> 00:04:21,040
uh

123
00:04:19,919 --> 00:04:22,639
which is

124
00:04:21,040 --> 00:04:24,400
you know largely shaped by science

125
00:04:22,639 --> 00:04:28,160
fiction movies

126
00:04:24,400 --> 00:04:30,400
uh and the you know ai as the people in

127
00:04:28,160 --> 00:04:32,800
the area field ai researchers think

128
00:04:30,400 --> 00:04:34,479
about it and uh you know you can also

129
00:04:32,800 --> 00:04:36,479
think about this as a question between

130
00:04:34,479 --> 00:04:39,360
you know difference between artificial

131
00:04:36,479 --> 00:04:41,280
general intelligence agi and ai as we

132
00:04:39,360 --> 00:04:43,840
know it today which is something you

133
00:04:41,280 --> 00:04:46,160
touch upon in detail in your book so

134
00:04:43,840 --> 00:04:49,199
maybe we'll start with eric so

135
00:04:46,160 --> 00:04:51,120
well dan and dan and i can can do any of

136
00:04:49,199 --> 00:04:53,840
these together the

137
00:04:51,120 --> 00:04:55,919
so if you talk to people about ai they

138
00:04:53,840 --> 00:04:58,240
basically believe the following thing

139
00:04:55,919 --> 00:05:01,840
there will be a

140
00:04:58,240 --> 00:05:04,720
female robot that will be invented

141
00:05:01,840 --> 00:05:07,360
that will slay all the men

142
00:05:04,720 --> 00:05:10,960
and uh which actually may be a good

143
00:05:07,360 --> 00:05:11,759
outcome in some future human scenarios

144
00:05:10,960 --> 00:05:13,759
um

145
00:05:11,759 --> 00:05:15,919
but i try to explain to people that that

146
00:05:13,759 --> 00:05:17,520
there's so many assumptions in that and

147
00:05:15,919 --> 00:05:19,199
we have a roboticist here in the room

148
00:05:17,520 --> 00:05:22,000
i'll let you decide

149
00:05:19,199 --> 00:05:23,199
or describe what is possible in robotics

150
00:05:22,000 --> 00:05:24,880
but

151
00:05:23,199 --> 00:05:26,639
where we are right now with ai is we

152
00:05:24,880 --> 00:05:28,479
don't quite understand how to generate

153
00:05:26,639 --> 00:05:30,800
volition

154
00:05:28,479 --> 00:05:32,720
we're very very good

155
00:05:30,800 --> 00:05:35,120
at pattern matching

156
00:05:32,720 --> 00:05:36,720
and dan of course is a is a world-class

157
00:05:35,120 --> 00:05:38,880
computer vision expert which is where

158
00:05:36,720 --> 00:05:40,720
this stuff came from so what i like to

159
00:05:38,880 --> 00:05:43,280
explain is that

160
00:05:40,720 --> 00:05:45,840
the first round of this which was 2011

161
00:05:43,280 --> 00:05:46,800
2012 imagenet which all of you know

162
00:05:45,840 --> 00:05:48,960
about

163
00:05:46,800 --> 00:05:51,520
was really the triumph of the computer

164
00:05:48,960 --> 00:05:53,120
vision over the human vision people

165
00:05:51,520 --> 00:05:55,120
and the computer vision

166
00:05:53,120 --> 00:05:57,759
fits in all sorts of ways and dan can

167
00:05:55,120 --> 00:06:01,199
describe the visual system and why it's

168
00:05:57,759 --> 00:06:03,039
still the primary source of progress

169
00:06:01,199 --> 00:06:04,479
vision is not the same thing as volition

170
00:06:03,039 --> 00:06:06,080
it's not the same thing as objective

171
00:06:04,479 --> 00:06:06,800
functions

172
00:06:06,080 --> 00:06:09,280
so

173
00:06:06,800 --> 00:06:11,199
my instinct and dan you you may or i

174
00:06:09,280 --> 00:06:13,120
think you dan is generally more

175
00:06:11,199 --> 00:06:16,000
conservative than i i write stuff and

176
00:06:13,120 --> 00:06:18,000
then he deletes it in our partnership

177
00:06:16,000 --> 00:06:18,840
um i never delete any of his stuff it

178
00:06:18,000 --> 00:06:22,240
seems

179
00:06:18,840 --> 00:06:24,000
unfair and so so in my idea

180
00:06:22,240 --> 00:06:26,880
what happens is

181
00:06:24,000 --> 00:06:29,639
we're going to get stuck in this pattern

182
00:06:26,880 --> 00:06:31,840
matching period and we're going to build

183
00:06:29,639 --> 00:06:33,520
extraordinarily powerful systems which

184
00:06:31,840 --> 00:06:35,600
we can describe

185
00:06:33,520 --> 00:06:36,880
the language model work that's going on

186
00:06:35,600 --> 00:06:39,840
now is

187
00:06:36,880 --> 00:06:41,840
it's at a pace that's breathtaking

188
00:06:39,840 --> 00:06:43,919
but that's not the same thing as you

189
00:06:41,840 --> 00:06:45,919
wake up in the morning and you decide to

190
00:06:43,919 --> 00:06:47,680
take a vacation

191
00:06:45,919 --> 00:06:50,400
work on something new learn and learn

192
00:06:47,680 --> 00:06:51,919
how to play the piano see a new friend

193
00:06:50,400 --> 00:06:52,720
we're just not there yet

194
00:06:51,919 --> 00:06:53,919
dan

195
00:06:52,720 --> 00:06:55,840
and i think that's great i think the

196
00:06:53,919 --> 00:06:58,400
thing i would add just back to the the

197
00:06:55,840 --> 00:07:00,240
science fiction side is you know fiction

198
00:06:58,400 --> 00:07:01,840
needs a good villain

199
00:07:00,240 --> 00:07:04,479
it's very hard to write good fiction

200
00:07:01,840 --> 00:07:06,960
without some kind of conflict in it uh

201
00:07:04,479 --> 00:07:10,160
and and the the the notion of the

202
00:07:06,960 --> 00:07:13,120
machine as a villain uh is is something

203
00:07:10,160 --> 00:07:14,639
that's existed for for a long time uh in

204
00:07:13,120 --> 00:07:17,840
in fiction

205
00:07:14,639 --> 00:07:19,919
and i think you know in the days before

206
00:07:17,840 --> 00:07:21,919
ai was becoming a reality it was much

207
00:07:19,919 --> 00:07:23,599
less relevant what the fiction said i

208
00:07:21,919 --> 00:07:24,560
think today

209
00:07:23,599 --> 00:07:26,639
uh

210
00:07:24,560 --> 00:07:29,199
you know people watch westworld or

211
00:07:26,639 --> 00:07:31,599
whatever you know that's uh that was out

212
00:07:29,199 --> 00:07:33,840
there uh and um and and that sort of

213
00:07:31,599 --> 00:07:35,360
fiction really informs

214
00:07:33,840 --> 00:07:36,880
what they think about the ai that

215
00:07:35,360 --> 00:07:39,039
they're interacting with every day right

216
00:07:36,880 --> 00:07:40,560
so you talked to alexa or google home

217
00:07:39,039 --> 00:07:42,080
and you just watched you know some

218
00:07:40,560 --> 00:07:42,880
science fiction on

219
00:07:42,080 --> 00:07:45,520
on

220
00:07:42,880 --> 00:07:47,759
uh you know on streaming video

221
00:07:45,520 --> 00:07:49,840
uh just the the temporal juxtaposition

222
00:07:47,759 --> 00:07:51,520
of those causes you to expect that the

223
00:07:49,840 --> 00:07:53,919
ai you're interacting with is gonna

224
00:07:51,520 --> 00:07:55,759
interact in ways uh where this wonderful

225
00:07:53,919 --> 00:07:58,160
storytelling is being done

226
00:07:55,759 --> 00:07:59,759
uh and and i think as eric said the key

227
00:07:58,160 --> 00:08:00,879
issue is that you know

228
00:07:59,759 --> 00:08:02,400
ai

229
00:08:00,879 --> 00:08:05,759
does not have

230
00:08:02,400 --> 00:08:08,160
uh you know volition intention uh it's

231
00:08:05,759 --> 00:08:10,240
it's really about uh perceiving the

232
00:08:08,160 --> 00:08:11,440
world and organizing the world

233
00:08:10,240 --> 00:08:12,400
uh

234
00:08:11,440 --> 00:08:15,039
and

235
00:08:12,400 --> 00:08:16,800
one of the problems with this narrative

236
00:08:15,039 --> 00:08:18,400
disconnect between the reality what

237
00:08:16,800 --> 00:08:19,759
people think is that people are now not

238
00:08:18,400 --> 00:08:22,240
understanding what the problems that

239
00:08:19,759 --> 00:08:23,840
real ai will create

240
00:08:22,240 --> 00:08:26,319
uh which i know you're going to get into

241
00:08:23,840 --> 00:08:29,120
in some of your questions but what

242
00:08:26,319 --> 00:08:31,680
happens to jobs what happens to fairness

243
00:08:29,120 --> 00:08:32,959
what happens to geopolitics

244
00:08:31,680 --> 00:08:34,640
and i think the

245
00:08:32,959 --> 00:08:36,880
most important question which is dr

246
00:08:34,640 --> 00:08:39,120
kissinger's question is what does it

247
00:08:36,880 --> 00:08:40,800
mean to be human when there's an

248
00:08:39,120 --> 00:08:43,599
intelligence

249
00:08:40,800 --> 00:08:46,000
that's not like us

250
00:08:43,599 --> 00:08:48,399
but really smart

251
00:08:46,000 --> 00:08:50,800
how do we perceive ourselves

252
00:08:48,399 --> 00:08:51,519
sort of how we got into this

253
00:08:50,800 --> 00:08:53,519
good

254
00:08:51,519 --> 00:08:56,320
fascinating it's very much

255
00:08:53,519 --> 00:08:57,600
a good segue to my next question and

256
00:08:56,320 --> 00:08:59,519
you know something you mentioned in a

257
00:08:57,600 --> 00:09:01,920
number of places in your book uh and

258
00:08:59,519 --> 00:09:03,680
i'll read exactly your words ai with its

259
00:09:01,920 --> 00:09:06,640
without significant fanfare or

260
00:09:03,680 --> 00:09:09,040
visibility is integrated into the basic

261
00:09:06,640 --> 00:09:11,839
fabric of human activity and i found

262
00:09:09,040 --> 00:09:14,480
that fascinating like it's shaping so

263
00:09:11,839 --> 00:09:16,800
much of our so many aspects of our lives

264
00:09:14,480 --> 00:09:19,120
it it's not visible we don't know it we

265
00:09:16,800 --> 00:09:21,600
depend on it but we don't know it so how

266
00:09:19,120 --> 00:09:22,399
is this change transforming the human

267
00:09:21,600 --> 00:09:24,160
experience

268
00:09:22,399 --> 00:09:25,519
and you know things like human

269
00:09:24,160 --> 00:09:28,320
perception

270
00:09:25,519 --> 00:09:29,760
cognition interactions between humans

271
00:09:28,320 --> 00:09:32,560
well let's let's start with what's going

272
00:09:29,760 --> 00:09:34,080
on at mit um and again

273
00:09:32,560 --> 00:09:35,200
you all are the experts of what's going

274
00:09:34,080 --> 00:09:36,720
on here

275
00:09:35,200 --> 00:09:38,160
but

276
00:09:36,720 --> 00:09:40,640
what i've reviewed is that

277
00:09:38,160 --> 00:09:42,320
systematically every aspect of mit's

278
00:09:40,640 --> 00:09:44,480
research is being changed by the

279
00:09:42,320 --> 00:09:46,880
presence of these new algorithms

280
00:09:44,480 --> 00:09:49,279
presumably that's beneficial presumably

281
00:09:46,880 --> 00:09:50,959
that leads to faster drugs you have this

282
00:09:49,279 --> 00:09:53,200
extraordinary success here with jim

283
00:09:50,959 --> 00:09:54,640
collins and allison and so forth which

284
00:09:53,200 --> 00:09:57,839
you all know about

285
00:09:54,640 --> 00:10:00,320
uh so there's this enormous wave of

286
00:09:57,839 --> 00:10:01,519
progress that's going to happen

287
00:10:00,320 --> 00:10:04,480
and

288
00:10:01,519 --> 00:10:06,320
people may or may not give ai the credit

289
00:10:04,480 --> 00:10:07,760
for the progress but the progress is

290
00:10:06,320 --> 00:10:09,600
coming

291
00:10:07,760 --> 00:10:11,279
and that's a good thing right it's a

292
00:10:09,600 --> 00:10:14,000
really good thing

293
00:10:11,279 --> 00:10:16,000
so then the next question is okay so and

294
00:10:14,000 --> 00:10:18,320
this is going to happen whether

295
00:10:16,000 --> 00:10:19,600
whether we as individuals choose it or

296
00:10:18,320 --> 00:10:21,519
not there's too many people working on

297
00:10:19,600 --> 00:10:23,279
it too many benefits you know let's say

298
00:10:21,519 --> 00:10:25,519
you're a luddite and you don't want

299
00:10:23,279 --> 00:10:27,760
drugs to get better you can't stop this

300
00:10:25,519 --> 00:10:28,959
now it's going to happen

301
00:10:27,760 --> 00:10:31,440
so then the next question is what

302
00:10:28,959 --> 00:10:33,440
happens to society

303
00:10:31,440 --> 00:10:34,880
in the book we spend quite a bit of time

304
00:10:33,440 --> 00:10:36,720
talking about

305
00:10:34,880 --> 00:10:38,320
targeting

306
00:10:36,720 --> 00:10:40,480
and professor madre and i were talking

307
00:10:38,320 --> 00:10:43,600
earlier about

308
00:10:40,480 --> 00:10:45,200
the chinese work in regulation

309
00:10:43,600 --> 00:10:46,480
this is shocking the chinese want to

310
00:10:45,200 --> 00:10:48,240
regulate stuff

311
00:10:46,480 --> 00:10:49,279
and they're very concerned about micro

312
00:10:48,240 --> 00:10:50,560
targeting

313
00:10:49,279 --> 00:10:52,320
right very interesting i think that's

314
00:10:50,560 --> 00:10:54,240
roughly what you said

315
00:10:52,320 --> 00:10:56,079
and why would they be concerned about

316
00:10:54,240 --> 00:10:57,519
that we don't really know but that's an

317
00:10:56,079 --> 00:10:58,880
example where people are beginning to

318
00:10:57,519 --> 00:11:01,120
think well maybe this thing's just a

319
00:10:58,880 --> 00:11:05,440
little bit too powerful

320
00:11:01,120 --> 00:11:08,800
in terms of its impact on on our society

321
00:11:05,440 --> 00:11:10,800
my simple answer and then dan you can

322
00:11:08,800 --> 00:11:12,399
i think you agree with this yep think

323
00:11:10,800 --> 00:11:14,399
about if you were founding a social

324
00:11:12,399 --> 00:11:15,680
media company today so you quit mit

325
00:11:14,399 --> 00:11:18,880
please don't do this

326
00:11:15,680 --> 00:11:20,720
form a startup in a year you have

327
00:11:18,880 --> 00:11:23,120
millions of users and

328
00:11:20,720 --> 00:11:24,880
your job is to maximize revenue

329
00:11:23,120 --> 00:11:26,560
you maximize revenue by maximizing

330
00:11:24,880 --> 00:11:30,320
engagement the best way to maximize

331
00:11:26,560 --> 00:11:31,519
engagement is by maximizing outrage

332
00:11:30,320 --> 00:11:33,279
because outrage is the thing that

333
00:11:31,519 --> 00:11:35,200
everybody talks about

334
00:11:33,279 --> 00:11:36,880
so here we are and we go from mit which

335
00:11:35,200 --> 00:11:38,800
is perfectly rational you know

336
00:11:36,880 --> 00:11:40,560
engineering sort of scientific place to

337
00:11:38,800 --> 00:11:42,240
a place where people just make up

338
00:11:40,560 --> 00:11:44,480
sorry i shouldn't say that but you get

339
00:11:42,240 --> 00:11:45,600
the idea

340
00:11:44,480 --> 00:11:46,720
and there's no

341
00:11:45,600 --> 00:11:49,120
there's no

342
00:11:46,720 --> 00:11:50,160
limit on that there's no way to stop

343
00:11:49,120 --> 00:11:52,079
that

344
00:11:50,160 --> 00:11:53,680
and i

345
00:11:52,079 --> 00:11:55,680
to be very clear i don't know how to

346
00:11:53,680 --> 00:11:57,920
solve this problem

347
00:11:55,680 --> 00:11:59,600
but that's a real problem that's coming

348
00:11:57,920 --> 00:12:01,760
from this technology

349
00:11:59,600 --> 00:12:03,600
you know i said if you if alexander

350
00:12:01,760 --> 00:12:05,120
graham bell today were alive today we

351
00:12:03,600 --> 00:12:07,360
would say what do you think about the

352
00:12:05,120 --> 00:12:08,800
telephone and he would say i love it and

353
00:12:07,360 --> 00:12:10,480
he said well when you invented the

354
00:12:08,800 --> 00:12:12,800
telephone it didn't occur to you that

355
00:12:10,480 --> 00:12:17,279
criminals would use your phone

356
00:12:12,800 --> 00:12:19,279
and he goes no i invented it for myself

357
00:12:17,279 --> 00:12:22,079
so it's not like this is the first time

358
00:12:19,279 --> 00:12:24,959
when an invention from you guys

359
00:12:22,079 --> 00:12:28,560
got misused by humanity

360
00:12:24,959 --> 00:12:28,560
and required a response

361
00:12:29,680 --> 00:12:35,920
so as usual eric lays out these uh grand

362
00:12:33,120 --> 00:12:38,800
landscapes where i i um there are so

363
00:12:35,920 --> 00:12:41,680
many things i could say let me let me uh

364
00:12:38,800 --> 00:12:43,279
so i i really liked this

365
00:12:41,680 --> 00:12:45,600
sort of distinction you made between

366
00:12:43,279 --> 00:12:48,560
sort of ai at mit

367
00:12:45,600 --> 00:12:50,959
and places like mit and ai in the world

368
00:12:48,560 --> 00:12:53,200
so i think ai at mit

369
00:12:50,959 --> 00:12:54,959
largely serves as a partner

370
00:12:53,200 --> 00:12:56,639
right it's something that we are

371
00:12:54,959 --> 00:12:59,040
deliberately using

372
00:12:56,639 --> 00:13:00,480
in in more and more uh

373
00:12:59,040 --> 00:13:02,800
laboratories and more and more

374
00:13:00,480 --> 00:13:04,560
investigations across the institute you

375
00:13:02,800 --> 00:13:08,160
know using machine learning modeling to

376
00:13:04,560 --> 00:13:09,920
help accelerate discovery uh in in more

377
00:13:08,160 --> 00:13:11,920
and more and more disciplines

378
00:13:09,920 --> 00:13:13,440
so it's really uh

379
00:13:11,920 --> 00:13:15,279
becomes uh

380
00:13:13,440 --> 00:13:17,120
almost a thought partner

381
00:13:15,279 --> 00:13:19,120
as you develop things

382
00:13:17,120 --> 00:13:20,800
and so that ai partnership model is very

383
00:13:19,120 --> 00:13:23,200
deliberate and it's one of the things we

384
00:13:20,800 --> 00:13:26,560
talk about some in the book

385
00:13:23,200 --> 00:13:28,639
but then when you look at ai as used uh

386
00:13:26,560 --> 00:13:30,000
in most people's daily lives including

387
00:13:28,639 --> 00:13:31,440
ours i mean

388
00:13:30,000 --> 00:13:33,040
at least the part of our daily life that

389
00:13:31,440 --> 00:13:35,440
isn't mit

390
00:13:33,040 --> 00:13:37,839
the vanishingly small part sometimes

391
00:13:35,440 --> 00:13:39,360
uh the um

392
00:13:37,839 --> 00:13:41,360
you know

393
00:13:39,360 --> 00:13:42,639
you're driving with the navigation

394
00:13:41,360 --> 00:13:43,680
system on

395
00:13:42,639 --> 00:13:46,079
you know you're

396
00:13:43,680 --> 00:13:48,160
you're deciding what movies to watch or

397
00:13:46,079 --> 00:13:50,399
what books to read because something

398
00:13:48,160 --> 00:13:52,160
pushed some recommendation at you

399
00:13:50,399 --> 00:13:53,839
uh you know if you're a social media

400
00:13:52,160 --> 00:13:55,440
user you're being inundated and things

401
00:13:53,839 --> 00:13:57,199
and all of these decisions are being

402
00:13:55,440 --> 00:13:58,560
made

403
00:13:57,199 --> 00:14:00,320
by

404
00:13:58,560 --> 00:14:01,519
by algorithms usually machine learning

405
00:14:00,320 --> 00:14:04,000
algorithms

406
00:14:01,519 --> 00:14:04,800
and so that's the very sort of passive

407
00:14:04,000 --> 00:14:06,079
part

408
00:14:04,800 --> 00:14:08,880
where

409
00:14:06,079 --> 00:14:10,480
even us experts don't stop and think how

410
00:14:08,880 --> 00:14:12,560
much we're depending

411
00:14:10,480 --> 00:14:14,959
uh on these things every day making

412
00:14:12,560 --> 00:14:17,120
decisions for us sort of presenting an

413
00:14:14,959 --> 00:14:19,120
option to us that we just follow without

414
00:14:17,120 --> 00:14:21,440
blindly without thinking about it and so

415
00:14:19,120 --> 00:14:23,839
i think this contrast between

416
00:14:21,440 --> 00:14:26,160
ai is a sort of conscious partner in

417
00:14:23,839 --> 00:14:28,560
work uh which we're very privileged at

418
00:14:26,160 --> 00:14:30,560
places like mit to be able to do

419
00:14:28,560 --> 00:14:32,320
and ai is sort of you know something

420
00:14:30,560 --> 00:14:34,399
that we passively comply with is a

421
00:14:32,320 --> 00:14:36,240
really really important distinction

422
00:14:34,399 --> 00:14:38,240
and you bring up the recommendation

423
00:14:36,240 --> 00:14:40,639
engines i think that the recommendation

424
00:14:38,240 --> 00:14:43,040
engines which have really blossomed and

425
00:14:40,639 --> 00:14:45,360
have been very successful because of ai

426
00:14:43,040 --> 00:14:48,560
are the first example of something where

427
00:14:45,360 --> 00:14:50,480
people are going to have to figure out

428
00:14:48,560 --> 00:14:52,720
what what how do you regulate them how

429
00:14:50,480 --> 00:14:54,880
do you address whatever failure modes

430
00:14:52,720 --> 00:14:57,120
they have

431
00:14:54,880 --> 00:14:58,639
and my guess is that p that countries

432
00:14:57,120 --> 00:15:01,279
will do this differently

433
00:14:58,639 --> 00:15:02,720
and it will be done generally badly

434
00:15:01,279 --> 00:15:03,920
right

435
00:15:02,720 --> 00:15:06,639
so

436
00:15:03,920 --> 00:15:08,560
it seems to me that

437
00:15:06,639 --> 00:15:10,639
one thing mit could do

438
00:15:08,560 --> 00:15:14,399
would be to try to figure out what

439
00:15:10,639 --> 00:15:16,800
happens after deep learning

440
00:15:14,399 --> 00:15:18,560
to change this landscape you know there

441
00:15:16,800 --> 00:15:19,839
will be a point where deep learning is

442
00:15:18,560 --> 00:15:22,399
mature

443
00:15:19,839 --> 00:15:24,480
and the phd topics are no longer as

444
00:15:22,399 --> 00:15:25,680
juicy as they are today

445
00:15:24,480 --> 00:15:27,680
and then there's going to be something

446
00:15:25,680 --> 00:15:29,040
after that it's always the case so what

447
00:15:27,680 --> 00:15:31,040
will it be

448
00:15:29,040 --> 00:15:33,199
and i think that's a legitimate mission

449
00:15:31,040 --> 00:15:34,639
for for you for you two right because

450
00:15:33,199 --> 00:15:36,160
you work in this space

451
00:15:34,639 --> 00:15:37,680
to corral the smartest people in the

452
00:15:36,160 --> 00:15:39,279
world to work on

453
00:15:37,680 --> 00:15:40,880
very good very good

454
00:15:39,279 --> 00:15:42,959
one thing i'm actually personally very

455
00:15:40,880 --> 00:15:45,279
curious and i was very glad to see it

456
00:15:42,959 --> 00:15:47,600
also mentioned in your book uh is

457
00:15:45,279 --> 00:15:48,880
basically to look backwards

458
00:15:47,600 --> 00:15:50,000
it's of course very important to

459
00:15:48,880 --> 00:15:52,240
understand

460
00:15:50,000 --> 00:15:54,800
impact of technological change on

461
00:15:52,240 --> 00:15:57,839
society and uh throughout history we've

462
00:15:54,800 --> 00:15:59,440
had many you know important moments of

463
00:15:57,839 --> 00:16:01,920
technological change

464
00:15:59,440 --> 00:16:04,639
you mentioned in your book that only

465
00:16:01,920 --> 00:16:06,720
rarely has technology fundamentally

466
00:16:04,639 --> 00:16:08,639
transformed the social and political

467
00:16:06,720 --> 00:16:10,720
structure of our society so i thought

468
00:16:08,639 --> 00:16:13,360
that's a very powerful statement and

469
00:16:10,720 --> 00:16:14,880
like why is ai different

470
00:16:13,360 --> 00:16:17,279
is it really

471
00:16:14,880 --> 00:16:19,199
much more transformative than you know

472
00:16:17,279 --> 00:16:21,600
if you think about

473
00:16:19,199 --> 00:16:24,079
some landmark moments agriculture

474
00:16:21,600 --> 00:16:26,240
printing press or even in the industrial

475
00:16:24,079 --> 00:16:29,519
technologies do you think it will lead

476
00:16:26,240 --> 00:16:31,759
to more fundamental social changes or

477
00:16:29,519 --> 00:16:34,079
consequences than what we had before and

478
00:16:31,759 --> 00:16:36,320
is it the scale why i don't think we've

479
00:16:34,079 --> 00:16:38,000
had this kind of innovation in history

480
00:16:36,320 --> 00:16:39,759
in the following reason

481
00:16:38,000 --> 00:16:41,279
currently at least as we understand it

482
00:16:39,759 --> 00:16:43,680
ai is

483
00:16:41,279 --> 00:16:46,160
imprecise it makes mistakes

484
00:16:43,680 --> 00:16:48,639
it's generative in the sense that it can

485
00:16:46,160 --> 00:16:51,600
generate new things

486
00:16:48,639 --> 00:16:53,199
it can actually have emergent behaviors

487
00:16:51,600 --> 00:16:54,880
if it begins to interact that are

488
00:16:53,199 --> 00:16:56,320
unpredictable

489
00:16:54,880 --> 00:16:58,560
and the most important thing is it's

490
00:16:56,320 --> 00:16:59,680
learning while it's doing

491
00:16:58,560 --> 00:17:01,279
so

492
00:16:59,680 --> 00:17:03,839
i'll give you an example

493
00:17:01,279 --> 00:17:05,280
uh and we mentioned this in in the book

494
00:17:03,839 --> 00:17:08,240
you've got a two-year-old you give the

495
00:17:05,280 --> 00:17:09,520
two-year-old a toy the toy is ai enabled

496
00:17:08,240 --> 00:17:11,360
kid kid

497
00:17:09,520 --> 00:17:12,959
the toy keeps the kid

498
00:17:11,360 --> 00:17:15,520
you know behaved and so forth until the

499
00:17:12,959 --> 00:17:17,439
kid gets older and the toy gets smarter

500
00:17:15,520 --> 00:17:19,919
so now they're 12.

501
00:17:17,439 --> 00:17:20,640
the toy and the child

502
00:17:19,919 --> 00:17:23,839
and

503
00:17:20,640 --> 00:17:25,039
the toy says i don't like this tv you

504
00:17:23,839 --> 00:17:27,760
know show

505
00:17:25,039 --> 00:17:29,440
and the kid says i agree with you

506
00:17:27,760 --> 00:17:30,559
how do you feel about that

507
00:17:29,440 --> 00:17:32,960
okay

508
00:17:30,559 --> 00:17:34,640
do you say well it's okay you know i bet

509
00:17:32,960 --> 00:17:37,840
the best friend for my kid is not a

510
00:17:34,640 --> 00:17:39,919
human it's a robot but i understand it

511
00:17:37,840 --> 00:17:42,880
but remember that this

512
00:17:39,919 --> 00:17:45,120
toy is learning

513
00:17:42,880 --> 00:17:46,559
so how do you feel if i then add its

514
00:17:45,120 --> 00:17:48,640
ability to

515
00:17:46,559 --> 00:17:50,160
learn something that we didn't

516
00:17:48,640 --> 00:17:51,919
understand

517
00:17:50,160 --> 00:17:52,799
that it thinks is true and it tells the

518
00:17:51,919 --> 00:17:54,559
kid

519
00:17:52,799 --> 00:17:56,320
and what if it's wrong

520
00:17:54,559 --> 00:17:59,520
right so now we have there's all sorts

521
00:17:56,320 --> 00:18:01,520
of rules in america around textbooks

522
00:17:59,520 --> 00:18:04,000
so each state has a list of textbooks

523
00:18:01,520 --> 00:18:05,280
that you're allowed to show teenagers

524
00:18:04,000 --> 00:18:06,240
whether they follow them is not the

525
00:18:05,280 --> 00:18:08,160
point

526
00:18:06,240 --> 00:18:12,400
right so do we have to have a list of

527
00:18:08,160 --> 00:18:14,000
approved learnings for toys for kids

528
00:18:12,400 --> 00:18:17,120
and they're not allowed to learn this

529
00:18:14,000 --> 00:18:18,799
well can you imagine that debate

530
00:18:17,120 --> 00:18:21,679
i just don't think we have even a

531
00:18:18,799 --> 00:18:23,200
framing for a technology so powerful

532
00:18:21,679 --> 00:18:27,200
that it does all of these things and

533
00:18:23,200 --> 00:18:27,200
it's learning at the same time

534
00:18:27,679 --> 00:18:33,120
i i think uh so first of all i think

535
00:18:29,760 --> 00:18:34,720
that's a really compelling example uh um

536
00:18:33,120 --> 00:18:37,360
and uh

537
00:18:34,720 --> 00:18:39,679
i i i did when you when you helped us

538
00:18:37,360 --> 00:18:41,280
craft it as we were writing the book

539
00:18:39,679 --> 00:18:42,880
eric's very good at saying we need to

540
00:18:41,280 --> 00:18:44,880
understand this

541
00:18:42,880 --> 00:18:47,440
and then not giving up

542
00:18:44,880 --> 00:18:49,600
uh the um but but i think one of the

543
00:18:47,440 --> 00:18:51,600
things about ai more generally that's

544
00:18:49,600 --> 00:18:52,880
that is so important that this example

545
00:18:51,600 --> 00:18:55,880
shows

546
00:18:52,880 --> 00:18:57,200
is um is its ability to

547
00:18:55,880 --> 00:18:58,880
[Music]

548
00:18:57,200 --> 00:19:00,320
perceive the world in ways differently

549
00:18:58,880 --> 00:19:02,880
than people do

550
00:19:00,320 --> 00:19:04,880
and then to interact with people

551
00:19:02,880 --> 00:19:07,120
uh at a level that it can change how

552
00:19:04,880 --> 00:19:09,120
people are perceiving the world right

553
00:19:07,120 --> 00:19:11,360
and we've never seen a

554
00:19:09,120 --> 00:19:14,320
technology like that it's the impact of

555
00:19:11,360 --> 00:19:15,919
it of what it's doing on the people

556
00:19:14,320 --> 00:19:17,039
that is different

557
00:19:15,919 --> 00:19:19,679
right

558
00:19:17,039 --> 00:19:21,200
before that we could sort of predict

559
00:19:19,679 --> 00:19:23,520
what things would do that gun was

560
00:19:21,200 --> 00:19:24,720
invented we understood it was for

561
00:19:23,520 --> 00:19:26,160
electric power was invented we

562
00:19:24,720 --> 00:19:28,000
understood it as for transportation we

563
00:19:26,160 --> 00:19:31,360
understood what it's for what happens

564
00:19:28,000 --> 00:19:33,520
when it starts doing its own thing

565
00:19:31,360 --> 00:19:35,600
about the fact that this general purpose

566
00:19:33,520 --> 00:19:37,200
and you know maybe it's across it's not

567
00:19:35,600 --> 00:19:39,440
really a tool for something but it's

568
00:19:37,200 --> 00:19:41,280
really across many many problems domains

569
00:19:39,440 --> 00:19:42,880
well there are many but but they haven't

570
00:19:41,280 --> 00:19:44,799
heard of it but there have been very

571
00:19:42,880 --> 00:19:46,320
many many general purpose tools in in

572
00:19:44,799 --> 00:19:48,480
the development of technology but

573
00:19:46,320 --> 00:19:50,160
they've been bounded by some physical

574
00:19:48,480 --> 00:19:52,240
limit or what have you

575
00:19:50,160 --> 00:19:54,320
but because this is around intelligence

576
00:19:52,240 --> 00:19:56,480
it doesn't seem to have a bound

577
00:19:54,320 --> 00:19:59,440
so we had a big fight about this and i

578
00:19:56,480 --> 00:20:00,240
gave the agi speech which goes something

579
00:19:59,440 --> 00:20:02,799
like

580
00:20:00,240 --> 00:20:04,840
there's going to be in the next 20 years

581
00:20:02,799 --> 00:20:07,840
something which has human level of

582
00:20:04,840 --> 00:20:08,799
intelligence that's going to be as smart

583
00:20:07,840 --> 00:20:10,799
as we

584
00:20:08,799 --> 00:20:12,720
incredibly flexible and it's really

585
00:20:10,799 --> 00:20:15,039
going to put us in our place and dan

586
00:20:12,720 --> 00:20:16,880
said no eric that's wrong

587
00:20:15,039 --> 00:20:18,720
what dan said is

588
00:20:16,880 --> 00:20:20,640
it's much more likely and i think he's

589
00:20:18,720 --> 00:20:24,080
right here

590
00:20:20,640 --> 00:20:25,200
that these things will not be human-like

591
00:20:24,080 --> 00:20:27,280
they'll be

592
00:20:25,200 --> 00:20:29,919
very intelligent but not like humans

593
00:20:27,280 --> 00:20:32,000
they'll have their own logic

594
00:20:29,919 --> 00:20:33,520
and what was interesting was i fought

595
00:20:32,000 --> 00:20:35,360
him over this issue for a while and then

596
00:20:33,520 --> 00:20:36,799
i was talking to these people here in

597
00:20:35,360 --> 00:20:39,120
fact

598
00:20:36,799 --> 00:20:39,120
about

599
00:20:39,360 --> 00:20:43,919
essentially synthetic biology

600
00:20:42,159 --> 00:20:45,520
and i had assumed that synthetic biology

601
00:20:43,919 --> 00:20:47,280
would build biological systems that are

602
00:20:45,520 --> 00:20:48,720
identical to current biology but when

603
00:20:47,280 --> 00:20:50,480
you talk to your synthetic biologists

604
00:20:48,720 --> 00:20:52,400
they say we use the principles of

605
00:20:50,480 --> 00:20:55,360
biology but they operate differently

606
00:20:52,400 --> 00:20:58,000
they solve different problems

607
00:20:55,360 --> 00:20:59,760
yeah like and to to those of us like

608
00:20:58,000 --> 00:21:01,520
of your in my generation

609
00:20:59,760 --> 00:21:03,280
where you know

610
00:21:01,520 --> 00:21:04,799
students we had these little logic kits

611
00:21:03,280 --> 00:21:06,960
that we built stuff out of with

612
00:21:04,799 --> 00:21:09,280
electronic circuits which now have all

613
00:21:06,960 --> 00:21:10,880
become a pretty much solid state

614
00:21:09,280 --> 00:21:12,240
they're basically building

615
00:21:10,880 --> 00:21:13,520
essentially in their view in synthetic

616
00:21:12,240 --> 00:21:14,480
biology they're building that kit of

617
00:21:13,520 --> 00:21:15,600
parts

618
00:21:14,480 --> 00:21:17,760
right

619
00:21:15,600 --> 00:21:19,520
but but but the important point here is

620
00:21:17,760 --> 00:21:21,760
that the process of designing these

621
00:21:19,520 --> 00:21:22,799
biological systems will look different

622
00:21:21,760 --> 00:21:24,720
than the

623
00:21:22,799 --> 00:21:26,320
natural biological systems using the

624
00:21:24,720 --> 00:21:27,760
same principles

625
00:21:26,320 --> 00:21:29,840
we should think of the same thing will

626
00:21:27,760 --> 00:21:33,760
occur in

627
00:21:29,840 --> 00:21:35,039
now that is going to drive us crazy

628
00:21:33,760 --> 00:21:37,440
because all of a sudden you're going to

629
00:21:35,039 --> 00:21:39,760
have this physicist computer thing

630
00:21:37,440 --> 00:21:41,760
that thinks the way a physicist

631
00:21:39,760 --> 00:21:43,440
does but the physicists don't feel like

632
00:21:41,760 --> 00:21:44,799
it's physics

633
00:21:43,440 --> 00:21:47,440
and the rest of us don't understand

634
00:21:44,799 --> 00:21:48,880
physics we won't understand it either

635
00:21:47,440 --> 00:21:49,919
right we're gonna have to sort all that

636
00:21:48,880 --> 00:21:51,360
out

637
00:21:49,919 --> 00:21:53,200
i'm already there with the language

638
00:21:51,360 --> 00:21:55,039
models today explain

639
00:21:53,200 --> 00:21:56,320
explain this

640
00:21:55,039 --> 00:21:58,159
um

641
00:21:56,320 --> 00:22:00,240
so give some background on the language

642
00:21:58,159 --> 00:22:01,600
yeah so um so there are a number of

643
00:22:00,240 --> 00:22:03,520
these uh

644
00:22:01,600 --> 00:22:07,360
language models today that are trained

645
00:22:03,520 --> 00:22:10,080
on large amounts of of of uh text

646
00:22:07,360 --> 00:22:13,039
uh and that can then produce

647
00:22:10,080 --> 00:22:15,039
uh extremely realistic looking text from

648
00:22:13,039 --> 00:22:17,679
very very small prompts

649
00:22:15,039 --> 00:22:20,640
uh and one of the ones that we we

650
00:22:17,679 --> 00:22:23,039
uh quoted in the book because it was so

651
00:22:20,640 --> 00:22:24,240
striking with respect to

652
00:22:23,039 --> 00:22:26,480
uh sort of the nature of the

653
00:22:24,240 --> 00:22:29,280
capabilities of ai

654
00:22:26,480 --> 00:22:31,919
is gpt3 which has already since been

655
00:22:29,280 --> 00:22:33,520
surpassed by a number of of other

656
00:22:31,919 --> 00:22:35,520
such systems which was developed by

657
00:22:33,520 --> 00:22:36,960
openai

658
00:22:35,520 --> 00:22:39,280
and

659
00:22:36,960 --> 00:22:41,600
the sort of prompt was asking

660
00:22:39,280 --> 00:22:43,360
gpt3 to talk about

661
00:22:41,600 --> 00:22:44,240
itself

662
00:22:43,360 --> 00:22:45,200
and

663
00:22:44,240 --> 00:22:47,280
and

664
00:22:45,200 --> 00:22:49,760
the the sort of dialogue that it

665
00:22:47,280 --> 00:22:52,400
synthesized

666
00:22:49,760 --> 00:22:53,440
included gbt3 saying

667
00:22:52,400 --> 00:22:55,919
you know

668
00:22:53,440 --> 00:22:58,880
i'm a language model i'm not intelligent

669
00:22:55,919 --> 00:23:01,840
like you are as a human

670
00:22:58,880 --> 00:23:03,919
and you know i can speak intelligently

671
00:23:01,840 --> 00:23:05,679
but i don't actually have intelligence

672
00:23:03,919 --> 00:23:07,919
all i have is a model of what human

673
00:23:05,679 --> 00:23:10,320
language looks like and i can produce

674
00:23:07,919 --> 00:23:12,320
things that look like human language and

675
00:23:10,320 --> 00:23:14,880
so what i find extremely frustrating

676
00:23:12,320 --> 00:23:15,760
already about these models is that

677
00:23:14,880 --> 00:23:17,760
um

678
00:23:15,760 --> 00:23:20,320
because human language is such a

679
00:23:17,760 --> 00:23:22,960
powerful means by which we communicate

680
00:23:20,320 --> 00:23:24,720
our own understanding with one another

681
00:23:22,960 --> 00:23:27,039
human to human forget about ai for a

682
00:23:24,720 --> 00:23:28,799
minute two humans what do we use to

683
00:23:27,039 --> 00:23:30,159
communicate our understanding language

684
00:23:28,799 --> 00:23:31,679
for the most part

685
00:23:30,159 --> 00:23:33,600
uh and so

686
00:23:31,679 --> 00:23:36,640
when something is very good at

687
00:23:33,600 --> 00:23:38,880
synthesizing powerful language but

688
00:23:36,640 --> 00:23:40,960
doesn't really necessarily have an

689
00:23:38,880 --> 00:23:42,880
understanding that's driving it

690
00:23:40,960 --> 00:23:46,320
uh that that that's something that i

691
00:23:42,880 --> 00:23:47,760
find insanely frustrating and so

692
00:23:46,320 --> 00:23:49,840
and i think it's worth building on this

693
00:23:47,760 --> 00:23:51,679
in the following way so if you talk to

694
00:23:49,840 --> 00:23:54,799
people doing research some of whom are

695
00:23:51,679 --> 00:23:56,480
here in this area in this area

696
00:23:54,799 --> 00:23:58,159
first those large language models are

697
00:23:56,480 --> 00:24:00,559
exploding they're exploding all over the

698
00:23:58,159 --> 00:24:02,240
world there's a race if you will in

699
00:24:00,559 --> 00:24:03,520
terms of scale and size of language

700
00:24:02,240 --> 00:24:06,240
models

701
00:24:03,520 --> 00:24:08,400
the training cost of gpt3 which is

702
00:24:06,240 --> 00:24:10,080
almost two years old was about 100

703
00:24:08,400 --> 00:24:12,159
million dollars if you include the

704
00:24:10,080 --> 00:24:14,240
electricity and the computers and the

705
00:24:12,159 --> 00:24:16,320
people and so forth

706
00:24:14,240 --> 00:24:18,320
the new ones are 10 times greater and

707
00:24:16,320 --> 00:24:20,880
using engineering techniques

708
00:24:18,320 --> 00:24:23,360
it's not going up linearly with the size

709
00:24:20,880 --> 00:24:25,600
of the model thank goodness

710
00:24:23,360 --> 00:24:27,919
and they're getting much much better the

711
00:24:25,600 --> 00:24:30,320
evidence so far is that the models get

712
00:24:27,919 --> 00:24:33,600
better with absolute scale they're not

713
00:24:30,320 --> 00:24:35,520
seeing a mean reversion yet

714
00:24:33,600 --> 00:24:36,880
they haven't hit some limit to how much

715
00:24:35,520 --> 00:24:38,000
they can know

716
00:24:36,880 --> 00:24:39,679
so the strange thing about these

717
00:24:38,000 --> 00:24:40,880
language models is you train them and

718
00:24:39,679 --> 00:24:41,760
then you try to figure out what they

719
00:24:40,880 --> 00:24:43,440
know

720
00:24:41,760 --> 00:24:45,039
which is the complete inverse of how

721
00:24:43,440 --> 00:24:47,039
programming is done you typically know

722
00:24:45,039 --> 00:24:49,600
what you're doing well here you train it

723
00:24:47,039 --> 00:24:51,360
and then this interesting things happen

724
00:24:49,600 --> 00:24:52,640
and the most recent

725
00:24:51,360 --> 00:24:54,880
there are two recent things that have

726
00:24:52,640 --> 00:24:56,559
gone on one is that microsoft

727
00:24:54,880 --> 00:24:57,840
has offered a series of products which

728
00:24:56,559 --> 00:24:59,440
are programming

729
00:24:57,840 --> 00:25:00,880
assistance

730
00:24:59,440 --> 00:25:03,360
and the reason to think that's going to

731
00:25:00,880 --> 00:25:04,799
be successful is first you're always

732
00:25:03,360 --> 00:25:06,880
better off in computer science when

733
00:25:04,799 --> 00:25:08,240
you're building something for yourself

734
00:25:06,880 --> 00:25:10,159
because then you get it right and you

735
00:25:08,240 --> 00:25:12,400
use it and you you iterate with it it's

736
00:25:10,159 --> 00:25:14,960
just the nature of the field

737
00:25:12,400 --> 00:25:16,880
and second you get a strong training

738
00:25:14,960 --> 00:25:19,440
signal back

739
00:25:16,880 --> 00:25:22,400
when the computer proposes the next line

740
00:25:19,440 --> 00:25:25,440
of code and you say no that's wrong it

741
00:25:22,400 --> 00:25:27,840
then puts that into its learning regime

742
00:25:25,440 --> 00:25:30,080
so there are four or five startups that

743
00:25:27,840 --> 00:25:31,840
i'm aware of that are now basically

744
00:25:30,080 --> 00:25:32,640
trying to build programming assistance

745
00:25:31,840 --> 00:25:34,080
of

746
00:25:32,640 --> 00:25:35,279
serious power

747
00:25:34,080 --> 00:25:37,120
to do that

748
00:25:35,279 --> 00:25:39,120
now that's an example of a highly

749
00:25:37,120 --> 00:25:41,039
specialized intelligence

750
00:25:39,120 --> 00:25:42,400
and you could imagine that eventually it

751
00:25:41,039 --> 00:25:44,080
could get to the point where you could

752
00:25:42,400 --> 00:25:45,760
say write me a program that will

753
00:25:44,080 --> 00:25:47,440
generate a browser

754
00:25:45,760 --> 00:25:48,880
and it will know enough to actually

755
00:25:47,440 --> 00:25:50,080
write the code

756
00:25:48,880 --> 00:25:51,360
now that's not the same thing as

757
00:25:50,080 --> 00:25:54,480
volition

758
00:25:51,360 --> 00:25:57,120
but it's scarily close to it

759
00:25:54,480 --> 00:25:58,880
another one is a product called lambda

760
00:25:57,120 --> 00:26:00,159
which was developed in the research labs

761
00:25:58,880 --> 00:26:02,240
at google

762
00:26:00,159 --> 00:26:03,840
the technical details were released this

763
00:26:02,240 --> 00:26:05,520
past week

764
00:26:03,840 --> 00:26:06,960
and one way to understand it is they

765
00:26:05,520 --> 00:26:09,039
trained it on

766
00:26:06,960 --> 00:26:12,000
a relatively large corpus of human

767
00:26:09,039 --> 00:26:14,240
conversation so it feels conversational

768
00:26:12,000 --> 00:26:16,240
and it's backed up by a lot of data this

769
00:26:14,240 --> 00:26:18,320
is google so obviously they have a great

770
00:26:16,240 --> 00:26:20,080
deal of research around around search

771
00:26:18,320 --> 00:26:22,400
and answers and so forth

772
00:26:20,080 --> 00:26:24,960
and it's you can ask it very complicated

773
00:26:22,400 --> 00:26:26,880
questions so a predictor for the next

774
00:26:24,960 --> 00:26:28,720
five years in this is the models will

775
00:26:26,880 --> 00:26:30,640
get much larger

776
00:26:28,720 --> 00:26:33,600
they'll be completely multimodal that

777
00:26:30,640 --> 00:26:35,840
means text and video and so forth and

778
00:26:33,600 --> 00:26:38,480
they'll get really good at

779
00:26:35,840 --> 00:26:40,480
summarization and answering questions

780
00:26:38,480 --> 00:26:42,400
now i don't know what that means for

781
00:26:40,480 --> 00:26:44,720
businesses or i can imagine businesses

782
00:26:42,400 --> 00:26:48,320
where that matters but it has a big

783
00:26:44,720 --> 00:26:49,440
impact in terms of human day-to-day life

784
00:26:48,320 --> 00:26:51,679
because

785
00:26:49,440 --> 00:26:52,720
now you type in a query into google

786
00:26:51,679 --> 00:26:54,960
hopefully

787
00:26:52,720 --> 00:26:56,640
and you see all these answers and then

788
00:26:54,960 --> 00:26:58,000
you figure out which one is crazy and

789
00:26:56,640 --> 00:26:59,600
which one is correct and so forth

790
00:26:58,000 --> 00:27:00,559
hopefully google gives you the right

791
00:26:59,600 --> 00:27:02,240
answer

792
00:27:00,559 --> 00:27:04,080
this next thing you'll just type your

793
00:27:02,240 --> 00:27:04,960
question and then this summary will come

794
00:27:04,080 --> 00:27:06,799
out

795
00:27:04,960 --> 00:27:10,000
and it's highly likely to be accurate

796
00:27:06,799 --> 00:27:10,000
that's a big change

797
00:27:10,480 --> 00:27:14,720
yeah and actually it points at an

798
00:27:12,159 --> 00:27:15,919
interesting uh sort of change that's

799
00:27:14,720 --> 00:27:18,399
been happening

800
00:27:15,919 --> 00:27:19,440
as these algorithms develop generally is

801
00:27:18,399 --> 00:27:21,760
um

802
00:27:19,440 --> 00:27:23,600
is sort of increasing homogeneity sort

803
00:27:21,760 --> 00:27:25,279
of loss of heterogeneity you know the

804
00:27:23,600 --> 00:27:27,120
world in general we benefit from the

805
00:27:25,279 --> 00:27:29,679
fact that we all have different views in

806
00:27:27,120 --> 00:27:32,480
this room uh i'm sure i can find topics

807
00:27:29,679 --> 00:27:33,760
that we all disagree on but the um

808
00:27:32,480 --> 00:27:35,840
but

809
00:27:33,760 --> 00:27:37,360
algorithms tend to so you know it'll

810
00:27:35,840 --> 00:27:38,799
produce a summary

811
00:27:37,360 --> 00:27:42,159
that uh

812
00:27:38,799 --> 00:27:44,799
you know given the particular query you

813
00:27:42,159 --> 00:27:46,960
you gave uh and there you know there's

814
00:27:44,799 --> 00:27:49,279
more richness in the language model than

815
00:27:46,960 --> 00:27:51,200
that but it these things are generally

816
00:27:49,279 --> 00:27:54,000
you know they'll take sort of the most

817
00:27:51,200 --> 00:27:55,840
likely summary given a particular query

818
00:27:54,000 --> 00:27:57,440
as opposed to at least today even if you

819
00:27:55,840 --> 00:28:00,159
get a bunch of garbage back from google

820
00:27:57,440 --> 00:28:01,840
you see some of that heterogeneity

821
00:28:00,159 --> 00:28:04,640
so this is i think another struggle that

822
00:28:01,840 --> 00:28:05,840
we have as ai gets better and better and

823
00:28:04,640 --> 00:28:07,600
believe me there's plenty of machine

824
00:28:05,840 --> 00:28:08,559
learning in what google gives back today

825
00:28:07,600 --> 00:28:11,360
it's not

826
00:28:08,559 --> 00:28:13,760
uh but but you still sort of play an

827
00:28:11,360 --> 00:28:16,720
active role it's filtered a lot but you

828
00:28:13,760 --> 00:28:18,559
still get some sense of of what's there

829
00:28:16,720 --> 00:28:20,960
when you start getting

830
00:28:18,559 --> 00:28:22,080
compelling sounding summaries that may

831
00:28:20,960 --> 00:28:23,679
just be

832
00:28:22,080 --> 00:28:25,520
okay that makes sense that's my

833
00:28:23,679 --> 00:28:27,279
understanding of the world and so i

834
00:28:25,520 --> 00:28:29,120
think this is another

835
00:28:27,279 --> 00:28:31,200
some of the broader philosophical

836
00:28:29,120 --> 00:28:33,760
questions that we tried to sort of

837
00:28:31,200 --> 00:28:37,200
investigate and raise in the book is

838
00:28:33,760 --> 00:28:38,799
some of these kinds of changes

839
00:28:37,200 --> 00:28:41,520
if we don't think about them and we

840
00:28:38,799 --> 00:28:42,559
don't think about how ai is interacting

841
00:28:41,520 --> 00:28:45,840
with us

842
00:28:42,559 --> 00:28:47,360
we'll substantively change uh how people

843
00:28:45,840 --> 00:28:49,039
interact with them i'll give you a

844
00:28:47,360 --> 00:28:51,200
simple example when i'm when i spend my

845
00:28:49,039 --> 00:28:52,880
time in dc talking to politicians if

846
00:28:51,200 --> 00:28:55,039
they're democrats i talk about utility

847
00:28:52,880 --> 00:28:56,399
functions and economic growth with

848
00:28:55,039 --> 00:28:59,600
republicans i talk about national

849
00:28:56,399 --> 00:29:01,200
security and i say the same things

850
00:28:59,600 --> 00:29:03,600
but i understand my audience well enough

851
00:29:01,200 --> 00:29:05,120
to shift what i say in words and

852
00:29:03,600 --> 00:29:06,960
emphasis

853
00:29:05,120 --> 00:29:09,840
uh when i was doing this when i was at

854
00:29:06,960 --> 00:29:11,200
google i would go visit a country and if

855
00:29:09,840 --> 00:29:13,200
the country would typically have a

856
00:29:11,200 --> 00:29:15,200
dictator and a number two that was not a

857
00:29:13,200 --> 00:29:16,159
dictator or vice versa

858
00:29:15,200 --> 00:29:17,600
and i would

859
00:29:16,159 --> 00:29:20,159
for one i would say the internet will

860
00:29:17,600 --> 00:29:22,080
benefit you economically and growth and

861
00:29:20,159 --> 00:29:23,600
the other one i would say that you need

862
00:29:22,080 --> 00:29:25,039
to do this

863
00:29:23,600 --> 00:29:26,960
and basically not talk about the

864
00:29:25,039 --> 00:29:28,880
openness that it invite

865
00:29:26,960 --> 00:29:31,679
in another case i would say this will be

866
00:29:28,880 --> 00:29:34,480
openness and democracy and all that

867
00:29:31,679 --> 00:29:37,600
i do this naturally we all do

868
00:29:34,480 --> 00:29:39,760
so when the system understands

869
00:29:37,600 --> 00:29:42,240
what your biases are

870
00:29:39,760 --> 00:29:44,720
and the summary is no longer the same

871
00:29:42,240 --> 00:29:47,200
summary that everybody else gets but

872
00:29:44,720 --> 00:29:49,200
it's tailored to you

873
00:29:47,200 --> 00:29:51,279
that is another set of issues so what

874
00:29:49,200 --> 00:29:52,240
happens is the moment in computer

875
00:29:51,279 --> 00:29:54,240
science

876
00:29:52,240 --> 00:29:55,440
remember the moment something happens

877
00:29:54,240 --> 00:29:56,960
then you have the the problem of

878
00:29:55,440 --> 00:29:59,760
recursion right so it's there's always

879
00:29:56,960 --> 00:30:00,880
going to be a successor that uses that

880
00:29:59,760 --> 00:30:03,279
so

881
00:30:00,880 --> 00:30:04,320
the ultimate outcome could be

882
00:30:03,279 --> 00:30:06,640
highly

883
00:30:04,320 --> 00:30:08,640
highly specialized narratives

884
00:30:06,640 --> 00:30:11,279
that are specific to you

885
00:30:08,640 --> 00:30:12,640
you have no idea why it's specific to

886
00:30:11,279 --> 00:30:14,880
you because it learns something about

887
00:30:12,640 --> 00:30:16,799
you that may or may not be true

888
00:30:14,880 --> 00:30:18,240
but nevertheless you got it and dan got

889
00:30:16,799 --> 00:30:20,480
something different and ozu got

890
00:30:18,240 --> 00:30:21,679
something different in and of itself who

891
00:30:20,480 --> 00:30:23,440
knows

892
00:30:21,679 --> 00:30:25,520
it's it's the

893
00:30:23,440 --> 00:30:27,279
the it's the um

894
00:30:25,520 --> 00:30:29,039
much more omnipresent version of the

895
00:30:27,279 --> 00:30:30,720
annoying thing that you know netflix

896
00:30:29,039 --> 00:30:32,240
learns something about you that actually

897
00:30:30,720 --> 00:30:33,919
has nothing to do with your taste and

898
00:30:32,240 --> 00:30:35,279
keeps recommending all kinds of stuff

899
00:30:33,919 --> 00:30:36,480
you don't want to watch

900
00:30:35,279 --> 00:30:38,320
and when this started you know the

901
00:30:36,480 --> 00:30:41,039
recommendation engines were relatively

902
00:30:38,320 --> 00:30:43,520
simple um i used to give the example uh

903
00:30:41,039 --> 00:30:45,279
when i was selling google google ads but

904
00:30:43,520 --> 00:30:47,279
the tv would show

905
00:30:45,279 --> 00:30:49,200
an ad for diapers

906
00:30:47,279 --> 00:30:51,039
and there's no diapers in the house well

907
00:30:49,200 --> 00:30:53,120
that's a wasted ad impression it should

908
00:30:51,039 --> 00:30:54,799
know something about the house

909
00:30:53,120 --> 00:30:56,159
well now in 20 years we've gotten to the

910
00:30:54,799 --> 00:30:57,360
point where it knows everything about

911
00:30:56,159 --> 00:30:59,600
the house

912
00:30:57,360 --> 00:30:59,600
right

913
00:30:59,840 --> 00:31:03,039
so

914
00:31:00,799 --> 00:31:04,720
just to follow up on some of the things

915
00:31:03,039 --> 00:31:07,200
you mentioned i'd like to sort of focus

916
00:31:04,720 --> 00:31:09,679
on the pace of change and how we adapt

917
00:31:07,200 --> 00:31:11,440
uh to this how we stay ahead of this and

918
00:31:09,679 --> 00:31:14,080
you do i think

919
00:31:11,440 --> 00:31:15,600
i found it fascinating that you also go

920
00:31:14,080 --> 00:31:18,159
back and you know

921
00:31:15,600 --> 00:31:20,480
similar to the previous looking backward

922
00:31:18,159 --> 00:31:21,200
that i talked about you consider sort of

923
00:31:20,480 --> 00:31:23,200
the

924
00:31:21,200 --> 00:31:27,440
evolution of human thought

925
00:31:23,200 --> 00:31:30,240
around reason with lots of philosophical

926
00:31:27,440 --> 00:31:32,880
underpinnings and pioneering

927
00:31:30,240 --> 00:31:35,440
contributions from kant to wittgenstein

928
00:31:32,880 --> 00:31:38,080
and then you sort of mention i think

929
00:31:35,440 --> 00:31:40,559
very much and i'll refer to my notes for

930
00:31:38,080 --> 00:31:42,480
that is that online platforms inundate

931
00:31:40,559 --> 00:31:44,559
users with opinions of millions of

932
00:31:42,480 --> 00:31:46,799
others depriving them of the reflection

933
00:31:44,559 --> 00:31:49,519
needed to develop that wisdom and the

934
00:31:46,799 --> 00:31:51,919
social values i sense an underlying

935
00:31:49,519 --> 00:31:54,720
worry or concerns about you know how

936
00:31:51,919 --> 00:31:56,799
we're going to be how our existing

937
00:31:54,720 --> 00:31:59,279
you know philosophical constructs or

938
00:31:56,799 --> 00:32:00,720
societal institutions will be whether

939
00:31:59,279 --> 00:32:02,399
they will be sufficient to deal with

940
00:32:00,720 --> 00:32:03,919
this change so what are your thoughts on

941
00:32:02,399 --> 00:32:05,600
that how do we

942
00:32:03,919 --> 00:32:08,559
deal with it how do we

943
00:32:05,600 --> 00:32:10,240
you know change our uh social uh

944
00:32:08,559 --> 00:32:12,159
government uh

945
00:32:10,240 --> 00:32:14,480
uh frameworks to be able to stay ahead

946
00:32:12,159 --> 00:32:16,799
of this change yep so i'm going to

947
00:32:14,480 --> 00:32:18,640
very s uh carefully not answer this

948
00:32:16,799 --> 00:32:20,320
question

949
00:32:18,640 --> 00:32:21,519
well just because so much of what we're

950
00:32:20,320 --> 00:32:23,120
we've been trying to do in the work

951
00:32:21,519 --> 00:32:26,080
we've been doing together is you know as

952
00:32:23,120 --> 00:32:28,080
you noted a suit to provide framing for

953
00:32:26,080 --> 00:32:29,519
people to to start to think about those

954
00:32:28,080 --> 00:32:32,159
things more and i think

955
00:32:29,519 --> 00:32:34,559
to me the historical framing that's

956
00:32:32,159 --> 00:32:36,640
very relevant and important to your your

957
00:32:34,559 --> 00:32:37,440
uh your question here is

958
00:32:36,640 --> 00:32:39,519
um

959
00:32:37,440 --> 00:32:42,960
if you if you look at um sort of

960
00:32:39,519 --> 00:32:45,279
evolution of human thought around the

961
00:32:42,960 --> 00:32:47,519
time period where in the west the you

962
00:32:45,279 --> 00:32:49,519
know movable type printing press

963
00:32:47,519 --> 00:32:51,440
uh moved us from a world where you know

964
00:32:49,519 --> 00:32:53,039
most people got information

965
00:32:51,440 --> 00:32:55,200
through word of mouth

966
00:32:53,039 --> 00:32:57,679
or from organized religion

967
00:32:55,200 --> 00:32:59,760
to where they could you know

968
00:32:57,679 --> 00:33:01,760
literacy became more and more uh

969
00:32:59,760 --> 00:33:03,919
widespread and people could read what

970
00:33:01,760 --> 00:33:06,320
they wanted to and form opinions from

971
00:33:03,919 --> 00:33:09,600
from from what they saw published

972
00:33:06,320 --> 00:33:12,080
and and the huge impact on society was

973
00:33:09,600 --> 00:33:13,840
moving from a world where

974
00:33:12,080 --> 00:33:15,679
faith was the main way that people

975
00:33:13,840 --> 00:33:17,440
understood the world around them to a

976
00:33:15,679 --> 00:33:20,159
world where

977
00:33:17,440 --> 00:33:22,640
human reason and faith together were how

978
00:33:20,159 --> 00:33:24,799
people understood the world

979
00:33:22,640 --> 00:33:26,720
and i think one of the the big changes

980
00:33:24,799 --> 00:33:28,159
with ai and we don't understand what it

981
00:33:26,720 --> 00:33:30,480
means yet

982
00:33:28,159 --> 00:33:31,600
uh is that there's another change

983
00:33:30,480 --> 00:33:33,760
happening

984
00:33:31,600 --> 00:33:34,559
that when you have another entity that

985
00:33:33,760 --> 00:33:37,440
is

986
00:33:34,559 --> 00:33:39,600
you know similarly intelligent to humans

987
00:33:37,440 --> 00:33:40,640
at least in some cases

988
00:33:39,600 --> 00:33:42,880
uh

989
00:33:40,640 --> 00:33:44,480
that's going to lead to some other way

990
00:33:42,880 --> 00:33:45,919
of looking at the world

991
00:33:44,480 --> 00:33:48,080
and it could be

992
00:33:45,919 --> 00:33:50,720
that we come to view

993
00:33:48,080 --> 00:33:53,919
you know humanity at large comes to view

994
00:33:50,720 --> 00:33:55,519
a.i as this omniscient oracle

995
00:33:53,919 --> 00:33:56,960
and then it sort of becomes another kind

996
00:33:55,519 --> 00:33:59,519
of faith

997
00:33:56,960 --> 00:34:02,000
right so you sort of you move from

998
00:33:59,519 --> 00:34:03,919
from you know faith to faith and reason

999
00:34:02,000 --> 00:34:05,200
to you know faith reason and another

1000
00:34:03,919 --> 00:34:08,159
kind of faith

1001
00:34:05,200 --> 00:34:10,879
uh or it could be that you know

1002
00:34:08,159 --> 00:34:13,200
ai evol is developed in ways that people

1003
00:34:10,879 --> 00:34:14,320
can really engage and can learn more and

1004
00:34:13,200 --> 00:34:16,000
can become

1005
00:34:14,320 --> 00:34:18,320
you know much more educated in

1006
00:34:16,000 --> 00:34:21,200
interlocutors of the world around them

1007
00:34:18,320 --> 00:34:22,720
uh and and in fact act to

1008
00:34:21,200 --> 00:34:23,679
reinforce human reasoning so i mean

1009
00:34:22,720 --> 00:34:25,599
they're

1010
00:34:23,679 --> 00:34:28,159
they're very different possible outcomes

1011
00:34:25,599 --> 00:34:29,520
that can come from how ai is developed

1012
00:34:28,159 --> 00:34:32,240
how humanity

1013
00:34:29,520 --> 00:34:34,560
starts to use ai um and and those kinds

1014
00:34:32,240 --> 00:34:36,240
of broad framings i think are incredibly

1015
00:34:34,560 --> 00:34:37,040
important and often

1016
00:34:36,240 --> 00:34:38,639
you know

1017
00:34:37,040 --> 00:34:40,000
not what we're thinking about when we're

1018
00:34:38,639 --> 00:34:42,399
when we're deploying these kinds of

1019
00:34:40,000 --> 00:34:44,879
technologies

1020
00:34:42,399 --> 00:34:44,879
the um

1021
00:34:46,560 --> 00:34:48,800
let's leave it at that i think that's

1022
00:34:47,760 --> 00:34:50,399
good

1023
00:34:48,800 --> 00:34:51,919
okay i'd like to move away from the

1024
00:34:50,399 --> 00:34:54,960
worries of it and then focus on the

1025
00:34:51,919 --> 00:34:58,000
positives like a new technology for the

1026
00:34:54,960 --> 00:34:59,200
greater good to improve human lives and

1027
00:34:58,000 --> 00:35:01,119
uh

1028
00:34:59,200 --> 00:35:02,960
condition and i know this is something

1029
00:35:01,119 --> 00:35:06,240
you both care a lot about

1030
00:35:02,960 --> 00:35:08,320
what are some areas that ai will have

1031
00:35:06,240 --> 00:35:10,320
the potential for immense positive

1032
00:35:08,320 --> 00:35:12,560
change in the you know short term long

1033
00:35:10,320 --> 00:35:13,359
term i think we would both answer this

1034
00:35:12,560 --> 00:35:15,440
with

1035
00:35:13,359 --> 00:35:18,480
one word it used to be plastics in the

1036
00:35:15,440 --> 00:35:20,000
movie now it's biology

1037
00:35:18,480 --> 00:35:20,960
right

1038
00:35:20,000 --> 00:35:22,560
yeah

1039
00:35:20,960 --> 00:35:25,440
one of my friends said that you know

1040
00:35:22,560 --> 00:35:28,880
physics requires math in the same way

1041
00:35:25,440 --> 00:35:31,760
that biology will require ai

1042
00:35:28,880 --> 00:35:35,280
the the biological systems are so

1043
00:35:31,760 --> 00:35:36,720
complicated and so incomputable in in

1044
00:35:35,280 --> 00:35:38,000
eric's terms

1045
00:35:36,720 --> 00:35:39,599
that the only way we're going to

1046
00:35:38,000 --> 00:35:42,160
understand these complicated living

1047
00:35:39,599 --> 00:35:45,040
systems um is through the kind of

1048
00:35:42,160 --> 00:35:47,200
modeling that ai it's as though ai was

1049
00:35:45,040 --> 00:35:49,280
invented to solve this problem it looks

1050
00:35:47,200 --> 00:35:50,320
for patterns it looks it looks for

1051
00:35:49,280 --> 00:35:52,960
changes

1052
00:35:50,320 --> 00:35:55,520
it can do generative work

1053
00:35:52,960 --> 00:35:57,040
uh for every analysis that an ai system

1054
00:35:55,520 --> 00:35:58,560
it can generate an equivalently

1055
00:35:57,040 --> 00:36:00,240
interesting answer

1056
00:35:58,560 --> 00:36:02,560
so

1057
00:36:00,240 --> 00:36:05,040
here at mit you have many people who are

1058
00:36:02,560 --> 00:36:08,240
using this to basically look at very

1059
00:36:05,040 --> 00:36:09,680
complicated biological processes for

1060
00:36:08,240 --> 00:36:11,440
various drug discovery and things like

1061
00:36:09,680 --> 00:36:13,040
this

1062
00:36:11,440 --> 00:36:15,359
the hallison example maybe you should

1063
00:36:13,040 --> 00:36:17,920
talk about is a it's a good example of

1064
00:36:15,359 --> 00:36:19,280
how traditionally stovepiped industries

1065
00:36:17,920 --> 00:36:21,200
have actually

1066
00:36:19,280 --> 00:36:23,119
worked together as a team

1067
00:36:21,200 --> 00:36:25,680
and i will say further that i think here

1068
00:36:23,119 --> 00:36:26,560
at mit every science project will have

1069
00:36:25,680 --> 00:36:29,440
an

1070
00:36:26,560 --> 00:36:31,440
analogous ai team with it

1071
00:36:29,440 --> 00:36:32,640
i think that's why steve schwartzman

1072
00:36:31,440 --> 00:36:35,599
generated

1073
00:36:32,640 --> 00:36:37,440
college i think that's why you came here

1074
00:36:35,599 --> 00:36:39,440
i think one thing you mentioned there

1075
00:36:37,440 --> 00:36:43,280
that i would just underscore

1076
00:36:39,440 --> 00:36:44,960
is that um you know so many existing

1077
00:36:43,280 --> 00:36:46,880
uh

1078
00:36:44,960 --> 00:36:48,640
things that we use in scientific and

1079
00:36:46,880 --> 00:36:50,480
engineering discovery

1080
00:36:48,640 --> 00:36:52,720
are for analysis

1081
00:36:50,480 --> 00:36:52,720
and

1082
00:36:52,880 --> 00:36:59,920
ai both enables analysis and synthesis

1083
00:36:58,000 --> 00:37:01,440
and so many problems that we're trying

1084
00:36:59,920 --> 00:37:03,839
to solve in science and engineering

1085
00:37:01,440 --> 00:37:06,000
synthesis is a key aspect of the problem

1086
00:37:03,839 --> 00:37:08,160
so it really transforms the landscape

1087
00:37:06,000 --> 00:37:10,079
and i i would agree that i see the

1088
00:37:08,160 --> 00:37:11,280
biological and life sciences at the core

1089
00:37:10,079 --> 00:37:13,839
of that but

1090
00:37:11,280 --> 00:37:15,520
um but you know materials and chemistry

1091
00:37:13,839 --> 00:37:17,280
and other things are also very hard and

1092
00:37:15,520 --> 00:37:18,160
even parts of physics frankly where the

1093
00:37:17,280 --> 00:37:19,119
math

1094
00:37:18,160 --> 00:37:22,880
only

1095
00:37:19,119 --> 00:37:25,680
gives you a very uh uh um you know broad

1096
00:37:22,880 --> 00:37:28,160
brush uh understanding of things where

1097
00:37:25,680 --> 00:37:29,680
ability to do analysis and synthesis

1098
00:37:28,160 --> 00:37:31,599
with machine learning i think is really

1099
00:37:29,680 --> 00:37:32,720
going to transform things yeah one of

1100
00:37:31,599 --> 00:37:35,520
the things that's interesting is that

1101
00:37:32,720 --> 00:37:36,720
the canonical model for ai research was

1102
00:37:35,520 --> 00:37:39,040
you get

1103
00:37:36,720 --> 00:37:41,280
lots and lots of data you analyze it

1104
00:37:39,040 --> 00:37:42,160
real world data so data matters a great

1105
00:37:41,280 --> 00:37:44,400
deal

1106
00:37:42,160 --> 00:37:46,720
and then you come up with insights

1107
00:37:44,400 --> 00:37:49,440
so i'm sitting in a meeting

1108
00:37:46,720 --> 00:37:50,640
at a research group that's studying the

1109
00:37:49,440 --> 00:37:53,680
question of

1110
00:37:50,640 --> 00:37:56,640
reasons that were never apparent to me

1111
00:37:53,680 --> 00:37:58,720
whether a mouse was sleeping or not

1112
00:37:56,640 --> 00:38:01,440
and it turns out that mouse sleeping is

1113
00:37:58,720 --> 00:38:03,599
difficult to detect

1114
00:38:01,440 --> 00:38:06,560
for reasons again i don't understand

1115
00:38:03,599 --> 00:38:08,640
and they couldn't come up with

1116
00:38:06,560 --> 00:38:11,040
training data by measuring it

1117
00:38:08,640 --> 00:38:12,640
so they built a simulated mouse using

1118
00:38:11,040 --> 00:38:14,640
normal mechanics

1119
00:38:12,640 --> 00:38:15,839
and they figured out how mice sleep

1120
00:38:14,640 --> 00:38:18,640
enough and then they generated the

1121
00:38:15,839 --> 00:38:20,240
training data synthetically

1122
00:38:18,640 --> 00:38:21,680
and i thought that's like a violation of

1123
00:38:20,240 --> 00:38:22,800
the first principle first principle is

1124
00:38:21,680 --> 00:38:24,880
you have to have lots of training data

1125
00:38:22,800 --> 00:38:27,359
well these guys generated it

1126
00:38:24,880 --> 00:38:29,599
so i think the second phase of ai

1127
00:38:27,359 --> 00:38:31,680
is that scientists are going to

1128
00:38:29,599 --> 00:38:33,839
use the known principles

1129
00:38:31,680 --> 00:38:37,040
of the world and generate the training

1130
00:38:33,839 --> 00:38:38,960
information using techniques like this

1131
00:38:37,040 --> 00:38:40,640
and that will then drive

1132
00:38:38,960 --> 00:38:42,960
both traditional learning reinforcement

1133
00:38:40,640 --> 00:38:44,240
learning and so forth

1134
00:38:42,960 --> 00:38:46,400
there are some things that would be

1135
00:38:44,240 --> 00:38:49,280
amazing for example if some of you could

1136
00:38:46,400 --> 00:38:51,440
invent a universal simulator

1137
00:38:49,280 --> 00:38:52,640
it could be used against across all of

1138
00:38:51,440 --> 00:38:54,320
these fields that would make a big

1139
00:38:52,640 --> 00:38:57,440
difference reinforcement learning

1140
00:38:54,320 --> 00:38:58,960
requires a fast simulator

1141
00:38:57,440 --> 00:39:01,520
because you're in reinforcement learning

1142
00:38:58,960 --> 00:39:04,320
you're looking at multiple forward paths

1143
00:39:01,520 --> 00:39:06,480
and you're optimizing the

1144
00:39:04,320 --> 00:39:08,800
the choices

1145
00:39:06,480 --> 00:39:10,960
i do think the power of simulation in

1146
00:39:08,800 --> 00:39:12,640
machine learning is something that uh

1147
00:39:10,960 --> 00:39:15,520
we're still only scratching the surface

1148
00:39:12,640 --> 00:39:17,440
of with reinforcement learning

1149
00:39:15,520 --> 00:39:18,960
i'll ask maybe one last question we're

1150
00:39:17,440 --> 00:39:20,960
running out of time unbelievable i have

1151
00:39:18,960 --> 00:39:23,359
so many more questions but i'd like to

1152
00:39:20,960 --> 00:39:25,040
also contrast some of these technologies

1153
00:39:23,359 --> 00:39:28,240
and what you're experiencing with social

1154
00:39:25,040 --> 00:39:30,240
media a topic very you know uh something

1155
00:39:28,240 --> 00:39:32,800
we talk a lot about at mit i know eric

1156
00:39:30,240 --> 00:39:34,560
you're very interested in uh like what's

1157
00:39:32,800 --> 00:39:36,560
the impact of ai technologies in that

1158
00:39:34,560 --> 00:39:38,160
domain where it's you know

1159
00:39:36,560 --> 00:39:40,400
even more interesting because these

1160
00:39:38,160 --> 00:39:42,640
systems help us you know deal with this

1161
00:39:40,400 --> 00:39:44,640
uh incredible amount of information but

1162
00:39:42,640 --> 00:39:47,200
at the same time they learn from us

1163
00:39:44,640 --> 00:39:50,079
shape our behavior and you know lead to

1164
00:39:47,200 --> 00:39:52,240
very profound uh social political well

1165
00:39:50,079 --> 00:39:54,000
everybody saw the coverage last year of

1166
00:39:52,240 --> 00:39:55,680
the what are called the facebook files

1167
00:39:54,000 --> 00:39:58,400
where internal documents were stolen

1168
00:39:55,680 --> 00:39:59,599
from facebook and leaked

1169
00:39:58,400 --> 00:40:00,560
and

1170
00:39:59,599 --> 00:40:02,640
there was a

1171
00:40:00,560 --> 00:40:03,760
big hubbub about that regulatory hub up

1172
00:40:02,640 --> 00:40:05,280
and so forth i don't know how that's

1173
00:40:03,760 --> 00:40:07,680
going to get resolved

1174
00:40:05,280 --> 00:40:09,839
but there were a series of

1175
00:40:07,680 --> 00:40:11,119
interviews with people who worked inside

1176
00:40:09,839 --> 00:40:12,480
of facebook

1177
00:40:11,119 --> 00:40:14,400
as opposed to people like me who just

1178
00:40:12,480 --> 00:40:16,079
make stuff up about this

1179
00:40:14,400 --> 00:40:17,440
and they asked them

1180
00:40:16,079 --> 00:40:18,400
how would you actually solve this

1181
00:40:17,440 --> 00:40:22,400
problem

1182
00:40:18,400 --> 00:40:24,720
and they said go back to the linear feed

1183
00:40:22,400 --> 00:40:26,720
i said that's interesting

1184
00:40:24,720 --> 00:40:29,440
so what had happened was in the last two

1185
00:40:26,720 --> 00:40:32,079
years ai who's a leading ai researcher a

1186
00:40:29,440 --> 00:40:34,000
facebook a leading ai researcher had

1187
00:40:32,079 --> 00:40:36,000
amped up the feed

1188
00:40:34,000 --> 00:40:38,319
using the signals

1189
00:40:36,000 --> 00:40:40,079
to make it more impactful and so forth

1190
00:40:38,319 --> 00:40:42,880
and they i assume that they did this

1191
00:40:40,079 --> 00:40:45,839
legitimately and for good reasons

1192
00:40:42,880 --> 00:40:48,880
so that's a good example where

1193
00:40:45,839 --> 00:40:50,880
you have a generally well-run company

1194
00:40:48,880 --> 00:40:52,720
um trying to do its mission in the world

1195
00:40:50,880 --> 00:40:55,520
and then they adopt ai and then they get

1196
00:40:52,720 --> 00:40:57,680
this huge impact in blowback

1197
00:40:55,520 --> 00:40:59,200
maybe they didn't understand the impact

1198
00:40:57,680 --> 00:41:01,599
they were getting or maybe they

1199
00:40:59,200 --> 00:41:04,400
misjudged the impact that they would get

1200
00:41:01,599 --> 00:41:06,720
but they're paying big time for it

1201
00:41:04,400 --> 00:41:08,319
there's a current example

1202
00:41:06,720 --> 00:41:09,599
um

1203
00:41:08,319 --> 00:41:12,240
and i don't know how that's going to get

1204
00:41:09,599 --> 00:41:14,560
resolved yeah i think social media

1205
00:41:12,240 --> 00:41:16,720
generally is a very you know the use of

1206
00:41:14,560 --> 00:41:17,839
ai in social media and social media

1207
00:41:16,720 --> 00:41:19,680
today

1208
00:41:17,839 --> 00:41:22,000
don't don't fool yourself into thinking

1209
00:41:19,680 --> 00:41:22,960
that today's social media could operate

1210
00:41:22,000 --> 00:41:25,440
without

1211
00:41:22,960 --> 00:41:27,280
massive use of machine learning uh and

1212
00:41:25,440 --> 00:41:28,800
and automated decision making it just it

1213
00:41:27,280 --> 00:41:32,160
doesn't scale

1214
00:41:28,800 --> 00:41:32,160
uh but but i think

1215
00:41:32,240 --> 00:41:38,560
the use of ai is acts as a big amplifier

1216
00:41:37,119 --> 00:41:40,800
of

1217
00:41:38,560 --> 00:41:42,160
everything that humans do

1218
00:41:40,800 --> 00:41:43,920
what's good

1219
00:41:42,160 --> 00:41:45,680
you know what we view as good what we

1220
00:41:43,920 --> 00:41:48,400
view is bad what we like about ourselves

1221
00:41:45,680 --> 00:41:50,640
what we don't like about ourselves

1222
00:41:48,400 --> 00:41:51,920
and i think you know partly

1223
00:41:50,640 --> 00:41:53,680
facebook

1224
00:41:51,920 --> 00:41:54,640
has gotten sort of caught up on you know

1225
00:41:53,680 --> 00:41:56,079
in

1226
00:41:54,640 --> 00:41:58,800
in making the

1227
00:41:56,079 --> 00:42:00,079
feed be so much more driven by by ai

1228
00:41:58,800 --> 00:42:01,280
recommendation

1229
00:42:00,079 --> 00:42:02,640
they've gotten caught up in that

1230
00:42:01,280 --> 00:42:05,920
amplification

1231
00:42:02,640 --> 00:42:07,359
uh that implication phenomenon so some

1232
00:42:05,920 --> 00:42:08,720
of the things that they're amplifying

1233
00:42:07,359 --> 00:42:10,720
presumably

1234
00:42:08,720 --> 00:42:12,240
are very valuable to their users and

1235
00:42:10,720 --> 00:42:14,640
that's getting a certain kind of

1236
00:42:12,240 --> 00:42:16,800
engagement and others are things where

1237
00:42:14,640 --> 00:42:17,760
you know various researchers at facebook

1238
00:42:16,800 --> 00:42:19,680
said

1239
00:42:17,760 --> 00:42:21,599
why are we amplifying this

1240
00:42:19,680 --> 00:42:23,839
uh and teasing those apart is not always

1241
00:42:21,599 --> 00:42:26,560
easy you know i told you

1242
00:42:23,839 --> 00:42:30,319
that i i'm a simple principal i'm in

1243
00:42:26,560 --> 00:42:32,560
favor of free speech for humans

1244
00:42:30,319 --> 00:42:34,160
not for robots

1245
00:42:32,560 --> 00:42:36,640
okay

1246
00:42:34,160 --> 00:42:38,160
so this amplification problem is very

1247
00:42:36,640 --> 00:42:40,400
real i'll give you another example and

1248
00:42:38,160 --> 00:42:41,599
this is from maybe seven years ago at

1249
00:42:40,400 --> 00:42:42,800
youtube

1250
00:42:41,599 --> 00:42:44,400
um

1251
00:42:42,800 --> 00:42:48,000
youtube would show

1252
00:42:44,400 --> 00:42:50,400
a video of something pretty nasty

1253
00:42:48,000 --> 00:42:52,720
and the recommendation engine was so

1254
00:42:50,400 --> 00:42:55,440
good that it would keep showing it

1255
00:42:52,720 --> 00:42:57,119
and there was evidence that this would

1256
00:42:55,440 --> 00:43:00,560
drive people into

1257
00:42:57,119 --> 00:43:03,440
terrorist or near terrorist activities

1258
00:43:00,560 --> 00:43:04,560
and so the way this was solved was the

1259
00:43:03,440 --> 00:43:07,839
company

1260
00:43:04,560 --> 00:43:09,520
defined a class of categories which were

1261
00:43:07,839 --> 00:43:12,480
special

1262
00:43:09,520 --> 00:43:15,359
and they were not to be amplified

1263
00:43:12,480 --> 00:43:17,119
so they if you saw one they weren't

1264
00:43:15,359 --> 00:43:19,440
censored but they weren't amplified so

1265
00:43:17,119 --> 00:43:22,079
you could find it and you could watch it

1266
00:43:19,440 --> 00:43:24,160
but it wouldn't suggest another one

1267
00:43:22,079 --> 00:43:26,640
and furthermore we wouldn't model we

1268
00:43:24,160 --> 00:43:30,319
wouldn't monetize it so that's an

1269
00:43:26,640 --> 00:43:31,839
example of a fairly crude and simple

1270
00:43:30,319 --> 00:43:32,720
change that you could make where you've

1271
00:43:31,839 --> 00:43:35,280
got

1272
00:43:32,720 --> 00:43:36,720
normal stuff and then let's just say on

1273
00:43:35,280 --> 00:43:38,640
the edge stuff there's clearly that

1274
00:43:36,720 --> 00:43:41,280
stuff that's illegal

1275
00:43:38,640 --> 00:43:44,240
but uh things which could be interpreted

1276
00:43:41,280 --> 00:43:46,319
as recruitment videos for bad acts

1277
00:43:44,240 --> 00:43:47,680
and we would and we had a process that

1278
00:43:46,319 --> 00:43:49,520
would judge that

1279
00:43:47,680 --> 00:43:52,160
so that's an example of the kind of

1280
00:43:49,520 --> 00:43:54,480
change that you could make

1281
00:43:52,160 --> 00:43:55,520
i don't think you're going to solve the

1282
00:43:54,480 --> 00:43:57,920
problem

1283
00:43:55,520 --> 00:43:59,920
in a fundamental way

1284
00:43:57,920 --> 00:44:01,599
and and i think there's a project here

1285
00:43:59,920 --> 00:44:03,359
looking at this there's other people i

1286
00:44:01,599 --> 00:44:06,160
know working on it but until we come up

1287
00:44:03,359 --> 00:44:07,599
with some new some new ideas

1288
00:44:06,160 --> 00:44:09,680
everyone involved in the content

1289
00:44:07,599 --> 00:44:12,319
business is going to fundamentally have

1290
00:44:09,680 --> 00:44:13,920
good content questionable content and

1291
00:44:12,319 --> 00:44:15,280
forbidden content forbidden content

1292
00:44:13,920 --> 00:44:17,920
you're not going to do

1293
00:44:15,280 --> 00:44:19,520
so child pornography things like that

1294
00:44:17,920 --> 00:44:22,000
the questionable content you're going to

1295
00:44:19,520 --> 00:44:24,480
have to not monetize it you're going to

1296
00:44:22,000 --> 00:44:26,560
have to not recommend it

1297
00:44:24,480 --> 00:44:27,920
and then you as the provider can make a

1298
00:44:26,560 --> 00:44:29,599
decision as to whether you want to host

1299
00:44:27,920 --> 00:44:31,680
it at all

1300
00:44:29,599 --> 00:44:33,280
and under american law

1301
00:44:31,680 --> 00:44:36,079
those decisions are made by the private

1302
00:44:33,280 --> 00:44:38,160
companies

1303
00:44:36,079 --> 00:44:39,200
i would just underscore

1304
00:44:38,160 --> 00:44:42,079
in the

1305
00:44:39,200 --> 00:44:43,440
you know not free speech for for robots

1306
00:44:42,079 --> 00:44:44,319
uh

1307
00:44:43,440 --> 00:44:45,920
that

1308
00:44:44,319 --> 00:44:47,040
the point that you made but i think that

1309
00:44:45,920 --> 00:44:48,880
people

1310
00:44:47,040 --> 00:44:51,760
often miss and that's why i just want to

1311
00:44:48,880 --> 00:44:53,520
underscore it is that when a social

1312
00:44:51,760 --> 00:44:55,760
media

1313
00:44:53,520 --> 00:44:57,119
algorithm is recommending is pushing

1314
00:44:55,760 --> 00:44:59,040
something at you

1315
00:44:57,119 --> 00:45:01,359
that's the robot speaking

1316
00:44:59,040 --> 00:45:03,280
it may be you know it is some human's

1317
00:45:01,359 --> 00:45:05,280
speech maybe maybe it was robot

1318
00:45:03,280 --> 00:45:08,000
generated speech to begin with but

1319
00:45:05,280 --> 00:45:08,960
assume it's some human speech that's

1320
00:45:08,000 --> 00:45:11,119
that's

1321
00:45:08,960 --> 00:45:12,800
you know that's human free speech

1322
00:45:11,119 --> 00:45:14,880
but the fact that it's pushed at

1323
00:45:12,800 --> 00:45:17,520
millions or billions of people

1324
00:45:14,880 --> 00:45:19,680
is actually the robot deciding to speak

1325
00:45:17,520 --> 00:45:21,440
and that is a very uh

1326
00:45:19,680 --> 00:45:23,599
complicated set of issues that we really

1327
00:45:21,440 --> 00:45:25,280
need to address as a society

1328
00:45:23,599 --> 00:45:26,800
yeah and then there's all sorts of

1329
00:45:25,280 --> 00:45:28,720
complicated problems we had one case

1330
00:45:26,800 --> 00:45:29,680
where we had we were studying this there

1331
00:45:28,720 --> 00:45:31,920
was a

1332
00:45:29,680 --> 00:45:33,760
person this was his speech but he set up

1333
00:45:31,920 --> 00:45:36,720
a twit tweeter bot

1334
00:45:33,760 --> 00:45:39,200
to tweet every hour 24 hours a day now

1335
00:45:36,720 --> 00:45:42,240
he wasn't awake every hour

1336
00:45:39,200 --> 00:45:44,560
so is that a permissible amplification

1337
00:45:42,240 --> 00:45:47,040
of his speech or should you say he can

1338
00:45:44,560 --> 00:45:48,960
only do that two-thirds of the day

1339
00:45:47,040 --> 00:45:50,560
you see that's where the the judgment

1340
00:45:48,960 --> 00:45:52,800
comes in

1341
00:45:50,560 --> 00:45:56,160
very good so i'd like to invite our

1342
00:45:52,800 --> 00:45:57,520
audience uh for some questions uh uh

1343
00:45:56,160 --> 00:46:01,839
and you know i think we'll have

1344
00:45:57,520 --> 00:46:01,839
microphones uh that eric has

1345
00:46:02,800 --> 00:46:07,040
hi um yeah i think for everything so far

1346
00:46:04,640 --> 00:46:09,599
uh in terms of um artificial general

1347
00:46:07,040 --> 00:46:12,240
intelligence uh an actual level of

1348
00:46:09,599 --> 00:46:13,839
analogy the field of orbital mechanics

1349
00:46:12,240 --> 00:46:16,160
made leaps and strides once people

1350
00:46:13,839 --> 00:46:17,920
started making or questioning uh bad

1351
00:46:16,160 --> 00:46:19,040
assumptions like circular orbits and

1352
00:46:17,920 --> 00:46:20,400
once people start asking the right

1353
00:46:19,040 --> 00:46:23,440
questions like what actually causes the

1354
00:46:20,400 --> 00:46:25,440
earth's orbits so for agi what are the

1355
00:46:23,440 --> 00:46:27,040
assumptions that we need to challenge or

1356
00:46:25,440 --> 00:46:28,160
the questions that we need to ask to

1357
00:46:27,040 --> 00:46:30,319
actually make the leaps and strides in

1358
00:46:28,160 --> 00:46:32,160
that field

1359
00:46:30,319 --> 00:46:36,640
well i think one of the questions since

1360
00:46:32,160 --> 00:46:38,000
i'm pro agi and dan is anti-agi

1361
00:46:36,640 --> 00:46:40,720
okay

1362
00:46:38,000 --> 00:46:43,839
let's just say that agi may be a

1363
00:46:40,720 --> 00:46:47,119
destination we never get to

1364
00:46:43,839 --> 00:46:49,839
it may be a construct that defines the

1365
00:46:47,119 --> 00:46:51,760
human progress toward these goals so

1366
00:46:49,839 --> 00:46:54,079
forth

1367
00:46:51,760 --> 00:46:57,119
or it may actually happen i don't think

1368
00:46:54,079 --> 00:46:59,839
we we know so that's the first question

1369
00:46:57,119 --> 00:47:04,480
and i define agi

1370
00:46:59,839 --> 00:47:05,839
as the ability to generate its own tasks

1371
00:47:04,480 --> 00:47:08,560
because that's the thing that to me

1372
00:47:05,839 --> 00:47:10,000
defines human intelligence over other

1373
00:47:08,560 --> 00:47:11,760
kinds of intelligence there are other

1374
00:47:10,000 --> 00:47:14,000
definitions

1375
00:47:11,760 --> 00:47:14,000
and

1376
00:47:14,640 --> 00:47:20,960
how would you build a system

1377
00:47:17,359 --> 00:47:24,160
using our kind of intuitive reasoning

1378
00:47:20,960 --> 00:47:25,440
it could generate new ideas

1379
00:47:24,160 --> 00:47:26,880
the

1380
00:47:25,440 --> 00:47:28,960
deep mind answer because i spent a lot

1381
00:47:26,880 --> 00:47:32,079
of time with them is that

1382
00:47:28,960 --> 00:47:35,200
you can using reinforcement learning

1383
00:47:32,079 --> 00:47:37,920
come up with predictable combinations

1384
00:47:35,200 --> 00:47:39,040
which you cannot explain as to why they

1385
00:47:37,920 --> 00:47:40,559
came up

1386
00:47:39,040 --> 00:47:42,800
and if you can't explain it that's

1387
00:47:40,559 --> 00:47:46,000
similar to human intuition

1388
00:47:42,800 --> 00:47:49,599
you know it but you can't explain it

1389
00:47:46,000 --> 00:47:52,160
so that's that sort of religious pitch

1390
00:47:49,599 --> 00:47:53,920
but my judgment is we don't have an

1391
00:47:52,160 --> 00:47:55,119
answer to the question of how do we go

1392
00:47:53,920 --> 00:47:57,720
from these

1393
00:47:55,119 --> 00:47:59,760
beautiful analytical systems which are

1394
00:47:57,720 --> 00:48:02,079
extraordinarily good

1395
00:47:59,760 --> 00:48:03,440
to where they generate their own

1396
00:48:02,079 --> 00:48:04,960
activity

1397
00:48:03,440 --> 00:48:07,839
you agree or disagree

1398
00:48:04,960 --> 00:48:11,440
i would actually agree

1399
00:48:07,839 --> 00:48:13,200
so what is your time frame for agi dan

1400
00:48:11,440 --> 00:48:16,559
well i'm still back on your initial

1401
00:48:13,200 --> 00:48:19,280
question of can it happen or not

1402
00:48:16,559 --> 00:48:19,280
i think you see the

1403
00:48:19,359 --> 00:48:20,280
of problem

1404
00:48:21,359 --> 00:48:24,160
other questions

1405
00:48:24,240 --> 00:48:27,119
and while they're getting to that i'd

1406
00:48:25,680 --> 00:48:29,920
like to share something and my

1407
00:48:27,119 --> 00:48:32,960
nine-year-old is very interested in agi

1408
00:48:29,920 --> 00:48:34,800
and reading your book and i was actually

1409
00:48:32,960 --> 00:48:37,119
telling him that you know the difference

1410
00:48:34,800 --> 00:48:39,440
between you that dance time frame versus

1411
00:48:37,119 --> 00:48:41,359
yours uh is different and he looked at

1412
00:48:39,440 --> 00:48:44,640
me and said i agree with dan but i'd

1413
00:48:41,359 --> 00:48:46,000
like to hear about eric's perspective

1414
00:48:44,640 --> 00:48:48,400
well if he's

1415
00:48:46,000 --> 00:48:51,200
let's see if he's nine his expected life

1416
00:48:48,400 --> 00:48:54,079
expectancy is age 100.

1417
00:48:51,200 --> 00:48:55,440
so in the next 91 years i think dan and

1418
00:48:54,079 --> 00:48:58,319
i would agree that there will be some

1419
00:48:55,440 --> 00:48:59,760
form of agi yes we may or may not see it

1420
00:48:58,319 --> 00:49:02,800
i will tell them that

1421
00:48:59,760 --> 00:49:04,240
so so your son is in a great position

1422
00:49:02,800 --> 00:49:07,280
i'm still not sure it'll have its own

1423
00:49:04,240 --> 00:49:08,640
motivation but uh let's see

1424
00:49:07,280 --> 00:49:10,160
where's the

1425
00:49:08,640 --> 00:49:13,839
i can't see yeah

1426
00:49:10,160 --> 00:49:16,960
so you you guys mentioned uh ai for

1427
00:49:13,839 --> 00:49:18,640
biology um and you guys also talked

1428
00:49:16,960 --> 00:49:19,680
about sort of the increasing capital

1429
00:49:18,640 --> 00:49:21,680
needs of training you know

1430
00:49:19,680 --> 00:49:24,160
state-of-the-art ai models right so the

1431
00:49:21,680 --> 00:49:26,720
typical like r0 grant amount is what

1432
00:49:24,160 --> 00:49:28,160
like 250 000 a year from the nih if

1433
00:49:26,720 --> 00:49:30,480
you're a well-funded startup you could

1434
00:49:28,160 --> 00:49:32,319
raise what 25 million in a series a

1435
00:49:30,480 --> 00:49:33,520
google just started their ai for drug

1436
00:49:32,319 --> 00:49:35,119
discovery

1437
00:49:33,520 --> 00:49:36,960
program where they could easily put you

1438
00:49:35,119 --> 00:49:39,440
know like 250 million a year into it if

1439
00:49:36,960 --> 00:49:42,400
they wanted to right so

1440
00:49:39,440 --> 00:49:43,920
it seems like ai research even applied

1441
00:49:42,400 --> 00:49:45,280
research in other fields is sort of

1442
00:49:43,920 --> 00:49:46,400
becoming

1443
00:49:45,280 --> 00:49:48,000
capital

1444
00:49:46,400 --> 00:49:49,839
intensive to the point where it could be

1445
00:49:48,000 --> 00:49:51,440
antithetical to like the principles of

1446
00:49:49,839 --> 00:49:53,119
open research

1447
00:49:51,440 --> 00:49:54,720
is this something that you guys could

1448
00:49:53,119 --> 00:49:56,839
see like challenging the development of

1449
00:49:54,720 --> 00:50:00,000
the field in the future it seems like a

1450
00:49:56,839 --> 00:50:02,079
concern uh an incredibly well put

1451
00:50:00,000 --> 00:50:03,680
question and i'll give you an example of

1452
00:50:02,079 --> 00:50:05,280
alpha fold

1453
00:50:03,680 --> 00:50:08,319
which you understand how fulfilled was

1454
00:50:05,280 --> 00:50:10,880
done elf folds a huge achievement

1455
00:50:08,319 --> 00:50:13,839
alpha fold was initially the technical

1456
00:50:10,880 --> 00:50:15,520
details of alpha hold were not released

1457
00:50:13,839 --> 00:50:17,200
and now i'm no longer at google so i'll

1458
00:50:15,520 --> 00:50:19,119
just say this without i don't mean to

1459
00:50:17,200 --> 00:50:20,640
criticize or praise

1460
00:50:19,119 --> 00:50:23,599
the baker lab

1461
00:50:20,640 --> 00:50:25,760
watched the video of alpha fold

1462
00:50:23,599 --> 00:50:27,760
reconfigured all of its stuff in some

1463
00:50:25,760 --> 00:50:30,240
complicated way i don't understand

1464
00:50:27,760 --> 00:50:32,960
and then started to produce

1465
00:50:30,240 --> 00:50:35,359
answers to human proteins

1466
00:50:32,960 --> 00:50:37,280
and then google then through deepmind

1467
00:50:35,359 --> 00:50:38,319
released everything

1468
00:50:37,280 --> 00:50:39,760
so

1469
00:50:38,319 --> 00:50:41,920
was it the

1470
00:50:39,760 --> 00:50:43,440
competition with alpha phil with the

1471
00:50:41,920 --> 00:50:45,280
with the baker lab that caused that

1472
00:50:43,440 --> 00:50:46,400
that's some articles have said that or

1473
00:50:45,280 --> 00:50:48,319
was it just

1474
00:50:46,400 --> 00:50:50,880
the lawyers slowing them down because

1475
00:50:48,319 --> 00:50:52,240
corporations lower slope slow people who

1476
00:50:50,880 --> 00:50:53,599
want to open source things down i don't

1477
00:50:52,240 --> 00:50:54,960
know

1478
00:50:53,599 --> 00:50:57,520
but the important point is we got the

1479
00:50:54,960 --> 00:50:59,839
information out and and understanding

1480
00:50:57,520 --> 00:51:02,400
the protein that affects humans and

1481
00:50:59,839 --> 00:51:04,960
their folding structure is a nobel prize

1482
00:51:02,400 --> 00:51:08,480
kind of achievement because it's like a

1483
00:51:04,960 --> 00:51:10,559
huge platform for knowledge in biology

1484
00:51:08,480 --> 00:51:12,720
so my guess is you're going to see the

1485
00:51:10,559 --> 00:51:14,400
usual mess you're going to have

1486
00:51:12,720 --> 00:51:15,359
corporations that keep it proprietary

1487
00:51:14,400 --> 00:51:16,720
you're going to have corporations that

1488
00:51:15,359 --> 00:51:18,640
behave the way google did which is their

1489
00:51:16,720 --> 00:51:20,240
release it you're going to see private

1490
00:51:18,640 --> 00:51:23,599
philanthropists which includes me that

1491
00:51:20,240 --> 00:51:24,800
are going to fund these models

1492
00:51:23,599 --> 00:51:26,559
and then you're going to see other

1493
00:51:24,800 --> 00:51:28,480
mechanisms

1494
00:51:26,559 --> 00:51:30,720
my guess is that some country the

1495
00:51:28,480 --> 00:51:32,480
obvious one would be china will actually

1496
00:51:30,720 --> 00:51:34,480
make this a national priority and we'll

1497
00:51:32,480 --> 00:51:37,359
just publish it all

1498
00:51:34,480 --> 00:51:38,960
so i think it'll resolve itself

1499
00:51:37,359 --> 00:51:39,839
but one of the things that's different

1500
00:51:38,960 --> 00:51:41,920
from

1501
00:51:39,839 --> 00:51:44,000
20 30 40 50 years old

1502
00:51:41,920 --> 00:51:46,160
is all of the progress is now occurring

1503
00:51:44,000 --> 00:51:50,160
in these corporations because they have

1504
00:51:46,160 --> 00:51:52,800
so much money relative to you

1505
00:51:50,160 --> 00:51:55,350
but we're smarter well

1506
00:51:52,800 --> 00:51:57,520
no dan since we hired all your people

1507
00:51:55,350 --> 00:51:59,760
[Laughter]

1508
00:51:57,520 --> 00:52:00,960
right we just transferred them from you

1509
00:51:59,760 --> 00:52:03,680
to me

1510
00:52:00,960 --> 00:52:05,680
uh no but more seriously so i agree with

1511
00:52:03,680 --> 00:52:08,000
you that i think that this is a

1512
00:52:05,680 --> 00:52:11,119
serious uh challenge

1513
00:52:08,000 --> 00:52:12,960
but i also think um that we're you know

1514
00:52:11,119 --> 00:52:14,960
when the research community gets focused

1515
00:52:12,960 --> 00:52:17,280
on challenges a lot of progress can be

1516
00:52:14,960 --> 00:52:19,119
made and important progress and a big

1517
00:52:17,280 --> 00:52:21,119
focus for a lot of machine learning work

1518
00:52:19,119 --> 00:52:22,960
now actually is training with much

1519
00:52:21,119 --> 00:52:25,280
smaller amounts of data

1520
00:52:22,960 --> 00:52:27,359
uh and um and you know in the

1521
00:52:25,280 --> 00:52:29,359
reinforcement learning setting with much

1522
00:52:27,359 --> 00:52:31,839
uh you know smaller numbers of

1523
00:52:29,359 --> 00:52:34,079
iterations of a simulation

1524
00:52:31,839 --> 00:52:36,720
and and i think this is an area that we

1525
00:52:34,079 --> 00:52:39,040
can and will make enormous advances in

1526
00:52:36,720 --> 00:52:40,880
uh but right now we are in a

1527
00:52:39,040 --> 00:52:43,599
uh whatever the opposite of a sweet spot

1528
00:52:40,880 --> 00:52:46,000
is with respect to this uh i do think

1529
00:52:43,599 --> 00:52:48,640
it's a big challenge but but but i think

1530
00:52:46,000 --> 00:52:50,960
that um there's already early signs of

1531
00:52:48,640 --> 00:52:52,960
progress in a number of ways that uh but

1532
00:52:50,960 --> 00:52:55,040
this is how it sort of happens right you

1533
00:52:52,960 --> 00:52:57,119
get this polyglot model

1534
00:52:55,040 --> 00:52:59,520
it's messy by design because of human

1535
00:52:57,119 --> 00:53:01,359
creativity and maybe ai creativity in

1536
00:52:59,520 --> 00:53:02,800
the future but eventually there will be

1537
00:53:01,359 --> 00:53:04,400
databases

1538
00:53:02,800 --> 00:53:05,599
and there'll be a database

1539
00:53:04,400 --> 00:53:07,839
of everything you need to know about

1540
00:53:05,599 --> 00:53:08,880
this aspect of biology and a database of

1541
00:53:07,839 --> 00:53:10,319
everything you need to know about this

1542
00:53:08,880 --> 00:53:12,000
aspect of physics

1543
00:53:10,319 --> 00:53:14,240
and it'll be generally understood to be

1544
00:53:12,000 --> 00:53:16,720
correct and those become the platforms

1545
00:53:14,240 --> 00:53:18,480
for the generation behind you

1546
00:53:16,720 --> 00:53:20,240
right you're struggling through this

1547
00:53:18,480 --> 00:53:21,760
strange problem of i don't have access

1548
00:53:20,240 --> 00:53:23,920
to it and these guys have it they won't

1549
00:53:21,760 --> 00:53:26,240
give it to me but the next generation

1550
00:53:23,920 --> 00:53:27,520
will have a common platform that's how

1551
00:53:26,240 --> 00:53:29,599
it always works and that's the reason to

1552
00:53:27,520 --> 00:53:31,680
be optimistic and more generally i

1553
00:53:29,599 --> 00:53:34,400
believe that constraints are really

1554
00:53:31,680 --> 00:53:36,559
important part of the creative process

1555
00:53:34,400 --> 00:53:38,079
and so you know academics have to work

1556
00:53:36,559 --> 00:53:40,000
on some different problems because we're

1557
00:53:38,079 --> 00:53:41,599
constrained we don't have 250 million

1558
00:53:40,000 --> 00:53:42,800
dollars worth of compute to put into

1559
00:53:41,599 --> 00:53:44,559
something

1560
00:53:42,800 --> 00:53:45,839
and so these more efficient algorithms

1561
00:53:44,559 --> 00:53:47,440
are something that we can and are

1562
00:53:45,839 --> 00:53:49,760
working on and to show you my

1563
00:53:47,440 --> 00:53:51,119
perspective uh as a computer scientist

1564
00:53:49,760 --> 00:53:52,079
who's been in the industry for a long

1565
00:53:51,119 --> 00:53:53,520
time

1566
00:53:52,079 --> 00:53:55,520
i was reasonably convinced that

1567
00:53:53,520 --> 00:53:57,520
universities were put with respect to

1568
00:53:55,520 --> 00:53:59,760
this stuff that we had simply drained

1569
00:53:57,520 --> 00:54:01,520
all the talent we'd taken all the money

1570
00:53:59,760 --> 00:54:03,119
and that there was nothing for them left

1571
00:54:01,520 --> 00:54:06,640
to do in these eight areas this is my

1572
00:54:03,119 --> 00:54:08,960
own arrogance clearly completely wrong

1573
00:54:06,640 --> 00:54:11,440
right and when i look at the last five

1574
00:54:08,960 --> 00:54:14,800
years of progress in science

1575
00:54:11,440 --> 00:54:16,720
astrophysics biology chemistry material

1576
00:54:14,800 --> 00:54:19,119
science a whole bunch of fields in

1577
00:54:16,720 --> 00:54:21,119
physics i don't understand

1578
00:54:19,119 --> 00:54:22,079
i listen to these guys presentations and

1579
00:54:21,119 --> 00:54:24,319
they are

1580
00:54:22,079 --> 00:54:25,760
world-class applications of some of

1581
00:54:24,319 --> 00:54:27,119
these underlying concepts and they're

1582
00:54:25,760 --> 00:54:29,599
different

1583
00:54:27,119 --> 00:54:31,520
so i think we're seeing a to be clear i

1584
00:54:29,599 --> 00:54:32,960
think we're seeing a renaissance in the

1585
00:54:31,520 --> 00:54:34,480
universities

1586
00:54:32,960 --> 00:54:36,079
now that the universities have got their

1587
00:54:34,480 --> 00:54:38,079
act together they got enough funding

1588
00:54:36,079 --> 00:54:40,799
people have a language you see a certain

1589
00:54:38,079 --> 00:54:42,319
amount of common goals you're seeing it

1590
00:54:40,799 --> 00:54:43,920
it feels very different from five years

1591
00:54:42,319 --> 00:54:46,559
ago i don't know if you agree with my

1592
00:54:43,920 --> 00:54:48,480
bias or not

1593
00:54:46,559 --> 00:54:50,160
well

1594
00:54:48,480 --> 00:54:51,680
to me

1595
00:54:50,160 --> 00:54:52,720
yes i mean i totally agree and i think

1596
00:54:51,680 --> 00:54:53,760
you know

1597
00:54:52,720 --> 00:54:55,119
part of

1598
00:54:53,760 --> 00:54:57,200
what we're trying to do with the college

1599
00:54:55,119 --> 00:55:00,480
of computing generally is that

1600
00:54:57,200 --> 00:55:02,960
computing and you know most notably ai

1601
00:55:00,480 --> 00:55:04,680
become part of the intellectual fabric

1602
00:55:02,960 --> 00:55:06,960
part of the way that people

1603
00:55:04,680 --> 00:55:09,359
conceptualize problems in a whole wide

1604
00:55:06,960 --> 00:55:11,040
variety of disciplines and i think that

1605
00:55:09,359 --> 00:55:13,359
you know there's still a lot to be done

1606
00:55:11,040 --> 00:55:16,319
to make that easier across

1607
00:55:13,359 --> 00:55:19,520
uh you know a wide variety of areas but

1608
00:55:16,319 --> 00:55:22,160
but i think that this is uh

1609
00:55:19,520 --> 00:55:23,680
definitely a golden age of of that kind

1610
00:55:22,160 --> 00:55:26,319
of transformation i mean i'll give you a

1611
00:55:23,680 --> 00:55:27,839
caltech example uh i first

1612
00:55:26,319 --> 00:55:29,839
figured out the world was different than

1613
00:55:27,839 --> 00:55:32,880
i said a few years ago when i was

1614
00:55:29,839 --> 00:55:34,880
visiting a group called the klima cli ma

1615
00:55:32,880 --> 00:55:35,839
model and mit is one of the partners in

1616
00:55:34,880 --> 00:55:38,480
this

1617
00:55:35,839 --> 00:55:41,040
mit does a lot of the the

1618
00:55:38,480 --> 00:55:42,720
surface c and modeling this is basically

1619
00:55:41,040 --> 00:55:44,240
climate modeling

1620
00:55:42,720 --> 00:55:46,000
and so the professor at caltech who's

1621
00:55:44,240 --> 00:55:48,880
one of these impossibly brilliant

1622
00:55:46,000 --> 00:55:50,240
physics types people is explaining the

1623
00:55:48,880 --> 00:55:51,599
most and he's talking about navier

1624
00:55:50,240 --> 00:55:53,760
stokes equations and he says well we

1625
00:55:51,599 --> 00:55:55,119
can't model clouds i said like what's

1626
00:55:53,760 --> 00:55:56,799
wrong with you why can't you it's just

1627
00:55:55,119 --> 00:55:59,839
not computable

1628
00:55:56,799 --> 00:56:00,880
so all of a sudden we show up with the

1629
00:55:59,839 --> 00:56:02,960
plug

1630
00:56:00,880 --> 00:56:05,200
that solves all of his other physics

1631
00:56:02,960 --> 00:56:06,799
problems because clouds are essential to

1632
00:56:05,200 --> 00:56:08,799
climate modeling

1633
00:56:06,799 --> 00:56:10,079
and so i visited with the team these are

1634
00:56:08,799 --> 00:56:11,760
all physicists

1635
00:56:10,079 --> 00:56:12,799
and they're impossibly intelligent

1636
00:56:11,760 --> 00:56:15,119
obviously

1637
00:56:12,799 --> 00:56:17,359
and they're all applying these tools to

1638
00:56:15,119 --> 00:56:18,400
physics which for them is relatively

1639
00:56:17,359 --> 00:56:21,119
simple

1640
00:56:18,400 --> 00:56:24,400
and i thought ah i got it we had to

1641
00:56:21,119 --> 00:56:26,799
build this basic set of platforms

1642
00:56:24,400 --> 00:56:28,240
they had to then plug it into their

1643
00:56:26,799 --> 00:56:30,160
model

1644
00:56:28,240 --> 00:56:31,920
another caltech example is that there is

1645
00:56:30,160 --> 00:56:34,720
a set of problems in chemistry which

1646
00:56:31,920 --> 00:56:36,799
involve relaxation of energy state to

1647
00:56:34,720 --> 00:56:39,119
find the lowest energy state that then

1648
00:56:36,799 --> 00:56:40,720
produces the right chemical reaction

1649
00:56:39,119 --> 00:56:43,200
well all of a sudden a bunch of graduate

1650
00:56:40,720 --> 00:56:45,280
students figured out you can use these

1651
00:56:43,200 --> 00:56:48,000
libraries to do it so all of a sudden

1652
00:56:45,280 --> 00:56:50,799
everyone at caltech was doing it

1653
00:56:48,000 --> 00:56:50,799
that's progress

1654
00:56:51,119 --> 00:56:54,160
like i said we're smarter

1655
00:56:54,319 --> 00:56:57,359
i'd like to maybe take a question from

1656
00:56:56,079 --> 00:56:59,760
the online

1657
00:56:57,359 --> 00:57:01,920
audience i have about 50 questions here

1658
00:56:59,760 --> 00:57:03,920
so i'm having a hard time picking one

1659
00:57:01,920 --> 00:57:06,480
two questions in four minutes i know i

1660
00:57:03,920 --> 00:57:08,160
know i was like oh gosh okay maybe we'll

1661
00:57:06,480 --> 00:57:10,400
just pick this one

1662
00:57:08,160 --> 00:57:12,079
for a long time ai has been regarded as

1663
00:57:10,400 --> 00:57:13,760
a collection of separate studies based

1664
00:57:12,079 --> 00:57:15,520
on entirely unrelated separate

1665
00:57:13,760 --> 00:57:19,200
principles i wonder if anything like a

1666
00:57:15,520 --> 00:57:19,200
general purpose ai is emerging

1667
00:57:21,280 --> 00:57:24,960
well i think you know what's happened as

1668
00:57:23,359 --> 00:57:27,200
much as i agree with eric that deep

1669
00:57:24,960 --> 00:57:29,200
learning is going to hit its uh

1670
00:57:27,200 --> 00:57:30,400
limits at some point here as everything

1671
00:57:29,200 --> 00:57:33,040
does

1672
00:57:30,400 --> 00:57:34,079
i think you know the extent to which

1673
00:57:33,040 --> 00:57:35,920
uh

1674
00:57:34,079 --> 00:57:37,599
deep learning

1675
00:57:35,920 --> 00:57:39,200
much of the application of which

1676
00:57:37,599 --> 00:57:40,480
actually started in computer vision in

1677
00:57:39,200 --> 00:57:43,839
my area

1678
00:57:40,480 --> 00:57:45,680
of research has uh you know developed

1679
00:57:43,839 --> 00:57:47,359
such that it's usable across a wide

1680
00:57:45,680 --> 00:57:49,119
variety of domains

1681
00:57:47,359 --> 00:57:50,720
if that if that's not the closest thing

1682
00:57:49,119 --> 00:57:52,960
to a common

1683
00:57:50,720 --> 00:57:55,839
approach i'm not sure what is so i think

1684
00:57:52,960 --> 00:57:57,200
you know in some ways many people in in

1685
00:57:55,839 --> 00:58:00,319
machine learning research they sort of

1686
00:57:57,200 --> 00:58:02,240
bemoan the fact that it feels very

1687
00:58:00,319 --> 00:58:04,160
sort of like everybody's

1688
00:58:02,240 --> 00:58:05,440
kind of developing the same algorithms

1689
00:58:04,160 --> 00:58:07,040
and then applying them to a lot of

1690
00:58:05,440 --> 00:58:08,720
different problems and

1691
00:58:07,040 --> 00:58:11,200
we've sort of lost the diversity of

1692
00:58:08,720 --> 00:58:13,520
various different ways of looking at

1693
00:58:11,200 --> 00:58:15,040
machine learning and ai problems

1694
00:58:13,520 --> 00:58:17,040
but again you know as eric was saying i

1695
00:58:15,040 --> 00:58:18,720
think these things go in cycles

1696
00:58:17,040 --> 00:58:20,799
uh and we'll hit an end and then

1697
00:58:18,720 --> 00:58:22,640
there'll be a time period where you know

1698
00:58:20,799 --> 00:58:25,520
there's a big diversity of approaches

1699
00:58:22,640 --> 00:58:28,000
again i mean 25 years ago people were

1700
00:58:25,520 --> 00:58:30,880
working on alternatives to cmos

1701
00:58:28,000 --> 00:58:33,920
and chips and then for many engineering

1702
00:58:30,880 --> 00:58:36,319
good engineering reasons cmos took over

1703
00:58:33,920 --> 00:58:38,079
there are still tiny pockets of people

1704
00:58:36,319 --> 00:58:40,319
who are not working on cmos in

1705
00:58:38,079 --> 00:58:42,160
universities somewhere in the world

1706
00:58:40,319 --> 00:58:45,040
but it sort of essentially eliminated

1707
00:58:42,160 --> 00:58:47,119
all variation for a while

1708
00:58:45,040 --> 00:58:49,599
and now as cmos begins to hit its

1709
00:58:47,119 --> 00:58:51,440
natural limits those people have an

1710
00:58:49,599 --> 00:58:52,720
opportunity again

1711
00:58:51,440 --> 00:58:54,319
my guess is that's what's going to

1712
00:58:52,720 --> 00:58:56,880
happen with deep learning

1713
00:58:54,319 --> 00:58:58,720
uh i remember 15 years ago looking this

1714
00:58:56,880 --> 00:59:00,240
is again before 2011

1715
00:58:58,720 --> 00:59:02,960
people would explain all these ai

1716
00:59:00,240 --> 00:59:04,640
theories and symbolic language analysis

1717
00:59:02,960 --> 00:59:07,040
and so forth and so on and i go okay

1718
00:59:04,640 --> 00:59:08,640
fine you know whatever

1719
00:59:07,040 --> 00:59:10,319
but

1720
00:59:08,640 --> 00:59:12,720
that has essentially been completely

1721
00:59:10,319 --> 00:59:13,680
usurped by these new technologies over a

1722
00:59:12,720 --> 00:59:15,440
decade

1723
00:59:13,680 --> 00:59:17,760
which is a relatively short i mean it

1724
00:59:15,440 --> 00:59:20,240
seems like a long time to you all as

1725
00:59:17,760 --> 00:59:23,040
students and faculty here but it's

1726
00:59:20,240 --> 00:59:26,319
actually a short time in human history

1727
00:59:23,040 --> 00:59:28,559
for such an extraordinary transformation

1728
00:59:26,319 --> 00:59:30,559
one more question i'm like

1729
00:59:28,559 --> 00:59:32,559
we want mit students to pursue

1730
00:59:30,559 --> 00:59:34,160
inventions and breakthroughs fearlessly

1731
00:59:32,559 --> 00:59:36,960
on the other hand we also hope they can

1732
00:59:34,160 --> 00:59:38,400
slow down ask why and think about social

1733
00:59:36,960 --> 00:59:40,799
and ethical responsibilities of

1734
00:59:38,400 --> 00:59:44,440
computing how do we strike a balance how

1735
00:59:40,799 --> 00:59:44,440
do we mentor them

1736
00:59:44,740 --> 00:59:50,000
[Laughter]

1737
00:59:47,920 --> 00:59:52,000
nice easy ending question

1738
00:59:50,000 --> 00:59:53,520
uh so i mean one thing that i think is

1739
00:59:52,000 --> 00:59:55,680
very important and actually this is

1740
00:59:53,520 --> 00:59:57,520
something that that that uh eric and

1741
00:59:55,680 --> 00:59:58,880
wendy schmidt have been very involved in

1742
00:59:57,520 --> 01:00:00,720
generally is

1743
00:59:58,880 --> 01:00:02,720
how do you train people in more than one

1744
01:00:00,720 --> 01:00:04,240
discipline right so the schmidt scholars

1745
01:00:02,720 --> 01:00:06,799
and the sciences is you know

1746
01:00:04,240 --> 01:00:08,640
postdoctoral scholars who do a postdoc

1747
01:00:06,799 --> 01:00:09,920
in a lab different from where they did

1748
01:00:08,640 --> 01:00:13,599
their phd

1749
01:00:09,920 --> 01:00:16,079
and i think that uh today to really

1750
01:00:13,599 --> 01:00:17,520
dig deeply into the social and ethical

1751
01:00:16,079 --> 01:00:19,839
questions around computing you need to

1752
01:00:17,520 --> 01:00:22,720
do something like that you need to say

1753
01:00:19,839 --> 01:00:25,760
you know i got my csphd i'm going to

1754
01:00:22,720 --> 01:00:27,280
spend a postdoc you know working in with

1755
01:00:25,760 --> 01:00:30,000
uh you know

1756
01:00:27,280 --> 01:00:31,920
in a scholarly fashion in

1757
01:00:30,000 --> 01:00:34,880
some discipline uh in the social

1758
01:00:31,920 --> 01:00:37,359
sciences or humanities um i think that

1759
01:00:34,880 --> 01:00:38,880
that right now we don't have that level

1760
01:00:37,359 --> 01:00:42,559
of expertise

1761
01:00:38,880 --> 01:00:44,079
without that kind of cross-training

1762
01:00:42,559 --> 01:00:46,400
we had one one questions from the

1763
01:00:44,079 --> 01:00:46,400
audience

1764
01:00:51,280 --> 01:00:55,119
hi um thank you so much for the content

1765
01:00:53,440 --> 01:00:56,960
so far i guess

1766
01:00:55,119 --> 01:00:59,839
it might be the last question today uh

1767
01:00:56,960 --> 01:01:01,599
so i'm gonna try and phrase it well um

1768
01:00:59,839 --> 01:01:03,200
i'm wondering

1769
01:01:01,599 --> 01:01:05,760
i guess maybe there's two parts to this

1770
01:01:03,200 --> 01:01:09,920
question one i'm wondering um how you

1771
01:01:05,760 --> 01:01:11,520
imagine an ideal human future and second

1772
01:01:09,920 --> 01:01:13,119
of all

1773
01:01:11,520 --> 01:01:15,520
what do you

1774
01:01:13,119 --> 01:01:16,799
what were your goals behind writing this

1775
01:01:15,520 --> 01:01:20,079
book

1776
01:01:16,799 --> 01:01:22,400
and how may or may not it relate to i

1777
01:01:20,079 --> 01:01:25,359
guess eric schmidt your work with the

1778
01:01:22,400 --> 01:01:27,760
national security sorry

1779
01:01:25,359 --> 01:01:29,760
ncsai sorry national security commission

1780
01:01:27,760 --> 01:01:30,960
on ai

1781
01:01:29,760 --> 01:01:33,200
um

1782
01:01:30,960 --> 01:01:36,160
i think the utopian future goes

1783
01:01:33,200 --> 01:01:36,160
something like this

1784
01:01:36,880 --> 01:01:42,079
humanity has spent so much of our time

1785
01:01:40,319 --> 01:01:43,920
dealing with basic needs and fighting

1786
01:01:42,079 --> 01:01:46,079
with each other so if you look at

1787
01:01:43,920 --> 01:01:48,720
history it's basically farming

1788
01:01:46,079 --> 01:01:51,520
the development of modern economics and

1789
01:01:48,720 --> 01:01:53,920
war right roughly speaking

1790
01:01:51,520 --> 01:01:57,039
and so you could imagine a situation

1791
01:01:53,920 --> 01:01:58,720
where the end stay this is when your son

1792
01:01:57,039 --> 01:02:01,200
is elderly

1793
01:01:58,720 --> 01:02:02,559
uh probably hard to imagine but it will

1794
01:02:01,200 --> 01:02:03,599
it will happen

1795
01:02:02,559 --> 01:02:06,319
um

1796
01:02:03,599 --> 01:02:09,760
that you can imagine this scenario that

1797
01:02:06,319 --> 01:02:11,359
the the needs of the average person are

1798
01:02:09,760 --> 01:02:13,440
largely met

1799
01:02:11,359 --> 01:02:14,880
through improvements in food and

1800
01:02:13,440 --> 01:02:16,720
material science

1801
01:02:14,880 --> 01:02:18,799
housing is inexpensive at least a

1802
01:02:16,720 --> 01:02:20,240
construction perspective because of all

1803
01:02:18,799 --> 01:02:22,799
of these developments that are going on

1804
01:02:20,240 --> 01:02:25,440
here at mit and elsewhere

1805
01:02:22,799 --> 01:02:26,960
and that people are free to

1806
01:02:25,440 --> 01:02:28,559
to pursue more things that are

1807
01:02:26,960 --> 01:02:30,319
interesting to them

1808
01:02:28,559 --> 01:02:32,240
and that the other thing that happens

1809
01:02:30,319 --> 01:02:34,640
with ai is we learn how to educate

1810
01:02:32,240 --> 01:02:36,400
people

1811
01:02:34,640 --> 01:02:37,920
i would argue that the that we haven't

1812
01:02:36,400 --> 01:02:40,799
talked about the impact of ai on

1813
01:02:37,920 --> 01:02:43,119
education because we haven't seen it yet

1814
01:02:40,799 --> 01:02:44,960
but it makes perfect sense to me that ai

1815
01:02:43,119 --> 01:02:46,480
systems should be able to figure out how

1816
01:02:44,960 --> 01:02:49,359
you learn

1817
01:02:46,480 --> 01:02:50,880
and and allow you to learn in the most

1818
01:02:49,359 --> 01:02:52,319
efficient way

1819
01:02:50,880 --> 01:02:54,160
and that should be true for every single

1820
01:02:52,319 --> 01:02:56,000
human being on the planet and it should

1821
01:02:54,160 --> 01:02:58,000
be free

1822
01:02:56,000 --> 01:03:01,119
i defy you to argue that that's a bad

1823
01:02:58,000 --> 01:03:03,599
thing right it's got to be good

1824
01:03:01,119 --> 01:03:06,799
right for humanity

1825
01:03:03,599 --> 01:03:08,400
so in that that sort of utopian view

1826
01:03:06,799 --> 01:03:09,680
we'll still fight because we like to

1827
01:03:08,400 --> 01:03:11,839
fight over things but we'll fight over

1828
01:03:09,680 --> 01:03:13,920
things which aren't dangerous

1829
01:03:11,839 --> 01:03:14,960
and we aren't harming people and things

1830
01:03:13,920 --> 01:03:17,119
like that

1831
01:03:14,960 --> 01:03:18,720
and that's a really good way to

1832
01:03:17,119 --> 01:03:20,400
understand this

1833
01:03:18,720 --> 01:03:22,160
the the

1834
01:03:20,400 --> 01:03:23,920
the work of the book

1835
01:03:22,160 --> 01:03:25,039
essentially paralleled the work of the

1836
01:03:23,920 --> 01:03:26,400
commission

1837
01:03:25,039 --> 01:03:27,680
because in the commission we talked

1838
01:03:26,400 --> 01:03:30,400
about

1839
01:03:27,680 --> 01:03:31,599
basically how did how to make sure that

1840
01:03:30,400 --> 01:03:33,200
ai

1841
01:03:31,599 --> 01:03:35,039
was part of our national security

1842
01:03:33,200 --> 01:03:36,960
strategy in the united states

1843
01:03:35,039 --> 01:03:39,599
we made lots of recommendations most of

1844
01:03:36,960 --> 01:03:41,599
which involve building talent

1845
01:03:39,599 --> 01:03:43,280
the government is slow to

1846
01:03:41,599 --> 01:03:45,200
there's this huge gap between the talent

1847
01:03:43,280 --> 01:03:48,799
here and the talent and the government

1848
01:03:45,200 --> 01:03:52,079
daniela who who is here

1849
01:03:48,799 --> 01:03:54,319
runs a whole program with the military

1850
01:03:52,079 --> 01:03:55,839
there's an air force group at lincoln

1851
01:03:54,319 --> 01:03:57,520
labs and so forth there's lots of really

1852
01:03:55,839 --> 01:03:59,039
smart people who are working on it but

1853
01:03:57,520 --> 01:04:00,799
there are

1854
01:03:59,039 --> 01:04:02,640
immigration people sending into the

1855
01:04:00,799 --> 01:04:05,920
military to bring their understanding of

1856
01:04:02,640 --> 01:04:05,920
ai up to this new standard

1857
01:04:06,319 --> 01:04:10,480
i guess maybe i'll get the last word

1858
01:04:08,960 --> 01:04:11,599
so to me

1859
01:04:10,480 --> 01:04:12,960
the book

1860
01:04:11,599 --> 01:04:14,880
so first of all i totally agree with

1861
01:04:12,960 --> 01:04:17,039
what eric said and i'm a terrible

1862
01:04:14,880 --> 01:04:18,720
futurist so it's good to have work with

1863
01:04:17,039 --> 01:04:19,760
somebody who can see the future better

1864
01:04:18,720 --> 01:04:22,559
than i can

1865
01:04:19,760 --> 01:04:24,880
uh but um but i think uh to your

1866
01:04:22,559 --> 01:04:27,200
question of the objective of the book

1867
01:04:24,880 --> 01:04:29,359
uh it really was to get people thinking

1868
01:04:27,200 --> 01:04:32,160
about ai in broader terms

1869
01:04:29,359 --> 01:04:35,280
i think that people come at it either

1870
01:04:32,160 --> 01:04:37,039
with a utopian view or a dystopian view

1871
01:04:35,280 --> 01:04:39,520
but the reality is going to be somewhere

1872
01:04:37,039 --> 01:04:42,000
in between people come at it with either

1873
01:04:39,520 --> 01:04:44,880
a technological understanding or a lack

1874
01:04:42,000 --> 01:04:47,119
of technological understanding uh and so

1875
01:04:44,880 --> 01:04:49,680
the idea was really to try to set these

1876
01:04:47,119 --> 01:04:51,680
changes in the context of hundreds of

1877
01:04:49,680 --> 01:04:53,920
years of humanity

1878
01:04:51,680 --> 01:04:55,200
and provide that as framing to have more

1879
01:04:53,920 --> 01:04:57,200
discussion of the kind that we've been

1880
01:04:55,200 --> 01:04:59,200
having here today and i hope that all of

1881
01:04:57,200 --> 01:05:00,160
you will continue to do that it's really

1882
01:04:59,200 --> 01:05:02,079
important

1883
01:05:00,160 --> 01:05:03,599
and your and your generation

1884
01:05:02,079 --> 01:05:05,039
is the one that's going to do that more

1885
01:05:03,599 --> 01:05:07,920
than ours

1886
01:05:05,039 --> 01:05:09,680
and let me uh let me finish by saying

1887
01:05:07,920 --> 01:05:11,039
that um

1888
01:05:09,680 --> 01:05:13,119
it's a privilege to be here it's a

1889
01:05:11,039 --> 01:05:14,799
privilege to be

1890
01:05:13,119 --> 01:05:16,960
crazily they made me a

1891
01:05:14,799 --> 01:05:20,160
a visiting professor here for which i'm

1892
01:05:16,960 --> 01:05:20,960
ill-suited but i'm very grateful

1893
01:05:20,160 --> 01:05:22,640
and

1894
01:05:20,960 --> 01:05:24,960
the quality of the talent in this

1895
01:05:22,640 --> 01:05:26,880
institution is so great that you should

1896
01:05:24,960 --> 01:05:29,359
be able to win

1897
01:05:26,880 --> 01:05:32,240
so what i need you to do is i need you

1898
01:05:29,359 --> 01:05:34,079
to work harder work faster take

1899
01:05:32,240 --> 01:05:36,640
advantage of this these things don't

1900
01:05:34,079 --> 01:05:39,520
occur that often in life this is this

1901
01:05:36,640 --> 01:05:43,200
period where this enormous

1902
01:05:39,520 --> 01:05:45,520
cambrian explosion of ideas

1903
01:05:43,200 --> 01:05:46,799
there are going to be enormous winners

1904
01:05:45,520 --> 01:05:49,119
in this

1905
01:05:46,799 --> 01:05:51,200
and you all to the degree that you can

1906
01:05:49,119 --> 01:05:53,280
shape them in the values that we're

1907
01:05:51,200 --> 01:05:55,839
trying to represent here

1908
01:05:53,280 --> 01:05:57,359
are really crucial small teams can have

1909
01:05:55,839 --> 01:05:59,520
an outsized impact we've seen this in

1910
01:05:57,359 --> 01:06:02,079
many many areas

1911
01:05:59,520 --> 01:06:03,520
and i would suggest some things to to to

1912
01:06:02,079 --> 01:06:05,440
work on i think i mentioned the

1913
01:06:03,520 --> 01:06:08,000
education one

1914
01:06:05,440 --> 01:06:09,520
um somebody's gonna break through this

1915
01:06:08,000 --> 01:06:10,799
there's an uh there's a school of

1916
01:06:09,520 --> 01:06:12,400
education here there's a bunch of people

1917
01:06:10,799 --> 01:06:14,000
working on it here at mit that i've been

1918
01:06:12,400 --> 01:06:15,760
working with somebody's gonna figure out

1919
01:06:14,000 --> 01:06:17,520
a way to do this at scale that will have

1920
01:06:15,760 --> 01:06:19,440
a huge impact

1921
01:06:17,520 --> 01:06:21,039
right

1922
01:06:19,440 --> 01:06:22,559
working on underlying algorithms

1923
01:06:21,039 --> 01:06:24,319
figuring out where to generalize these

1924
01:06:22,559 --> 01:06:26,400
algorithms so they apply across all

1925
01:06:24,319 --> 01:06:28,559
these fields and then applying them in

1926
01:06:26,400 --> 01:06:30,799
each field in a clever way

1927
01:06:28,559 --> 01:06:33,440
to really deliver breakthroughs and this

1928
01:06:30,799 --> 01:06:35,200
means you know and when i talk to normal

1929
01:06:33,440 --> 01:06:36,640
people as opposed to tech people

1930
01:06:35,200 --> 01:06:38,160
a number of people said i just can't

1931
01:06:36,640 --> 01:06:40,160
take it there's so much going on and i

1932
01:06:38,160 --> 01:06:41,680
said you haven't seen anything yet we're

1933
01:06:40,160 --> 01:06:44,960
at the beginning

1934
01:06:41,680 --> 01:06:47,760
of the power of these tools to transform

1935
01:06:44,960 --> 01:06:50,000
science and business and society but

1936
01:06:47,760 --> 01:06:52,319
it's up to us how that happens and

1937
01:06:50,000 --> 01:06:53,760
frankly it's up to you how this happens

1938
01:06:52,319 --> 01:06:55,839
and that's why it's such a privilege to

1939
01:06:53,760 --> 01:06:57,359
work with you and you

1940
01:06:55,839 --> 01:07:00,160
um and the other teams that i've had a

1941
01:06:57,359 --> 01:07:02,480
chance to work with here at mit

1942
01:07:00,160 --> 01:07:04,720
thank you very much i have much more to

1943
01:07:02,480 --> 01:07:06,880
ask and talk about that we're at the top

1944
01:07:04,720 --> 01:07:08,720
of the hour so thanks very much for the

1945
01:07:06,880 --> 01:07:10,079
fantastic conversation thanks everyone

1946
01:07:08,720 --> 01:07:12,799
for joining us

1947
01:07:10,079 --> 01:07:14,170
and stay healthy more than anything and

1948
01:07:12,799 --> 01:07:20,910
thank you thank you

1949
01:07:14,170 --> 01:07:20,910
[Applause]

