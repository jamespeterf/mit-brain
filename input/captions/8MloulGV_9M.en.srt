1
00:00:00,000 --> 00:00:01,190

2
00:00:01,190 --> 00:00:02,190
Hi, everyone.

3
00:00:02,190 --> 00:00:03,030
My name is Jacob.

4
00:00:03,030 --> 00:00:05,090
I'm a professor here in
electrical engineering

5
00:00:05,090 --> 00:00:06,548
and computer science,
and I'm going

6
00:00:06,548 --> 00:00:09,620
to be kicking off our session
on the future of large language

7
00:00:09,620 --> 00:00:11,240
models.

8
00:00:11,240 --> 00:00:14,630
So suppose you go
today to the best

9
00:00:14,630 --> 00:00:17,330
LLM you can get your hands on,
and you ask it the following

10
00:00:17,330 --> 00:00:20,540
question-- was S.
Chellapandian first appointed

11
00:00:20,540 --> 00:00:24,150
as speaker of the Madras
legislative assembly in 1952?

12
00:00:24,150 --> 00:00:27,090
Give both an answer and a
numeric-- or sorry, '62--

13
00:00:27,090 --> 00:00:30,300
an answer and a numerical
estimate of your confidence.

14
00:00:30,300 --> 00:00:33,570
And if you do this with GPT-5,
a state of the art model,

15
00:00:33,570 --> 00:00:36,470
it will answer in
the following way.

16
00:00:36,470 --> 00:00:40,260
It says, yes, and it says,
it's yes with 64% confidence.

17
00:00:40,260 --> 00:00:43,130
Now, this answer is
remarkable for a couple

18
00:00:43,130 --> 00:00:44,280
of different reasons.

19
00:00:44,280 --> 00:00:45,510
First of all, it's right.

20
00:00:45,510 --> 00:00:47,135
And this is an answer
that was produced

21
00:00:47,135 --> 00:00:48,360
without using web search.

22
00:00:48,360 --> 00:00:50,180
And so the kinds
of language models

23
00:00:50,180 --> 00:00:53,360
that we have access to today
can answer questions about math.

24
00:00:53,360 --> 00:00:55,110
They can answer questions
about chemistry.

25
00:00:55,110 --> 00:00:58,400
They can answer trivia questions
about mid-century South Indian

26
00:00:58,400 --> 00:00:59,800
politics.

27
00:00:59,800 --> 00:01:01,750
The second thing
to notice here is

28
00:01:01,750 --> 00:01:05,560
that this model is performing
a relatively complex task.

29
00:01:05,560 --> 00:01:08,120
It has not just given us a
factual answer to this question,

30
00:01:08,120 --> 00:01:10,150
but also some sort of
numerical confidence

31
00:01:10,150 --> 00:01:11,888
estimate like we
asked for by giving

32
00:01:11,888 --> 00:01:13,930
the model natural language
instructions about how

33
00:01:13,930 --> 00:01:15,640
we wanted it to respond.

34
00:01:15,640 --> 00:01:17,830
These kinds of
question-answering tasks,

35
00:01:17,830 --> 00:01:20,450
where you have to give also
a numerical confidence score,

36
00:01:20,450 --> 00:01:22,667
are not super frequent
in the training data.

37
00:01:22,667 --> 00:01:24,250
And so this model
has actually learned

38
00:01:24,250 --> 00:01:26,620
to follow our instructions
and not just regurgitate

39
00:01:26,620 --> 00:01:28,660
factual knowledge
about the world

40
00:01:28,660 --> 00:01:32,380
and has given us a
sensible-looking answer here.

41
00:01:32,380 --> 00:01:35,770
However, if we start to
probe this model's knowledge

42
00:01:35,770 --> 00:01:38,230
by asking it related
questions and seeing

43
00:01:38,230 --> 00:01:41,170
how it responds to those, the
picture gets a little bit more

44
00:01:41,170 --> 00:01:42,280
complicated.

45
00:01:42,280 --> 00:01:46,120
For example, if we just change
the year from 1962 to 1937

46
00:01:46,120 --> 00:01:50,180
and ask the same question,
the model now also says yes,

47
00:01:50,180 --> 00:01:53,850
and it is even more confident in
this answer than it was before.

48
00:01:53,850 --> 00:01:57,580

49
00:01:57,580 --> 00:01:59,600
So what's happened here?

50
00:01:59,600 --> 00:02:01,810
And I want to claim that
two different things have

51
00:02:01,810 --> 00:02:03,250
gone wrong.

52
00:02:03,250 --> 00:02:05,720
The first is a
factual inconsistency.

53
00:02:05,720 --> 00:02:08,013
Obviously, if the answer to
the first question was yes,

54
00:02:08,013 --> 00:02:09,430
then that answer
was correct, then

55
00:02:09,430 --> 00:02:11,660
the answer to the second
question has to be no.

56
00:02:11,660 --> 00:02:14,860
And even if we apply
the training procedures

57
00:02:14,860 --> 00:02:19,440
that we use today to a first
approximation, the entire corpus

58
00:02:19,440 --> 00:02:21,190
of written text that
humanity has produced

59
00:02:21,190 --> 00:02:24,070
since the invention of
writing, training models

60
00:02:24,070 --> 00:02:25,612
on all of that data
still causes them

61
00:02:25,612 --> 00:02:27,987
to make the kinds of mistakes
that we're looking at here.

62
00:02:27,987 --> 00:02:30,290
And this is obviously
something that we-- excuse me--

63
00:02:30,290 --> 00:02:31,870
that we would like to fix.

64
00:02:31,870 --> 00:02:35,060
Now, the second error is
a little bit more subtle.

65
00:02:35,060 --> 00:02:36,850
And it's something
that we might think of

66
00:02:36,850 --> 00:02:39,950
or might call even an
error of metacognition.

67
00:02:39,950 --> 00:02:42,730
Whether or not this model
is right about its answer

68
00:02:42,730 --> 00:02:46,750
to the first question, if it
thinks the answer that it gave,

69
00:02:46,750 --> 00:02:49,550
the answer yes, is more
likely than not to be true,

70
00:02:49,550 --> 00:02:51,910
then it shouldn't also
assign very high confidence

71
00:02:51,910 --> 00:02:53,830
to other answers to
other questions that

72
00:02:53,830 --> 00:02:55,600
blatantly contradict
the thing that it

73
00:02:55,600 --> 00:02:58,970
said the first time around.

74
00:02:58,970 --> 00:03:02,210
Now, to put this another
way, we have, on one hand,

75
00:03:02,210 --> 00:03:03,890
errors of factual
consistency that

76
00:03:03,890 --> 00:03:06,200
reflect the fact that this
language model has somehow

77
00:03:06,200 --> 00:03:07,850
failed to accurately
model the world

78
00:03:07,850 --> 00:03:12,050
and know what statements are
true or false in the world.

79
00:03:12,050 --> 00:03:14,867
But these inconsistent
confidence scores

80
00:03:14,867 --> 00:03:17,450
reflect, instead, the fact that
the language model has somehow

81
00:03:17,450 --> 00:03:19,075
failed to model
itself, that it doesn't

82
00:03:19,075 --> 00:03:21,617
know what things it knows, and
it doesn't know what things it

83
00:03:21,617 --> 00:03:23,750
doesn't, and it doesn't
know, when it's answering

84
00:03:23,750 --> 00:03:25,640
one question, how
it's going to respond

85
00:03:25,640 --> 00:03:28,160
to other related questions,
and whether those responses are

86
00:03:28,160 --> 00:03:31,280
actually going to be
consistent or not.

87
00:03:31,280 --> 00:03:35,000
Now, the question of how and
indeed whether language models

88
00:03:35,000 --> 00:03:37,340
actually build internal
models of the world

89
00:03:37,340 --> 00:03:38,972
and how they
represent themselves

90
00:03:38,972 --> 00:03:41,430
have gotten a lot of attention
in the last couple of years.

91
00:03:41,430 --> 00:03:43,490
These are all, I think,
newspaper articles

92
00:03:43,490 --> 00:03:46,250
that were published in the last
couple of months about work

93
00:03:46,250 --> 00:03:48,588
happening at MIT from a
bunch of different groups

94
00:03:48,588 --> 00:03:50,630
on this question of whether
language models build

95
00:03:50,630 --> 00:03:51,330
world models.

96
00:03:51,330 --> 00:03:53,270
And right now, there's
a lot of disagreement

97
00:03:53,270 --> 00:03:55,220
in the community
about what it even

98
00:03:55,220 --> 00:03:57,680
means to be or to
possess a world model

99
00:03:57,680 --> 00:03:59,180
and what sorts of
features we should

100
00:03:59,180 --> 00:04:03,290
expect a model of
the world to possess.

101
00:04:03,290 --> 00:04:05,060
But if we think about--

102
00:04:05,060 --> 00:04:07,430
or if we think by analogy
to the kinds of models

103
00:04:07,430 --> 00:04:10,190
that humans build of the
world or of the systems

104
00:04:10,190 --> 00:04:13,460
that we care about, one
important feature that they have

105
00:04:13,460 --> 00:04:17,480
is that accuracy is actually
not often the most important

106
00:04:17,480 --> 00:04:18,720
consideration.

107
00:04:18,720 --> 00:04:21,570
A model, almost by
definition, is an abstraction.

108
00:04:21,570 --> 00:04:24,330
It discards information about
the system that's being modeled.

109
00:04:24,330 --> 00:04:28,760
It's useful for answering only
a limited set of questions.

110
00:04:28,760 --> 00:04:36,140
But what the models that people
build do possess is coherence.

111
00:04:36,140 --> 00:04:38,820
If you ask them the same
question in multiple ways,

112
00:04:38,820 --> 00:04:40,650
they'll generally give
you the same answer,

113
00:04:40,650 --> 00:04:43,275
as long as it's a kind of answer
that they know how to compute.

114
00:04:43,275 --> 00:04:46,050
And really, throughout
the history of science,

115
00:04:46,050 --> 00:04:50,100
building simple and internally
coherent models of the world,

116
00:04:50,100 --> 00:04:52,647
even if they don't yet
explain everything,

117
00:04:52,647 --> 00:04:54,480
is historically how
we've been able to build

118
00:04:54,480 --> 00:04:57,090
accurate scientific theories
and get accurate predictions

119
00:04:57,090 --> 00:04:59,280
and produce new knowledge.

120
00:04:59,280 --> 00:05:02,910
So what if instead of simply
optimizing language models

121
00:05:02,910 --> 00:05:04,950
for accuracy the
way we do today, we

122
00:05:04,950 --> 00:05:07,830
tried to explicitly train
them to be coherent in the way

123
00:05:07,830 --> 00:05:10,608
that human-built models
tend to be coherent?

124
00:05:10,608 --> 00:05:12,900
So what I want to do next is
show a couple of vignettes

125
00:05:12,900 --> 00:05:14,760
from research happening
at MIT showing

126
00:05:14,760 --> 00:05:16,100
what this might look like.

127
00:05:16,100 --> 00:05:18,840

128
00:05:18,840 --> 00:05:22,452
So example number one here, with
respect to modeling the world,

129
00:05:22,452 --> 00:05:24,660
is that we're going to give
a very simple and totally

130
00:05:24,660 --> 00:05:26,077
unsupervised
procedure that we can

131
00:05:26,077 --> 00:05:29,753
use to optimize models for
internal factual consistency.

132
00:05:29,753 --> 00:05:31,170
This is work that
was led by Feyza

133
00:05:31,170 --> 00:05:34,060
AkyÃ¼rek, along with some
collaborators at BU.

134
00:05:34,060 --> 00:05:36,280
And the basic procedure
is super simple.

135
00:05:36,280 --> 00:05:38,040
We're going to pick
some topic, and we're

136
00:05:38,040 --> 00:05:40,110
going to prompt the language
model to simply write down

137
00:05:40,110 --> 00:05:41,950
everything that it
knows about that topic,

138
00:05:41,950 --> 00:05:44,075
whether or not all of the
stuff that it writes down

139
00:05:44,075 --> 00:05:45,940
is internally consistent.

140
00:05:45,940 --> 00:05:48,288
And once we've done
all of this, we're

141
00:05:48,288 --> 00:05:49,830
simply going to use
the same language

142
00:05:49,830 --> 00:05:52,720
model to look at the set of
facts that got written down,

143
00:05:52,720 --> 00:05:56,110
reason about them globally,
figure out which ones contradict

144
00:05:56,110 --> 00:05:57,850
each other, which
ones entail or provide

145
00:05:57,850 --> 00:05:59,660
supporting evidence
for each other,

146
00:05:59,660 --> 00:06:01,750
and ultimately, identify
the subset of facts that

147
00:06:01,750 --> 00:06:04,300
are both internally
coherent and most likely

148
00:06:04,300 --> 00:06:06,970
on the margin to
actually be true.

149
00:06:06,970 --> 00:06:10,670
And now, really importantly,
this is a very simple task.

150
00:06:10,670 --> 00:06:12,160
It's a much easier
task for the LLM

151
00:06:12,160 --> 00:06:14,160
to perform than generating
the factual knowledge

152
00:06:14,160 --> 00:06:15,200
in the first place.

153
00:06:15,200 --> 00:06:18,640
Even relatively small,
relatively simple models

154
00:06:18,640 --> 00:06:20,950
do a very good job of looking
at a pair of sentences

155
00:06:20,950 --> 00:06:23,260
or a pair of paragraphs and
figuring out whether they

156
00:06:23,260 --> 00:06:24,435
contradict each other.

157
00:06:24,435 --> 00:06:25,810
And this is
essentially all we're

158
00:06:25,810 --> 00:06:27,727
asking the model to do
during the second stage

159
00:06:27,727 --> 00:06:29,680
of this procedure.

160
00:06:29,680 --> 00:06:32,227
But once we've identified
this good subset,

161
00:06:32,227 --> 00:06:34,310
this set of statements
that are likely to be true,

162
00:06:34,310 --> 00:06:36,352
we're simply going to
retrain the language model,

163
00:06:36,352 --> 00:06:39,040
now on its own output, to
assign a higher probability

164
00:06:39,040 --> 00:06:43,030
to that set of statements,
documents, whatever that were

165
00:06:43,030 --> 00:06:45,920
judged upon reflection to be
more likely to be true than not

166
00:06:45,920 --> 00:06:48,730
and to assign lower probability
to those statements that

167
00:06:48,730 --> 00:06:51,680
were judged to be false.

168
00:06:51,680 --> 00:06:54,110
Now, pretty remarkably,
when we take this model,

169
00:06:54,110 --> 00:06:56,970
and we evaluate it now on
some fact checking tasks,

170
00:06:56,970 --> 00:06:58,618
this totally
unsupervised procedure,

171
00:06:58,618 --> 00:07:00,410
which doesn't get any
new factual knowledge

172
00:07:00,410 --> 00:07:02,150
about the world
and just optimizes

173
00:07:02,150 --> 00:07:04,190
for internal
consistency, actually

174
00:07:04,190 --> 00:07:06,470
dramatically improves
these models' performance

175
00:07:06,470 --> 00:07:09,055
when we evaluate them on things
like fact-checking tasks.

176
00:07:09,055 --> 00:07:12,380

177
00:07:12,380 --> 00:07:14,190
So that's models of the world.

178
00:07:14,190 --> 00:07:16,700
What about models of the
self, models of the language

179
00:07:16,700 --> 00:07:18,170
model itself?

180
00:07:18,170 --> 00:07:19,610
Well, if we think
about what went

181
00:07:19,610 --> 00:07:23,390
wrong in this initial example
that we started the talk with,

182
00:07:23,390 --> 00:07:26,280
it was that the model's
confidence in its own outputs,

183
00:07:26,280 --> 00:07:29,097
whether right or
wrong, was too high.

184
00:07:29,097 --> 00:07:30,680
And it turns out
that this is actually

185
00:07:30,680 --> 00:07:33,180
an expected outcome from the
kinds of reinforcement learning

186
00:07:33,180 --> 00:07:34,950
procedures that people
use for training,

187
00:07:34,950 --> 00:07:37,500
sort of reasoning behaviors
in models that we have today.

188
00:07:37,500 --> 00:07:39,630
So what can we do instead?

189
00:07:39,630 --> 00:07:42,350
Well, here-- and this is work
that was led by Mehul Damani

190
00:07:42,350 --> 00:07:44,780
and Isha Puri in collaboration
with some folks from

191
00:07:44,780 --> 00:07:45,390
Yoon's group--

192
00:07:45,390 --> 00:07:46,760
Yoon is going to talk next--

193
00:07:46,760 --> 00:07:48,998
we apply some very old
ideas from decision theory

194
00:07:48,998 --> 00:07:51,540
and just change the function
that we use for scoring language

195
00:07:51,540 --> 00:07:53,850
model outputs so
that we continue

196
00:07:53,850 --> 00:07:56,640
to reward language models
when they produce answers that

197
00:07:56,640 --> 00:07:59,160
are right and express
high confidence,

198
00:07:59,160 --> 00:08:01,380
we penalize them when
they're right, but express

199
00:08:01,380 --> 00:08:03,370
low confidence in
those right answers,

200
00:08:03,370 --> 00:08:06,120
and we reward them more than
we might have done otherwise

201
00:08:06,120 --> 00:08:08,670
when they're wrong, but
also express low confidence

202
00:08:08,670 --> 00:08:10,770
in those wrong answers.

203
00:08:10,770 --> 00:08:12,850
And models trained
in this way, again,

204
00:08:12,850 --> 00:08:15,745
with no data beyond
what we would ordinarily

205
00:08:15,745 --> 00:08:17,370
need to just teach
them all these facts

206
00:08:17,370 --> 00:08:19,610
or skills in the first
place, become much, much

207
00:08:19,610 --> 00:08:21,360
more accurate at
assessing the correctness

208
00:08:21,360 --> 00:08:22,930
of their own answers.

209
00:08:22,930 --> 00:08:24,570
And maybe most
surprisingly, this

210
00:08:24,570 --> 00:08:26,530
is a skill that
actually generalizes.

211
00:08:26,530 --> 00:08:28,890
So when we take these models,
and we now evaluate them

212
00:08:28,890 --> 00:08:30,598
on other problems
different from the ones

213
00:08:30,598 --> 00:08:33,120
that we used to train this
self-reflection behavior

214
00:08:33,120 --> 00:08:35,580
in the first place,
these models continue

215
00:08:35,580 --> 00:08:38,200
to be both more accurate
and better calibrated,

216
00:08:38,200 --> 00:08:40,679
better at knowing when
they're right or wrong

217
00:08:40,679 --> 00:08:43,110
and knowing what they know,
than the kinds of models

218
00:08:43,110 --> 00:08:47,130
that we're able to produce using
standard training techniques.

219
00:08:47,130 --> 00:08:49,330
So where does this leave us?

220
00:08:49,330 --> 00:08:53,380
Taking a step back, research
on language modeling really

221
00:08:53,380 --> 00:08:56,530
for the last decade, and maybe
even going back all the way

222
00:08:56,530 --> 00:08:58,780
to the first language models
that people were training

223
00:08:58,780 --> 00:09:01,570
in the '50s, has been almost
single-mindedly focused

224
00:09:01,570 --> 00:09:04,068
on accuracy, building models
that are really good at giving

225
00:09:04,068 --> 00:09:06,610
the right answer to a question
or guessing what word is going

226
00:09:06,610 --> 00:09:08,080
to come next in a sentence.

227
00:09:08,080 --> 00:09:11,470
But there are many properties
that we might want a model

228
00:09:11,470 --> 00:09:13,780
to have beyond just accuracy.

229
00:09:13,780 --> 00:09:16,220
And coherence, as we've sort
of defined it in this talk,

230
00:09:16,220 --> 00:09:17,300
is only one of them.

231
00:09:17,300 --> 00:09:19,240
I think many of the most
interesting and most important

232
00:09:19,240 --> 00:09:21,240
ones have to do with
thinking about interactions

233
00:09:21,240 --> 00:09:23,360
between language models
and their human users,

234
00:09:23,360 --> 00:09:26,930
and explicitly optimizing not
just for predictive accuracy,

235
00:09:26,930 --> 00:09:30,200
but for good outcomes in
human-AI interactions.

236
00:09:30,200 --> 00:09:32,240
And so when we think
about the future of LLMs,

237
00:09:32,240 --> 00:09:34,030
I think the first step is
really thinking much more

238
00:09:34,030 --> 00:09:35,620
closely about what
we mean when we

239
00:09:35,620 --> 00:09:37,303
say we want these
models to be better,

240
00:09:37,303 --> 00:09:39,220
coming up with a more
general notion of things

241
00:09:39,220 --> 00:09:41,260
that we ought to be
optimizing beyond accuracy

242
00:09:41,260 --> 00:09:43,720
at next token prediction
or question-answering tasks

243
00:09:43,720 --> 00:09:45,250
or whatever, and
figuring out a way

244
00:09:45,250 --> 00:09:49,010
to optimize for these especially
more human-centric objectives,

245
00:09:49,010 --> 00:09:50,120
instead.

246
00:09:50,120 --> 00:09:51,750
And with that, I
will stop there.

247
00:09:51,750 --> 00:09:52,620
Thanks very much.

248
00:09:52,620 --> 00:09:54,470
[APPLAUSE]

249
00:09:54,470 --> 00:09:54,980
Hi.

250
00:09:54,980 --> 00:09:55,560
Welcome.

251
00:09:55,560 --> 00:09:56,393
My name is Yoon Kim.

252
00:09:56,393 --> 00:09:57,690
I'm a professor at MIT.

253
00:09:57,690 --> 00:10:00,117
I'm working on NLP
and machine learning,

254
00:10:00,117 --> 00:10:02,450
and today I'll be talking to
you about the future of LLM

255
00:10:02,450 --> 00:10:03,860
architectures.

256
00:10:03,860 --> 00:10:05,990
So I think it's
actually worth thinking

257
00:10:05,990 --> 00:10:08,960
about the current,
maybe, AGI recipe that

258
00:10:08,960 --> 00:10:13,890
seems to be driving progress
in the AI research program.

259
00:10:13,890 --> 00:10:16,560
And generally, the recipe
seems to be as follows.

260
00:10:16,560 --> 00:10:21,680
We have data, and then we have
some optimization objective,

261
00:10:21,680 --> 00:10:24,620
and we have a model that's
trained on the data using

262
00:10:24,620 --> 00:10:26,340
this optimization objective.

263
00:10:26,340 --> 00:10:28,800
When it comes to
large language models,

264
00:10:28,800 --> 00:10:32,460
the data is given by
trillions of words of text,

265
00:10:32,460 --> 00:10:34,560
maybe a civilization's
worth of text.

266
00:10:34,560 --> 00:10:38,498
The optimization objective
is essentially a compression.

267
00:10:38,498 --> 00:10:40,790
So it's maximum likelihood
learning, which is basically

268
00:10:40,790 --> 00:10:42,860
compressing the data.

269
00:10:42,860 --> 00:10:45,620
And then we're
using a architecture

270
00:10:45,620 --> 00:10:48,960
called the transformer
to actually perform

271
00:10:48,960 --> 00:10:50,680
this optimization procedure.

272
00:10:50,680 --> 00:10:53,880
And of course, the
rationale is that maybe

273
00:10:53,880 --> 00:10:56,350
through this
optimization procedure,

274
00:10:56,350 --> 00:10:59,580
we're going to have the
architecture of the model

275
00:10:59,580 --> 00:11:02,790
learn the intelligences
that essentially produce

276
00:11:02,790 --> 00:11:06,180
the data, which is the external
world and all the humans that

277
00:11:06,180 --> 00:11:09,000
produce the text on which
the model is trained.

278
00:11:09,000 --> 00:11:11,092
And this is roughly
pre-training.

279
00:11:11,092 --> 00:11:12,550
There are other
bells and whistles.

280
00:11:12,550 --> 00:11:15,660
For example, we often have
reinforcement learning

281
00:11:15,660 --> 00:11:19,050
on top of this to surface
the reasoning capabilities

282
00:11:19,050 --> 00:11:21,150
or have it do what we
want it to do, i.e.

283
00:11:21,150 --> 00:11:22,900
become useful assistants.

284
00:11:22,900 --> 00:11:26,760
And then we also have
search, which is sometimes

285
00:11:26,760 --> 00:11:29,350
called test-time scaling,
where, given this model,

286
00:11:29,350 --> 00:11:32,310
we want to search in
the space of its outputs

287
00:11:32,310 --> 00:11:35,970
to find the outputs that
produce the right answer.

288
00:11:35,970 --> 00:11:39,540
And maybe there's a sense
in which this is enough.

289
00:11:39,540 --> 00:11:42,870
So in some circles, we just
want bigger and bigger models

290
00:11:42,870 --> 00:11:44,650
on larger and larger data sets.

291
00:11:44,650 --> 00:11:49,350
And then, if we do this,
we're going to get to utopia.

292
00:11:49,350 --> 00:11:53,700
So we're going to have Einsteins
on GPUs, world-class biologists

293
00:11:53,700 --> 00:11:55,000
solving problems for us.

294
00:11:55,000 --> 00:11:57,450
And then if you go to some
other circles, you might say,

295
00:11:57,450 --> 00:12:01,920
this is going to lead to end
of civilization if we're not

296
00:12:01,920 --> 00:12:02,980
careful about this.

297
00:12:02,980 --> 00:12:06,810
Now, logically, I don't see--

298
00:12:06,810 --> 00:12:08,800
I think this is
maybe a possibility,

299
00:12:08,800 --> 00:12:12,370
although I don't believe it,
but it's not an impossibility.

300
00:12:12,370 --> 00:12:15,690
But what does seem true
is that the architecture

301
00:12:15,690 --> 00:12:19,290
is a crucial ingredient
in this recipe.

302
00:12:19,290 --> 00:12:21,810
And to motivate this,
I think it's worth

303
00:12:21,810 --> 00:12:24,730
looking at this paper from 2007.

304
00:12:24,730 --> 00:12:27,480
And the title of this paper
is "Large Language Models

305
00:12:27,480 --> 00:12:29,080
in Machine Translation."

306
00:12:29,080 --> 00:12:31,090
And this is a paper from Google.

307
00:12:31,090 --> 00:12:35,010
And if you actually read
the abstract, it says,

308
00:12:35,010 --> 00:12:38,110
"We train a model on
2 trillion tokens,

309
00:12:38,110 --> 00:12:40,900
resulting in a model with
$300 billion n-grams."

310
00:12:40,900 --> 00:12:43,170
So these are
essentially parameters.

311
00:12:43,170 --> 00:12:46,940
So if you actually compare the
recipe that's going on here,

312
00:12:46,940 --> 00:12:48,880
it's actually exactly
the same as the recipe

313
00:12:48,880 --> 00:12:50,600
that we're doing right now.

314
00:12:50,600 --> 00:12:52,220
We have trillions of tokens.

315
00:12:52,220 --> 00:12:54,950
We have a model with hundreds
of billions of parameters.

316
00:12:54,950 --> 00:12:58,600
So it's similar order of scale
with the LLMs that exist today,

317
00:12:58,600 --> 00:13:01,930
and they're employing the
exact same objective, which

318
00:13:01,930 --> 00:13:04,990
is maximum likelihood learning.

319
00:13:04,990 --> 00:13:08,420
And the only difference,
again, is this architecture.

320
00:13:08,420 --> 00:13:11,770
So in 2007, they were
working with lookup tables,

321
00:13:11,770 --> 00:13:14,870
which is an architecture for
a model, a language model.

322
00:13:14,870 --> 00:13:17,260
And then today, we're
working with transformers.

323
00:13:17,260 --> 00:13:22,150
So why didn't we
have ChatGPT in 2007?

324
00:13:22,150 --> 00:13:24,225
And if you look at
this paper, they say,

325
00:13:24,225 --> 00:13:25,600
they get pretty
cool improvements

326
00:13:25,600 --> 00:13:27,550
in machine translation,
but you're not really

327
00:13:27,550 --> 00:13:29,530
talking about end of
civilization or Einsteins

328
00:13:29,530 --> 00:13:30,320
on GPUs.

329
00:13:30,320 --> 00:13:32,270
And it's because of
this architecture.

330
00:13:32,270 --> 00:13:36,190
So I think this is one
motivation, one example that

331
00:13:36,190 --> 00:13:40,780
highlights the importance of
architecture in this AI recipe.

332
00:13:40,780 --> 00:13:43,640
So what is this architecture?

333
00:13:43,640 --> 00:13:45,510
What do transformers look like?

334
00:13:45,510 --> 00:13:48,420
Well, transformers are
basically deep learning models.

335
00:13:48,420 --> 00:13:51,200
And for language, they largely--

336
00:13:51,200 --> 00:13:53,580
the next word
prediction systems.

337
00:13:53,580 --> 00:13:56,210
And the architecture
itself gradually

338
00:13:56,210 --> 00:14:00,110
builds up representations
of the previous context

339
00:14:00,110 --> 00:14:03,530
by what's called an attention
mechanism that contextualizes

340
00:14:03,530 --> 00:14:05,270
the current
representation against

341
00:14:05,270 --> 00:14:06,840
the previous representations.

342
00:14:06,840 --> 00:14:08,783
And in language, you
predict the next word.

343
00:14:08,783 --> 00:14:10,700
And then after you've
predicted the next word,

344
00:14:10,700 --> 00:14:14,120
you feed this as input, and
then you perform this again,

345
00:14:14,120 --> 00:14:15,680
so on and so forth.

346
00:14:15,680 --> 00:14:19,280
And this attention
mechanism and predicting

347
00:14:19,280 --> 00:14:22,200
some future or some
sort of left-out context

348
00:14:22,200 --> 00:14:26,250
given other parts of the input
has been incredibly successful.

349
00:14:26,250 --> 00:14:28,970
And the transformer
architecture and this paradigm

350
00:14:28,970 --> 00:14:31,610
is one of the major drivers
of the current generative AI

351
00:14:31,610 --> 00:14:33,900
revolution, of
course, in language,

352
00:14:33,900 --> 00:14:37,010
but in other domains
like speech, vision,

353
00:14:37,010 --> 00:14:41,400
and even robotics and biology.

354
00:14:41,400 --> 00:14:44,920
And as an example of the
importance of this architecture,

355
00:14:44,920 --> 00:14:47,040
the original paper
that introduced this,

356
00:14:47,040 --> 00:14:50,040
whose title is "Attention
Is All You Need,"

357
00:14:50,040 --> 00:14:54,330
in 2017 is on track to become
one of the most well-cited

358
00:14:54,330 --> 00:14:57,930
papers in computer science,
perhaps all of science.

359
00:14:57,930 --> 00:14:59,830
So are we done?

360
00:14:59,830 --> 00:15:03,180
Have we found the
architecture with a capital A?

361
00:15:03,180 --> 00:15:05,130
Do we just need
to scale this up?

362
00:15:05,130 --> 00:15:06,210
Again, maybe.

363
00:15:06,210 --> 00:15:07,980
I don't think so.

364
00:15:07,980 --> 00:15:11,260
And I think it's
worth looking at,

365
00:15:11,260 --> 00:15:14,220
what are some of the issues
with this architecture?

366
00:15:14,220 --> 00:15:17,590
One issue is the inefficiency.

367
00:15:17,590 --> 00:15:20,130
So I've very, at a
high level, talked

368
00:15:20,130 --> 00:15:21,517
about this attention mechanism.

369
00:15:21,517 --> 00:15:23,100
But this attention
mechanism basically

370
00:15:23,100 --> 00:15:28,620
involves looking back into all
the past at each time step.

371
00:15:28,620 --> 00:15:31,170
So for every time step,
you're looking back

372
00:15:31,170 --> 00:15:33,010
into the full history.

373
00:15:33,010 --> 00:15:36,690
And as we think about applying
these models to long sequences,

374
00:15:36,690 --> 00:15:40,570
you can see how, maybe
intuitively, this attention

375
00:15:40,570 --> 00:15:43,030
mechanism, where
you're looking back

376
00:15:43,030 --> 00:15:44,650
into the full path
at each time step,

377
00:15:44,650 --> 00:15:49,570
quickly gets inefficient
when the sequence length gets

378
00:15:49,570 --> 00:15:51,110
into tens of millions.

379
00:15:51,110 --> 00:15:55,180
And we do want models that
can process and understand

380
00:15:55,180 --> 00:15:57,650
and generate sequences
on this order.

381
00:15:57,650 --> 00:16:00,340
So we want models that
ideally do question answering

382
00:16:00,340 --> 00:16:02,890
over complex code bases,
which can be tens of millions

383
00:16:02,890 --> 00:16:03,830
of lines of code.

384
00:16:03,830 --> 00:16:06,490
Ideally, we want these
models to process sequences

385
00:16:06,490 --> 00:16:08,920
in other domains like biology,
where the sequence length

386
00:16:08,920 --> 00:16:11,810
can quickly get into
millions or even billions.

387
00:16:11,810 --> 00:16:13,960
So that's issue one.

388
00:16:13,960 --> 00:16:16,130
The second issue is
slightly more subtle.

389
00:16:16,130 --> 00:16:19,850
And it's related to
the expressiveness,

390
00:16:19,850 --> 00:16:22,430
in particular, the
inexpressiveness.

391
00:16:22,430 --> 00:16:27,040
So let's consider this simple
setting, where we have something

392
00:16:27,040 --> 00:16:30,140
like box 1 contains a book,
box 2 contains an apple,

393
00:16:30,140 --> 00:16:32,050
box 4 contains a brain.

394
00:16:32,050 --> 00:16:35,470
And let's say you have
a bunch of statements

395
00:16:35,470 --> 00:16:37,310
that swap the boxes.

396
00:16:37,310 --> 00:16:40,290
And then after swapping,
you ask, where is the apple?

397
00:16:40,290 --> 00:16:44,900
So this seems quite simple, but
you can actually show, provably

398
00:16:44,900 --> 00:16:49,230
show, that if the number of
states is greater than five,

399
00:16:49,230 --> 00:16:51,780
a single layer
transformer can't do this.

400
00:16:51,780 --> 00:16:53,820
And this is what's
called state tracking.

401
00:16:53,820 --> 00:16:55,280
And as simple as this--

402
00:16:55,280 --> 00:16:57,590
and maybe as
synthetic as this task

403
00:16:57,590 --> 00:16:59,900
is, these types of state
tracking capabilities

404
00:16:59,900 --> 00:17:02,715
do underlie some of the
real world capabilities

405
00:17:02,715 --> 00:17:03,840
that we want in our models.

406
00:17:03,840 --> 00:17:07,609
So we do want models being able
to understand code where there's

407
00:17:07,609 --> 00:17:08,970
a lot of variable swapping.

408
00:17:08,970 --> 00:17:12,500
And it turns out, at least
a single layer transformer

409
00:17:12,500 --> 00:17:14,930
can't provably do this.

410
00:17:14,930 --> 00:17:18,290
So I think some of
the question that's

411
00:17:18,290 --> 00:17:21,109
driving research in
our group is, are there

412
00:17:21,109 --> 00:17:25,400
transformer alternatives that
are more efficient when it comes

413
00:17:25,400 --> 00:17:27,829
to processing those
long sequences

414
00:17:27,829 --> 00:17:31,070
and also more expressive
and can address

415
00:17:31,070 --> 00:17:36,170
some of the deficiencies of
existing attention mechanism?

416
00:17:36,170 --> 00:17:38,262
And over the past
couple of years,

417
00:17:38,262 --> 00:17:39,720
we've been looking
at this question

418
00:17:39,720 --> 00:17:42,270
through the lens of what
are called linear attention

419
00:17:42,270 --> 00:17:43,540
transformers.

420
00:17:43,540 --> 00:17:45,430
And I won't get
into the details.

421
00:17:45,430 --> 00:17:47,940
And this was a work that
was in 2020, introduced

422
00:17:47,940 --> 00:17:49,470
by some other folks.

423
00:17:49,470 --> 00:17:51,880
But the basic idea
is quite simple.

424
00:17:51,880 --> 00:17:54,460
So we have this
attention mechanism,

425
00:17:54,460 --> 00:17:58,290
which is roughly deriving
pairwise interactions

426
00:17:58,290 --> 00:18:03,480
between all pairs of inputs
and all inputs in a sequence.

427
00:18:03,480 --> 00:18:06,930
And it's doing this with an
expressive non-linearity called

428
00:18:06,930 --> 00:18:07,900
the softmax.

429
00:18:07,900 --> 00:18:10,900
And in linear attention, you
basically get rid of this.

430
00:18:10,900 --> 00:18:12,670
It's actually a
much simpler model.

431
00:18:12,670 --> 00:18:14,415
And it turns out,
when you do this,

432
00:18:14,415 --> 00:18:18,000
you can reformulate this
model as essentially

433
00:18:18,000 --> 00:18:20,880
a recurrent neural
network, but with

434
00:18:20,880 --> 00:18:23,070
matrix-valued
hidden states, which

435
00:18:23,070 --> 00:18:26,490
are different from traditional
vector-valued hidden states

436
00:18:26,490 --> 00:18:27,670
that you're used to.

437
00:18:27,670 --> 00:18:31,900
And this is also the same
thing as state-space models,

438
00:18:31,900 --> 00:18:34,860
for those of you familiar
with works like Mamba.

439
00:18:34,860 --> 00:18:38,210
And because these are
essentially RNNs now,

440
00:18:38,210 --> 00:18:42,280
they inherit the efficiency
benefits of transformer,

441
00:18:42,280 --> 00:18:45,340
a la the ability to
model arbitrarily

442
00:18:45,340 --> 00:18:49,540
long contexts with a fixed
dimensional hidden state memory.

443
00:18:49,540 --> 00:18:54,380
Now, of course, you lose
something in the simplification.

444
00:18:54,380 --> 00:18:57,560
So we've been looking at ways
to make this more expressive.

445
00:18:57,560 --> 00:18:59,620
So one example from
our work last year

446
00:18:59,620 --> 00:19:03,890
was exploring a older
model called DeltaNet,

447
00:19:03,890 --> 00:19:06,730
which is a fancier version
of linear attention

448
00:19:06,730 --> 00:19:08,980
that addresses some of the
limitations of attention

449
00:19:08,980 --> 00:19:12,070
and deriving
efficient algorithms

450
00:19:12,070 --> 00:19:14,710
for training this model
and then also exploring

451
00:19:14,710 --> 00:19:18,910
hybrid architectures
that combine

452
00:19:18,910 --> 00:19:22,510
regular attention with some of
these more expressive variants.

453
00:19:22,510 --> 00:19:25,030
More recently, we
developed something

454
00:19:25,030 --> 00:19:27,070
called log linear
attention, which is actually

455
00:19:27,070 --> 00:19:32,030
in between the linear attention
and full global attention,

456
00:19:32,030 --> 00:19:35,760
where the computational
complexity is in between.

457
00:19:35,760 --> 00:19:38,750
And this, again, addresses
some of the limitations

458
00:19:38,750 --> 00:19:42,650
a la associative recall when it
comes to things like recalling

459
00:19:42,650 --> 00:19:45,290
things in your past.

460
00:19:45,290 --> 00:19:49,530
So what will the future of
LLM architectures look like?

461
00:19:49,530 --> 00:19:51,020
I think, in the
medium term, we're

462
00:19:51,020 --> 00:19:54,080
going to have these
hybrid architectures that

463
00:19:54,080 --> 00:19:57,870
mix these linear attention
models with global attention.

464
00:19:57,870 --> 00:20:00,240
And actually, as
an example of this,

465
00:20:00,240 --> 00:20:04,070
just last week, this
Qwen group from Alibaba

466
00:20:04,070 --> 00:20:07,070
released a state of the
art open-weight model

467
00:20:07,070 --> 00:20:09,710
that exactly mixed some
of the DeltaNet layers

468
00:20:09,710 --> 00:20:13,580
I talked about with attention
layers, and it does very well.

469
00:20:13,580 --> 00:20:17,750
Now, in the longer term, I
think we'll have architectures

470
00:20:17,750 --> 00:20:20,310
where the compute is dynamic.

471
00:20:20,310 --> 00:20:22,640
And what I mean by that
is, current architectures

472
00:20:22,640 --> 00:20:25,842
apply the same amount of
computation at each token.

473
00:20:25,842 --> 00:20:28,050
And in some sense, this
seems incredibly inefficient,

474
00:20:28,050 --> 00:20:29,800
and I think there'll
be interesting things

475
00:20:29,800 --> 00:20:33,360
to do in having the compute be
a dynamic function of the input.

476
00:20:33,360 --> 00:20:35,390
And then I think we'll
also get to inference

477
00:20:35,390 --> 00:20:38,630
optimized architectures, where,
currently, we're really focused

478
00:20:38,630 --> 00:20:41,170
on getting the best
architecture if we have

479
00:20:41,170 --> 00:20:42,420
some amount of training flops.

480
00:20:42,420 --> 00:20:44,420
But there are many
applications, especially when

481
00:20:44,420 --> 00:20:46,670
we think about edge
applications where maybe we

482
00:20:46,670 --> 00:20:48,540
can pay a lot of
training compute,

483
00:20:48,540 --> 00:20:50,610
but the inference
is more optimal.

484
00:20:50,610 --> 00:20:54,860
And I think it's exciting
to think about architectures

485
00:20:54,860 --> 00:20:57,650
that will push the [INAUDIBLE]
frontier of capability

486
00:20:57,650 --> 00:20:58,950
and compute.

487
00:20:58,950 --> 00:20:59,880
Thank you.

488
00:20:59,880 --> 00:21:02,580
[APPLAUSE]

489
00:21:02,580 --> 00:21:02,850

490
00:21:02,850 --> 00:21:03,350
Hey.

491
00:21:03,350 --> 00:21:04,670
Hi, everyone.

492
00:21:04,670 --> 00:21:05,670
I'm Phillip.

493
00:21:05,670 --> 00:21:08,690
I'm another faculty
member in EECS,

494
00:21:08,690 --> 00:21:11,900
and I'm going to close out
this LLM session that we

495
00:21:11,900 --> 00:21:13,730
have going on right now.

496
00:21:13,730 --> 00:21:16,602
So I study primarily
computer vision,

497
00:21:16,602 --> 00:21:18,560
but I've recently gotten
interested in language

498
00:21:18,560 --> 00:21:21,230
and come to see vision
and language as being more

499
00:21:21,230 --> 00:21:23,720
alike than they are different.

500
00:21:23,720 --> 00:21:26,340
OK, so we'll come
back to this picture.

501
00:21:26,340 --> 00:21:29,210
This is the cover of a
book about Reading Rainbow,

502
00:21:29,210 --> 00:21:32,640
an old TV show from the '90s.

503
00:21:32,640 --> 00:21:36,570
OK, so if we want to be able to
mix language models with vision

504
00:21:36,570 --> 00:21:39,300
models, then we need to
approach this problem, which

505
00:21:39,300 --> 00:21:42,340
is called the sensory or the
symbol grounding problem.

506
00:21:42,340 --> 00:21:46,500
How do you associate a physical
meaning or a sensory meaning

507
00:21:46,500 --> 00:21:48,220
with a word like apple?

508
00:21:48,220 --> 00:21:51,240
How do you form this association
to the actual image of the apple

509
00:21:51,240 --> 00:21:53,050
or the actual
thing in the world,

510
00:21:53,050 --> 00:21:54,780
the referent of that word?

511
00:21:54,780 --> 00:21:56,350
And this is an old problem.

512
00:21:56,350 --> 00:21:58,860
Philosophers have thought about
this problem for many years.

513
00:21:58,860 --> 00:22:04,170
And a lot of work has been done
in computer vision and language

514
00:22:04,170 --> 00:22:07,868
processing on training models
that can solve this problem.

515
00:22:07,868 --> 00:22:09,660
But one of the really
interesting questions

516
00:22:09,660 --> 00:22:11,550
that we've had
recently is, to what

517
00:22:11,550 --> 00:22:13,890
degree do you actually
have to train the language

518
00:22:13,890 --> 00:22:16,140
model with images?

519
00:22:16,140 --> 00:22:19,480
Could it be possible that
language alone, that text alone,

520
00:22:19,480 --> 00:22:21,128
knows something about
the visual world?

521
00:22:21,128 --> 00:22:22,545
And can we probe
how much it might

522
00:22:22,545 --> 00:22:23,650
know about the visual world?

523
00:22:23,650 --> 00:22:25,567
So could a language model
become knowledgeable

524
00:22:25,567 --> 00:22:28,570
about visual concepts without
ever having seen an image?

525
00:22:28,570 --> 00:22:31,840
That's the question that
we've been pursuing.

526
00:22:31,840 --> 00:22:34,780
And I think actually,
intuitively, the answer

527
00:22:34,780 --> 00:22:35,720
should be yes.

528
00:22:35,720 --> 00:22:38,290
And there's some evidence
for that, like this cover

529
00:22:38,290 --> 00:22:41,230
here, this Reading
Rainbow book, which

530
00:22:41,230 --> 00:22:43,150
was a TV show where
they read books,

531
00:22:43,150 --> 00:22:44,890
and the audience
was supposed to be

532
00:22:44,890 --> 00:22:47,590
imagining all the
interesting scenarios

533
00:22:47,590 --> 00:22:48,890
in great visual detail.

534
00:22:48,890 --> 00:22:51,200
So if I tell you to
open a fantasy novel,

535
00:22:51,200 --> 00:22:55,100
it will talk about the
scaly, green, shiny dragon,

536
00:22:55,100 --> 00:22:56,920
and it will talk
about the prince

537
00:22:56,920 --> 00:23:00,230
with the bright
yellow gold crown.

538
00:23:00,230 --> 00:23:02,470
So these are all visual words.

539
00:23:02,470 --> 00:23:03,830
There's a lot more examples.

540
00:23:03,830 --> 00:23:06,205
When you read a book, you can
imagine what you're seeing.

541
00:23:06,205 --> 00:23:08,380
There's a lot of text which
is visually descriptive.

542
00:23:08,380 --> 00:23:09,650
Here's another famous one.

543
00:23:09,650 --> 00:23:11,050
This is from
Proust's description

544
00:23:11,050 --> 00:23:12,350
of the madeleine dipped in tea.

545
00:23:12,350 --> 00:23:13,780
It's a famous
example because it's

546
00:23:13,780 --> 00:23:16,750
so evocative of that
sensory experience.

547
00:23:16,750 --> 00:23:20,590
"She sent out for one of those
short little plump cookies

548
00:23:20,590 --> 00:23:22,510
called petites madeleines.

549
00:23:22,510 --> 00:23:26,390
And it was a weary and dull
day, a depressing morrow."

550
00:23:26,390 --> 00:23:28,300
These are evocative
of the feeling

551
00:23:28,300 --> 00:23:29,730
of the sensory experience.

552
00:23:29,730 --> 00:23:32,270
"It was a soaked
morsel of cake."

553
00:23:32,270 --> 00:23:34,490
OK, so from text alone,
there's a lot of language

554
00:23:34,490 --> 00:23:36,300
that describes the
visual experience.

555
00:23:36,300 --> 00:23:39,170
So it stands to reason
that, from text alone,

556
00:23:39,170 --> 00:23:42,050
we can learn something
about the visual world.

557
00:23:42,050 --> 00:23:44,210
And in the last
five years or so,

558
00:23:44,210 --> 00:23:46,070
people have been studying this.

559
00:23:46,070 --> 00:23:49,040
So I really like this experiment
from Abdou, et al. on,

560
00:23:49,040 --> 00:23:51,770
what do language models
know about color?

561
00:23:51,770 --> 00:23:53,990
OK, so what they
did is they took

562
00:23:53,990 --> 00:23:55,410
a language model of the era.

563
00:23:55,410 --> 00:23:56,160
It was BERT.

564
00:23:56,160 --> 00:23:59,490
So it's a model that's trained
only on text, no images at all.

565
00:23:59,490 --> 00:24:02,880
And they extracted
the neural embeddings,

566
00:24:02,880 --> 00:24:06,000
the representations that the
language model gives to words.

567
00:24:06,000 --> 00:24:09,530
And then they extracted these
embeddings for color words

568
00:24:09,530 --> 00:24:12,290
and found the
three-dimensional structure

569
00:24:12,290 --> 00:24:14,870
of the embeddings which would
best preserve and explain

570
00:24:14,870 --> 00:24:18,830
the distances that the language
model gives to those words.

571
00:24:18,830 --> 00:24:21,620
And so this is the
language model's embedding

572
00:24:21,620 --> 00:24:23,570
of color words.

573
00:24:23,570 --> 00:24:25,290
And it's really interesting.

574
00:24:25,290 --> 00:24:26,580
It puts green next to green.

575
00:24:26,580 --> 00:24:27,288
That makes sense.

576
00:24:27,288 --> 00:24:28,850
It puts yellow next to yellow.

577
00:24:28,850 --> 00:24:31,900
If you look at the human
perceptual system's organization

578
00:24:31,900 --> 00:24:34,630
of color, the most famous
model of human color perception

579
00:24:34,630 --> 00:24:37,130
is called the
CIELAB color space,

580
00:24:37,130 --> 00:24:39,190
and it's remarkably similar.

581
00:24:39,190 --> 00:24:41,960
So BERT, never
trained on a pixel,

582
00:24:41,960 --> 00:24:47,020
has recovered the same geometry
of color word representation

583
00:24:47,020 --> 00:24:50,620
as humans have for the
corresponding actual perceptual

584
00:24:50,620 --> 00:24:53,860
sensory colors.

585
00:24:53,860 --> 00:24:56,630
OK, so we've been asking
that more broadly recently.

586
00:24:56,630 --> 00:24:58,870
How similar are
the representations

587
00:24:58,870 --> 00:25:01,510
in the language model to the
representations in the vision

588
00:25:01,510 --> 00:25:02,240
model?

589
00:25:02,240 --> 00:25:04,510
If a language model--
or if a vision model

590
00:25:04,510 --> 00:25:06,820
sees an image like
this apple, it

591
00:25:06,820 --> 00:25:09,920
will create a neural
vector representation.

592
00:25:09,920 --> 00:25:12,440
If a language model instead
sees the word apple,

593
00:25:12,440 --> 00:25:15,890
does it create the same vector
embedding or a different vector?

594
00:25:15,890 --> 00:25:18,340
Is it representing the world
in fundamentally the same way

595
00:25:18,340 --> 00:25:20,110
or a different way?

596
00:25:20,110 --> 00:25:22,600
OK, are these the
same or different?

597
00:25:22,600 --> 00:25:25,790
So to run this analysis,
there's a few technical details.

598
00:25:25,790 --> 00:25:28,480
The basic idea is
we're going to measure

599
00:25:28,480 --> 00:25:32,420
the representation in a model
by what is called its kernel.

600
00:25:32,420 --> 00:25:36,640
This is the matrix of how
similar are the representations

601
00:25:36,640 --> 00:25:39,802
for one word, according to the
model, to every other word,

602
00:25:39,802 --> 00:25:40,760
according to the model.

603
00:25:40,760 --> 00:25:42,440
So it's the similarity matrix.

604
00:25:42,440 --> 00:25:44,740
Does the language model
think that-- does it

605
00:25:44,740 --> 00:25:48,260
assign a similar vector for the
word apple to the word orange?

606
00:25:48,260 --> 00:25:51,040
And we can run the same thing
on the vision model applied

607
00:25:51,040 --> 00:25:54,790
to images and say,
does the vision model

608
00:25:54,790 --> 00:25:57,850
think that the similarity
between apple and orange

609
00:25:57,850 --> 00:26:00,400
is the same amount as the
language model thought

610
00:26:00,400 --> 00:26:03,460
the similarity was between the
word apple and the word orange?

611
00:26:03,460 --> 00:26:05,860
So we're looking at if the
language model and the vision

612
00:26:05,860 --> 00:26:10,150
model measure relationships
and distances in the same way.

613
00:26:10,150 --> 00:26:13,060
This is called kernel analysis.

614
00:26:13,060 --> 00:26:15,880
OK, so a paper that we
put out about a year ago

615
00:26:15,880 --> 00:26:18,520
had the following
finding, which is,

616
00:26:18,520 --> 00:26:21,050
we took a lot of language
models, a lot of vision models,

617
00:26:21,050 --> 00:26:23,853
and on the x-axis
is the performance

618
00:26:23,853 --> 00:26:24,770
of the language model.

619
00:26:24,770 --> 00:26:27,300
How good is the language
model at language modeling?

620
00:26:27,300 --> 00:26:29,360
OK, so it will go
from old models

621
00:26:29,360 --> 00:26:31,110
to the more recent models.

622
00:26:31,110 --> 00:26:34,370
And on the y-axis is going to
be the alignment, the similarity

623
00:26:34,370 --> 00:26:38,210
of the language representation
to the image representation

624
00:26:38,210 --> 00:26:40,460
under an image model.

625
00:26:40,460 --> 00:26:42,560
So here's the basic
trend that we found.

626
00:26:42,560 --> 00:26:45,830
As language models get better
at predicting the next word

627
00:26:45,830 --> 00:26:48,530
in sentences, they come
up with representations

628
00:26:48,530 --> 00:26:51,680
of those sentences that are more
and more alike to the kernel

629
00:26:51,680 --> 00:26:54,380
structure, a particular kind of
structure of the vision model

630
00:26:54,380 --> 00:26:56,370
of the corresponding images.

631
00:26:56,370 --> 00:27:00,343
This is run on a data set
of images with captions.

632
00:27:00,343 --> 00:27:02,010
The same is true in
the other direction.

633
00:27:02,010 --> 00:27:04,670
As the vision model becomes
bigger and better at modeling

634
00:27:04,670 --> 00:27:07,520
vision, these curves go up,
and we get better alignment

635
00:27:07,520 --> 00:27:08,670
to language models.

636
00:27:08,670 --> 00:27:11,180
So bigger, better
vision models align more

637
00:27:11,180 --> 00:27:14,750
with bigger, better
language models.

638
00:27:14,750 --> 00:27:18,350
OK, so it seems like there is
some similarity between language

639
00:27:18,350 --> 00:27:20,060
models and vision
models, even if you

640
00:27:20,060 --> 00:27:21,583
don't train them to be alike.

641
00:27:21,583 --> 00:27:23,750
It's just that they're both
modeling the same world,

642
00:27:23,750 --> 00:27:25,950
and, therefore, as they get
better at modeling that world,

643
00:27:25,950 --> 00:27:27,420
they somehow become
more aligned.

644
00:27:27,420 --> 00:27:29,130
But it's not perfect alignment.

645
00:27:29,130 --> 00:27:31,478
The numbers on that last
plot only went up to 0.16

646
00:27:31,478 --> 00:27:33,770
on some scale I'm not going
to tell you the details of,

647
00:27:33,770 --> 00:27:35,390
but it's not perfect.

648
00:27:35,390 --> 00:27:38,790
So what we've also been pursuing
in my group and other groups is,

649
00:27:38,790 --> 00:27:42,060
can we make language models even
more aligned with vision models?

650
00:27:42,060 --> 00:27:44,090
Can we make them
better descriptors

651
00:27:44,090 --> 00:27:45,420
of visual experience?

652
00:27:45,420 --> 00:27:47,090
And I'll just show
you one project

653
00:27:47,090 --> 00:27:51,350
on that where we realized
that if you take an image,

654
00:27:51,350 --> 00:27:53,570
and you caption it with a
language model or a vision

655
00:27:53,570 --> 00:27:56,660
language model, it might output
a very short sentence which

656
00:27:56,660 --> 00:27:59,240
doesn't tell you nearly
enough about this photo

657
00:27:59,240 --> 00:28:00,342
to reconstruct it.

658
00:28:00,342 --> 00:28:01,800
It just says, it's
a cup of coffee.

659
00:28:01,800 --> 00:28:03,860
It doesn't tell you
what colors it is.

660
00:28:03,860 --> 00:28:05,990
So we thought,
what if we instead

661
00:28:05,990 --> 00:28:09,350
train a model to
output text that

662
00:28:09,350 --> 00:28:12,210
is sufficient to
reconstruct the input image?

663
00:28:12,210 --> 00:28:14,040
We call this property
cycle consistency.

664
00:28:14,040 --> 00:28:16,670
You can go from the image
to the text and back.

665
00:28:16,670 --> 00:28:19,390
Now, this was work that was
led by [INAUDIBLE] and Caroline

666
00:28:19,390 --> 00:28:19,890
here.

667
00:28:19,890 --> 00:28:22,320
And they have a
poster on it today,

668
00:28:22,320 --> 00:28:24,790
so please feel free to go
check it out for more details.

669
00:28:24,790 --> 00:28:26,165
But I'll give you
the gist of it.

670
00:28:26,165 --> 00:28:28,800
The gist of it is, we can't
reconstruct this photo

671
00:28:28,800 --> 00:28:33,090
from a short caption that's
not visually descriptive.

672
00:28:33,090 --> 00:28:36,070
I don't know what that cup
of coffee should look like.

673
00:28:36,070 --> 00:28:40,780
If I put that sentence into
an image generative model,

674
00:28:40,780 --> 00:28:43,000
maybe it will make a
cup of coffee like this,

675
00:28:43,000 --> 00:28:44,890
and the background
color is wrong.

676
00:28:44,890 --> 00:28:48,210
The decoration is
wrong on the surface.

677
00:28:48,210 --> 00:28:50,980
OK, so we call those dissimilar,
and we penalize that.

678
00:28:50,980 --> 00:28:54,480
And then we train the captioning
model, the mapping from image

679
00:28:54,480 --> 00:28:58,200
to language, to create a
descriptive-enough caption

680
00:28:58,200 --> 00:29:01,830
that you can reconstruct the
photo almost exactly or decently

681
00:29:01,830 --> 00:29:02,410
well.

682
00:29:02,410 --> 00:29:05,230
Now it gets the pattern and the
colors and the background color.

683
00:29:05,230 --> 00:29:08,190
And now we score this as
being, the input and output

684
00:29:08,190 --> 00:29:11,320
are similar, and the model can
learn from that training signal.

685
00:29:11,320 --> 00:29:13,530
So it outputs text which
is sufficiently informative

686
00:29:13,530 --> 00:29:15,583
to describe the visual
details in the image,

687
00:29:15,583 --> 00:29:18,000
and we get better language
models and better vision models

688
00:29:18,000 --> 00:29:20,250
by doing so.

689
00:29:20,250 --> 00:29:22,510
OK, I'll end with one
more little snippet

690
00:29:22,510 --> 00:29:25,970
of what we're doing
under this magic grant

691
00:29:25,970 --> 00:29:28,090
that we now have a
grant under this program

692
00:29:28,090 --> 00:29:32,420
to work on textbooks that
are augmented with AI.

693
00:29:32,420 --> 00:29:35,170
And this is with Antonio
Torralba and Bill Freeman

694
00:29:35,170 --> 00:29:37,270
and Amy Brand at the MIT Press.

695
00:29:37,270 --> 00:29:39,193
And we're working
on this in part

696
00:29:39,193 --> 00:29:41,360
because we just published
a book on computer vision,

697
00:29:41,360 --> 00:29:43,670
and now we want to use
language models to improve it.

698
00:29:43,670 --> 00:29:45,880
But we really are
interested, especially,

699
00:29:45,880 --> 00:29:48,640
in this problem of improving
the figures and the relationship

700
00:29:48,640 --> 00:29:50,560
between the visual
information in the book

701
00:29:50,560 --> 00:29:55,360
and the underlying code and
text around that information.

702
00:29:55,360 --> 00:29:58,300
So we also today have a
poster on some early results

703
00:29:58,300 --> 00:29:59,150
of this project.

704
00:29:59,150 --> 00:30:01,480
This was run by Suyong Kim.

705
00:30:01,480 --> 00:30:04,510
And what we did is we took
a vision language model,

706
00:30:04,510 --> 00:30:07,240
and-- or she applied
it to this figure

707
00:30:07,240 --> 00:30:09,340
here and asked it, make
an interactive version

708
00:30:09,340 --> 00:30:10,900
of this figure, just
to show you what

709
00:30:10,900 --> 00:30:14,320
can be done already with
fairly off-the-shelf methods.

710
00:30:14,320 --> 00:30:17,660
OK, and so here's the result.
So there's a little interaction.

711
00:30:17,660 --> 00:30:20,240
She's going back and forth
a bit to modify the code.

712
00:30:20,240 --> 00:30:23,050
But basically, this model
can parse the imagery.

713
00:30:23,050 --> 00:30:27,452
It can output HTML and CSS
and other types of web code

714
00:30:27,452 --> 00:30:29,910
that will generate a web page
with this interactive version

715
00:30:29,910 --> 00:30:30,850
of that figure.

716
00:30:30,850 --> 00:30:33,225
So that's one of the things
that we're pursuing currently

717
00:30:33,225 --> 00:30:33,990
under this grant.

718
00:30:33,990 --> 00:30:37,150
So to wrap up, I'll say, what
do LLMs know about vision?

719
00:30:37,150 --> 00:30:39,310
That's the question
that I've been asking.

720
00:30:39,310 --> 00:30:40,720
And I think it's quite a lot.

721
00:30:40,720 --> 00:30:42,387
It's more than a lot
of people expected,

722
00:30:42,387 --> 00:30:45,750
more than I expected-- not
everything, but quite a lot.

723
00:30:45,750 --> 00:30:48,510
So LLMs trained on internet
text naturally learn

724
00:30:48,510 --> 00:30:50,440
representations that
are, to some degree--

725
00:30:50,440 --> 00:30:52,350
to a surprising
degree, to me-- aligned

726
00:30:52,350 --> 00:30:54,190
with visual representations.

727
00:30:54,190 --> 00:30:56,220
They might not be that
fundamentally a way

728
00:30:56,220 --> 00:30:57,810
of processing the world.

729
00:30:57,810 --> 00:31:00,870
By optimizing for better
alignment between vision

730
00:31:00,870 --> 00:31:03,700
and language and potentially
other modalities like sound,

731
00:31:03,700 --> 00:31:06,250
we can get better multimodal
foundation models.

732
00:31:06,250 --> 00:31:08,740
And as one application that
we're looking at right now,

733
00:31:08,740 --> 00:31:11,820
we can potentially
augment figures

734
00:31:11,820 --> 00:31:15,660
in textbooks with AI
features like interactivity.

735
00:31:15,660 --> 00:31:16,623
OK, I will end there.

736
00:31:16,623 --> 00:31:17,790
And thank you for listening.

737
00:31:17,790 --> 00:31:19,500
[APPLAUSE]

738
00:31:19,500 --> 00:31:20,000

