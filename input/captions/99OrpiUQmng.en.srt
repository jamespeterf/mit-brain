1
00:00:02,880 --> 00:00:08,320
Welcome to the MIA 10-year anniversary 
celebration. I'm Caroline Ooler,  

2
00:00:08,320 --> 00:00:12,480
director of the Eric and Wendy Schmidt 
Center at the Broad Institute, which has  

3
00:00:12,480 --> 00:00:18,720
been proudly hosting the models, Inference, and 
Algorithms MIA meetings for the past few years.  

4
00:00:18,720 --> 00:00:24,240
We're so excited to bring the MIA community 
together today for this milestone event.  

5
00:00:24,240 --> 00:00:29,920
It's the perfect chance to reflect on the past 
10 years, celebrate all that's been accomplished,  

6
00:00:29,920 --> 00:00:37,360
and look ahead to the dynamic future of MIA. A 
special thanks to our panelists, Alex Blowmendal,  

7
00:00:37,360 --> 00:00:45,360
one of MIA's co-founders, Aviv Rev, Alexandrina 
Goeva, and Gavororg Gregorian. We're so happy  

8
00:00:45,360 --> 00:00:50,720
to have you. MIA has been instrumental in 
bridging communities across mathematics,  

9
00:00:50,720 --> 00:00:56,080
statistics, machine learning, and biology. 
We've been hearing from leading and emerging  

10
00:00:56,080 --> 00:01:00,560
researchers about the latest research 
that's going on, but also the motivation  

11
00:01:00,560 --> 00:01:05,920
and intuition that go into the development 
of the proposed methods. Again and again,  

12
00:01:05,920 --> 00:01:11,680
participants leave the talks having absorbed 
new ideas that they apply to their own research,  

13
00:01:11,680 --> 00:01:17,440
and that's what makes MIA so unique. Through these 
meetings, we're proud to have built a community  

14
00:01:17,440 --> 00:01:24,640
where disciplines meet and new ideas spark. We 
can't wait to see what the future of MIA holds and  

15
00:01:24,640 --> 00:01:30,320
the broader interdisciplinary field it continues 
to shape. And now I'll hand it over to other  

16
00:01:30,320 --> 00:01:48,240
members of the steering committee. Thank you for 
being such an important part of the MIA community.

17
00:02:00,640 --> 00:02:06,160
MIA is the avenue where you get the most 
cutting edge development and application  

18
00:02:06,160 --> 00:02:11,600
of machine learning. MIA is a seminar series where 
we bring in scientists and researchers from other  

19
00:02:11,600 --> 00:02:17,120
universities and even from all over the world. So 
it it just creates communities which is important  

20
00:02:17,120 --> 00:02:24,240
in science. All are coming together to solve a 
problem that will eventually impact patients life.  

21
00:02:24,240 --> 00:02:28,880
That's the community that NIA has created. It's an 
interesting community, right? I mean now the field  

22
00:02:28,880 --> 00:02:34,960
of machine learning or artificial intelligence 
or modeling is really you know fusing essentially  

23
00:02:34,960 --> 00:02:38,960
with the biological systems. 10 years ago the two 
fields would really speak different languages,  

24
00:02:38,960 --> 00:02:42,960
have different cultures, different taste and 
so on. and more and more I think we're seeing  

25
00:02:42,960 --> 00:02:48,640
a convergence of the biological sciences and the 
computational ones becoming part. It's wonderful  

26
00:02:48,640 --> 00:02:53,760
to have such a seminar hosted here uh where 
different researchers can come and and also  

27
00:02:53,760 --> 00:02:58,800
focusing on some of the details and technicalities 
that go behind behind the methods that have been  

28
00:02:58,800 --> 00:03:04,160
developed for biological data. MIA community is 
very unique in the sense it's very cross-sectional  

29
00:03:04,160 --> 00:03:10,720
because we focus on models and inference 
and analysis as opposed to like a specific  

30
00:03:10,720 --> 00:03:16,080
biological problem. The current place where 
MIA is especially with its strong association  

31
00:03:16,080 --> 00:03:22,880
with the Schmidt center is that there are a lot of 
people who can provide input to projects that are  

32
00:03:22,880 --> 00:03:27,600
happening in other parts of the broad and maybe 
even contribute. So I think kind of harnessing  

33
00:03:27,600 --> 00:03:32,720
that availability of computational expertise and 
the variety of computational expertise I think  

34
00:03:32,720 --> 00:03:39,440
would be very helpful. I've found mentors, I found 
collaborators, people that I get to just kind of  

35
00:03:39,440 --> 00:03:44,640
talk to about my science um thought partners. It's 
been really fun to be a part of the community and  

36
00:03:44,640 --> 00:03:48,560
then just kind of be able to like if I have a 
random idea, I be like, "Oh, this person had  

37
00:03:48,560 --> 00:03:53,440
talked about something adjacent to it." It's quite 
exciting how much MIA has evolved over the past 10  

38
00:03:53,440 --> 00:03:57,520
years. But I feel like that central theme of being 
a community of people that can work together and  

39
00:03:57,520 --> 00:04:02,800
like kind of just rift off of ideas has stayed 
true, which is exciting. I hope that, you know,  

40
00:04:02,800 --> 00:04:08,320
as as the field evolves, we're able to continually 
kind of change what we're talking about and change  

41
00:04:08,320 --> 00:04:13,120
the nuances that we are considering in these 
conversations. Personally, I've I've gained so  

42
00:04:13,120 --> 00:04:18,000
much. I love this community. I presented here 
six years ago and it was probably one of the  

43
00:04:18,000 --> 00:04:22,480
most like interactive talks I've ever been in with 
like the most like diverse group of computational  

44
00:04:22,480 --> 00:04:27,040
people probably that I get to interact with. You 
have people who are in statistics and math and  

45
00:04:27,040 --> 00:04:32,080
physics and they all have this interest in biology 
kind of widely speaking. The richness of always  

46
00:04:32,080 --> 00:04:38,000
trying to to look out for new ideas and uh I just 
think it's a special community. This community is  

47
00:04:38,000 --> 00:04:41,200
important because it's really important to stay 
grounded in both like the fundamentals as well  

48
00:04:41,200 --> 00:04:46,400
as like the new things that are emerging in our 
in our field. I think it really helps in cross  

49
00:04:46,400 --> 00:04:51,040
crosscontamination between different fields. Like 
I think those are one of the few places where you  

50
00:04:51,040 --> 00:04:55,760
can take a computer scientist and a biologist and 
both of them can enjoy the talk while in other  

51
00:04:55,760 --> 00:05:01,120
cases one of them will not have enough knowledge 
to understand what we're talking about. Presenting  

52
00:05:01,120 --> 00:05:06,400
at MIA is a very uh unique experience. The 
separation between the primer and the main  

53
00:05:06,400 --> 00:05:13,360
talk forces you to sit back and think about 
what is actually the best way to deliver this  

54
00:05:13,360 --> 00:05:18,400
material and to engage with the audience. Getting 
to hang out with a bunch of cool speakers and cool  

55
00:05:18,400 --> 00:05:22,800
organizers. And so I think that the MIA is just 
a very fun community in that regard. The concept  

56
00:05:22,800 --> 00:05:30,880
of MIA is very special. In that span of a few 
hours, you're getting a beautiful throughine from  

57
00:05:30,880 --> 00:05:37,920
foundational concepts all the way to the research 
that is state-of-the-art. I hope that others who  

58
00:05:37,920 --> 00:05:43,040
attend the talks, they really do feel welcomed 
to join in the conversation, whether they're in  

59
00:05:43,040 --> 00:05:49,520
the room or online, and recognize that MIA is more 
than just the talk series. We're putting a lot of,  

60
00:05:49,520 --> 00:05:53,840
you know, powerful heads together. We're getting 
a lot of people in the same room who, you know,  

61
00:05:53,840 --> 00:05:58,640
don't have the same background, may not have 
the same goals, but have have to understand  

62
00:05:58,640 --> 00:06:04,000
in principle all of the same uh types 
of uh of things. At the end of the day,  

63
00:06:04,000 --> 00:06:10,880
what's cool is that um you have a core group 
of people from at the broad, other institutions  

64
00:06:10,880 --> 00:06:16,240
and also industry that are coming that you see it, 
you know, again and again. One thing that's really  

65
00:06:16,240 --> 00:06:21,760
cool is that we have a lot of junior people um 
and they feel comfortable asking questions and  

66
00:06:21,760 --> 00:06:27,280
and having discussions and I think that um makes 
it really a lot more lively. The community that  

67
00:06:27,280 --> 00:06:33,520
the MIE has created is so meaningful especially 
because it's trying to bridge the gap between  

68
00:06:33,520 --> 00:06:40,160
industry and academia that has existed for a 
long time in this field. And I think with AI  

69
00:06:40,160 --> 00:06:45,360
needing more computational resources, bridging 
that gap is super important right now. Hopefully  

70
00:06:45,360 --> 00:07:04,640
this community grows more and more and more 
people are involved and learning a lot from it.

71
00:07:04,640 --> 00:07:08,640
Um so uh our panelists today are uh Aviv Rev  

72
00:07:08,640 --> 00:07:15,200
um currently at Jenn um yeah 
please come up. Um uh absolutely

73
00:07:16,720 --> 00:07:19,920
um

74
00:07:19,920 --> 00:07:25,360
uh right we have so we have a vivv um you know 
formerly of the broad and currently in Jennche  

75
00:07:25,360 --> 00:07:33,040
um uh we have Alex Blowendall who is uh still at 
the broad one of the MIA co-founders uh we have uh  

76
00:07:33,040 --> 00:07:38,480
Alexandrina Gova who was a former MIA co-chair 
currently University of Toronto does a lot of  

77
00:07:38,480 --> 00:07:43,840
single cell uh and uh you know uh neuroscience 
and things like that. And we have uh Gavorg  

78
00:07:43,840 --> 00:07:50,480
Gavor Gavorian currently at Gen Generate Biio 
Medicines. Um all people with some history here in  

79
00:07:50,480 --> 00:07:56,880
the program. And I guess just to get us started, 
um I guess I would just like to for you to each  

80
00:07:56,880 --> 00:08:04,080
sort of tell us a little bit about you know your 
your history uh with the broad with uh MIA and  

81
00:08:04,080 --> 00:08:12,720
the the field in general and you know in light of 
it being such a uh dynamic and you know uh almost  

82
00:08:12,720 --> 00:08:19,520
turbulent last 10 years. The last 10 years have 
been so big for for biio medicine and u uh machine  

83
00:08:19,520 --> 00:08:24,160
learning. this intersection. Uh tell us a little 
bit about something that may have surprised you,  

84
00:08:24,160 --> 00:08:30,400
something that you weren't expecting that 
did happen over the past uh decade or so.  

85
00:08:30,400 --> 00:08:38,480
Um you'll have to turn them on though. There's a 
little switch. Yeah, I'll get started. Please do.  

86
00:08:38,480 --> 00:08:46,240
Um I'm Aviv. I'm currently in Janet and Ro and I 
was um for many years actually here at the Broad.  

87
00:08:46,240 --> 00:08:52,400
uh I was a faculty member here and at MIT I was 
at the use and I founded and actually ran the  

88
00:08:52,400 --> 00:09:01,680
Clarman cell observatory um for several years in 
way back more than 10 years ago we had a process  

89
00:09:01,680 --> 00:09:07,280
at the broad that we called broad next which 
was about that was when broad turned 10 I see  

90
00:09:07,280 --> 00:09:13,680
no I'm nodding some people here are you know have 
been here for a while um we had a process at the  

91
00:09:13,680 --> 00:09:19,360
broad to try and imagine what our next 10 years 
should look like and I had the pleasure to chair  

92
00:09:19,360 --> 00:09:25,200
the scientific frontiers part of it which was was 
like what would our science look like at that in  

93
00:09:25,200 --> 00:09:34,160
the future and in some way that I can't remember 
I ran into Alex and John I just can't remember the  

94
00:09:34,160 --> 00:09:40,400
context we were doing and then we did these things 
with the math department you remember but I can't  

95
00:09:40,400 --> 00:09:45,920
remember exactly why but it was in the context 
of the broad neck then and they had this idea to  

96
00:09:45,920 --> 00:09:52,000
do MIA and my contribution to MIA is that I said 
well we had a little bit of funding for scientific  

97
00:09:52,000 --> 00:09:58,720
frontiers to do things and we paid for the 
bagels. That is actually historically accurate.  

98
00:09:58,720 --> 00:10:06,400
It is literally true. I think we also helped 
a little bit at pulling the math to the front.  

99
00:10:06,400 --> 00:10:13,280
Something that struck me when I watched the video 
is when I came here, people did math on the walls,  

100
00:10:13,280 --> 00:10:17,600
right? Because you couldn't do actually the 
science without it, but they didn't do math on  

101
00:10:17,600 --> 00:10:23,040
the slides. It just wasn't there. When you would 
come to I would do the same when I would prepare  

102
00:10:23,040 --> 00:10:29,920
my talk, I would bend over backward to make it 
go away, right? to simplify, to cartoonify it,  

103
00:10:29,920 --> 00:10:35,280
to explain the intuition, but never to actually 
put the equation because there wouldn't be  

104
00:10:35,280 --> 00:10:40,320
anyone in the audience would resonate with the 
equation or would want to know it. And moreover,  

105
00:10:40,320 --> 00:10:45,600
it was actually a deterrent. It was something 
that would make oh the talk is inaccessible.  

106
00:10:45,600 --> 00:10:50,800
This doesn't matter for me. This is why are you 
showing me that? I think MIA brought the maths  

107
00:10:50,800 --> 00:10:57,680
back on the slide, weirdly enough. Yeah. And and 
that was a huge thing. I was asked what surprised  

108
00:10:57,680 --> 00:11:03,120
me in the last two decades in the last decade 
and I'm going to just use two things. There's  

109
00:11:03,120 --> 00:11:09,280
probably many others but they are very personal 
to me. First of all, I never would have imagined  

110
00:11:09,280 --> 00:11:16,240
I would use the word artificial intelligence with 
a straight face every day and actually mean it  

111
00:11:16,240 --> 00:11:20,240
right. It was always you don't use that word. You 
just say machine learning. It's all just machine  

112
00:11:20,240 --> 00:11:27,920
learning. I treat it very differently now. I think 
that surprised me beyond my wildest wildest dreams  

113
00:11:27,920 --> 00:11:33,440
and I've seen a lot of science dreams come true in 
the last decade. Single cell genomics happening,  

114
00:11:33,440 --> 00:11:38,800
crisper happening and deep learning happening 
in a single year was like the best thing ever.  

115
00:11:38,800 --> 00:11:45,360
But the the AI piece that came later that was 
it was kind of the dream but beyond the dream  

116
00:11:45,360 --> 00:11:50,240
and being able to just say it every day is fun. 
So I just do. So that's one. And the second one  

117
00:11:50,240 --> 00:11:57,600
which is more of a motivator is and that's for 
me related to my own personal move is that I  

118
00:11:57,600 --> 00:12:04,960
never imagined how hard it is to make medicines 
and how satisfying it is at the same time. Doing  

119
00:12:04,960 --> 00:12:10,320
something that's like that failure and yet you 
sometimes get bizarre satisfaction out of is  

120
00:12:10,320 --> 00:12:17,920
unusual. Um but but that's for me the other big 
thing. I do think AI can actually help with that.

121
00:12:22,160 --> 00:12:29,360
real move next. True. Um,

122
00:12:29,360 --> 00:12:35,200
yeah. Um, I just want to say first of all, this is 
it's just um feels a little bit surreal to be at  

123
00:12:35,200 --> 00:12:40,960
this. Yeah. And emotional. I'm having really good 
emotions. No. To to be with all of you and and the  

124
00:12:40,960 --> 00:12:45,280
the folks who have helped build this community. 
And I'll say more about that in a moment. Um,  

125
00:12:45,280 --> 00:12:48,640
but yeah. and and Aiv, you're hard act 
to follow, but it's also amazing to be  

126
00:12:48,640 --> 00:12:54,160
sitting next to you on this panel after all 
this time, you know. Yeah. Yeah. Awesome. Um  

127
00:12:54,160 --> 00:13:00,160
I'm Alex Blumenal. I am an institute scientist 
at the Broad. Um I'm focused uh at this point  

128
00:13:00,160 --> 00:13:04,880
on on interdisciplinary and including external 
collaborations that bring to bear, you know,  

129
00:13:04,880 --> 00:13:10,320
sort of state-of-the-art computational techniques 
on the problems of uh translational genomics and  

130
00:13:10,320 --> 00:13:13,840
genomic medicine. And so right now I'm working 
with the translational genomics group at the  

131
00:13:13,840 --> 00:13:19,680
at MPG the program for medical and population 
genetics. Um where I I for a while I was at the  

132
00:13:19,680 --> 00:13:26,800
data sciences platform um and you know um I spent 
some time in um well let let me go back to the to  

133
00:13:26,800 --> 00:13:32,480
the beginning. So I joined the bro 10 years ago 
um originally um in Ben Neil's group co-founding  

134
00:13:32,480 --> 00:13:41,840
uh the hail project for scalable uh genomic 
analysis with John Bloom who's in the audience. Uh

135
00:13:41,840 --> 00:13:51,040
and um and with cotton seed uh and we were sort 
of all mathematicians getting really interested  

136
00:13:51,040 --> 00:13:55,440
in in you know genomics and genomic medicine. 
the opportunities that that we sort of started  

137
00:13:55,440 --> 00:13:59,920
perceived as we started to get to know people in 
the broad community. Um, and trying to learn a  

138
00:13:59,920 --> 00:14:04,160
lot in a really short amount of time and trying 
to sort of teach ourselves as much genetics and  

139
00:14:04,160 --> 00:14:10,320
computational biology and machine learning, you 
know, as as as we could. And uh you realized  

140
00:14:10,320 --> 00:14:14,560
there were all these amazing people here working 
in these in these areas in their individual groups  

141
00:14:14,560 --> 00:14:19,600
and labs, but that there wasn't a central kind 
of community for people to come together and  

142
00:14:19,600 --> 00:14:23,840
share ideas and teach and learn, you know, teach 
each other and learn from each other uh and and  

143
00:14:23,840 --> 00:14:30,240
spark new collaborations and all of that, you 
know. Uh and so I mean it sort of started as  

144
00:14:30,240 --> 00:14:37,040
uh a reading club and and then it grew quite 
quickly and people stat math reading club smirk.  

145
00:14:38,480 --> 00:14:43,520
That's right. In the data sciences platform and 
then it it sort of grew and and for that there was  

146
00:14:43,520 --> 00:14:47,280
this like FOMO moment where people were like hey 
I heard about this thing why am I not invited and  

147
00:14:47,280 --> 00:14:52,320
then we were like okay we definitely need to you 
know move to the auditorium and make it bigger and  

148
00:14:52,320 --> 00:14:59,120
uh at that point you know and and have bagels and 
have bagels. Yeah. Now there was a desperate need  

149
00:14:59,120 --> 00:15:05,440
for those bagels. Yeah. Alex, can you remember 
some of the first sort of topics that you you  

150
00:15:05,440 --> 00:15:11,120
in this reading club and kind of Yeah, I mean 
we we focused on basics which you know uh for  

151
00:15:11,120 --> 00:15:16,080
for a while uh really foundational stuff like you 
know dimensional reduction and you know different  

152
00:15:16,080 --> 00:15:22,960
different aspects of regression and you know um 
yeah yeah yeah regularization and you know and  

153
00:15:22,960 --> 00:15:27,360
that was that was good for my own learning as 
well you know and and you know to teach us to  

154
00:15:27,360 --> 00:15:31,840
learn twice and all that and I'd done a a lot of 
teaching in in in math mathematics before but but  

155
00:15:31,840 --> 00:15:36,640
to really get into that space in this in this 
sort of interdiciplinary field was a whole new  

156
00:15:36,640 --> 00:15:42,800
thing. Um yeah so a Aviv's sponsorship um not just 
at the level of bagels but at the level of like  

157
00:15:42,800 --> 00:15:48,480
institutional support um actually meant a lot and 
I I really want to call that in a serious way. Um,  

158
00:15:48,480 --> 00:15:51,840
no, it it changed changed everything and made 
it possible for this for this community to grow  

159
00:15:51,840 --> 00:15:57,280
in the way that it did. And um, and so did the 
whole, you know, team of people that that sort of  

160
00:15:57,280 --> 00:16:01,600
formed. I mean, John and I quickly realized that 
we couldn't host a seminar series by ourselves,  

161
00:16:01,600 --> 00:16:06,560
that this was uh, neither scalable nor 
like a good idea. Um, and and to to bring  

162
00:16:06,560 --> 00:16:11,920
in people who represented all these other areas of 
expertise and all these other perspectives and um,  

163
00:16:11,920 --> 00:16:16,960
Alex uh co-chared with me for a long time too 
and and really helped build this community. So,  

164
00:16:16,960 --> 00:16:22,400
it's awesome to be sitting next to you 
and see you again as well, Alex. Um,  

165
00:16:22,400 --> 00:16:26,480
uh, and then, you know, and and so all 
the people on the steering committee um,  

166
00:16:26,480 --> 00:16:30,640
are incredible administrative assistants. Uh, I I 
believe it may have been, correct me if I'm wrong,  

167
00:16:30,640 --> 00:16:34,960
but it may have actually been Leticia's idea. 
She's unfortunately no longer at the Broad, but,  

168
00:16:34,960 --> 00:16:40,960
uh, to to do this 10th anniversary. Wait, 
there's 10 years like we should do a thing,  

169
00:16:40,960 --> 00:16:47,040
you know. So um you know and we've had incredible 
incredible energy and and you know organizational  

170
00:16:47,040 --> 00:16:50,240
support and then also the AV team you know 
Scott and Russell and all the other people  

171
00:16:50,240 --> 00:16:58,560
on that team. So I I want to express a lot of 
gratitude as well. um as I sit here today. Uh

172
00:16:58,560 --> 00:17:06,400
oh, the surprise. Yeah. Um the part that 
something that surprised me I I guess,  

173
00:17:06,400 --> 00:17:17,360
you know, I've I've the not just how challenging, 
but how interesting the challenges are of true  

174
00:17:17,360 --> 00:17:23,680
interdisciplinary collaboration. Um, it's a bit 
of a bit of a a kind of vague vague answer maybe,  

175
00:17:23,680 --> 00:17:29,600
but it's really like the devil's in the details. 
Um, getting people to actually come together. Um,  

176
00:17:29,600 --> 00:17:34,640
realizing that, you know, people don't necessarily 
understand the words you're using or the way  

177
00:17:34,640 --> 00:17:38,480
you're using them and really just digging in 
and finding that shared understanding and and  

178
00:17:38,480 --> 00:17:42,800
cohabiting a problem space together and really 
making progress with people across different  

179
00:17:42,800 --> 00:17:48,480
disciplines toward one shared goal. Um, you know, 
it's it's just it's been totally fascinating.  

180
00:17:48,480 --> 00:17:52,320
I don't think I would have predicted 
um how satisfying it is to spend time  

181
00:17:52,320 --> 00:18:06,800
on that specifically to look to to you know 
as a as a primary challenge um 10 years ago.

182
00:18:06,800 --> 00:18:13,840
Wow. Hi everybody. This this looks like like 
MIA like I remember it. A room full of people.  

183
00:18:13,840 --> 00:18:19,040
Um I don't know why we're not writing on the 
whiteboard even though equations on the slide.  

184
00:18:19,040 --> 00:18:26,240
Yes. Uh but also a lot of whiteboard to to to proc 
to process stuff. Yeah. So yeah, my name is Alex  

185
00:18:26,240 --> 00:18:33,360
Gova. I'm so happy to be here. Um it brings a 
lot of emotions to be back and I I don't know  

186
00:18:33,360 --> 00:18:41,120
if I can speak to the whole 10 years of MIA or 
ML and biology or AI and biology because actually  

187
00:18:41,120 --> 00:18:47,680
came to this field maybe about seven eight years 
ago and it was through maybe a chance encounter  

188
00:18:47,680 --> 00:18:54,400
with Alex who came to give a seminar talk at at 
my PhD department which was BEu math and stats  

189
00:18:54,400 --> 00:19:00,720
department. So I was doing theoretical uh what 
we call machine learning now uh at that time not  

190
00:19:00,720 --> 00:19:06,160
biology uh but was thinking maybe not about the 
next 10 years of biology and AI. I was thinking  

191
00:19:06,160 --> 00:19:16,320
about the next 10 years of me in science and was 
um really um just feeling a pull to use that rigor  

192
00:19:16,320 --> 00:19:23,440
that I like to work on but to channel it in both 
pedagogical directions and also in applications.  

193
00:19:23,440 --> 00:19:28,400
And I had a passion for biology since high school 
but I wasn't working working in it actively. And  

194
00:19:28,400 --> 00:19:34,400
so when I met Alex you know another mathematician 
who was now working at the broad and of course I  

195
00:19:34,400 --> 00:19:40,560
knew the broad I had walked by it so many times 
and knew about the amazing science but realizing  

196
00:19:40,560 --> 00:19:46,080
that there can be a use for someone with my skill 
set at the place like that was really inspiring  

197
00:19:46,080 --> 00:19:52,400
and MIA already existed. Alex let me know about 
it. I started attending it, met John and Alex. And  

198
00:19:52,400 --> 00:19:58,640
then just one thing led to another. I I became a 
postock here and was one of those people that was  

199
00:19:58,640 --> 00:20:05,840
um recruited to to help with the organization of 
MIA. And let me tell you now that I've been away  

200
00:20:05,840 --> 00:20:12,560
from the broad for about a year and a half at my 
position in Toronto. I I can really appreciate how  

201
00:20:12,560 --> 00:20:18,960
special MIA is and how special the broad is for 
being able to cultivate an environment like that.  

202
00:20:19,920 --> 00:20:28,320
It's really unique. Um, okay. Now, briefly, 
something that has surprised me. Um, yeah,  

203
00:20:28,320 --> 00:20:32,640
perhaps as someone trained in math 
where things follow logically and  

204
00:20:32,640 --> 00:20:39,120
usually always lead to the same conclusion 
every time, um, it definitely surprises me  

205
00:20:39,120 --> 00:20:46,320
that the same data in the hands of different 
people could lead to very different results.

206
00:20:47,680 --> 00:20:54,800
I don't know if artificial intelligence can help 
us with that, but we'll see. The same person on a  

207
00:20:54,800 --> 00:21:08,640
different day that that right hopefully not. Thank 
you. Yeah, I know. Hi everybody. My name is Gorg  

208
00:21:08,640 --> 00:21:16,720
Gregorian. I'm a co-founder and CTO at Generate 
by Medicines. Um, so my relationship to MIT, uh,  

209
00:21:16,720 --> 00:21:23,120
I I've never been at the Broad, but I was I was a 
PhD student, um, and in the biology department in  

210
00:21:23,120 --> 00:21:29,280
Amy Keading's lab. Uh, so I was more the have 
walked by this building many times and then,  

211
00:21:29,280 --> 00:21:36,720
you know, have been inside since then many times. 
Um, but yeah, I think I MIT is where, you know,  

212
00:21:36,720 --> 00:21:43,520
many ways my scientific journey started. I was a 
PhD student here. I I studied protein design uh at  

213
00:21:43,520 --> 00:21:48,880
a time when we won't say how many years ago it was 
but it was a number of years ago at the time where  

214
00:21:48,880 --> 00:21:56,160
the field was really just starting um and um and 
it was really fun um and I was you know schooled  

215
00:21:56,160 --> 00:22:02,480
in the in the frame of thought that was the 
predominant one at the time uh for understanding  

216
00:22:02,480 --> 00:22:07,520
molecules which is the what I call the bottom up 
you know the the standard reductionist approach  

217
00:22:07,520 --> 00:22:13,040
where we characterize details of molecular 
interactions, atomistic interactions and then  

218
00:22:13,040 --> 00:22:22,080
integrate them forward to make predictions. 
Um I went on to um do a posuck in in in in  

219
00:22:22,080 --> 00:22:29,920
at University of Pennsylvania build similarly in 
protein design and started my lab uh at Dartmouth.  

220
00:22:29,920 --> 00:22:36,480
Now the thing that I think has been surprising 
first of all I think it's just been such an  

221
00:22:36,480 --> 00:22:43,120
amazing time to be probably in almost any science 
but but especially in sort of generally molecular  

222
00:22:43,120 --> 00:22:49,840
science and that that connection between molecules 
and biology because the methodology happened the  

223
00:22:49,840 --> 00:22:55,200
kind of the experimental methodology to look 
to see to measure to understand at scale sort  

224
00:22:55,200 --> 00:23:01,120
of happened at the same time as the methodology 
to integrate over all those data and and come up  

225
00:23:01,120 --> 00:23:05,520
with some you know interesting generalizations. 
All of that happened at the same time. So when I  

226
00:23:05,520 --> 00:23:15,120
actually think back to my PhD days, all the things 
that we used to muse and dream about as surely  

227
00:23:15,120 --> 00:23:21,040
unrealistic, but boy wouldn't it be amazing if 
that were true actually happened. Right? So that's  

228
00:23:21,040 --> 00:23:28,480
that's maybe like broadly surprise number one. 
I I very much relate to what Aviv said about AI.  

229
00:23:28,480 --> 00:23:35,600
I mean when we started generate you know I was it 
was a hard and fast rule like no you know piece  

230
00:23:35,600 --> 00:23:41,920
of PR or anything that came out of generate allow 
was allowed to use AI. Well I I people wanted to  

231
00:23:41,920 --> 00:23:48,000
say because we had smarter PR people than I than 
I am and they wanted to use AI. They wanted to get  

232
00:23:48,000 --> 00:23:54,000
on that train and I was like no way you can't do 
it. It's not scientific. We're scientific here.  

233
00:23:54,000 --> 00:23:58,800
You know, other companies can say whatever they 
want, but we're going to say machine learning.  

234
00:23:58,800 --> 00:24:04,000
And it's actually this year, Aviv, I don't know if 
it happened earlier for you, but it's I eventually  

235
00:24:04,000 --> 00:24:12,400
gave in. I Yeah. No, I gave in late. I was when it 
was like, okay, I think this doesn't this doesn't  

236
00:24:12,400 --> 00:24:18,880
make matter anymore. You can call it whatever we 
you want. The point is it's amazing. Um so that so  

237
00:24:18,880 --> 00:24:24,480
I relate to that surprise but I think that the 
deep scientific surprise for me and it kind of  

238
00:24:24,480 --> 00:24:29,840
actually is the through line for for my career 
my scientific career why I started generate and  

239
00:24:29,840 --> 00:24:35,280
all of it when I went to my pos so I learned kind 
of like the molecular view of the world in my PhD  

240
00:24:35,280 --> 00:24:41,520
and I went for my postoc and Bill Degrrad he's 
a trained chemist so he was very interested in  

241
00:24:41,520 --> 00:24:46,880
protein design and computational techniques but he 
could also just look at structures and understand  

242
00:24:46,880 --> 00:24:53,600
stuff about them, right? In a way that I was just 
completely blown away. And it got me that that was  

243
00:24:53,600 --> 00:24:58,960
the thing that got me curious about well clearly 
he's not computing Vanwell's energy in his head,  

244
00:24:58,960 --> 00:25:04,000
right? He's just looking at this stuff and 
somehow from experience from having seen other  

245
00:25:04,000 --> 00:25:10,800
things before coming up with fairly quantitative 
ways to think, describe and predict these things  

246
00:25:10,800 --> 00:25:16,000
and they actually work more frequently than my 
calculations do. So I think that kind of where  

247
00:25:16,000 --> 00:25:23,360
that seed of like huh is there maybe a different 
way to tackle this sort of set of problems. Um you  

248
00:25:23,360 --> 00:25:30,000
know I was always into rigor you know data fitting 
was was a bad word right like I was not into that  

249
00:25:30,000 --> 00:25:35,840
but but at the same time I couldn't really I 
couldn't I couldn't otherwise explain what I  

250
00:25:35,840 --> 00:25:41,360
was seeing. So that got me down that path. So I 
think the surprise was that when I finally said  

251
00:25:41,360 --> 00:25:45,440
okay well let me see what you can accomplish 
with a datadriven model you know can can you  

252
00:25:45,440 --> 00:25:51,360
build some type of a you know rigorous statistical 
methodology maybe doesn't assume anything about  

253
00:25:51,360 --> 00:25:58,800
atoms or understanding of physics and nevertheless 
works I think the surprise was that out of the box  

254
00:25:58,800 --> 00:26:04,720
in immediately that worked better than anything 
that I had done you know previously in my entire  

255
00:26:04,720 --> 00:26:10,080
kind of scientific career now it didn't work a lot 
better. But this is where the benefit of having  

256
00:26:10,080 --> 00:26:16,480
hit your head against that wall for for decades 
came in because what I saw I already knew this is  

257
00:26:16,480 --> 00:26:22,000
the way to tackle this problem and then came many 
written grants that people didn't understand and  

258
00:26:22,000 --> 00:26:27,200
like just frustration why isn't thing things why 
aren't things moving faster which is why you know  

259
00:26:27,200 --> 00:26:33,920
I eventually founded the company and that kind of 
that's how I scratched that itch but I think the  

260
00:26:33,920 --> 00:26:39,040
extent to which that sort of crazy idea actually 
turned out to be true and sort of really flipped  

261
00:26:39,040 --> 00:26:45,200
how you think about complex systems. At least for 
me, I think that's that's my big surprise. That's  

262
00:26:45,200 --> 00:26:50,240
really that's that's really fascinating. You know, 
this from all of you getting the sense that oh,  

263
00:26:50,240 --> 00:26:54,560
it's amazing that it works, but from Alex 
and Viv, there's also the side of, you know,  

264
00:26:54,560 --> 00:26:59,760
there's there's deep challenges to getting getting 
it to execute. And it seems that you guys started  

265
00:26:59,760 --> 00:27:05,280
MIA with this um you know, intuition that learning 
from data was the thing that we needed to learn.  

266
00:27:05,280 --> 00:27:10,400
Um, and as a community, we're all really focused 
on, you know, thinking about what are the skills  

267
00:27:10,400 --> 00:27:15,120
that are going to be relevant for the for the 
future and how how we set ourselves up for those.  

268
00:27:15,120 --> 00:27:19,280
Um, you know, it' be great to hear, you know, what 
what do you think those skills are going to be for  

269
00:27:19,280 --> 00:27:23,840
the next 10 years? Is there a core set of things 
that we all need to all need to do or is there,  

270
00:27:23,840 --> 00:27:28,240
you know, how do we differentiate ourselves 
in this kind of turbulent time where stuff is  

271
00:27:28,240 --> 00:27:34,800
really exciting and works really well, but also 
there are some genuine challenges to be solved.  

272
00:27:34,800 --> 00:27:41,280
I I can try and and comment on this. I will start 
by saying I'm not the most planful person. I think  

273
00:27:41,280 --> 00:27:46,720
people retrofit the story after it happened is 
a very typical thing. It's also done by the way  

274
00:27:46,720 --> 00:27:51,040
in how people write papers. We did this and 
then we did that. Whereas you know for a fact  

275
00:27:51,040 --> 00:27:55,440
you didn't do this and then had a brilliant idea 
and then did that. You did this for one reason,  

276
00:27:55,440 --> 00:27:59,760
you did that for another reason. When you put 
the paper together, you finally see that this  

277
00:27:59,760 --> 00:28:05,760
is actually related to that. It's fine. It's just 
not a historical account. But um I think one one  

278
00:28:05,760 --> 00:28:10,960
thing to really realize is that you can be in too 
many in two different points in science and we're  

279
00:28:10,960 --> 00:28:17,840
decidedly in one kind versus the other. Most of 
the time you're within an established situation.  

280
00:28:17,840 --> 00:28:22,640
So there's kind of a foundation of knowledge, a 
way of doing things. It's not just the knowledge,  

281
00:28:22,640 --> 00:28:27,760
it's also the way in which you think in the 
science and the way in which you operate. what  

282
00:28:27,760 --> 00:28:32,640
is considered to be an interesting answer which 
by the way is dramatically different from field  

283
00:28:32,640 --> 00:28:38,960
to field. Physicists and biologists have exactly 
opposite views on phenomen phenomenology which  

284
00:28:38,960 --> 00:28:45,520
physicists love and biologists don't. They just 
don't and that's it. You can't really say why  

285
00:28:45,520 --> 00:28:51,920
but they don't and the others do. So that kind 
of distinction really matters. That's if you're  

286
00:28:51,920 --> 00:28:58,000
in a field in it in its more established moment, 
all of these things are there. And then questions  

287
00:28:58,000 --> 00:29:04,720
like what are the skills for are really they have 
a very low uncertainty related to them and that's  

288
00:29:04,720 --> 00:29:11,200
most of the time and then once in a while you're 
in a moment where the field does not exist even  

289
00:29:11,200 --> 00:29:17,040
no one has had it before. So if you wind back 
to like 2012 and single cell genomics there  

290
00:29:17,040 --> 00:29:22,640
wasn't single cell genomics. It didn't exist. So 
the first time you got data, there was nothing.  

291
00:29:22,640 --> 00:29:27,040
There was no skill you could learn or paper you 
could read. You kind of had to hack it together  

292
00:29:27,040 --> 00:29:31,040
and cobble it. And sometimes seven years later 
when people still do it that way, you're like,  

293
00:29:31,040 --> 00:29:38,000
it was just some idea. It wasn't even a good one, 
right? It was just some way to get ahead with it.  

294
00:29:38,000 --> 00:29:45,040
So in my opinion, we're in that moment, but way 
more big time than in anything else that I have  

295
00:29:45,040 --> 00:29:49,600
seen in my my years in science. And I agree with 
Gavfork. That's why I said for me 2012 was the  

296
00:29:49,600 --> 00:29:56,000
dream 2011 2012 actually it started in 2011 a lot 
of things were already happening there in 2012  

297
00:29:56,000 --> 00:30:01,600
you see them out right either your own or that 
those of others and you're like all of my wishes  

298
00:30:01,600 --> 00:30:06,880
just came through I just don't know how that is 
possible and it takes 10 years to see how they all  

299
00:30:06,880 --> 00:30:14,000
play out but you know okay so this is the moment 
now just like way more amplified I think like in  

300
00:30:14,000 --> 00:30:19,680
several ways and fundamentally because the way of 
thinking about science is changing dramatically  

301
00:30:19,680 --> 00:30:23,840
and you're going to make it happen. And you're not 
going to make it happen by knowing ahead of time  

302
00:30:23,840 --> 00:30:27,680
what are the skills because those skills have 
not been invented yet. You're going to invent  

303
00:30:27,680 --> 00:30:33,920
them by actually doing it. If you go back to other 
moments in science that were like that. It was the  

304
00:30:33,920 --> 00:30:38,800
same. There weren't skills and techniques 
and approaches. People kind of, you know,  

305
00:30:38,800 --> 00:30:43,520
they hacked them together and they existed because 
they had a passion for the problem and a sense of  

306
00:30:43,520 --> 00:30:50,000
the opportunity. And the human mind is still a 
pretty unbelievably amazing thing. It just comes  

307
00:30:50,000 --> 00:30:59,600
up with stuff way out of distribution. So it's to 
me that's that's where you want to anchor in more  

308
00:30:59,600 --> 00:31:05,440
than in the specifics. And you want to remember 
that just like the algorithms learn, so does the  

309
00:31:05,440 --> 00:31:10,160
human brain. And if there's some skill that you 
will need, you're smart. You're going to go ahead  

310
00:31:10,160 --> 00:31:14,880
and you're going to learn it. And the things 
we do today are definitely not what I learned  

311
00:31:14,880 --> 00:31:20,720
when I was in uh university. I'm sure not what you 
learned when you were in undergrad. And it doesn't  

312
00:31:20,720 --> 00:31:26,000
matter. Math will carry you through. By the way, 
that's one of the one of the really really awesome  

313
00:31:26,000 --> 00:31:33,360
things about it. So kind of everything is going 
to change. I'll give you two examples of why I  

314
00:31:33,360 --> 00:31:39,440
mean it's going to change. So one is in biological 
sciences people still look for the answer in the  

315
00:31:39,440 --> 00:31:45,680
data. in the data that they actually measured 
to the vast extent. And we're moving to a world  

316
00:31:45,680 --> 00:31:50,960
where you look for the answer in the model, not 
in the data. For biologist, this is a dramatic  

317
00:31:50,960 --> 00:31:57,520
shift. So if you look at single cell genomics, 
for single cell genomics, you want to find a cell,  

318
00:31:57,520 --> 00:32:02,960
go profile a lot of cells, you'll find a cell. No 
one has found yet a cell out of a model. Not one,  

319
00:32:02,960 --> 00:32:06,160
as far as I know. Maybe they have, but they 
haven't published it. Or maybe they've published  

320
00:32:06,160 --> 00:32:11,520
it and I haven't read it or the AI didn't read 
it for me. But in any case, the the you discover  

321
00:32:11,520 --> 00:32:18,240
it by its existence in front of you, measured 
from the system. If the models really get there,  

322
00:32:18,240 --> 00:32:23,920
they will say, well, there needs to exist a cell 
type that's like that and it's going to look like  

323
00:32:23,920 --> 00:32:29,680
this and it's never been measured. That means the 
model actually found something interesting for me.  

324
00:32:29,680 --> 00:32:34,240
It didn't just organize, represent, allow me to 
do all sorts of parlor tricks. It did something  

325
00:32:34,240 --> 00:32:38,400
fundamental. If you live in a world like that, 
you think differently in science, in biological  

326
00:32:38,400 --> 00:32:44,160
sciences specifically and maybe other things will 
change too. Labs are going to change dramatically.  

327
00:32:44,160 --> 00:32:50,240
The way labs and computation will change. So a 
computational scientist will have to will will  

328
00:32:50,240 --> 00:32:56,080
have as part of their algorithm. They will have 
the experiment. The experiment will not generate  

329
00:32:56,080 --> 00:33:02,080
data on which you will work. The experiment will 
be part of the algorithm, the technique that you  

330
00:33:02,080 --> 00:33:08,240
develop. It won't be like this and then that. 
Those kinds of things are a different way of  

331
00:33:08,240 --> 00:33:14,960
thinking and it takes time to adapt to it and it's 
it's done as a collective. No human does it alone.  

332
00:33:14,960 --> 00:33:22,960
So that's that's to me the skills uh question. 
Maybe if I can add to that I I I completely  

333
00:33:22,960 --> 00:33:29,360
agree with what Aviv said and and um and what 
an eloquent way to express it. Maybe one thing  

334
00:33:29,360 --> 00:33:36,000
I would add is a I hear this question a lot a lot, 
right? And I I think a little bit of the question  

335
00:33:36,000 --> 00:33:41,280
behind the question, it's not everything, but a 
little bit behind it is um these tools just came  

336
00:33:41,280 --> 00:33:47,280
out and they're like really good. They're really 
smart. They know a lot more than I do. They can do  

337
00:33:47,280 --> 00:33:52,880
a lot more than I can do. What what is it that we 
need to learn anymore? Do we still have to teach,  

338
00:33:52,880 --> 00:33:57,520
you know, you what about me? Do we still have to 
teach kids? Like how does that change education?  

339
00:33:57,520 --> 00:34:02,400
And I think um you know maybe just to step back 
and think about that for just a minute. Obviously,  

340
00:34:02,400 --> 00:34:06,960
how do we kind of typically instruct science, 
right? There is the skills and the knowledge,  

341
00:34:06,960 --> 00:34:11,760
right? That we we we teach people for a while, 
right? We kind of tell them about the ideal gas  

342
00:34:11,760 --> 00:34:16,000
that this thing exists. This is the equation. 
Maybe we derive it for them. That kind of  

343
00:34:16,000 --> 00:34:21,200
understanding, we tell them how real gases behave. 
Then we teach them how to integrate by parts and  

344
00:34:21,200 --> 00:34:25,280
give them a thousand examples. So, they get really 
good at. So, that that's a really important part  

345
00:34:25,280 --> 00:34:31,520
right now of becoming a scientist. And then later 
when you sort of prove yourself as an artisan  

346
00:34:31,520 --> 00:34:35,840
that you can do these things that you have the 
knowledge and you know how to use the tools later  

347
00:34:35,840 --> 00:34:41,600
in the current curriculum you get to think about 
new questions think about science because now  

348
00:34:41,600 --> 00:34:45,680
you've got you you've got the capacity and that's 
what you know I don't even know what the right  

349
00:34:45,680 --> 00:34:49,600
word for it is but let's call it like the art 
of doing science right that's where you develop  

350
00:34:49,600 --> 00:34:55,360
your taste that's where you kind of develop your 
algorithm for how you put a problem into parts  

351
00:34:55,360 --> 00:35:01,760
and solve it I think people are really worried 
about what about the skills like which skills do  

352
00:35:01,760 --> 00:35:07,040
we teach do we still teach people to integrate 
you know how all that stuff and I think that's  

353
00:35:07,040 --> 00:35:12,320
maybe getting ahead of the ourselves a little bit 
like that stuff will shake itself out for instance  

354
00:35:12,320 --> 00:35:18,800
we still teach kids to calculate many many years 
after calculators are you know exist and we they  

355
00:35:18,800 --> 00:35:23,040
will always do better than humans at every kind 
of calculation but we we turned it turned out  

356
00:35:23,040 --> 00:35:28,480
it's still important to have people have intuition 
about quantities piece because that that's really  

357
00:35:28,480 --> 00:35:32,640
helpful. Um I think something like that will 
shake out in other things as well. But what is  

358
00:35:32,640 --> 00:35:38,880
happening is because the of these tools I think 
we can move up that that artistic part of science  

359
00:35:38,880 --> 00:35:44,320
much earlier because maybe you don't actually 
need to like prove yourself out on all aspects  

360
00:35:44,320 --> 00:35:49,840
of every every passive piece of skill set to 
actually start engaging with real questions. Maybe  

361
00:35:49,840 --> 00:35:56,480
that's actually what you know LLMs and automatic 
reasoning kind of systems allow you to do is to  

362
00:35:56,480 --> 00:36:02,640
introduce for for folks for students the ability 
to engage with real interesting questions much  

363
00:36:02,640 --> 00:36:07,520
earlier and teach them to do the real research the 
real question asking much earlier at least that's  

364
00:36:07,520 --> 00:36:14,800
that's kind of one one thing I would add I guess 
I just want to um give a gentle counterpoint which  

365
00:36:14,800 --> 00:36:19,280
is just to maybe strike a note of caution not to 
skip the basics because I think it can be tempting  

366
00:36:19,280 --> 00:36:25,360
to fly with these these new techniques and these 
giant data sets and you know you know um into this  

367
00:36:25,360 --> 00:36:31,520
space which I I wholeheartedly agree u with a 
gavor um it's it's that's where the future is  

368
00:36:31,520 --> 00:36:37,840
uh and but at the at the same time um I think it's 
important to understand what you're doing and just  

369
00:36:37,840 --> 00:36:44,240
like we teach kids to multiply and you know and do 
long division I think it's important to understand  

370
00:36:44,240 --> 00:36:48,960
you know basic what we really do still think 
of as ML models what nobody would call AI you  

371
00:36:48,960 --> 00:36:52,880
and and to understand what it means to train and 
fit these models and what is actually going on  

372
00:36:52,880 --> 00:36:57,760
when you when you fit these models and you know 
so I just you know if if the question is about  

373
00:36:57,760 --> 00:37:02,800
students then I do I do think it's important to 
to take some time to not skip that you know I  

374
00:37:02,800 --> 00:37:06,960
just want to say to the record that I fully agree 
I didn't actually answer the question for say an  

375
00:37:06,960 --> 00:37:12,960
undergrad student high school student even in 
the grad student phase you're still kind of in  

376
00:37:12,960 --> 00:37:19,120
that transition I think the audience a lot of it 
is a little a little after that and still asks,  

377
00:37:19,120 --> 00:37:25,440
but I I think at every point in your career, there 
are some basics that you actually don't know.  

378
00:37:25,440 --> 00:37:32,720
Yeah. Or that you've known and you kind of started 
forgetting. Totally. And it's kind of you notice  

379
00:37:32,720 --> 00:37:39,840
it when you have kids and they're at a certain age 
and you're like, I totally know this stuff. And  

380
00:37:39,840 --> 00:37:45,520
you're like, I don't remember. I haven't touched 
it in many, many years. this angle of math or  

381
00:37:45,520 --> 00:37:52,080
this aspect of of another area of science or even 
something like in you know human history that you  

382
00:37:52,080 --> 00:37:57,280
knew you really knew you understood you had a deep 
understanding of it even and it's kind of like you  

383
00:37:57,280 --> 00:38:02,880
it's gotten fuzzy. So first of all you can very 
easily re-enter into it. it's actually here.  

384
00:38:02,880 --> 00:38:06,720
You kind of have to like try it a couple of time 
then it's like click and it's like yeah it's like  

385
00:38:06,720 --> 00:38:15,200
riding the bike piece and having that inside you 
shapes your thinking every day. So I agree with  

386
00:38:15,200 --> 00:38:23,200
the calculator in that versus the calculations 
the basics and so on and also you have to remember  

387
00:38:23,200 --> 00:38:28,480
this is really a formidable computational thing 
and you need to feed things like that. You have to  

388
00:38:28,480 --> 00:38:34,400
train them. You have to you pre- trainin them. You 
have to fine-tune them. All those things we do all  

389
00:38:34,400 --> 00:38:39,840
the time. That's what makes them go. So I am I'm 
completely with you. I was just trying to give one  

390
00:38:39,840 --> 00:38:44,720
other thought that occurred to me when you were 
talking. I think the question is 50% fear like  

391
00:38:44,720 --> 00:38:48,960
what's going to happen to me. But there's also 
going to be a lot of frustration along the road.  

392
00:38:48,960 --> 00:38:54,000
I think you mentioned frustration. Frustration is 
an incredibly positive emotion if you think about  

393
00:38:54,000 --> 00:38:58,560
it from the opportunity side. It's like you get 
frustrated when you think something can happen  

394
00:38:58,560 --> 00:39:03,440
but it's not happening. Not happening makes you 
unhappy. So that's the negative side but something  

395
00:39:03,440 --> 00:39:08,960
can happen. So when you get frustrated go and feel 
that something can happen. And as Gavorg said he  

396
00:39:08,960 --> 00:39:15,600
just went and did something a little different and 
it all worked out really nicely. Yeah. Yeah. I I  

397
00:39:15,600 --> 00:39:21,280
agree with all of the points made so far. And just 
maybe to put it more abstractly, I would say the  

398
00:39:21,280 --> 00:39:27,760
core skills that I think are important and will 
continue to be important are to think creatively,  

399
00:39:27,760 --> 00:39:34,400
collaboratively, and critically. So you have to 
be able to come up with questions and creative  

400
00:39:34,400 --> 00:39:39,360
ways to solve them with other people. But you also 
have to be able to think critically. And this is  

401
00:39:39,360 --> 00:39:46,880
a lot of where that rigor is is really important 
to know whether um a solution is is worthwhile.  

402
00:39:47,600 --> 00:39:52,000
Um I think I have one last piece which is about 
communication. I I think you should you know  

403
00:39:52,000 --> 00:39:58,720
that part deserves to be taken seriously as a 
as a first class um skill uh especially in in  

404
00:39:58,720 --> 00:40:05,120
this interdisciplinary context. So, you know, um, 
practice explaining your ideas to people who don't  

405
00:40:05,120 --> 00:40:10,960
share your background and, you know, realize when 
they don't use the words the same way you do or,  

406
00:40:10,960 --> 00:40:15,040
you know, and and this is related to teaching, 
but it it goes all the way through to to,  

407
00:40:15,040 --> 00:40:20,240
you know, the cutting edge research. That's a 
very familiar for me as a a physicist who had  

408
00:40:20,240 --> 00:40:24,880
to talk to a lot of biologists, especially in my 
PhD and posttock. In light of time, I think we're  

409
00:40:24,880 --> 00:40:30,400
going to move to the next question. But we've I 
have to add one more thing because Alex Alex by  

410
00:40:30,400 --> 00:40:38,640
now said Alex Alex that Alex said by now twice on 
rigger. There's going to be so much vibe and fuzz  

411
00:40:38,640 --> 00:40:46,080
actually in the world now because of AI AI. Yeah. 
A lot of it and people could seem like they know  

412
00:40:46,080 --> 00:40:52,000
what they're talking about. That's actually going 
to become more and more and more of a problem.  

413
00:40:52,000 --> 00:40:56,320
It's an it's an empowering thing too because 
you can kind of manage around things you don't  

414
00:40:56,320 --> 00:41:04,000
exactly know with help which is nice but rigor is 
going to be extraordinarily important and I I at  

415
00:41:04,000 --> 00:41:08,400
least have never found something that teaches 
rigor in the same way that math does which is  

416
00:41:08,400 --> 00:41:14,320
why I think that fundamental is is extraordinarily 
important but it's literally needed in order also  

417
00:41:14,320 --> 00:41:21,600
to serve as a counterpoint to something that will 
become almost there fuzzily there vibe like there  

418
00:41:21,600 --> 00:41:28,720
but it's not actually there. So that's that's 
a responsibility I think for this community.  

419
00:41:28,720 --> 00:41:33,440
Absolutely. Um so we've been talking a little bit 
about you know questions like what are we going  

420
00:41:33,440 --> 00:41:37,680
to be what are the questions going to be? What are 
the questions that people have and as researchers  

421
00:41:37,680 --> 00:41:42,560
we're always asking questions and you know there 
have been a lot of interesting questions that have  

422
00:41:42,560 --> 00:41:47,840
been at the seed of these efforts that sort 
of brought out new fields like single cell  

423
00:41:47,840 --> 00:41:52,320
etc. you know almost denovo you know we we ask 
these questions and we realize we don't know the  

424
00:41:52,320 --> 00:41:59,360
answer and that pushes us forward and I think that 
uh we're sort of at an interesting space in uh  

425
00:41:59,360 --> 00:42:06,000
machine learning and biio medicine right now where 
uh number one new questions are sort of you know  

426
00:42:06,000 --> 00:42:14,800
coalescing uh still sort of in coate um but at the 
same time they are not really as uh uh localized  

427
00:42:14,800 --> 00:42:19,600
just in the academy as they might have been 
so like you in Genentech you are producing you  

428
00:42:19,600 --> 00:42:24,480
know and you know a lot of these pharmaceutical 
companies are publishing cutting edge machine  

429
00:42:24,480 --> 00:42:30,400
learning papers uh in ways that I don't think 
would have been you know uh understood or expected  

430
00:42:30,400 --> 00:42:36,320
uh 10 years ago and I think you know how I guess 
my next question is going to be how are we going  

431
00:42:36,320 --> 00:42:40,000
to what are the kinds of questions that we're 
going to be addressing in this new context and  

432
00:42:40,000 --> 00:42:47,440
how are we going to be splitting the effort for 
addressing these questions between the you know  

433
00:42:47,440 --> 00:42:54,960
the academic community, you know, medicine itself 
like medical schools and also industry um and you  

434
00:42:54,960 --> 00:43:02,800
know, how are these sort of new configurations 
going to be um pushing forward the science?

435
00:43:02,800 --> 00:43:08,720
Maybe I can jump in um this time. So yeah, I 
think maybe I I'll answer the first one and I  

436
00:43:08,720 --> 00:43:13,360
think there's a piece of an answer for the second 
one. So which is you know what questions are going  

437
00:43:13,360 --> 00:43:19,040
to be important? I mean so first of all like 
I think it goes back to Socrates if I remember  

438
00:43:19,040 --> 00:43:23,520
correctly right asking the right question is 
half the answer or something like that in a  

439
00:43:23,520 --> 00:43:33,120
Socratic method and and so I'm a big big believer 
in that um but my interpretation of that concept  

440
00:43:33,120 --> 00:43:39,120
is is less that you know life is about winning the 
question asking lottery that like you just have to  

441
00:43:39,120 --> 00:43:45,200
ask the right question and then everything will 
happen. Um because I think actually the limiting  

442
00:43:45,200 --> 00:43:52,000
resource especially today is not the questions. 
I mean they can be come up with almost at scale  

443
00:43:52,000 --> 00:43:56,480
nowadays and I'm not just saying everything has 
to be done by AI but AI can help with that. We  

444
00:43:56,480 --> 00:44:03,920
can ask questions. I think the limiting resource 
is just curiosity is kind of a good question is  

445
00:44:03,920 --> 00:44:09,280
a question that you're really curious about 
enough that you're going to you know going  

446
00:44:09,280 --> 00:44:15,840
to keep digging and and get to an answer. Um I 
think it's very difficult to predict a priori  

447
00:44:15,840 --> 00:44:21,280
what's really going to move science. I mean, 
you know, we can have some predictive ability,  

448
00:44:21,280 --> 00:44:28,800
but I think it's a lot easier to ascertain whether 
you're curious enough about something than its  

449
00:44:28,800 --> 00:44:35,920
ultimate impact. And I would I would definitely 
index a lot on curiosity and make sure that what  

450
00:44:35,920 --> 00:44:41,120
you're asking you really, really want to find out. 
Um, and there's a good chance if you do it well,  

451
00:44:41,120 --> 00:44:47,760
it'll be impactful and useful. And I would care 
less my my personal opinion. Maybe others have a  

452
00:44:47,760 --> 00:44:52,880
different algorithm, but I would care less about 
predicting, you know, the right question to ask to  

453
00:44:52,880 --> 00:44:59,040
land the right paper to land the right, you know, 
next job and so on. I would actually mostly mostly  

454
00:44:59,040 --> 00:45:04,560
really index on on curiosity. Now, in terms of 
how so maybe that there's a there's a thing in  

455
00:45:04,560 --> 00:45:10,640
there about how to divide. I mean, do we have 
to divide? I mean, if if we go by curiosity as  

456
00:45:10,640 --> 00:45:16,800
a as a as a principle, then I think a company 
should be answering questions they're really  

457
00:45:16,800 --> 00:45:21,360
curious about and academia should be answering 
questions they're really curious about. Well,  

458
00:45:21,360 --> 00:45:26,080
let's not divide, maybe collaborate. Yeah. 
Yeah. There's a lot of industry academic  

459
00:45:26,080 --> 00:45:29,760
collaborations especially blowing up in the 
farm space right now. Yeah. Yeah. I mean,  

460
00:45:29,760 --> 00:45:36,080
I think that that's amazing. I I I just think that 
there's a moment now where you know particularly  

461
00:45:36,080 --> 00:45:40,960
around biology and scalable techniques applied 
to biology and insights and so on where you know  

462
00:45:40,960 --> 00:45:46,720
let's be honest profit motive on the one hand 
foundational understanding on the other hand are  

463
00:45:46,720 --> 00:45:52,160
are just kind of converging right and like we all 
want the same thing probably for different reasons  

464
00:45:52,160 --> 00:45:58,240
uh or a mixture of reasons but let's let's use 
that to our advantage and correct some problems.

465
00:46:04,640 --> 00:46:11,520
share some reflections. I I think the really 
big questions tend to stay for a really really  

466
00:46:11,520 --> 00:46:18,080
long time. So it's more like they're more specific 
formulation shifts and often that form sometimes  

467
00:46:18,080 --> 00:46:22,400
that formulation shifts for interesting 
conceptual reasons. People literally do not  

468
00:46:22,400 --> 00:46:26,480
see something is there until they know it's 
there and then they ask questions about it.  

469
00:46:26,480 --> 00:46:31,360
Nice example is cells. Before the 1600s, we 
didn't know cells existed. No one would ask  

470
00:46:31,360 --> 00:46:34,720
questions about something they don't know 
is there. But once you know it's there,  

471
00:46:34,720 --> 00:46:39,360
people ask a lot of questions about it. Same 
is true for atoms for that matter. It's not  

472
00:46:39,360 --> 00:46:45,040
unique to cells. I just like them. So that 
type of thing you don't get that often. And  

473
00:46:45,040 --> 00:46:48,880
then the questions tend to last for a really 
really long time. For many of those things,  

474
00:46:48,880 --> 00:46:53,760
they tend to end up being in science complex 
enough that we don't solve them that quickly.  

475
00:46:53,760 --> 00:46:59,680
There's a second layer which is sort of how you 
formulate the question and that often shifts but  

476
00:46:59,680 --> 00:47:05,440
what you can actually do rather than the concepts 
that you have and when you can start doing  

477
00:47:05,440 --> 00:47:10,160
something you start asking questions that fit 
the things that you can do. So that's the way in  

478
00:47:10,160 --> 00:47:17,600
which in which those things shift but many of our 
questions kind of kind of stay really the same.  

479
00:47:18,480 --> 00:47:25,200
I would encourage people to think maybe about two 
two perspectives that could be useful. Sometimes  

480
00:47:25,200 --> 00:47:31,360
a question in biology, I'm focusing specifically 
on that biology, molecules, medicine. Sometimes a  

481
00:47:31,360 --> 00:47:38,640
question sits in kind of firmly in the not really 
tractable category for a while and it become not  

482
00:47:38,640 --> 00:47:46,800
really practically tractable and can move into 
the tractable territory or I would say possibly  

483
00:47:46,800 --> 00:47:52,240
tractable territory and when that moment can 
happen that's something to pay a lot of attention  

484
00:47:52,240 --> 00:47:57,840
to and you want to ask yourself that. So a nice 
example of that is combinations in genetics.  

485
00:47:57,840 --> 00:48:04,400
combinations in genetics was seen as a practically 
impra intractable problem because you could very  

486
00:48:04,400 --> 00:48:08,720
quickly do back of the envelope calculation 
the number blows up you cannot do that many  

487
00:48:08,720 --> 00:48:13,200
experiments actually I can repeat that not you not 
anyone can do that number of experiments there's  

488
00:48:13,200 --> 00:48:17,760
not enough biology physical cells in the world for 
example for all the experiments you would need to  

489
00:48:17,760 --> 00:48:23,440
do in order to test every possibility and hence 
the problem sits firmly in that category except  

490
00:48:23,440 --> 00:48:27,680
in some in the general sense in the specific 
sense you can always do an experiment and see  

491
00:48:27,680 --> 00:48:31,680
what happens. But in the general sense, I won't 
be able to tell you what will happen in the next  

492
00:48:31,680 --> 00:48:36,320
one. People don't think that anymore about this 
problem. They understand it differently. Partly  

493
00:48:36,320 --> 00:48:41,200
because data changed and partly because our our 
our computational understanding changed on what  

494
00:48:41,200 --> 00:48:45,840
can happen. And those two together have said, "Oh, 
the system might be organized a little differently  

495
00:48:45,840 --> 00:48:49,600
than I thought." And if it's organized this way, 
today I understand that things like that might  

496
00:48:49,600 --> 00:48:54,880
actually be much more generally addressable. That 
opens up a whole industry of people I don't mean  

497
00:48:54,880 --> 00:49:00,160
in the corporate sense of people solving the 
problem which is not solved yet. It's not even  

498
00:49:00,160 --> 00:49:05,600
close to solved yet maybe but it's definitely 
moving as a problem that people are working on  

499
00:49:05,600 --> 00:49:10,800
rather than a problem that sits in that other 
category of there's no point because you can't  

500
00:49:10,800 --> 00:49:16,560
there are others other problems that are like that 
and some of them are very big and can be very very  

501
00:49:16,560 --> 00:49:22,800
impactful. I agree with Gavorg that I wouldn't 
worry too much about how the problems partition.  

502
00:49:22,800 --> 00:49:31,200
People go after a problem when it's relevant for 
what interests them and interest includes their  

503
00:49:31,200 --> 00:49:36,000
curiosity. But humans tend to be curious about 
things that they're interested in. Why? Sometimes  

504
00:49:36,000 --> 00:49:41,760
they're interested for this reason. People are 
motivated by many many many different reasons. But  

505
00:49:41,760 --> 00:49:47,920
once they're interested, they work on something. 
So I I agree that you will see the problems and  

506
00:49:47,920 --> 00:49:55,680
the places in the world that tackle them to be uh 
to shift like that based on it. Uh and finally for  

507
00:49:55,680 --> 00:50:02,240
the industry versus academia divide at least for 
the biological and biomedical sciences industry  

508
00:50:02,240 --> 00:50:07,360
scientists and uh academic scientists tend to 
work on the same problems. This is actually  

509
00:50:07,360 --> 00:50:12,640
not new at all. It's no no correct for decades 
and decades and decades because it's actually  

510
00:50:12,640 --> 00:50:20,000
the same problems in industry. People just try to 
solve it ultimately in the human organism because  

511
00:50:20,000 --> 00:50:25,920
they try to address diseases and diseases of 
humans happen in humans. So that adds a whole  

512
00:50:25,920 --> 00:50:31,360
world of constraints and considerations and so 
on but it's actually the same problems. So it's  

513
00:50:31,360 --> 00:50:36,880
very interesting that you guys raised the issue of 
pragmat the principles of you know curiosity but  

514
00:50:36,880 --> 00:50:42,640
also pragmatism what you can actually do but you 
know the things that you choose to do for example  

515
00:50:42,640 --> 00:50:49,920
Alex you you know chose go over you chose to leap 
into biology that's that shaped your curiosity  

516
00:50:49,920 --> 00:50:56,000
and so how do you how do you think about that and 
you know reflecting on that on that decision Alex  

517
00:50:56,000 --> 00:51:02,720
uh you know yeah do you see yourself driven by 
pragmatism ism or curiosity or you know how how  

518
00:51:02,720 --> 00:51:08,960
do you think about that? I mean when I made this 
shift myself a little over a decade ago it was  

519
00:51:08,960 --> 00:51:16,320
both. It was very much both. Um I think um I was 
I was mo I I had always been sort of fascinated  

520
00:51:16,320 --> 00:51:24,400
from afar by you know the genetic code and the 
secret of life and all of that. um but uh hadn't  

521
00:51:24,400 --> 00:51:29,680
realized the you know uh the opportunities of 
bringing like quantitative tools to bear because  

522
00:51:29,680 --> 00:51:34,000
of the you know the way that data was scaling up 
the way that sequencing was becoming cheaper and  

523
00:51:34,000 --> 00:51:41,760
more scalable and all that. Um and then I but I I 
also was myself I was very motivated also by the  

524
00:51:41,760 --> 00:51:50,320
uh the potential of um these things to you know 
getting that actually transform our understanding  

525
00:51:50,320 --> 00:51:54,240
of health and disease and get all the way to 
the clinical endpoint like this this this this  

526
00:51:54,240 --> 00:51:59,360
problems that are actually in the physical and in 
in particular in the in the human world. um you  

527
00:51:59,360 --> 00:52:03,760
know as a as a pure mathematician who was used 
to thinking about you know abstract theoretical  

528
00:52:03,760 --> 00:52:08,640
frameworks and theorems and proofs this was kind 
of revolutionary to me and and very motivating  

529
00:52:08,640 --> 00:52:16,240
um you know and so I it it was very much both 
for me um and uh and I guess I guess a third  

530
00:52:16,240 --> 00:52:24,080
motivation for me was getting to work uh not just 
with a community of of fellow experts in the same  

531
00:52:24,080 --> 00:52:28,880
sort of subd discipline but getting to work in in 
an interdisiplinary way and and you know the the  

532
00:52:28,880 --> 00:52:33,520
dynamical nature of that kind of collaboration 
was was a huge draw for me too. It was just it  

533
00:52:33,520 --> 00:52:42,240
was just more fun. um you know so I guess um I 
wanted to reflect a little bit on something that  

534
00:52:42,240 --> 00:52:47,440
came up in the previous question too if it's okay 
to go back u just give an example of the kind of  

535
00:52:47,440 --> 00:52:52,320
um I I think some of these grand challenges stick 
around for a long time um but I also think there  

536
00:52:52,320 --> 00:52:57,440
can be these these shifts that occur on kind of 
decade time scales and I just wanted to give an  

537
00:52:57,440 --> 00:53:01,920
example of such a shift um you know one of the 
one of the sort of two grand challenges that  

538
00:53:01,920 --> 00:53:07,840
Todd Gallop the current road director laid out uh 
recently as we kind of enter the next decade after  

539
00:53:07,840 --> 00:53:13,280
the first 20 years of the broad is is you know 
um converting genetic variation associated with  

540
00:53:13,280 --> 00:53:18,320
disease into mechanistic insights that can inform 
you know therapeutic hypothesis and and this is  

541
00:53:18,320 --> 00:53:23,280
uh for a long time it was let's get all the 
genetic variation associated with disease and  

542
00:53:23,280 --> 00:53:29,040
that was like the thing and it was sort of I guess 
there was this this this assumption that that you  

543
00:53:29,040 --> 00:53:32,720
know that that would show us the way and then we 
would then in then we would understand you know  

544
00:53:32,720 --> 00:53:38,560
the the gene pains and the cell types and and I 
guess partly due at least in common and complex  

545
00:53:38,560 --> 00:53:44,480
disease the the challenges of polygenicity um you 
know due to for various reasons it it just that  

546
00:53:44,480 --> 00:53:48,640
that next step of translation into mechanistic 
insights has been far from automatic and it  

547
00:53:48,640 --> 00:53:52,720
turns out like that's the thing we totally 
need to focus on as a field there you know  

548
00:53:52,720 --> 00:53:58,240
um the the other by the way the other the other 
grand challenge um from the broads perspective  

549
00:53:58,240 --> 00:54:03,360
around is around programmable therapeutics and 
scaling up genetic medicines to really be scalable  

550
00:54:03,360 --> 00:54:14,000
across Ross common and rare disease. Um but um I 
guess it's um it's just it's it's interesting to  

551
00:54:14,000 --> 00:54:19,760
watch these shifts occur at at the sort of macro 
level of the fields and then have that inform the  

552
00:54:19,760 --> 00:54:23,680
problems that we that we each think about. I 
I think the division between I want to speak  

553
00:54:23,680 --> 00:54:32,800
briefly about that too. Division between different 
kind of sectors of the biomedical ecosystem. Um

554
00:54:32,800 --> 00:54:39,200
there I don't know if I have a single uh you know 
insight there but I guess I do want to surface  

555
00:54:39,200 --> 00:54:43,520
uh a concern I sometimes have that I think we 
should all be thinking about as members of of  

556
00:54:43,520 --> 00:54:48,400
this community and just of society more broadly 
which is um sometimes I worry that the incentive  

557
00:54:48,400 --> 00:54:54,800
structures that we have um don't necessarily 
guarantee that the fruits of all the research that  

558
00:54:54,800 --> 00:54:59,920
we do and then this you know the curiosity and the 
research enterprise guys will actually transform  

559
00:54:59,920 --> 00:55:05,520
the standard of care and bring benefit to all, 
you know, um and uh not just those who can afford  

560
00:55:05,520 --> 00:55:09,520
access to sort of cutting edge medicines. And this 
is this is to be sure a conversation that goes  

561
00:55:09,520 --> 00:55:14,400
beyond science to the policy level, but I I think 
as scientists in a changing and challenging world,  

562
00:55:14,400 --> 00:55:18,640
this is, you know, we need to be part of that 
conversation and engage with these problems. Yeah,  

563
00:55:18,640 --> 00:55:24,560
that's a that's a that's a really good point. Um 
Alex, just a quick comment. Yeah, on the on the  

564
00:55:24,560 --> 00:55:33,040
on the questions question. Um yeah um yeah the 
the the yeah the pragmatism versus curiosity and  

565
00:55:33,040 --> 00:55:40,320
and bi interdicciplinary side of things. How 
how do you yeah I I can relate to all of the  

566
00:55:40,320 --> 00:55:48,000
three main motivations that Alex shared with us 
as well. Uh I just vividly remember my PhD office  

567
00:55:48,000 --> 00:55:55,520
in a basement without windows and my occasional 
interactions with other humans around my research.  

568
00:55:56,320 --> 00:56:01,920
And um it uh was and remains very 
important for me to be useful. And  

569
00:56:01,920 --> 00:56:06,640
I just found that I'm more useful when I 
have regular opportunities to interact with  

570
00:56:06,640 --> 00:56:20,800
humans. Not everyone's like that. I I just and 
I might change. But that was a big motivation.

571
00:56:20,800 --> 00:56:26,240
So do we have time for some audience questions? 
Um, I think we actually are at basically at time  

572
00:56:26,240 --> 00:56:30,880
now. We've managed to to fill the entire block 
and we actually have a couple of lightning talks  

573
00:56:30,880 --> 00:56:36,400
coming up uh right now after we get set up. We 
have and we're going to be starting with uh Tvor  

574
00:56:36,400 --> 00:56:40,640
getting set up. But I would also like to thank 
our lovely panelists for a really enlightening,  

575
00:56:40,640 --> 00:56:46,880
insightful and uh you know a grand perspective uh 
in this discussion that we've just had here. So  

576
00:56:46,880 --> 00:56:50,880
I'd really I really appreciate all of your 
your insights and uh contributions here.  

577
00:56:50,880 --> 00:56:55,760
I want to take an a second to close back to 
where we started like how MIA started and I  

578
00:56:55,760 --> 00:57:00,240
told you about that scientific frontiers effort. 
I said this many times I just want to say it on  

579
00:57:00,240 --> 00:57:06,640
the record that I believe from all the things 
that we actually did out of scientific frontiers  

580
00:57:06,640 --> 00:57:17,360
formally this which was the investment of 
the bagels was by far the most impactful.

