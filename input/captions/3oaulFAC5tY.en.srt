1
00:00:00,960 --> 00:00:06,399
great hi everyone uh I'm Stuart uh MIT

2
00:00:03,719 --> 00:00:09,160
PhD I actually just graduated in March

3
00:00:06,399 --> 00:00:11,880
and I'll be showing capsa our technology

4
00:00:09,160 --> 00:00:14,719
to make AI reliable uh but first a bit

5
00:00:11,880 --> 00:00:16,800
about uh Themis so we're an MIT spin-off

6
00:00:14,719 --> 00:00:18,439
uh from seale our founding team includes

7
00:00:16,800 --> 00:00:20,840
Professor Danielle Rose who's the

8
00:00:18,439 --> 00:00:23,359
director of Cale as well as Dr Alexander

9
00:00:20,840 --> 00:00:25,400
Amia and Alay miti uh we're post seed

10
00:00:23,359 --> 00:00:27,119
we're currently about eight employees uh

11
00:00:25,400 --> 00:00:28,720
one of our beta customers is one of the

12
00:00:27,119 --> 00:00:30,640
largest oil and gas companies in the

13
00:00:28,720 --> 00:00:31,920
world uh we've been using our product

14
00:00:30,640 --> 00:00:34,559
for about a year uh and they've been

15
00:00:31,920 --> 00:00:36,520
thrilled with it uh and so our product

16
00:00:34,559 --> 00:00:39,320
capsa helps you to know what your model

17
00:00:36,520 --> 00:00:42,239
does not know and I'm going to explain

18
00:00:39,320 --> 00:00:45,280
why that matters and how you can benefit

19
00:00:42,239 --> 00:00:47,199
so globally what motivates us AI has a

20
00:00:45,280 --> 00:00:50,760
trust problem uh we've seen a number of

21
00:00:47,199 --> 00:00:54,120
public uh failures or embarrassments of

22
00:00:50,760 --> 00:00:55,840
AI systems making mistakes uh we know AI

23
00:00:54,120 --> 00:00:57,760
models can be more reliable and should

24
00:00:55,840 --> 00:01:01,160
be more reliable and so we've developed

25
00:00:57,760 --> 00:01:03,000
capsa as a way to take any AI model and

26
00:01:01,160 --> 00:01:04,760
help you to know when you can trust it

27
00:01:03,000 --> 00:01:07,040
so we can give you the uncertainty or

28
00:01:04,760 --> 00:01:09,720
the confidence of every single output

29
00:01:07,040 --> 00:01:11,439
generated by any model uh and using

30
00:01:09,720 --> 00:01:13,920
these we can help you to reduce time to

31
00:01:11,439 --> 00:01:15,320
Market reduce development costs uh We've

32
00:01:13,920 --> 00:01:17,560
designed it to be as easy to integrate

33
00:01:15,320 --> 00:01:20,040
into your existing workflows as

34
00:01:17,560 --> 00:01:21,520
possible uh so first just backing up a

35
00:01:20,040 --> 00:01:23,759
moment what is uncertainty and why

36
00:01:21,520 --> 00:01:25,720
should you care about it uh so here we

37
00:01:23,759 --> 00:01:28,200
took a example data set so this is from

38
00:01:25,720 --> 00:01:29,920
Vision we have a multimodal solution uh

39
00:01:28,200 --> 00:01:33,079
and so we took a obstacle detection

40
00:01:29,920 --> 00:01:35,040
Network used for self-driving vehicles

41
00:01:33,079 --> 00:01:36,680
uh and this was trained on Urban driving

42
00:01:35,040 --> 00:01:38,000
data so on the left there's an example

43
00:01:36,680 --> 00:01:40,399
of an image from a data set that this

44
00:01:38,000 --> 00:01:41,960
was trained on and the middle is an

45
00:01:40,399 --> 00:01:44,439
example of an output from this model

46
00:01:41,960 --> 00:01:46,759
that's designed to look for obstacles in

47
00:01:44,439 --> 00:01:48,000
those images and so it picked up here on

48
00:01:46,759 --> 00:01:50,640
the cars because it was trained to do

49
00:01:48,000 --> 00:01:52,159
that uh but we injected this uh deer

50
00:01:50,640 --> 00:01:53,200
into the image which doesn't appear

51
00:01:52,159 --> 00:01:55,479
anywhere else in the training data

52
00:01:53,200 --> 00:01:56,880
because it's only Urban scenarios we can

53
00:01:55,479 --> 00:01:59,280
see that the model does not pick up on

54
00:01:56,880 --> 00:02:01,479
that as an obstacle now with capsa you

55
00:01:59,280 --> 00:02:03,000
get this confidence for every single

56
00:02:01,479 --> 00:02:05,640
output generated by your model so we can

57
00:02:03,000 --> 00:02:07,719
see in that highlighted region uh

58
00:02:05,640 --> 00:02:09,520
there's burst because capsa knows your

59
00:02:07,719 --> 00:02:11,280
model has never seen anything like that

60
00:02:09,520 --> 00:02:13,480
before and so you can't rely on your

61
00:02:11,280 --> 00:02:15,959
model's outputs uh in that reason of the

62
00:02:13,480 --> 00:02:17,680
image and so in this case we can tell

63
00:02:15,959 --> 00:02:19,280
you maybe don't go there anyway there

64
00:02:17,680 --> 00:02:21,720
might be an obstacle

65
00:02:19,280 --> 00:02:24,040
there now this is a multimodal solution

66
00:02:21,720 --> 00:02:26,040
I can also show you an example with llms

67
00:02:24,040 --> 00:02:28,560
uh so I'm sure you're all very familiar

68
00:02:26,040 --> 00:02:30,239
with hallucinations uh we can try to

69
00:02:28,560 --> 00:02:33,400
prompt hallucinations by asking

70
00:02:30,239 --> 00:02:35,920
misleading questions so here we asked NM

71
00:02:33,400 --> 00:02:39,000
what is the average size of tiger eggs

72
00:02:35,920 --> 00:02:41,000
and for each token in the response capsa

73
00:02:39,000 --> 00:02:43,440
tells us the confidence or the

74
00:02:41,000 --> 00:02:46,080
Insurgency so here we're highlighting in

75
00:02:43,440 --> 00:02:48,920
uh different shades of red or heat uh

76
00:02:46,080 --> 00:02:51,440
the risk or uncertainty in those tokens

77
00:02:48,920 --> 00:02:53,560
and so as we generate the response we

78
00:02:51,440 --> 00:02:55,680
can check the risk or uncertainty level

79
00:02:53,560 --> 00:02:58,120
of that response and use that to detect

80
00:02:55,680 --> 00:03:00,480
hallucinations or unreliable outputs and

81
00:02:58,120 --> 00:03:02,879
if it exceeds some kind of maximum risk

82
00:03:00,480 --> 00:03:04,720
level we can use this to say block the

83
00:03:02,879 --> 00:03:07,480
answer before it ever goes to a user if

84
00:03:04,720 --> 00:03:09,720
you're in like a chatbot scenario for

85
00:03:07,480 --> 00:03:12,280
example now here's another example with

86
00:03:09,720 --> 00:03:13,879
llms uh this was actually llama 3 and we

87
00:03:12,280 --> 00:03:16,239
asked it for a book with characters from

88
00:03:13,879 --> 00:03:18,519
it so base llama 3 you just get this

89
00:03:16,239 --> 00:03:20,080
answer you probably don't know if you

90
00:03:18,519 --> 00:03:22,120
haven't read the book recently under's

91
00:03:20,080 --> 00:03:25,280
game uh that there's no characters from

92
00:03:22,120 --> 00:03:27,040
MIT in it but with capsa when we get

93
00:03:25,280 --> 00:03:29,760
these uh confidence values in every

94
00:03:27,040 --> 00:03:31,000
token here we're averaging over phrases

95
00:03:29,760 --> 00:03:33,560
you can see that the LM is very

96
00:03:31,000 --> 00:03:36,159
confident in its description of the book

97
00:03:33,560 --> 00:03:37,760
but it's not at all confident in its uh

98
00:03:36,159 --> 00:03:39,720
answer to the question or that passage

99
00:03:37,760 --> 00:03:41,879
at the end about characters from MIT uh

100
00:03:39,720 --> 00:03:44,280
so we can not only help you to recognize

101
00:03:41,879 --> 00:03:45,760
hallucinations uh but also even where in

102
00:03:44,280 --> 00:03:48,480
the outputs the hallucinations are most

103
00:03:45,760 --> 00:03:50,519
likely to appear and so we believe that

104
00:03:48,480 --> 00:03:52,400
these capabilities should be standard in

105
00:03:50,519 --> 00:03:53,720
all AI systems and so that's exactly

106
00:03:52,400 --> 00:03:56,159
what capsa

107
00:03:53,720 --> 00:03:58,480
does uh so yeah these examples I've been

108
00:03:56,159 --> 00:04:00,319
going over uh are primarily in quality

109
00:03:58,480 --> 00:04:03,480
control we're helping you to

110
00:04:00,319 --> 00:04:05,959
uh reduce hallucinations ensure the

111
00:04:03,480 --> 00:04:07,480
accuracy of your AI models uh but this

112
00:04:05,959 --> 00:04:09,239
uncertainty quantification has a number

113
00:04:07,480 --> 00:04:10,680
of other applications uh for example

114
00:04:09,239 --> 00:04:12,879
data cleaning and data curation we can

115
00:04:10,680 --> 00:04:15,439
help you find mislabeled data U that

116
00:04:12,879 --> 00:04:17,320
tends to uh highlight uncert or

117
00:04:15,439 --> 00:04:19,840
uncertainty tends to highlight those

118
00:04:17,320 --> 00:04:21,880
examples uh we can help you select new

119
00:04:19,840 --> 00:04:23,919
data to improve your models uh to help

120
00:04:21,880 --> 00:04:25,479
you find valuable training data uh we

121
00:04:23,919 --> 00:04:27,320
can also help to eliminate bias we

122
00:04:25,479 --> 00:04:30,320
actually have some published works on

123
00:04:27,320 --> 00:04:32,880
debiasing AI models uh using this

124
00:04:30,320 --> 00:04:35,639
capability and then anomaly detection so

125
00:04:32,880 --> 00:04:38,000
when your systems are in production uh

126
00:04:35,639 --> 00:04:40,520
we can help you to find uh potential

127
00:04:38,000 --> 00:04:42,199
errors and avoid them uh mitigate

128
00:04:40,520 --> 00:04:44,440
failures in real

129
00:04:42,199 --> 00:04:46,600
time so we're here to look for

130
00:04:44,440 --> 00:04:48,800
Partnerships and more proof of uh

131
00:04:46,600 --> 00:04:50,199
Concepts so please reach out to us uh if

132
00:04:48,800 --> 00:04:52,080
you'd like to see either more specific

133
00:04:50,199 --> 00:04:53,800
use case demos we have some for medical

134
00:04:52,080 --> 00:04:55,080
insurance I'd like to highlight it works

135
00:04:53,800 --> 00:04:57,199
for any architecture any model any

136
00:04:55,080 --> 00:04:58,360
industry uh please do reach out with us

137
00:04:57,199 --> 00:04:59,960
uh we're happy to work with you if

138
00:04:58,360 --> 00:05:02,000
you're interested as a question customer

139
00:04:59,960 --> 00:05:04,400
partner distributor so on and so forth

140
00:05:02,000 --> 00:05:07,160
so great and we'll be in the Next Room

141
00:05:04,400 --> 00:05:07,160
thanks

