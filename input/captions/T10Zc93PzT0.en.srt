1
00:00:00,080 --> 00:00:06,080
Welcome everyone. Today I am joined by

2
00:00:03,280 --> 00:00:08,160
professor Rama who is a professor of

3
00:00:06,080 --> 00:00:10,880
practice in AI and machine learning here

4
00:00:08,160 --> 00:00:13,360
at MIT Sloan. My name is Michelle Lee.

5
00:00:10,880 --> 00:00:15,120
I'm the director of business analytics

6
00:00:13,360 --> 00:00:17,279
and we're really excited to have a

7
00:00:15,120 --> 00:00:20,480
conversation with Rama and his journey

8
00:00:17,279 --> 00:00:22,400
here at MIT. So to start Rama, could you

9
00:00:20,480 --> 00:00:24,640
please uh introduce yourself to the

10
00:00:22,400 --> 00:00:26,960
audience and tell us about how you

11
00:00:24,640 --> 00:00:28,640
arrived here at MIT? Yeah, sure. First

12
00:00:26,960 --> 00:00:31,359
of all, Michelle, thanks for having me

13
00:00:28,640 --> 00:00:33,520
here. It's great to be here. Um, so I'm

14
00:00:31,359 --> 00:00:35,520
actually an MIT alum myself. Um, I grew

15
00:00:33,520 --> 00:00:37,360
up in India. Then I came to MIT for

16
00:00:35,520 --> 00:00:39,760
graduate school. Uh, I got my masters in

17
00:00:37,360 --> 00:00:42,320
PhD in operations research here. Uh, and

18
00:00:39,760 --> 00:00:44,239
then I went on to industry. I spent a

19
00:00:42,320 --> 00:00:46,480
number of years at Mckenzie. Uh, and

20
00:00:44,239 --> 00:00:48,480
then I got bitten with entrepreneurial

21
00:00:46,480 --> 00:00:50,399
bug. Um, so I was essentially an

22
00:00:48,480 --> 00:00:52,559
analytics entrepreneur for 20 plus

23
00:00:50,399 --> 00:00:55,360
years. Um, I was involved in a bunch of

24
00:00:52,559 --> 00:00:56,719
startups. uh all all these startups the

25
00:00:55,360 --> 00:00:58,879
common thread was that they would

26
00:00:56,719 --> 00:01:00,559
identify an interesting business problem

27
00:00:58,879 --> 00:01:02,559
uh which seemed amendable to a sort of

28
00:01:00,559 --> 00:01:05,199
an analytics data modeling sort of

29
00:01:02,559 --> 00:01:06,479
approach uh we would build a solution we

30
00:01:05,199 --> 00:01:07,760
would take it to market and then we

31
00:01:06,479 --> 00:01:09,439
would you know try to convince people to

32
00:01:07,760 --> 00:01:11,680
use it and get value from it after doing

33
00:01:09,439 --> 00:01:14,159
it a bunch of times um I had the

34
00:01:11,680 --> 00:01:16,479
opportunity to join the faculty here at

35
00:01:14,159 --> 00:01:18,880
Sloan uh as part of the operations

36
00:01:16,479 --> 00:01:20,880
research and statistics group uh so at

37
00:01:18,880 --> 00:01:23,439
Sloan I'm a professor of the practice as

38
00:01:20,880 --> 00:01:25,360
you mentioned uh and I teach courses

39
00:01:23,439 --> 00:01:28,000
broadly in the data science, machine

40
00:01:25,360 --> 00:01:31,200
learning, deep learning areas. Um, and

41
00:01:28,000 --> 00:01:33,439
my uh research really focuses on how do

42
00:01:31,200 --> 00:01:36,159
you take predictive AI and generative AI

43
00:01:33,439 --> 00:01:37,840
and make it really useful and practical

44
00:01:36,159 --> 00:01:39,520
for business decision makers and

45
00:01:37,840 --> 00:01:41,600
technical leaders in businesses. That's

46
00:01:39,520 --> 00:01:42,960
really my primary focus. I do advise a

47
00:01:41,600 --> 00:01:46,000
number of large companies and a number

48
00:01:42,960 --> 00:01:48,240
of startups on how to essentially build

49
00:01:46,000 --> 00:01:50,560
intelligent products and services. Wow.

50
00:01:48,240 --> 00:01:52,479
How did you get started with Gen AI?

51
00:01:50,560 --> 00:01:54,640
because genai hasn't existed for that

52
00:01:52,479 --> 00:01:57,280
long, right? Um what was your path

53
00:01:54,640 --> 00:01:59,600
there? Well, actually um as I mentioned

54
00:01:57,280 --> 00:02:01,439
earlier, I teach courses in deep

55
00:01:59,600 --> 00:02:03,840
learning and data science, broadly

56
00:02:01,439 --> 00:02:05,439
speaking. And one particular course that

57
00:02:03,840 --> 00:02:07,280
I've been teaching for about four years

58
00:02:05,439 --> 00:02:09,679
now is a course called hands-on deep

59
00:02:07,280 --> 00:02:11,920
learning. And as folks might know,

60
00:02:09,679 --> 00:02:13,920
generative AI is actually built on deep

61
00:02:11,920 --> 00:02:16,319
learning. It is in fact an application

62
00:02:13,920 --> 00:02:18,640
of deep learning. And so since we were

63
00:02:16,319 --> 00:02:20,800
teaching deep learning uh we got exposed

64
00:02:18,640 --> 00:02:22,400
to generative AI very early on. We were

65
00:02:20,800 --> 00:02:25,200
sort of teaching about generative

66
00:02:22,400 --> 00:02:27,440
modeling and generative AI well before

67
00:02:25,200 --> 00:02:29,120
Chad GPD hit the mainstream. Of course

68
00:02:27,440 --> 00:02:31,200
we like everyone else were super

69
00:02:29,120 --> 00:02:34,080
surprised uh at the reaction that Chad

70
00:02:31,200 --> 00:02:37,200
GPD evoked in the public. Uh and and

71
00:02:34,080 --> 00:02:38,879
since then uh we have just been tracking

72
00:02:37,200 --> 00:02:40,640
the research we have been following the

73
00:02:38,879 --> 00:02:43,040
applications. we've been seeing the

74
00:02:40,640 --> 00:02:45,680
diffusion of this technology into sort

75
00:02:43,040 --> 00:02:48,160
of businesses and so on. And so I have

76
00:02:45,680 --> 00:02:50,560
essentially sort of um gotten sucked

77
00:02:48,160 --> 00:02:52,879
into this rabbit hole in a big way. Um

78
00:02:50,560 --> 00:02:55,120
and so I spent a lot of my time keeping

79
00:02:52,879 --> 00:02:57,280
up with the technology and then

80
00:02:55,120 --> 00:02:59,200
specifically thinking about all right

81
00:02:57,280 --> 00:03:01,200
this sounds really cool and exciting.

82
00:02:59,200 --> 00:03:03,599
How do we make it practically useful to

83
00:03:01,200 --> 00:03:05,599
businesses? Uh often times executives

84
00:03:03,599 --> 00:03:07,680
come to me and say uh you know things

85
00:03:05,599 --> 00:03:09,200
are changing so fast uh should we you

86
00:03:07,680 --> 00:03:11,040
know stand on the sidelines and wait for

87
00:03:09,200 --> 00:03:13,040
the dust to settle or should we sort of

88
00:03:11,040 --> 00:03:15,280
dive in and see what's going on? How do

89
00:03:13,040 --> 00:03:18,080
we dive in without taking undue risks?

90
00:03:15,280 --> 00:03:20,080
So these are all questions that to

91
00:03:18,080 --> 00:03:21,920
answer properly you need to be standing

92
00:03:20,080 --> 00:03:24,640
at the intersection of the technology

93
00:03:21,920 --> 00:03:26,560
and business and therefore bring a very

94
00:03:24,640 --> 00:03:28,239
pragmatic lens to the world bank. So

95
00:03:26,560 --> 00:03:31,120
that's sort of what I do most of the

96
00:03:28,239 --> 00:03:32,720
time when I'm not teaching. Uh but uh I

97
00:03:31,120 --> 00:03:35,519
hope that gives you a sense for how I

98
00:03:32,720 --> 00:03:37,040
got pulled into Gai. Yeah, that's great.

99
00:03:35,519 --> 00:03:39,360
And well, you are teaching you're

100
00:03:37,040 --> 00:03:43,280
teaching companies for example. Could

101
00:03:39,360 --> 00:03:45,599
you give us an overview of your course,

102
00:03:43,280 --> 00:03:48,319
the hands-on deep learning course and

103
00:03:45,599 --> 00:03:50,080
how it evolves over the semester and

104
00:03:48,319 --> 00:03:53,040
what do students experience in the

105
00:03:50,080 --> 00:03:54,640
course? Oh yeah, absolutely. Um so

106
00:03:53,040 --> 00:03:56,720
hands-on deep learning is a very

107
00:03:54,640 --> 00:03:58,480
interesting course because first of all

108
00:03:56,720 --> 00:04:01,599
it's a course that's situated in the

109
00:03:58,480 --> 00:04:04,000
business school yet it actually is quite

110
00:04:01,599 --> 00:04:05,439
programming heavy so one might think on

111
00:04:04,000 --> 00:04:08,080
the surface that you know business

112
00:04:05,439 --> 00:04:09,439
school students MBA students perhaps uh

113
00:04:08,080 --> 00:04:13,040
you know are not that interested in

114
00:04:09,439 --> 00:04:14,560
programming uh but what I saw in the

115
00:04:13,040 --> 00:04:16,720
early years when I joined the Sloan

116
00:04:14,560 --> 00:04:18,880
faculty is that there's a fair number of

117
00:04:16,720 --> 00:04:21,040
people coming to Sloan in the various

118
00:04:18,880 --> 00:04:22,400
graduate programs who are sort of not

119
00:04:21,040 --> 00:04:23,680
content content with just reading about

120
00:04:22,400 --> 00:04:26,800
things. They actually want to build

121
00:04:23,680 --> 00:04:29,280
things. And so we felt that there was

122
00:04:26,800 --> 00:04:31,919
indeed demand and value for having a

123
00:04:29,280 --> 00:04:33,840
course which not only teaches you how

124
00:04:31,919 --> 00:04:35,440
these things work but actually shows you

125
00:04:33,840 --> 00:04:37,120
how to build things that work. Every

126
00:04:35,440 --> 00:04:38,720
lecture ends up being you know maybe

127
00:04:37,120 --> 00:04:40,720
half of it is sort of a traditional

128
00:04:38,720 --> 00:04:42,800
lecture and then the the second half

129
00:04:40,720 --> 00:04:44,960
ends up being you know we will

130
00:04:42,800 --> 00:04:47,040
essentially you know fire up a a Jupyter

131
00:04:44,960 --> 00:04:48,880
notebook or a collab notebook and then

132
00:04:47,040 --> 00:04:51,600
do some live programming in front of

133
00:04:48,880 --> 00:04:54,080
everybody. Um and I think and they can

134
00:04:51,600 --> 00:04:55,440
see the the the model or the system

135
00:04:54,080 --> 00:04:57,600
essentially taking shape before their

136
00:04:55,440 --> 00:04:59,120
very eyes and that has a very huge

137
00:04:57,600 --> 00:05:00,720
impact on people particularly those who

138
00:04:59,120 --> 00:05:02,320
may not be very used to programming

139
00:05:00,720 --> 00:05:03,520
because they can see that you know even

140
00:05:02,320 --> 00:05:05,360
people like me who have been programming

141
00:05:03,520 --> 00:05:06,800
for a long time we will make mistakes we

142
00:05:05,360 --> 00:05:08,560
will course correct we will sort of

143
00:05:06,800 --> 00:05:10,720
figure it out as we go and I think all

144
00:05:08,560 --> 00:05:12,560
of it is part of the ethos of the class

145
00:05:10,720 --> 00:05:14,320
um so I actually tell students in the

146
00:05:12,560 --> 00:05:16,800
very first lecture that my promise to

147
00:05:14,320 --> 00:05:19,440
you is that if you have a great idea for

148
00:05:16,800 --> 00:05:21,680
a product or service you You don't have

149
00:05:19,440 --> 00:05:23,600
to call your techie friend for help. You

150
00:05:21,680 --> 00:05:26,800
can roll up your sleeves and build the

151
00:05:23,600 --> 00:05:29,039
000 word version of the thing yourself

152
00:05:26,800 --> 00:05:30,639
and then you can go from there. So in

153
00:05:29,039 --> 00:05:33,440
some sense the course could literally be

154
00:05:30,639 --> 00:05:35,919
called AI for builders uh as opposed to

155
00:05:33,440 --> 00:05:38,400
just AI for managers and I think it is

156
00:05:35,919 --> 00:05:40,639
for that reason uh for instance that

157
00:05:38,400 --> 00:05:42,560
like the MBAN students find it super

158
00:05:40,639 --> 00:05:44,639
interesting because MBAN students are

159
00:05:42,560 --> 00:05:45,680
builders um you know they're not content

160
00:05:44,639 --> 00:05:46,880
with just reading about things. They

161
00:05:45,680 --> 00:05:48,479
actually want to build things. Uh they

162
00:05:46,880 --> 00:05:50,160
want to make a difference to the world.

163
00:05:48,479 --> 00:05:51,680
Um so that's really what the course is

164
00:05:50,160 --> 00:05:53,759
about and I guess the final thing I want

165
00:05:51,680 --> 00:05:55,520
to add is that um the course is

166
00:05:53,759 --> 00:05:58,560
extremely action-packed in the sense

167
00:05:55,520 --> 00:06:01,680
that we go from zero to the cutting edge

168
00:05:58,560 --> 00:06:03,520
of AI in literally 12 lectures uh which

169
00:06:01,680 --> 00:06:07,280
means that things happen at a dizzying

170
00:06:03,520 --> 00:06:09,600
pace number one uh and number two um you

171
00:06:07,280 --> 00:06:11,360
know very often we have these live demos

172
00:06:09,600 --> 00:06:12,880
in the class where you know we will

173
00:06:11,360 --> 00:06:14,240
build these things in front of everybody

174
00:06:12,880 --> 00:06:16,560
else and then we'll just put it to the

175
00:06:14,240 --> 00:06:18,639
test in front of everybody. No canned

176
00:06:16,560 --> 00:06:20,720
demos. These are all live demos in the

177
00:06:18,639 --> 00:06:22,720
wild. And I think that's all part of the

178
00:06:20,720 --> 00:06:24,479
mix that makes the course very exciting

179
00:06:22,720 --> 00:06:26,560
to not just the students but also to the

180
00:06:24,479 --> 00:06:29,039
people like me who teach it. Could you

181
00:06:26,560 --> 00:06:31,120
give us an example of a live demo? Oh

182
00:06:29,039 --> 00:06:33,919
yeah, sure. So uh for for instance when

183
00:06:31,120 --> 00:06:35,360
I teach um how to do uh image

184
00:06:33,919 --> 00:06:36,720
classification which is a computer

185
00:06:35,360 --> 00:06:38,639
vision problem.

186
00:06:36,720 --> 00:06:41,280
uh you know what what we do is we

187
00:06:38,639 --> 00:06:43,039
actually you know we web scrape uh you

188
00:06:41,280 --> 00:06:45,360
know 100 handbag images from the

189
00:06:43,039 --> 00:06:46,960
internet another 100 handbag images of

190
00:06:45,360 --> 00:06:48,479
shoes sorry image of shoes from the

191
00:06:46,960 --> 00:06:50,639
internet and then we say okay with these

192
00:06:48,479 --> 00:06:53,120
200 images can we actually build a shoe

193
00:06:50,639 --> 00:06:55,199
handbag classifier and we build it from

194
00:06:53,120 --> 00:06:57,280
scratch in front of everybody and then

195
00:06:55,199 --> 00:07:01,039
once I build it and the and the model is

196
00:06:57,280 --> 00:07:03,199
ready say 20 minutes in I invite a shoe

197
00:07:01,039 --> 00:07:04,400
or a handbag from the class so we

198
00:07:03,199 --> 00:07:06,240
actually have students come in with

199
00:07:04,400 --> 00:07:07,440
their shoes or their handbag and they

200
00:07:06,240 --> 00:07:09,520
literally you know put in front of the

201
00:07:07,440 --> 00:07:11,039
camera and see if it works. Uh

202
00:07:09,520 --> 00:07:13,199
fortunately the last four years it has

203
00:07:11,039 --> 00:07:15,440
worked every single time but I will I

204
00:07:13,199 --> 00:07:17,039
will confess that till it actually works

205
00:07:15,440 --> 00:07:19,039
I am a little nervous. You never know

206
00:07:17,039 --> 00:07:20,560
right. Yeah. Very cool. And is

207
00:07:19,039 --> 00:07:22,240
everything can you talk about the

208
00:07:20,560 --> 00:07:24,560
technology stack is everything done in

209
00:07:22,240 --> 00:07:27,759
Python? Yes. Uh the course is 100%

210
00:07:24,560 --> 00:07:29,919
Python. Uh we uh do everything using uh

211
00:07:27,759 --> 00:07:33,120
Google Collab. Okay. Uh which gives you

212
00:07:29,919 --> 00:07:34,960
access to GPUs. Uh we also actually you

213
00:07:33,120 --> 00:07:37,680
know allow the students to subscribe to

214
00:07:34,960 --> 00:07:39,919
not the free tier but a paid tier. Sloan

215
00:07:37,680 --> 00:07:41,599
will you know take care of those costs

216
00:07:39,919 --> 00:07:45,520
so that students have access to more

217
00:07:41,599 --> 00:07:48,160
powerful GPUs for their work. uh and we

218
00:07:45,520 --> 00:07:50,160
so far we have been using a keras uh

219
00:07:48,160 --> 00:07:52,319
which is a very popular uh deep learning

220
00:07:50,160 --> 00:07:54,240
framework and I want to say that the the

221
00:07:52,319 --> 00:07:56,800
final project the students complete in

222
00:07:54,240 --> 00:08:00,479
the course they're very exciting and I

223
00:07:56,800 --> 00:08:01,919
I'm always super thrilled to see the the

224
00:08:00,479 --> 00:08:03,520
way in which students take the things

225
00:08:01,919 --> 00:08:05,520
that they've learned in the classroom

226
00:08:03,520 --> 00:08:07,440
and then apply it to actually build

227
00:08:05,520 --> 00:08:09,759
something which is you know very

228
00:08:07,440 --> 00:08:11,599
significant and often substantial. Uh

229
00:08:09,759 --> 00:08:12,960
and in the past students have talked to

230
00:08:11,599 --> 00:08:14,319
me about actually taking what they have

231
00:08:12,960 --> 00:08:16,000
done in their final projects and

232
00:08:14,319 --> 00:08:17,680
creating a startup out of it. Well, you

233
00:08:16,000 --> 00:08:20,160
would be the perfect person to talk to

234
00:08:17,680 --> 00:08:21,840
about startups. Um well, I I' I've done

235
00:08:20,160 --> 00:08:23,840
my fair share of startup mistakes. So,

236
00:08:21,840 --> 00:08:26,720
yes, I have something to offer certainly

237
00:08:23,840 --> 00:08:29,919
on that front. Um so, what do you see

238
00:08:26,720 --> 00:08:32,880
for the future for Gen AI and deep

239
00:08:29,919 --> 00:08:34,560
learning? Yeah, that's a very difficult

240
00:08:32,880 --> 00:08:36,880
and profound question, Michelle. So

241
00:08:34,560 --> 00:08:38,640
first of all I do think that there is a

242
00:08:36,880 --> 00:08:39,919
lots of things going on in deep learning

243
00:08:38,640 --> 00:08:43,360
that have nothing to do with generative

244
00:08:39,919 --> 00:08:46,480
AI and those applications are in my

245
00:08:43,360 --> 00:08:48,240
opinion you know unaloyed good things

246
00:08:46,480 --> 00:08:50,640
meaning you know if you if you look at

247
00:08:48,240 --> 00:08:52,880
things like uh protein folding right we

248
00:08:50,640 --> 00:08:55,120
have solved protein folding alpha fold

249
00:08:52,880 --> 00:08:56,640
uh and there are so many incredibly

250
00:08:55,120 --> 00:08:58,399
beautiful and successful applications of

251
00:08:56,640 --> 00:09:00,160
deep learning so that is going to

252
00:08:58,399 --> 00:09:01,920
continue to accelerate I have no doubt

253
00:09:00,160 --> 00:09:03,920
about it and I'm all for it now

254
00:09:01,920 --> 00:09:06,160
generative AI is different in the sense

255
00:09:03,920 --> 00:09:09,519
that um incredibly powerful, very

256
00:09:06,160 --> 00:09:12,240
versatile uh and we are seeing massive

257
00:09:09,519 --> 00:09:14,640
impact, positive impact from genai uh in

258
00:09:12,240 --> 00:09:17,120
terms of you know uh healthcare and drug

259
00:09:14,640 --> 00:09:18,959
discovery and things like that. Um but I

260
00:09:17,120 --> 00:09:21,680
think we should also be fully cognizant

261
00:09:18,959 --> 00:09:24,959
of the fact that um you know given its

262
00:09:21,680 --> 00:09:27,360
ability to do various knowledge work

263
00:09:24,959 --> 00:09:29,519
tasks uh that there might be negative

264
00:09:27,360 --> 00:09:32,000
impacts on employment and jobs and the

265
00:09:29,519 --> 00:09:34,000
economy. uh of course we also we have

266
00:09:32,000 --> 00:09:36,320
heard about the fact that these are very

267
00:09:34,000 --> 00:09:38,080
energy guzzling systems so there's going

268
00:09:36,320 --> 00:09:40,240
to be an impact on carbon and emissions

269
00:09:38,080 --> 00:09:43,680
in the environment so so I think with

270
00:09:40,240 --> 00:09:45,600
generative AI I think massive positive

271
00:09:43,680 --> 00:09:47,200
impact is certainly possible but there

272
00:09:45,600 --> 00:09:50,240
is some possibility of negative impact

273
00:09:47,200 --> 00:09:52,800
as well and we as decision makers and

274
00:09:50,240 --> 00:09:55,120
builders and modelers and so on I think

275
00:09:52,800 --> 00:09:57,279
we need to sort of have a very holistic

276
00:09:55,120 --> 00:09:58,480
360 view of what's going on so that we

277
00:09:57,279 --> 00:10:00,240
sort of make the right decisions and

278
00:09:58,480 --> 00:10:02,720
steer things in the right action

279
00:10:00,240 --> 00:10:06,880
generative AI can do a lot of things

280
00:10:02,720 --> 00:10:09,600
which um I feel that maybe students

281
00:10:06,880 --> 00:10:12,480
don't have to learn anymore um so you

282
00:10:09,600 --> 00:10:14,160
know Arvin Aran of Princeton had a great

283
00:10:12,480 --> 00:10:16,399
framework for thinking about this and he

284
00:10:14,160 --> 00:10:18,320
talked about how there are essential

285
00:10:16,399 --> 00:10:20,320
skills and incidental skills we should

286
00:10:18,320 --> 00:10:21,920
look at what we are teaching and we say

287
00:10:20,320 --> 00:10:24,320
okay what is essential and what is

288
00:10:21,920 --> 00:10:26,160
incidental and we should make sure that

289
00:10:24,320 --> 00:10:27,680
students absolutely learn the essential

290
00:10:26,160 --> 00:10:29,600
skills that they don't short

291
00:10:27,680 --> 00:10:32,160
shortcircuit their learning by using

292
00:10:29,600 --> 00:10:33,920
genai to sort of do those things but the

293
00:10:32,160 --> 00:10:36,240
incidental stuff I think it's fine they

294
00:10:33,920 --> 00:10:38,079
can use ji for it and now what is

295
00:10:36,240 --> 00:10:39,680
incidental what is obviously the

296
00:10:38,079 --> 00:10:40,880
important question and I think the

297
00:10:39,680 --> 00:10:42,800
answer varies from discipline to

298
00:10:40,880 --> 00:10:45,120
discipline but I think we all have to

299
00:10:42,800 --> 00:10:46,640
really think about it carefully u and so

300
00:10:45,120 --> 00:10:49,360
for instance if you take something like

301
00:10:46,640 --> 00:10:51,519
machine learning um generative AI

302
00:10:49,360 --> 00:10:54,000
systems can essentially you upload a

303
00:10:51,519 --> 00:10:55,600
data set it can easily do random forest

304
00:10:54,000 --> 00:10:57,680
or gradient boosting or any of the

305
00:10:55,600 --> 00:10:59,680
standard methods um so the question

306
00:10:57,680 --> 00:11:01,120
becomes if If you want to just use these

307
00:10:59,680 --> 00:11:03,760
systems as opposed to build these

308
00:11:01,120 --> 00:11:05,839
systems, do you really need to know the

309
00:11:03,760 --> 00:11:08,240
nth level of detail of all the ways in

310
00:11:05,839 --> 00:11:10,079
which these algorithms work? Or should

311
00:11:08,240 --> 00:11:12,480
you really be focused on how do I get

312
00:11:10,079 --> 00:11:14,880
the right data to feed these systems?

313
00:11:12,480 --> 00:11:17,440
Once the system builds a model, how do I

314
00:11:14,880 --> 00:11:18,880
actually uh evaluate the model? How do I

315
00:11:17,440 --> 00:11:20,320
test the the kick the tires in the

316
00:11:18,880 --> 00:11:22,240
model? How do I make sure it's ready for

317
00:11:20,320 --> 00:11:24,320
deployment? How do I make sure it's not

318
00:11:22,240 --> 00:11:26,000
perpetuating bias? How do I make sure

319
00:11:24,320 --> 00:11:28,160
that it doesn't collapse with the with

320
00:11:26,000 --> 00:11:29,920
one wrong example when you start to

321
00:11:28,160 --> 00:11:32,079
deploy? Those are all very important

322
00:11:29,920 --> 00:11:34,160
questions. So I think over time we're

323
00:11:32,079 --> 00:11:35,680
going to gravitate towards um you know

324
00:11:34,160 --> 00:11:38,000
these kinds of very important things

325
00:11:35,680 --> 00:11:40,880
which are not quite amenable to LLMs and

326
00:11:38,000 --> 00:11:44,000
genai yet um and make sure that we

327
00:11:40,880 --> 00:11:45,600
reflect that in the way we teach. Great

328
00:11:44,000 --> 00:11:48,399
well thank you so much for your time

329
00:11:45,600 --> 00:11:51,040
today um and thank you for listening to

330
00:11:48,399 --> 00:11:52,240
Rama and see you next time. You're very

331
00:11:51,040 --> 00:11:53,519
welcome Michelle. It was great to be

332
00:11:52,240 --> 00:11:55,600
here and I enjoyed the conversation.

333
00:11:53,519 --> 00:11:57,440
Thank you. Thank you. Hi everyone. Thank

334
00:11:55,600 --> 00:11:59,440
you for watching. If you like this

335
00:11:57,440 --> 00:12:02,079
video, check out our other videos in the

336
00:11:59,440 --> 00:12:04,720
series where I interview influential

337
00:12:02,079 --> 00:12:05,920
professors at MIT Sloan. If you would

338
00:12:04,720 --> 00:12:07,920
like to learn more about the Master

339
00:12:05,920 --> 00:12:10,399
Business Analytics program, we also have

340
00:12:07,920 --> 00:12:12,399
a playlist where you can see more videos

341
00:12:10,399 --> 00:12:16,560
about the admissions process, student

342
00:12:12,399 --> 00:12:16,560
life, and the program overall.

