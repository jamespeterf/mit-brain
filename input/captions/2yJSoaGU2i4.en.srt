1
00:00:01,040 --> 00:00:04,920
good to see you everyone so um in this

2
00:00:03,280 --> 00:00:08,240
second talk I will talk about um

3
00:00:04,920 --> 00:00:11,599
generative modeling so first question um

4
00:00:08,240 --> 00:00:14,120
who has used chb or anything like that

5
00:00:11,599 --> 00:00:16,960
maybe everyone right but who have heard

6
00:00:14,120 --> 00:00:19,119
the term of generative model before you

7
00:00:16,960 --> 00:00:23,119
got to know CH

8
00:00:19,119 --> 00:00:25,599
GPT well still quite a lot yeah so um in

9
00:00:23,119 --> 00:00:27,960
this talk I will give um very high level

10
00:00:25,599 --> 00:00:31,000
overview of um what generative modeling

11
00:00:27,960 --> 00:00:32,879
is and how it's impacting um our life

12
00:00:31,000 --> 00:00:35,640
and and our future

13
00:00:32,879 --> 00:00:39,280
research so there is no doubt that we

14
00:00:35,640 --> 00:00:42,200
are in the so-called gen AI area so um

15
00:00:39,280 --> 00:00:44,680
for public audience perhaps this moment

16
00:00:42,200 --> 00:00:47,079
happens when chbt or or many other chat

17
00:00:44,680 --> 00:00:50,199
BS were introduced so you can

18
00:00:47,079 --> 00:00:52,320
communicate with um the computer in

19
00:00:50,199 --> 00:00:54,440
natural languages you can talk you can

20
00:00:52,320 --> 00:00:57,079
ask the computer whatever problems and

21
00:00:54,440 --> 00:01:00,320
um it's it's just like an agent that can

22
00:00:57,079 --> 00:01:03,000
help you to solve um many issues so this

23
00:01:00,320 --> 00:01:04,839
is not the only um generative AI model

24
00:01:03,000 --> 00:01:06,680
so another very popular and very

25
00:01:04,839 --> 00:01:09,280
powerful tool is the so-called text to

26
00:01:06,680 --> 00:01:12,799
image generation so for example um the

27
00:01:09,280 --> 00:01:15,000
users can give um the computer some text

28
00:01:12,799 --> 00:01:17,880
which is usually called a prompt and

29
00:01:15,000 --> 00:01:23,560
then the computer can generate an image

30
00:01:17,880 --> 00:01:23,560
so um for example change the adap

31
00:01:31,479 --> 00:01:34,159
let me remove

32
00:01:41,960 --> 00:01:45,920
this okay I hope it

33
00:01:48,560 --> 00:01:53,880
works okay sorry about that so for

34
00:01:51,560 --> 00:01:57,119
example um in in this case the promp

35
00:01:53,880 --> 00:01:58,719
would be um a Teddy be teaching a course

36
00:01:57,119 --> 00:02:01,560
with generative model written on

37
00:01:58,719 --> 00:02:03,759
blackbot so it is very likely that um

38
00:02:01,560 --> 00:02:06,000
the computer hasn't the computer

39
00:02:03,759 --> 00:02:08,920
algorithms hasn't seen this exact image

40
00:02:06,000 --> 00:02:11,400
before um but this is how it can produce

41
00:02:08,920 --> 00:02:14,080
from the given text

42
00:02:11,400 --> 00:02:15,480
prompt so um we can even go one step

43
00:02:14,080 --> 00:02:17,560
further we can ask the computer

44
00:02:15,480 --> 00:02:20,319
algorithm to generate a video and this

45
00:02:17,560 --> 00:02:23,959
is um what is generated by Sora um one

46
00:02:20,319 --> 00:02:27,480
year ago so um this is just really

47
00:02:23,959 --> 00:02:32,440
impressive so I believe perhaps um no

48
00:02:27,480 --> 00:02:35,879
producers have ever um um kept um take a

49
00:02:32,440 --> 00:02:38,840
video in this wave um having so many um

50
00:02:35,879 --> 00:02:42,040
um paper planes flying on top of trees

51
00:02:38,840 --> 00:02:45,760
or or or Forest so this is completely

52
00:02:42,040 --> 00:02:48,280
generated by kind of the imagination of

53
00:02:45,760 --> 00:02:50,640
the computer

54
00:02:48,280 --> 00:02:52,879
algorithms so um actually generative

55
00:02:50,640 --> 00:02:56,480
models could be very powerful um

56
00:02:52,879 --> 00:02:59,640
productive um tools um in our daily life

57
00:02:56,480 --> 00:03:02,599
for example um it's still kind of a trap

58
00:02:59,640 --> 00:03:06,080
B but it is a TR bot that can help us to

59
00:03:02,599 --> 00:03:10,480
write code so um this is kind of an AI

60
00:03:06,080 --> 00:03:12,560
assistant so it um um it can read your

61
00:03:10,480 --> 00:03:15,120
code it can try to fix the issue of your

62
00:03:12,560 --> 00:03:17,400
code and you can directly communicate

63
00:03:15,120 --> 00:03:20,840
with the agent using natural language

64
00:03:17,400 --> 00:03:24,400
and the agent will turn it into um code

65
00:03:20,840 --> 00:03:28,680
so in some sense um perhaps the previous

66
00:03:24,400 --> 00:03:31,000
um programming language is C++ python or

67
00:03:28,680 --> 00:03:35,599
Java the next level of programming

68
00:03:31,000 --> 00:03:38,120
language would just be English or human

69
00:03:35,599 --> 00:03:39,959
language so um it's more than that it's

70
00:03:38,120 --> 00:03:42,360
more than just computer science actually

71
00:03:39,959 --> 00:03:46,680
gen models has been used in many

72
00:03:42,360 --> 00:03:48,760
scientific uh problems so this is um uh

73
00:03:46,680 --> 00:03:51,439
an application which is called protein

74
00:03:48,760 --> 00:03:54,439
design and generation so the ultimate

75
00:03:51,439 --> 00:03:56,599
goal is to design or generate some type

76
00:03:54,439 --> 00:03:58,959
of proteins that can solve um some

77
00:03:56,599 --> 00:04:02,040
problems um um that we we care about

78
00:03:58,959 --> 00:04:05,280
let's say uh are some um very um um

79
00:04:02,040 --> 00:04:08,879
dangerous uh very fatal diseases so this

80
00:04:05,280 --> 00:04:11,280
work is called um RF diffusion um it is

81
00:04:08,879 --> 00:04:14,640
it's actually part of the uh um the work

82
00:04:11,280 --> 00:04:16,959
of the Nobel Prize winner um this

83
00:04:14,640 --> 00:04:18,560
year and there are many other scientific

84
00:04:16,959 --> 00:04:21,400
problem that can benefit from Al

85
00:04:18,560 --> 00:04:24,160
generative modeling so um this is a work

86
00:04:21,400 --> 00:04:26,440
from Deep mine a few years ago so um

87
00:04:24,160 --> 00:04:28,960
they can use this model to predict um

88
00:04:26,440 --> 00:04:31,240
the the the weather change um over the

89
00:04:28,960 --> 00:04:34,639
um next several hours or or next several

90
00:04:31,240 --> 00:04:37,759
days so this would be a very difficult

91
00:04:34,639 --> 00:04:40,639
problem um for um classical algorithms

92
00:04:37,759 --> 00:04:42,360
because as we may know um the change of

93
00:04:40,639 --> 00:04:44,400
weather or the change of climate is

94
00:04:42,360 --> 00:04:47,680
chaotic so it is very difficult to

95
00:04:44,400 --> 00:04:51,400
predict it precisely so we may not want

96
00:04:47,680 --> 00:04:53,720
to have the exact physical um state of

97
00:04:51,400 --> 00:04:55,840
that moment what we want is some

98
00:04:53,720 --> 00:04:57,639
qualitative behavior let's say whether

99
00:04:55,840 --> 00:05:00,199
it's raining or whether it is windy at

100
00:04:57,639 --> 00:05:01,919
that moment and in this sense G models

101
00:05:00,199 --> 00:05:05,680
of deep learning could provide a very

102
00:05:01,919 --> 00:05:05,680
good um solution to this

103
00:05:06,120 --> 00:05:15,160
problem so actually um before the reason

104
00:05:10,520 --> 00:05:18,560
emergence of um gen models uh in in in

105
00:05:15,160 --> 00:05:21,319
um in our daily life um actually genive

106
00:05:18,560 --> 00:05:25,800
models have been used or developed um

107
00:05:21,319 --> 00:05:28,319
for decades so um this is a tool which

108
00:05:25,800 --> 00:05:31,880
is called patch match or um that's

109
00:05:28,319 --> 00:05:35,000
called as um content aware field in the

110
00:05:31,880 --> 00:05:38,880
software Photoshop so it was a very

111
00:05:35,000 --> 00:05:41,800
impressive tool um when I was um PhD

112
00:05:38,880 --> 00:05:44,319
students and at that time I work exactly

113
00:05:41,800 --> 00:05:47,000
on the same problem so the problem here

114
00:05:44,319 --> 00:05:50,120
is that you will be given a photo and

115
00:05:47,000 --> 00:05:52,919
the user can specify some area or some

116
00:05:50,120 --> 00:05:55,400
um structures in the photo and their

117
00:05:52,919 --> 00:05:57,759
computer algorithm is trying to fix the

118
00:05:55,400 --> 00:06:01,319
photo or edit the photo based on their

119
00:05:57,759 --> 00:06:03,400
uh um user um um instructions so

120
00:06:01,319 --> 00:06:06,560
actually at that time there is no deep

121
00:06:03,400 --> 00:06:08,520
learning and to be H for this

122
00:06:06,560 --> 00:06:10,800
application or for this um algorithm

123
00:06:08,520 --> 00:06:12,680
there is even no machine learning so it

124
00:06:10,800 --> 00:06:15,440
is a very classical computer vision

125
00:06:12,680 --> 00:06:17,120
algorithm but conceptually this is also

126
00:06:15,440 --> 00:06:20,080
a kind of generative

127
00:06:17,120 --> 00:06:22,319
modeling so the technique behind this

128
00:06:20,080 --> 00:06:26,440
generative model actually can Dat Back

129
00:06:22,319 --> 00:06:28,759
even um another 10 years ago so this is

130
00:06:26,440 --> 00:06:31,000
an algorithm that is called um texture

131
00:06:28,759 --> 00:06:33,199
synthesiz the goal here is that you will

132
00:06:31,000 --> 00:06:35,720
be given an example texture and you want

133
00:06:33,199 --> 00:06:37,880
to extend the texture to um to to a

134
00:06:35,720 --> 00:06:40,520
bigger image or or or you want to past

135
00:06:37,880 --> 00:06:43,800
the uh the texture to some 3D objects

136
00:06:40,520 --> 00:06:46,240
that um that you care about so the idea

137
00:06:43,800 --> 00:06:49,319
here is is just very simple you just try

138
00:06:46,240 --> 00:06:51,840
to synthesize um the texture pixel pixel

139
00:06:49,319 --> 00:06:54,400
by pixel based on what um has been

140
00:06:51,840 --> 00:06:58,120
synthesized so in today's word this is

141
00:06:54,400 --> 00:06:58,120
actually an autor regressive

142
00:06:58,319 --> 00:07:04,400
model okay so um then this is basically

143
00:07:02,440 --> 00:07:07,720
what I'm going to talk about in in in

144
00:07:04,400 --> 00:07:10,720
this talk I will very quickly um go

145
00:07:07,720 --> 00:07:13,400
through uh the concept of what is a

146
00:07:10,720 --> 00:07:15,960
generative model and then I will

147
00:07:13,400 --> 00:07:18,440
introduce some approaches some Modern

148
00:07:15,960 --> 00:07:20,400
approaches um to how we can build

149
00:07:18,440 --> 00:07:22,400
generative models using today's um deep

150
00:07:20,400 --> 00:07:26,400
neural networks and then I will also

151
00:07:22,400 --> 00:07:28,199
talk about um how we can um formulate um

152
00:07:26,400 --> 00:07:31,120
real world problems into kind of

153
00:07:28,199 --> 00:07:33,440
generative modeling

154
00:07:31,120 --> 00:07:36,800
okay so first what are generative models

155
00:07:33,440 --> 00:07:38,599
so um it turns out this is a very

156
00:07:36,800 --> 00:07:41,039
difficult question because when the

157
00:07:38,599 --> 00:07:43,280
gentic models becomes more powerful and

158
00:07:41,039 --> 00:07:46,159
and more and more powerful the scope of

159
00:07:43,280 --> 00:07:48,039
the gentic models keep changing so even

160
00:07:46,159 --> 00:07:50,360
though I will talk about um some

161
00:07:48,039 --> 00:07:52,319
classical definitions of gentic model I

162
00:07:50,360 --> 00:07:54,159
just want to say perhaps today every

163
00:07:52,319 --> 00:07:56,000
single problem could be formulated as a

164
00:07:54,159 --> 00:07:59,680
kind of gen

165
00:07:56,000 --> 00:08:01,759
models so now let's look at the

166
00:07:59,680 --> 00:08:04,560
applications or scenarios we have just

167
00:08:01,759 --> 00:08:08,080
introduced so what do these scenarios

168
00:08:04,560 --> 00:08:10,759
have in common so for example on image

169
00:08:08,080 --> 00:08:13,159
generation or video generation or and

170
00:08:10,759 --> 00:08:15,639
text generation actually there are

171
00:08:13,159 --> 00:08:17,879
multiple predictions or or or actually

172
00:08:15,639 --> 00:08:20,360
conceptually infinite predictions just

173
00:08:17,879 --> 00:08:22,960
to one input let's say if you want the

174
00:08:20,360 --> 00:08:25,479
computer to generate an image of cat so

175
00:08:22,960 --> 00:08:27,879
you will tell the computer this is a cat

176
00:08:25,479 --> 00:08:31,039
I want a cat and conceptually there is

177
00:08:27,879 --> 00:08:32,959
an infinite number of possible C so

178
00:08:31,039 --> 00:08:34,800
another property of generative model is

179
00:08:32,959 --> 00:08:37,279
that so some predictions are more

180
00:08:34,800 --> 00:08:41,039
plausible than some others so for

181
00:08:37,279 --> 00:08:43,800
example um if you want um let's say a

182
00:08:41,039 --> 00:08:46,880
cat and then um their computer May

183
00:08:43,800 --> 00:08:50,320
generate a lion or it can also generate

184
00:08:46,880 --> 00:08:52,839
a a dog and then perhaps for um in

185
00:08:50,320 --> 00:08:56,000
common sense a lion is more plausible

186
00:08:52,839 --> 00:08:58,079
than a dog in this scenario um of course

187
00:08:56,000 --> 00:09:01,720
a cat is more plausible than a

188
00:08:58,079 --> 00:09:03,680
lion and another very intriguing

189
00:09:01,720 --> 00:09:06,839
property of gentic modeling is that your

190
00:09:03,680 --> 00:09:09,440
training data may not contain the exact

191
00:09:06,839 --> 00:09:12,399
solution so as we have seen I believe

192
00:09:09,440 --> 00:09:14,800
the computer has never seen a teddy bear

193
00:09:12,399 --> 00:09:17,000
standing in front of a blackb and and

194
00:09:14,800 --> 00:09:19,480
teaching um generative models and

195
00:09:17,000 --> 00:09:22,760
similarly the computer may not um must

196
00:09:19,480 --> 00:09:26,839
have not seen these um paper planes

197
00:09:22,760 --> 00:09:30,079
flying on top of forest so this is kind

198
00:09:26,839 --> 00:09:32,120
of an outof distribution generation so

199
00:09:30,079 --> 00:09:34,560
the computer algorithms were trained on

200
00:09:32,120 --> 00:09:36,399
some data but what they are generating

201
00:09:34,560 --> 00:09:41,120
is some distribution that could be

202
00:09:36,399 --> 00:09:43,360
outside of uh um the training data so um

203
00:09:41,120 --> 00:09:45,360
in addition most of the time the

204
00:09:43,360 --> 00:09:47,959
predictions of generative models could

205
00:09:45,360 --> 00:09:49,440
be more complex and and more informative

206
00:09:47,959 --> 00:09:52,200
and conceptually it is higher

207
00:09:49,440 --> 00:09:55,120
dimensional um than their input so for

208
00:09:52,200 --> 00:09:57,320
example in text to image generation if

209
00:09:55,120 --> 00:09:59,720
you want a computer to generate a cat

210
00:09:57,320 --> 00:10:02,760
which is just a very short word and

211
00:09:59,720 --> 00:10:06,480
their output image would would have um

212
00:10:02,760 --> 00:10:09,560
um millions of pixels or maybe even more

213
00:10:06,480 --> 00:10:11,839
so um all these properties make J model

214
00:10:09,560 --> 00:10:13,680
um way more difficult than some of the

215
00:10:11,839 --> 00:10:15,959
classical um deep learning or

216
00:10:13,680 --> 00:10:19,560
recognition

217
00:10:15,959 --> 00:10:22,320
problems so um in a textbook this is

218
00:10:19,560 --> 00:10:26,399
kind of um formal definition of what

219
00:10:22,320 --> 00:10:28,600
would be um gentic model um usually when

220
00:10:26,399 --> 00:10:30,720
gentic model is introduced it people

221
00:10:28,600 --> 00:10:33,120
would compare it with a so-called

222
00:10:30,720 --> 00:10:36,399
discriminative model so what is a

223
00:10:33,120 --> 00:10:39,320
discriminative model so typically um as

224
00:10:36,399 --> 00:10:41,959
you have seen in Philip talk if we care

225
00:10:39,320 --> 00:10:44,360
about image an image classification

226
00:10:41,959 --> 00:10:45,920
problem you will be given an image then

227
00:10:44,360 --> 00:10:47,920
you are going to train a model for

228
00:10:45,920 --> 00:10:49,920
example a neuron Network then you want

229
00:10:47,920 --> 00:10:52,880
the neuron Network to Output a label

230
00:10:49,920 --> 00:10:55,839
let's say a doc and conceptually in this

231
00:10:52,880 --> 00:10:58,279
very simple scenario we can just imagine

232
00:10:55,839 --> 00:11:00,320
um generative model as reversing this

233
00:10:58,279 --> 00:11:03,320
process so in this case you would be

234
00:11:00,320 --> 00:11:05,639
given a dog and then you would like to

235
00:11:03,320 --> 00:11:07,680
train a model again which could be a

236
00:11:05,639 --> 00:11:10,440
neuron Network and then you want to

237
00:11:07,680 --> 00:11:12,480
Output the image which is X so in this

238
00:11:10,440 --> 00:11:14,480
case there would be many possible output

239
00:11:12,480 --> 00:11:16,800
many possible docks the output will be

240
00:11:14,480 --> 00:11:19,160
higher dimensional and the output the

241
00:11:16,800 --> 00:11:21,760
output would be another dock that you

242
00:11:19,160 --> 00:11:24,959
haven't seen

243
00:11:21,760 --> 00:11:26,279
before then un conceptually this is kind

244
00:11:24,959 --> 00:11:29,720
of a

245
00:11:26,279 --> 00:11:31,800
probabilistic um visualization of what

246
00:11:29,720 --> 00:11:35,760
would be a discriminative model and what

247
00:11:31,800 --> 00:11:39,800
would be a generative model so

248
00:11:35,760 --> 00:11:42,240
um so on the on the left um on the left

249
00:11:39,800 --> 00:11:44,880
hand side um is a discriminative model

250
00:11:42,240 --> 00:11:46,839
you have you have some um Green Dot

251
00:11:44,880 --> 00:11:48,959
which is one class and some orange dot

252
00:11:46,839 --> 00:11:51,120
which is another class so the goal of a

253
00:11:48,959 --> 00:11:53,079
discriminate model is just to find a

254
00:11:51,120 --> 00:11:57,000
boundary that can separate these two

255
00:11:53,079 --> 00:11:59,839
classes which seems easier then

256
00:11:57,000 --> 00:12:01,120
conceptually the task is try to find out

257
00:11:59,839 --> 00:12:03,079
this conditional probability

258
00:12:01,120 --> 00:12:05,959
distribution which means you will be

259
00:12:03,079 --> 00:12:08,160
given X such as an image then you want

260
00:12:05,959 --> 00:12:11,040
to estimate the probability of Y such as

261
00:12:08,160 --> 00:12:12,720
um it is a label zero or label one then

262
00:12:11,040 --> 00:12:14,920
as a comparison in the context of a

263
00:12:12,720 --> 00:12:16,959
generative model you would still be

264
00:12:14,920 --> 00:12:20,000
given the same data the same the same

265
00:12:16,959 --> 00:12:21,760
dots but the goal here is to estimate

266
00:12:20,000 --> 00:12:23,880
the probability uh the probability

267
00:12:21,760 --> 00:12:27,240
distribution of this dots let's say in

268
00:12:23,880 --> 00:12:29,519
the case of um this class which

269
00:12:27,240 --> 00:12:32,399
corresponds to yals to 1 you want to

270
00:12:29,519 --> 00:12:34,800
estimate what is the um conditional

271
00:12:32,399 --> 00:12:38,199
probability distribution of um this

272
00:12:34,800 --> 00:12:41,120
class so conceptually in alernative

273
00:12:38,199 --> 00:12:44,360
model we care about probabilistic

274
00:12:41,120 --> 00:12:46,720
modeling so that is the key problem

275
00:12:44,360 --> 00:12:50,040
alernative model want to address and

276
00:12:46,720 --> 00:12:53,320
that is also the key

277
00:12:50,040 --> 00:12:55,399
challenge so you may wonder why why

278
00:12:53,320 --> 00:12:58,600
there is probability and why we care

279
00:12:55,399 --> 00:13:00,480
about um probabilistic modeling and

280
00:12:58,600 --> 00:13:02,880
actually in many of the real world

281
00:13:00,480 --> 00:13:05,279
problems we can assume there are some

282
00:13:02,880 --> 00:13:07,760
underlying distributions and you can

283
00:13:05,279 --> 00:13:11,399
also assume your data is actually

284
00:13:07,760 --> 00:13:14,920
generated by some very complicated um

285
00:13:11,399 --> 00:13:18,040
word models so for example um if we care

286
00:13:14,920 --> 00:13:20,279
about a human face images um we can

287
00:13:18,040 --> 00:13:23,839
formulate the problem as there would be

288
00:13:20,279 --> 00:13:27,240
some um latent factors such as the poe

289
00:13:23,839 --> 00:13:29,440
the lighting um the scale and as

290
00:13:27,240 --> 00:13:31,639
actually the identity of the face so

291
00:13:29,440 --> 00:13:33,000
this would be the latent factors and

292
00:13:31,639 --> 00:13:35,680
then you assume there will be some

293
00:13:33,000 --> 00:13:37,920
distributions about these latent factors

294
00:13:35,680 --> 00:13:41,079
and these latent factors would be

295
00:13:37,920 --> 00:13:44,519
rendered by a word model so that is for

296
00:13:41,079 --> 00:13:48,639
example how you can um project a 3D

297
00:13:44,519 --> 00:13:52,040
object onto a a 2d GD of pixels and then

298
00:13:48,639 --> 00:13:54,440
this um um this latent um factors will

299
00:13:52,040 --> 00:13:57,639
be rendered by this word model and what

300
00:13:54,440 --> 00:13:59,639
you can actually observe is just a 2d GD

301
00:13:57,639 --> 00:14:03,199
so that is the observation X

302
00:13:59,639 --> 00:14:05,839
then your 2D GD would follow some very

303
00:14:03,199 --> 00:14:08,680
complicated distributions that cannot

304
00:14:05,839 --> 00:14:12,480
simply be described by some um um

305
00:14:08,680 --> 00:14:14,360
underlying distributions so this is um

306
00:14:12,480 --> 00:14:16,839
why we care about um probalistic

307
00:14:14,360 --> 00:14:19,600
modeling and a generative model is

308
00:14:16,839 --> 00:14:23,560
trying to to uncover these um underlying

309
00:14:19,600 --> 00:14:26,399
factors or or to to reverse this

310
00:14:23,560 --> 00:14:28,800
process now so for example let's say we

311
00:14:26,399 --> 00:14:32,120
have some data let's say I have a data

312
00:14:28,800 --> 00:14:34,399
set of docs which means I have many data

313
00:14:32,120 --> 00:14:37,639
points um and every single data point

314
00:14:34,399 --> 00:14:39,959
would correspond to one image of a doc

315
00:14:37,639 --> 00:14:42,240
then conceptually we imagine there is

316
00:14:39,959 --> 00:14:46,000
some underlying distribution that can

317
00:14:42,240 --> 00:14:48,040
model the distribution of all docs so um

318
00:14:46,000 --> 00:14:51,199
it's worth noticing that this is already

319
00:14:48,040 --> 00:14:54,000
part of your modeling um because you can

320
00:14:51,199 --> 00:14:56,440
model the underlying word generator uh

321
00:14:54,000 --> 00:14:58,240
in many different ways so even though we

322
00:14:56,440 --> 00:14:59,560
often assume there is this underlying

323
00:14:58,240 --> 00:15:02,880
distribution

324
00:14:59,560 --> 00:15:05,199
this distribution is a part of the

325
00:15:02,880 --> 00:15:08,639
modeling and then the goal of generative

326
00:15:05,199 --> 00:15:12,160
modeling is to learn a neuron Network or

327
00:15:08,639 --> 00:15:15,160
perhaps another model to approximate

328
00:15:12,160 --> 00:15:17,600
this distribution let's say um this this

329
00:15:15,160 --> 00:15:20,720
this red distribution is what we can

330
00:15:17,600 --> 00:15:25,839
learn um from a neuron Network and the

331
00:15:20,720 --> 00:15:27,959
goal here is to minimize um the distance

332
00:15:25,839 --> 00:15:29,720
between the data distribution and the

333
00:15:27,959 --> 00:15:31,600
distribution you estimate

334
00:15:29,720 --> 00:15:33,759
this is still a very difficult problem

335
00:15:31,600 --> 00:15:36,199
there are many solution to this problem

336
00:15:33,759 --> 00:15:37,959
but conceptually almost all um existing

337
00:15:36,199 --> 00:15:39,720
generative models could be um um

338
00:15:37,959 --> 00:15:42,880
formulated in this way and they are just

339
00:15:39,720 --> 00:15:44,240
trying to address the challenge um

340
00:15:42,880 --> 00:15:47,759
exposed by this

341
00:15:44,240 --> 00:15:50,120
problem then conceptually assuming um

342
00:15:47,759 --> 00:15:53,319
your model has done a good job of this

343
00:15:50,120 --> 00:15:56,480
then you can start to sample from the um

344
00:15:53,319 --> 00:15:59,079
distribution you estimated so um if your

345
00:15:56,480 --> 00:16:01,440
model is doing a good work that means

346
00:15:59,079 --> 00:16:04,720
when you sample from distribution this

347
00:16:01,440 --> 00:16:06,680
distribution you would you you will be

348
00:16:04,720 --> 00:16:09,160
doing something that is conceptually

349
00:16:06,680 --> 00:16:11,319
similar to um sampling from the original

350
00:16:09,160 --> 00:16:14,079
data distribution that in this case

351
00:16:11,319 --> 00:16:17,199
hopefully it will produce another doc

352
00:16:14,079 --> 00:16:21,560
that um your algorithm haven't

353
00:16:17,199 --> 00:16:24,880
seen then um it is um also possible to

354
00:16:21,560 --> 00:16:26,920
do the probability estimation so that is

355
00:16:24,880 --> 00:16:29,600
your model would be given another image

356
00:16:26,920 --> 00:16:35,000
let's say a cat and then you can ask the

357
00:16:29,600 --> 00:16:37,480
model um How likely this image um is

358
00:16:35,000 --> 00:16:40,360
under the original data distribution

359
00:16:37,480 --> 00:16:42,120
then in this case um if the original

360
00:16:40,360 --> 00:16:44,880
data distribution are about dogs and the

361
00:16:42,120 --> 00:16:48,600
input image is a cat then hopefully it

362
00:16:44,880 --> 00:16:51,759
will produce a low estimation of the

363
00:16:48,600 --> 00:16:56,560
probability density so um this is kind

364
00:16:51,759 --> 00:16:58,920
of um how we can um use probability

365
00:16:56,560 --> 00:17:01,800
modeling to uh um formulate the

366
00:16:58,920 --> 00:17:01,800
generative modeling

367
00:17:02,079 --> 00:17:08,480
problem um then as you can imagine the

368
00:17:05,039 --> 00:17:11,120
most powerful tool today for us to

369
00:17:08,480 --> 00:17:14,000
address generative modeling is deep

370
00:17:11,120 --> 00:17:16,199
learning so um Philip has given a very

371
00:17:14,000 --> 00:17:18,720
excellent and very quick um um

372
00:17:16,199 --> 00:17:21,520
introduction of what deep learning is so

373
00:17:18,720 --> 00:17:24,520
conceptually in a in a nutshell um deep

374
00:17:21,520 --> 00:17:27,480
learning is representation learning so

375
00:17:24,520 --> 00:17:29,919
what Philip has introduced is a process

376
00:17:27,480 --> 00:17:32,039
of learning to represent the data or

377
00:17:29,919 --> 00:17:34,160
conceptually the data instances that

378
00:17:32,039 --> 00:17:36,360
means you will be given the data let's

379
00:17:34,160 --> 00:17:40,240
say the images and then you want to map

380
00:17:36,360 --> 00:17:42,600
the images to labels so um this is one

381
00:17:40,240 --> 00:17:43,799
way of using deep new networks for

382
00:17:42,600 --> 00:17:46,120
representation

383
00:17:43,799 --> 00:17:47,960
learning then in the case of um

384
00:17:46,120 --> 00:17:50,240
generative modeling actually there is

385
00:17:47,960 --> 00:17:51,919
another way of using deep learning but

386
00:17:50,240 --> 00:17:55,000
still for the goal of representation

387
00:17:51,919 --> 00:17:57,360
learning so that is we don't just want

388
00:17:55,000 --> 00:17:59,400
to learn the representation of one

389
00:17:57,360 --> 00:18:01,400
single data instances

390
00:17:59,400 --> 00:18:04,159
so we want to learn the representation

391
00:18:01,400 --> 00:18:06,120
of a probability distribution so that is

392
00:18:04,159 --> 00:18:08,799
a more complicated problem and

393
00:18:06,120 --> 00:18:11,320
conceptually it can be viewed as

394
00:18:08,799 --> 00:18:14,400
learning the mapping the other way let's

395
00:18:11,320 --> 00:18:17,760
say um here the output would be the

396
00:18:14,400 --> 00:18:19,559
labels let's say um on the um the label

397
00:18:17,760 --> 00:18:23,200
of cats or the label of dogs and then

398
00:18:19,559 --> 00:18:27,480
you want to m m map it back to the pxel

399
00:18:23,200 --> 00:18:30,200
space so um then as you can imagine deep

400
00:18:27,480 --> 00:18:32,200
learning or deep uh networks is a very

401
00:18:30,200 --> 00:18:34,400
powerful tool for generative modeling

402
00:18:32,200 --> 00:18:37,840
and conceptually when you use this tool

403
00:18:34,400 --> 00:18:40,600
for this problem the models are actually

404
00:18:37,840 --> 00:18:42,559
simultaneously playing these two roles

405
00:18:40,600 --> 00:18:45,000
so first learning to represent data

406
00:18:42,559 --> 00:18:48,120
instances and second learning to

407
00:18:45,000 --> 00:18:48,120
represent probability

408
00:18:48,480 --> 00:18:53,640
distributions then this is conceptually

409
00:18:50,880 --> 00:18:55,919
what a model would be look like so um

410
00:18:53,640 --> 00:18:57,679
your model will be given a very simple

411
00:18:55,919 --> 00:18:59,679
distribution for example um it could be

412
00:18:57,679 --> 00:19:01,760
a gaan distribution or it could be a

413
00:18:59,679 --> 00:19:03,600
uniform distribution it doesn't matter

414
00:19:01,760 --> 00:19:06,200
so in the case of an image this would

415
00:19:03,600 --> 00:19:09,280
look like just a a completely noisy

416
00:19:06,200 --> 00:19:12,440
image then the goal is to learn a neural

417
00:19:09,280 --> 00:19:16,520
network such that it can map a noisy

418
00:19:12,440 --> 00:19:18,840
image to just another image um in in in

419
00:19:16,520 --> 00:19:21,159
in the output space then conceptually if

420
00:19:18,840 --> 00:19:24,880
your model can do a good work hopefully

421
00:19:21,159 --> 00:19:27,120
the output would be a visually

422
00:19:24,880 --> 00:19:31,960
reasonable image such as a doc in this

423
00:19:27,120 --> 00:19:34,799
case then um you can just keep sampling

424
00:19:31,960 --> 00:19:36,520
um noise from the input distribution and

425
00:19:34,799 --> 00:19:38,600
hopefully the neuron network will turn

426
00:19:36,520 --> 00:19:41,559
everything into some meaningful images

427
00:19:38,600 --> 00:19:44,200
in the output then conceptually when you

428
00:19:41,559 --> 00:19:47,120
do this actually your neuron network is

429
00:19:44,200 --> 00:19:49,200
trying to map a simple distribution

430
00:19:47,120 --> 00:19:51,200
let's say um gaussin distribution here

431
00:19:49,200 --> 00:19:53,600
to another distribution which

432
00:19:51,200 --> 00:19:56,080
conceptually is to approximate the

433
00:19:53,600 --> 00:19:58,720
underlying data distribution then in

434
00:19:56,080 --> 00:20:01,480
this sense a generative model is a

435
00:19:58,720 --> 00:20:04,200
mapping between distributions it is not

436
00:20:01,480 --> 00:20:07,640
just a mapping between a pair of data

437
00:20:04,200 --> 00:20:11,799
points and and and a label it is it goes

438
00:20:07,640 --> 00:20:11,799
from one distribution to another

439
00:20:12,200 --> 00:20:16,240
distribution um the next slid would be a

440
00:20:14,679 --> 00:20:18,360
little bit technical perhaps and I can

441
00:20:16,240 --> 00:20:20,200
go very quickly so these are some of the

442
00:20:18,360 --> 00:20:22,559
um um fundamental elements of a

443
00:20:20,200 --> 00:20:25,600
deorative model so first of all you may

444
00:20:22,559 --> 00:20:29,240
need to formulate a real world problem

445
00:20:25,600 --> 00:20:32,440
um as a probabilistic model or or

446
00:20:29,240 --> 00:20:35,320
um ative model this is kind of one of

447
00:20:32,440 --> 00:20:37,799
the most critical Parts um for us to to

448
00:20:35,320 --> 00:20:40,280
to design an algorithm and after you can

449
00:20:37,799 --> 00:20:42,600
do that you need some representations

450
00:20:40,280 --> 00:20:44,360
and today um usually this is a neural

451
00:20:42,600 --> 00:20:47,520
network so you want to represent the

452
00:20:44,360 --> 00:20:49,039
data and their distribution and then um

453
00:20:47,520 --> 00:20:50,919
you need to introduce some objective

454
00:20:49,039 --> 00:20:52,640
functions to measure the difference

455
00:20:50,919 --> 00:20:54,320
between two distributions and then you

456
00:20:52,640 --> 00:20:56,440
need an Optimizer that can solve the

457
00:20:54,320 --> 00:20:58,039
very difficult optimization problem and

458
00:20:56,440 --> 00:20:59,640
then you also need an inference

459
00:20:58,039 --> 00:21:01,320
algorithm which is conceptually a

460
00:20:59,640 --> 00:21:05,320
sampler that can sample from the

461
00:21:01,320 --> 00:21:08,200
underlying distribution um so um today

462
00:21:05,320 --> 00:21:12,039
many of the uh um mathematical or

463
00:21:08,200 --> 00:21:15,440
theoretical um research would be about

464
00:21:12,039 --> 00:21:18,880
um one or many element in in this list

465
00:21:15,440 --> 00:21:21,640
um so I'm not going to um delve into the

466
00:21:18,880 --> 00:21:23,960
details but next I'm going to give a

467
00:21:21,640 --> 00:21:26,120
very high level and very quick overview

468
00:21:23,960 --> 00:21:29,440
of um what are some of the modern

469
00:21:26,120 --> 00:21:31,760
approaches and popular approaches um to

470
00:21:29,440 --> 00:21:34,200
generative models and and I'm also going

471
00:21:31,760 --> 00:21:36,960
to explain why a generative model is a

472
00:21:34,200 --> 00:21:40,480
hard problem so this is the figure you

473
00:21:36,960 --> 00:21:42,919
have just seen so um as as you can see

474
00:21:40,480 --> 00:21:45,039
the problem here is that if your model

475
00:21:42,919 --> 00:21:47,440
will be given one noisy uh image or

476
00:21:45,039 --> 00:21:50,120
noise input you you you want it to map

477
00:21:47,440 --> 00:21:54,760
it um to to map the noise to an output

478
00:21:50,120 --> 00:21:56,720
image so why this is hard so um recall

479
00:21:54,760 --> 00:21:58,760
that in Philip's talk he has talked

480
00:21:56,720 --> 00:22:01,039
about the problem of supervise learning

481
00:21:58,760 --> 00:22:04,679
so in that case you will be given one

482
00:22:01,039 --> 00:22:08,400
image and also a label of that image so

483
00:22:04,679 --> 00:22:10,840
you have a pair of input and output so

484
00:22:08,400 --> 00:22:12,760
that is a very well formulated problem

485
00:22:10,840 --> 00:22:15,720
of supervise learning and that problem

486
00:22:12,760 --> 00:22:19,200
is easy for modern um neuron networks to

487
00:22:15,720 --> 00:22:21,880
solve but in the case of gentic modeling

488
00:22:19,200 --> 00:22:24,559
conceptually it is an unsupervised

489
00:22:21,880 --> 00:22:27,039
learning problem so that is you will be

490
00:22:24,559 --> 00:22:29,240
given an image but then conceptually you

491
00:22:27,039 --> 00:22:31,840
have no idea what

492
00:22:29,240 --> 00:22:35,120
and input noise would corresponds to

493
00:22:31,840 --> 00:22:38,039
that image then this correspondence or

494
00:22:35,120 --> 00:22:40,120
this pairing problem is also what your

495
00:22:38,039 --> 00:22:43,039
underlying algorithm should try to

496
00:22:40,120 --> 00:22:45,159
figure out so then in this sense

497
00:22:43,039 --> 00:22:49,640
conceptually it is not just about

498
00:22:45,159 --> 00:22:51,679
mapping pairs um of images or pairs of

499
00:22:49,640 --> 00:22:54,120
data it's about mapping two

500
00:22:51,679 --> 00:22:56,480
distributions so you want to map a

501
00:22:54,120 --> 00:22:58,320
simple gaussian distribution to a very

502
00:22:56,480 --> 00:23:02,520
complicated data distribution and this

503
00:22:58,320 --> 00:23:06,480
is is why gentic modeling is

504
00:23:02,520 --> 00:23:08,320
hard so there are many um effective and

505
00:23:06,480 --> 00:23:10,640
very smart algorithm to address this

506
00:23:08,320 --> 00:23:13,520
problem so I will start from some um

507
00:23:10,640 --> 00:23:15,960
very fundamental and and and and very

508
00:23:13,520 --> 00:23:17,799
elegant algorithm and then um I will

509
00:23:15,960 --> 00:23:21,120
start to talk about um some of the state

510
00:23:17,799 --> 00:23:22,559
ofth Arts algorithm today so um first I

511
00:23:21,120 --> 00:23:26,159
will talk about um variational

512
00:23:22,559 --> 00:23:28,080
autoencoders of vae so conceptually in

513
00:23:26,159 --> 00:23:31,039
generative model as we have introduced

514
00:23:28,080 --> 00:23:33,240
you want to map an input distribution to

515
00:23:31,039 --> 00:23:35,240
an output distribution then we can

516
00:23:33,240 --> 00:23:37,559
formulate this as an autoencoding

517
00:23:35,240 --> 00:23:39,880
problem that means if you will have the

518
00:23:37,559 --> 00:23:42,400
distribution of the data then you can

519
00:23:39,880 --> 00:23:44,960
train another neuron Network to map the

520
00:23:42,400 --> 00:23:47,159
data distribution to the the

521
00:23:44,960 --> 00:23:50,120
distribution you like let's say a gon

522
00:23:47,159 --> 00:23:51,880
distribution then you can then then

523
00:23:50,120 --> 00:23:54,640
after you have this distribution you can

524
00:23:51,880 --> 00:23:56,960
learn the generator to to um transform

525
00:23:54,640 --> 00:23:58,600
it back then conceptually you compute

526
00:23:56,960 --> 00:24:00,960
the distance between the inputs and and

527
00:23:58,600 --> 00:24:03,679
the output so this is a the very

528
00:24:00,960 --> 00:24:06,240
classical idea of Auto encoding in deep

529
00:24:03,679 --> 00:24:08,039
learning but uh In classical algorithms

530
00:24:06,240 --> 00:24:10,960
usually conceptually this would be

531
00:24:08,039 --> 00:24:13,080
applied to the concept of U um data

532
00:24:10,960 --> 00:24:15,640
instances that is you apply this to

533
00:24:13,080 --> 00:24:18,200
every single images so in the case of

534
00:24:15,640 --> 00:24:21,960
variational autoencoder conceptually the

535
00:24:18,200 --> 00:24:24,200
concept of autoencoding is applied on

536
00:24:21,960 --> 00:24:26,559
the distribution so you can just imagine

537
00:24:24,200 --> 00:24:28,600
this distribution is is just one object

538
00:24:26,559 --> 00:24:31,440
it's just one entity that you want to

539
00:24:28,600 --> 00:24:34,159
you you you you want to uh process so

540
00:24:31,440 --> 00:24:36,039
you transform this object into this

541
00:24:34,159 --> 00:24:40,679
simpler object and then you transform it

542
00:24:36,039 --> 00:24:44,520
back so then this is um the autoencoding

543
00:24:40,679 --> 00:24:48,440
idea the another very popular solution

544
00:24:44,520 --> 00:24:50,360
um that is kind of the beginning of um

545
00:24:48,440 --> 00:24:52,720
um research in generative modeling 10

546
00:24:50,360 --> 00:24:56,200
years ago um is called the generative

547
00:24:52,720 --> 00:24:59,240
aers real networks or in short G and or

548
00:24:56,200 --> 00:25:01,399
again um concept

549
00:24:59,240 --> 00:25:03,320
again also just want to learn a

550
00:25:01,399 --> 00:25:05,440
generator that goes from a simple

551
00:25:03,320 --> 00:25:07,960
distribution to the data distribution

552
00:25:05,440 --> 00:25:11,799
but instead of introducing another

553
00:25:07,960 --> 00:25:14,600
Network before the data uh before the

554
00:25:11,799 --> 00:25:17,039
simple distribution in the case of again

555
00:25:14,600 --> 00:25:20,320
it it introduced the extion network

556
00:25:17,039 --> 00:25:23,039
after um you you you have obtained the U

557
00:25:20,320 --> 00:25:25,080
um estimate distribution so um this

558
00:25:23,039 --> 00:25:27,679
extra neur uh neuron Network would be

559
00:25:25,080 --> 00:25:30,480
called a discriminator the goal of the

560
00:25:27,679 --> 00:25:33,000
discriminator is to tell whether your

561
00:25:30,480 --> 00:25:35,559
sample is from the predicted

562
00:25:33,000 --> 00:25:37,960
distribution or is from the real

563
00:25:35,559 --> 00:25:40,120
distribution then if discriminator

564
00:25:37,960 --> 00:25:43,520
cannot tell which distribution it is

565
00:25:40,120 --> 00:25:46,320
from then that then it means this these

566
00:25:43,520 --> 00:25:50,600
two distributions would be very similar

567
00:25:46,320 --> 00:25:52,440
so um gen is kind of the most uh um

568
00:25:50,600 --> 00:25:56,760
popular and most powerful generative

569
00:25:52,440 --> 00:25:59,640
models um over the last um decades until

570
00:25:56,760 --> 00:26:02,919
um um the there are some very powerful

571
00:25:59,640 --> 00:26:05,360
tools um came out over the last um three

572
00:26:02,919 --> 00:26:09,240
or four

573
00:26:05,360 --> 00:26:11,880
years so um another very powerful

574
00:26:09,240 --> 00:26:15,120
generative modeling tool is called Auto

575
00:26:11,880 --> 00:26:17,840
regressive models and in the context of

576
00:26:15,120 --> 00:26:21,440
um natural language processing this is

577
00:26:17,840 --> 00:26:24,440
usually known as next token prediction

578
00:26:21,440 --> 00:26:27,520
um but concep the idea of Auto

579
00:26:24,440 --> 00:26:31,039
regressive or Auto regression is more

580
00:26:27,520 --> 00:26:34,919
than just in the next token so um

581
00:26:31,039 --> 00:26:38,039
basically if we care about um

582
00:26:34,919 --> 00:26:42,360
probability that involve many um

583
00:26:38,039 --> 00:26:45,039
elements or many variables then um

584
00:26:42,360 --> 00:26:47,679
following the very basic principle of um

585
00:26:45,039 --> 00:26:50,480
probability Theory we can always

586
00:26:47,679 --> 00:26:54,399
decompose this joint probability into a

587
00:26:50,480 --> 00:26:56,799
train of many conditional probabilities

588
00:26:54,399 --> 00:26:58,840
so the key idea of autor regressive

589
00:26:56,799 --> 00:27:03,559
modeling is

590
00:26:58,840 --> 00:27:04,760
to um model every single conditional

591
00:27:03,559 --> 00:27:08,200
property

592
00:27:04,760 --> 00:27:12,240
individually um rather than modeling the

593
00:27:08,200 --> 00:27:14,360
the entire joint probability so if you

594
00:27:12,240 --> 00:27:16,679
do this decomposition following the

595
00:27:14,360 --> 00:27:19,240
order of the sequence let's say in this

596
00:27:16,679 --> 00:27:22,320
case you want to predict X1 first and

597
00:27:19,240 --> 00:27:24,440
then you predict X2 um conditional on X1

598
00:27:22,320 --> 00:27:27,000
and so and so forth if you follow this

599
00:27:24,440 --> 00:27:30,080
sequential order then you can turn your

600
00:27:27,000 --> 00:27:33,440
problem into next token prediction so um

601
00:27:30,080 --> 00:27:37,480
this idea of um Auto regressive model is

602
00:27:33,440 --> 00:27:40,760
to break a very complicated problem into

603
00:27:37,480 --> 00:27:44,559
a bunch of simpler and smaller

604
00:27:40,760 --> 00:27:47,559
problems so for example um in this case

605
00:27:44,559 --> 00:27:50,120
um in in the first output you you will

606
00:27:47,559 --> 00:27:52,960
estimate a very simple and lower

607
00:27:50,120 --> 00:27:54,440
dimensional um distribution um in this

608
00:27:52,960 --> 00:27:56,480
illustration for example it would be a

609
00:27:54,440 --> 00:27:59,320
one-dimensional distribution and then in

610
00:27:56,480 --> 00:28:01,679
the second note it will predict um the

611
00:27:59,320 --> 00:28:03,039
next dimension of the variable that it

612
00:28:01,679 --> 00:28:05,279
will be a two-dimensional uh

613
00:28:03,039 --> 00:28:07,039
distribution and so and so forth uh it

614
00:28:05,279 --> 00:28:09,080
will be difficult to to visualize a

615
00:28:07,039 --> 00:28:11,840
higher dimensional distribution but

616
00:28:09,080 --> 00:28:13,640
conceptually when you do this um it

617
00:28:11,840 --> 00:28:16,120
would be a distribution in a high

618
00:28:13,640 --> 00:28:19,440
dimensional space so this is the the key

619
00:28:16,120 --> 00:28:19,440
idea of autor regressive

620
00:28:20,320 --> 00:28:26,080
modeling then um over the last three or

621
00:28:23,679 --> 00:28:29,399
four years there is a very powerful

622
00:28:26,080 --> 00:28:31,440
model emerging especially in the context

623
00:28:29,399 --> 00:28:35,279
of image generation and in computer

624
00:28:31,440 --> 00:28:39,799
vision so this model was motivated by um

625
00:28:35,279 --> 00:28:43,720
thermodynamics in physics so um the idea

626
00:28:39,799 --> 00:28:45,799
is that um you can you can um formulate

627
00:28:43,720 --> 00:28:50,000
the problem

628
00:28:45,799 --> 00:28:52,880
as um repeatedly corrupting the clean

629
00:28:50,000 --> 00:28:55,320
data or input image by adding gaussian

630
00:28:52,880 --> 00:28:59,279
noise and then you can progressively

631
00:28:55,320 --> 00:29:01,679
turn it into a fully noise image and

632
00:28:59,279 --> 00:29:04,080
then the learning the goal of learning

633
00:29:01,679 --> 00:29:06,159
is to reverse this process and if you

634
00:29:04,080 --> 00:29:10,559
can do that then you can progressively

635
00:29:06,159 --> 00:29:12,880
go from a noisy input back to the clean

636
00:29:10,559 --> 00:29:15,480
image and this this idea is called a

637
00:29:12,880 --> 00:29:17,840
diffusion or it is often Al also called

638
00:29:15,480 --> 00:29:21,279
denoising

639
00:29:17,840 --> 00:29:23,399
diffusion so conceptually um using the

640
00:29:21,279 --> 00:29:26,240
terminology of um probability or

641
00:29:23,399 --> 00:29:29,000
probability distribution this means you

642
00:29:26,240 --> 00:29:30,880
will have an input data distribution

643
00:29:29,000 --> 00:29:33,279
hopefully uh it will be about clean

644
00:29:30,880 --> 00:29:36,000
images and then you just repeat

645
00:29:33,279 --> 00:29:38,200
repeatedly adding noise on top of it

646
00:29:36,000 --> 00:29:42,360
then conceptually this is just like

647
00:29:38,200 --> 00:29:45,320
running a convolutional kernel on top of

648
00:29:42,360 --> 00:29:47,399
the distribution space and by doing it

649
00:29:45,320 --> 00:29:50,840
many times ultimately you will turn the

650
00:29:47,399 --> 00:29:53,320
data distribution into a gan

651
00:29:50,840 --> 00:29:56,320
distribution so then your model is just

652
00:29:53,320 --> 00:29:58,720
trying to to to learn to reverse this

653
00:29:56,320 --> 00:30:02,200
process

654
00:29:58,720 --> 00:30:05,080
so this is what a diffusion model um may

655
00:30:02,200 --> 00:30:09,320
look like um at inference time it will

656
00:30:05,080 --> 00:30:11,799
start from a very um simple distribution

657
00:30:09,320 --> 00:30:14,159
say a gin and then it will progressively

658
00:30:11,799 --> 00:30:15,880
reverse the process and go back to the

659
00:30:14,159 --> 00:30:18,760
data

660
00:30:15,880 --> 00:30:22,279
distribution so actually this visual uh

661
00:30:18,760 --> 00:30:25,120
this visualization is very similar to

662
00:30:22,279 --> 00:30:28,360
many of the concepts um that are popular

663
00:30:25,120 --> 00:30:30,799
in graphics so for example you can

664
00:30:28,360 --> 00:30:36,159
imagine the starting points of this

665
00:30:30,799 --> 00:30:39,559
process is some colonal shape let's say

666
00:30:36,159 --> 00:30:42,919
um it would be a sphere or it would be a

667
00:30:39,559 --> 00:30:47,039
cylinder then you want to progressively

668
00:30:42,919 --> 00:30:49,440
morph or warp this object this shape

669
00:30:47,039 --> 00:30:52,320
into another shape that you like let's

670
00:30:49,440 --> 00:30:56,679
say um this could be for example just a

671
00:30:52,320 --> 00:30:58,799
mountain or um a bunny so you want to

672
00:30:56,679 --> 00:31:03,240
progressively walk

673
00:30:58,799 --> 00:31:06,120
the input sphere to a bunny and this is

674
00:31:03,240 --> 00:31:10,080
a very well studied problem so inic case

675
00:31:06,120 --> 00:31:11,600
of distribution modeling we can imagine

676
00:31:10,080 --> 00:31:15,919
this

677
00:31:11,600 --> 00:31:18,880
distribution literally as a geometric

678
00:31:15,919 --> 00:31:22,399
entity and then you can formulate a

679
00:31:18,880 --> 00:31:25,440
process to do this transformation and

680
00:31:22,399 --> 00:31:28,320
actually what I have just described is

681
00:31:25,440 --> 00:31:32,000
kind of an emerging idea with which is

682
00:31:28,320 --> 00:31:34,840
called flow matching so you want to flow

683
00:31:32,000 --> 00:31:38,399
from a very simple object or very simple

684
00:31:34,840 --> 00:31:42,320
shape such as a sphere to another more

685
00:31:38,399 --> 00:31:44,519
complicated shape such as a bunny and if

686
00:31:42,320 --> 00:31:47,279
you have this algorithm and then if you

687
00:31:44,519 --> 00:31:49,480
formulate your underlying shape as some

688
00:31:47,279 --> 00:31:52,880
probability distributions and then you

689
00:31:49,480 --> 00:31:55,600
can use this idea to do a um probability

690
00:31:52,880 --> 00:31:58,039
modeling that is um generative modeling

691
00:31:55,600 --> 00:32:00,080
so here conceptually this is just

692
00:31:58,039 --> 00:32:02,679
another visualization of the same thing

693
00:32:00,080 --> 00:32:06,440
you will be starting from some

694
00:32:02,679 --> 00:32:08,279
um simple distribution let's say a gaan

695
00:32:06,440 --> 00:32:10,919
and this would be your data distribution

696
00:32:08,279 --> 00:32:14,120
that you want to model the goal here is

697
00:32:10,919 --> 00:32:17,960
to progressively change your input

698
00:32:14,120 --> 00:32:17,960
distribution to the output

699
00:32:18,320 --> 00:32:23,679
distribution then um there are many

700
00:32:21,559 --> 00:32:26,600
excellent Solutions in computer Graphics

701
00:32:23,679 --> 00:32:28,600
to this problem so one idea here is to

702
00:32:26,600 --> 00:32:31,480
learn a flow field

703
00:32:28,600 --> 00:32:34,039
so you can imagine if this is literally

704
00:32:31,480 --> 00:32:37,039
a 3D object then you will have some um

705
00:32:34,039 --> 00:32:39,600
3D vertexes or 3D surfaces so you want

706
00:32:37,039 --> 00:32:42,639
to gradually move these three uh uh the

707
00:32:39,600 --> 00:32:45,320
these 3D surfaces um from the sphere to

708
00:32:42,639 --> 00:32:48,200
some 3D surfaces um in in in your in

709
00:32:45,320 --> 00:32:50,480
your bunny so um then if you do that

710
00:32:48,200 --> 00:32:54,240
then there will be a flow field that can

711
00:32:50,480 --> 00:32:56,440
be constructed um um via this process so

712
00:32:54,240 --> 00:32:58,480
um there there will be a lot of

713
00:32:56,440 --> 00:33:00,639
mathematic details behind F and of

714
00:32:58,480 --> 00:33:03,399
course I'm not going to delve into it um

715
00:33:00,639 --> 00:33:06,320
but this is kind of the high level idea

716
00:33:03,399 --> 00:33:09,399
of um the latest progress in um um

717
00:33:06,320 --> 00:33:12,000
generative modeling that is um flow

718
00:33:09,399 --> 00:33:15,240
matching so U conceptually these are

719
00:33:12,000 --> 00:33:19,279
some of the popular approaches um today

720
00:33:15,240 --> 00:33:21,639
um to gentic models um I I haven't

721
00:33:19,279 --> 00:33:24,279
covered any of the mathematical details

722
00:33:21,639 --> 00:33:27,639
um but it is kind of fun to walk through

723
00:33:24,279 --> 00:33:30,639
all these methods um the point I'm going

724
00:33:27,639 --> 00:33:33,440
to to make is that in all of these

725
00:33:30,639 --> 00:33:36,519
generative models there would be some

726
00:33:33,440 --> 00:33:40,279
deep neuron networks as the building

727
00:33:36,519 --> 00:33:42,960
block and conceptually this is just like

728
00:33:40,279 --> 00:33:46,039
in deep neuron networks there would be

729
00:33:42,960 --> 00:33:49,360
some layers as the building block so the

730
00:33:46,039 --> 00:33:51,159
layers are those uh um modules that um

731
00:33:49,360 --> 00:33:52,760
Philip have just introduced so they

732
00:33:51,159 --> 00:33:54,880
could be a linear layer they could be a

733
00:33:52,760 --> 00:33:57,919
Ru um they could be a normalization

734
00:33:54,880 --> 00:33:59,600
layer or a soft Max layer so neuron

735
00:33:57,919 --> 00:34:03,760
networks

736
00:33:59,600 --> 00:34:06,720
are some entities that thata is built by

737
00:34:03,760 --> 00:34:09,159
so-called layers and today these gen

738
00:34:06,720 --> 00:34:11,839
genitive models are some entities that

739
00:34:09,159 --> 00:34:14,599
are built by Deep neuron networks so in

740
00:34:11,839 --> 00:34:18,240
this sense the gentic models are the

741
00:34:14,599 --> 00:34:18,240
next level of um

742
00:34:19,520 --> 00:34:24,599
abstractions okay um then next I will

743
00:34:22,320 --> 00:34:26,599
talk about how we can use this

744
00:34:24,599 --> 00:34:28,280
mathematical models or this uh um

745
00:34:26,599 --> 00:34:30,960
theoretical models of of generative

746
00:34:28,280 --> 00:34:33,159
modeling um in the context of solving a

747
00:34:30,960 --> 00:34:38,440
real world

748
00:34:33,159 --> 00:34:40,919
problems so as we have um introduce the

749
00:34:38,440 --> 00:34:44,040
the key problem in generative model is

750
00:34:40,919 --> 00:34:46,800
about this conditional distribution so

751
00:34:44,040 --> 00:34:49,760
you you you want to model a distribution

752
00:34:46,800 --> 00:34:53,800
that conceptually um you will be given

753
00:34:49,760 --> 00:34:57,880
the condition why and um it is about the

754
00:34:53,800 --> 00:35:02,760
distribution of your data X but then

755
00:34:57,880 --> 00:35:03,920
reality what what is y and what is x in

756
00:35:02,760 --> 00:35:07,040
in common

757
00:35:03,920 --> 00:35:09,680
terminology Y is called the conditions

758
00:35:07,040 --> 00:35:11,920
let's say you want to generate a cat um

759
00:35:09,680 --> 00:35:15,640
it could also be some constraints let's

760
00:35:11,920 --> 00:35:18,920
say um you you you you don't want to uh

761
00:35:15,640 --> 00:35:22,119
generate um some type of output images

762
00:35:18,920 --> 00:35:24,520
it could also be labels or text labels

763
00:35:22,119 --> 00:35:26,720
or or maybe some other labels and it

764
00:35:24,520 --> 00:35:29,240
could also be um attribute let's say you

765
00:35:26,720 --> 00:35:33,079
want to generate a big object or a small

766
00:35:29,240 --> 00:35:35,119
object so in most of the case the label

767
00:35:33,079 --> 00:35:38,400
um the condition y would be more

768
00:35:35,119 --> 00:35:41,680
abstract and it will be less

769
00:35:38,400 --> 00:35:46,040
informative and as a comparison the

770
00:35:41,680 --> 00:35:48,640
output X is usually called the data or

771
00:35:46,040 --> 00:35:52,079
it would be the observations or or

772
00:35:48,640 --> 00:35:54,960
measurements um of the

773
00:35:52,079 --> 00:35:57,200
um of the samples that you can that you

774
00:35:54,960 --> 00:35:59,240
can see in the real world problems so in

775
00:35:57,200 --> 00:36:03,359
the case of of image generation then

776
00:35:59,240 --> 00:36:05,640
usually X is just the image so usually X

777
00:36:03,359 --> 00:36:07,079
would be more concrete than the

778
00:36:05,640 --> 00:36:10,839
condition Y and it would be more

779
00:36:07,079 --> 00:36:13,760
informative it would be more um higher

780
00:36:10,839 --> 00:36:16,680
dimensional um now let's go through the

781
00:36:13,760 --> 00:36:18,599
um applications we have just introduced

782
00:36:16,680 --> 00:36:23,280
and and let's discuss what would be X

783
00:36:18,599 --> 00:36:25,359
and what would be y um in the case of uh

784
00:36:23,280 --> 00:36:28,319
in in in the context of a natural

785
00:36:25,359 --> 00:36:30,040
language conversation or chatbot the

786
00:36:28,319 --> 00:36:34,240
condition y would be the so-called

787
00:36:30,040 --> 00:36:36,119
prompt that is given by the user um and

788
00:36:34,240 --> 00:36:38,800
the output X would be the response of

789
00:36:36,119 --> 00:36:40,520
the chatbot so usually the output is

790
00:36:38,800 --> 00:36:43,560
higher dimensional and there would be

791
00:36:40,520 --> 00:36:46,079
many um plausible outputs that can

792
00:36:43,560 --> 00:36:48,720
correspond to the same

793
00:36:46,079 --> 00:36:50,839
prompt and similarly in the context of

794
00:36:48,720 --> 00:36:53,480
um text to image generation or text to

795
00:36:50,839 --> 00:36:55,520
video Generation Um the condition would

796
00:36:53,480 --> 00:36:57,599
be the text prompt um it could be a

797
00:36:55,520 --> 00:36:59,920
sentence it could be a class label it

798
00:36:57,599 --> 00:37:01,720
could be some attribute and the output

799
00:36:59,920 --> 00:37:03,280
would be the generated visual content

800
00:37:01,720 --> 00:37:05,720
such as an image and video and the

801
00:37:03,280 --> 00:37:08,280
output is higher dimensional it is more

802
00:37:05,720 --> 00:37:11,440
complicated so these are kind of typical

803
00:37:08,280 --> 00:37:15,079
use cases and of course this is also the

804
00:37:11,440 --> 00:37:17,599
case uh in terms of um 3D generation in

805
00:37:15,079 --> 00:37:20,240
this case the condition would still be a

806
00:37:17,599 --> 00:37:23,880
text prompt and the output would be um

807
00:37:20,240 --> 00:37:25,880
the 3D text structures um in this um

808
00:37:23,880 --> 00:37:28,319
computer vision of Graphics application

809
00:37:25,880 --> 00:37:31,480
the the 3D text structure would be the

810
00:37:28,319 --> 00:37:35,040
shapes um textures or or maybe even

811
00:37:31,480 --> 00:37:35,040
illuminations of the underlying

812
00:37:35,480 --> 00:37:41,200
object and then we can move one step

813
00:37:38,200 --> 00:37:43,200
further um um then we can generalize the

814
00:37:41,200 --> 00:37:48,119
scenario to to the problem of let's say

815
00:37:43,200 --> 00:37:51,680
protain generation in this case um the

816
00:37:48,119 --> 00:37:54,240
input condition could still be some

817
00:37:51,680 --> 00:37:56,960
prompt it could still be some text let's

818
00:37:54,240 --> 00:38:00,200
say you can you can try to tell the

819
00:37:56,960 --> 00:38:04,280
computer that I want to generate a

820
00:38:00,200 --> 00:38:06,640
protein that can cure cancer so that is

821
00:38:04,280 --> 00:38:09,079
valid but the problem here is that

822
00:38:06,640 --> 00:38:11,440
there's no way for the computer to

823
00:38:09,079 --> 00:38:13,640
understand what does it mean by cure

824
00:38:11,440 --> 00:38:17,480
cancer or or what it can do to cure

825
00:38:13,640 --> 00:38:20,599
cancer so in in this case there would be

826
00:38:17,480 --> 00:38:23,480
a lot of research in how you can

827
00:38:20,599 --> 00:38:27,920
represent the underlying condition that

828
00:38:23,480 --> 00:38:30,640
you care about so you want your output

829
00:38:27,920 --> 00:38:32,560
protein to have some properties and you

830
00:38:30,640 --> 00:38:35,599
hope that those property would be

831
00:38:32,560 --> 00:38:39,000
related to let's say cure cancer or

832
00:38:35,599 --> 00:38:41,920
curing some U um special diseases so in

833
00:38:39,000 --> 00:38:45,000
this case there uh um the condition

834
00:38:41,920 --> 00:38:47,440
would be more abstract it could also be

835
00:38:45,000 --> 00:38:49,440
a higher dimensional because it is the

836
00:38:47,440 --> 00:38:52,640
abstraction of some behaviors let's say

837
00:38:49,440 --> 00:38:55,680
cure cancer and the

838
00:38:52,640 --> 00:38:57,520
output would be another representation

839
00:38:55,680 --> 00:38:59,359
that is um also higher dimensional you

840
00:38:57,520 --> 00:39:02,440
know let's say the protein structure in

841
00:38:59,359 --> 00:39:05,440
3D it would just be like another kind of

842
00:39:02,440 --> 00:39:05,440
um um 3D

843
00:39:06,079 --> 00:39:14,480
object um then let's talk about some

844
00:39:11,280 --> 00:39:18,160
other scenarios that typically people

845
00:39:14,480 --> 00:39:20,720
won't think of as a generative model

846
00:39:18,160 --> 00:39:22,720
let's say this is a very classical case

847
00:39:20,720 --> 00:39:25,880
that people will regard as

848
00:39:22,720 --> 00:39:28,680
discriminative models we have introduced

849
00:39:25,880 --> 00:39:31,119
so this is um oh oh sorry about that not

850
00:39:28,680 --> 00:39:33,960
not not this one so this is um the the

851
00:39:31,119 --> 00:39:36,359
the typical case of um image generation

852
00:39:33,960 --> 00:39:38,560
as well so you will you will be given a

853
00:39:36,359 --> 00:39:41,200
class label and then your algorithm will

854
00:39:38,560 --> 00:39:43,200
be asked to generate the output image so

855
00:39:41,200 --> 00:39:45,880
this is the so-called class conditional

856
00:39:43,200 --> 00:39:48,480
case which means um your y would be very

857
00:39:45,880 --> 00:39:50,880
specific about one label but then there

858
00:39:48,480 --> 00:39:54,560
is another scenario where you can

859
00:39:50,880 --> 00:39:57,800
imagine you won't you won't be given any

860
00:39:54,560 --> 00:40:00,640
uh uh conditions so that means you want

861
00:39:57,800 --> 00:40:03,240
to generate the data output that will

862
00:40:00,640 --> 00:40:07,119
follow the entire distribution of the

863
00:40:03,240 --> 00:40:10,319
data that in this case um you you can

864
00:40:07,119 --> 00:40:13,000
imagine the underlying condition as an

865
00:40:10,319 --> 00:40:15,720
implicit condition which means you want

866
00:40:13,000 --> 00:40:19,400
the image to follow the distribution of

867
00:40:15,720 --> 00:40:24,119
your underlying data sets

868
00:40:19,400 --> 00:40:26,960
and if um if your if your model can do a

869
00:40:24,119 --> 00:40:29,520
good job in in in this regard so it will

870
00:40:26,960 --> 00:40:31,960
try try to distinguish the distribution

871
00:40:29,520 --> 00:40:35,200
of this data set with the distribution

872
00:40:31,960 --> 00:40:35,200
of um any other data

873
00:40:35,240 --> 00:40:42,319
set okay then this is um the case I have

874
00:40:39,240 --> 00:40:46,079
um just confused with so this is the

875
00:40:42,319 --> 00:40:49,400
idea that we can um apply a generative

876
00:40:46,079 --> 00:40:52,000
modeling to the scenario of um um

877
00:40:49,400 --> 00:40:54,480
discriminative modeling so here is a

878
00:40:52,000 --> 00:40:56,520
very typical case um of supervised

879
00:40:54,480 --> 00:40:59,359
learning or of um um discriminative

880
00:40:56,520 --> 00:41:02,119
learning that is image classification so

881
00:40:59,359 --> 00:41:05,119
you will be given an image and then you

882
00:41:02,119 --> 00:41:06,920
want to estimate the label of that image

883
00:41:05,119 --> 00:41:09,760
and if we want to formulate this as

884
00:41:06,920 --> 00:41:12,800
ative model then in this case actually

885
00:41:09,760 --> 00:41:14,000
Y which was a label in almost all our

886
00:41:12,800 --> 00:41:18,480
previous

887
00:41:14,000 --> 00:41:20,520
examples would be the image in this case

888
00:41:18,480 --> 00:41:26,200
so the image is your condition in this

889
00:41:20,520 --> 00:41:28,079
case and then the class label X would be

890
00:41:26,200 --> 00:41:31,680
um the

891
00:41:28,079 --> 00:41:34,680
um predicted output so you want to model

892
00:41:31,680 --> 00:41:39,280
the probability distribution of your

893
00:41:34,680 --> 00:41:41,599
output so um just because this model um

894
00:41:39,280 --> 00:41:43,920
this problem is is too simple and too

895
00:41:41,599 --> 00:41:47,240
trial and usually people won't think

896
00:41:43,920 --> 00:41:48,079
about it as a generative model um but it

897
00:41:47,240 --> 00:41:51,960
can

898
00:41:48,079 --> 00:41:55,000
be so then what is the point here so if

899
00:41:51,960 --> 00:41:57,240
you can model image classification as a

900
00:41:55,000 --> 00:42:01,839
generative model then actually you can

901
00:41:57,240 --> 00:42:04,280
can extend the scenario from um close

902
00:42:01,839 --> 00:42:07,720
vocabulary classification which means

903
00:42:04,280 --> 00:42:11,680
you will be given a predefined set um of

904
00:42:07,720 --> 00:42:14,160
class labels to the scenario of open

905
00:42:11,680 --> 00:42:17,240
vocabulary recognition that means you

906
00:42:14,160 --> 00:42:20,240
won't be given a predefined set of class

907
00:42:17,240 --> 00:42:23,599
labels so that means there could be many

908
00:42:20,240 --> 00:42:26,119
plausible answers to the same image that

909
00:42:23,599 --> 00:42:29,559
in this case you will still be given one

910
00:42:26,119 --> 00:42:32,440
image but then your output is no longer

911
00:42:29,559 --> 00:42:35,440
one unique correct answer there could be

912
00:42:32,440 --> 00:42:37,720
many different possible answers that can

913
00:42:35,440 --> 00:42:40,760
describe this image for example in this

914
00:42:37,720 --> 00:42:44,920
case these are all reasonable answers to

915
00:42:40,760 --> 00:42:48,640
say this is a bird or a flamingo this is

916
00:42:44,920 --> 00:42:50,720
a red color or perhaps orange color so

917
00:42:48,640 --> 00:42:52,880
as you can see even for this very

918
00:42:50,720 --> 00:42:55,559
classical um image classification

919
00:42:52,880 --> 00:42:58,480
problem if we try to formulate it as a

920
00:42:55,559 --> 00:43:01,240
generative model the it could also opens

921
00:42:58,480 --> 00:43:05,800
up new opportunities and it will enable

922
00:43:01,240 --> 00:43:08,359
new applications um that is um that that

923
00:43:05,800 --> 00:43:10,319
that is nontypical for um classical

924
00:43:08,359 --> 00:43:12,760
discriminative

925
00:43:10,319 --> 00:43:15,559
models and we can even move one step

926
00:43:12,760 --> 00:43:18,440
forward so you can you can imagine the

927
00:43:15,559 --> 00:43:21,280
input condition Y is still an image and

928
00:43:18,440 --> 00:43:25,520
you want the output not just be a label

929
00:43:21,280 --> 00:43:28,599
or or or or shocked um description it

930
00:43:25,520 --> 00:43:31,319
can be an entire sentence or it can even

931
00:43:28,599 --> 00:43:34,200
be some paragraphs that can describe

932
00:43:31,319 --> 00:43:36,280
this image so actually this is also a

933
00:43:34,200 --> 00:43:39,040
classical problem in competivision which

934
00:43:36,280 --> 00:43:41,640
is known as um image captioning so you

935
00:43:39,040 --> 00:43:44,440
want the computer to write a caption

936
00:43:41,640 --> 00:43:44,440
about this

937
00:43:45,200 --> 00:43:51,839
image and then with this context we can

938
00:43:47,599 --> 00:43:55,480
even move one step forward so then this

939
00:43:51,839 --> 00:43:57,160
image could just be part of the input in

940
00:43:55,480 --> 00:43:59,319
your conversation in your natural

941
00:43:57,160 --> 00:44:01,839
language conversation with your chat

942
00:43:59,319 --> 00:44:04,000
booot then in this scenario the

943
00:44:01,839 --> 00:44:06,559
condition would be the input image and

944
00:44:04,000 --> 00:44:09,960
some other text that is some other text

945
00:44:06,559 --> 00:44:12,440
prompt that is given by the user and the

946
00:44:09,960 --> 00:44:14,640
output would be the response of the

947
00:44:12,440 --> 00:44:17,920
chatbot based on this image and the text

948
00:44:14,640 --> 00:44:20,559
prompt let's say in this scenario um

949
00:44:17,920 --> 00:44:23,720
given this image the user could ask what

950
00:44:20,559 --> 00:44:28,240
is UN user about this image and the

951
00:44:23,720 --> 00:44:30,960
tboard can try to come up with some um

952
00:44:28,240 --> 00:44:35,760
answers about this problem and it says

953
00:44:30,960 --> 00:44:37,960
um it is just unusual to um Iron close

954
00:44:35,760 --> 00:44:42,440
um on ironing part attached to to the

955
00:44:37,960 --> 00:44:42,440
roof of a moving taxi so

956
00:44:43,040 --> 00:44:49,040
um um in many other real world problems

957
00:44:46,240 --> 00:44:53,040
such as robotics we can also formulate

958
00:44:49,040 --> 00:44:57,319
the problem of policy learning um as a

959
00:44:53,040 --> 00:45:00,000
generative model so for example um if in

960
00:44:57,319 --> 00:45:01,280
in in robotics control there could be

961
00:45:00,000 --> 00:45:04,800
many

962
00:45:01,280 --> 00:45:07,680
plausible um trajectories many plausible

963
00:45:04,800 --> 00:45:11,079
policies um that can fulfill the same

964
00:45:07,680 --> 00:45:13,680
task so in this case for example um you

965
00:45:11,079 --> 00:45:17,880
want the robot to move this t-shaped

966
00:45:13,680 --> 00:45:20,640
objects into the Target location so um

967
00:45:17,880 --> 00:45:22,480
the robot could either move from the

968
00:45:20,640 --> 00:45:25,200
right hand side or it could move from

969
00:45:22,480 --> 00:45:27,800
the uh the left hand side so both

970
00:45:25,200 --> 00:45:31,680
trajectories are plausible so there is

971
00:45:27,800 --> 00:45:34,280
no single unique answer so then this is

972
00:45:31,680 --> 00:45:36,800
why this Al this also where we can use

973
00:45:34,280 --> 00:45:39,599
objective models um to model this um

974
00:45:36,800 --> 00:45:43,040
policy learning

975
00:45:39,599 --> 00:45:46,599
problem so in general this is what what

976
00:45:43,040 --> 00:45:49,280
we have just seen so a generative model

977
00:45:46,599 --> 00:45:51,640
conceptually just care about this

978
00:45:49,280 --> 00:45:54,559
conditional distribution so in my

979
00:45:51,640 --> 00:45:58,319
opinion actually there is no any

980
00:45:54,559 --> 00:46:01,400
constraints or or or um or requirements

981
00:45:58,319 --> 00:46:03,839
about what can be X or what can be y so

982
00:46:01,400 --> 00:46:04,960
they conceptually they can be anything

983
00:46:03,839 --> 00:46:08,680
so that

984
00:46:04,960 --> 00:46:11,760
means we can use generative models to

985
00:46:08,680 --> 00:46:14,160
solve many kinds of real world problems

986
00:46:11,760 --> 00:46:16,240
we can just try to formulate all these

987
00:46:14,160 --> 00:46:19,079
real world problems as kind of

988
00:46:16,240 --> 00:46:22,800
conditional um distribution problem and

989
00:46:19,079 --> 00:46:26,040
then we can try to apply the latest uh

990
00:46:22,800 --> 00:46:29,760
um advance in generative models um as a

991
00:46:26,040 --> 00:46:33,079
tool for this problem so this is also

992
00:46:29,760 --> 00:46:36,160
partially why gentic models is becoming

993
00:46:33,079 --> 00:46:39,839
more and more common today um for people

994
00:46:36,160 --> 00:46:39,839
to solve real world

995
00:46:41,200 --> 00:46:46,480
problems so um this will be the last

996
00:46:43,559 --> 00:46:50,160
slides of this talk um but I just want

997
00:46:46,480 --> 00:46:52,839
to um give some of the high level idea

998
00:46:50,160 --> 00:46:56,680
um and convey some of the most important

999
00:46:52,839 --> 00:46:59,839
messages in my mind so as we have seen

1000
00:46:56,680 --> 00:47:02,079
uh um generative models have some deep

1001
00:46:59,839 --> 00:47:05,079
neuron networks as they are building

1002
00:47:02,079 --> 00:47:07,160
block so this is just like deep neuronet

1003
00:47:05,079 --> 00:47:10,520
will have some layers as they are

1004
00:47:07,160 --> 00:47:12,880
building blocks so 10 years ago the

1005
00:47:10,520 --> 00:47:16,040
research in deep learning has been

1006
00:47:12,880 --> 00:47:18,920
mainly about these layers so let's say

1007
00:47:16,040 --> 00:47:22,680
convolutions activation functions

1008
00:47:18,920 --> 00:47:25,839
normalizations self attention layers Etc

1009
00:47:22,680 --> 00:47:27,480
so that is the research about one decade

1010
00:47:25,839 --> 00:47:30,079
ago

1011
00:47:27,480 --> 00:47:32,640
and then we have generative models and

1012
00:47:30,079 --> 00:47:34,000
generative models becomes the next level

1013
00:47:32,640 --> 00:47:37,000
of

1014
00:47:34,000 --> 00:47:39,240
abstractions um all previous research on

1015
00:47:37,000 --> 00:47:41,520
deep neuron network still applied but

1016
00:47:39,240 --> 00:47:44,640
there is a new level of research that

1017
00:47:41,520 --> 00:47:47,839
would be built around generative

1018
00:47:44,640 --> 00:47:50,400
models then moving forward when people

1019
00:47:47,839 --> 00:47:52,319
use these generative models to do more

1020
00:47:50,400 --> 00:47:55,599
amazing stuff let's say large language

1021
00:47:52,319 --> 00:47:57,160
models reasoning and agentic machine

1022
00:47:55,599 --> 00:47:58,640
learning which we will cover in in the

1023
00:47:57,160 --> 00:48:01,839
remaining parts of this

1024
00:47:58,640 --> 00:48:05,359
talk in this case these existing

1025
00:48:01,839 --> 00:48:08,880
generative models will become another

1026
00:48:05,359 --> 00:48:11,319
level of building blocks so as we can

1027
00:48:08,880 --> 00:48:14,559
see and as you have seen from um Philips

1028
00:48:11,319 --> 00:48:18,040
uh introduction slides we are building a

1029
00:48:14,559 --> 00:48:21,359
a stack of many different levels of

1030
00:48:18,040 --> 00:48:23,800
models so these are different levels of

1031
00:48:21,359 --> 00:48:27,119
abstractions so the abstractions could

1032
00:48:23,800 --> 00:48:29,520
be layers could be deep neural networks

1033
00:48:27,119 --> 00:48:32,800
they could be generative models and they

1034
00:48:29,520 --> 00:48:36,720
could be um reasoning agents so this is

1035
00:48:32,800 --> 00:48:39,839
just like how computer science has has

1036
00:48:36,720 --> 00:48:42,400
progressed over um um the last century

1037
00:48:39,839 --> 00:48:45,319
or also so people are building different

1038
00:48:42,400 --> 00:48:49,400
levels of abstractions and then we can

1039
00:48:45,319 --> 00:48:51,760
unlock different levels of new opport uh

1040
00:48:49,400 --> 00:48:54,920
opportunities so in this sense I would

1041
00:48:51,760 --> 00:48:57,400
say so um dtive model is the next level

1042
00:48:54,920 --> 00:49:00,160
of De learning and it's also the next

1043
00:48:57,400 --> 00:49:03,640
level of obstruction and building blocks

1044
00:49:00,160 --> 00:49:06,630
and with that that um is the end of my

1045
00:49:03,640 --> 00:49:11,809
talk

1046
00:49:06,630 --> 00:49:11,809
[Applause]

1047
00:49:35,119 --> 00:49:42,119
mapping distribs which is much

1048
00:49:39,240 --> 00:49:45,760
higher that mean they are inferior in

1049
00:49:42,119 --> 00:49:49,000
that simp you mean uh gentic models is

1050
00:49:45,760 --> 00:49:51,400
inere in the simple Tas of um supervised

1051
00:49:49,000 --> 00:49:53,240
learning yes

1052
00:49:51,400 --> 00:49:57,520
[Music]

1053
00:49:53,240 --> 00:50:00,520
um I think there is no um

1054
00:49:57,520 --> 00:50:04,480
certain answer yet because in some sense

1055
00:50:00,520 --> 00:50:06,040
I think it is is it is not yet a common

1056
00:50:04,480 --> 00:50:08,440
understanding that you can address a

1057
00:50:06,040 --> 00:50:12,160
discriminative problem using generative

1058
00:50:08,440 --> 00:50:14,599
models so if it is a very easy let's say

1059
00:50:12,160 --> 00:50:16,760
close vocabulary classification task if

1060
00:50:14,599 --> 00:50:19,280
you very clearly know that you have 10

1061
00:50:16,760 --> 00:50:22,720
10 possible labels or or 1,000 possible

1062
00:50:19,280 --> 00:50:25,520
labels then um usually a simple solution

1063
00:50:22,720 --> 00:50:27,440
is sufficient but even in the case of

1064
00:50:25,520 --> 00:50:29,079
the so-called open vocabulary

1065
00:50:27,440 --> 00:50:31,240
recognition let's say you will be given

1066
00:50:29,079 --> 00:50:35,119
one image you still want one label let's

1067
00:50:31,240 --> 00:50:37,200
say a hashtag then you you can still

1068
00:50:35,119 --> 00:50:39,200
have a vocabulary but that vocabulary is

1069
00:50:37,200 --> 00:50:41,319
just the English vocabulary the human

1070
00:50:39,200 --> 00:50:43,079
vocabulary it could be very long and

1071
00:50:41,319 --> 00:50:45,440
even in that case I think a generative

1072
00:50:43,079 --> 00:50:47,920
model is is a good idea then if you want

1073
00:50:45,440 --> 00:50:51,160
to move move one step further you want

1074
00:50:47,920 --> 00:50:52,680
to have a sentence as a description or

1075
00:50:51,160 --> 00:50:55,040
if you want to start have some

1076
00:50:52,680 --> 00:50:57,880
conversations that is based on this

1077
00:50:55,040 --> 00:50:59,839
image then agative model is perhaps the

1078
00:50:57,880 --> 00:51:01,680
only solution that that you you you you

1079
00:50:59,839 --> 00:51:06,440
should

1080
00:51:01,680 --> 00:51:06,440
use great presentation two questions we

1081
00:51:20,280 --> 00:51:25,799
talking is it we

1082
00:51:39,920 --> 00:51:46,599
MH a good question so the first question

1083
00:51:42,559 --> 00:51:49,599
is is it possible to um go the other way

1084
00:51:46,599 --> 00:51:54,079
so um I think it depends on what is the

1085
00:51:49,599 --> 00:51:56,799
method so I think recently the answer is

1086
00:51:54,079 --> 00:51:59,720
yes so the flow maturing algorithm can

1087
00:51:56,799 --> 00:52:02,240
can enable us to do that so as you can

1088
00:51:59,720 --> 00:52:05,119
imagine in my in my analogy if you

1089
00:52:02,240 --> 00:52:07,920
imagine flow matching as as moving from

1090
00:52:05,119 --> 00:52:09,359
a sphere to a rabbit then conceptually

1091
00:52:07,920 --> 00:52:11,359
it doesn't need to be a sphere it could

1092
00:52:09,359 --> 00:52:14,480
be a cat you can move from a cat to a

1093
00:52:11,359 --> 00:52:17,319
rabbit then in this enery that means you

1094
00:52:14,480 --> 00:52:19,599
can transform from one arbitrary

1095
00:52:17,319 --> 00:52:21,599
distribution to another arbitr

1096
00:52:19,599 --> 00:52:23,839
distribution and then they are they are

1097
00:52:21,599 --> 00:52:26,520
their position is just symmetric so then

1098
00:52:23,839 --> 00:52:28,240
conceptually you can swap them right so

1099
00:52:26,520 --> 00:52:30,720
so that's the first question the second

1100
00:52:28,240 --> 00:52:32,480
question is about um if I recall

1101
00:52:30,720 --> 00:52:36,440
correctly is about um the robotic

1102
00:52:32,480 --> 00:52:38,960
scenario so is there um a clear

1103
00:52:36,440 --> 00:52:38,960
objective

1104
00:52:39,799 --> 00:52:43,079
function yeah

1105
00:52:43,319 --> 00:52:48,559
that's yeah that's a good question I

1106
00:52:45,680 --> 00:52:50,960
think um it is more like a distinction

1107
00:52:48,559 --> 00:52:52,599
between reinforcement learning and

1108
00:52:50,960 --> 00:52:55,040
imitation learning or basically just

1109
00:52:52,599 --> 00:52:56,920
supervised learning so I think

1110
00:52:55,040 --> 00:52:58,359
conceptually we can always formulate the

1111
00:52:56,920 --> 00:53:01,040
problem as reinforcement learning that

1112
00:52:58,359 --> 00:53:03,160
is you just want to uh uh approach the

1113
00:53:01,040 --> 00:53:04,680
goal so let's say if the goal is to move

1114
00:53:03,160 --> 00:53:07,319
the t-shape object to the Target

1115
00:53:04,680 --> 00:53:10,440
location and then if you can do that you

1116
00:53:07,319 --> 00:53:12,799
have the reward if you can't do that you

1117
00:53:10,440 --> 00:53:15,200
you have nothing or your reward is zero

1118
00:53:12,799 --> 00:53:17,960
that that that's possible then imitation

1119
00:53:15,200 --> 00:53:20,079
learning or um supervised learning is

1120
00:53:17,960 --> 00:53:21,799
the other way so you you you try to give

1121
00:53:20,079 --> 00:53:23,680
some examples of what would be the

1122
00:53:21,799 --> 00:53:25,240
possible trajectory and then I try to

1123
00:53:23,680 --> 00:53:27,720
make make the

1124
00:53:25,240 --> 00:53:29,400
behavior um yeah I think I can take

1125
00:53:27,720 --> 00:53:31,040
questions offline um because I'm

1126
00:53:29,400 --> 00:53:32,960
overtime and let's move on to the next

1127
00:53:31,040 --> 00:53:37,679
talk

1128
00:53:32,960 --> 00:53:37,679
[Applause]

