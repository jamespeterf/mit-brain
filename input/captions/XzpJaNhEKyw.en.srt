1
00:00:12,840 --> 00:00:16,119
really excited to be with you all today

2
00:00:14,080 --> 00:00:18,279
because uh we like lined up with some

3
00:00:16,119 --> 00:00:20,400
new exciting results that we can share

4
00:00:18,279 --> 00:00:22,400
today uh so you guys are going to see

5
00:00:20,400 --> 00:00:23,760
some new stuff I put at the end though

6
00:00:22,400 --> 00:00:25,039
so you have to listen through all the

7
00:00:23,760 --> 00:00:27,519
other normal stuff but I think I think

8
00:00:25,039 --> 00:00:28,920
you'll find it exciting um so yeah I'll

9
00:00:27,519 --> 00:00:32,079
just start by introducing future house

10
00:00:28,920 --> 00:00:33,879
so um future house is a a nonprofit

11
00:00:32,079 --> 00:00:37,160
based in San Francisco funded primarily

12
00:00:33,879 --> 00:00:39,520
by Eric Schmidt um and uh we're about 20

13
00:00:37,160 --> 00:00:42,760
people we have a wet lab that's what's

14
00:00:39,520 --> 00:00:45,160
in the background um and uh we're trying

15
00:00:42,760 --> 00:00:46,960
to basically solve big problems in in

16
00:00:45,160 --> 00:00:49,640
automating science and a lot of it is

17
00:00:46,960 --> 00:00:49,640
focused in the domain of

18
00:00:50,559 --> 00:00:56,199
biology so uh before we talk about Ai

19
00:00:54,079 --> 00:00:58,640
and science um I think over the last

20
00:00:56,199 --> 00:01:00,640
maybe 20 years or so the kind of nature

21
00:00:58,640 --> 00:01:02,320
of science has been changing so this is

22
00:01:00,640 --> 00:01:05,560
a plot of the number of papers submitted

23
00:01:02,320 --> 00:01:07,799
to Archive uh as a function of month and

24
00:01:05,560 --> 00:01:09,479
you can see sort of around where chat

25
00:01:07,799 --> 00:01:11,159
gbt is introduced we don't really see

26
00:01:09,479 --> 00:01:13,479
that much of a change in this sort of

27
00:01:11,159 --> 00:01:15,000
trend of increasing number of papers um

28
00:01:13,479 --> 00:01:16,439
there's a little bit of a dip during the

29
00:01:15,000 --> 00:01:18,159
pandemic and then you know sort of

30
00:01:16,439 --> 00:01:19,600
picked up right around when chat gbt was

31
00:01:18,159 --> 00:01:20,920
released but basically we've been seeing

32
00:01:19,600 --> 00:01:22,680
more and more papers now and actually I

33
00:01:20,920 --> 00:01:24,200
think just a couple days ago archive

34
00:01:22,680 --> 00:01:26,479
announced they hit 25,000 monthly

35
00:01:24,200 --> 00:01:28,079
submissions um this access only goes up

36
00:01:26,479 --> 00:01:31,520
to 23,000 so you can see it's out of

37
00:01:28,079 --> 00:01:32,920
date um uh six months later lat and also

38
00:01:31,520 --> 00:01:34,520
the kind of papers there's more papers

39
00:01:32,920 --> 00:01:36,439
being published every year also the kind

40
00:01:34,520 --> 00:01:39,320
of papers being published is changing so

41
00:01:36,439 --> 00:01:41,000
in 1980 the number of paper authors was

42
00:01:39,320 --> 00:01:43,759
about one and now the number of

43
00:01:41,000 --> 00:01:45,159
co-authors is reaching uh about five I

44
00:01:43,759 --> 00:01:46,479
didn't have data all the way up to today

45
00:01:45,159 --> 00:01:48,360
but I think it's it's pretty close to

46
00:01:46,479 --> 00:01:49,880
five now and so it's not only more

47
00:01:48,360 --> 00:01:51,280
papers but also the complexity to the

48
00:01:49,880 --> 00:01:54,200
number of sort of Specialists that's

49
00:01:51,280 --> 00:01:58,079
required to get a paper published is is

50
00:01:54,200 --> 00:01:59,439
increasing um and so basically from a

51
00:01:58,079 --> 00:02:01,719
from a perspective of someone trying to

52
00:01:59,439 --> 00:02:03,920
just do science you know forget AI

53
00:02:01,719 --> 00:02:05,320
you're having an increase in the sort of

54
00:02:03,920 --> 00:02:07,039
intellectual bottlenecks of doing

55
00:02:05,320 --> 00:02:08,679
science basically you need to consider

56
00:02:07,039 --> 00:02:10,119
more and more papers every year the you

57
00:02:08,679 --> 00:02:11,800
know the reference sections and papers

58
00:02:10,119 --> 00:02:13,080
is growing the number of papers in your

59
00:02:11,800 --> 00:02:14,200
field is growing the number of journals

60
00:02:13,080 --> 00:02:16,000
in your field is probably also growing

61
00:02:14,200 --> 00:02:18,959
as well and so we see you know about

62
00:02:16,000 --> 00:02:20,360
five million papers per year um and I

63
00:02:18,959 --> 00:02:22,319
think it's now going to be seven million

64
00:02:20,360 --> 00:02:24,000
for this most most recent year and

65
00:02:22,319 --> 00:02:25,319
there's about 250 million papers out

66
00:02:24,000 --> 00:02:27,720
there today so you're seeing like you

67
00:02:25,319 --> 00:02:30,000
know pretty big percentage growth in the

68
00:02:27,720 --> 00:02:32,680
number of research papers

69
00:02:30,000 --> 00:02:35,000
and also the cost of data is is coming

70
00:02:32,680 --> 00:02:37,800
down as well so I have here a number of

71
00:02:35,000 --> 00:02:40,519
$200 for a whole genome sequence of a

72
00:02:37,800 --> 00:02:42,959
human um is about a dollar per gig of

73
00:02:40,519 --> 00:02:44,519
sequencing I hear from Pharma companies

74
00:02:42,959 --> 00:02:47,280
that it's actually closer to like 80

75
00:02:44,519 --> 00:02:49,840
to100 per per whole genome sequen so

76
00:02:47,280 --> 00:02:51,599
we're seeing you know a rapid drop in um

77
00:02:49,840 --> 00:02:56,040
in the cost somebody's shaking their

78
00:02:51,599 --> 00:02:58,800
head in the back disagree yeah

79
00:02:56,040 --> 00:03:00,280
$500 all right we can we can we can

80
00:02:58,800 --> 00:03:02,800
quibble on this I talked to to the to

81
00:03:00,280 --> 00:03:05,200
the VP of AI of uh of Ro I think it was

82
00:03:02,800 --> 00:03:07,879
and he was saying they were down to $100

83
00:03:05,200 --> 00:03:11,239
but but interesting all right so maybe

84
00:03:07,879 --> 00:03:13,440
$500 it's fine um and then also there's

85
00:03:11,239 --> 00:03:15,239
there's less disruptive paper so along

86
00:03:13,440 --> 00:03:16,640
with basically the increase in in like

87
00:03:15,239 --> 00:03:18,360
literature we have to go through there's

88
00:03:16,640 --> 00:03:20,200
multiple measures of basic that decline

89
00:03:18,360 --> 00:03:21,120
in the productivity of science I think

90
00:03:20,200 --> 00:03:22,720
this one is maybe a little more

91
00:03:21,120 --> 00:03:26,560
complicated complicated there's this

92
00:03:22,720 --> 00:03:27,560
paper put out by um uh uh Park and Funk

93
00:03:26,560 --> 00:03:30,159
and

94
00:03:27,560 --> 00:03:31,360
basically they tried to contr for the

95
00:03:30,159 --> 00:03:32,680
number of papers that are that are

96
00:03:31,360 --> 00:03:34,560
coming out and they basically looked at

97
00:03:32,680 --> 00:03:36,680
you know a few top journals and saw how

98
00:03:34,560 --> 00:03:39,519
often like a sort of canonical citation

99
00:03:36,680 --> 00:03:41,799
was displaced by another one so like

100
00:03:39,519 --> 00:03:42,879
maybe there's something about you know

101
00:03:41,799 --> 00:03:44,000
how the ribosome works and there's

102
00:03:42,879 --> 00:03:45,560
another paper that comes out that has a

103
00:03:44,000 --> 00:03:47,400
better description of it how often does

104
00:03:45,560 --> 00:03:48,920
that displace the previous one and we

105
00:03:47,400 --> 00:03:50,760
see that in biology specifically the

106
00:03:48,920 --> 00:03:52,879
number of these these disruptive

107
00:03:50,760 --> 00:03:54,519
disruptive papers is going down I think

108
00:03:52,879 --> 00:03:56,439
you know it's hard to measure things

109
00:03:54,519 --> 00:03:58,400
like disruptive papers um but you can

110
00:03:56,439 --> 00:03:59,959
also see this in in sort of productivity

111
00:03:58,400 --> 00:04:02,159
in the pharmaceutical industry right so

112
00:03:59,959 --> 00:04:03,920
this this thing called eooms law which

113
00:04:02,159 --> 00:04:05,599
is kind of out of date is not as true

114
00:04:03,920 --> 00:04:07,439
but still we're seeing you know over a

115
00:04:05,599 --> 00:04:10,040
billion dollars per per FDA approved

116
00:04:07,439 --> 00:04:11,920
drug right now in research expenditures

117
00:04:10,040 --> 00:04:14,560
um I've heard people argue that's you

118
00:04:11,920 --> 00:04:17,320
know an R&D tax loophole and and also

119
00:04:14,560 --> 00:04:18,280
it's no longer holds but uh anyway

120
00:04:17,320 --> 00:04:19,639
there's a lot of signals I think that

121
00:04:18,280 --> 00:04:21,759
show that it's getting harder and harder

122
00:04:19,639 --> 00:04:24,280
to to make you know uh advances in the

123
00:04:21,759 --> 00:04:25,479
frontier of Science and so basically the

124
00:04:24,280 --> 00:04:27,560
conclusion of all of this is that modern

125
00:04:25,479 --> 00:04:28,639
science is hard okay and so when we

126
00:04:27,560 --> 00:04:30,479
think about what are the next kind of

127
00:04:28,639 --> 00:04:32,800
big breakthroughs uh that that are going

128
00:04:30,479 --> 00:04:34,440
to come out um they're really hard to

129
00:04:32,800 --> 00:04:35,919
even conceptualize as fitting in one

130
00:04:34,440 --> 00:04:38,479
paper or one equation or one sort of

131
00:04:35,919 --> 00:04:40,080
breakthrough right and so I think the

132
00:04:38,479 --> 00:04:41,840
the kind of scale of science is reaching

133
00:04:40,080 --> 00:04:43,639
a point where it may not be just one you

134
00:04:41,840 --> 00:04:45,320
know group or may not be one or two

135
00:04:43,639 --> 00:04:47,039
groups and even if it's a Consortium it

136
00:04:45,320 --> 00:04:49,320
may be difficult for a whole Consortium

137
00:04:47,039 --> 00:04:50,880
to work together coherently on a problem

138
00:04:49,320 --> 00:04:53,800
and so that's sort of where future house

139
00:04:50,880 --> 00:04:56,600
comes in is that you're set up as trying

140
00:04:53,800 --> 00:04:58,440
to take some of the advances in in Ai

141
00:04:56,600 --> 00:04:59,759
and apply them to automate the processes

142
00:04:58,440 --> 00:05:02,440
of these intellectual bottlenecks

143
00:04:59,759 --> 00:05:05,240
growing in scientific

144
00:05:02,440 --> 00:05:07,160
discovery so um that's sort of the

145
00:05:05,240 --> 00:05:08,520
mission right so so a lot of the the

146
00:05:07,160 --> 00:05:10,080
work that I'll talk to you today about

147
00:05:08,520 --> 00:05:12,800
is different ways that we can try to

148
00:05:10,080 --> 00:05:15,440
accelerate Discovery so before we get

149
00:05:12,800 --> 00:05:17,199
too far um you know I think uh everyone

150
00:05:15,440 --> 00:05:18,840
talks about large language models and

151
00:05:17,199 --> 00:05:20,560
they're doing programming and they can

152
00:05:18,840 --> 00:05:22,680
schedule your restaurant reservations

153
00:05:20,560 --> 00:05:24,520
and book a plane ticket for you so can

154
00:05:22,680 --> 00:05:27,240
they just do biology with with no effort

155
00:05:24,520 --> 00:05:28,560
at all um so I mean obviously it sounds

156
00:05:27,240 --> 00:05:30,680
ridiculous but we did want to see to

157
00:05:28,560 --> 00:05:32,479
measure it like you know is there some

158
00:05:30,680 --> 00:05:34,280
like Benchmark we can look at to see the

159
00:05:32,479 --> 00:05:35,479
progress in biology and so we looked at

160
00:05:34,280 --> 00:05:38,479
some of the existing benchmarks so this

161
00:05:35,479 --> 00:05:40,039
is MM Pro this is kind of one of these

162
00:05:38,479 --> 00:05:42,199
commonly used benchmarks to see how good

163
00:05:40,039 --> 00:05:43,919
language models are here is one of the

164
00:05:42,199 --> 00:05:45,680
questions from the biology category so

165
00:05:43,919 --> 00:05:47,680
as of 2017 how many one-year-olds have

166
00:05:45,680 --> 00:05:49,000
been vaccinated against any disease so

167
00:05:47,680 --> 00:05:50,880
when people say you know these models

168
00:05:49,000 --> 00:05:52,000
are you know getting 80% or superhuman

169
00:05:50,880 --> 00:05:53,000
performance on biology these are the

170
00:05:52,000 --> 00:05:55,199
kinds of questions you're using to

171
00:05:53,000 --> 00:05:57,240
measure it and um I don't know about you

172
00:05:55,199 --> 00:05:58,840
all but I don't come across this kind of

173
00:05:57,240 --> 00:06:01,360
you know difficult intellectual task on

174
00:05:58,840 --> 00:06:03,000
a daily basis

175
00:06:01,360 --> 00:06:05,080
um's another one from the data set I

176
00:06:03,000 --> 00:06:06,800
just thought was interesting uh title of

177
00:06:05,080 --> 00:06:08,240
this album this one I found very

178
00:06:06,800 --> 00:06:10,840
interesting it's find lthm of 3 to the

179
00:06:08,240 --> 00:06:12,880
two okay see I feel like if you pulled a

180
00:06:10,840 --> 00:06:14,520
group of scientists they would disagree

181
00:06:12,880 --> 00:06:17,199
on what the logarithm of 3 to the two is

182
00:06:14,520 --> 00:06:20,599
right the base two could be base e could

183
00:06:17,199 --> 00:06:22,080
be base 10 I actually unfortunately I

184
00:06:20,599 --> 00:06:23,759
didn't write down the answer in my slide

185
00:06:22,080 --> 00:06:25,400
notes so I don't know what is the

186
00:06:23,759 --> 00:06:28,319
official official answer that large

187
00:06:25,400 --> 00:06:30,520
language models choose for this

188
00:06:28,319 --> 00:06:32,919
question and so you know people have

189
00:06:30,520 --> 00:06:35,680
recognized that and so the sort of like

190
00:06:32,919 --> 00:06:36,919
best effort model or sorry Benchmark

191
00:06:35,680 --> 00:06:37,720
right now is something called Humanity's

192
00:06:36,919 --> 00:06:41,240
last

193
00:06:37,720 --> 00:06:42,680
exam catchy name can't forget it um but

194
00:06:41,240 --> 00:06:45,080
really even look at the questions the

195
00:06:42,680 --> 00:06:46,479
questions are still textbook questions

196
00:06:45,080 --> 00:06:48,319
like these are questions which have a

197
00:06:46,479 --> 00:06:49,880
clear correct answer and it's like

198
00:06:48,319 --> 00:06:51,520
you're trying to just you know solve

199
00:06:49,880 --> 00:06:53,759
some kind of known problem it doesn't

200
00:06:51,520 --> 00:06:56,599
really get at the frontier of

201
00:06:53,759 --> 00:06:58,400
science so at futur s we put together a

202
00:06:56,599 --> 00:07:00,080
a benchmark we put down put together two

203
00:06:58,400 --> 00:07:01,560
now I'm going to talk about one we put

204
00:07:00,080 --> 00:07:04,599
out in June but we put out a new one a

205
00:07:01,560 --> 00:07:05,919
couple like a week ago but what we tried

206
00:07:04,599 --> 00:07:08,720
to do is capture some of the things that

207
00:07:05,919 --> 00:07:09,879
we come across in our own work so I have

208
00:07:08,720 --> 00:07:11,400
one on the right hand side which is

209
00:07:09,879 --> 00:07:13,560
cloning Center this is made from like

210
00:07:11,400 --> 00:07:15,960
PhD students lab notebooks questions

211
00:07:13,560 --> 00:07:17,479
that they actually faced um and then we

212
00:07:15,960 --> 00:07:19,599
convert them into like questions that we

213
00:07:17,479 --> 00:07:21,319
could measure uh uh with multiple choice

214
00:07:19,599 --> 00:07:22,520
answers multiple choice answers is a

215
00:07:21,319 --> 00:07:23,599
really hard thing to do I'm going to

216
00:07:22,520 --> 00:07:25,039
talk at the end of my talk about how to

217
00:07:23,599 --> 00:07:26,879
get away from multiple choice answers

218
00:07:25,039 --> 00:07:28,520
but just it's so much easier to evaluate

219
00:07:26,879 --> 00:07:30,080
the models with multiple choice answers

220
00:07:28,520 --> 00:07:32,039
so a lot of the work in these things is

221
00:07:30,080 --> 00:07:34,280
making sure they're good distractors as

222
00:07:32,039 --> 00:07:36,560
they're called um but then we also have

223
00:07:34,280 --> 00:07:38,680
things like lit QA L QA was built to be

224
00:07:36,560 --> 00:07:40,759
a bunch of um questions that you could

225
00:07:38,680 --> 00:07:42,160
only answer by finding some specific

226
00:07:40,759 --> 00:07:44,360
paper in the literature and reading the

227
00:07:42,160 --> 00:07:46,520
full text paper of that the full text of

228
00:07:44,360 --> 00:07:48,039
that paper so the ideaas to get away

229
00:07:46,520 --> 00:07:49,759
from like the memorization we don't want

230
00:07:48,039 --> 00:07:51,400
to measure if the models have memorized

231
00:07:49,759 --> 00:07:53,240
all of scientific literature we want to

232
00:07:51,400 --> 00:07:56,240
see if they can answer a new kind of

233
00:07:53,240 --> 00:07:58,000
question and then cqa is just like uh

234
00:07:56,240 --> 00:07:59,680
just working with biological sequence

235
00:07:58,000 --> 00:08:01,199
data we also did things that are

236
00:07:59,680 --> 00:08:02,759
multimodal I won't really talk about

237
00:08:01,199 --> 00:08:04,400
those but um basically can you read a

238
00:08:02,759 --> 00:08:05,919
table and not just like convert the

239
00:08:04,400 --> 00:08:08,520
table into text but can you actually

240
00:08:05,919 --> 00:08:11,159
answer a meaningful question about the

241
00:08:08,520 --> 00:08:14,080
table so we put out this Benchmark and

242
00:08:11,159 --> 00:08:15,919
we measured we hired a bunch of PhD

243
00:08:14,080 --> 00:08:17,840
biologists if any of you want to make

244
00:08:15,919 --> 00:08:19,840
some money on the side you can also be a

245
00:08:17,840 --> 00:08:22,759
future house contractor to take tests

246
00:08:19,840 --> 00:08:24,560
and pit yourself against AI but we hire

247
00:08:22,759 --> 00:08:26,080
these humans as in in the green bar and

248
00:08:24,560 --> 00:08:27,759
you can see that the humans are just

249
00:08:26,080 --> 00:08:29,680
destroying the models in every single

250
00:08:27,759 --> 00:08:31,440
category except for reading tables which

251
00:08:29,680 --> 00:08:34,200
I find to be very interesting but I

252
00:08:31,440 --> 00:08:35,839
think also like um uh it kind of makes

253
00:08:34,200 --> 00:08:38,080
sense because we really couldn't have

254
00:08:35,839 --> 00:08:40,640
that hard of questions on reading tables

255
00:08:38,080 --> 00:08:42,080
uh but anyway so I think and you know

256
00:08:40,640 --> 00:08:43,479
this is random right so this is the

257
00:08:42,080 --> 00:08:45,279
random line here so most of these models

258
00:08:43,479 --> 00:08:47,320
are right around random and I think this

259
00:08:45,279 --> 00:08:48,839
captures much better the experience of

260
00:08:47,320 --> 00:08:50,600
someone who does science with what

261
00:08:48,839 --> 00:08:52,080
happens when you go to chat gb.com right

262
00:08:50,600 --> 00:08:53,519
if you go to chat gbt and you put in

263
00:08:52,080 --> 00:08:55,279
your questions a lot of times it's just

264
00:08:53,519 --> 00:08:57,000
like not missing or it's not getting the

265
00:08:55,279 --> 00:09:00,200
point and I think this Benchmark kind of

266
00:08:57,000 --> 00:09:01,200
captures that uh experience uh better

267
00:09:00,200 --> 00:09:02,920
whereas you know you you see other

268
00:09:01,200 --> 00:09:05,920
benchmarks they're all you know the the

269
00:09:02,920 --> 00:09:07,480
AI is super human already um I do want

270
00:09:05,920 --> 00:09:10,160
to mention like one interesting thing

271
00:09:07,480 --> 00:09:11,160
that we found in this is that uh um you

272
00:09:10,160 --> 00:09:13,959
know these reasoning models have been

273
00:09:11,160 --> 00:09:15,040
coming out now and even though reasoning

274
00:09:13,959 --> 00:09:16,720
models actually help with some

275
00:09:15,040 --> 00:09:18,640
benchmarks like protocol QA which is

276
00:09:16,720 --> 00:09:20,200
basically lab protocols we make a very

277
00:09:18,640 --> 00:09:21,959
clear mistake in the protocol such that

278
00:09:20,200 --> 00:09:23,480
it would be impossible to do reasoning

279
00:09:21,959 --> 00:09:25,480
models get better at that spotting those

280
00:09:23,480 --> 00:09:26,600
mistakes but other ones these like lab

281
00:09:25,480 --> 00:09:28,120
these cloning scen which is coming from

282
00:09:26,600 --> 00:09:29,079
Lab notebooks reasing models don't do

283
00:09:28,120 --> 00:09:30,760
any better right so there's really

284
00:09:29,079 --> 00:09:32,640
something missing from these models to

285
00:09:30,760 --> 00:09:36,160
to accomplish these

286
00:09:32,640 --> 00:09:38,120
tasks okay so um basically we have to do

287
00:09:36,160 --> 00:09:40,680
some work and so at future house we've

288
00:09:38,120 --> 00:09:43,120
been trying to build AI systems and so

289
00:09:40,680 --> 00:09:44,880
AI systems are things like agents that

290
00:09:43,120 --> 00:09:47,240
as language models connected with tools

291
00:09:44,880 --> 00:09:50,279
that you as a scientist might use um

292
00:09:47,240 --> 00:09:54,040
like Google Scholar or like you know uh

293
00:09:50,279 --> 00:09:55,240
benchling or um leod Dynamics and so you

294
00:09:54,040 --> 00:09:57,440
know basically what we're doing future

295
00:09:55,240 --> 00:09:59,000
house one two steps one two three we

296
00:09:57,440 --> 00:10:00,959
built an organization that is built

297
00:09:59,000 --> 00:10:03,000
around trying to to build scientific

298
00:10:00,959 --> 00:10:05,160
agents that can use these tools uh we

299
00:10:03,000 --> 00:10:06,640
measured human uh performance on these

300
00:10:05,160 --> 00:10:08,079
things and we are able to exceed Human

301
00:10:06,640 --> 00:10:10,640
Performance and now we're trying to get

302
00:10:08,079 --> 00:10:13,959
to like making scientific discoveries at

303
00:10:10,640 --> 00:10:19,079
scale all right so what is an

304
00:10:13,959 --> 00:10:21,720
agent this is a real question here um so

305
00:10:19,079 --> 00:10:23,079
uh I think we try to use the the like

306
00:10:21,720 --> 00:10:24,920
nomenclature from reinforcement learning

307
00:10:23,079 --> 00:10:26,399
if any of you know a little bit about

308
00:10:24,920 --> 00:10:28,360
reinforcement learning basically you

309
00:10:26,399 --> 00:10:31,160
can't really describe an agent without

310
00:10:28,360 --> 00:10:34,600
describing the Environ en that it's in

311
00:10:31,160 --> 00:10:37,120
so we have um an environment here and we

312
00:10:34,600 --> 00:10:39,160
have the agent that is is using tools

313
00:10:37,120 --> 00:10:40,399
inside of an environment and it

314
00:10:39,160 --> 00:10:42,480
communicates back and forth to this

315
00:10:40,399 --> 00:10:44,800
environment using natural language and

316
00:10:42,480 --> 00:10:47,839
so instead of like you know I have an

317
00:10:44,800 --> 00:10:49,440
arithm arithmetic question here and I

318
00:10:47,839 --> 00:10:51,480
answer it using a calculator rather than

319
00:10:49,440 --> 00:10:52,800
just trying to do the arithmetic itself

320
00:10:51,480 --> 00:10:54,959
and this agent is a language model that

321
00:10:52,800 --> 00:10:56,959
uses these tools to answer the question

322
00:10:54,959 --> 00:10:58,399
and sort of an agent is basically

323
00:10:56,959 --> 00:10:59,760
coupled to this environment which is a

324
00:10:58,399 --> 00:11:02,480
set of tools that are specific for

325
00:10:59,760 --> 00:11:05,399
solving a task um this is super tiny but

326
00:11:02,480 --> 00:11:07,800
basically we we tried to put out a repo

327
00:11:05,399 --> 00:11:09,560
um I we succeeded in making a a code

328
00:11:07,800 --> 00:11:12,320
repository to Define your own agents and

329
00:11:09,560 --> 00:11:13,560
use them uh in code uh for your own

330
00:11:12,320 --> 00:11:14,880
tasks like if you have a bunch of tools

331
00:11:13,560 --> 00:11:18,560
you want to use you can combine them

332
00:11:14,880 --> 00:11:18,560
with uh with the language model

333
00:11:20,600 --> 00:11:28,200
question yes are all the questions text

334
00:11:23,920 --> 00:11:31,240
based questions um so uh in our like

335
00:11:28,200 --> 00:11:33,720
repository no you can use uh multimodal

336
00:11:31,240 --> 00:11:36,240
inputs um most stuff I'm going to talk

337
00:11:33,720 --> 00:11:39,320
about today is mostly text but some of

338
00:11:36,240 --> 00:11:41,279
it and has like uh molecular structures

339
00:11:39,320 --> 00:11:43,200
which are graphs but usually we

340
00:11:41,279 --> 00:11:44,880
represent the graphs as a string so it

341
00:11:43,200 --> 00:11:48,279
ends up being text in the

342
00:11:44,880 --> 00:11:49,760
end um so a lot of the work that we've

343
00:11:48,279 --> 00:11:51,440
done at future house is basically how do

344
00:11:49,760 --> 00:11:53,880
we go beyond just taking a language

345
00:11:51,440 --> 00:11:56,160
model putting it into an environment and

346
00:11:53,880 --> 00:11:57,720
praying that it works so a lot of the

347
00:11:56,160 --> 00:11:59,680
work is basically how do you train these

348
00:11:57,720 --> 00:12:00,880
things to do better and you know in the

349
00:11:59,680 --> 00:12:02,279
interest of time I'm not going to get

350
00:12:00,880 --> 00:12:04,839
into a lot of detail about how we

351
00:12:02,279 --> 00:12:06,600
actually train these things but it's um

352
00:12:04,839 --> 00:12:08,000
it involves you know just sort of normal

353
00:12:06,600 --> 00:12:09,200
uh deep learning methods like you build

354
00:12:08,000 --> 00:12:10,959
a computational graph you take a

355
00:12:09,200 --> 00:12:13,240
derivative you propagate through the the

356
00:12:10,959 --> 00:12:14,720
the graph so before we you know get into

357
00:12:13,240 --> 00:12:16,680
the training thing like if I just take

358
00:12:14,720 --> 00:12:18,040
an an agent one of these agents that on

359
00:12:16,680 --> 00:12:20,320
the previous slides you saw were doing

360
00:12:18,040 --> 00:12:22,240
very poorly and I give them some tools

361
00:12:20,320 --> 00:12:23,560
we see there's some gain so basically I

362
00:12:22,240 --> 00:12:25,279
have here like this is how well the

363
00:12:23,560 --> 00:12:26,360
models did without tools and then I give

364
00:12:25,279 --> 00:12:27,959
them tools and they get a little bit

365
00:12:26,360 --> 00:12:30,399
better okay I give them tools they get a

366
00:12:27,959 --> 00:12:32,320
little bit better not huge game right so

367
00:12:30,399 --> 00:12:34,519
so it's nice that if you just tell the

368
00:12:32,320 --> 00:12:36,279
llm no you don't know how to do

369
00:12:34,519 --> 00:12:37,839
arithmetic use the calculator tool or no

370
00:12:36,279 --> 00:12:40,160
you don't know how to read you know you

371
00:12:37,839 --> 00:12:42,040
don't know how to translate from a

372
00:12:40,160 --> 00:12:44,760
nucleotides to amino acids you have to

373
00:12:42,040 --> 00:12:47,440
use the nucleotide translation

374
00:12:44,760 --> 00:12:50,760
tool but then what we do is we actually

375
00:12:47,440 --> 00:12:53,160
take these these models that can do okay

376
00:12:50,760 --> 00:12:55,519
and then we basically let them try it

377
00:12:53,160 --> 00:12:56,959
out and this is showing you like uh this

378
00:12:55,519 --> 00:12:58,920
little diagram showing you all the

379
00:12:56,959 --> 00:13:01,199
different tool calls that they make so

380
00:12:58,920 --> 00:13:02,040
each color is a different tool and then

381
00:13:01,199 --> 00:13:04,240
in this case they're trying to do

382
00:13:02,040 --> 00:13:06,000
molecular cloning anybody who done

383
00:13:04,240 --> 00:13:08,839
molecular cloning before but

384
00:13:06,000 --> 00:13:10,519
uh I to learn that quite a bit for this

385
00:13:08,839 --> 00:13:11,800
project but essentially um we have a

386
00:13:10,519 --> 00:13:13,360
bunch of questions about M cloning and

387
00:13:11,800 --> 00:13:14,519
then we let these agents go give it a

388
00:13:13,360 --> 00:13:15,839
try and then the ones that are

389
00:13:14,519 --> 00:13:17,440
successful we Mark with green and the

390
00:13:15,839 --> 00:13:19,800
ones that are unsuccessful we Mark in

391
00:13:17,440 --> 00:13:21,040
red and then we throw away the red ones

392
00:13:19,800 --> 00:13:22,800
and then we just train on these

393
00:13:21,040 --> 00:13:24,839
successful

394
00:13:22,800 --> 00:13:27,760
trajectories and at the end you

395
00:13:24,839 --> 00:13:29,920
basically can start doing these cycles

396
00:13:27,760 --> 00:13:32,639
of let the agents go try it out see if

397
00:13:29,920 --> 00:13:34,600
they make the right uh plasma um or get

398
00:13:32,639 --> 00:13:37,199
the right insertion and then if they got

399
00:13:34,600 --> 00:13:38,760
it right we like take those successful

400
00:13:37,199 --> 00:13:41,639
trajectories and then we train the model

401
00:13:38,760 --> 00:13:43,240
to do like this is good job and over

402
00:13:41,639 --> 00:13:44,680
time this is just like GPU hours and

403
00:13:43,240 --> 00:13:46,079
this is like what a Frontier Model is

404
00:13:44,680 --> 00:13:48,079
like you know if you take these Frontier

405
00:13:46,079 --> 00:13:49,800
models we get better and better and

406
00:13:48,079 --> 00:13:51,399
these are our own models these are 14

407
00:13:49,800 --> 00:13:52,600
billion parameter models sounds like a

408
00:13:51,399 --> 00:13:55,160
lot but actually you can run that on

409
00:13:52,600 --> 00:13:56,600
your laptop no problem and so we can get

410
00:13:55,160 --> 00:13:58,160
past these Frontier models which as you

411
00:13:56,600 --> 00:13:59,519
know are like hundreds of billions of

412
00:13:58,160 --> 00:14:00,880
parameters and you can't run them on

413
00:13:59,519 --> 00:14:03,959
your own laptop they have to be run on

414
00:14:00,880 --> 00:14:06,160
you know special data center GPU uh

415
00:14:03,959 --> 00:14:08,279
servers and we did this on these two

416
00:14:06,160 --> 00:14:10,440
tasks one of these cqa this is the one

417
00:14:08,279 --> 00:14:12,519
about molecular cloning lqa is about

418
00:14:10,440 --> 00:14:15,399
using tools like Google Scholar citation

419
00:14:12,519 --> 00:14:16,399
graph uh basically to to answer

420
00:14:15,399 --> 00:14:18,399
questions about

421
00:14:16,399 --> 00:14:21,120
literature what's interesting is that

422
00:14:18,399 --> 00:14:22,040
when you start this is like what the the

423
00:14:21,120 --> 00:14:23,440
start looks like these are the

424
00:14:22,040 --> 00:14:25,680
trajectories that we just get from

425
00:14:23,440 --> 00:14:28,279
randomly trying it out and then over

426
00:14:25,680 --> 00:14:29,959
time it looks like this and so basically

427
00:14:28,279 --> 00:14:32,680
we start here and it's exploring a whole

428
00:14:29,959 --> 00:14:34,519
bunch sometimes it's calling like the um

429
00:14:32,680 --> 00:14:35,880
the annotate plasmid tool like five

430
00:14:34,519 --> 00:14:37,800
times you know and doesn't really help

431
00:14:35,880 --> 00:14:39,160
to annotate it a bunch of times and then

432
00:14:37,800 --> 00:14:41,480
here it gets much more focused and then

433
00:14:39,160 --> 00:14:43,320
it does a better job at tasks It's

434
00:14:41,480 --> 00:14:46,279
actually an 8 billion Prim Mon it's 14

435
00:14:43,320 --> 00:14:48,959
it's 8 billion Prim model

436
00:14:46,279 --> 00:14:50,519
yeah great question um we basically have

437
00:14:48,959 --> 00:14:52,680
some so so the question was how do you

438
00:14:50,519 --> 00:14:56,079
define successful trajectories um the

439
00:14:52,680 --> 00:14:58,480
tasks are things like um I don't know uh

440
00:14:56,079 --> 00:15:01,480
uh how many what's the longest open

441
00:14:58,480 --> 00:15:03,320
reading frame in this plas okay and then

442
00:15:01,480 --> 00:15:04,600
so we know like what is the longest open

443
00:15:03,320 --> 00:15:06,279
reading frame in the plasmid from like

444
00:15:04,600 --> 00:15:08,399
doing it by hand we can see if the model

445
00:15:06,279 --> 00:15:10,360
can figure that out from uh from looking

446
00:15:08,399 --> 00:15:12,519
at it that's example question like I

447
00:15:10,360 --> 00:15:15,560
think the they're more like how do you

448
00:15:12,519 --> 00:15:18,680
clone M Scarlet into this uh into this

449
00:15:15,560 --> 00:15:21,320
plasmid so it'll go into eoli and

450
00:15:18,680 --> 00:15:23,040
express um and then it has to go like to

451
00:15:21,320 --> 00:15:24,399
add Gene and find the right sequences

452
00:15:23,040 --> 00:15:28,920
and download them and then come up with

453
00:15:24,399 --> 00:15:28,920
the protocol then result an expression

454
00:15:29,440 --> 00:15:34,839
so uh in the end we get these agents out

455
00:15:33,160 --> 00:15:36,639
they can run to do these M cloning

456
00:15:34,839 --> 00:15:38,000
things and they're costing you know a

457
00:15:36,639 --> 00:15:39,880
very small amount of money so these are

458
00:15:38,000 --> 00:15:41,240
basically our models here and then these

459
00:15:39,880 --> 00:15:42,880
are the frontier models so we're like

460
00:15:41,240 --> 00:15:44,600
two or three orders of magnitude lower

461
00:15:42,880 --> 00:15:45,920
in cost one of the cool things you can

462
00:15:44,600 --> 00:15:47,240
do with that is that if it's this cheap

463
00:15:45,920 --> 00:15:49,440
you can actually just run it a thousand

464
00:15:47,240 --> 00:15:50,959
times and then take the consensus you

465
00:15:49,440 --> 00:15:52,560
can just have a majority vote so you

466
00:15:50,959 --> 00:15:54,519
have thousand agents and they all try

467
00:15:52,560 --> 00:15:56,279
like what's the next step and they vote

468
00:15:54,519 --> 00:15:59,639
like I think we should you know I think

469
00:15:56,279 --> 00:16:00,720
we should do uh um Gibson assembly and

470
00:15:59,639 --> 00:16:01,800
then some like no I don't think we

471
00:16:00,720 --> 00:16:03,279
should use Gibson assembly I think we

472
00:16:01,800 --> 00:16:05,079
should do know restriction enzyme

473
00:16:03,279 --> 00:16:06,519
cloning and they take the the vote and

474
00:16:05,079 --> 00:16:08,040
then they go with whatever is voted for

475
00:16:06,519 --> 00:16:09,480
and that actually boosts accuracy by

476
00:16:08,040 --> 00:16:11,800
quite a

477
00:16:09,480 --> 00:16:13,199
bit so when we're done with this we're

478
00:16:11,800 --> 00:16:15,440
done with training we have a trained

479
00:16:13,199 --> 00:16:16,880
agent what we do is we connect it with

480
00:16:15,440 --> 00:16:19,240
the environment and we package the whole

481
00:16:16,880 --> 00:16:21,759
thing up and we call it a crow and so

482
00:16:19,240 --> 00:16:25,440
it's a crow because crows the birds can

483
00:16:21,759 --> 00:16:26,680
actually like speak uh English you guys

484
00:16:25,440 --> 00:16:28,279
know this you go to YouTube you can see

485
00:16:26,680 --> 00:16:29,920
videos of crows speaking they can speak

486
00:16:28,279 --> 00:16:31,920
like a parrot can speak but they know

487
00:16:29,920 --> 00:16:34,000
how to use tools and so this is like you

488
00:16:31,920 --> 00:16:37,399
know the the bird part and this is the

489
00:16:34,000 --> 00:16:40,199
the tool use part all

490
00:16:37,399 --> 00:16:42,639
right uh so here are some of the crows

491
00:16:40,199 --> 00:16:44,720
which we've uh launched at at future

492
00:16:42,639 --> 00:16:46,920
house um I shouldn't say that we've

493
00:16:44,720 --> 00:16:49,360
launched so protein Crow is something

494
00:16:46,920 --> 00:16:50,240
that's coming soon made by Manu here so

495
00:16:49,360 --> 00:16:52,279
if you have any questions about protein

496
00:16:50,240 --> 00:16:54,199
Crow you can talk to Manu um it's a

497
00:16:52,279 --> 00:16:55,680
really exciting uh tool for basically

498
00:16:54,199 --> 00:16:57,880
just doing protein design in this kind

499
00:16:55,680 --> 00:16:59,880
of environment um and then we have uh

500
00:16:57,880 --> 00:17:01,040
molecular cloning Crow it wasn't

501
00:16:59,880 --> 00:17:03,880
actually cool enough that we called it

502
00:17:01,040 --> 00:17:05,720
Mo cloning Crow because um I don't know

503
00:17:03,880 --> 00:17:06,959
I we just didn't find MC cloning that

504
00:17:05,720 --> 00:17:08,880
exciting so we didn't really push too

505
00:17:06,959 --> 00:17:10,839
hard in it then we have chem cro which

506
00:17:08,880 --> 00:17:13,799
uh Sam who's going to give the next talk

507
00:17:10,839 --> 00:17:17,400
who maybe here soon is she did the work

508
00:17:13,799 --> 00:17:19,959
on chem and then paper QA um which we've

509
00:17:17,400 --> 00:17:21,760
retrospectively called Wiki Crow which

510
00:17:19,959 --> 00:17:23,919
does literature research oh I so I do

511
00:17:21,760 --> 00:17:25,720
have a couple slides on protein Crow um

512
00:17:23,919 --> 00:17:28,960
I'm going to go through this super quick

513
00:17:25,720 --> 00:17:31,120
um uh but basically protein Crow has

514
00:17:28,960 --> 00:17:33,520
some uh existing deep learning models

515
00:17:31,120 --> 00:17:35,039
existing tools uh like bioinformatics

516
00:17:33,520 --> 00:17:37,440
that can call our literature research

517
00:17:35,039 --> 00:17:40,080
agent and then here's an example of like

518
00:17:37,440 --> 00:17:44,240
design 92 binders for

519
00:17:40,080 --> 00:17:47,400
pdl1 so we have a video here yes so here

520
00:17:44,240 --> 00:17:50,440
is like uh proin Crow uh calling the

521
00:17:47,400 --> 00:17:53,240
tools so this is sort of the steps that

522
00:17:50,440 --> 00:17:54,960
the the agent is taken here uh sometimes

523
00:17:53,240 --> 00:17:56,440
the models will emit like a rationale

524
00:17:54,960 --> 00:17:57,960
for why they made the decision so this

525
00:17:56,440 --> 00:17:59,520
is like what it's thinking it's doing in

526
00:17:57,960 --> 00:18:00,960
the in the trajectory and then it's

527
00:17:59,520 --> 00:18:02,480
calling these different tools that call

528
00:18:00,960 --> 00:18:05,000
these underlying deep learning models

529
00:18:02,480 --> 00:18:07,200
like protein M protein mpnn or RF

530
00:18:05,000 --> 00:18:08,440
diffusion and you can see it's basically

531
00:18:07,200 --> 00:18:10,559
reading through the output from these

532
00:18:08,440 --> 00:18:12,480
tools and then over time it's Gathering

533
00:18:10,559 --> 00:18:14,039
into these into like a library of of

534
00:18:12,480 --> 00:18:16,080
known binders that we want or sorry of

535
00:18:14,039 --> 00:18:19,159
binders that thinks are good and at the

536
00:18:16,080 --> 00:18:20,760
end um because we're uh oh did I lose my

537
00:18:19,159 --> 00:18:22,039
internet connection well anyway there

538
00:18:20,760 --> 00:18:24,080
should be like a video of a protein

539
00:18:22,039 --> 00:18:26,679
rotating here

540
00:18:24,080 --> 00:18:27,840
um but anyway uh we did do wet lab

541
00:18:26,679 --> 00:18:31,120
validation and found that some of these

542
00:18:27,840 --> 00:18:33,520
binders actually did pl1 so it's very

543
00:18:31,120 --> 00:18:34,960
exting I have a question yes I'm

544
00:18:33,520 --> 00:18:37,039
actually just curious why you didn't

545
00:18:34,960 --> 00:18:39,080
sort of pursue the molecular cloning

546
00:18:37,039 --> 00:18:40,880
tool a bit more because I mean people

547
00:18:39,080 --> 00:18:43,200
are doing a lot on protein design and

548
00:18:40,880 --> 00:18:44,880
chemical design and to me actually I

549
00:18:43,200 --> 00:18:46,400
think the most interesting thing and the

550
00:18:44,880 --> 00:18:48,720
molecular cloning if you were to make it

551
00:18:46,400 --> 00:18:52,320
a tool is it's one of these common lab

552
00:18:48,720 --> 00:18:54,559
techniques that I mean it so often fails

553
00:18:52,320 --> 00:18:56,720
um I'm sure everyone who's done cloning

554
00:18:54,559 --> 00:18:58,480
has an example of something that you

555
00:18:56,720 --> 00:19:01,640
know looked like it was going to totally

556
00:18:58,480 --> 00:19:03,000
work and then it didn't and uh I'm sort

557
00:19:01,640 --> 00:19:05,039
of interested if there were any

558
00:19:03,000 --> 00:19:06,919
conversations you and people on the team

559
00:19:05,039 --> 00:19:08,880
had about having a model that actually

560
00:19:06,919 --> 00:19:10,520
gave you like success rates of of

561
00:19:08,880 --> 00:19:11,840
cloning and speeding that up I think

562
00:19:10,520 --> 00:19:13,559
that's sort of actually the most

563
00:19:11,840 --> 00:19:15,960
interesting thing to pursue in that

564
00:19:13,559 --> 00:19:18,039
space a lot of people know how to do the

565
00:19:15,960 --> 00:19:20,200
cloning but yeah predicting whether it's

566
00:19:18,039 --> 00:19:21,799
going to work uh I don't know if there's

567
00:19:20,200 --> 00:19:25,280
a lot of tools out there that really

568
00:19:21,799 --> 00:19:27,480
tell us that so there is um uh

569
00:19:25,280 --> 00:19:30,039
a

570
00:19:27,480 --> 00:19:31,919
um yeah basically the reason we didn't

571
00:19:30,039 --> 00:19:33,760
really pursue it is we have to make this

572
00:19:31,919 --> 00:19:35,600
kind of calculus of like how impactful

573
00:19:33,760 --> 00:19:36,720
is the work like where you know are

574
00:19:35,600 --> 00:19:38,440
people going to be really excited about

575
00:19:36,720 --> 00:19:40,240
it and I think mro cloning is a great

576
00:19:38,440 --> 00:19:41,600
thing for a lot of you know molecular

577
00:19:40,240 --> 00:19:43,919
biologists they will be excited about it

578
00:19:41,600 --> 00:19:46,559
but I think it's just harder to make it

579
00:19:43,919 --> 00:19:48,440
understandable as high impact work so

580
00:19:46,559 --> 00:19:49,480
I'm happy to go back to it but it's like

581
00:19:48,440 --> 00:19:50,480
one of these things too where you need

582
00:19:49,480 --> 00:19:51,640
somebody on the team to be really

583
00:19:50,480 --> 00:19:53,360
excited about a project to push it

584
00:19:51,640 --> 00:19:55,240
forward and everybody in the biology

585
00:19:53,360 --> 00:19:56,640
team is just like not super excited

586
00:19:55,240 --> 00:19:59,919
about doing let I'm like to validate

587
00:19:56,640 --> 00:20:02,039
these models but if anybody you know

588
00:19:59,919 --> 00:20:05,000
yeah right if anybody has the passion

589
00:20:02,039 --> 00:20:06,880
for cloning Crow email me and I I we

590
00:20:05,000 --> 00:20:08,440
will happily send you designs and you

591
00:20:06,880 --> 00:20:11,320
will know that your cloning experiments

592
00:20:08,440 --> 00:20:14,280
will be leading to a greater

593
00:20:11,320 --> 00:20:17,480
cause okay so I want to talk now about

594
00:20:14,280 --> 00:20:20,000
paper QA so this is again an agent in an

595
00:20:17,480 --> 00:20:22,200
environment and here we're I'm going to

596
00:20:20,000 --> 00:20:24,960
show you basically a complete sort of

597
00:20:22,200 --> 00:20:26,159
measure against humans eat humans and

598
00:20:24,960 --> 00:20:29,159
what can you actually do with these sort

599
00:20:26,159 --> 00:20:33,200
of automated scientific agents so paper

600
00:20:29,159 --> 00:20:35,600
QA has uh a few tools one of them is

601
00:20:33,200 --> 00:20:37,200
Citation traversal so basically I have a

602
00:20:35,600 --> 00:20:40,080
paper I think is relevant who cited this

603
00:20:37,200 --> 00:20:41,559
paper and what does this paper site um

604
00:20:40,080 --> 00:20:42,640
paper search which is like a Google

605
00:20:41,559 --> 00:20:45,120
Scholar

606
00:20:42,640 --> 00:20:46,120
query um gather evidence which is where

607
00:20:45,120 --> 00:20:47,720
like okay I have a bunch of papers I

608
00:20:46,120 --> 00:20:49,280
think are relevant let me read them Page

609
00:20:47,720 --> 00:20:50,679
by page and see if there's any passages

610
00:20:49,280 --> 00:20:52,520
in that paper that could be helpful for

611
00:20:50,679 --> 00:20:54,080
answering my question and then it

612
00:20:52,520 --> 00:20:55,200
generates an answer basically takes all

613
00:20:54,080 --> 00:20:56,400
the information it knows and tries to

614
00:20:55,200 --> 00:20:57,679
answer a question and I probably should

615
00:20:56,400 --> 00:21:01,000
have said this but the input to paper QA

616
00:20:57,679 --> 00:21:04,039
is a question like has anybody studied

617
00:21:01,000 --> 00:21:05,919
this before or List you know three

618
00:21:04,039 --> 00:21:07,159
hypotheses for how to make a room

619
00:21:05,919 --> 00:21:09,880
temperature super

620
00:21:07,159 --> 00:21:11,080
conductor um and in fact actually now

621
00:21:09,880 --> 00:21:13,440
this is out of date is that we now

622
00:21:11,080 --> 00:21:14,279
actually have clinical trials as a way

623
00:21:13,440 --> 00:21:15,720
can search to can search through

624
00:21:14,279 --> 00:21:16,919
clinical trials and we also have like

625
00:21:15,720 --> 00:21:19,799
open targets so we can look at things

626
00:21:16,919 --> 00:21:21,960
like gws vas you know henic data um

627
00:21:19,799 --> 00:21:22,720
that's put together into a nice API by

628
00:21:21,960 --> 00:21:25,320
open

629
00:21:22,720 --> 00:21:27,279
targets which has dep map as well which

630
00:21:25,320 --> 00:21:29,720
came from the rooad um so here's an

631
00:21:27,279 --> 00:21:31,440
example of what this looks like so um we

632
00:21:29,720 --> 00:21:32,799
have here like Has anyone used llm

633
00:21:31,440 --> 00:21:34,640
agents to automate the discovery of a

634
00:21:32,799 --> 00:21:37,039
novel molecular chroma for that's the

635
00:21:34,640 --> 00:21:38,960
question I asked it um you can see the

636
00:21:37,039 --> 00:21:40,360
basically the the decisions the agent

637
00:21:38,960 --> 00:21:42,080
makes at the top left so it's finding

638
00:21:40,360 --> 00:21:43,240
papers these are some papers it's found

639
00:21:42,080 --> 00:21:46,159
and it's trying to download them from

640
00:21:43,240 --> 00:21:48,840
various sources like preprints uh from

641
00:21:46,159 --> 00:21:50,919
uh Open Access literature and then now

642
00:21:48,840 --> 00:21:54,159
basically it's reading Page by Page each

643
00:21:50,919 --> 00:21:56,880
of the the documents that it downloaded

644
00:21:54,159 --> 00:21:58,720
and um it's basically decided okay these

645
00:21:56,880 --> 00:21:59,960
were interesting passages so hold on

646
00:21:58,720 --> 00:22:02,240
them and then so now it's giving the

647
00:21:59,960 --> 00:22:04,200
answer to the question okay and then yes

648
00:22:02,240 --> 00:22:06,880
somebody's done it in fact it brought up

649
00:22:04,200 --> 00:22:10,080
a paper called chro which Sam Cox will

650
00:22:06,880 --> 00:22:10,080
be presenting on the next

651
00:22:10,600 --> 00:22:17,039
one okay so um when we started this

652
00:22:14,000 --> 00:22:18,600
project we were this is January 1 2024

653
00:22:17,039 --> 00:22:20,480
we have a benchmark L QA which I've

654
00:22:18,600 --> 00:22:23,200
described before we were getting like

655
00:22:20,480 --> 00:22:25,000
35% with our agent and then um you know

656
00:22:23,200 --> 00:22:26,919
over time doing a lot of changes

657
00:22:25,000 --> 00:22:29,440
improvements making the tools better

658
00:22:26,919 --> 00:22:32,480
like getting uh our ability to get more

659
00:22:29,440 --> 00:22:33,520
papers um we able to get to basically

660
00:22:32,480 --> 00:22:34,960
right around where human level

661
00:22:33,520 --> 00:22:36,640
performances and we hired like

662
00:22:34,960 --> 00:22:39,039
biologists like PhD level biologists to

663
00:22:36,640 --> 00:22:40,480
do this and then after that we were able

664
00:22:39,039 --> 00:22:42,559
to do this training procedure on it and

665
00:22:40,480 --> 00:22:46,360
we brought it from 70% up to you know

666
00:22:42,559 --> 00:22:49,080
90% so basically if you ask a PhD

667
00:22:46,360 --> 00:22:50,600
biologist and we pay them like $10 per

668
00:22:49,080 --> 00:22:52,480
correct question like answer this

669
00:22:50,600 --> 00:22:53,880
question you have unlimited time you can

670
00:22:52,480 --> 00:22:56,279
use any tools you would like you can use

671
00:22:53,880 --> 00:22:58,679
Google Scholar Etc um they get you know

672
00:22:56,279 --> 00:23:03,080
a 70% accuracy or so and we get 90%

673
00:22:58,679 --> 00:23:05,240
accuracy on this so yeah pretty cool I'm

674
00:23:03,080 --> 00:23:06,840
sure if we charged $1,000 you know like

675
00:23:05,240 --> 00:23:08,559
if we give him ,000 the question I'm

676
00:23:06,840 --> 00:23:10,039
sure the accuracy would go up but I

677
00:23:08,559 --> 00:23:12,520
think it's like reaching the point where

678
00:23:10,039 --> 00:23:13,559
it's like okay for $10 like I'll put in

679
00:23:12,520 --> 00:23:16,200
this amount of effort and we're able to

680
00:23:13,559 --> 00:23:17,919
beat that effort now answering like

681
00:23:16,200 --> 00:23:19,960
these lit QA questions which are

682
00:23:17,919 --> 00:23:22,200
multiple choice questions that are on

683
00:23:19,960 --> 00:23:24,080
you know Niche topics that is not really

684
00:23:22,200 --> 00:23:25,760
represented of doing science so we

685
00:23:24,080 --> 00:23:28,559
decided to try to find like a task that

686
00:23:25,760 --> 00:23:30,360
we didn't train on that is like uh still

687
00:23:28,559 --> 00:23:32,279
maybe better representation of

688
00:23:30,360 --> 00:23:34,480
synthesizing knowledge and so what we

689
00:23:32,279 --> 00:23:36,320
did is we basically had our system write

690
00:23:34,480 --> 00:23:38,600
Wikipedia articles and then we took

691
00:23:36,320 --> 00:23:40,559
human Wikipedia and just to set a

692
00:23:38,600 --> 00:23:41,640
control we looked at the human proteum

693
00:23:40,559 --> 00:23:44,279
so we have every single Gene that

694
00:23:41,640 --> 00:23:45,880
encodes for a protein and we basically

695
00:23:44,279 --> 00:23:47,279
found which ones have Wikipedia articles

696
00:23:45,880 --> 00:23:48,840
written by humans and which one and then

697
00:23:47,279 --> 00:23:51,200
we wrote corresponding ones written by

698
00:23:48,840 --> 00:23:52,480
our system then we extracted fragments

699
00:23:51,200 --> 00:23:55,240
so like a paragraph or a couple

700
00:23:52,480 --> 00:23:57,600
sentences from each Wikipedia the AI

701
00:23:55,240 --> 00:23:59,840
gener Wikipedia or the human Wikipedia

702
00:23:57,600 --> 00:24:02,400
we had PhD ologist again grade which

703
00:23:59,840 --> 00:24:05,320
ones they thought were more accurate and

704
00:24:02,400 --> 00:24:08,240
we beat the human Wikipedia on

705
00:24:05,320 --> 00:24:10,200
accuracy um and then we did another

706
00:24:08,240 --> 00:24:12,360
project which I think Sam Sam you g talk

707
00:24:10,200 --> 00:24:13,520
about this yeah okay great so I won't

708
00:24:12,360 --> 00:24:15,080
get into this too much but one of the

709
00:24:13,520 --> 00:24:18,600
cool things you can do with this is you

710
00:24:15,080 --> 00:24:23,840
can actually take a claim like uh I

711
00:24:18,600 --> 00:24:26,240
don't know um uh the peak uh intensity

712
00:24:23,840 --> 00:24:27,480
of light of the sun is green um and then

713
00:24:26,240 --> 00:24:29,600
you can go see if anybody's contradicted

714
00:24:27,480 --> 00:24:31,640
that literature before by the way it is

715
00:24:29,600 --> 00:24:33,760
green it's crazy if any of you have

716
00:24:31,640 --> 00:24:35,399
little kids like my son is like you know

717
00:24:33,760 --> 00:24:36,720
what color is you know the sun it's like

718
00:24:35,399 --> 00:24:37,960
oh it's yellow and it's like okay but I

719
00:24:36,720 --> 00:24:39,399
heard at school that it's white because

720
00:24:37,960 --> 00:24:40,520
the astronauts on the International

721
00:24:39,399 --> 00:24:42,080
Space Station take a picture and it's

722
00:24:40,520 --> 00:24:43,480
white and then I'm like trying to figure

723
00:24:42,080 --> 00:24:44,559
it out apparently it's also green

724
00:24:43,480 --> 00:24:46,600
because I feel like it's black body

725
00:24:44,559 --> 00:24:48,520
radiation but the peak intensity is that

726
00:24:46,600 --> 00:24:49,440
green color so I don't know what Co sun

727
00:24:48,520 --> 00:24:53,960
is

728
00:24:49,440 --> 00:24:56,000
anymore yes question just just I just

729
00:24:53,960 --> 00:24:58,480
wanted to follow to follow up on that I

730
00:24:56,000 --> 00:24:59,799
actually had this question on the AR

731
00:24:58,480 --> 00:25:02,760
slide like

732
00:24:59,799 --> 00:25:05,919
yeah you if you're training the models

733
00:25:02,760 --> 00:25:06,880
on the like existing Corpus of the

734
00:25:05,919 --> 00:25:09,880
science

735
00:25:06,880 --> 00:25:12,360
papers what's your opinion on the

736
00:25:09,880 --> 00:25:14,760
reproducibility crisis like is there any

737
00:25:12,360 --> 00:25:17,120
way to mitigate that because if there

738
00:25:14,760 --> 00:25:19,960
are like thousands of papers especially

739
00:25:17,120 --> 00:25:24,640
on archive where they like not be

740
00:25:19,960 --> 00:25:27,360
reviewed significant like maybe like 10

741
00:25:24,640 --> 00:25:30,000
in the worst case 20 20% of those are

742
00:25:27,360 --> 00:25:32,240
like just not good yeah yeah this is a

743
00:25:30,000 --> 00:25:33,360
great question so um I I'll give five

744
00:25:32,240 --> 00:25:35,279
answers to this question because I think

745
00:25:33,360 --> 00:25:37,399
about it a lot okay answer number one is

746
00:25:35,279 --> 00:25:39,919
that we use something called the

747
00:25:37,399 --> 00:25:42,440
finished publication forums ranking of

748
00:25:39,919 --> 00:25:45,279
Journal tiers so these are things that

749
00:25:42,440 --> 00:25:46,640
identify if a journal is predatory or if

750
00:25:45,279 --> 00:25:48,399
it's like you know a pure reviewed

751
00:25:46,640 --> 00:25:49,720
Journal so we use that on all sources

752
00:25:48,399 --> 00:25:51,679
that come into our system obviously

753
00:25:49,720 --> 00:25:53,320
won't catch archive but that is like one

754
00:25:51,679 --> 00:25:56,279
big issue is there's a lot of journals

755
00:25:53,320 --> 00:25:59,399
there's a lot of like papers in like

756
00:25:56,279 --> 00:26:01,480
mdpi or something or or even like posos

757
00:25:59,399 --> 00:26:02,880
there's like a lot of papers that are um

758
00:26:01,480 --> 00:26:05,120
just like not good quality so we try to

759
00:26:02,880 --> 00:26:08,559
use that we also use citation count you

760
00:26:05,120 --> 00:26:09,960
know um I'm sorry and I hope we come up

761
00:26:08,559 --> 00:26:12,080
with a better idea at some point in the

762
00:26:09,960 --> 00:26:13,720
future but for now um citation count is

763
00:26:12,080 --> 00:26:16,679
a good measure of if something is like

764
00:26:13,720 --> 00:26:18,159
you know just not really I don't want to

765
00:26:16,679 --> 00:26:19,399
it's just not in mainstream so it's like

766
00:26:18,159 --> 00:26:21,200
there's a lot of paper you guys may not

767
00:26:19,399 --> 00:26:24,159
know this but if you go search like has

768
00:26:21,200 --> 00:26:26,960
anybody shown that like putting a magnet

769
00:26:24,159 --> 00:26:29,360
around your neck is going to make you

770
00:26:26,960 --> 00:26:31,039
like cure your can answer there are

771
00:26:29,360 --> 00:26:33,120
Papers written about that there are Pap

772
00:26:31,039 --> 00:26:34,440
about the cosmic memory of water you

773
00:26:33,120 --> 00:26:36,279
know there's papers about all kinds of

774
00:26:34,440 --> 00:26:37,440
crap it's crazy but these journals these

775
00:26:36,279 --> 00:26:38,760
like bottom tier journals that just

776
00:26:37,440 --> 00:26:40,000
publish anything and so you have to have

777
00:26:38,760 --> 00:26:42,840
some kind of system and citation count

778
00:26:40,000 --> 00:26:44,640
is one signal there but I've written on

779
00:26:42,840 --> 00:26:47,840
my blog plugging my blog here I have

780
00:26:44,640 --> 00:26:50,159
blog diffuse .1 where I put my unserious

781
00:26:47,840 --> 00:26:52,120
academic work so I've written I made

782
00:26:50,159 --> 00:26:54,080
some tools and I've had thoughts about

783
00:26:52,120 --> 00:26:55,640
how to do how to spot fraudulent papers

784
00:26:54,080 --> 00:26:57,159
but I just don't I'm not an expert in

785
00:26:55,640 --> 00:26:58,679
this topic so I just have some hobest

786
00:26:57,159 --> 00:27:00,279
ideas but I don't think we've solved it

787
00:26:58,679 --> 00:27:01,600
and it is something that's even more

788
00:27:00,279 --> 00:27:02,520
important for LMS because LMS are

789
00:27:01,600 --> 00:27:04,880
extremely

790
00:27:02,520 --> 00:27:06,720
goal

791
00:27:04,880 --> 00:27:08,559
question yes so the question is is it

792
00:27:06,720 --> 00:27:11,360
aware of retractions and Corrections yes

793
00:27:08,559 --> 00:27:13,679
we do use retraction watches uh DB which

794
00:27:11,360 --> 00:27:15,960
is hosted on Cross ref to check if

795
00:27:13,679 --> 00:27:18,480
something is retracted I will say that

796
00:27:15,960 --> 00:27:20,720
retractions are very like like it's a

797
00:27:18,480 --> 00:27:23,320
volunteer run database it's not always

798
00:27:20,720 --> 00:27:26,080
up to date and so that is not really

799
00:27:23,320 --> 00:27:29,240
well done but we do we do use the the

800
00:27:26,080 --> 00:27:31,919
existing sources there question yeah so

801
00:27:29,240 --> 00:27:34,320
um I know one major challenge for those

802
00:27:31,919 --> 00:27:36,880
large language models is integration of

803
00:27:34,320 --> 00:27:38,760
new information and in research I think

804
00:27:36,880 --> 00:27:40,960
this is like a super important thing

805
00:27:38,760 --> 00:27:43,039
since the new development in the last

806
00:27:40,960 --> 00:27:45,360
two years would be very fundamental for

807
00:27:43,039 --> 00:27:47,679
people to solve the new biological

808
00:27:45,360 --> 00:27:49,840
question so how do you plan to tackle

809
00:27:47,679 --> 00:27:52,200
this yeah so this system that we're

810
00:27:49,840 --> 00:27:56,279
talking about this this agent system um

811
00:27:52,200 --> 00:27:58,799
we tell it to only use information from

812
00:27:56,279 --> 00:28:00,200
the sources we provide to it and so

813
00:27:58,799 --> 00:28:01,840
we've really tried to make sure that the

814
00:28:00,200 --> 00:28:03,840
model is only using scientific

815
00:28:01,840 --> 00:28:05,240
literature or the or the clinical trials

816
00:28:03,840 --> 00:28:06,679
or the henic data is only trying to use

817
00:28:05,240 --> 00:28:08,320
that to answer the questions and one of

818
00:28:06,679 --> 00:28:09,919
the reasons why I don't know if I should

819
00:28:08,320 --> 00:28:12,039
mention this but like every time it

820
00:28:09,919 --> 00:28:14,279
emits a sentence every single sentence

821
00:28:12,039 --> 00:28:16,240
has a citation and that citation is to a

822
00:28:14,279 --> 00:28:17,480
page number on a source so you can track

823
00:28:16,240 --> 00:28:19,080
the Providence of everything that comes

824
00:28:17,480 --> 00:28:21,760
out of this model down to like the

825
00:28:19,080 --> 00:28:23,559
individual page in a research paper uh

826
00:28:21,760 --> 00:28:25,960
sorry I think what that means like oh

827
00:28:23,559 --> 00:28:28,480
like if you want to build like a um AI

828
00:28:25,960 --> 00:28:30,080
biologist how do you make sure that he

829
00:28:28,480 --> 00:28:32,279
how do you feed new information to it

830
00:28:30,080 --> 00:28:34,760
without retraining the big

831
00:28:32,279 --> 00:28:37,679
model right so I guess what I'm trying

832
00:28:34,760 --> 00:28:39,519
to articulate is that um this model

833
00:28:37,679 --> 00:28:41,039
tries to only reason from information we

834
00:28:39,519 --> 00:28:42,720
present to it and so we just make sure

835
00:28:41,039 --> 00:28:45,039
the information we present is from like

836
00:28:42,720 --> 00:28:48,080
recent papers or from sources that we're

837
00:28:45,039 --> 00:28:49,440
confident in and so we're trying to I

838
00:28:48,080 --> 00:28:52,480
mean you're getting at a good point is

839
00:28:49,440 --> 00:28:54,279
that like the dream is that you can

840
00:28:52,480 --> 00:28:56,080
separate out the model's knowledge

841
00:28:54,279 --> 00:28:57,159
Corpus from its ability to reason

842
00:28:56,080 --> 00:28:58,559
because right now they're somehow

843
00:28:57,159 --> 00:29:01,480
coupled so you need really really big

844
00:28:58,559 --> 00:29:02,720
models to do reasoning but then you like

845
00:29:01,480 --> 00:29:04,159
have all these problems like then you

846
00:29:02,720 --> 00:29:05,919
need a really big model and if it's like

847
00:29:04,159 --> 00:29:07,640
three years out a date it doesn't know

848
00:29:05,919 --> 00:29:09,039
more recent stuff and so yeah we try as

849
00:29:07,640 --> 00:29:10,360
much as we can to separate out like the

850
00:29:09,039 --> 00:29:11,960
ability for the models to use

851
00:29:10,360 --> 00:29:14,760
information and reason from their

852
00:29:11,960 --> 00:29:17,679
ability to memorize all of

853
00:29:14,760 --> 00:29:21,760
science related question yeah is it

854
00:29:17,679 --> 00:29:24,519
possible to validate the this notion

855
00:29:21,760 --> 00:29:26,880
that like whenever it is telling you

856
00:29:24,519 --> 00:29:29,440
something it has the citation that's

857
00:29:26,880 --> 00:29:31,760
directly from the thing ET yes so in

858
00:29:29,440 --> 00:29:35,519
fact this is like the majority of this

859
00:29:31,760 --> 00:29:37,279
eval here was to make sure that the

860
00:29:35,519 --> 00:29:39,039
citations and the claims that are in the

861
00:29:37,279 --> 00:29:41,000
Wikipedia articles it writes are

862
00:29:39,039 --> 00:29:45,320
consistent with the site sources that it

863
00:29:41,000 --> 00:29:48,120
cited and so this like 86% those 14%

864
00:29:45,320 --> 00:29:50,039
like failure that is when it cited a

865
00:29:48,120 --> 00:29:53,480
source but it either mischaracterized

866
00:29:50,039 --> 00:29:55,519
the source or it made a claim that um

867
00:29:53,480 --> 00:29:57,320
didn't site a source and it should have

868
00:29:55,519 --> 00:29:58,600
been cited so those are examples of of

869
00:29:57,320 --> 00:30:01,240
failure so that's the sort of failure

870
00:29:58,600 --> 00:30:03,080
mode of of that and actually that's how

871
00:30:01,240 --> 00:30:04,720
we this whole eval is just are you

872
00:30:03,080 --> 00:30:06,679
consistent with the sources you site and

873
00:30:04,720 --> 00:30:10,080
are you accurate within the context of

874
00:30:06,679 --> 00:30:13,200
those sources so humans hallucinate at a

875
00:30:10,080 --> 00:30:15,440
higher rate than than the language

876
00:30:13,200 --> 00:30:18,559
model wait one more question and then I

877
00:30:15,440 --> 00:30:21,240
have so many more more exciting things I

878
00:30:18,559 --> 00:30:23,039
about that you trained the model from

879
00:30:21,240 --> 00:30:24,799
scratch or do you use a pre-trained

880
00:30:23,039 --> 00:30:27,399
alignment ont to only look at these

881
00:30:24,799 --> 00:30:29,960
sources because if it's the letter it

882
00:30:27,399 --> 00:30:32,200
may be difficult to tell if it's indeed

883
00:30:29,960 --> 00:30:35,279
only looking at those sources

884
00:30:32,200 --> 00:30:37,320
or this is a good question so this plot

885
00:30:35,279 --> 00:30:39,039
I'm showing here this lit QA Benchmark

886
00:30:37,320 --> 00:30:42,200
we made lit QA two different ways we

887
00:30:39,039 --> 00:30:44,399
made lit QA like we made a set of 60

888
00:30:42,200 --> 00:30:46,360
questions or 55 questions or something

889
00:30:44,399 --> 00:30:47,760
where all of the papers were published

890
00:30:46,360 --> 00:30:49,919
after the cut off date of the

891
00:30:47,760 --> 00:30:51,600
pre-trained language model so those

892
00:30:49,919 --> 00:30:53,399
papers were not in the pre-training

893
00:30:51,600 --> 00:30:54,880
Corpus and then what we did is we asked

894
00:30:53,399 --> 00:30:56,600
it questions that it had to look at the

895
00:30:54,880 --> 00:30:58,679
paper to answer okay so then we're like

896
00:30:56,600 --> 00:31:00,320
measuring you know does does it really

897
00:30:58,679 --> 00:31:02,639
like look at these papers or is it just

898
00:31:00,320 --> 00:31:05,679
trying to recall and then like attribute

899
00:31:02,639 --> 00:31:08,519
post Haw and the second thing is that we

900
00:31:05,679 --> 00:31:10,720
also check on the on the data set like

901
00:31:08,519 --> 00:31:12,120
we go to these pre-trained models and we

902
00:31:10,720 --> 00:31:13,320
ask the questions of the pre-train model

903
00:31:12,120 --> 00:31:15,039
without access to these tools without

904
00:31:13,320 --> 00:31:16,399
access to the papers you know what's the

905
00:31:15,039 --> 00:31:17,440
answer to these questions so let's we

906
00:31:16,399 --> 00:31:20,000
have a measure of like a baseline

907
00:31:17,440 --> 00:31:22,039
performance and I think it's like 25% on

908
00:31:20,000 --> 00:31:23,519
this on this Benchmark of how good the

909
00:31:22,039 --> 00:31:25,760
pre-train model does without access to

910
00:31:23,519 --> 00:31:30,200
the tools and then so the other point is

911
00:31:25,760 --> 00:31:32,679
that when this bump from like here to

912
00:31:30,200 --> 00:31:35,919
here is when we switch to our own model

913
00:31:32,679 --> 00:31:37,799
that was um you know like 8 billion

914
00:31:35,919 --> 00:31:39,399
parameters and of course 8 billion is a

915
00:31:37,799 --> 00:31:40,639
lot but that's not enough to fit all of

916
00:31:39,399 --> 00:31:41,960
scientific literature so it's not really

917
00:31:40,639 --> 00:31:46,039
at the same scale of something like you

918
00:31:41,960 --> 00:31:48,320
know um GPT 4 which is um you know over

919
00:31:46,039 --> 00:31:50,919
100 billion

920
00:31:48,320 --> 00:31:53,159
parameters awesome great

921
00:31:50,919 --> 00:31:55,320
questions um I just want to show you

922
00:31:53,159 --> 00:31:56,399
guys like uh one quick application here

923
00:31:55,320 --> 00:31:58,200
so like here's an example a question

924
00:31:56,399 --> 00:32:00,440
that you can ask this is there genetic

925
00:31:58,200 --> 00:32:03,159
evidence supporting tr2 association with

926
00:32:00,440 --> 00:32:04,559
tated neurog generation uh speculate on

927
00:32:03,159 --> 00:32:07,080
the plausible mechanism and your

928
00:32:04,559 --> 00:32:09,039
response okay and then it gives you know

929
00:32:07,080 --> 00:32:10,840
some ideas about this and it cites these

930
00:32:09,039 --> 00:32:13,200
and this is these this is about the

931
00:32:10,840 --> 00:32:16,000
sources say like on page one page eight

932
00:32:13,200 --> 00:32:17,000
to nine um so you can trace back to see

933
00:32:16,000 --> 00:32:20,799
where it came from and then if you look

934
00:32:17,000 --> 00:32:24,120
at the sources here um you can see you

935
00:32:20,799 --> 00:32:25,960
know which which sources it came

936
00:32:24,120 --> 00:32:27,440
from and then you can see all the

937
00:32:25,960 --> 00:32:29,240
reasoning process like why it shows

938
00:32:27,440 --> 00:32:31,240
different ources what score it gave the

939
00:32:29,240 --> 00:32:32,720
sources how it came to the conclusion of

940
00:32:31,240 --> 00:32:36,320
them etc

941
00:32:32,720 --> 00:32:37,760
etc cool so I have one more which you'll

942
00:32:36,320 --> 00:32:40,799
show here is we have this website has

943
00:32:37,760 --> 00:32:42,440
anyone.com and you can ask a question

944
00:32:40,799 --> 00:32:44,200
like okay has anyone ever made split gfp

945
00:32:42,440 --> 00:32:45,639
that's one's too easy okay let's see has

946
00:32:44,200 --> 00:32:48,320
anyone determined the specific mechanism

947
00:32:45,639 --> 00:32:49,840
by which pfna disrupts thyroid function

948
00:32:48,320 --> 00:32:51,559
so we can click over to this one and

949
00:32:49,840 --> 00:32:53,080
then it will go through and basically

950
00:32:51,559 --> 00:32:55,000
give you an answer like no no one has

951
00:32:53,080 --> 00:32:57,320
determined this conclusion and it has

952
00:32:55,000 --> 00:32:59,360
you know some description why it came to

953
00:32:57,320 --> 00:33:01,240
that conclusion and it has these sources

954
00:32:59,360 --> 00:33:03,080
cited and then you can go back and you

955
00:33:01,240 --> 00:33:04,559
can see okay like what did it site it

956
00:33:03,080 --> 00:33:06,000
cited this paper which has five

957
00:33:04,559 --> 00:33:08,440
citations it's from a peer review

958
00:33:06,000 --> 00:33:10,559
journal has 81 citations from piew

959
00:33:08,440 --> 00:33:13,559
Journal this is 250 and from the highest

960
00:33:10,559 --> 00:33:15,720
quality piew Journal etc etc you can see

961
00:33:13,559 --> 00:33:17,320
like how long it thought for this was a

962
00:33:15,720 --> 00:33:19,200
a cached question but it took you know

963
00:33:17,320 --> 00:33:20,600
three minutes to answer this question

964
00:33:19,200 --> 00:33:21,919
cost 32 cents I don't know why you guys

965
00:33:20,600 --> 00:33:26,919
need to know that but that's how much we

966
00:33:21,919 --> 00:33:29,080
paid um yeah cool so then um the last

967
00:33:26,919 --> 00:33:31,399
sort of application I'll mention is is

968
00:33:29,080 --> 00:33:34,000
this Wiki Crow so there's a website Wiki

969
00:33:31,399 --> 00:33:35,559
crow. and what we did is we basically

970
00:33:34,000 --> 00:33:37,760
finished off Wikipedia for the human

971
00:33:35,559 --> 00:33:39,799
prodium so there's about 2,700 articles

972
00:33:37,760 --> 00:33:41,039
in Wikipedia for the human prodium um so

973
00:33:39,799 --> 00:33:43,559
there's quite a few missing there's

974
00:33:41,039 --> 00:33:46,159
about 20,000 protein coding genes uh in

975
00:33:43,559 --> 00:33:49,000
human genome and so we basically you

976
00:33:46,159 --> 00:33:51,120
know finished writing uh about 177,000

977
00:33:49,000 --> 00:33:53,039
articles took about 48 hours and so then

978
00:33:51,120 --> 00:33:54,559
you have like a an upto-date version and

979
00:33:53,039 --> 00:33:56,399
I'll mention too that some of Wikipedia

980
00:33:54,559 --> 00:33:58,679
is out of date we can run this every 48

981
00:33:56,399 --> 00:34:00,960
Hours you know to update the knowledge

982
00:33:58,679 --> 00:34:02,760
of of science on the human so if you go

983
00:34:00,960 --> 00:34:06,240
to the website you can put in any random

984
00:34:02,760 --> 00:34:07,600
gen that you you want like lipg um and

985
00:34:06,240 --> 00:34:10,720
then it has you know some some

986
00:34:07,600 --> 00:34:12,760
information here uh about uh the the

987
00:34:10,720 --> 00:34:14,280
Gene and the structure and then it has

988
00:34:12,760 --> 00:34:15,599
the overview here and this is all

989
00:34:14,280 --> 00:34:17,200
generated from our system and you can

990
00:34:15,599 --> 00:34:18,520
read through it and basically if you

991
00:34:17,200 --> 00:34:19,560
ever come across a gene from some kind

992
00:34:18,520 --> 00:34:22,079
of screening and you don't know what it

993
00:34:19,560 --> 00:34:23,520
does uh you could try this out and maybe

994
00:34:22,079 --> 00:34:25,480
you find it's

995
00:34:23,520 --> 00:34:27,960
interesting one of the cool things about

996
00:34:25,480 --> 00:34:29,679
this work though do I have this slide

997
00:34:27,960 --> 00:34:30,760
yeah okay great so one of the cool

998
00:34:29,679 --> 00:34:32,240
things about this is that there has not

999
00:34:30,760 --> 00:34:34,359
really been a text embedding of human

1000
00:34:32,240 --> 00:34:36,599
proteum right so like you can obviously

1001
00:34:34,359 --> 00:34:39,200
embed the proteins where you can embed

1002
00:34:36,599 --> 00:34:40,280
the genes but what we're able to do is

1003
00:34:39,200 --> 00:34:41,599
actually since we have now a text

1004
00:34:40,280 --> 00:34:43,240
description of what is the function

1005
00:34:41,599 --> 00:34:44,320
what's the disease interactions like

1006
00:34:43,240 --> 00:34:46,200
what are the physical interactions of

1007
00:34:44,320 --> 00:34:48,320
all these different person coding genes

1008
00:34:46,200 --> 00:34:49,839
we can actually embed them all so like

1009
00:34:48,320 --> 00:34:51,320
you know take all the words in it and

1010
00:34:49,839 --> 00:34:52,359
then find out which ones are similar

1011
00:34:51,320 --> 00:34:53,639
they have a complete embedding of the

1012
00:34:52,359 --> 00:34:54,760
human proteum and you see like things

1013
00:34:53,639 --> 00:34:56,440
that you expect like here's a bunch of

1014
00:34:54,760 --> 00:34:57,960
receptors right here's a bunch of DNA

1015
00:34:56,440 --> 00:34:58,800
Associated ones and inside the DNA

1016
00:34:57,960 --> 00:35:00,920
Associated ones or a lot of

1017
00:34:58,800 --> 00:35:02,359
transcription factors and so what we did

1018
00:35:00,920 --> 00:35:04,359
from that is we actually took that

1019
00:35:02,359 --> 00:35:06,200
embedding data and we built this uh

1020
00:35:04,359 --> 00:35:08,839
related Gene section so these related

1021
00:35:06,200 --> 00:35:10,839
genes are genes that have like a close

1022
00:35:08,839 --> 00:35:12,400
text embedding that you wouldn't like

1023
00:35:10,839 --> 00:35:14,920
find from any other way except for

1024
00:35:12,400 --> 00:35:18,040
synthesizing the knowledge of the genes

1025
00:35:14,920 --> 00:35:18,040
anyway kind of cool maybe

1026
00:35:19,640 --> 00:35:23,480
interesting so if you embed the entire

1027
00:35:22,079 --> 00:35:25,040
text summary of the gene I imagine you

1028
00:35:23,480 --> 00:35:26,839
get something very different from taking

1029
00:35:25,040 --> 00:35:28,280
each section or or subsections and

1030
00:35:26,839 --> 00:35:30,119
embedding those and

1031
00:35:28,280 --> 00:35:31,280
you get much more diverse neighborhood

1032
00:35:30,119 --> 00:35:33,440
trying to find okay you know these set

1033
00:35:31,280 --> 00:35:35,400
of genes are membrane proteins in these

1034
00:35:33,440 --> 00:35:37,000
types of cells versus the entire more

1035
00:35:35,400 --> 00:35:39,160
average out thing is that an experiment

1036
00:35:37,000 --> 00:35:41,200
you ran um so actually I think these are

1037
00:35:39,160 --> 00:35:43,040
run on just the overview okay because

1038
00:35:41,200 --> 00:35:44,480
what you're getting at is is important

1039
00:35:43,040 --> 00:35:46,560
because for some genes we don't know

1040
00:35:44,480 --> 00:35:48,000
what like disease associations they have

1041
00:35:46,560 --> 00:35:49,200
so that section is empty so this is

1042
00:35:48,000 --> 00:35:50,640
around the overview which is like a

1043
00:35:49,200 --> 00:35:51,720
summary of all the sections but you're

1044
00:35:50,640 --> 00:35:52,960
right you could do something like I only

1045
00:35:51,720 --> 00:35:54,720
want to know the physical interactions

1046
00:35:52,960 --> 00:35:57,440
and then you can build like a new

1047
00:35:54,720 --> 00:36:00,240
physical interaction graph that is up to

1048
00:35:57,440 --> 00:36:01,680
date you know and and covers you know as

1049
00:36:00,240 --> 00:36:04,359
many papers as we possi can but it might

1050
00:36:01,680 --> 00:36:06,720
be more interesting uh as AOL for you

1051
00:36:04,359 --> 00:36:08,760
know probing like what kind of you know

1052
00:36:06,720 --> 00:36:12,400
Gene uh or which protein protein

1053
00:36:08,760 --> 00:36:15,839
interactions should I be looking at yeah

1054
00:36:12,400 --> 00:36:16,920
cool so um you know like I said one of

1055
00:36:15,839 --> 00:36:18,720
the things that's unique about future

1056
00:36:16,920 --> 00:36:20,839
house is we're like an Engineering Group

1057
00:36:18,720 --> 00:36:23,000
and a science group so rather than just

1058
00:36:20,839 --> 00:36:25,240
make this like a demo thing we like uh

1059
00:36:23,000 --> 00:36:26,599
built it into a platform and so we can

1060
00:36:25,240 --> 00:36:28,319
run I think this is these are out ofate

1061
00:36:26,599 --> 00:36:30,319
numbers basically we can run 25 things

1062
00:36:28,319 --> 00:36:31,760
per minute um we have about 150 million

1063
00:36:30,319 --> 00:36:33,240
papers we can look at and so we could

1064
00:36:31,760 --> 00:36:35,359
write a Wikipedia page for every human

1065
00:36:33,240 --> 00:36:36,599
disease every 3.5 days so basically

1066
00:36:35,359 --> 00:36:38,119
every three and a half days we summarize

1067
00:36:36,599 --> 00:36:41,560
all the new literature rewrite it for

1068
00:36:38,119 --> 00:36:43,200
all the 25,000 human diseases um we

1069
00:36:41,560 --> 00:36:44,880
could also check for contradictions in

1070
00:36:43,200 --> 00:36:46,640
every single paper on archive that comes

1071
00:36:44,880 --> 00:36:48,599
out every month basically summary of

1072
00:36:46,640 --> 00:36:50,760
where this fits does it disagree agree

1073
00:36:48,599 --> 00:36:52,079
what's consensus on literature um we're

1074
00:36:50,760 --> 00:36:53,920
getting there but we're almost you know

1075
00:36:52,079 --> 00:36:55,200
10x in this capacity and so we could

1076
00:36:53,920 --> 00:36:57,319
check every single Paper that's

1077
00:36:55,200 --> 00:36:59,520
published every year for contradictions

1078
00:36:57,319 --> 00:37:01,119
uh literature disagreement literature um

1079
00:36:59,520 --> 00:37:02,760
we could rewrite all of Wikipedia every

1080
00:37:01,119 --> 00:37:07,040
3 weeks as

1081
00:37:02,760 --> 00:37:08,400
well cool so now that I have exhausted

1082
00:37:07,040 --> 00:37:11,240
almost all of my time I'm going to tell

1083
00:37:08,400 --> 00:37:13,680
you about exciting new results okay so

1084
00:37:11,240 --> 00:37:15,599
um over the last like couple months

1085
00:37:13,680 --> 00:37:17,720
there's been a real big change in how uh

1086
00:37:15,599 --> 00:37:19,200
language models work so we have been

1087
00:37:17,720 --> 00:37:20,640
stuck in this Paradigm called

1088
00:37:19,200 --> 00:37:21,880
pre-training for a long time this is

1089
00:37:20,640 --> 00:37:22,920
probably something many of you familiar

1090
00:37:21,880 --> 00:37:24,200
with it's basically to get a smarter

1091
00:37:22,920 --> 00:37:25,680
model you need more data you need more

1092
00:37:24,200 --> 00:37:30,000
compute and they got to get bigger this

1093
00:37:25,680 --> 00:37:32,040
is like gpt2 3 4 right GPT 5 and it's

1094
00:37:30,000 --> 00:37:33,240
become a really difficult thing to scale

1095
00:37:32,040 --> 00:37:35,560
and it also is very interesting

1096
00:37:33,240 --> 00:37:37,520
phenomena is that um pulling all of your

1097
00:37:35,560 --> 00:37:39,800
compute into one big model and then

1098
00:37:37,520 --> 00:37:41,680
serving it for a very tiny cost to you

1099
00:37:39,800 --> 00:37:43,680
is a great business strategy and that

1100
00:37:41,680 --> 00:37:45,720
means there collectivization basically

1101
00:37:43,680 --> 00:37:47,240
all of the compute is being concentrated

1102
00:37:45,720 --> 00:37:48,640
in just a few organizations because they

1103
00:37:47,240 --> 00:37:51,000
can make these really big models that

1104
00:37:48,640 --> 00:37:52,160
then have a low marginal cost and then

1105
00:37:51,000 --> 00:37:53,720
there's this thing called scaffolding

1106
00:37:52,160 --> 00:37:55,040
this is mostly of the work that I

1107
00:37:53,720 --> 00:37:57,160
presented was that we basically used

1108
00:37:55,040 --> 00:37:59,079
domain knowledge to put together some

1109
00:37:57,160 --> 00:38:01,160
tool tools and information that help the

1110
00:37:59,079 --> 00:38:03,119
the language models do better in

1111
00:38:01,160 --> 00:38:04,319
scientific environments there's a new

1112
00:38:03,119 --> 00:38:06,040
kind of Paradigm which has come out

1113
00:38:04,319 --> 00:38:08,640
recently which is reasoning models and

1114
00:38:06,040 --> 00:38:10,760
so reasoning models require domain

1115
00:38:08,640 --> 00:38:12,640
knowledge but they require much less

1116
00:38:10,760 --> 00:38:14,000
data and they require much less compute

1117
00:38:12,640 --> 00:38:16,800
and so I think we're going to see like a

1118
00:38:14,000 --> 00:38:18,000
real like a reverse of the trend where

1119
00:38:16,800 --> 00:38:20,079
it's basically becoming more and more

1120
00:38:18,000 --> 00:38:22,440
Out Of Reach for academics to do any

1121
00:38:20,079 --> 00:38:24,400
kind of work in this area because big

1122
00:38:22,440 --> 00:38:25,680
models just win and so now things are

1123
00:38:24,400 --> 00:38:27,680
actually flattening back out where you

1124
00:38:25,680 --> 00:38:29,400
can actually have small models that you

1125
00:38:27,680 --> 00:38:30,960
can use at a more expensive compute like

1126
00:38:29,400 --> 00:38:32,720
it takes 10 minutes for them to answer a

1127
00:38:30,960 --> 00:38:34,040
question rather than 10 seconds but you

1128
00:38:32,720 --> 00:38:35,560
can get the same performance and you can

1129
00:38:34,040 --> 00:38:38,119
make them domain

1130
00:38:35,560 --> 00:38:39,160
specialized um so I'm going to skip over

1131
00:38:38,119 --> 00:38:40,440
this but basically I'll just want to

1132
00:38:39,160 --> 00:38:42,760
show you some evidence of this new

1133
00:38:40,440 --> 00:38:44,319
dimension of scaling so this is sort of

1134
00:38:42,760 --> 00:38:46,040
this is a benchmark comm's last exam

1135
00:38:44,319 --> 00:38:48,200
again so basically here's the sort of

1136
00:38:46,040 --> 00:38:50,319
typical scaling we have like gbt

1137
00:38:48,200 --> 00:38:51,960
40 pretty big model then Sonic comes out

1138
00:38:50,319 --> 00:38:53,680
it's a little bit bigger model and then

1139
00:38:51,960 --> 00:38:55,800
01 comes out and then we have this like

1140
00:38:53,680 --> 00:38:57,319
change from traditional pre-training

1141
00:38:55,800 --> 00:38:58,720
models to now reasoning models and you

1142
00:38:57,319 --> 00:39:01,000
can you can see this Benchmark went from

1143
00:38:58,720 --> 00:39:03,560
being whatever 8% to then becoming like

1144
00:39:01,000 --> 00:39:05,400
30% in a just matter of a few weeks as

1145
00:39:03,560 --> 00:39:06,599
models were released that were reasoning

1146
00:39:05,400 --> 00:39:08,640
and you have this like new dimension

1147
00:39:06,599 --> 00:39:09,920
where you can basically make the models

1148
00:39:08,640 --> 00:39:11,720
um not just bigger but you can make them

1149
00:39:09,920 --> 00:39:13,240
think for longer you can make them

1150
00:39:11,720 --> 00:39:15,880
specialized in domain by training them

1151
00:39:13,240 --> 00:39:17,200
on reasoning in in one topic and this

1152
00:39:15,880 --> 00:39:18,599
has exploded into the open source

1153
00:39:17,200 --> 00:39:21,040
community so here's a plot of a bunch of

1154
00:39:18,599 --> 00:39:23,839
recent models from Mostly open source

1155
00:39:21,040 --> 00:39:26,839
groups and this is model size and then

1156
00:39:23,839 --> 00:39:29,400
this is the accuracy on mathematics and

1157
00:39:26,839 --> 00:39:32,000
you can see that we are getting models

1158
00:39:29,400 --> 00:39:35,720
this model here is 1.5 billion

1159
00:39:32,000 --> 00:39:38,160
parameters it is matching V3 which is

1160
00:39:35,720 --> 00:39:40,240
680 billion parameters so this model

1161
00:39:38,160 --> 00:39:42,319
cost probably $100 million a train this

1162
00:39:40,240 --> 00:39:45,119
model cost about $500 a train and so

1163
00:39:42,319 --> 00:39:46,880
we're able to get this huge flattening

1164
00:39:45,119 --> 00:39:49,160
of these language models with reasoning

1165
00:39:46,880 --> 00:39:51,079
and you can see everything at the top

1166
00:39:49,160 --> 00:39:54,040
has reasoning and so reasoning is just

1167
00:39:51,079 --> 00:39:56,400
completely you know just destroyed our

1168
00:39:54,040 --> 00:39:58,200
predictive uh Power in this field these

1169
00:39:56,400 --> 00:40:00,160
quen models these qu models are like as

1170
00:39:58,200 --> 00:40:01,800
you increase the model size the

1171
00:40:00,160 --> 00:40:03,240
performance gets better right and even

1172
00:40:01,800 --> 00:40:04,400
it starts to diminish in returns as you

1173
00:40:03,240 --> 00:40:06,079
make them bigger and bigger these are

1174
00:40:04,400 --> 00:40:08,280
all trained in the same pattern

1175
00:40:06,079 --> 00:40:11,240
completely gone so now we have like a

1176
00:40:08,280 --> 00:40:13,040
real big you know disruption in how we

1177
00:40:11,240 --> 00:40:15,920
think about language

1178
00:40:13,040 --> 00:40:17,640
models and so the question you know for

1179
00:40:15,920 --> 00:40:18,839
for us at future house and I think many

1180
00:40:17,640 --> 00:40:20,920
people in the community is can we do

1181
00:40:18,839 --> 00:40:22,960
this in scientific reasoning because

1182
00:40:20,920 --> 00:40:24,640
it's only been shown for mathematics yet

1183
00:40:22,960 --> 00:40:26,560
but there's now this opportunity to try

1184
00:40:24,640 --> 00:40:28,240
to apply this kind of new I don't know

1185
00:40:26,560 --> 00:40:31,560
new economics of of language models in

1186
00:40:28,240 --> 00:40:33,200
scientific domains so here is some

1187
00:40:31,560 --> 00:40:36,240
output from a reasoning model we've been

1188
00:40:33,200 --> 00:40:37,880
training on chemistry so what happens is

1189
00:40:36,240 --> 00:40:39,560
that you still have these sort of like

1190
00:40:37,880 --> 00:40:41,319
chemistry specialized models where you

1191
00:40:39,560 --> 00:40:42,839
can give it molecules and predict the

1192
00:40:41,319 --> 00:40:44,560
property of it or give it a reaction

1193
00:40:42,839 --> 00:40:46,760
predicts the reaction outcome but now it

1194
00:40:44,560 --> 00:40:48,800
gives you a reasoning Trace about how it

1195
00:40:46,760 --> 00:40:50,119
came to the conclusion and so what

1196
00:40:48,800 --> 00:40:51,839
happens is we're building these domain

1197
00:40:50,119 --> 00:40:54,480
specific models that have these scaling

1198
00:40:51,839 --> 00:40:57,359
Properties by reasoning in a very long

1199
00:40:54,480 --> 00:40:59,240
thought in English

1200
00:40:57,359 --> 00:41:00,319
so this is uh one where it's just you

1201
00:40:59,240 --> 00:41:01,640
know this kind of a simple chemical

1202
00:41:00,319 --> 00:41:02,880
reaction but you can read through like

1203
00:41:01,640 --> 00:41:04,400
how it reasons to the reaction I think

1204
00:41:02,880 --> 00:41:05,920
it's very exciting as a scientist to be

1205
00:41:04,400 --> 00:41:08,000
like oh yeah I can see how it came to

1206
00:41:05,920 --> 00:41:09,640
the conclusion but also these reasoning

1207
00:41:08,000 --> 00:41:12,400
tokens improve

1208
00:41:09,640 --> 00:41:14,720
performance um I'm going to skip over

1209
00:41:12,400 --> 00:41:16,640
this plot here but basically the blue

1210
00:41:14,720 --> 00:41:17,880
one is is our reasoning model this is

1211
00:41:16,640 --> 00:41:20,560
very early this is like literally

1212
00:41:17,880 --> 00:41:22,040
trained for like uh like three days so

1213
00:41:20,560 --> 00:41:24,000
it's going to need to be cooking for a

1214
00:41:22,040 --> 00:41:25,440
little bit longer but the red is 03 mini

1215
00:41:24,000 --> 00:41:26,960
High which is sort of the best Frontier

1216
00:41:25,440 --> 00:41:28,520
Model out there and so we're beating it

1217
00:41:26,960 --> 00:41:29,800
a lot these different tasks I'll show

1218
00:41:28,520 --> 00:41:31,800
you guys some examples of tasks in a

1219
00:41:29,800 --> 00:41:33,599
moment we have human comparison for one

1220
00:41:31,800 --> 00:41:35,079
of these things and so we beat humans on

1221
00:41:33,599 --> 00:41:38,200
on predicting the outcome of a chemical

1222
00:41:35,079 --> 00:41:39,520
reaction um and uh it's very exciting

1223
00:41:38,200 --> 00:41:41,760
very exciting time for us future ask

1224
00:41:39,520 --> 00:41:43,000
right now so what the idea behind

1225
00:41:41,760 --> 00:41:44,640
reasoning models is that we don't

1226
00:41:43,000 --> 00:41:46,359
actually have to have a bunch of like

1227
00:41:44,640 --> 00:41:48,160
prompt completion like a bunch of work

1228
00:41:46,359 --> 00:41:50,520
through problems we need instead as

1229
00:41:48,160 --> 00:41:52,359
verifiable rewards so here's an example

1230
00:41:50,520 --> 00:41:53,720
of a verifiable reward propose a

1231
00:41:52,359 --> 00:41:55,560
modification to this molecule to

1232
00:41:53,720 --> 00:41:57,680
increase its solubility by about One log

1233
00:41:55,560 --> 00:42:00,599
s it's about like you know log s s is

1234
00:41:57,680 --> 00:42:02,359
the solil water without affecting its

1235
00:42:00,599 --> 00:42:03,599
scaffold so this is kind of like a

1236
00:42:02,359 --> 00:42:05,680
question that a medicinal chemist might

1237
00:42:03,599 --> 00:42:07,160
ask is like okay I need to like a adjust

1238
00:42:05,680 --> 00:42:08,359
the solubility molecule but I know it

1239
00:42:07,160 --> 00:42:09,920
has drug activity so I don't want to

1240
00:42:08,359 --> 00:42:11,040
adjust the scaffold so this is an

1241
00:42:09,920 --> 00:42:13,920
example of a question where there is no

1242
00:42:11,040 --> 00:42:15,839
one right answer but we can verify a

1243
00:42:13,920 --> 00:42:17,240
proposed answer and what's exciting

1244
00:42:15,839 --> 00:42:18,640
about this is this kind of represents a

1245
00:42:17,240 --> 00:42:20,400
lot of the tasks we do in biology or a

1246
00:42:18,640 --> 00:42:21,839
lot of tasks we do in chemistry is like

1247
00:42:20,400 --> 00:42:22,960
not often there's like one solution but

1248
00:42:21,839 --> 00:42:24,040
we might be able to characterize a

1249
00:42:22,960 --> 00:42:27,200
proposed

1250
00:42:24,040 --> 00:42:28,960
solution and it's also possible of doing

1251
00:42:27,200 --> 00:42:30,559
new tasks that was just like never

1252
00:42:28,960 --> 00:42:32,760
possible before so there's an example of

1253
00:42:30,559 --> 00:42:34,319
a task which is totally insane it

1254
00:42:32,760 --> 00:42:35,680
shouldn't work okay I'll read you the

1255
00:42:34,319 --> 00:42:39,559
question the reasoning is kind of

1256
00:42:35,680 --> 00:42:43,480
detailed but it says um a a c17

1257
00:42:39,559 --> 00:42:44,880
h157 metabolite from lais lolus was

1258
00:42:43,480 --> 00:42:48,520
identified what's a biologically

1259
00:42:44,880 --> 00:42:50,280
plausible compound for this as Smiles so

1260
00:42:48,520 --> 00:42:52,200
this is the molecular formula you can

1261
00:42:50,280 --> 00:42:54,319
get molecular formulas Dirt Cheap you

1262
00:42:52,200 --> 00:42:56,319
get these from MPC there's you know

1263
00:42:54,319 --> 00:42:58,200
probably millions of these formulas of

1264
00:42:56,319 --> 00:42:59,400
of things extracted organisms finding

1265
00:42:58,200 --> 00:43:00,880
the molecular structure is actually very

1266
00:42:59,400 --> 00:43:03,280
hard and it requires like an expert to

1267
00:43:00,880 --> 00:43:05,240
look at NMR data it takes a long time

1268
00:43:03,280 --> 00:43:07,440
here the model reasons through like

1269
00:43:05,240 --> 00:43:09,359
knowing about the metabolism knowing you

1270
00:43:07,440 --> 00:43:11,240
know this kind of organism class what

1271
00:43:09,359 --> 00:43:13,960
are possible structures that could fit

1272
00:43:11,240 --> 00:43:15,800
this formula we gave this task to humans

1273
00:43:13,960 --> 00:43:17,599
and they were just like this don't be

1274
00:43:15,800 --> 00:43:19,359
silly no one can do this right basically

1275
00:43:17,599 --> 00:43:20,319
it's it's really hard as a human to sit

1276
00:43:19,359 --> 00:43:22,079
there and reason about all this while

1277
00:43:20,319 --> 00:43:24,680
building up a a molecule that has the

1278
00:43:22,079 --> 00:43:26,240
exact stomry of what you want and this

1279
00:43:24,680 --> 00:43:28,160
is the reason very interesting to to

1280
00:43:26,240 --> 00:43:30,079
read through it basically it comes to

1281
00:43:28,160 --> 00:43:32,359
the conclusion about what is the right

1282
00:43:30,079 --> 00:43:34,359
structure and this is one of these tasks

1283
00:43:32,359 --> 00:43:36,760
where when you see the training curve if

1284
00:43:34,359 --> 00:43:38,520
the model gets z% 0% 0% after like

1285
00:43:36,760 --> 00:43:39,640
whatever 12 hours no Frontier Model gets

1286
00:43:38,520 --> 00:43:41,599
anything nonzero and all of a sudden

1287
00:43:39,640 --> 00:43:44,200
starts to be able to do this task it's

1288
00:43:41,599 --> 00:43:46,079
super exciting

1289
00:43:44,200 --> 00:43:48,480
question I can just repeat your question

1290
00:43:46,079 --> 00:43:51,319
if it's short oh there you

1291
00:43:48,480 --> 00:43:53,160
go I've uh worked with people before who

1292
00:43:51,319 --> 00:43:55,800
have actually been trying to use you

1293
00:43:53,160 --> 00:43:58,040
know who found it difficult to get

1294
00:43:55,800 --> 00:44:01,200
actually maybe this last year maybe the

1295
00:43:58,040 --> 00:44:04,680
world is different now yeah um uh have

1296
00:44:01,200 --> 00:44:08,200
found it difficult for to produ get LMS

1297
00:44:04,680 --> 00:44:11,040
to produce Smiles that are actually like

1298
00:44:08,200 --> 00:44:15,559
syntactically correct yes so let me show

1299
00:44:11,040 --> 00:44:17,680
you this this right here this is can you

1300
00:44:15,559 --> 00:44:19,680
make a molecule where I've completed

1301
00:44:17,680 --> 00:44:21,079
half of it okay so I have a molecule

1302
00:44:19,680 --> 00:44:22,440
I've completed some fraction of it can

1303
00:44:21,079 --> 00:44:25,040
you complete it and make it sure it's

1304
00:44:22,440 --> 00:44:27,599
valid okay so it gets 96% of this task

1305
00:44:25,040 --> 00:44:30,520
in the Frontier Model o03 mini gets % on

1306
00:44:27,599 --> 00:44:33,200
this task and so you're right now to be

1307
00:44:30,520 --> 00:44:34,760
to be fair like if you do O3 mini on

1308
00:44:33,200 --> 00:44:36,160
like a small molecule or like you know

1309
00:44:34,760 --> 00:44:37,760
something simple like what's the smiles

1310
00:44:36,160 --> 00:44:39,720
of arginine no problem but these are all

1311
00:44:37,760 --> 00:44:41,040
very large like natural products and

1312
00:44:39,720 --> 00:44:42,760
they're very hard to close all the Rings

1313
00:44:41,040 --> 00:44:45,079
and satisfy the valencies so is this

1314
00:44:42,760 --> 00:44:46,280
something where this is like a sub one

1315
00:44:45,079 --> 00:44:47,800
of these subm modules that you were

1316
00:44:46,280 --> 00:44:51,280
talking about before where you can you

1317
00:44:47,800 --> 00:44:54,000
can call a no this is a completely it's

1318
00:44:51,280 --> 00:44:55,760
just a one language model one purpose

1319
00:44:54,000 --> 00:44:58,680
built reasoning language model only for

1320
00:44:55,760 --> 00:45:00,400
these tasks so no tools involved here

1321
00:44:58,680 --> 00:45:01,960
very exciting and it's also small enough

1322
00:45:00,400 --> 00:45:03,440
that it can run on a laptop so I think

1323
00:45:01,960 --> 00:45:05,040
when we get this model finished people

1324
00:45:03,440 --> 00:45:06,880
can find tune it on whatever chemistry

1325
00:45:05,040 --> 00:45:08,400
domain tasks they want and I think we're

1326
00:45:06,880 --> 00:45:11,160
going to set up a recipe that people can

1327
00:45:08,400 --> 00:45:14,640
repeat for you know protein function for

1328
00:45:11,160 --> 00:45:16,960
enzyme function for genome

1329
00:45:14,640 --> 00:45:19,200
models awesome so I think this is you

1330
00:45:16,960 --> 00:45:21,200
know an example of like just one of the

1331
00:45:19,200 --> 00:45:22,960
things that's been I think one more

1332
00:45:21,200 --> 00:45:25,640
slide yeah so one of the things that's

1333
00:45:22,960 --> 00:45:27,480
been difficult are we out of time oh

1334
00:45:25,640 --> 00:45:29,520
okay sorry one more question

1335
00:45:27,480 --> 00:45:31,839
um I think what might be interesting

1336
00:45:29,520 --> 00:45:34,480
with this um and really show its power

1337
00:45:31,839 --> 00:45:36,800
is um sort of like zero shot is kind of

1338
00:45:34,480 --> 00:45:38,319
cool um so like with EVO right and they

1339
00:45:36,800 --> 00:45:40,920
have the cast system do you know how

1340
00:45:38,319 --> 00:45:44,200
many samples they drew uh I think they

1341
00:45:40,920 --> 00:45:46,920
drew 1,00 two million they drew two

1342
00:45:44,200 --> 00:45:48,400
million cast yeah to do the the invivo

1343
00:45:46,920 --> 00:45:49,720
validation right so they drew two

1344
00:45:48,400 --> 00:45:52,920
million of them oh they sampled two

1345
00:45:49,720 --> 00:45:55,760
million yeah and then and then they did

1346
00:45:52,920 --> 00:45:57,960
U alignment and all these things then

1347
00:45:55,760 --> 00:45:59,319
they got 11 but then only worked yeah

1348
00:45:57,960 --> 00:46:00,880
because they have 11 you know that they

1349
00:45:59,319 --> 00:46:02,839
didn't initially pick 11 they probably

1350
00:46:00,880 --> 00:46:04,680
picked 10 the 10 didn't work and they

1351
00:46:02,839 --> 00:46:09,040
picked another one because no one picks

1352
00:46:04,680 --> 00:46:10,720
11 so I think so I I think one and you

1353
00:46:09,040 --> 00:46:13,200
know it was called zero shot but

1354
00:46:10,720 --> 00:46:14,880
whatever um so I think this is kind of

1355
00:46:13,200 --> 00:46:16,800
cool if you could

1356
00:46:14,880 --> 00:46:19,559
demonstrate chemistry just to match them

1357
00:46:16,800 --> 00:46:21,559
and show no it's like truly zero shot I

1358
00:46:19,559 --> 00:46:22,720
know sorry it's a bad question it's but

1359
00:46:21,559 --> 00:46:24,760
I think that that's kind of cool

1360
00:46:22,720 --> 00:46:27,640
actually if you can do truly zero

1361
00:46:24,760 --> 00:46:28,839
shot um yeah so

1362
00:46:27,640 --> 00:46:30,640
I think I think you're on something

1363
00:46:28,839 --> 00:46:33,319
there but I do want to say is that if

1364
00:46:30,640 --> 00:46:36,480
for me if I'm given two models one model

1365
00:46:33,319 --> 00:46:39,440
is really good accuracy and one model

1366
00:46:36,480 --> 00:46:40,880
gives me a long like page reasoning

1367
00:46:39,440 --> 00:46:42,680
description in English about how it came

1368
00:46:40,880 --> 00:46:45,160
to the conclusion I would rather have

1369
00:46:42,680 --> 00:46:46,760
that one and it really is very

1370
00:46:45,160 --> 00:46:48,319
interesting to see the conclusion what's

1371
00:46:46,760 --> 00:46:49,760
really nice about the the reasoning is

1372
00:46:48,319 --> 00:46:51,200
you can see where it makes mistakes

1373
00:46:49,760 --> 00:46:52,800
right or like maybe it doesn't get it

1374
00:46:51,200 --> 00:46:55,400
right and you can say go back but assume

1375
00:46:52,800 --> 00:46:58,119
this right I think the reasoning itself

1376
00:46:55,400 --> 00:47:00,240
I think is just so powerful as a

1377
00:46:58,119 --> 00:47:02,760
scientist because these models that are

1378
00:47:00,240 --> 00:47:03,839
put out like like Evo 2 is great and I

1379
00:47:02,760 --> 00:47:05,200
think um there's been some great

1380
00:47:03,839 --> 00:47:06,520
chemistry models out there there's some

1381
00:47:05,200 --> 00:47:07,960
good you know protein there protein

1382
00:47:06,520 --> 00:47:09,119
structure prediction models but imagine

1383
00:47:07,960 --> 00:47:10,240
if you went to Alpha fold and you asked

1384
00:47:09,119 --> 00:47:11,760
to fold something and it gave a bunch of

1385
00:47:10,240 --> 00:47:13,079
reasoning about why this fold goes here

1386
00:47:11,760 --> 00:47:14,599
why this fold goes here right or why it

1387
00:47:13,079 --> 00:47:16,839
thinks this should be Helix that would

1388
00:47:14,599 --> 00:47:18,280
be Monumental right I think that's one

1389
00:47:16,839 --> 00:47:19,760
of the missing pieces for all of this

1390
00:47:18,280 --> 00:47:22,880
stuff is like okay great you can do

1391
00:47:19,760 --> 00:47:24,640
blackbox accuracy but explain why and so

1392
00:47:22,880 --> 00:47:27,119
I I think this model even if it maybe

1393
00:47:24,640 --> 00:47:28,559
it's not as good as the the best model

1394
00:47:27,119 --> 00:47:29,720
in some of these tasks maybe there's

1395
00:47:28,559 --> 00:47:31,440
going to be there's probably a better

1396
00:47:29,720 --> 00:47:33,200
model that's out there for some of these

1397
00:47:31,440 --> 00:47:34,920
things the fact that it reasons in

1398
00:47:33,200 --> 00:47:36,960
English and that it can do all the tasks

1399
00:47:34,920 --> 00:47:38,520
right you can combine 15 different tasks

1400
00:47:36,960 --> 00:47:40,040
in here make sure the solubility is here

1401
00:47:38,520 --> 00:47:42,160
make sure it's not carcinogenic make

1402
00:47:40,040 --> 00:47:45,160
sure it's commercially synthesizable in

1403
00:47:42,160 --> 00:47:46,640
one step it's really exciting stuff all

1404
00:47:45,160 --> 00:47:48,400
right I will just mention that we

1405
00:47:46,640 --> 00:47:50,359
actually gave the reasoning traces to

1406
00:47:48,400 --> 00:47:52,040
Medicinal chemists that are in the

1407
00:47:50,359 --> 00:47:53,240
pharmaceutical industry we don't know

1408
00:47:52,040 --> 00:47:55,480
how to die with these things so we've

1409
00:47:53,240 --> 00:47:57,319
did way too many like rubri too big of a

1410
00:47:55,480 --> 00:47:58,960
rubric but we checked to see like is the

1411
00:47:57,319 --> 00:48:00,760
reasoning relevant to the question like

1412
00:47:58,960 --> 00:48:02,520
does it go from the you know prompt to

1413
00:48:00,760 --> 00:48:04,319
the end is it faithful are there any

1414
00:48:02,520 --> 00:48:05,640
like reasoning jumps or it just ends

1415
00:48:04,319 --> 00:48:08,319
halfway or doesn't go all the way to the

1416
00:48:05,640 --> 00:48:10,880
end um how much is it like a human

1417
00:48:08,319 --> 00:48:12,520
reasoning uh is it make any mistakes

1418
00:48:10,880 --> 00:48:13,920
like does it hallucinate a structure is

1419
00:48:12,520 --> 00:48:16,760
it propos something that's not the right

1420
00:48:13,920 --> 00:48:17,920
name of a structure um does it explain

1421
00:48:16,760 --> 00:48:19,599
it I actually don't know there's between

1422
00:48:17,920 --> 00:48:21,559
this and humanness but I'm not sure and

1423
00:48:19,599 --> 00:48:24,240
then readability is like uh are all the

1424
00:48:21,559 --> 00:48:25,559
smiles valid or things like that so

1425
00:48:24,240 --> 00:48:27,359
again we don't have a baseline we're

1426
00:48:25,559 --> 00:48:28,559
figuring that out but the medicinal

1427
00:48:27,359 --> 00:48:30,319
chemists were all very impressed and

1428
00:48:28,559 --> 00:48:31,599
thought it was a really interesting

1429
00:48:30,319 --> 00:48:33,400
useful tool to be able to read the

1430
00:48:31,599 --> 00:48:36,000
reasoning in these

1431
00:48:33,400 --> 00:48:38,839
predictions awesome so we will release

1432
00:48:36,000 --> 00:48:40,359
the model weight soon um and a technical

1433
00:48:38,839 --> 00:48:41,440
report but I think it's just such an

1434
00:48:40,359 --> 00:48:43,839
exciting breakthrough and I think it's

1435
00:48:41,440 --> 00:48:44,800
just going to like this topic of being

1436
00:48:43,839 --> 00:48:47,200
able to build these models like we're

1437
00:48:44,800 --> 00:48:48,920
talking 50,000 verifiable rewards so I

1438
00:48:47,200 --> 00:48:50,599
think in the past it was like okay you

1439
00:48:48,920 --> 00:48:52,200
need to bring me 50 billion tokens for

1440
00:48:50,599 --> 00:48:53,920
me to train a model in your domain now

1441
00:48:52,200 --> 00:48:55,520
you need to bring me 50,000 verifiable

1442
00:48:53,920 --> 00:48:57,680
rewards which I think is very achievable

1443
00:48:55,520 --> 00:49:01,160
by many different groups

1444
00:48:57,680 --> 00:49:04,040
awesome uh yada yada yada uh I will just

1445
00:49:01,160 --> 00:49:05,599
mention the last thing is um we actually

1446
00:49:04,040 --> 00:49:08,280
I think have made a lot of really great

1447
00:49:05,599 --> 00:49:10,880
scientific agents like cloning Crow um

1448
00:49:08,280 --> 00:49:12,680
like protein crow like uh paper QA we're

1449
00:49:10,880 --> 00:49:14,559
going to be releasing a platform soon

1450
00:49:12,680 --> 00:49:16,200
where you can go and by soon I mean like

1451
00:49:14,559 --> 00:49:17,480
within weeks where you can go and use

1452
00:49:16,200 --> 00:49:19,359
these things yourself and it will have

1453
00:49:17,480 --> 00:49:20,880
an API so if you want to like use these

1454
00:49:19,359 --> 00:49:23,079
things as Tools in your own workflows

1455
00:49:20,880 --> 00:49:25,359
you can call the API or you can use them

1456
00:49:23,079 --> 00:49:28,000
and we have a super cool design design

1457
00:49:25,359 --> 00:49:29,520
is so cool it's like driving a spaceship

1458
00:49:28,000 --> 00:49:30,520
I don't have any pictures of it but it's

1459
00:49:29,520 --> 00:49:33,960
going to be

1460
00:49:30,520 --> 00:49:36,920
great um cool so uh obviously huge team

1461
00:49:33,960 --> 00:49:38,880
here Sam Cox uh is g to give a talk next

1462
00:49:36,920 --> 00:49:40,240
about some of this work Mike skinsky uh

1463
00:49:38,880 --> 00:49:43,559
led a lot of the paper QA engineering

1464
00:49:40,240 --> 00:49:46,040
work um and then Manu God I hope you're

1465
00:49:43,559 --> 00:49:49,160
still on here Manu oh

1466
00:49:46,040 --> 00:49:50,880
no were you on here ever okay we gotta

1467
00:49:49,160 --> 00:49:54,720
get manu's picture Manu did did the work

1468
00:49:50,880 --> 00:49:57,200
on protein Pro um and then uh Sam is the

1469
00:49:54,720 --> 00:49:58,880
uh director of future house and um yeah

1470
00:49:57,200 --> 00:50:00,359
we have big team contributing to this

1471
00:49:58,880 --> 00:50:03,440
and uh with that I'll take any

1472
00:50:00,359 --> 00:50:03,440
additional questions and thanks for your

1473
00:50:04,839 --> 00:50:09,200
attention thank you so uh yeah I'm Sam

1474
00:50:07,880 --> 00:50:10,680
I'm at teacher house and also at

1475
00:50:09,200 --> 00:50:12,920
University of Rochester and I'm gonna be

1476
00:50:10,680 --> 00:50:15,040
talking about work and happened in both

1477
00:50:12,920 --> 00:50:18,119
of those places um on automating

1478
00:50:15,040 --> 00:50:20,680
chemistry workflows with llm based

1479
00:50:18,119 --> 00:50:22,720
agents um I'll start with Kim cro this

1480
00:50:20,680 --> 00:50:24,240
work is kind of old now so I know that

1481
00:50:22,720 --> 00:50:25,000
all of you have read it and studied it

1482
00:50:24,240 --> 00:50:28,200
very

1483
00:50:25,000 --> 00:50:29,559
deeply uh but this is kind of silly but

1484
00:50:28,200 --> 00:50:31,280
um when I was starting this project I

1485
00:50:29,559 --> 00:50:33,359
was like I don't really know how good

1486
00:50:31,280 --> 00:50:35,559
llms are at these things I don't have a

1487
00:50:33,359 --> 00:50:36,839
good intuition for this um but LMS

1488
00:50:35,559 --> 00:50:39,480
probably do because they're very

1489
00:50:36,839 --> 00:50:41,520
self-aware so I asked um what are you

1490
00:50:39,480 --> 00:50:43,160
good at and what are you bad at when it

1491
00:50:41,520 --> 00:50:45,640
comes to science so they're very good at

1492
00:50:43,160 --> 00:50:47,280
doing like basic fact retrieval um very

1493
00:50:45,640 --> 00:50:49,920
simple problem solving and they have a

1494
00:50:47,280 --> 00:50:52,119
high level understanding um but they're

1495
00:50:49,920 --> 00:50:54,559
really bad at like obviously any updates

1496
00:50:52,119 --> 00:50:56,240
so if you do any computational work or

1497
00:50:54,559 --> 00:50:57,760
if you need to reference any literature

1498
00:50:56,240 --> 00:51:00,079
it can't really have it doesn't have

1499
00:50:57,760 --> 00:51:02,640
updated information um it's really bad

1500
00:51:00,079 --> 00:51:05,200
at Advanced problem solving and also it

1501
00:51:02,640 --> 00:51:07,520
tends to make errors very

1502
00:51:05,200 --> 00:51:08,839
confidently so when we're thinking about

1503
00:51:07,520 --> 00:51:09,880
like trying to implement these when we

1504
00:51:08,839 --> 00:51:12,599
were thinking about trying to implement

1505
00:51:09,880 --> 00:51:14,400
these into uh chemistry workflows um we

1506
00:51:12,599 --> 00:51:17,240
started thinking about

1507
00:51:14,400 --> 00:51:19,000
agents so kcro follows a chain of

1508
00:51:17,240 --> 00:51:20,680
thought process where you give it a

1509
00:51:19,000 --> 00:51:22,480
bunch of tools and prompts the tools are

1510
00:51:20,680 --> 00:51:24,799
just like python functions like convert

1511
00:51:22,480 --> 00:51:27,520
selfies to Smiles or convert the name or

1512
00:51:24,799 --> 00:51:30,119
check uh look look up the price plan the

1513
00:51:27,520 --> 00:51:31,720
synthesis and it thinks okay what do I

1514
00:51:30,119 --> 00:51:34,119
need to do what have I done where am I

1515
00:51:31,720 --> 00:51:35,880
at in this process um which of my tools

1516
00:51:34,119 --> 00:51:38,559
is best suited for the subtask that I

1517
00:51:35,880 --> 00:51:40,400
need to do um what should my input be

1518
00:51:38,559 --> 00:51:41,599
and then it uses the tool and decides

1519
00:51:40,400 --> 00:51:43,160
and tries to figure out what the tool

1520
00:51:41,599 --> 00:51:46,079
told me and where do I go from here so

1521
00:51:43,160 --> 00:51:50,200
it's able to iterate and like um react

1522
00:51:46,079 --> 00:51:53,000
to failures and do like a react

1523
00:51:50,200 --> 00:51:55,520
process when we were developing kcro um

1524
00:51:53,000 --> 00:51:56,960
the way that we thought about this is um

1525
00:51:55,520 --> 00:51:59,119
we really wanted to kind of make it an

1526
00:51:56,960 --> 00:52:00,680
assistant where it like completes um

1527
00:51:59,119 --> 00:52:02,400
chemistry tasks so we thought what kind

1528
00:52:00,680 --> 00:52:04,440
of questions do chemists even want to

1529
00:52:02,400 --> 00:52:05,799
answer um what tools are needed to

1530
00:52:04,440 --> 00:52:07,079
answer these questions and then where

1531
00:52:05,799 --> 00:52:08,839
does the agent fall short because when

1532
00:52:07,079 --> 00:52:10,040
you're developing an agent it's only as

1533
00:52:08,839 --> 00:52:11,880
good as the tools that you give it and

1534
00:52:10,040 --> 00:52:15,200
it can only do what the like the tools

1535
00:52:11,880 --> 00:52:17,480
that you give it and so we ended up

1536
00:52:15,200 --> 00:52:19,680
giving um all of these tools here where

1537
00:52:17,480 --> 00:52:21,880
we have search tools like paper QA like

1538
00:52:19,680 --> 00:52:24,040
um Andrew mentioned web search a bunch

1539
00:52:21,880 --> 00:52:26,280
of conversion molecular tools synthesis

1540
00:52:24,040 --> 00:52:29,000
tools safety and then some basic link

1541
00:52:26,280 --> 00:52:30,640
Chang tools like um man I don't know let

1542
00:52:29,000 --> 00:52:32,839
me ask the human or let me try to run

1543
00:52:30,640 --> 00:52:35,160
some python

1544
00:52:32,839 --> 00:52:36,960
code um the thing that's really

1545
00:52:35,160 --> 00:52:38,640
difficult that um we all probably know

1546
00:52:36,960 --> 00:52:41,000
and Andrew also mentioned is how in the

1547
00:52:38,640 --> 00:52:42,200
world do you even evaluate these things

1548
00:52:41,000 --> 00:52:45,720
uh the first method which is really

1549
00:52:42,200 --> 00:52:47,200
popular at the time is just asking gp4

1550
00:52:45,720 --> 00:52:49,480
um hey you're in the role of a chemistry

1551
00:52:47,200 --> 00:52:52,839
teacher evaluating two students one is

1552
00:52:49,480 --> 00:52:54,400
gp4 and one is Kim Crow your task is to

1553
00:52:52,839 --> 00:52:56,520
basically give them like a review

1554
00:52:54,400 --> 00:52:59,440
highlight the strengths and weaknesses

1555
00:52:56,520 --> 00:53:01,240
and then also follow give it some grade

1556
00:52:59,440 --> 00:53:03,480
um this is a really unbiased method um

1557
00:53:01,240 --> 00:53:06,280
all teachers are very unbiased grading

1558
00:53:03,480 --> 00:53:08,200
students um but we found that gp4 can't

1559
00:53:06,280 --> 00:53:09,839
evaluate the responses that it can't

1560
00:53:08,200 --> 00:53:11,960
answer so if you ask it a really hard

1561
00:53:09,839 --> 00:53:13,960
question and it gives you a nonsense

1562
00:53:11,960 --> 00:53:16,240
answer it doesn't have the knowledge

1563
00:53:13,960 --> 00:53:19,200
obviously to evaluate it and so it ends

1564
00:53:16,240 --> 00:53:20,520
up um grading based on it like tells you

1565
00:53:19,200 --> 00:53:21,960
that it's really good chemistry but

1566
00:53:20,520 --> 00:53:24,680
actually it just thinks that it like

1567
00:53:21,960 --> 00:53:28,079
flows very nicely or the grammar is very

1568
00:53:24,680 --> 00:53:30,119
nice and it evaluates kind of nonsense

1569
00:53:28,079 --> 00:53:31,799
so we needed another type of evaluation

1570
00:53:30,119 --> 00:53:33,319
and we Ed humans so we gave a team of

1571
00:53:31,799 --> 00:53:35,839
chemists and chemical Engineers the

1572
00:53:33,319 --> 00:53:39,079
questions the question and then both uh

1573
00:53:35,839 --> 00:53:39,079
final answers formatted

1574
00:53:39,240 --> 00:53:44,720
identically and so what we're seeing

1575
00:53:41,160 --> 00:53:48,359
here is on the left we have the um delta

1576
00:53:44,720 --> 00:53:50,280
mean expert scores for gb4 and chro and

1577
00:53:48,359 --> 00:53:51,920
in each category we have increasing

1578
00:53:50,280 --> 00:53:54,520
difficulty going down so for organic

1579
00:53:51,920 --> 00:53:56,280
synthesis task Tas 13 is much more

1580
00:53:54,520 --> 00:53:57,880
difficult than task seven where

1581
00:53:56,280 --> 00:54:00,319
difficult ulty is like the complexity of

1582
00:53:57,880 --> 00:54:03,160
the task the number of steps required

1583
00:54:00,319 --> 00:54:04,720
and we see that in like dp24 is not bad

1584
00:54:03,160 --> 00:54:06,880
it's able to complete some of the tasks

1585
00:54:04,720 --> 00:54:10,319
humans actually prefer it on like 14 and

1586
00:54:06,880 --> 00:54:13,079
four and two and 10 um but as the tasks

1587
00:54:10,319 --> 00:54:15,680
get more complex it really drops off um

1588
00:54:13,079 --> 00:54:18,760
we see that in um both organic synthesis

1589
00:54:15,680 --> 00:54:20,240
and chemical logic and knowledge um this

1590
00:54:18,760 --> 00:54:23,000
is not surprising I don't know if you've

1591
00:54:20,240 --> 00:54:25,640
ever tried to ask like gp4 to plan and

1592
00:54:23,000 --> 00:54:26,960
execute a synthesis it can't do that so

1593
00:54:25,640 --> 00:54:30,280
this is not super

1594
00:54:26,960 --> 00:54:32,799
surprising um and so we found that uh

1595
00:54:30,280 --> 00:54:34,480
gb4 gives complete responses which is

1596
00:54:32,799 --> 00:54:36,200
great but it has a lot of

1597
00:54:34,480 --> 00:54:38,599
hallucinations um that are very

1598
00:54:36,200 --> 00:54:40,440
believable in some cases it's hard to

1599
00:54:38,599 --> 00:54:42,000
interpret so it doesn't tell you like

1600
00:54:40,440 --> 00:54:43,960
how it thought about it necessarily it

1601
00:54:42,000 --> 00:54:45,440
just gives you an answer um and it

1602
00:54:43,960 --> 00:54:46,720
doesn't have access to any up-to-date

1603
00:54:45,440 --> 00:54:49,880
information so some of these would

1604
00:54:46,720 --> 00:54:51,640
require like reading a specific paper or

1605
00:54:49,880 --> 00:54:53,160
running like a python package that it

1606
00:54:51,640 --> 00:54:55,680
just wasn't able to

1607
00:54:53,160 --> 00:54:58,280
do uh whereas Kim cro with Kim cro we

1608
00:54:55,680 --> 00:54:59,480
found chemically accurate Solutions um

1609
00:54:58,280 --> 00:55:01,559
it's great because it's modular and

1610
00:54:59,480 --> 00:55:04,040
extensible so all a like all agents so

1611
00:55:01,559 --> 00:55:05,920
if you provide like you decided hey I

1612
00:55:04,040 --> 00:55:07,040
really want to do protein design you can

1613
00:55:05,920 --> 00:55:11,079
provide it with some protein design

1614
00:55:07,040 --> 00:55:12,799
tools to do that um although chemistry

1615
00:55:11,079 --> 00:55:15,400
is easier and then um it still

1616
00:55:12,799 --> 00:55:17,000
occasionally has F flawed conclusions

1617
00:55:15,400 --> 00:55:19,160
because it's limited by the llm right

1618
00:55:17,000 --> 00:55:22,920
like it still is going to make mistakes

1619
00:55:19,160 --> 00:55:22,920
um and it's very limited by the tools

1620
00:55:23,000 --> 00:55:28,039
quality um but one of the coolest things

1621
00:55:25,000 --> 00:55:29,920
we did with kimc is um we gave some

1622
00:55:28,039 --> 00:55:31,640
chromophor data and we said gave it some

1623
00:55:29,920 --> 00:55:34,480
instructions all the way on the top left

1624
00:55:31,640 --> 00:55:36,640
clean the data um use only the data with

1625
00:55:34,480 --> 00:55:38,680
a specific solvent So Pro uh filter it

1626
00:55:36,640 --> 00:55:40,799
process it train a random Forest machine

1627
00:55:38,680 --> 00:55:43,760
learning model make predictions and then

1628
00:55:40,799 --> 00:55:46,440
suggest a synthetic plan for one uh one

1629
00:55:43,760 --> 00:55:49,200
with the wavelength closest to 369

1630
00:55:46,440 --> 00:55:51,119
nanometers um it did everything that we

1631
00:55:49,200 --> 00:55:52,920
said and we get a final answer and then

1632
00:55:51,119 --> 00:55:54,839
humans actually synthesized the proposed

1633
00:55:52,920 --> 00:55:56,640
molecule and we had it sitting in the

1634
00:55:54,839 --> 00:55:58,920
lab and it followed all of the

1635
00:55:56,640 --> 00:56:01,119
instructions um so we actually able to

1636
00:55:58,920 --> 00:56:03,440
synthesize like a novel chromophor with

1637
00:56:01,119 --> 00:56:05,880
kcro where the only human intervention

1638
00:56:03,440 --> 00:56:10,359
is giving it the data telling it to do

1639
00:56:05,880 --> 00:56:10,359
it and then testing it in the lab

1640
00:56:11,520 --> 00:56:16,680
um so with Kim cro we found that LMS can

1641
00:56:14,880 --> 00:56:19,119
use tools to complete chemistry tasks

1642
00:56:16,680 --> 00:56:21,079
which is not I think like a super

1643
00:56:19,119 --> 00:56:22,880
trivial thing um and it can also

1644
00:56:21,079 --> 00:56:24,200
complete novel Discovery and design so

1645
00:56:22,880 --> 00:56:26,720
we can do novel things with it which

1646
00:56:24,200 --> 00:56:28,079
llms can't do on their own um but we

1647
00:56:26,720 --> 00:56:30,760
also found that agents are limited by

1648
00:56:28,079 --> 00:56:32,799
Tool quality and I didn't go into this

1649
00:56:30,760 --> 00:56:34,000
but you can read the paper uh safety is

1650
00:56:32,799 --> 00:56:36,760
a really important concern when you

1651
00:56:34,000 --> 00:56:39,000
start giving llms the ability to do

1652
00:56:36,760 --> 00:56:43,520
chemistry and um like hook it up to a

1653
00:56:39,000 --> 00:56:43,520
lab obviously um so that's a big

1654
00:56:43,680 --> 00:56:47,680
consideration so the next thing we

1655
00:56:45,480 --> 00:56:49,760
started thinking about is like okay Chim

1656
00:56:47,680 --> 00:56:51,799
informatics is cool and super difficult

1657
00:56:49,760 --> 00:56:53,480
but what if we wanted to do like a

1658
00:56:51,799 --> 00:56:55,599
really complex task that maybe runs for

1659
00:56:53,480 --> 00:56:58,440
a really long time or has to handle tons

1660
00:56:55,599 --> 00:57:00,119
of files or has to do um interact with

1661
00:56:58,440 --> 00:57:02,400
like multiple packages that are very

1662
00:57:00,119 --> 00:57:04,200
heavy and so we started thinking about

1663
00:57:02,400 --> 00:57:06,920
automating molecular Dynamics workflows

1664
00:57:04,200 --> 00:57:09,920
with llms

1665
00:57:06,920 --> 00:57:11,799
also um so the way that this works

1666
00:57:09,920 --> 00:57:14,880
similarly is it follows like this react

1667
00:57:11,799 --> 00:57:16,079
framework where it thinks acts observes

1668
00:57:14,880 --> 00:57:18,520
and then it decides if it needs to

1669
00:57:16,079 --> 00:57:20,599
complete the next sub task or react and

1670
00:57:18,520 --> 00:57:23,200
troubleshoot um with this one

1671
00:57:20,599 --> 00:57:24,640
specifically compared to Kim cro that

1672
00:57:23,200 --> 00:57:27,160
reactive troubleshoot is really

1673
00:57:24,640 --> 00:57:29,440
important because um MD is pretty

1674
00:57:27,160 --> 00:57:31,400
finicky and so it has to react and like

1675
00:57:29,440 --> 00:57:36,200
update its parameters and change things

1676
00:57:31,400 --> 00:57:38,599
as it goes um this one had or MD MD Crow

1677
00:57:36,200 --> 00:57:40,520
has currently about 45 tools um

1678
00:57:38,599 --> 00:57:43,039
including again paper QA for literature

1679
00:57:40,520 --> 00:57:44,799
search and then using um databases like

1680
00:57:43,039 --> 00:57:46,720
unipro and that sort of thing to get

1681
00:57:44,799 --> 00:57:49,000
upto-date information downloading pdb

1682
00:57:46,720 --> 00:57:52,720
files and then also interfacing with the

1683
00:57:49,000 --> 00:57:54,799
pdb files um simulating with openmm

1684
00:57:52,720 --> 00:57:57,480
using Paco and then also some analysis

1685
00:57:54,799 --> 00:57:59,720
tools using um MD trash all of the

1686
00:57:57,480 --> 00:58:03,240
things that we implemented were in MD um

1687
00:57:59,720 --> 00:58:04,760
open mm and MD trash because it was easy

1688
00:58:03,240 --> 00:58:06,520
to evaluate if it's all in one package

1689
00:58:04,760 --> 00:58:08,640
but again these are modular modular so

1690
00:58:06,520 --> 00:58:10,720
if you wanted to do growmax you could

1691
00:58:08,640 --> 00:58:15,319
implement it into gromax but that would

1692
00:58:10,720 --> 00:58:17,640
be more difficult um we did a series of

1693
00:58:15,319 --> 00:58:19,680
uh 25 tasks ranging from one to 10

1694
00:58:17,640 --> 00:58:24,319
subtasks where one might just be

1695
00:58:19,680 --> 00:58:27,520
download the pdb file for protein IL YZ

1696
00:58:24,319 --> 00:58:29,839
IL YZ um and then a 10-step one also is

1697
00:58:27,520 --> 00:58:30,839
shown here where we have to simulate at

1698
00:58:29,839 --> 00:58:32,799
two different

1699
00:58:30,839 --> 00:58:34,480
temperatures we have to do a bunch of

1700
00:58:32,799 --> 00:58:37,599
analyses and then we have to look at

1701
00:58:34,480 --> 00:58:37,599
before and after and do some

1702
00:58:38,440 --> 00:58:42,480
comparison okay so what we found in this

1703
00:58:41,240 --> 00:58:44,160
uh this project which is super

1704
00:58:42,480 --> 00:58:46,400
interesting is if you look all the way

1705
00:58:44,160 --> 00:58:48,240
on the left we have accuracy which um

1706
00:58:46,400 --> 00:58:51,280
were just uh evaluated by humans if it

1707
00:58:48,240 --> 00:58:52,960
was able to do it in a reasonable way um

1708
00:58:51,280 --> 00:58:55,200
and get like a reasonable

1709
00:58:52,960 --> 00:58:57,160
response um if you look at the purple

1710
00:58:55,200 --> 00:58:58,480
line gp24

1711
00:58:57,160 --> 00:59:00,319
like we kind of expect out of these

1712
00:58:58,480 --> 00:59:02,280
models that gp40 would be pretty good

1713
00:59:00,319 --> 00:59:06,760
but actually llama 3 which is open

1714
00:59:02,280 --> 00:59:09,280
source model is pretty act equal um I

1715
00:59:06,760 --> 00:59:13,280
had a question about the task initi uh

1716
00:59:09,280 --> 00:59:17,400
the MD like initialization yeah um and I

1717
00:59:13,280 --> 00:59:20,799
was curious like how does the L pick the

1718
00:59:17,400 --> 00:59:23,920
parameters and also I don't know because

1719
00:59:20,799 --> 00:59:27,119
I remember from um Andrew's presentation

1720
00:59:23,920 --> 00:59:29,599
also there was like some like logic to

1721
00:59:27,119 --> 00:59:33,200
how it picks and decides which task to

1722
00:59:29,599 --> 00:59:36,760
do how like if there's like how do you

1723
00:59:33,200 --> 00:59:38,520
approach changing parameters like if it

1724
00:59:36,760 --> 00:59:40,000
like does it do one set of parameters

1725
00:59:38,520 --> 00:59:43,160
and then change to another set of

1726
00:59:40,000 --> 00:59:45,280
parameters if that doesn't work or yeah

1727
00:59:43,160 --> 00:59:47,920
yeah so we have like default parameters

1728
00:59:45,280 --> 00:59:50,640
for a lot of them um

1729
00:59:47,920 --> 00:59:53,319
because just defaulted in the code like

1730
00:59:50,640 --> 00:59:55,839
all that's all the software does also um

1731
00:59:53,319 --> 00:59:57,400
but we it chooses the parameters based

1732
00:59:55,839 --> 00:59:58,880
on the The Prompt so you might say like

1733
00:59:57,400 --> 01:00:01,000
the temperature or The Ensemble on the

1734
00:59:58,880 --> 01:00:02,599
prompt otherwise we kind of let it

1735
01:00:01,000 --> 01:00:05,599
choose one thing that we found really

1736
01:00:02,599 --> 01:00:07,839
interesting um is that if you don't

1737
01:00:05,599 --> 01:00:11,240
specify The Ensemble it just kind of

1738
01:00:07,839 --> 01:00:13,799
picks whatever it wants obviously um but

1739
01:00:11,240 --> 01:00:16,039
the models have favorites so like Claude

1740
01:00:13,799 --> 01:00:18,319
consistently always no matter what wants

1741
01:00:16,039 --> 01:00:20,440
to do npt um and you kind of have to

1742
01:00:18,319 --> 01:00:22,119
like force it to not do it which uh to

1743
01:00:20,440 --> 01:00:23,880
its downfall and then the other models

1744
01:00:22,119 --> 01:00:26,079
like to like diversify a little bit so

1745
01:00:23,880 --> 01:00:28,319
this is something that actually with m

1746
01:00:26,079 --> 01:00:30,920
also we're thinking about um building

1747
01:00:28,319 --> 01:00:33,200
like a more stringent eval set for to

1748
01:00:30,920 --> 01:00:36,480
make sure that every single like uh

1749
01:00:33,200 --> 01:00:37,360
minute parameter is defined correctly

1750
01:00:36,480 --> 01:00:40,409
thank

1751
01:00:37,360 --> 01:00:40,409
[Music]

1752
01:00:41,200 --> 01:00:46,960
you you said something like oh it's

1753
01:00:43,680 --> 01:00:49,960
surprising that um LMA actually

1754
01:00:46,960 --> 01:00:52,680
performed uh as as good as other bigger

1755
01:00:49,960 --> 01:00:55,960
models but then to me okay the test here

1756
01:00:52,680 --> 01:00:58,280
is like pretty straightforward um like

1757
01:00:55,960 --> 01:01:00,240
procing getting pdb processing pdb is

1758
01:00:58,280 --> 01:01:03,039
something like you can just you can just

1759
01:01:00,240 --> 01:01:05,160
write you can hard code those processes

1760
01:01:03,039 --> 01:01:07,400
so I wonder in that sense maybe it's not

1761
01:01:05,160 --> 01:01:09,280
that surprising that the smaller model

1762
01:01:07,400 --> 01:01:12,799
works as good as the big

1763
01:01:09,280 --> 01:01:14,200
one um I think it's not not necessarily

1764
01:01:12,799 --> 01:01:16,200
the case that it's super simple because

1765
01:01:14,200 --> 01:01:16,960
there's are like super straightforward

1766
01:01:16,200 --> 01:01:19,839
and

1767
01:01:16,960 --> 01:01:21,960
actually here we test um with this is

1768
01:01:19,839 --> 01:01:23,319
with GPT 40 but we test with like just

1769
01:01:21,960 --> 01:01:25,480
actually having it write the code like

1770
01:01:23,319 --> 01:01:27,280
hardcode it which is the direct llm and

1771
01:01:25,480 --> 01:01:28,440
then also just having it do a react

1772
01:01:27,280 --> 01:01:30,400
where we don't give it any tools but

1773
01:01:28,440 --> 01:01:33,720
it's able to write python code and it

1774
01:01:30,400 --> 01:01:36,400
actually performs pretty poorly on these

1775
01:01:33,720 --> 01:01:38,280
um so I don't think it's necessarily the

1776
01:01:36,400 --> 01:01:43,920
case I think that where some of these

1777
01:01:38,280 --> 01:01:45,559
other models um fail so I mean GPT 3.5

1778
01:01:43,920 --> 01:01:48,280
never stood a chance but the other

1779
01:01:45,559 --> 01:01:50,079
models that are more medium um they

1780
01:01:48,280 --> 01:01:52,920
really just fail in using the tools

1781
01:01:50,079 --> 01:01:54,359
correctly um and choosing the parameters

1782
01:01:52,920 --> 01:01:57,839
wait so so I'm trying to understand like

1783
01:01:54,359 --> 01:02:01,000
where do they fail so like choosing um

1784
01:01:57,839 --> 01:02:03,279
like code coding that part to me is it

1785
01:02:01,000 --> 01:02:06,160
should be like there only this few

1786
01:02:03,279 --> 01:02:08,559
common lines you can you can write for

1787
01:02:06,160 --> 01:02:10,839
running MD simulation so do they mostly

1788
01:02:08,559 --> 01:02:13,279
feel like the choosing right parameter

1789
01:02:10,839 --> 01:02:16,799
part and what do you mean by wrong

1790
01:02:13,279 --> 01:02:18,119
parameter yeah yeah so um for instance

1791
01:02:16,799 --> 01:02:20,960
it would like forget to choose

1792
01:02:18,119 --> 01:02:24,079
parameters a lot of the ones the ones

1793
01:02:20,960 --> 01:02:26,079
that perform more poorly like um

1794
01:02:24,079 --> 01:02:30,599
actually the medium ones like the claws

1795
01:02:26,079 --> 01:02:32,400
and the like gp4 turbo and the Llama 70b

1796
01:02:30,599 --> 01:02:34,880
um a lot of what they do is just because

1797
01:02:32,400 --> 01:02:36,599
the process is so long right like

1798
01:02:34,880 --> 01:02:38,880
running an MD simulation and also like

1799
01:02:36,599 --> 01:02:42,400
if you ask it to do 15 analysis steps it

1800
01:02:38,880 --> 01:02:46,680
just kind of forgets things um and it

1801
01:02:42,400 --> 01:02:48,039
will just hallucinate so it will like or

1802
01:02:46,680 --> 01:02:50,520
it'll give the when I meant with the

1803
01:02:48,039 --> 01:02:52,880
wrong parameters is not what I not how I

1804
01:02:50,520 --> 01:02:54,000
said it but the um like wrong file like

1805
01:02:52,880 --> 01:02:56,000
it'll try to

1806
01:02:54,000 --> 01:02:57,520
run um if it runs two different

1807
01:02:56,000 --> 01:03:00,119
simulations it and then it's supposed to

1808
01:02:57,520 --> 01:03:02,760
do rmsd on both it'll just do it both on

1809
01:03:00,119 --> 01:03:05,240
one and it just mixes those things up

1810
01:03:02,760 --> 01:03:08,240
okay interesting yeah like very silly

1811
01:03:05,240 --> 01:03:08,240
mistakes

1812
01:03:09,079 --> 01:03:15,960
yeah there is one question online can

1813
01:03:12,119 --> 01:03:15,960
you unmute yourself and ask the

1814
01:03:18,079 --> 01:03:24,279
question oh yes um I'm curious what is

1815
01:03:21,440 --> 01:03:26,559
the contribution of largel model um to

1816
01:03:24,279 --> 01:03:28,480
MD CR is it used to convert human

1817
01:03:26,559 --> 01:03:31,640
requests into a pipeline of existing

1818
01:03:28,480 --> 01:03:34,240
tools and how does um your pip uh your

1819
01:03:31,640 --> 01:03:35,799
tool compared to a pipeline of tools

1820
01:03:34,240 --> 01:03:39,839
thank

1821
01:03:35,799 --> 01:03:42,760
you yeah so the MDC is an llm agent so

1822
01:03:39,839 --> 01:03:44,240
it takes the user request and then like

1823
01:03:42,760 --> 01:03:47,039
reasons through what tools it wants to

1824
01:03:44,240 --> 01:03:49,079
use all of the tools most of the tools

1825
01:03:47,039 --> 01:03:50,680
are not llm based they're like just

1826
01:03:49,079 --> 01:03:52,920
computational tools that are built on

1827
01:03:50,680 --> 01:03:54,799
regular software um but it does the

1828
01:03:52,920 --> 01:03:58,480
reasoning process of choosing the tools

1829
01:03:54,799 --> 01:03:58,480
and then the inputs to the tools as

1830
01:03:59,240 --> 01:04:05,079
well got it

1831
01:04:02,079 --> 01:04:05,079
thanks

1832
01:04:06,039 --> 01:04:10,680
cool Okay cool so um one of the other

1833
01:04:09,160 --> 01:04:12,640
things we found and this is Sim uh

1834
01:04:10,680 --> 01:04:14,480
getting at with the question I was asked

1835
01:04:12,640 --> 01:04:17,039
a second ago is um all the way on the

1836
01:04:14,480 --> 01:04:18,760
bottom left we have each of the models

1837
01:04:17,039 --> 01:04:21,799
and then the accuracy as the number of

1838
01:04:18,760 --> 01:04:24,319
subtasks required increases um and you

1839
01:04:21,799 --> 01:04:26,720
can see like GPT 40 even which is like

1840
01:04:24,319 --> 01:04:29,240
one of the best models still kind of

1841
01:04:26,720 --> 01:04:30,880
dies off after a while and only gets

1842
01:04:29,240 --> 01:04:33,279
like as it gets more complicated and we

1843
01:04:30,880 --> 01:04:36,000
ask it to do more things it still is not

1844
01:04:33,279 --> 01:04:39,200
100% um and so there's still a good way

1845
01:04:36,000 --> 01:04:43,400
to go with this um a lot of what we saw

1846
01:04:39,200 --> 01:04:45,200
with um GPT 40 is it doesn't really skip

1847
01:04:43,400 --> 01:04:46,440
steps it's really good about like

1848
01:04:45,200 --> 01:04:49,480
completing the right steps and actually

1849
01:04:46,440 --> 01:04:52,520
the same with llama 3 uh 405b it's good

1850
01:04:49,480 --> 01:04:54,279
really good at like completing the steps

1851
01:04:52,520 --> 01:04:56,359
say like checking them off calling the

1852
01:04:54,279 --> 01:04:57,599
tools um but again it like gives the

1853
01:04:56,359 --> 01:05:00,480
wrong parameters and it makes silly

1854
01:04:57,599 --> 01:05:02,119
mistakes like using if it runs at two

1855
01:05:00,480 --> 01:05:03,720
different temperatures it uses only one

1856
01:05:02,119 --> 01:05:06,880
temperature and just runs it twice for

1857
01:05:03,720 --> 01:05:09,520
RSD um whereas like uh Cloud 3 Opus and

1858
01:05:06,880 --> 01:05:11,359
even Cloud 3.5 Sonet will give you kind

1859
01:05:09,520 --> 01:05:13,000
of silly things where it's like it won't

1860
01:05:11,359 --> 01:05:15,520
run RSD and then it'll just give you a

1861
01:05:13,000 --> 01:05:17,680
number like it did um it's very honest

1862
01:05:15,520 --> 01:05:17,680
that

1863
01:05:17,960 --> 01:05:24,599
way um I kind of talked about this but

1864
01:05:21,680 --> 01:05:26,880
just to come back to uh the MD Crow

1865
01:05:24,599 --> 01:05:28,799
built on gp24 versus react and just

1866
01:05:26,880 --> 01:05:32,119
asking the LM to write the code

1867
01:05:28,799 --> 01:05:34,799
similarly we see a trend where again the

1868
01:05:32,119 --> 01:05:37,480
performance drops off even for empd Crow

1869
01:05:34,799 --> 01:05:40,559
as the complexity increases um but for

1870
01:05:37,480 --> 01:05:43,200
the react and direct llm it can complete

1871
01:05:40,559 --> 01:05:45,880
some of them uh certainly the one step

1872
01:05:43,200 --> 01:05:48,599
one it can write code to to fetch a pdb

1873
01:05:45,880 --> 01:05:51,880
file and then it drops very quickly with

1874
01:05:48,599 --> 01:05:53,440
um like no performance after that sorry

1875
01:05:51,880 --> 01:05:56,160
just

1876
01:05:53,440 --> 01:05:58,400
question if like

1877
01:05:56,160 --> 01:06:01,760
how would this table look if you added

1878
01:05:58,400 --> 01:06:03,359
the recent reasoning reasoning models

1879
01:06:01,760 --> 01:06:08,960
like gep CH

1880
01:06:03,359 --> 01:06:11,880
the O 03 from open AI like in your

1881
01:06:08,960 --> 01:06:15,200
opinion would it would it like strongly

1882
01:06:11,880 --> 01:06:16,960
improve or jtic margin I definitely

1883
01:06:15,200 --> 01:06:19,520
think so because I think um we have not

1884
01:06:16,960 --> 01:06:23,880
run um MD Crow based on reasoning models

1885
01:06:19,520 --> 01:06:26,200
yet um that's something that we are

1886
01:06:23,880 --> 01:06:27,079
doing now let me answer question because

1887
01:06:26,200 --> 01:06:29,880
actually we've been thinking about this

1888
01:06:27,079 --> 01:06:31,920
in future houses we we tried 03 mini

1889
01:06:29,880 --> 01:06:35,039
High which is supposed to be the best

1890
01:06:31,920 --> 01:06:37,000
model in like paper QA we tried it in um

1891
01:06:35,039 --> 01:06:40,200
the molecular cloning tasks and it got

1892
01:06:37,000 --> 01:06:42,359
0% every single time and um the reason

1893
01:06:40,200 --> 01:06:44,839
why is that it just uh it doesn't seem

1894
01:06:42,359 --> 01:06:46,039
to handle tool descriptions very well

1895
01:06:44,839 --> 01:06:47,599
and I don't know if there's some like

1896
01:06:46,039 --> 01:06:50,200
additional work they need to open a eye

1897
01:06:47,599 --> 01:06:52,359
or something or it's uh just like not

1898
01:06:50,200 --> 01:06:55,000
ready for for calling tools but it but

1899
01:06:52,359 --> 01:06:56,880
it um it would basically always forget

1900
01:06:55,000 --> 01:06:58,839
to submit the answer or it would like

1901
01:06:56,880 --> 01:07:00,760
always forget to like uh download

1902
01:06:58,839 --> 01:07:01,880
something um so I think that one's still

1903
01:07:00,760 --> 01:07:03,559
iring out some Kinks but I think in

1904
01:07:01,880 --> 01:07:05,599
theory yes reasoning model should do

1905
01:07:03,559 --> 01:07:08,839
better um and I think we also tried

1906
01:07:05,599 --> 01:07:12,480
Cloud 3.7 thinking and we didn't see any

1907
01:07:08,839 --> 01:07:16,440
difference with Cloud 3.6 so we haven't

1908
01:07:12,480 --> 01:07:19,160
seen a gain there and I don't know why

1909
01:07:16,440 --> 01:07:20,640
but uh I mean I think that I mean it's

1910
01:07:19,160 --> 01:07:22,160
kind of obvious that the next step for

1911
01:07:20,640 --> 01:07:23,760
these things is to let them think for

1912
01:07:22,160 --> 01:07:25,119
longer because it's such a complicated

1913
01:07:23,760 --> 01:07:28,599
environment but yeah we haven't seen any

1914
01:07:25,119 --> 01:07:28,599
gains zero shot

1915
01:07:29,279 --> 01:07:34,799
yet so the question was deep seek R1 uh

1916
01:07:32,200 --> 01:07:36,920
no deep seek R1 um doesn't seem to call

1917
01:07:34,799 --> 01:07:38,160
tools very well and um one of the

1918
01:07:36,920 --> 01:07:40,960
problems is reasoning models are very

1919
01:07:38,160 --> 01:07:42,119
very long Generation Um and then when

1920
01:07:40,960 --> 01:07:43,160
you do an agent you have a very long

1921
01:07:42,119 --> 01:07:44,160
long prompt because you have a whole

1922
01:07:43,160 --> 01:07:45,720
bunch of tools you have to describe and

1923
01:07:44,160 --> 01:07:49,480
so actually run out of context quickly

1924
01:07:45,720 --> 01:07:51,319
in these settings but certainly I think

1925
01:07:49,480 --> 01:07:53,480
yes not the using tools would not do

1926
01:07:51,319 --> 01:07:56,160
well but in this comparison if we asked

1927
01:07:53,480 --> 01:07:57,480
it to do the direct Lim call it

1928
01:07:56,160 --> 01:08:01,000
certainly would be

1929
01:07:57,480 --> 01:08:03,599
better I'm trying to understand like

1930
01:08:01,000 --> 01:08:05,480
okay I understand this might be um like

1931
01:08:03,599 --> 01:08:08,760
a first test case for something that

1932
01:08:05,480 --> 01:08:11,520
will be more complicated later but then

1933
01:08:08,760 --> 01:08:13,760
U just in the case of MD I'm roosing

1934
01:08:11,520 --> 01:08:15,960
over your uh preprint and then I

1935
01:08:13,760 --> 01:08:18,920
realized that this workflow is what most

1936
01:08:15,960 --> 01:08:21,839
MD Scholars would do and and have um

1937
01:08:18,920 --> 01:08:25,120
have code for already so it's a pipeline

1938
01:08:21,839 --> 01:08:27,560
that you can basically fully hard code

1939
01:08:25,120 --> 01:08:29,679
MH so then what's the point of using

1940
01:08:27,560 --> 01:08:31,679
large language model to to recode each

1941
01:08:29,679 --> 01:08:33,640
of those steps like for example ret Tri

1942
01:08:31,679 --> 01:08:39,319
pdb is literally

1943
01:08:33,640 --> 01:08:42,600
just yeah with w g with a pdv ID yeah so

1944
01:08:39,319 --> 01:08:44,719
what's the point of asking the to recoe

1945
01:08:42,600 --> 01:08:47,520
those kind of rep repetitive

1946
01:08:44,719 --> 01:08:48,480
lines yeah so I think the if I

1947
01:08:47,520 --> 01:08:49,839
understand your question correctly the

1948
01:08:48,480 --> 01:08:52,560
point uh that we were trying to get out

1949
01:08:49,839 --> 01:08:55,000
with mdro is trying to see like we we

1950
01:08:52,560 --> 01:08:56,520
did chro and we saw that it can do some

1951
01:08:55,000 --> 01:08:59,159
chemistry TK but those are relatively

1952
01:08:56,520 --> 01:09:00,600
simple um and the like you get the

1953
01:08:59,159 --> 01:09:02,120
answer back immediately there's no file

1954
01:09:00,600 --> 01:09:04,000
handling it's it's very like simple

1955
01:09:02,120 --> 01:09:05,600
string in string out and then with MD

1956
01:09:04,000 --> 01:09:07,400
Crow we wanted to kind of test that and

1957
01:09:05,600 --> 01:09:10,359
bring it to the extreme of seeing it can

1958
01:09:07,400 --> 01:09:12,880
llms do the more complex um processes

1959
01:09:10,359 --> 01:09:14,239
with the more complex tool calling which

1960
01:09:12,880 --> 01:09:18,359
part of the process is something that

1961
01:09:14,239 --> 01:09:20,920
requires like reasoning for example um

1962
01:09:18,359 --> 01:09:24,920
yeah like for example I I think um

1963
01:09:20,920 --> 01:09:28,400
download uh pdb or run simulation or uh

1964
01:09:24,920 --> 01:09:31,319
run says those are pre-existing packages

1965
01:09:28,400 --> 01:09:34,120
um so what which part is the reasoning

1966
01:09:31,319 --> 01:09:37,440
part coming so the reasoning then comes

1967
01:09:34,120 --> 01:09:39,480
in in the like choosing the parameters

1968
01:09:37,440 --> 01:09:40,960
for the tools so if we don't give it all

1969
01:09:39,480 --> 01:09:43,480
of the parameters for the simulation for

1970
01:09:40,960 --> 01:09:46,120
example or if it has to look up like if

1971
01:09:43,480 --> 01:09:47,719
we say run this at appropriate like run

1972
01:09:46,120 --> 01:09:49,040
some simulation at appropriate

1973
01:09:47,719 --> 01:09:51,199
temperature based on this paper and

1974
01:09:49,040 --> 01:09:53,600
literature and doing that piece and then

1975
01:09:51,199 --> 01:09:54,880
also like breaking down the task into

1976
01:09:53,600 --> 01:09:57,199
subtasks and actually making sure it

1977
01:09:54,880 --> 01:09:58,800
completes all of those um but we weren't

1978
01:09:57,199 --> 01:10:01,320
necessarily trying to evaluate its

1979
01:09:58,800 --> 01:10:05,400
ability to reason about MD on this on

1980
01:10:01,320 --> 01:10:05,400
this project oh yes thank you

1981
01:10:08,320 --> 01:10:15,159
yeah uh I'm curious if you looked at all

1982
01:10:12,040 --> 01:10:17,560
and I know this this also was this

1983
01:10:15,159 --> 01:10:20,080
project was maybe a couple years ago but

1984
01:10:17,560 --> 01:10:22,960
if you've looked at all at memory or at

1985
01:10:20,080 --> 01:10:24,440
storing memories um I just thought of it

1986
01:10:22,960 --> 01:10:25,960
when we were talking about the reasoning

1987
01:10:24,440 --> 01:10:27,719
models kind of

1988
01:10:25,960 --> 01:10:30,560
for getting things that go out of the

1989
01:10:27,719 --> 01:10:32,760
long context window

1990
01:10:30,560 --> 01:10:35,800
yeah that's a great question so we also

1991
01:10:32,760 --> 01:10:37,600
implemented memory in IND Crow um we

1992
01:10:35,800 --> 01:10:40,199
implemented it as kind of like a chat

1993
01:10:37,600 --> 01:10:41,920
feature because um like you don't want

1994
01:10:40,199 --> 01:10:43,320
to sit there and have a chat bot that

1995
01:10:41,920 --> 01:10:46,520
runs a simulation that you have to wait

1996
01:10:43,320 --> 01:10:48,560
on um but we also store them so this is

1997
01:10:46,520 --> 01:10:50,280
like an example of how you can like chat

1998
01:10:48,560 --> 01:10:52,600
with it but the way that it implements

1999
01:10:50,280 --> 01:10:54,080
memory is it takes the entire trajectory

2000
01:10:52,600 --> 01:10:55,880
that it ran and its thought process and

2001
01:10:54,080 --> 01:10:59,159
then just uses an llm to summarize that

2002
01:10:55,880 --> 01:11:00,840
and store it into memory um the way that

2003
01:10:59,159 --> 01:11:02,280
this is like a kind of silly example of

2004
01:11:00,840 --> 01:11:04,640
breaking up the steps but the way that

2005
01:11:02,280 --> 01:11:06,640
like I ran this in practice is if it

2006
01:11:04,640 --> 01:11:08,199
like really screwed up I would be like

2007
01:11:06,640 --> 01:11:10,440
hey remember when you did that that was

2008
01:11:08,199 --> 01:11:11,920
wrong can you try again with some like

2009
01:11:10,440 --> 01:11:15,600
uh change to it so we did Implement

2010
01:11:11,920 --> 01:11:17,760
memory in this as well

2011
01:11:15,600 --> 01:11:20,480
yeah

2012
01:11:17,760 --> 01:11:23,320
okay um the other thing that we tested

2013
01:11:20,480 --> 01:11:25,760
with MD Crow is really diving into that

2014
01:11:23,320 --> 01:11:27,440
like increasing complexity and the way

2015
01:11:25,760 --> 01:11:29,159
we did this is we built another small

2016
01:11:27,440 --> 01:11:30,719
set of 10 tasks where they like

2017
01:11:29,159 --> 01:11:32,920
literally build on each other like if

2018
01:11:30,719 --> 01:11:34,480
step one is just download a p2b step two

2019
01:11:32,920 --> 01:11:36,320
is download and clean it step three is

2020
01:11:34,480 --> 01:11:38,440
download Clean and simulate so it's just

2021
01:11:36,320 --> 01:11:40,280
literally building on each other um and

2022
01:11:38,440 --> 01:11:44,239
again these are like relatively simple

2023
01:11:40,280 --> 01:11:47,120
tasks um but we find that uh some of the

2024
01:11:44,239 --> 01:11:48,840
models like TPT 40 and llama 3 still

2025
01:11:47,120 --> 01:11:50,840
have a pretty good the diagonal is

2026
01:11:48,840 --> 01:11:52,480
perfect where the number of subtasks

2027
01:11:50,840 --> 01:11:55,320
completed versus the number of subtasks

2028
01:11:52,480 --> 01:11:57,199
required is like pretty diagonal um but

2029
01:11:55,320 --> 01:12:00,760
interestingly we see that like all of

2030
01:11:57,199 --> 01:12:04,280
the Claud models really are not great um

2031
01:12:00,760 --> 01:12:07,400
and the reason for that is we tried to

2032
01:12:04,280 --> 01:12:09,040
be very objective TR and we didn't like

2033
01:12:07,400 --> 01:12:10,679
look at the performance of one model and

2034
01:12:09,040 --> 01:12:12,199
then adjust our system in any way we

2035
01:12:10,679 --> 01:12:14,679
tried to just have like okay we built

2036
01:12:12,199 --> 01:12:17,719
the system can they do the tasks but if

2037
01:12:14,679 --> 01:12:19,679
you were to fix it and look at the cloud

2038
01:12:17,719 --> 01:12:21,000
models and say like hey they're really

2039
01:12:19,679 --> 01:12:23,040
bad at these four things they keep

2040
01:12:21,000 --> 01:12:24,679
making the same mistakes um we didn't

2041
01:12:23,040 --> 01:12:26,280
see this with gbt 40 or any of the other

2042
01:12:24,679 --> 01:12:29,480
models where like consistently all make

2043
01:12:26,280 --> 01:12:32,360
the same mistakes um Claud just really

2044
01:12:29,480 --> 01:12:34,440
really likes npt but is really bad at it

2045
01:12:32,360 --> 01:12:36,960
and so if you do make some adjustments

2046
01:12:34,440 --> 01:12:39,120
like you kind of default the pressure to

2047
01:12:36,960 --> 01:12:43,000
a reasonable reasonable pressure or you

2048
01:12:39,120 --> 01:12:44,400
make some like not great adjustments uh

2049
01:12:43,000 --> 01:12:46,280
you actually can get the performance

2050
01:12:44,400 --> 01:12:48,639
like pretty decent and get it to match

2051
01:12:46,280 --> 01:12:52,880
and get it to bump up there um all this

2052
01:12:48,639 --> 01:12:54,280
to say like even with we see like pretty

2053
01:12:52,880 --> 01:12:57,199
good performance with the open source

2054
01:12:54,280 --> 01:12:58,960
model but even like just with Claude you

2055
01:12:57,199 --> 01:13:01,239
can get pretty decent performance if you

2056
01:12:58,960 --> 01:13:04,040
really wanted to use Claude by making

2057
01:13:01,239 --> 01:13:06,280
like model specific adjustments um to

2058
01:13:04,040 --> 01:13:06,280
the

2059
01:13:06,560 --> 01:13:12,000
system

2060
01:13:08,719 --> 01:13:14,120
okay so with MD with Kim Crow we found

2061
01:13:12,000 --> 01:13:15,920
um the same conclusions and with MD Crow

2062
01:13:14,120 --> 01:13:19,080
we found that like even the new llms

2063
01:13:15,920 --> 01:13:21,239
can't handle these like complex MD tasks

2064
01:13:19,080 --> 01:13:22,960
again the tasks are not complex to like

2065
01:13:21,239 --> 01:13:25,920
somebody who does MD regularly but

2066
01:13:22,960 --> 01:13:27,960
they're complex complex for an llm

2067
01:13:25,920 --> 01:13:30,320
um and also open source LMS are pretty

2068
01:13:27,960 --> 01:13:31,920
comparable to the frontier models for MD

2069
01:13:30,320 --> 01:13:33,639
Crow which is very interesting because

2070
01:13:31,920 --> 01:13:36,560
MD Crow still struggles with these more

2071
01:13:33,639 --> 01:13:40,679
complex tasks these more like multi-step

2072
01:13:36,560 --> 01:13:40,679
these like long um these long

2073
01:13:43,040 --> 01:13:48,040
trajectories okay so the last thing that

2074
01:13:46,159 --> 01:13:50,400
I'm gonna talk about is contro which

2075
01:13:48,040 --> 01:13:53,159
Andrew I think kind of hinted at um and

2076
01:13:50,400 --> 01:13:54,880
this is using paper QA to detect

2077
01:13:53,159 --> 01:13:56,360
contradictions in biology literature so

2078
01:13:54,880 --> 01:13:59,400
this is thinking about how we can use

2079
01:13:56,360 --> 01:14:01,159
these llm agents um to a scale that like

2080
01:13:59,400 --> 01:14:03,600
humans can't do because like like you

2081
01:14:01,159 --> 01:14:05,199
said humans can run these MD workflows

2082
01:14:03,600 --> 01:14:08,560
humans can do these cheminformatics

2083
01:14:05,199 --> 01:14:10,440
tasks um but humans can't detect like

2084
01:14:08,560 --> 01:14:13,520
contradictions in every single Paper in

2085
01:14:10,440 --> 01:14:16,120
literature against hundreds of

2086
01:14:13,520 --> 01:14:18,400
papers so contro combines claim

2087
01:14:16,120 --> 01:14:19,800
extraction with paper qa2 to detect

2088
01:14:18,400 --> 01:14:22,760
contradictions and literature so you

2089
01:14:19,800 --> 01:14:24,600
input a paper and it chunks out it

2090
01:14:22,760 --> 01:14:26,199
splits into chunks extracts claims and

2091
01:14:24,600 --> 01:14:28,000
filters them and then puts them into

2092
01:14:26,199 --> 01:14:30,719
contrad uh contradiction detection

2093
01:14:28,000 --> 01:14:32,159
prompt and paper qa2 and it gives you an

2094
01:14:30,719 --> 01:14:35,679
answer like a reasoning that you can

2095
01:14:32,159 --> 01:14:37,840
read um with sources and then also a

2096
01:14:35,679 --> 01:14:40,000
lyer scale of yes there's explicit

2097
01:14:37,840 --> 01:14:41,800
contradictions to in literature um all

2098
01:14:40,000 --> 01:14:43,920
the way down to five which is I have no

2099
01:14:41,800 --> 01:14:45,920
idea I've never heard of this before and

2100
01:14:43,920 --> 01:14:48,920
zero explicit agreement like everything

2101
01:14:45,920 --> 01:14:51,199
really agrees so we took the L QA data

2102
01:14:48,920 --> 01:14:52,760
set that Andrew introduced and split

2103
01:14:51,199 --> 01:14:55,159
half of them into true statements like

2104
01:14:52,760 --> 01:14:57,520
the sky is blue and half into not true

2105
01:14:55,159 --> 01:15:00,120
statements like the sky is not blue um

2106
01:14:57,520 --> 01:15:04,120
and put them all into uh contrac and

2107
01:15:00,120 --> 01:15:06,360
we've got a uh Au or of

2108
01:15:04,120 --> 01:15:10,080
0.842 it's like a proof of concept that

2109
01:15:06,360 --> 01:15:10,080
we're able to actually detect these in

2110
01:15:11,320 --> 01:15:15,159
literature so the next thing we did is

2111
01:15:13,760 --> 01:15:17,040
we were like okay let's just put it

2112
01:15:15,159 --> 01:15:18,840
let's run it on a 100 random papers so

2113
01:15:17,040 --> 01:15:21,639
we went in our database and found 100

2114
01:15:18,840 --> 01:15:23,159
random biology papers from all over the

2115
01:15:21,639 --> 01:15:26,960
place all sorts of journals from like

2116
01:15:23,159 --> 01:15:28,600
1982 to current all over the place um

2117
01:15:26,960 --> 01:15:30,760
and we found that all the way on the

2118
01:15:28,600 --> 01:15:33,000
left here most papers have pretty few

2119
01:15:30,760 --> 01:15:35,719
contradictions that we found um which is

2120
01:15:33,000 --> 01:15:38,080
great and expected if we found like 15

2121
01:15:35,719 --> 01:15:39,480
on every paper that would be insane but

2122
01:15:38,080 --> 01:15:40,760
if we look at the number of papers with

2123
01:15:39,480 --> 01:15:43,280
each number of contradictions we have

2124
01:15:40,760 --> 01:15:45,120
Zer one two three being the highest

2125
01:15:43,280 --> 01:15:46,239
which is pretty reasonable and we

2126
01:15:45,120 --> 01:15:48,679
actually found

2127
01:15:46,239 --> 01:15:50,639
2.34 uh contradictions per paper on

2128
01:15:48,679 --> 01:15:53,880
average in the papers we looked

2129
01:15:50,639 --> 01:15:56,000
at um then we use human experts so the

2130
01:15:53,880 --> 01:15:59,840
PhD students again

2131
01:15:56,000 --> 01:16:02,000
um and ask them to First Look at uh the

2132
01:15:59,840 --> 01:16:03,920
claims that Contra looked at and the

2133
01:16:02,000 --> 01:16:05,560
reasoning and its score and just do you

2134
01:16:03,920 --> 01:16:07,239
agree with it based on its reasoning

2135
01:16:05,560 --> 01:16:09,520
based on the literature it looked at do

2136
01:16:07,239 --> 01:16:11,520
you think this is a contradiction and so

2137
01:16:09,520 --> 01:16:13,239
for these scores of eight n and 10 which

2138
01:16:11,520 --> 01:16:17,080
are we consider a contradiction if it's

2139
01:16:13,239 --> 01:16:19,080
above that humans agree with 70% of them

2140
01:16:17,080 --> 01:16:22,400
um which might seem a little bit low to

2141
01:16:19,080 --> 01:16:24,000
you so then we asked humans okay you can

2142
01:16:22,400 --> 01:16:25,159
look at every paper that we're not going

2143
01:16:24,000 --> 01:16:26,719
to ask you to look at every paper paper

2144
01:16:25,159 --> 01:16:29,880
ever but you'll look at every paper that

2145
01:16:26,719 --> 01:16:31,719
paper QA even considered and see if you

2146
01:16:29,880 --> 01:16:34,320
think there's a contradiction and so

2147
01:16:31,719 --> 01:16:36,440
what we're looking at here is I sorted

2148
01:16:34,320 --> 01:16:39,400
it so that it's pretty but there's no

2149
01:16:36,440 --> 01:16:41,560
like correlation along the x-axis um the

2150
01:16:39,400 --> 01:16:43,800
stars are contrac scores so these are

2151
01:16:41,560 --> 01:16:47,040
ones that it scored eight nine and then

2152
01:16:43,800 --> 01:16:49,560
like four below so agree it all agrees

2153
01:16:47,040 --> 01:16:51,040
or it all contradict contradicts and

2154
01:16:49,560 --> 01:16:52,960
like you can kind of tell that humans

2155
01:16:51,040 --> 01:16:55,280
don't really agree with contra all the

2156
01:16:52,960 --> 01:16:56,719
time right like on the all the way on

2157
01:16:55,280 --> 01:16:59,320
the left you see that it gave it a nine

2158
01:16:56,719 --> 01:17:01,719
and humans gave it like a a one but if

2159
01:16:59,320 --> 01:17:04,080
you look at the horizontal lines we had

2160
01:17:01,719 --> 01:17:05,679
like three humans on each claim humans

2161
01:17:04,080 --> 01:17:08,320
don't even agree with each other on in

2162
01:17:05,679 --> 01:17:10,400
the large um some of them literally gave

2163
01:17:08,320 --> 01:17:12,840
like yes 100% this all of the literature

2164
01:17:10,400 --> 01:17:15,400
agrees and on the same claim another

2165
01:17:12,840 --> 01:17:18,880
human gave 100% they all just they all

2166
01:17:15,400 --> 01:17:21,000
contradict it so I think this shows that

2167
01:17:18,880 --> 01:17:22,639
like humans in general agree with contra

2168
01:17:21,000 --> 01:17:25,480
were able to find contradictions in

2169
01:17:22,639 --> 01:17:27,800
literature um and this is a really hard

2170
01:17:25,480 --> 01:17:29,639
task to do so this is something that

2171
01:17:27,800 --> 01:17:31,639
we're um using llm agents to do that

2172
01:17:29,639 --> 01:17:33,800
humans can't do in fact one of our

2173
01:17:31,639 --> 01:17:35,280
contractors even said this is the

2174
01:17:33,800 --> 01:17:37,719
hardest task we asked them to do and

2175
01:17:35,280 --> 01:17:39,639
they don't want to do it anymore so this

2176
01:17:37,719 --> 01:17:42,080
is something that's it's a great example

2177
01:17:39,639 --> 01:17:45,400
of how we can use these llms to scale

2178
01:17:42,080 --> 01:17:49,520
into areas that are like not feasible or

2179
01:17:45,400 --> 01:17:49,520
um like good for humans to

2180
01:17:53,600 --> 01:17:57,800
do really like this uh this

2181
01:17:56,440 --> 01:18:00,360
contradiction because I feel like a lot

2182
01:17:57,800 --> 01:18:02,360
of the papers I write are like almost

2183
01:18:00,360 --> 01:18:04,000
like spite papers where I see at a

2184
01:18:02,360 --> 01:18:05,199
conference someone claiming something

2185
01:18:04,000 --> 01:18:07,120
and then you go and then someone

2186
01:18:05,199 --> 01:18:08,920
disagrees and you're like I could prove

2187
01:18:07,120 --> 01:18:10,320
that right like I could prove that wrong

2188
01:18:08,920 --> 01:18:11,639
and so I think it's really cool because

2189
01:18:10,320 --> 01:18:13,120
like a lot of you know science comes

2190
01:18:11,639 --> 01:18:15,400
from contradictions I was wondering if

2191
01:18:13,120 --> 01:18:17,960
you've done any like structured searches

2192
01:18:15,400 --> 01:18:19,560
like that like let's say you know I go

2193
01:18:17,960 --> 01:18:21,760
I'm a neuroscience I go to like nature

2194
01:18:19,560 --> 01:18:23,159
reviews neuroscience and like for my

2195
01:18:21,760 --> 01:18:24,920
topic which is connectomic I just

2196
01:18:23,159 --> 01:18:27,000
download every review that mention

2197
01:18:24,920 --> 01:18:28,840
conics and put it in can you start

2198
01:18:27,000 --> 01:18:30,880
getting like scientific Concepts and

2199
01:18:28,840 --> 01:18:32,440
then like testing them have some Metric

2200
01:18:30,880 --> 01:18:34,679
of that yeah this is a great question so

2201
01:18:32,440 --> 01:18:37,120
this is something uh we actually Dove

2202
01:18:34,679 --> 01:18:39,719
we've dove into this a lot actually um

2203
01:18:37,120 --> 01:18:41,400
for different topics um originally when

2204
01:18:39,719 --> 01:18:43,600
we started this we thought we would find

2205
01:18:41,400 --> 01:18:46,120
really like hotly contested areas of

2206
01:18:43,600 --> 01:18:48,199
science or like places to do research um

2207
01:18:46,120 --> 01:18:50,560
a lot of what we found is like authors

2208
01:18:48,199 --> 01:18:52,000
don't do their job finding literature

2209
01:18:50,560 --> 01:18:55,040
and reading it before they write their

2210
01:18:52,000 --> 01:18:57,199
papers um but certainly we also did look

2211
01:18:55,040 --> 01:18:59,280
topics and this is the font is very

2212
01:18:57,199 --> 01:19:00,159
small I'm sorry but we looked at across

2213
01:18:59,280 --> 01:19:03,360
different

2214
01:19:00,159 --> 01:19:06,280
journals um and so on the bottom it's

2215
01:19:03,360 --> 01:19:08,480
like kind of like topics um I don't have

2216
01:19:06,280 --> 01:19:09,800
any plots of like the specific areas but

2217
01:19:08,480 --> 01:19:12,840
you can see like nature aging nature

2218
01:19:09,800 --> 01:19:15,679
biotech nature chemistry um and this is

2219
01:19:12,840 --> 01:19:18,199
not public data yet uh we see that like

2220
01:19:15,679 --> 01:19:20,040
nature aging uh the contradictions

2221
01:19:18,199 --> 01:19:21,920
number of contradictions per word is a

2222
01:19:20,040 --> 01:19:23,320
lot higher than like nature methods

2223
01:19:21,920 --> 01:19:24,520
which is great because why would there

2224
01:19:23,320 --> 01:19:26,960
be so many contradictions in nature

2225
01:19:24,520 --> 01:19:29,040
method methods and then we asked an llm

2226
01:19:26,960 --> 01:19:31,360
to take the reasoning that the contrac

2227
01:19:29,040 --> 01:19:33,560
outputed and the claim plus a little

2228
01:19:31,360 --> 01:19:35,080
context that the claim came from and

2229
01:19:33,560 --> 01:19:37,800
whether it's a contradiction or not and

2230
01:19:35,080 --> 01:19:39,719
categorized it as inherited so is this

2231
01:19:37,800 --> 01:19:41,639
like introduction like it like cites a

2232
01:19:39,719 --> 01:19:43,960
paper and the paper has been refuted or

2233
01:19:41,639 --> 01:19:45,320
it incorrectly cites it um has this

2234
01:19:43,960 --> 01:19:46,840
something that said that's like a

2235
01:19:45,320 --> 01:19:49,239
speculation that was later

2236
01:19:46,840 --> 01:19:52,560
disproven does it overstate the evidence

2237
01:19:49,239 --> 01:19:54,280
so if it finds like um pitbulls really

2238
01:19:52,560 --> 01:19:55,159
like bananas and it says all dogss like

2239
01:19:54,280 --> 01:19:56,920
bananas

2240
01:19:55,159 --> 01:20:00,080
very like absurd because it has that

2241
01:19:56,920 --> 01:20:02,199
context um if it's just like an outdated

2242
01:20:00,080 --> 01:20:04,199
claim or if it's empirical disagreement

2243
01:20:02,199 --> 01:20:05,480
which is very common right like paper

2244
01:20:04,199 --> 01:20:08,280
can just find different things maybe

2245
01:20:05,480 --> 01:20:10,199
they use different samples maybe they um

2246
01:20:08,280 --> 01:20:12,480
like just find different things that's

2247
01:20:10,199 --> 01:20:15,880
very common in science um and we find

2248
01:20:12,480 --> 01:20:18,480
that like nature aging is really high

2249
01:20:15,880 --> 01:20:19,920
and that's because it's like you can

2250
01:20:18,480 --> 01:20:22,000
kind of split it out and see the

2251
01:20:19,920 --> 01:20:25,320
different ways and how but we did look

2252
01:20:22,000 --> 01:20:26,600
at different topics as well yeah

2253
01:20:25,320 --> 01:20:31,000
but then you have to download all the

2254
01:20:26,600 --> 01:20:34,199
papers which is which is the the thing

2255
01:20:31,000 --> 01:20:38,199
yeah sorry if you go back two slides for

2256
01:20:34,199 --> 01:20:40,840
that Benchmark Au you see it's 084 what

2257
01:20:38,199 --> 01:20:43,040
is uh the contradiction struggling to

2258
01:20:40,840 --> 01:20:48,040
detect and are you guys how are you guys

2259
01:20:43,040 --> 01:20:49,600
addressing that yeah so um this was I

2260
01:20:48,040 --> 01:20:51,520
don't remember what the recall of like

2261
01:20:49,600 --> 01:20:55,000
QA was at this point but it was also not

2262
01:20:51,520 --> 01:20:55,000
100 do you remember

2263
01:20:55,880 --> 01:21:00,280
yeah so the recall of lit QA meaning

2264
01:20:58,320 --> 01:21:01,560
like can it if given the question did it

2265
01:21:00,280 --> 01:21:05,400
find the right paper in general because

2266
01:21:01,560 --> 01:21:08,239
there's only one paper was like 60

2267
01:21:05,400 --> 01:21:09,280
67% um and so we're getting 53% so it's

2268
01:21:08,239 --> 01:21:13,040
possible that it's literally just not

2269
01:21:09,280 --> 01:21:14,679
finding the paper um in most cases um I

2270
01:21:13,040 --> 01:21:15,760
didn't dive really into farther into

2271
01:21:14,679 --> 01:21:17,280
that because this was kind of just like

2272
01:21:15,760 --> 01:21:18,639
a proof of concept that it's able to

2273
01:21:17,280 --> 01:21:21,840
detect

2274
01:21:18,639 --> 01:21:25,040
them but the recall of these papers in

2275
01:21:21,840 --> 01:21:27,239
general was not 100 so it never a chance

2276
01:21:25,040 --> 01:21:27,239
there

2277
01:21:29,000 --> 01:21:32,480
either any other

2278
01:21:37,159 --> 01:21:45,040
questions okay yeah I think that's all I

2279
01:21:41,800 --> 01:21:46,920
uh mostly have just final conclusions um

2280
01:21:45,040 --> 01:21:49,920
we kind of with Kim Crow we found like

2281
01:21:46,920 --> 01:21:51,679
okay cool these LMS can do science um

2282
01:21:49,920 --> 01:21:54,800
but they're limited by the tools uh with

2283
01:21:51,679 --> 01:21:57,000
MD Crow we found it we can do more

2284
01:21:54,800 --> 01:21:58,639
complete more complex tasks but it tends

2285
01:21:57,000 --> 01:22:00,639
to kind of struggle and open source

2286
01:21:58,639 --> 01:22:03,120
models are kind of equal still to the

2287
01:22:00,639 --> 01:22:05,520
frontier models and then with contro we

2288
01:22:03,120 --> 01:22:08,080
found uh hey this is a really hard task

2289
01:22:05,520 --> 01:22:10,120
but it's necessary for science um if we

2290
01:22:08,080 --> 01:22:12,960
use contrac humans agree with 70% which

2291
01:22:10,120 --> 01:22:15,520
is a pretty good amount um and then also

2292
01:22:12,960 --> 01:22:17,120
pre-printed papers shocker have more

2293
01:22:15,520 --> 01:22:20,400
contradictions than most peer-reviewed

2294
01:22:17,120 --> 01:22:22,239
Journal papers um so that's kind of the

2295
01:22:20,400 --> 01:22:25,719
uh and then obviously we can scale much

2296
01:22:22,239 --> 01:22:25,719
easier with contra

