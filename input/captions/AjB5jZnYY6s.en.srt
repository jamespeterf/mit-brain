1
00:00:04,720 --> 00:00:11,719
oh hello SE hello Aspen welcome to my

2
00:00:08,960 --> 00:00:14,280
home thank you for having us what makes

3
00:00:11,719 --> 00:00:18,920
this space feel like home besides my pet

4
00:00:14,280 --> 00:00:21,279
cow pig uh Good Vibes good food

5
00:00:18,920 --> 00:00:23,519
definitely good people good things all

6
00:00:21,279 --> 00:00:24,800
around that's all you need now I want to

7
00:00:23,519 --> 00:00:26,920
ask you a few questions about your

8
00:00:24,800 --> 00:00:28,679
research you started your journey in HCI

9
00:00:26,920 --> 00:00:30,960
and data visualization and then you

10
00:00:28,679 --> 00:00:33,200
moved to ml and interpret tell me about

11
00:00:30,960 --> 00:00:35,680
that yeah I think on the surface those

12
00:00:33,200 --> 00:00:37,559
two Fields seem very different but they

13
00:00:35,680 --> 00:00:41,879
both deal with this fundamental question

14
00:00:37,559 --> 00:00:43,719
of data and how do we as people make

15
00:00:41,879 --> 00:00:46,079
inferences on that data and make

16
00:00:43,719 --> 00:00:49,440
decisions with that data so that's you

17
00:00:46,079 --> 00:00:51,840
know the connection um I really wanted

18
00:00:49,440 --> 00:00:53,760
to be a well-rounded researcher and I

19
00:00:51,840 --> 00:00:56,079
loved the way that machine learning

20
00:00:53,760 --> 00:00:58,879
researchers and theorists often

21
00:00:56,079 --> 00:01:00,600
formalize evaluate and ask questions and

22
00:00:58,879 --> 00:01:03,239
so yeah that's that's that's really what

23
00:01:00,600 --> 00:01:05,600
got us here is well-roundedness okay

24
00:01:03,239 --> 00:01:06,960
very interdisciplinary approach now I'm

25
00:01:05,600 --> 00:01:09,119
getting a little distracted by that

26
00:01:06,960 --> 00:01:11,240
record player and you have any favorites

27
00:01:09,119 --> 00:01:14,119
I have so many um I listen to this

28
00:01:11,240 --> 00:01:18,080
record a lot when I'm working it's

29
00:01:14,119 --> 00:01:20,479
moonchild um it's got that R&B lowii

30
00:01:18,080 --> 00:01:24,320
energy that I think is just very

31
00:01:20,479 --> 00:01:25,840
conducive to work so priorities all

32
00:01:24,320 --> 00:01:27,640
right I'll have to give that a listen

33
00:01:25,840 --> 00:01:29,920
now back to your research one pillar is

34
00:01:27,640 --> 00:01:32,520
data sets how significant is building

35
00:01:29,920 --> 00:01:34,880
representative data sets in today's ml

36
00:01:32,520 --> 00:01:38,119
landscape I think it's one of the most

37
00:01:34,880 --> 00:01:39,759
important questions that we have is like

38
00:01:38,119 --> 00:01:41,840
a lot of the problems that come from

39
00:01:39,759 --> 00:01:44,280
bias um and issues of fairness in

40
00:01:41,840 --> 00:01:46,479
machine learning tie back to having not

41
00:01:44,280 --> 00:01:48,479
representative data sets so how can we

42
00:01:46,479 --> 00:01:51,000
build those data sets that cover you

43
00:01:48,479 --> 00:01:54,119
know the people the axes that we care

44
00:01:51,000 --> 00:01:55,799
about so that the product or tool that

45
00:01:54,119 --> 00:01:57,920
we build is is working the way we want

46
00:01:55,799 --> 00:01:59,439
it to and what are some fundamental

47
00:01:57,920 --> 00:02:01,680
challenges that you've run into doing

48
00:01:59,439 --> 00:02:03,240
this type of work well anytime you're

49
00:02:01,680 --> 00:02:05,320
trying to build something that is

50
00:02:03,240 --> 00:02:07,560
representative it's very hard to figure

51
00:02:05,320 --> 00:02:09,959
out what is not represented so I would

52
00:02:07,560 --> 00:02:11,319
say that is the number one challenge all

53
00:02:09,959 --> 00:02:13,760
right now I've heard of something called

54
00:02:11,319 --> 00:02:17,319
out of distribution methods what are

55
00:02:13,760 --> 00:02:19,280
they oh good question um out of

56
00:02:17,319 --> 00:02:21,720
distribution methods are this kind of

57
00:02:19,280 --> 00:02:24,080
like fueled in machine learning where

58
00:02:21,720 --> 00:02:26,400
people ask okay well I know what is in

59
00:02:24,080 --> 00:02:28,680
my training set um I know what my model

60
00:02:26,400 --> 00:02:30,480
has learned now can I figure out what

61
00:02:28,680 --> 00:02:32,319
the model is going to kind of do poorly

62
00:02:30,480 --> 00:02:34,519
on that maybe is not within that

63
00:02:32,319 --> 00:02:36,280
distribution so it's out of distribution

64
00:02:34,519 --> 00:02:37,360
sometimes we call that anomaly detection

65
00:02:36,280 --> 00:02:39,680
it depends on if you're working with

66
00:02:37,360 --> 00:02:41,480
time series or you know a different data

67
00:02:39,680 --> 00:02:42,879
type and how are you connecting that

68
00:02:41,480 --> 00:02:45,519
with data

69
00:02:42,879 --> 00:02:47,599
collection that is also a really good

70
00:02:45,519 --> 00:02:52,319
question um so I think until maybe

71
00:02:47,599 --> 00:02:54,519
recently we really had these topics very

72
00:02:52,319 --> 00:02:55,840
separated um but it turns out if you

73
00:02:54,519 --> 00:02:58,440
have a sense of the underlying

74
00:02:55,840 --> 00:03:00,200
distribution of your data set um and you

75
00:02:58,440 --> 00:03:03,000
take a partially trained model you can

76
00:03:00,200 --> 00:03:05,840
use that model and those ad distribution

77
00:03:03,000 --> 00:03:07,799
methods to Target direct and iterate

78
00:03:05,840 --> 00:03:09,480
your data collection so that in the long

79
00:03:07,799 --> 00:03:12,799
run you have a much more representative

80
00:03:09,480 --> 00:03:14,239
data set okay I see a big pile of books

81
00:03:12,799 --> 00:03:16,360
behind you as well sorry I keep getting

82
00:03:14,239 --> 00:03:17,799
distracted any favorites here oh I've

83
00:03:16,360 --> 00:03:22,000
got a lot

84
00:03:17,799 --> 00:03:25,280
um let's go with these two uh whereas by

85
00:03:22,000 --> 00:03:27,560
L long Soldier amazing amazing poet she

86
00:03:25,280 --> 00:03:30,319
really knows how to work with sound I

87
00:03:27,560 --> 00:03:34,120
love it and

88
00:03:30,319 --> 00:03:37,480
objectivity got to have it got to love

89
00:03:34,120 --> 00:03:38,760
it you know what time it is it's time to

90
00:03:37,480 --> 00:03:41,239
go to

91
00:03:38,760 --> 00:03:43,560
work speaking of that what's a typical

92
00:03:41,239 --> 00:03:46,720
day like at se- sale you know what I'll

93
00:03:43,560 --> 00:03:50,560
show you let's go on my way to work I

94
00:03:46,720 --> 00:03:52,480
usually listen to audiobooks podcasts or

95
00:03:50,560 --> 00:03:54,560
the occasional Spotify playlist what are

96
00:03:52,480 --> 00:03:58,200
you listening to right now I'm listening

97
00:03:54,560 --> 00:04:00,280
to ku's radio West it is a fantastic

98
00:03:58,200 --> 00:04:01,959
Radio show/ podcast

99
00:04:00,280 --> 00:04:04,159
okay sounds very popular for the East

100
00:04:01,959 --> 00:04:06,560
Coast now another pillar of your

101
00:04:04,159 --> 00:04:08,079
research fundamental ml work super

102
00:04:06,560 --> 00:04:11,079
applicable right now can you explain the

103
00:04:08,079 --> 00:04:14,480
debate on emergent World models um sure

104
00:04:11,079 --> 00:04:17,040
uh large language models um have been

105
00:04:14,480 --> 00:04:18,840
facing this really big question which is

106
00:04:17,040 --> 00:04:20,959
are they just stochastic parrots so

107
00:04:18,840 --> 00:04:22,000
memorizing surface statistics or are

108
00:04:20,959 --> 00:04:24,080
they learning something a little bit

109
00:04:22,000 --> 00:04:27,000
more meaningful about the world uh it's

110
00:04:24,080 --> 00:04:29,000
an ongoing debate like you said so there

111
00:04:27,000 --> 00:04:30,400
you go it's it's been summarized all

112
00:04:29,000 --> 00:04:32,080
right so how do you you think looking at

113
00:04:30,400 --> 00:04:33,400
really specific context is helping

114
00:04:32,080 --> 00:04:37,000
explore this

115
00:04:33,400 --> 00:04:40,639
question um that's a good question uh so

116
00:04:37,000 --> 00:04:42,479
a couple months ago my co-authors and I

117
00:04:40,639 --> 00:04:45,280
presented some work at I CLE we

118
00:04:42,479 --> 00:04:47,039
basically found an emergent World model

119
00:04:45,280 --> 00:04:49,280
uh in a large language model that was

120
00:04:47,039 --> 00:04:50,840
trained from scratch on a fellow moov so

121
00:04:49,280 --> 00:04:54,080
super super tightly constrained

122
00:04:50,840 --> 00:04:56,440
environment and that model was

123
00:04:54,080 --> 00:04:58,440
controllable and causal which is really

124
00:04:56,440 --> 00:05:00,440
exciting um for the future of

125
00:04:58,440 --> 00:05:02,520
interpretability and my my opinion but

126
00:05:00,440 --> 00:05:03,880
it's also just the beginning of the work

127
00:05:02,520 --> 00:05:07,000
so does that mean we're a little bit

128
00:05:03,880 --> 00:05:09,440
closer to deploying safer AI uh we're

129
00:05:07,000 --> 00:05:12,800
definitely not actively de deploying

130
00:05:09,440 --> 00:05:15,600
safer uh ML and uh I would say that we

131
00:05:12,800 --> 00:05:17,320
are also fully not finished with the

132
00:05:15,600 --> 00:05:19,520
interpretability research landscape

133
00:05:17,320 --> 00:05:21,720
although we're getting a lot closer okay

134
00:05:19,520 --> 00:05:23,039
so then with the ubiquity of llms people

135
00:05:21,720 --> 00:05:25,199
are starting to use them to generate

136
00:05:23,039 --> 00:05:27,360
training data sets so what are some

137
00:05:25,199 --> 00:05:31,440
challenges in using them for

138
00:05:27,360 --> 00:05:32,560
that another fantastic question Rachel

139
00:05:31,440 --> 00:05:35,639
um

140
00:05:32,560 --> 00:05:36,960
I I have to say so you know one of the

141
00:05:35,639 --> 00:05:38,600
nice things about large language models

142
00:05:36,960 --> 00:05:41,759
is they're really good at producing

143
00:05:38,600 --> 00:05:43,000
realistic outputs um but they're not so

144
00:05:41,759 --> 00:05:45,240
good at producing realistic

145
00:05:43,000 --> 00:05:49,160
distributions so any model that you

146
00:05:45,240 --> 00:05:52,120
train on that resulting distribution may

147
00:05:49,160 --> 00:05:54,560
have some problems look it's the

148
00:05:52,120 --> 00:05:56,880
gnome I'm going to go to work because I

149
00:05:54,560 --> 00:05:59,120
am now incredibly late from spending

150
00:05:56,880 --> 00:06:01,479
time with you guys I'll see you at the

151
00:05:59,120 --> 00:06:01,479
office

152
00:06:05,880 --> 00:06:13,720
oh my gosh welcome to work oh well thank

153
00:06:09,960 --> 00:06:15,720
you Aspen of course all right Aspen tell

154
00:06:13,720 --> 00:06:18,520
me a little bit about your current work

155
00:06:15,720 --> 00:06:21,080
the AI supply chain what is that that's

156
00:06:18,520 --> 00:06:22,720
a another good question Rachel you're so

157
00:06:21,080 --> 00:06:27,039
full of

158
00:06:22,720 --> 00:06:28,400
them um so up until now all the AI

159
00:06:27,039 --> 00:06:31,599
products that you might interact with

160
00:06:28,400 --> 00:06:34,160
like chat Bots Alexa an AI radiology

161
00:06:31,599 --> 00:06:37,039
assistant were largely built in-house

162
00:06:34,160 --> 00:06:39,720
along one pipeline so data cleaning

163
00:06:37,039 --> 00:06:42,800
development refinement evaluation

164
00:06:39,720 --> 00:06:45,680
training Etc was kind of ordered and

165
00:06:42,800 --> 00:06:46,960
organized by like one entity now it's a

166
00:06:45,680 --> 00:06:49,400
little different the products that you

167
00:06:46,960 --> 00:06:51,080
interact with are basically a bunch of

168
00:06:49,400 --> 00:06:54,879
different things glued together whether

169
00:06:51,080 --> 00:06:57,960
that's data sets models fine-tuned

170
00:06:54,879 --> 00:06:59,800
models the world is changing and we're

171
00:06:57,960 --> 00:07:01,599
thinking of this as a dispers learning

172
00:06:59,800 --> 00:07:04,759
setting where you can have a lot of

173
00:07:01,599 --> 00:07:07,080
different entities influencing the

174
00:07:04,759 --> 00:07:08,960
downstream inferences okay and how does

175
00:07:07,080 --> 00:07:11,400
dispersed learning impact the mitigation

176
00:07:08,960 --> 00:07:13,440
of bias and fairness in AI that's a

177
00:07:11,400 --> 00:07:15,800
really good question so when you have

178
00:07:13,440 --> 00:07:19,280
you know more than

179
00:07:15,800 --> 00:07:21,759
one entity contributing to a downstream

180
00:07:19,280 --> 00:07:24,400
product um and by entity it could be an

181
00:07:21,759 --> 00:07:26,479
organization it could be a person um but

182
00:07:24,400 --> 00:07:28,840
you know you have kind of like this idea

183
00:07:26,479 --> 00:07:31,840
of the downstream product doesn't have

184
00:07:28,840 --> 00:07:34,280
control over up and the Upstream

185
00:07:31,840 --> 00:07:36,039
products that are influencing it um as a

186
00:07:34,280 --> 00:07:37,960
result it's really hard to tell if

187
00:07:36,039 --> 00:07:41,440
something changes upstream and there's

188
00:07:37,960 --> 00:07:43,000
now a failure here who's responsible and

189
00:07:41,440 --> 00:07:45,199
this is a question of like liability and

190
00:07:43,000 --> 00:07:47,720
accountability and it's one of the big

191
00:07:45,199 --> 00:07:50,280
questions that we are trying to explore

192
00:07:47,720 --> 00:07:52,159
with the disperse learning setting and

193
00:07:50,280 --> 00:07:55,360
how could these challenges alter future

194
00:07:52,159 --> 00:08:00,440
developments in ml yeah I mean that's a

195
00:07:55,360 --> 00:08:02,720
really you know big problem um in

196
00:08:00,440 --> 00:08:05,520
disperse learning in the AI supply chain

197
00:08:02,720 --> 00:08:07,560
uh we're now trying to figure out how do

198
00:08:05,520 --> 00:08:09,759
you deal with bias and fairness when

199
00:08:07,560 --> 00:08:13,720
there are many actors that are playing a

200
00:08:09,759 --> 00:08:16,120
role in learning um it's hard you can

201
00:08:13,720 --> 00:08:20,759
have two perfectly Fair models and you

202
00:08:16,120 --> 00:08:21,800
maybe combine them um through um voting

203
00:08:20,759 --> 00:08:23,879
there's a lot of different ways to

204
00:08:21,800 --> 00:08:27,280
combine models but um you can average

205
00:08:23,879 --> 00:08:30,000
their weights for example and the result

206
00:08:27,280 --> 00:08:33,519
you know has this weird sense of like

207
00:08:30,000 --> 00:08:36,479
well who did what and now how do we know

208
00:08:33,519 --> 00:08:39,080
what's wrong um so it challenges

209
00:08:36,479 --> 00:08:42,479
stereotypical debugging strategies um

210
00:08:39,080 --> 00:08:44,800
debiasing um strategies Etc okay big

211
00:08:42,479 --> 00:08:47,440
open problem so I want to shift a little

212
00:08:44,800 --> 00:08:49,440
bit how have all of your non-academic

213
00:08:47,440 --> 00:08:53,040
experiences shaped your research and

214
00:08:49,440 --> 00:08:54,920
policy interests oh my gosh uh well I

215
00:08:53,040 --> 00:08:58,519
mean I think that's a really big

216
00:08:54,920 --> 00:09:00,240
question and one that is hard to answer

217
00:08:58,519 --> 00:09:04,240
uh I would say

218
00:09:00,240 --> 00:09:08,120
I I'm just very passionate about making

219
00:09:04,240 --> 00:09:10,360
people's lives better um and kind of

220
00:09:08,120 --> 00:09:11,720
getting to the heart of Truth and there

221
00:09:10,360 --> 00:09:13,040
are many ways to approach that from a

222
00:09:11,720 --> 00:09:15,120
policy perspective and there are many

223
00:09:13,040 --> 00:09:17,079
ways to approach that from a research

224
00:09:15,120 --> 00:09:18,399
perspective okay now for all of this

225
00:09:17,079 --> 00:09:20,720
work how do you balance the

226
00:09:18,399 --> 00:09:22,480
interdisciplinary nature of projects

227
00:09:20,720 --> 00:09:25,000
especially with Fields like sociology

228
00:09:22,480 --> 00:09:26,760
and anthropology I think finding good

229
00:09:25,000 --> 00:09:29,640
collaborators is an excellent way to do

230
00:09:26,760 --> 00:09:30,839
that um also reading a lot

231
00:09:29,640 --> 00:09:32,519
how did you stay on top of what's

232
00:09:30,839 --> 00:09:34,720
happening in your research field I think

233
00:09:32,519 --> 00:09:36,480
my lab is a really good resource uh and

234
00:09:34,720 --> 00:09:40,959
academic Twitter although I guess it's

235
00:09:36,480 --> 00:09:42,440
called X now um yeah talking to people

236
00:09:40,959 --> 00:09:44,000
it's a good way to do it all right

237
00:09:42,440 --> 00:09:46,640
moving on from the more technical stuff

238
00:09:44,000 --> 00:09:48,120
you moved from Utah to Cambridge for MIT

239
00:09:46,640 --> 00:09:51,120
that must have been quite the transition

240
00:09:48,120 --> 00:09:53,440
big culture shock for sure all right so

241
00:09:51,120 --> 00:09:55,600
did you venture outside of MIT for any

242
00:09:53,440 --> 00:09:58,120
collaborations now that you've been here

243
00:09:55,600 --> 00:10:00,360
you know what I did I got really lucky

244
00:09:58,120 --> 00:10:02,680
with um collaborations at Harvard and

245
00:10:00,360 --> 00:10:05,640
with people at Caltech and art center

246
00:10:02,680 --> 00:10:08,000
and with people at Apple from doing

247
00:10:05,640 --> 00:10:10,120
internships so very happy about those

248
00:10:08,000 --> 00:10:11,839
experiences that's wonderful now

249
00:10:10,120 --> 00:10:14,120
research can obviously be a long and

250
00:10:11,839 --> 00:10:16,120
arduous process so talk to me about some

251
00:10:14,120 --> 00:10:18,120
of the setbacks and what insights you've

252
00:10:16,120 --> 00:10:20,200
gleaned from

253
00:10:18,120 --> 00:10:23,320
that yeah I

254
00:10:20,200 --> 00:10:25,440
mean research is an arduous process and

255
00:10:23,320 --> 00:10:27,240
you often think something will work and

256
00:10:25,440 --> 00:10:29,680
then it doesn't and then you try another

257
00:10:27,240 --> 00:10:32,760
thing and it doesn't and

258
00:10:29,680 --> 00:10:35,920
uh I think one of the biggest things

259
00:10:32,760 --> 00:10:39,240
that you can develop as a researcher is

260
00:10:35,920 --> 00:10:42,560
a sense of resiliency and stubbornness

261
00:10:39,240 --> 00:10:44,920
um and a thick skin as someone once told

262
00:10:42,560 --> 00:10:48,360
me and throughout that process have you

263
00:10:44,920 --> 00:10:50,360
had any amazing mentorships or mentors

264
00:10:48,360 --> 00:10:52,480
you know what I really I definitely have

265
00:10:50,360 --> 00:10:54,680
um I think my current adviser Alexander

266
00:10:52,480 --> 00:10:57,760
majer is an amazing Mentor I got really

267
00:10:54,680 --> 00:11:00,200
lucky in undergrad with having amazing

268
00:10:57,760 --> 00:11:01,920
women mentors that that were professors

269
00:11:00,200 --> 00:11:05,920
that otherwise I probably would not have

270
00:11:01,920 --> 00:11:08,240
even considered Academia an option so uh

271
00:11:05,920 --> 00:11:10,720
yeah how has your approach to research

272
00:11:08,240 --> 00:11:13,160
matured over time you get better at

273
00:11:10,720 --> 00:11:16,120
asking good questions you have a better

274
00:11:13,160 --> 00:11:17,360
sense of the existing literature of what

275
00:11:16,120 --> 00:11:19,760
questions are interesting to the

276
00:11:17,360 --> 00:11:23,720
research community and to yourself and

277
00:11:19,760 --> 00:11:25,120
also how to start answering them so uh

278
00:11:23,720 --> 00:11:29,079
maturity I

279
00:11:25,120 --> 00:11:30,839
guess all right so are there any fun

280
00:11:29,079 --> 00:11:32,519
might Traditions or hidden spots on

281
00:11:30,839 --> 00:11:36,160
campus that Outsiders might not know

282
00:11:32,519 --> 00:11:40,440
about um maybe the muddy it's a

283
00:11:36,160 --> 00:11:44,519
pub uh for MIT Affiliates and it is

284
00:11:40,440 --> 00:11:47,800
quite the space to hear the occasional

285
00:11:44,519 --> 00:11:49,720
witty banter it's quite a scene is there

286
00:11:47,800 --> 00:11:51,760
anything about MIT that you wish you

287
00:11:49,720 --> 00:11:54,920
knew before you started

288
00:11:51,760 --> 00:11:57,600
here yeah um you know what they say

289
00:11:54,920 --> 00:12:01,200
about the hose being a water hose when

290
00:11:57,600 --> 00:12:02,360
you come here uh it's definitely true um

291
00:12:01,200 --> 00:12:03,720
you feel like you're drowning for a

292
00:12:02,360 --> 00:12:06,360
little bit and then you get your feet

293
00:12:03,720 --> 00:12:09,000
under you and you recalibrate and figure

294
00:12:06,360 --> 00:12:12,200
out your people and takes a minute but

295
00:12:09,000 --> 00:12:13,959
it's worth it I can only imagine so

296
00:12:12,200 --> 00:12:15,720
after a long day's work then what helps

297
00:12:13,959 --> 00:12:17,320
you

298
00:12:15,720 --> 00:12:23,279
recharge

299
00:12:17,320 --> 00:12:28,199
um yoga mindfulness sleeping all good

300
00:12:23,279 --> 00:12:29,959
things oh hi hi asan I was wondering

301
00:12:28,199 --> 00:12:31,639
through you Journey do you have any

302
00:12:29,959 --> 00:12:34,920
piece of advice that is the most

303
00:12:31,639 --> 00:12:36,360
impactful find people you like and work

304
00:12:34,920 --> 00:12:38,959
with them although we haven't worked

305
00:12:36,360 --> 00:12:42,600
together yet this isia she's

306
00:12:38,959 --> 00:12:45,160
awesome all right we're going now byee

307
00:12:42,600 --> 00:12:47,040
Aspen favorite CS resource the Internet

308
00:12:45,160 --> 00:12:49,160
movie oh God this is going to be hard

309
00:12:47,040 --> 00:12:50,320
happy as Lazaro all right favorite

310
00:12:49,160 --> 00:12:52,199
restaurant in

311
00:12:50,320 --> 00:12:55,959
Boston I didn't say this was going to be

312
00:12:52,199 --> 00:12:57,760
easy ah um times out isn't a restaurant

313
00:12:55,959 --> 00:12:59,959
but it's got a bunch of options and I

314
00:12:57,760 --> 00:13:04,240
like it okay

315
00:12:59,959 --> 00:13:06,480
artist um my mom class at MIT bills and

316
00:13:04,240 --> 00:13:08,440
billions oh you're doing great aren't

317
00:13:06,480 --> 00:13:10,160
you H this is pretty tough all right if

318
00:13:08,440 --> 00:13:11,399
you're stranded on a desert island three

319
00:13:10,160 --> 00:13:16,320
things you could take with you what's it

320
00:13:11,399 --> 00:13:19,680
going to be ooh uh I guess a fishing

321
00:13:16,320 --> 00:13:21,600
pole sunscreen and a desalinator is that

322
00:13:19,680 --> 00:13:23,279
is that how you say that not what I

323
00:13:21,600 --> 00:13:25,920
would choose but okay what's one thing

324
00:13:23,279 --> 00:13:29,440
that recently inspired you besides you

325
00:13:25,920 --> 00:13:31,480
oh you're too kind um

326
00:13:29,440 --> 00:13:33,320
I'm taking a Harvard class called The

327
00:13:31,480 --> 00:13:35,079
Art of listening and I think it's been

328
00:13:33,320 --> 00:13:37,320
really inspirational it sounds

329
00:13:35,079 --> 00:13:40,040
incredible all right Dream lab anyone

330
00:13:37,320 --> 00:13:42,760
from history who's coming you know what

331
00:13:40,040 --> 00:13:44,560
I don't know those people so I'll

332
00:13:42,760 --> 00:13:46,560
probably stick to the people I'm working

333
00:13:44,560 --> 00:13:48,519
with now because they're wonderful kind

334
00:13:46,560 --> 00:13:51,160
and super smart favorite language to

335
00:13:48,519 --> 00:13:53,759
program in Python all right if you could

336
00:13:51,160 --> 00:13:55,240
have any piece of AI technology automate

337
00:13:53,759 --> 00:13:58,440
something what would it be I want the

338
00:13:55,240 --> 00:14:00,079
perfect laundr mat I would like that as

339
00:13:58,440 --> 00:14:01,880
well let me know if you figure it out

340
00:14:00,079 --> 00:14:04,720
okay if your research project was a dish

341
00:14:01,880 --> 00:14:07,160
what would it be

342
00:14:04,720 --> 00:14:08,759
Sushi I love raw fish all right if you

343
00:14:07,160 --> 00:14:10,600
hadn't pursued a career in Academia in

344
00:14:08,759 --> 00:14:12,680
research what other career path would

345
00:14:10,600 --> 00:14:15,639
you have taken I mean this is still in

346
00:14:12,680 --> 00:14:16,800
research but I loved dolphins and whales

347
00:14:15,639 --> 00:14:19,480
when I was growing up and I really

348
00:14:16,800 --> 00:14:21,480
wanted to be a theologist so probably

349
00:14:19,480 --> 00:14:23,199
that okay so that was the dream as a kid

350
00:14:21,480 --> 00:14:27,160
any other

351
00:14:23,199 --> 00:14:29,519
dreams I mean besides being Mia ham for

352
00:14:27,160 --> 00:14:32,240
those of you who know who she is

353
00:14:29,519 --> 00:14:33,759
no okay based off of the pingpong skills

354
00:14:32,240 --> 00:14:35,519
I think we we might have a little

355
00:14:33,759 --> 00:14:37,920
difficulty there no I'm just kidding

356
00:14:35,519 --> 00:14:40,600
just kidding okay I think you were the

357
00:14:37,920 --> 00:14:42,639
problem it's possible so do you have any

358
00:14:40,600 --> 00:14:44,560
hidden skills or talents that don't

359
00:14:42,639 --> 00:14:47,519
involve ping pong or

360
00:14:44,560 --> 00:14:50,480
soccer I have so many um they're hard to

361
00:14:47,519 --> 00:14:52,440
count probably thrifting though everyone

362
00:14:50,480 --> 00:14:53,800
asks for my advice you're hopping in a

363
00:14:52,440 --> 00:14:55,600
time machine you're going back to any

364
00:14:53,800 --> 00:14:57,680
time period just for a day where would

365
00:14:55,600 --> 00:14:59,440
it be I don't want to go anywhere I

366
00:14:57,680 --> 00:15:02,600
really like um

367
00:14:59,440 --> 00:15:04,560
um it might sound a little wild but I

368
00:15:02,600 --> 00:15:05,959
think that the kind of work that people

369
00:15:04,560 --> 00:15:07,920
are doing and the kind of questions that

370
00:15:05,959 --> 00:15:10,320
people are doing right now are really

371
00:15:07,920 --> 00:15:13,440
important and also I don't want to be a

372
00:15:10,320 --> 00:15:15,839
woman in most any other era so I will

373
00:15:13,440 --> 00:15:17,600
second that what if you could go travel

374
00:15:15,839 --> 00:15:20,320
into the

375
00:15:17,600 --> 00:15:22,199
future can I travel like 200 years in

376
00:15:20,320 --> 00:15:24,480
the future and just see where we're at

377
00:15:22,199 --> 00:15:27,360
I'm hoping climate change is a problem

378
00:15:24,480 --> 00:15:29,160
solved that would be a pipe dream now

379
00:15:27,360 --> 00:15:31,560
what does artistic expression mean to

380
00:15:29,160 --> 00:15:33,560
you it means mindfulness and being

381
00:15:31,560 --> 00:15:35,279
grounded in your world and in your

382
00:15:33,560 --> 00:15:37,440
feelings and in the experiences of

383
00:15:35,279 --> 00:15:38,800
others now what has learning about

384
00:15:37,440 --> 00:15:43,199
machines taught you about the way that

385
00:15:38,800 --> 00:15:44,639
humans learn ooh uh a couple years ago I

386
00:15:43,199 --> 00:15:46,839
read this really interesting paper about

387
00:15:44,639 --> 00:15:50,160
numerosity which is you know how many

388
00:15:46,839 --> 00:15:52,199
objects are in a particular image and

389
00:15:50,160 --> 00:15:53,639
how we can recognize that and they used

390
00:15:52,199 --> 00:15:55,319
a neural network to explore this

391
00:15:53,639 --> 00:15:57,560
question and it had a lot of interesting

392
00:15:55,319 --> 00:16:00,120
implications for how people understand

393
00:15:57,560 --> 00:16:03,440
the count of OB objects all right A lot

394
00:16:00,120 --> 00:16:05,079
of people are fearful of AI are you I

395
00:16:03,440 --> 00:16:06,959
don't think I'm afraid of the same

396
00:16:05,079 --> 00:16:11,519
things that people are afraid of in pop

397
00:16:06,959 --> 00:16:14,360
culture at the moment um so yes and no

398
00:16:11,519 --> 00:16:17,199
what are you most excited about for

399
00:16:14,360 --> 00:16:19,880
AI I'm really excited about this like

400
00:16:17,199 --> 00:16:21,880
growing potential for the applications

401
00:16:19,880 --> 00:16:23,759
of machine learning and things that can

402
00:16:21,880 --> 00:16:25,680
help people so kind of like a feminist

403
00:16:23,759 --> 00:16:28,680
agenda you know um a friend of mine just

404
00:16:25,680 --> 00:16:30,040
did a bunch of work in capturing and I

405
00:16:28,680 --> 00:16:31,720
mean it's a little it's a little sad but

406
00:16:30,040 --> 00:16:33,240
she did a lot of really interesting work

407
00:16:31,720 --> 00:16:35,639
on capturing and highlighting these

408
00:16:33,240 --> 00:16:37,519
instances of femicide and I think that's

409
00:16:35,639 --> 00:16:40,519
incredibly important and it's hard to do

410
00:16:37,519 --> 00:16:41,959
without algorithmic support absolutely

411
00:16:40,519 --> 00:16:43,240
now in 5 years what do you want to be

412
00:16:41,959 --> 00:16:45,959
working

413
00:16:43,240 --> 00:16:48,880
on I hope I'm continuing work that I

414
00:16:45,959 --> 00:16:51,040
feel strongly about and that has impact

415
00:16:48,880 --> 00:16:55,160
um I

416
00:16:51,040 --> 00:16:56,279
hope honestly I I really hope that I am

417
00:16:55,160 --> 00:16:59,240
finding

418
00:16:56,279 --> 00:17:00,920
collaborations that are you know also

419
00:16:59,240 --> 00:17:03,079
interdisciplinary and that feel

420
00:17:00,920 --> 00:17:04,799
motivating to me beautiful now what

421
00:17:03,079 --> 00:17:06,280
advice would you offer to upcoming

422
00:17:04,799 --> 00:17:07,240
researchers trying to get involved in

423
00:17:06,280 --> 00:17:09,720
the

424
00:17:07,240 --> 00:17:11,839
field find someone that you think is

425
00:17:09,720 --> 00:17:14,640
exciting and interesting to talk to and

426
00:17:11,839 --> 00:17:16,959
is in honestly doing interesting things

427
00:17:14,640 --> 00:17:20,120
and literally just have a conversation

428
00:17:16,959 --> 00:17:22,480
with them it's one of the best ways to

429
00:17:20,120 --> 00:17:24,439
learn about a new space and it's also

430
00:17:22,480 --> 00:17:26,559
one of the best ways to be inspired okay

431
00:17:24,439 --> 00:17:29,919
last and final question we'll let you go

432
00:17:26,559 --> 00:17:32,000
where does the name Aspen come from

433
00:17:29,919 --> 00:17:35,000
um my mom likes trees oh I like trees

434
00:17:32,000 --> 00:17:38,000
too all right thanks Aspen we'll see you

435
00:17:35,000 --> 00:17:38,000
later

