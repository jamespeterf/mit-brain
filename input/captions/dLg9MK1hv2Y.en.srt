1
00:00:00,160 --> 00:00:04,560
Today on Chalk Radio, we're talking with

2
00:00:02,080 --> 00:00:06,000
economist John Gruber, whose lecture

3
00:00:04,560 --> 00:00:07,839
videos have been [music] viewed millions

4
00:00:06,000 --> 00:00:09,840
of times on our channel. We talk about

5
00:00:07,839 --> 00:00:11,040
the role of trade-offs in everyday life.

6
00:00:09,840 --> 00:00:12,800
>> Let's say you're dating someone and

7
00:00:11,040 --> 00:00:15,679
you're thinking about marriage. Well,

8
00:00:12,800 --> 00:00:17,520
you basically have a opportunity cost.

9
00:00:15,679 --> 00:00:18,240
>> The intersection of economics and

10
00:00:17,520 --> 00:00:19,520
emotions.

11
00:00:18,240 --> 00:00:21,119
>> Like I said, the standard model of

12
00:00:19,520 --> 00:00:22,880
economics is no room for emotion.

13
00:00:21,119 --> 00:00:24,480
>> Oh, I don't know if I belong in

14
00:00:22,880 --> 00:00:26,720
economics. Then what would you say to

15
00:00:24,480 --> 00:00:28,800
that? and how his interest in economics

16
00:00:26,720 --> 00:00:29,359
is largely driven by his passion for

17
00:00:28,800 --> 00:00:31,920
healthcare.

18
00:00:29,359 --> 00:00:33,280
>> And Louis Manand really inspired me that

19
00:00:31,920 --> 00:00:34,480
we need to not just think about

20
00:00:33,280 --> 00:00:35,760
something of political science as an

21
00:00:34,480 --> 00:00:37,440
abstract concept, but really get our

22
00:00:35,760 --> 00:00:38,559
hands dirty and think about how we can

23
00:00:37,440 --> 00:00:41,040
make the world a better place.

24
00:00:38,559 --> 00:00:41,840
>> And we asked the question, can AI fix

25
00:00:41,040 --> 00:00:43,360
the economy?

26
00:00:41,840 --> 00:00:44,320
>> First of all, you should feel bad for my

27
00:00:43,360 --> 00:00:44,879
students who have to read this

28
00:00:44,320 --> 00:00:45,280
handwriting.

29
00:00:44,879 --> 00:00:46,160
>> Okay.

30
00:00:45,280 --> 00:00:48,719
>> But I wrote uh

31
00:00:46,160 --> 00:00:51,360
>> find out his answer and much more on

32
00:00:48,719 --> 00:00:52,800
this episode of Chalk Radio. John, thank

33
00:00:51,360 --> 00:00:54,079
you for being here today.

34
00:00:52,800 --> 00:00:56,160
>> Great to be here, Sarah. Thanks for

35
00:00:54,079 --> 00:00:57,840
having me. Oh, you're so welcome. Uh,

36
00:00:56,160 --> 00:00:59,520
you're an economist, so obviously you

37
00:00:57,840 --> 00:01:01,840
looked at the opportunity costs of

38
00:00:59,520 --> 00:01:03,440
joining me here today. Um, so what are

39
00:01:01,840 --> 00:01:04,080
some of the trade-offs you made to be

40
00:01:03,440 --> 00:01:05,439
here?

41
00:01:04,080 --> 00:01:07,520
>> Uh, it's a great question because that's

42
00:01:05,439 --> 00:01:10,640
how economists look at everything. Uh,

43
00:01:07,520 --> 00:01:12,880
we recognize that every action has an

44
00:01:10,640 --> 00:01:14,320
alternative and that's part of why they

45
00:01:12,880 --> 00:01:16,320
call us the dismal science because we

46
00:01:14,320 --> 00:01:17,439
say nothing's free. Um, every time you

47
00:01:16,320 --> 00:01:19,439
make a decision, there's something else

48
00:01:17,439 --> 00:01:21,439
you could be doing instead. So, let's

49
00:01:19,439 --> 00:01:23,520
take my decision to come here today. I

50
00:01:21,439 --> 00:01:25,520
could be doing economic research, which

51
00:01:23,520 --> 00:01:27,040
I love. Uh, I could be back doing some

52
00:01:25,520 --> 00:01:28,479
of the administrative work I have to do

53
00:01:27,040 --> 00:01:30,400
as chairman of the MIT economics

54
00:01:28,479 --> 00:01:32,320
department. I could be home walking my

55
00:01:30,400 --> 00:01:34,240
granddog who's at my house right now.

56
00:01:32,320 --> 00:01:35,759
So, those are the opportunity costs.

57
00:01:34,240 --> 00:01:40,000
What are the benefits? Well, the benefit

58
00:01:35,759 --> 00:01:41,439
is that I feel that uh academia today

59
00:01:40,000 --> 00:01:43,040
suffers from a lack of communication

60
00:01:41,439 --> 00:01:45,200
with the public that we are not doing a

61
00:01:43,040 --> 00:01:47,200
good enough job as academics talking to

62
00:01:45,200 --> 00:01:49,520
the public, explaining why what we do is

63
00:01:47,200 --> 00:01:53,119
so important to their lives and to our

64
00:01:49,520 --> 00:01:54,799
country. I mean, as a consumer in

65
00:01:53,119 --> 00:01:56,720
society, I certainly have a lot of

66
00:01:54,799 --> 00:01:59,680
questions about economics that I'm

67
00:01:56,720 --> 00:02:01,600
hoping you can help me unravel today as

68
00:01:59,680 --> 00:02:03,200
we get into this. But how did you find

69
00:02:01,600 --> 00:02:05,200
your way to economics?

70
00:02:03,200 --> 00:02:06,799
>> Well, actually, it's a great chance to

71
00:02:05,200 --> 00:02:08,239
talk about you as ring something that

72
00:02:06,799 --> 00:02:09,920
represented my journey to economics.

73
00:02:08,239 --> 00:02:14,080
Now, my journey to economics started as

74
00:02:09,920 --> 00:02:15,680
a freshman at MIT in 1983. Um, I was one

75
00:02:14,080 --> 00:02:19,280
of those kids who was good at math but

76
00:02:15,680 --> 00:02:22,959
didn't really love math. Um and I took

77
00:02:19,280 --> 00:02:26,720
1401 which is the course I now teach and

78
00:02:22,959 --> 00:02:28,400
uh I uh fell in love with it. I said wow

79
00:02:26,720 --> 00:02:29,920
you can use math to do really

80
00:02:28,400 --> 00:02:31,440
interesting and worthwhile things and

81
00:02:29,920 --> 00:02:34,640
not just proofs that I found abstract

82
00:02:31,440 --> 00:02:36,480
but actually as I say in 1401 you can

83
00:02:34,640 --> 00:02:38,560
actually do optimization problems that

84
00:02:36,480 --> 00:02:40,000
solve people's lives. Uh and that was

85
00:02:38,560 --> 00:02:41,599
very exciting for me. So my object I

86
00:02:40,000 --> 00:02:44,080
brought is the MIT department of

87
00:02:41,599 --> 00:02:45,840
economics hat. Uh I I didn't have

88
00:02:44,080 --> 00:02:47,519
anything from 1983 I could bring but

89
00:02:45,840 --> 00:02:49,760
this sort of represents the key step in

90
00:02:47,519 --> 00:02:51,120
my journey was taking 1401 and just

91
00:02:49,760 --> 00:02:53,120
obviously what a thrill it was to come

92
00:02:51,120 --> 00:02:54,560
back in 1992 and get to join and be part

93
00:02:53,120 --> 00:02:55,760
of this department and now ultimately

94
00:02:54,560 --> 00:02:56,560
teach the course that turned me on to

95
00:02:55,760 --> 00:02:59,840
economics.

96
00:02:56,560 --> 00:03:01,840
>> Yeah. Um do you have a teacher in mind

97
00:02:59,840 --> 00:03:03,599
that really helped hook you on to

98
00:03:01,840 --> 00:03:06,319
economics and helped you show how

99
00:03:03,599 --> 00:03:07,120
powerful these applications can be in

100
00:03:06,319 --> 00:03:08,879
the real world?

101
00:03:07,120 --> 00:03:10,720
>> You know there were so many great

102
00:03:08,879 --> 00:03:12,480
undergrad teachers I had at MIT. I

103
00:03:10,720 --> 00:03:15,040
really uh many of them and then became

104
00:03:12,480 --> 00:03:17,840
my colleagues Jim Purba who taught me

105
00:03:15,040 --> 00:03:20,720
1430 really turned me on to statistics.

106
00:03:17,840 --> 00:03:23,040
Um I had the wonderful late Rudy Dorbush

107
00:03:20,720 --> 00:03:24,800
teach me international economics. Bob

108
00:03:23,040 --> 00:03:28,239
Solo, Nobel Prize wing economist taught

109
00:03:24,800 --> 00:03:30,319
me macroeconomics. Uh I did Europs with

110
00:03:28,239 --> 00:03:32,159
uh professors Dick Eouse and Peter Teman

111
00:03:30,319 --> 00:03:33,920
and Rudy Dornbush. So I just had

112
00:03:32,159 --> 00:03:35,840
wonderful mentors undergraduate here and

113
00:03:33,920 --> 00:03:37,519
then went on to grad school at Harvard

114
00:03:35,840 --> 00:03:39,120
and had great mentors there too. So I've

115
00:03:37,519 --> 00:03:40,400
just been very lucky. We have an

116
00:03:39,120 --> 00:03:41,840
incredible economics community here in

117
00:03:40,400 --> 00:03:42,319
Cambridge and I've been lucky to benefit

118
00:03:41,840 --> 00:03:44,720
from that.

119
00:03:42,319 --> 00:03:47,599
>> So, let's talk about economics and human

120
00:03:44,720 --> 00:03:49,599
decision-making. Um, you've noted in one

121
00:03:47,599 --> 00:03:51,760
of your lectures that economics is

122
00:03:49,599 --> 00:03:53,360
sometimes called the right-wing science

123
00:03:51,760 --> 00:03:55,920
and I'm wondering if you could unpack

124
00:03:53,360 --> 00:03:57,920
that for me and let me know if there's

125
00:03:55,920 --> 00:03:59,360
more nuance to that thing.

126
00:03:57,920 --> 00:04:01,360
>> It's not really called that. It's sort

127
00:03:59,360 --> 00:04:02,640
of how I introduce to students that it's

128
00:04:01,360 --> 00:04:03,760
sort of a fundamentally right-wing

129
00:04:02,640 --> 00:04:06,400
science. What do I mean by that?

130
00:04:03,760 --> 00:04:09,040
>> Yeah. What I mean by that is economics

131
00:04:06,400 --> 00:04:11,599
has a set of basic models that are

132
00:04:09,040 --> 00:04:14,319
unbelievably powerful in describing much

133
00:04:11,599 --> 00:04:16,799
of the world and those basic models make

134
00:04:14,319 --> 00:04:19,120
a series of assumptions under which the

135
00:04:16,799 --> 00:04:21,280
market knows best. So take the basic

136
00:04:19,120 --> 00:04:22,960
core assumptions of economic models and

137
00:04:21,280 --> 00:04:24,240
run them through. They deliver the

138
00:04:22,960 --> 00:04:25,600
outcome. The best thing you can do is

139
00:04:24,240 --> 00:04:28,080
just let the market run and get out of

140
00:04:25,600 --> 00:04:30,080
the way. Now obviously in reality that's

141
00:04:28,080 --> 00:04:31,440
wrong. And what I do in the second half

142
00:04:30,080 --> 00:04:33,440
of my course is teach all the reasons

143
00:04:31,440 --> 00:04:36,080
why that's wrong. And then I go on and

144
00:04:33,440 --> 00:04:38,000
teach another course at MIT called 1441

145
00:04:36,080 --> 00:04:40,639
public economics where I spend much more

146
00:04:38,000 --> 00:04:42,639
time talking about these market failures

147
00:04:40,639 --> 00:04:44,639
and the role the government has in in a

148
00:04:42,639 --> 00:04:46,639
properly functioning economy. But the

149
00:04:44,639 --> 00:04:48,639
point is where economics start is this

150
00:04:46,639 --> 00:04:50,880
notion that Adam Smith had of the

151
00:04:48,639 --> 00:04:53,280
invisible hand. The notion the market is

152
00:04:50,880 --> 00:04:55,040
the best way to allocate goods across

153
00:04:53,280 --> 00:04:58,240
people and allocate production across

154
00:04:55,040 --> 00:04:59,759
firms which is absolutely true. the

155
00:04:58,240 --> 00:05:02,479
market is a much better allocation

156
00:04:59,759 --> 00:05:05,280
mechanism than is say a communist

157
00:05:02,479 --> 00:05:07,120
government. That said, an unfettered

158
00:05:05,280 --> 00:05:08,720
market will not get it right because

159
00:05:07,120 --> 00:05:11,120
there are a lot of failures in those

160
00:05:08,720 --> 00:05:13,199
markets. And that's why economists tend

161
00:05:11,120 --> 00:05:15,280
to think that the right answer is sort

162
00:05:13,199 --> 00:05:17,039
of a if you'll think about if you think

163
00:05:15,280 --> 00:05:19,280
about an economy as bowling, think of

164
00:05:17,039 --> 00:05:21,199
sort of capitalism with with gutter

165
00:05:19,280 --> 00:05:23,199
guards. The idea is that there should be

166
00:05:21,199 --> 00:05:24,639
freedom to bowl the ball, but there

167
00:05:23,199 --> 00:05:26,720
should be some protection against really

168
00:05:24,639 --> 00:05:29,440
damaging going to the gutter and and

169
00:05:26,720 --> 00:05:30,880
ending a disastrous outcome.

170
00:05:29,440 --> 00:05:33,759
>> That's a really interesting way to think

171
00:05:30,880 --> 00:05:36,160
about it. Um, talking about models

172
00:05:33,759 --> 00:05:38,479
themselves, like what are some like

173
00:05:36,160 --> 00:05:42,160
tricky human behaviors that economic

174
00:05:38,479 --> 00:05:43,039
models can help us make sense of those

175
00:05:42,160 --> 00:05:46,240
decisions?

176
00:05:43,039 --> 00:05:46,720
>> Let me give you uh a couple of examples.

177
00:05:46,240 --> 00:05:51,120
Okay.

178
00:05:46,720 --> 00:05:53,840
>> So, one great example is when I give in

179
00:05:51,120 --> 00:05:54,560
lecture about um concert tickets.

180
00:05:53,840 --> 00:05:56,160
>> Okay.

181
00:05:54,560 --> 00:05:58,960
>> So, I talk in lecture about the time

182
00:05:56,160 --> 00:06:00,320
that I like the band Journey. Uh Journey

183
00:05:58,960 --> 00:06:01,680
wasn't really Journey. It's like a

184
00:06:00,320 --> 00:06:03,280
Journey cover band. They have like the

185
00:06:01,680 --> 00:06:05,360
new singer. And I bought tickets and it

186
00:06:03,280 --> 00:06:06,880
was $250 for my wife and I. And then I

187
00:06:05,360 --> 00:06:09,039
looked at list and I thought, you know,

188
00:06:06,880 --> 00:06:09,840
I really don't want to go. I'm not that

189
00:06:09,039 --> 00:06:10,160
excited to go.

190
00:06:09,840 --> 00:06:11,600
>> Okay.

191
00:06:10,160 --> 00:06:13,759
>> So, I'm going to sell these tickets.

192
00:06:11,600 --> 00:06:15,759
Now, a common intuition would be, well,

193
00:06:13,759 --> 00:06:17,039
you paid 250, you should ask 250, and

194
00:06:15,759 --> 00:06:18,880
you should be upset if you get less than

195
00:06:17,039 --> 00:06:19,440
250. But that would be wrong.

196
00:06:18,880 --> 00:06:21,440
>> Okay, how's that?

197
00:06:19,440 --> 00:06:23,199
>> Why is that wrong? Cuz the 250 is gone.

198
00:06:21,440 --> 00:06:24,479
Economics, we're all about looking

199
00:06:23,199 --> 00:06:26,319
forward to the next step in the

200
00:06:24,479 --> 00:06:27,759
decision. The 250 is gone. The next step

201
00:06:26,319 --> 00:06:30,560
is

202
00:06:27,759 --> 00:06:33,280
>> how much would I be willing to pay to

203
00:06:30,560 --> 00:06:36,880
still go? So, in other words, if I say,

204
00:06:33,280 --> 00:06:38,160
"Look, I wouldn't go even if I I

205
00:06:36,880 --> 00:06:39,600
wouldn't bend any money to go to this

206
00:06:38,160 --> 00:06:41,440
concert, then I should be willing to

207
00:06:39,600 --> 00:06:43,280
accept any price over a dollar for those

208
00:06:41,440 --> 00:06:44,880
tickets because I get zero value from

209
00:06:43,280 --> 00:06:47,039
going. So, as long as I get a dollar for

210
00:06:44,880 --> 00:06:48,160
the tickets, I'm better off. The 250 is

211
00:06:47,039 --> 00:06:49,520
gone. Okay, I already made that

212
00:06:48,160 --> 00:06:51,280
decision. That's gone." So, really, I

213
00:06:49,520 --> 00:06:54,160
should say to myself, well, if I get

214
00:06:51,280 --> 00:06:55,919
less than say 100, what I say at that

215
00:06:54,160 --> 00:06:57,199
point, I'd rather go.

216
00:06:55,919 --> 00:06:58,319
>> Uh, and that's the way I should make

217
00:06:57,199 --> 00:06:59,840
that decision. That's not very

218
00:06:58,319 --> 00:07:01,599
intuitive. That's tricky, right?

219
00:06:59,840 --> 00:07:03,440
>> No. And it's also like I'm not sure

220
00:07:01,599 --> 00:07:06,720
that's real life. Like let's say I spend

221
00:07:03,440 --> 00:07:08,000
250, I can't go for some some reason,

222
00:07:06,720 --> 00:07:09,039
right? That doesn't mean I don't want

223
00:07:08,000 --> 00:07:10,800
the 250 back.

224
00:07:09,039 --> 00:07:13,599
>> You want the 250, but it also doesn't

225
00:07:10,800 --> 00:07:16,160
mean that you should feel like the 250

226
00:07:13,599 --> 00:07:17,919
is the right answer at that point. If

227
00:07:16,160 --> 00:07:19,919
you literally can't go, then the right

228
00:07:17,919 --> 00:07:22,479
answer is whatever the market will bear.

229
00:07:19,919 --> 00:07:24,160
>> And uh but the point is, I think a lot

230
00:07:22,479 --> 00:07:25,520
of people will kick themselves. Why did

231
00:07:24,160 --> 00:07:26,800
I buy those tickets? I can't go. Well,

232
00:07:25,520 --> 00:07:28,319
if you can't go for a reason you didn't

233
00:07:26,800 --> 00:07:29,680
anticipate and you get less, that's not

234
00:07:28,319 --> 00:07:31,840
your fault. That's where the market is.

235
00:07:29,680 --> 00:07:34,240
>> Mhm. I mean, I find it so interesting to

236
00:07:31,840 --> 00:07:35,039
think about how emotions and economics

237
00:07:34,240 --> 00:07:36,319
intersect.

238
00:07:35,039 --> 00:07:37,919
>> Like I said, the standard model of

239
00:07:36,319 --> 00:07:39,759
economics is no room for emotion.

240
00:07:37,919 --> 00:07:41,440
>> Oh, I don't know if I belong in

241
00:07:39,759 --> 00:07:43,759
economics. Then what would you say to

242
00:07:41,440 --> 00:07:45,039
>> you do you do because

243
00:07:43,759 --> 00:07:46,800
>> here's the way I like to think about

244
00:07:45,039 --> 00:07:48,160
building up economics. There's a basic

245
00:07:46,800 --> 00:07:50,800
model that doesn't have room for

246
00:07:48,160 --> 00:07:52,160
emotion. It hasn't room for preferences.

247
00:07:50,800 --> 00:07:54,479
You can like one thing more than

248
00:07:52,160 --> 00:07:56,639
another, but that's all those

249
00:07:54,479 --> 00:07:59,039
preferences are all optimized. You

250
00:07:56,639 --> 00:08:00,720
choose the best possible outcome. in a

251
00:07:59,039 --> 00:08:01,280
calm rational way that doesn't feature

252
00:08:00,720 --> 00:08:02,240
emotions.

253
00:08:01,280 --> 00:08:05,199
>> Okay,

254
00:08:02,240 --> 00:08:06,879
>> that model is unbelievably powerful.

255
00:08:05,199 --> 00:08:08,639
>> So much of the world can be described by

256
00:08:06,879 --> 00:08:08,960
that, but a lot can't.

257
00:08:08,639 --> 00:08:10,479
>> Yeah.

258
00:08:08,960 --> 00:08:12,080
>> So, what we do in economics, we start

259
00:08:10,479 --> 00:08:13,840
with that basic core model, that

260
00:08:12,080 --> 00:08:15,280
right-wing model, if you will. Okay. And

261
00:08:13,840 --> 00:08:16,639
then we build on it. And when we build

262
00:08:15,280 --> 00:08:19,440
on it through the field of behavioral

263
00:08:16,639 --> 00:08:21,599
economics, which is bringing psychology

264
00:08:19,440 --> 00:08:22,479
into economics and saying emotions

265
00:08:21,599 --> 00:08:24,639
matter,

266
00:08:22,479 --> 00:08:26,319
>> problems making decisions matter, people

267
00:08:24,639 --> 00:08:28,560
aren't able to control their actions in

268
00:08:26,319 --> 00:08:31,280
a way they'd like to. We build that in

269
00:08:28,560 --> 00:08:32,479
and in some sense what I teach in 1401

270
00:08:31,280 --> 00:08:33,120
is a scaffolding

271
00:08:32,479 --> 00:08:35,279
>> okay

272
00:08:33,120 --> 00:08:38,240
>> of a basic model that by itself does

273
00:08:35,279 --> 00:08:39,680
quite well but you can build on to

274
00:08:38,240 --> 00:08:40,399
really explain more and more of human

275
00:08:39,680 --> 00:08:42,240
behavior.

276
00:08:40,399 --> 00:08:44,320
>> You know the statistician George box

277
00:08:42,240 --> 00:08:47,600
said that all models are wrong but some

278
00:08:44,320 --> 00:08:49,120
are useful. The point of a model is that

279
00:08:47,600 --> 00:08:50,640
it's not a description. It's not a

280
00:08:49,120 --> 00:08:51,760
perfect description of reality. It was a

281
00:08:50,640 --> 00:08:52,800
perfect description of reality it

282
00:08:51,760 --> 00:08:55,680
wouldn't be a model. It would be

283
00:08:52,800 --> 00:08:58,160
reality. So are you saying that like

284
00:08:55,680 --> 00:09:00,000
those of us out in the world not in an

285
00:08:58,160 --> 00:09:01,600
economics department might be kind of

286
00:09:00,000 --> 00:09:01,920
using these models without thinking

287
00:09:01,600 --> 00:09:03,279
about

288
00:09:01,920 --> 00:09:04,640
>> Oh we are absolutely using these models.

289
00:09:03,279 --> 00:09:08,080
Part of what is exciting about taking

290
00:09:04,640 --> 00:09:10,000
economics is it opens your eyes to

291
00:09:08,080 --> 00:09:12,959
realize the things you're already doing

292
00:09:10,000 --> 00:09:14,399
>> and puts a framework to be consistent in

293
00:09:12,959 --> 00:09:16,320
how you apply that.

294
00:09:14,399 --> 00:09:18,000
>> So that's really what what we're doing

295
00:09:16,320 --> 00:09:19,839
is sort of it really is innate human

296
00:09:18,000 --> 00:09:20,959
nature in many ways that we're modeling

297
00:09:19,839 --> 00:09:22,560
like I said initially without the

298
00:09:20,959 --> 00:09:24,560
emotion. We add that in later.

299
00:09:22,560 --> 00:09:26,720
>> Um, but basically it's it's a set of

300
00:09:24,560 --> 00:09:28,399
tools that help you sort of understand

301
00:09:26,720 --> 00:09:29,760
why people do what they do.

302
00:09:28,399 --> 00:09:31,120
>> I can imagine that would be really

303
00:09:29,760 --> 00:09:32,480
important in business.

304
00:09:31,120 --> 00:09:34,480
>> It's very important in business. It's

305
00:09:32,480 --> 00:09:36,240
very important in life. I'm I'm I'm an

306
00:09:34,480 --> 00:09:37,600
economic imperialist. There's no

307
00:09:36,240 --> 00:09:39,279
decision you can talk to me about that

308
00:09:37,600 --> 00:09:40,480
can't you can't bring an economic model.

309
00:09:39,279 --> 00:09:41,680
One of the most famous economists was

310
00:09:40,480 --> 00:09:43,519
Gary Becker. He was at the University of

311
00:09:41,680 --> 00:09:45,600
Chicago and Gary Becker developed the

312
00:09:43,519 --> 00:09:47,360
economic models of love, economic models

313
00:09:45,600 --> 00:09:49,760
of marriage, economic models of crime,

314
00:09:47,360 --> 00:09:51,760
economic models of discrimination. All

315
00:09:49,760 --> 00:09:52,800
things which really economics has a

316
00:09:51,760 --> 00:09:54,399
framework to put on.

317
00:09:52,800 --> 00:09:55,600
>> What? I haven't really thought about it

318
00:09:54,399 --> 00:09:57,839
in that way before.

319
00:09:55,600 --> 00:09:58,560
>> Well, think about an economic model of

320
00:09:57,839 --> 00:09:58,959
marriage.

321
00:09:58,560 --> 00:10:00,160
>> Okay,

322
00:09:58,959 --> 00:10:01,920
>> let's say you're dating someone and

323
00:10:00,160 --> 00:10:04,959
you're thinking about marriage. Well,

324
00:10:01,920 --> 00:10:06,240
you basically have a opportunity cost

325
00:10:04,959 --> 00:10:09,120
>> of that marriage and you have a

326
00:10:06,240 --> 00:10:10,560
trade-off of you are gaining certain

327
00:10:09,120 --> 00:10:12,240
things by being married and giving up

328
00:10:10,560 --> 00:10:13,839
certain things by being married. And

329
00:10:12,240 --> 00:10:15,920
economics can bring a framework to help

330
00:10:13,839 --> 00:10:17,519
you evaluate that decision.

331
00:10:15,920 --> 00:10:19,040
>> Hm, this is interesting. Yeah, I'm going

332
00:10:17,519 --> 00:10:20,560
to have to give this some thought. Uh

333
00:10:19,040 --> 00:10:22,880
you've especially thought a lot about

334
00:10:20,560 --> 00:10:25,200
how economics can be applied in health

335
00:10:22,880 --> 00:10:26,640
care. Yeah. Like you're basically a

336
00:10:25,200 --> 00:10:26,959
healthc care economist. Would that be

337
00:10:26,640 --> 00:10:29,200
the right

338
00:10:26,959 --> 00:10:32,959
>> a lot of what I do for my time. Yeah.

339
00:10:29,200 --> 00:10:36,320
>> So what are some ways that you're using

340
00:10:32,959 --> 00:10:40,160
economics to think about the inequities

341
00:10:36,320 --> 00:10:41,920
in our health care system and how models

342
00:10:40,160 --> 00:10:44,000
can help us address some of those

343
00:10:41,920 --> 00:10:45,040
inequities? Can you like give us some

344
00:10:44,000 --> 00:10:47,360
tangible examples?

345
00:10:45,040 --> 00:10:50,079
>> Yeah. So so here would be a tangible

346
00:10:47,360 --> 00:10:51,760
example. Um, a tangible example would be

347
00:10:50,079 --> 00:10:53,120
we have many uninsured individuals in

348
00:10:51,760 --> 00:10:53,680
America. About 30 million uninsured

349
00:10:53,120 --> 00:10:54,560
individuals.

350
00:10:53,680 --> 00:10:56,959
>> Yeah.

351
00:10:54,560 --> 00:10:58,480
>> And you might ask, well, is that worth

352
00:10:56,959 --> 00:11:00,720
doing anything about? We probably have

353
00:10:58,480 --> 00:11:02,640
more than 30 million people who don't

354
00:11:00,720 --> 00:11:03,680
have beautiful flat screen televisions.

355
00:11:02,640 --> 00:11:06,399
Are we really going to do something

356
00:11:03,680 --> 00:11:07,760
about that? I don't think we should. So,

357
00:11:06,399 --> 00:11:09,600
why is health insurance different from a

358
00:11:07,760 --> 00:11:12,160
flat screen television? And what

359
00:11:09,600 --> 00:11:14,480
economics tells is different because the

360
00:11:12,160 --> 00:11:16,959
market for health care suffers from a

361
00:11:14,480 --> 00:11:18,800
set of market failures that the market

362
00:11:16,959 --> 00:11:21,040
for flat screen televisions doesn't.

363
00:11:18,800 --> 00:11:22,800
Flat screen televisions is a basic 1401

364
00:11:21,040 --> 00:11:25,040
model. You got supply, demand, the

365
00:11:22,800 --> 00:11:26,720
market works. Health care, there's a

366
00:11:25,040 --> 00:11:28,720
number of problems. For example, if I

367
00:11:26,720 --> 00:11:30,079
don't get health insurance and as a

368
00:11:28,720 --> 00:11:31,920
result I get sick and I come in here

369
00:11:30,079 --> 00:11:33,839
today and I cough on you and you get

370
00:11:31,920 --> 00:11:35,760
sick, then I've imposed a cost on you

371
00:11:33,839 --> 00:11:37,760
that I'm not incurring. We call that an

372
00:11:35,760 --> 00:11:39,600
externality. That's a problem with the

373
00:11:37,760 --> 00:11:41,680
healthare market. Another problem with

374
00:11:39,600 --> 00:11:44,800
the health care market is that people

375
00:11:41,680 --> 00:11:46,240
might not understand the risk of getting

376
00:11:44,800 --> 00:11:48,160
sick. Many people, we have what we call

377
00:11:46,240 --> 00:11:49,760
the young invincibles. People who are

378
00:11:48,160 --> 00:11:51,600
35, they're like, I'll never get sick.

379
00:11:49,760 --> 00:11:52,800
Especially guys, you know, I can't get

380
00:11:51,600 --> 00:11:54,240
pregnant. I'm never getting sick. What

381
00:11:52,800 --> 00:11:55,839
do I need health insurance for? They're

382
00:11:54,240 --> 00:11:57,600
wrong. They can get by a car. They can

383
00:11:55,839 --> 00:11:59,040
get sick for many reasons. That's a

384
00:11:57,600 --> 00:11:59,920
failure of information or a failure of

385
00:11:59,040 --> 00:12:03,600
reasoning.

386
00:11:59,920 --> 00:12:06,560
>> Um, we have failures on the market side.

387
00:12:03,600 --> 00:12:08,320
for example, um, economic models work

388
00:12:06,560 --> 00:12:09,600
best when markets are competitive, when

389
00:12:08,320 --> 00:12:11,920
there's many competitors trying to

390
00:12:09,600 --> 00:12:13,360
compete for your business. Well, that's

391
00:12:11,920 --> 00:12:15,279
not true in healthcare. If you're a

392
00:12:13,360 --> 00:12:18,160
hospital, if you're a hospital on

393
00:12:15,279 --> 00:12:20,320
Nantucket, you got a monopoly because

394
00:12:18,160 --> 00:12:21,920
you're the hospital. Uh, you're there's

395
00:12:20,320 --> 00:12:23,440
no competition. People aren't going to

396
00:12:21,920 --> 00:12:26,320
take a ferry to get their heart attack

397
00:12:23,440 --> 00:12:28,399
dealt with. So, for all these reasons,

398
00:12:26,320 --> 00:12:30,000
we need to bring in the tools, other

399
00:12:28,399 --> 00:12:31,839
tools to help think about how we fix

400
00:12:30,000 --> 00:12:33,279
problems in the healthcare markets. Can

401
00:12:31,839 --> 00:12:36,160
you tell me a little bit about your

402
00:12:33,279 --> 00:12:38,480
involvement in the Affordable Care Act?

403
00:12:36,160 --> 00:12:41,440
>> Yeah. So, um, the proper way to make

404
00:12:38,480 --> 00:12:43,680
government policy is to understand as

405
00:12:41,440 --> 00:12:45,440
much as possible what affects changes,

406
00:12:43,680 --> 00:12:46,880
what effects the policy will do, to not

407
00:12:45,440 --> 00:12:48,240
fly by the seat of your pants, but to

408
00:12:46,880 --> 00:12:48,639
really have a model,

409
00:12:48,240 --> 00:12:50,240
>> yes,

410
00:12:48,639 --> 00:12:52,880
>> of how it will work.

411
00:12:50,240 --> 00:12:54,800
>> I developed that a model like that. So

412
00:12:52,880 --> 00:12:57,120
basically I developed what was called a

413
00:12:54,800 --> 00:12:59,040
giant computer micro simulation model

414
00:12:57,120 --> 00:13:01,040
which essentially was a fancy way for

415
00:12:59,040 --> 00:13:03,120
saying was a lot of computer code that

416
00:13:01,040 --> 00:13:05,120
could basically use the best available

417
00:13:03,120 --> 00:13:07,440
economic evidence to say to a policy

418
00:13:05,120 --> 00:13:09,040
maker if you do policy X Y will happen.

419
00:13:07,440 --> 00:13:10,399
If you do this policy this many people

420
00:13:09,040 --> 00:13:14,800
gain health insurance coverage this is

421
00:13:10,399 --> 00:13:16,639
what it'll cost. Um that model was um

422
00:13:14,800 --> 00:13:18,240
used by Governor Romney state of

423
00:13:16,639 --> 00:13:19,680
Massachusetts to help figure out the

424
00:13:18,240 --> 00:13:21,519
best way to set up our healthcare reform

425
00:13:19,680 --> 00:13:23,040
here in Massachusetts. Once we'd set it

426
00:13:21,519 --> 00:13:24,079
up, I then got to be on the board that

427
00:13:23,040 --> 00:13:26,079
actually implemented the healthcare

428
00:13:24,079 --> 00:13:28,160
reform. So I got to actually go from

429
00:13:26,079 --> 00:13:29,519
abstract academic to actually making

430
00:13:28,160 --> 00:13:31,279
decisions like what health insurance

431
00:13:29,519 --> 00:13:33,279
should people get, which was very eye

432
00:13:31,279 --> 00:13:35,120
opening. I did that for a decade and

433
00:13:33,279 --> 00:13:37,120
then I was able to bring that model to

434
00:13:35,120 --> 00:13:38,399
bear on the national level uh working

435
00:13:37,120 --> 00:13:40,000
for both President Obama and the

436
00:13:38,399 --> 00:13:41,200
Congress to help them understand the

437
00:13:40,000 --> 00:13:42,639
trade-offs and setting the afford

438
00:13:41,200 --> 00:13:44,160
setting up the Affordable Care Act.

439
00:13:42,639 --> 00:13:45,600
>> What did that feel like for you as a

440
00:13:44,160 --> 00:13:47,839
person? That's like a tremendous

441
00:13:45,600 --> 00:13:49,279
responsibility that affects

442
00:13:47,839 --> 00:13:50,480
>> Well, I mean this is this is this is

443
00:13:49,279 --> 00:13:51,839
what's wonderful about not being the

444
00:13:50,480 --> 00:13:52,240
politician and being the adviser.

445
00:13:51,839 --> 00:13:53,360
>> Yeah.

446
00:13:52,240 --> 00:13:55,440
>> Which is [clears throat]

447
00:13:53,360 --> 00:13:57,040
it's not that much responsibility

448
00:13:55,440 --> 00:13:58,399
because I mean basically it's an

449
00:13:57,040 --> 00:14:00,959
exciting opportunity to be involved. But

450
00:13:58,399 --> 00:14:02,560
ultimately the advantage and

451
00:14:00,959 --> 00:14:04,160
disadvantage of my role as an expert and

452
00:14:02,560 --> 00:14:06,000
not a politician is I hand them the

453
00:14:04,160 --> 00:14:07,279
numbers and then I walk away.

454
00:14:06,000 --> 00:14:08,720
>> So I can say to them, look, here's the

455
00:14:07,279 --> 00:14:10,480
choices you can make.

456
00:14:08,720 --> 00:14:12,320
>> But ultimately they make the choices.

457
00:14:10,480 --> 00:14:13,760
Now that can be very frustrating because

458
00:14:12,320 --> 00:14:15,760
they can make a choice. I may have view

459
00:14:13,760 --> 00:14:17,120
on those choices and it's not my choice.

460
00:14:15,760 --> 00:14:18,959
On the other hand, it's also less

461
00:14:17,120 --> 00:14:21,440
stressful because it's not my choice. So

462
00:14:18,959 --> 00:14:23,279
in some sense my my job it's stressful

463
00:14:21,440 --> 00:14:23,519
because I wanted the best possible job

464
00:14:23,279 --> 00:14:25,199
right

465
00:14:23,519 --> 00:14:26,880
>> but my job is to present them with a set

466
00:14:25,199 --> 00:14:28,160
of options but ultimately it's the

467
00:14:26,880 --> 00:14:30,639
policy makers that decide.

468
00:14:28,160 --> 00:14:33,199
>> I mean all your work in healthcare it

469
00:14:30,639 --> 00:14:34,880
came from somewhere you know why health

470
00:14:33,199 --> 00:14:37,600
care of all the things.

471
00:14:34,880 --> 00:14:41,040
>> Great question. So um as an

472
00:14:37,600 --> 00:14:42,639
undergraduate at MIT uh I was very

473
00:14:41,040 --> 00:14:43,839
inspired by a political science course I

474
00:14:42,639 --> 00:14:46,079
took a professor who was here named

475
00:14:43,839 --> 00:14:48,240
Louis Manand and Louis Manand really

476
00:14:46,079 --> 00:14:49,519
inspired me that we need to not just

477
00:14:48,240 --> 00:14:50,800
think about something like political

478
00:14:49,519 --> 00:14:52,399
science as an abstract concept but

479
00:14:50,800 --> 00:14:53,680
really get our hands dirty and think

480
00:14:52,399 --> 00:14:55,519
about how we can make the world a better

481
00:14:53,680 --> 00:14:56,880
place and it really inspired me to think

482
00:14:55,519 --> 00:14:58,560
about making the world a better place

483
00:14:56,880 --> 00:14:59,760
and I really decided when I went to

484
00:14:58,560 --> 00:15:00,560
economics that's what I wanted to do

485
00:14:59,760 --> 00:15:02,800
with it

486
00:15:00,560 --> 00:15:04,240
>> and in 1990 when I was searching around

487
00:15:02,800 --> 00:15:06,320
for what I want to work on healthcare

488
00:15:04,240 --> 00:15:07,920
was a very big topic IC it was sort of

489
00:15:06,320 --> 00:15:09,440
the beginning of a real movement towards

490
00:15:07,920 --> 00:15:11,519
what became eventually the Clinton

491
00:15:09,440 --> 00:15:13,440
healthcare reform thinking about new

492
00:15:11,519 --> 00:15:14,959
models of universal healthcare and I

493
00:15:13,440 --> 00:15:16,240
realized this is an area that economics

494
00:15:14,959 --> 00:15:17,920
had a lot to contribute to and that I

495
00:15:16,240 --> 00:15:19,920
had a lot to contribute to in trying to

496
00:15:17,920 --> 00:15:22,079
make the world a better place

497
00:15:19,920 --> 00:15:24,079
>> it's so central like when I think about

498
00:15:22,079 --> 00:15:26,240
employment health care and health

499
00:15:24,079 --> 00:15:29,040
insurance is such a huge part of it you

500
00:15:26,240 --> 00:15:30,399
know it affects jobs and like what

501
00:15:29,040 --> 00:15:31,360
decisions I'm going to make about where

502
00:15:30,399 --> 00:15:32,800
I'll work

503
00:15:31,360 --> 00:15:34,160
>> well first of all it's 18% of our

504
00:15:32,800 --> 00:15:36,560
economy it's the biggest sector of our

505
00:15:34,160 --> 00:15:37,920
economy Um, it's the largest single

506
00:15:36,560 --> 00:15:40,000
government expenditure at both the state

507
00:15:37,920 --> 00:15:41,760
and national level. So just magnitudes,

508
00:15:40,000 --> 00:15:43,519
it matters. But you're right, it affects

509
00:15:41,760 --> 00:15:45,120
everything. We have an employment based

510
00:15:43,519 --> 00:15:46,800
health insurance system in America.

511
00:15:45,120 --> 00:15:48,160
About 60% of Americans get health

512
00:15:46,800 --> 00:15:49,360
insurance from their employer. That

513
00:15:48,160 --> 00:15:50,560
means an important part of your job

514
00:15:49,360 --> 00:15:52,240
decision is your health insurance

515
00:15:50,560 --> 00:15:54,160
decision. An important part of an

516
00:15:52,240 --> 00:15:55,040
employer's compensation decision is how

517
00:15:54,160 --> 00:15:56,000
they're going to get how much health

518
00:15:55,040 --> 00:16:00,480
insurance they're going to give their

519
00:15:56,000 --> 00:16:03,040
employees. It's it's a huge sector of of

520
00:16:00,480 --> 00:16:04,320
getting jobs. So healthcare jobs is one

521
00:16:03,040 --> 00:16:06,079
of the main drivers of economic growth

522
00:16:04,320 --> 00:16:07,600
here in Massachusetts. It it it

523
00:16:06,079 --> 00:16:09,759
permeates everything, not to mention

524
00:16:07,600 --> 00:16:12,079
permeating our actual physical health.

525
00:16:09,759 --> 00:16:14,639
Our health is determined by many things,

526
00:16:12,079 --> 00:16:16,720
primarily our genetics. Uh but a lot of

527
00:16:14,639 --> 00:16:18,639
us by the health care we get and so it

528
00:16:16,720 --> 00:16:20,240
it affects all aspects of our lives.

529
00:16:18,639 --> 00:16:22,959
>> I know in some ways it feels a little

530
00:16:20,240 --> 00:16:24,560
unfair like I make decisions about my

531
00:16:22,959 --> 00:16:25,920
own health based on what my insurance

532
00:16:24,560 --> 00:16:29,279
will cover.

533
00:16:25,920 --> 00:16:30,800
>> You know, this is the challenge. Um the

534
00:16:29,279 --> 00:16:32,560
challenge is let's go back to this

535
00:16:30,800 --> 00:16:34,079
conversation started. There's no free

536
00:16:32,560 --> 00:16:34,399
lunch. You can't have it all.

537
00:16:34,079 --> 00:16:36,160
>> Yeah.

538
00:16:34,399 --> 00:16:37,600
>> We would love a world where we all got

539
00:16:36,160 --> 00:16:39,040
to get whatever care we wanted whenever

540
00:16:37,600 --> 00:16:39,519
we wanted from whoever we wanted.

541
00:16:39,040 --> 00:16:41,199
>> Yeah.

542
00:16:39,519 --> 00:16:43,199
>> We could have that world if we only

543
00:16:41,199 --> 00:16:44,560
spend a lot more on healthare.

544
00:16:43,199 --> 00:16:45,759
>> The restrictions that are put in place

545
00:16:44,560 --> 00:16:48,480
that cause people to hate insurance

546
00:16:45,759 --> 00:16:49,759
companies also keep our premiums down.

547
00:16:48,480 --> 00:16:51,440
>> If the insurance companies got rid of

548
00:16:49,759 --> 00:16:52,720
those restrictions, we'd have more

549
00:16:51,440 --> 00:16:53,279
choice and higher health insurance

550
00:16:52,720 --> 00:16:54,000
premiums.

551
00:16:53,279 --> 00:16:55,759
>> So trade-offs.

552
00:16:54,000 --> 00:16:58,079
>> So there's a trade-off. So the example I

553
00:16:55,759 --> 00:16:59,519
teach in my class I like is that

554
00:16:58,079 --> 00:17:01,680
students in my class don't remember what

555
00:16:59,519 --> 00:17:03,360
it was like to fly in the 1970s.

556
00:17:01,680 --> 00:17:03,759
>> The 1970s flying in airplanes was

557
00:17:03,360 --> 00:17:04,480
awesome.

558
00:17:03,759 --> 00:17:06,959
>> What was it like?

559
00:17:04,480 --> 00:17:11,039
>> Huge leg room, free food, real

560
00:17:06,959 --> 00:17:13,600
silverware, free booze. It was in coach.

561
00:17:11,039 --> 00:17:14,880
It was incredible. Um uh and

562
00:17:13,600 --> 00:17:16,400
unbelievably expensive and most

563
00:17:14,880 --> 00:17:18,559
Americans never flew.

564
00:17:16,400 --> 00:17:20,799
>> Then we deregulated the airlines in the

565
00:17:18,559 --> 00:17:23,520
late 1970s and allowed air much more

566
00:17:20,799 --> 00:17:25,039
entry of airlines. prices came way down

567
00:17:23,520 --> 00:17:25,760
and service got way worse.

568
00:17:25,039 --> 00:17:27,600
>> Okay?

569
00:17:25,760 --> 00:17:29,360
>> Now, people today complain about the

570
00:17:27,600 --> 00:17:30,480
worst service, but they don't complain

571
00:17:29,360 --> 00:17:30,799
about the low prices.

572
00:17:30,480 --> 00:17:32,080
>> Okay?

573
00:17:30,799 --> 00:17:33,600
>> And if it really is true that people

574
00:17:32,080 --> 00:17:35,840
didn't like the bad service, there would

575
00:17:33,600 --> 00:17:37,440
emerge an airline which would be like

576
00:17:35,840 --> 00:17:39,440
the old airlines, super nice and super

577
00:17:37,440 --> 00:17:41,039
expensive. Now, we have charter jets,

578
00:17:39,440 --> 00:17:43,120
but the bottom line is people have

579
00:17:41,039 --> 00:17:45,120
revealed they'd rather pay low prices

580
00:17:43,120 --> 00:17:47,360
for mediocre flights than pay high

581
00:17:45,120 --> 00:17:48,960
prices for better flights.

582
00:17:47,360 --> 00:17:51,120
>> There's trade-offs in life. The the

583
00:17:48,960 --> 00:17:52,559
difference with flights is it's pretty

584
00:17:51,120 --> 00:17:53,840
much for the market to decide. Yeah.

585
00:17:52,559 --> 00:17:54,880
With healthcare, there's a government

586
00:17:53,840 --> 00:17:55,760
role in deciding that. There's a

587
00:17:54,880 --> 00:17:57,600
government role in, for example,

588
00:17:55,760 --> 00:17:59,039
deciding what's the minimum insurance

589
00:17:57,600 --> 00:18:00,160
package people should have. The

590
00:17:59,039 --> 00:18:01,760
government has a role with airlines,

591
00:18:00,160 --> 00:18:02,080
too. We decide on safety.

592
00:18:01,760 --> 00:18:03,440
>> Sure.

593
00:18:02,080 --> 00:18:04,799
>> But we don't decide on leg room as a

594
00:18:03,440 --> 00:18:06,000
government. We don't decide on food. We

595
00:18:04,799 --> 00:18:06,960
don't decide on baggage fees.

596
00:18:06,000 --> 00:18:08,640
>> Yes, that's clear.

597
00:18:06,960 --> 00:18:10,000
>> When we go to healthare, the government

598
00:18:08,640 --> 00:18:11,919
has a role in deciding, look, what's a

599
00:18:10,000 --> 00:18:13,679
minimum acceptable health care package?

600
00:18:11,919 --> 00:18:14,880
What's, you know, what's a reasonable

601
00:18:13,679 --> 00:18:16,880
way that we pay for our healthcare

602
00:18:14,880 --> 00:18:17,919
services? there's more of a role, but

603
00:18:16,880 --> 00:18:18,480
ultimately you can't avoid the

604
00:18:17,919 --> 00:18:19,919
trade-offs.

605
00:18:18,480 --> 00:18:23,760
>> So, I have a question about models. So,

606
00:18:19,919 --> 00:18:27,360
you put your professor hat on. So, I've

607
00:18:23,760 --> 00:18:30,559
been hearing a lot about um preference

608
00:18:27,360 --> 00:18:32,240
pricing, AI surveillance pricing.

609
00:18:30,559 --> 00:18:34,400
>> So, I'd like you to explain what that

610
00:18:32,240 --> 00:18:36,799
means, but then also what does that do

611
00:18:34,400 --> 00:18:38,160
to basic the basic economic model of

612
00:18:36,799 --> 00:18:41,039
supply and demand?

613
00:18:38,160 --> 00:18:42,880
>> Yep. So, at the heart of economics is a

614
00:18:41,039 --> 00:18:44,320
trade-off between equity and efficiency.

615
00:18:42,880 --> 00:18:46,559
>> Okay. Okay, what do I mean by that? A

616
00:18:44,320 --> 00:18:48,640
lot of basically think of it as how big

617
00:18:46,559 --> 00:18:49,039
is the pie versus how you slice up the

618
00:18:48,640 --> 00:18:49,840
pie.

619
00:18:49,039 --> 00:18:52,880
>> Okay,

620
00:18:49,840 --> 00:18:54,720
>> and what determines the size of the pie

621
00:18:52,880 --> 00:18:56,559
is can we make the best match between

622
00:18:54,720 --> 00:18:58,160
what people want and what gets produced.

623
00:18:56,559 --> 00:19:00,720
Okay, is a key thing there. What

624
00:18:58,160 --> 00:19:02,880
algorithmic pricing does is it really

625
00:19:00,720 --> 00:19:06,480
improves that match because it lets me

626
00:19:02,880 --> 00:19:08,080
say, well, Sarah really likes this thing

627
00:19:06,480 --> 00:19:09,360
and so I'm going to make sure that she

628
00:19:08,080 --> 00:19:11,120
gets it. Now, I'm going to charge her a

629
00:19:09,360 --> 00:19:12,720
higher price, but if I didn't charge a

630
00:19:11,120 --> 00:19:13,440
higher price, I might not make it. So

631
00:19:12,720 --> 00:19:14,880
let's say

632
00:19:13,440 --> 00:19:15,280
>> but that's such a benevolent way to

633
00:19:14,880 --> 00:19:16,320
think about

634
00:19:15,280 --> 00:19:17,280
>> No, let me finish. Let me

635
00:19:16,320 --> 00:19:19,360
>> I feel like that's not true.

636
00:19:17,280 --> 00:19:20,640
>> Okay, it is true. So the thing is if you

637
00:19:19,360 --> 00:19:23,280
don't want it, you don't have to buy it.

638
00:19:20,640 --> 00:19:25,200
>> Okay, but that's for like things that I

639
00:19:23,280 --> 00:19:28,160
want. What about basic needs like buying

640
00:19:25,200 --> 00:19:30,160
milk for my child? Like uh I won't I

641
00:19:28,160 --> 00:19:32,480
won't name the stores, but you can do

642
00:19:30,160 --> 00:19:34,960
online grocery shopping and they clearly

643
00:19:32,480 --> 00:19:38,080
know what I buy and like at what price

644
00:19:34,960 --> 00:19:40,400
I'm willing to buy it, right? and I went

645
00:19:38,080 --> 00:19:42,320
on a site like that and they wouldn't

646
00:19:40,400 --> 00:19:43,679
tell me my price until I added it to the

647
00:19:42,320 --> 00:19:48,160
cart.

648
00:19:43,679 --> 00:19:50,240
>> Okay, that is a great example of why you

649
00:19:48,160 --> 00:19:50,559
can't just leave the free market alone.

650
00:19:50,240 --> 00:19:52,400
>> Yeah,

651
00:19:50,559 --> 00:19:55,840
>> here here's a different question. You go

652
00:19:52,400 --> 00:19:57,440
online and they say, "Aha, I know Sarah

653
00:19:55,840 --> 00:19:58,799
likes this, so I'm going to start

654
00:19:57,440 --> 00:19:59,440
raising the price of that sort of thing

655
00:19:58,799 --> 00:20:00,799
to her."

656
00:19:59,440 --> 00:20:04,000
>> Yeah, that feels bad.

657
00:20:00,799 --> 00:20:05,679
>> It feels unfair. But then that comes the

658
00:20:04,000 --> 00:20:07,280
second issue, which is equity.

659
00:20:05,679 --> 00:20:09,200
>> Okay. which is what happens to that

660
00:20:07,280 --> 00:20:12,240
money they make from that. Now, if they

661
00:20:09,200 --> 00:20:14,400
take that money and it goes to Jeff

662
00:20:12,240 --> 00:20:15,039
Bezos, that is unfair.

663
00:20:14,400 --> 00:20:17,840
>> Mhm.

664
00:20:15,039 --> 00:20:19,600
>> But if Amazon is in a competitive

665
00:20:17,840 --> 00:20:21,760
marketplace, we can go back to whether

666
00:20:19,600 --> 00:20:22,160
they are. But imagine they were.

667
00:20:21,760 --> 00:20:23,360
>> Okay.

668
00:20:22,160 --> 00:20:25,039
>> Then that money wouldn't go to Jeff

669
00:20:23,360 --> 00:20:26,799
Bezos. That would mean they'd charge

670
00:20:25,039 --> 00:20:28,640
less for something else. In a really

671
00:20:26,799 --> 00:20:29,760
competitive market, their profits are

672
00:20:28,640 --> 00:20:30,720
limited. So, they're going to make money

673
00:20:29,760 --> 00:20:32,080
off you. They're going to make less

674
00:20:30,720 --> 00:20:33,679
money off someone else.

675
00:20:32,080 --> 00:20:36,480
>> So, that's sort of a redistribution

676
00:20:33,679 --> 00:20:37,840
issue. That's an efficiency issue. Now,

677
00:20:36,480 --> 00:20:39,280
that's different if all the extra money

678
00:20:37,840 --> 00:20:40,880
goes to Jeff Bezos.

679
00:20:39,280 --> 00:20:42,320
>> And that's once again why we can't just

680
00:20:40,880 --> 00:20:43,760
leave the market alone, right? Why you

681
00:20:42,320 --> 00:20:45,679
need a government involved to make sure

682
00:20:43,760 --> 00:20:47,200
that you have enough competition. But in

683
00:20:45,679 --> 00:20:49,099
a standard economic model,

684
00:20:47,200 --> 00:20:50,320
>> if we have algorithmic pricing and

685
00:20:49,099 --> 00:20:52,080
[clears throat] people's prices better

686
00:20:50,320 --> 00:20:54,559
reflect their preferences,

687
00:20:52,080 --> 00:20:55,679
>> then that makes things more efficient.

688
00:20:54,559 --> 00:20:56,400
>> The question is whether it makes things

689
00:20:55,679 --> 00:20:58,559
more equitable

690
00:20:56,400 --> 00:21:00,080
>> and and so how are economists thinking

691
00:20:58,559 --> 00:21:01,280
about this? Are they grappling with it?

692
00:21:00,080 --> 00:21:02,480
This is a fairly new thing.

693
00:21:01,280 --> 00:21:03,840
>> It's a fairly new thing. I think we're

694
00:21:02,480 --> 00:21:08,000
early to grappling with it. Okay.

695
00:21:03,840 --> 00:21:11,600
>> I think we are trying to think about um

696
00:21:08,000 --> 00:21:13,039
essentially what you do when this

697
00:21:11,600 --> 00:21:13,600
becomes exploitative.

698
00:21:13,039 --> 00:21:15,520
>> Yeah.

699
00:21:13,600 --> 00:21:16,799
>> Um because of lack of information.

700
00:21:15,520 --> 00:21:17,520
That's one thing we need to think hard

701
00:21:16,799 --> 00:21:20,559
about

702
00:21:17,520 --> 00:21:21,440
>> whether it is uh leads to increased

703
00:21:20,559 --> 00:21:22,960
market failures because of

704
00:21:21,440 --> 00:21:24,559
monopolization of markets and so the

705
00:21:22,960 --> 00:21:26,080
money just goes to Jeff Bezos and not to

706
00:21:24,559 --> 00:21:26,640
not to pick on him but you know what I

707
00:21:26,080 --> 00:21:29,039
mean.

708
00:21:26,640 --> 00:21:31,120
>> And also

709
00:21:29,039 --> 00:21:33,520
>> the fact that there's an a fundamental

710
00:21:31,120 --> 00:21:35,520
inequity. So I you're right. I made it.

711
00:21:33,520 --> 00:21:36,240
I made the good case. Let me do the

712
00:21:35,520 --> 00:21:36,720
opposite case.

713
00:21:36,240 --> 00:21:38,559
>> Okay.

714
00:21:36,720 --> 00:21:39,919
>> Let's say that

715
00:21:38,559 --> 00:21:42,880
>> you're well off

716
00:21:39,919 --> 00:21:43,280
>> and you can shop from lots of options.

717
00:21:42,880 --> 00:21:44,559
>> Okay.

718
00:21:43,280 --> 00:21:45,679
>> So, they know if they price high to

719
00:21:44,559 --> 00:21:47,760
Sarah here, Sarah's going to go over

720
00:21:45,679 --> 00:21:49,600
there and buy it. Now, imagine a poor

721
00:21:47,760 --> 00:21:50,640
person in inner city who only has one

722
00:21:49,600 --> 00:21:51,200
store they can go to.

723
00:21:50,640 --> 00:21:53,520
>> Yeah.

724
00:21:51,200 --> 00:21:55,360
>> That store could then say, "Aha, I've

725
00:21:53,520 --> 00:21:56,960
used algorithmic determination to decide

726
00:21:55,360 --> 00:21:58,320
this person has no car. They have

727
00:21:56,960 --> 00:21:58,960
nowhere else to go. I'm going to charge

728
00:21:58,320 --> 00:22:00,000
them a lot."

729
00:21:58,960 --> 00:22:01,039
>> Yeah. Terrible.

730
00:22:00,000 --> 00:22:03,120
>> That is unfair.

731
00:22:01,039 --> 00:22:05,120
>> Yes. But that is has to separate

732
00:22:03,120 --> 00:22:06,480
separate the efficiency from the

733
00:22:05,120 --> 00:22:08,240
fairness

734
00:22:06,480 --> 00:22:08,799
>> and the proper role of societies to

735
00:22:08,240 --> 00:22:10,640
address both.

736
00:22:08,799 --> 00:22:11,600
>> But I wanted to go back to AI for a

737
00:22:10,640 --> 00:22:13,919
second

738
00:22:11,600 --> 00:22:17,360
>> and just how economists are thinking

739
00:22:13,919 --> 00:22:20,159
about the role of AI before we started

740
00:22:17,360 --> 00:22:21,760
the podcast. I asked you a question and

741
00:22:20,159 --> 00:22:24,320
you wrote an answer and you haven't

742
00:22:21,760 --> 00:22:26,880
shown me the answer yet, but I asked you

743
00:22:24,320 --> 00:22:28,080
um can AI fix the economy? So I'm

744
00:22:26,880 --> 00:22:30,000
wondering if you could show me your

745
00:22:28,080 --> 00:22:31,200
answer now. And you first of all you

746
00:22:30,000 --> 00:22:32,240
feel bad for my students because they

747
00:22:31,200 --> 00:22:32,640
have to read this handwriting.

748
00:22:32,240 --> 00:22:35,200
>> Okay.

749
00:22:32,640 --> 00:22:36,559
>> But I wrote uh it's up to us.

750
00:22:35,200 --> 00:22:38,320
>> It's up to us.

751
00:22:36,559 --> 00:22:40,880
>> And basically what do I mean by that?

752
00:22:38,320 --> 00:22:42,240
What I mean by that is really paring a

753
00:22:40,880 --> 00:22:44,480
wonderful book

754
00:22:42,240 --> 00:22:46,159
>> called power and progress by my

755
00:22:44,480 --> 00:22:47,679
colleagues my Nobel Prize winning

756
00:22:46,159 --> 00:22:49,120
colleagues Doras Mogul and Simon

757
00:22:47,679 --> 00:22:50,720
Johnson.

758
00:22:49,120 --> 00:22:52,640
>> They talk about through the whole

759
00:22:50,720 --> 00:22:55,600
history of technology

760
00:22:52,640 --> 00:22:58,640
>> that technology can always be used for

761
00:22:55,600 --> 00:23:00,480
good or for bad. And really it's up to

762
00:22:58,640 --> 00:23:01,679
us to direct it to decide which

763
00:23:00,480 --> 00:23:02,400
direction it's going to go in.

764
00:23:01,679 --> 00:23:05,039
>> Mhm.

765
00:23:02,400 --> 00:23:06,400
>> So often asked this question which is I

766
00:23:05,039 --> 00:23:06,880
feel like social media has led to very

767
00:23:06,400 --> 00:23:06,880
many ter tr ter tr ter tr ter tr ter tr

768
00:23:06,880 --> 00:23:08,799
ter tr ter tr ter tr ter tr ter tr

769
00:23:06,880 --> 00:23:11,440
terrible outcomes in our society. And I

770
00:23:08,799 --> 00:23:12,559
often think 25 years ago if we knew what

771
00:23:11,440 --> 00:23:13,919
we know now what would we have done

772
00:23:12,559 --> 00:23:15,120
differently with social media? What

773
00:23:13,919 --> 00:23:16,480
could we have done differently? I don't

774
00:23:15,120 --> 00:23:18,000
really know the answer but I feel like

775
00:23:16,480 --> 00:23:20,240
that's a conversation we we should be

776
00:23:18,000 --> 00:23:21,679
having now about AI. We shouldn't get

777
00:23:20,240 --> 00:23:23,440
behind. We know bad things are going to

778
00:23:21,679 --> 00:23:24,960
come out of AI. We know good things have

779
00:23:23,440 --> 00:23:26,080
come out come out of AI just like bad

780
00:23:24,960 --> 00:23:27,840
and good things have come with social

781
00:23:26,080 --> 00:23:29,360
media. We need to be getting ahead of

782
00:23:27,840 --> 00:23:30,320
the bad things in a way we didn't with

783
00:23:29,360 --> 00:23:32,240
social media.

784
00:23:30,320 --> 00:23:33,360
>> So it's up to us how AI is going to

785
00:23:32,240 --> 00:23:34,320
affect the economy and how it's going to

786
00:23:33,360 --> 00:23:35,840
affect life.

787
00:23:34,320 --> 00:23:37,360
>> It's up to whether we establish the

788
00:23:35,840 --> 00:23:39,520
proper regulatory frameworks and the

789
00:23:37,360 --> 00:23:41,440
proper way of thinking about it so that

790
00:23:39,520 --> 00:23:43,039
we can deal with the negative effects

791
00:23:41,440 --> 00:23:44,799
while capturing the positive effects.

792
00:23:43,039 --> 00:23:46,880
>> Yeah. So it's sort of like the bowling

793
00:23:44,799 --> 00:23:47,840
analogy that you use. We need the

794
00:23:46,880 --> 00:23:48,799
gutters full.

795
00:23:47,840 --> 00:23:49,760
>> Yeah.

796
00:23:48,799 --> 00:23:50,640
>> We need some gutter guards.

797
00:23:49,760 --> 00:23:51,600
>> We need some gutter guards. Like when

798
00:23:50,640 --> 00:23:52,880
you bowl with your little kid and you

799
00:23:51,600 --> 00:23:54,640
got the gutter guards, we need some

800
00:23:52,880 --> 00:23:56,080
gutter guards to allow the ball to rock

801
00:23:54,640 --> 00:23:57,840
around the alley, but so we don't go in

802
00:23:56,080 --> 00:23:59,440
the gutters, which could be existential

803
00:23:57,840 --> 00:24:02,880
destruction rather than a ball in the

804
00:23:59,440 --> 00:24:05,600
alley. But the bottom line is we have

805
00:24:02,880 --> 00:24:07,600
lots of hard questions that this is

806
00:24:05,600 --> 00:24:09,120
raising, but it's still early days. I

807
00:24:07,600 --> 00:24:11,039
know it's moving fast, but it's still

808
00:24:09,120 --> 00:24:13,600
early days. I guess I'm not optimistic

809
00:24:11,039 --> 00:24:15,279
that I see people engaging with with

810
00:24:13,600 --> 00:24:17,600
these hard questions in a constructive

811
00:24:15,279 --> 00:24:19,360
way. And I feel like we need we need

812
00:24:17,600 --> 00:24:21,200
disinterested people, expert people who

813
00:24:19,360 --> 00:24:22,559
aren't there to make a buck out of it to

814
00:24:21,200 --> 00:24:22,880
really start thinking hard about these

815
00:24:22,559 --> 00:24:24,400
things.

816
00:24:22,880 --> 00:24:26,559
>> Um, you said in one of your lectures,

817
00:24:24,400 --> 00:24:28,526
maybe about two years ago, which was

818
00:24:26,559 --> 00:24:29,039
like pre where we are now with AI,

819
00:24:28,526 --> 00:24:30,960
[laughter]

820
00:24:29,039 --> 00:24:33,200
>> you said um, and I'll read you the

821
00:24:30,960 --> 00:24:35,919
quote, um, any government, no matter how

822
00:24:33,200 --> 00:24:38,400
large and how benevolent, cannot make

823
00:24:35,919 --> 00:24:40,559
the enormous massive quantity of

824
00:24:38,400 --> 00:24:42,880
billions and billions of decisions that

825
00:24:40,559 --> 00:24:45,120
need to be made. Think about every good

826
00:24:42,880 --> 00:24:47,039
we consume in America. a government

827
00:24:45,120 --> 00:24:48,720
deciding how much of it to make and

828
00:24:47,039 --> 00:24:49,840
literally who gets it. It's

829
00:24:48,720 --> 00:24:51,120
overwhelming.

830
00:24:49,840 --> 00:24:53,039
>> This is, you know, this is from my

831
00:24:51,120 --> 00:24:54,799
introductory lecture to the class

832
00:24:53,039 --> 00:24:56,960
>> where I talk about the fundamental

833
00:24:54,799 --> 00:24:59,440
nature of capitalism. Capitalism has a

834
00:24:56,960 --> 00:25:01,840
bad name uh right now in many circles.

835
00:24:59,440 --> 00:25:03,600
But let's remember that what capitalism

836
00:25:01,840 --> 00:25:05,679
is essentially about letting the market,

837
00:25:03,600 --> 00:25:07,760
the invisible hand of the market make

838
00:25:05,679 --> 00:25:09,440
those billions of decisions. The

839
00:25:07,760 --> 00:25:11,440
opposite is a what we call a command

840
00:25:09,440 --> 00:25:13,039
economy which is often in communist

841
00:25:11,440 --> 00:25:14,480
societies which is where the government

842
00:25:13,039 --> 00:25:17,600
would make those decisions. I don't

843
00:25:14,480 --> 00:25:19,200
really see AI making it feasible to have

844
00:25:17,600 --> 00:25:20,720
a command economy because ultimately

845
00:25:19,200 --> 00:25:22,960
that's about people's preferences and

846
00:25:20,720 --> 00:25:25,679
things. I I think AI comes to earlier

847
00:25:22,960 --> 00:25:28,400
conversation AI is going to really match

848
00:25:25,679 --> 00:25:30,240
people more with exactly the goods they

849
00:25:28,400 --> 00:25:32,159
want and need and match the production

850
00:25:30,240 --> 00:25:33,600
of that and then the question is who

851
00:25:32,159 --> 00:25:36,159
benefits from that. Mhm.

852
00:25:33,600 --> 00:25:37,840
>> I'd like to think there's a world where

853
00:25:36,159 --> 00:25:39,200
consumers benefit, where they get goods

854
00:25:37,840 --> 00:25:40,480
they wouldn't have otherwise gotten at

855
00:25:39,200 --> 00:25:43,200
prices below what they were willing to

856
00:25:40,480 --> 00:25:45,120
pay and get some surplus from that. But

857
00:25:43,200 --> 00:25:47,039
that might not happen. Like I said, you

858
00:25:45,120 --> 00:25:49,520
know, and like Jeron Stein's book

859
00:25:47,039 --> 00:25:50,960
highlights, technological advancement is

860
00:25:49,520 --> 00:25:52,720
a choice. We may instead end up in a

861
00:25:50,960 --> 00:25:54,480
world where consumers get exploited and

862
00:25:52,720 --> 00:25:56,240
a few monopolists get very rich and that

863
00:25:54,480 --> 00:25:58,080
would be a bad outcome indeed. Mhm.

864
00:25:56,240 --> 00:26:00,080
You've been an avid publisher on open

865
00:25:58,080 --> 00:26:03,520
courseware which we've so appreciated

866
00:26:00,080 --> 00:26:06,000
and I often think how can MIT justify

867
00:26:03,520 --> 00:26:08,880
giving their proprietary content away

868
00:26:06,000 --> 00:26:11,760
for free in this economy.

869
00:26:08,880 --> 00:26:13,600
>> Uh it's a great question. Um I think

870
00:26:11,760 --> 00:26:17,840
that it actually relates to another

871
00:26:13,600 --> 00:26:21,039
discussion we had which is that tuition

872
00:26:17,840 --> 00:26:23,200
MIT is very high. Uh now if your if your

873
00:26:21,039 --> 00:26:24,799
income's below $200,000 a year it's free

874
00:26:23,200 --> 00:26:26,880
but above that it's very high. And

875
00:26:24,799 --> 00:26:28,400
people often say, "Well, that's unfair.

876
00:26:26,880 --> 00:26:29,440
Uh, people shouldn't pay that much." But

877
00:26:28,400 --> 00:26:31,120
you have to remember what's done with

878
00:26:29,440 --> 00:26:33,679
that money. Part of what's done with

879
00:26:31,120 --> 00:26:36,080
that money is producing things like OCW.

880
00:26:33,679 --> 00:26:38,320
We have to remember that organizations

881
00:26:36,080 --> 00:26:40,320
like MIT

882
00:26:38,320 --> 00:26:41,919
do not break even on every transaction.

883
00:26:40,320 --> 00:26:43,600
They win in some transactions, they lose

884
00:26:41,919 --> 00:26:45,440
in some transactions. They're trying to

885
00:26:43,600 --> 00:26:46,880
serve their broad public function. One

886
00:26:45,440 --> 00:26:48,400
of those functions is educating the

887
00:26:46,880 --> 00:26:51,120
public. And MIT is willing to lose money

888
00:26:48,400 --> 00:26:52,480
in OCW because they know it's very

889
00:26:51,120 --> 00:26:54,240
important to get that information out

890
00:26:52,480 --> 00:26:55,760
there. So, John, you've shared a lot of

891
00:26:54,240 --> 00:26:57,840
your thinking and a lot of your teaching

892
00:26:55,760 --> 00:26:59,520
on MIT Open Courseware, which we're

893
00:26:57,840 --> 00:27:01,679
really grateful for. I'm wondering if

894
00:26:59,520 --> 00:27:03,200
you could tell our listeners and our

895
00:27:01,679 --> 00:27:03,919
viewers what some of those offerings

896
00:27:03,200 --> 00:27:06,559
are.

897
00:27:03,919 --> 00:27:09,200
>> Yeah, so um 1401, the course that turned

898
00:27:06,559 --> 00:27:11,120
me on to economics. Um I've now I now

899
00:27:09,200 --> 00:27:12,640
have an updated version of OCW. I had a

900
00:27:11,120 --> 00:27:16,000
version from 2018. There's now a version

901
00:27:12,640 --> 00:27:17,840
from 2023 up on OCW. We're about to put

902
00:27:16,000 --> 00:27:20,000
up uh the other course I teach at MIT,

903
00:27:17,840 --> 00:27:22,640
which is 1441, which is public

904
00:27:20,000 --> 00:27:23,840
economics. um which is really about the

905
00:27:22,640 --> 00:27:25,200
role of the government in the economy

906
00:27:23,840 --> 00:27:26,320
sort of taking as a starting point that

907
00:27:25,200 --> 00:27:27,600
there are these market failures and

908
00:27:26,320 --> 00:27:29,200
problems and what should the government

909
00:27:27,600 --> 00:27:31,200
do about them and those are my two main

910
00:27:29,200 --> 00:27:32,400
offerings on OCW there's enormous number

911
00:27:31,200 --> 00:27:34,000
of offerings from the economics

912
00:27:32,400 --> 00:27:35,840
department as chairman of the department

913
00:27:34,000 --> 00:27:37,120
I want to also say yeah uh from the

914
00:27:35,840 --> 00:27:38,640
economics department at large there's

915
00:27:37,120 --> 00:27:40,080
many courses that people should look up

916
00:27:38,640 --> 00:27:42,080
and take we have wonderful teachers and

917
00:27:40,080 --> 00:27:45,120
great material and the last resource I'd

918
00:27:42,080 --> 00:27:46,640
point out is actually um you know it's

919
00:27:45,120 --> 00:27:48,720
funny Sarah you're probably too young to

920
00:27:46,640 --> 00:27:50,080
remember but when I was young and you'd

921
00:27:48,720 --> 00:27:51,919
fly in a plane you'd actually talk to

922
00:27:50,080 --> 00:27:52,240
the person next to you before phones.

923
00:27:51,919 --> 00:27:53,360
Oh, wow.

924
00:27:52,240 --> 00:27:56,399
>> You'd actually talk because there was

925
00:27:53,360 --> 00:27:58,159
nothing else to do. Um uh and the number

926
00:27:56,399 --> 00:27:59,760
of times someone said to me, "Oh,

927
00:27:58,159 --> 00:28:01,039
economics. I hated that class." And I

928
00:27:59,760 --> 00:28:02,640
thought, "How can that be?" And I

929
00:28:01,039 --> 00:28:03,840
realized it's really badly taught,

930
00:28:02,640 --> 00:28:04,480
especially kids who take it in high

931
00:28:03,840 --> 00:28:06,559
school.

932
00:28:04,480 --> 00:28:08,640
>> About 80,000 kids a year take AP take

933
00:28:06,559 --> 00:28:10,320
the AP microeconomics exam. And they're

934
00:28:08,640 --> 00:28:11,440
typically taught by the gym teacher or

935
00:28:10,320 --> 00:28:12,960
the history teacher or someone who

936
00:28:11,440 --> 00:28:15,520
doesn't really understand economics. So,

937
00:28:12,960 --> 00:28:17,840
I took a sobatical and made with MITx an

938
00:28:15,520 --> 00:28:21,520
AP economics high school class. If you

939
00:28:17,840 --> 00:28:22,799
go to MITx, it's uh microeconomics MITx.

940
00:28:21,520 --> 00:28:24,080
And that's a I think a really good

941
00:28:22,799 --> 00:28:25,039
resource for high schoolers and others

942
00:28:24,080 --> 00:28:25,760
who really want to get started with

943
00:28:25,039 --> 00:28:27,520
economics.

944
00:28:25,760 --> 00:28:28,799
>> John, thank you so much for being here.

945
00:28:27,520 --> 00:28:29,120
>> It's my pleasure. Thanks for having me,

946
00:28:28,799 --> 00:28:31,534
Sarah.

947
00:28:29,120 --> 00:28:33,554
>> You're so welcome.

948
00:28:31,534 --> 00:28:33,554
[music]

