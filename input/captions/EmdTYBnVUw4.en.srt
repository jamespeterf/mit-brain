1
00:00:13,519 --> 00:00:19,760
Hi everyone. Thanks a lot for joining.

2
00:00:16,240 --> 00:00:22,400
Uh it is my great pleasure to introduce

3
00:00:19,760 --> 00:00:24,800
today's speaker, Professor Joshua Yang.

4
00:00:22,400 --> 00:00:26,560
Uh who is the Arthur Freeman Chair

5
00:00:24,800 --> 00:00:29,359
Professor of Electrical and Computer

6
00:00:26,560 --> 00:00:32,800
Engineering at University of Southern

7
00:00:29,359 --> 00:00:35,200
California. Uh Professor Yang joined USC

8
00:00:32,800 --> 00:00:38,160
in 2020 coming from the faculty of the

9
00:00:35,200 --> 00:00:40,640
University of Massachusetts Ammerst

10
00:00:38,160 --> 00:00:42,480
uh specializing in postmos hardware for

11
00:00:40,640 --> 00:00:45,360
neuromorphic computing, machine learning

12
00:00:42,480 --> 00:00:47,120
and artificial intelligence. uh he has

13
00:00:45,360 --> 00:00:49,520
published many groundbreaking research

14
00:00:47,120 --> 00:00:52,000
papers in these domains. Uh and his

15
00:00:49,520 --> 00:00:55,760
innovative work has led to granting of

16
00:00:52,000 --> 00:00:57,920
over 120 US patents. Professor Yang is

17
00:00:55,760 --> 00:01:00,000
the associate editor of science advances

18
00:00:57,920 --> 00:01:02,719
on neuromorphic topics and the founding

19
00:01:00,000 --> 00:01:05,040
chair of the ITE neuromorphic computing

20
00:01:02,719 --> 00:01:07,119
technical committee. He serves as the

21
00:01:05,040 --> 00:01:09,119
director of an air force funded center

22
00:01:07,119 --> 00:01:13,600
of excellence on neuromorphic computing

23
00:01:09,119 --> 00:01:16,880
at USC and he is recognized as a clarate

24
00:01:13,600 --> 00:01:19,360
clarivate a highly cited researcher and

25
00:01:16,880 --> 00:01:21,360
listed among the top best scientist in

26
00:01:19,360 --> 00:01:24,400
the electronics and electrical

27
00:01:21,360 --> 00:01:27,280
engineering category by research.com. He

28
00:01:24,400 --> 00:01:30,320
was elected fellow of the IT in 2020

29
00:01:27,280 --> 00:01:33,840
2022 and of the national academy of

30
00:01:30,320 --> 00:01:35,680
inventors in 2023 for his contributions

31
00:01:33,840 --> 00:01:38,479
to resistive switching materials and

32
00:01:35,680 --> 00:01:40,560
devices for nonvolatile me memory and

33
00:01:38,479 --> 00:01:42,320
neuromorphic computing. So it is

34
00:01:40,560 --> 00:01:44,479
tremendous pleasure to be able to hear

35
00:01:42,320 --> 00:01:46,960
from him today. Thank you for joining us

36
00:01:44,479 --> 00:01:51,360
and we're excited to hear your talk.

37
00:01:46,960 --> 00:01:53,200
>> Thank you. Yeah, thanks uh finance for

38
00:01:51,360 --> 00:01:55,759
the candy invitation and the

39
00:01:53,200 --> 00:01:58,240
introduction and uh good afternoon

40
00:01:55,759 --> 00:02:01,119
everyone. It's great to be here see a

41
00:01:58,240 --> 00:02:03,119
lot of older friends and uh I didn't

42
00:02:01,119 --> 00:02:05,360
expect you know Friday afternoon there

43
00:02:03,119 --> 00:02:08,399
are so many people still interested in

44
00:02:05,360 --> 00:02:11,039
this topic. Thanks for coming. So what

45
00:02:08,399 --> 00:02:13,599
I'm going to talk about uh um neomorphic

46
00:02:11,039 --> 00:02:16,800
computing uh with different type of

47
00:02:13,599 --> 00:02:20,480
memory stores. Okay. So um speaking

48
00:02:16,800 --> 00:02:23,599
about neomorphic computing actually um

49
00:02:20,480 --> 00:02:27,040
one month and a half ago I was in a

50
00:02:23,599 --> 00:02:31,280
conference and during lunchtime I was at

51
00:02:27,040 --> 00:02:33,840
a lunch table with a bunch of editors

52
00:02:31,280 --> 00:02:36,239
from nature research journals. I was

53
00:02:33,840 --> 00:02:38,720
trying to advocate neuromorphic and

54
00:02:36,239 --> 00:02:42,000
trying to talk about how exciting it is,

55
00:02:38,720 --> 00:02:44,319
how promising the future is and then one

56
00:02:42,000 --> 00:02:46,879
of the editor you because camera I'm not

57
00:02:44,319 --> 00:02:51,440
going to name it.

58
00:02:46,879 --> 00:02:53,760
So uh suddenly said um didn't jacoma

59
00:02:51,440 --> 00:02:56,560
indeed just wrote a paper said

60
00:02:53,760 --> 00:02:58,160
neomorphic is dead.

61
00:02:56,560 --> 00:03:01,440
So

62
00:02:58,160 --> 00:03:04,400
I I was said what? By saying that

63
00:03:01,440 --> 00:03:08,080
actually I exposed

64
00:03:04,400 --> 00:03:10,400
my ignorance of two aspect. First I

65
00:03:08,080 --> 00:03:12,879
didn't even know that paper exist.

66
00:03:10,400 --> 00:03:15,840
Second I didn't know why someone like

67
00:03:12,879 --> 00:03:18,400
Jacoma would say something like that. So

68
00:03:15,840 --> 00:03:20,720
then uh after that conversation you can

69
00:03:18,400 --> 00:03:23,920
imagine immed immediately I went to my

70
00:03:20,720 --> 00:03:25,920
hotel room and find out their paper and

71
00:03:23,920 --> 00:03:29,440
here it is

72
00:03:25,920 --> 00:03:33,120
it does say neomorphic is dead and but

73
00:03:29,440 --> 00:03:37,280
it also says long live neomorphic. So if

74
00:03:33,120 --> 00:03:41,360
you read it it says um actually it

75
00:03:37,280 --> 00:03:44,879
blames the recent shift toward digital

76
00:03:41,360 --> 00:03:47,920
and AIcentric approach. In that sense he

77
00:03:44,879 --> 00:03:50,799
think the old um you know definition of

78
00:03:47,920 --> 00:03:54,879
neomorphic is there because they shift

79
00:03:50,799 --> 00:03:59,519
but he blamed this trend and he advocate

80
00:03:54,879 --> 00:04:03,360
um the you know for us going back to the

81
00:03:59,519 --> 00:04:07,280
original um principle of neuromorphic

82
00:04:03,360 --> 00:04:10,560
and emphas emphasizing on analog brain

83
00:04:07,280 --> 00:04:13,840
inspired systems and also the importance

84
00:04:10,560 --> 00:04:16,880
of fundamental neuron science for the

85
00:04:13,840 --> 00:04:19,759
future progress. So basically in that

86
00:04:16,880 --> 00:04:21,519
sense um that's a long live neomorphic.

87
00:04:19,759 --> 00:04:24,080
Okay, it still have a very bright future

88
00:04:21,519 --> 00:04:27,919
if you do it right. Okay, how to do it

89
00:04:24,080 --> 00:04:29,600
right? And if you keep reading and uh at

90
00:04:27,919 --> 00:04:33,600
the beginning he said the term

91
00:04:29,600 --> 00:04:37,919
neuromorphic was originally coined to

92
00:04:33,600 --> 00:04:42,479
directly emulate the bios system not

93
00:04:37,919 --> 00:04:45,280
simulate right and to mimic the dynamics

94
00:04:42,479 --> 00:04:48,240
of iron channels. So after reading to

95
00:04:45,280 --> 00:04:51,040
here then I was much relieved. So

96
00:04:48,240 --> 00:04:54,720
because this is exactly something we

97
00:04:51,040 --> 00:04:58,720
have been doing in the last uh decade.

98
00:04:54,720 --> 00:05:02,720
For example uh starting with this paper

99
00:04:58,720 --> 00:05:08,880
uh we started to advocate that you can

100
00:05:02,720 --> 00:05:11,840
use memorist diffusive dynamics to mimic

101
00:05:08,880 --> 00:05:14,720
um the synapse faithfully

102
00:05:11,840 --> 00:05:17,199
and more recently we did it for neuron.

103
00:05:14,720 --> 00:05:20,320
I I will talk about that later. So

104
00:05:17,199 --> 00:05:22,800
basically it's good. Okay. So neomorphic

105
00:05:20,320 --> 00:05:25,680
is still alive and we can we can talk

106
00:05:22,800 --> 00:05:30,639
about it and do it right. So then what

107
00:05:25,680 --> 00:05:34,080
does neomorphic covers? Um obviously it

108
00:05:30,639 --> 00:05:36,960
will cover uh the natural intelligence

109
00:05:34,080 --> 00:05:40,000
approach. So in this approach you use

110
00:05:36,960 --> 00:05:43,520
spike based signals, you use dynamic

111
00:05:40,000 --> 00:05:46,880
synapse and neurons and you use the

112
00:05:43,520 --> 00:05:49,919
biological learning rules right. Um but

113
00:05:46,880 --> 00:05:52,479
as Jacoma said in his paper and there

114
00:05:49,919 --> 00:05:55,600
are a lot of people also trying to use

115
00:05:52,479 --> 00:05:57,280
neuromorphic to cover other types like

116
00:05:55,600 --> 00:06:01,280
the

117
00:05:57,280 --> 00:06:04,800
uh digital uh AIcentric approach in this

118
00:06:01,280 --> 00:06:07,280
uh category um you are using value based

119
00:06:04,800 --> 00:06:10,319
signal instead of spec you after

120
00:06:07,280 --> 00:06:12,880
training you have a fixed weights and

121
00:06:10,319 --> 00:06:17,360
static neuron and you are using

122
00:06:12,880 --> 00:06:20,400
gradientbased learning um such as

123
00:06:17,360 --> 00:06:23,120
uh back propagation rather than here you

124
00:06:20,400 --> 00:06:24,720
know such as STDP spike time independent

125
00:06:23,120 --> 00:06:28,479
plasticity.

126
00:06:24,720 --> 00:06:32,000
So in this approach

127
00:06:28,479 --> 00:06:36,560
um the mainstream um computing platform

128
00:06:32,000 --> 00:06:41,120
you use uh digital but no architecture

129
00:06:36,560 --> 00:06:44,720
right. So such computing platform

130
00:06:41,120 --> 00:06:48,160
bear almost no similarity to neuronet

131
00:06:44,720 --> 00:06:49,840
network. So in that sense it is a

132
00:06:48,160 --> 00:06:52,560
simulator you know of the neuron

133
00:06:49,840 --> 00:06:54,960
network. It's a um it's a mainstream

134
00:06:52,560 --> 00:06:59,599
though it is something large scale you

135
00:06:54,960 --> 00:07:02,880
can use but it also face the deep crisis

136
00:06:59,599 --> 00:07:05,680
of energy efficiency right and our

137
00:07:02,880 --> 00:07:08,639
long-term goal of course is to have the

138
00:07:05,680 --> 00:07:11,440
natural intelligence emulator right

139
00:07:08,639 --> 00:07:14,080
rather than just a simulator and uh

140
00:07:11,440 --> 00:07:15,759
using analog approach but that is a very

141
00:07:14,080 --> 00:07:19,199
long-term goal you know it's not going

142
00:07:15,759 --> 00:07:22,880
to happen very soon at large scale but

143
00:07:19,199 --> 00:07:25,520
there is a middle stage. So in this uh

144
00:07:22,880 --> 00:07:27,280
middle stage what we can do is we can

145
00:07:25,520 --> 00:07:30,720
use some of the neuron science

146
00:07:27,280 --> 00:07:33,599
principles not all of them but some such

147
00:07:30,720 --> 00:07:36,800
as inmemory computing, analog computing,

148
00:07:33,599 --> 00:07:39,199
parallel computing with only these three

149
00:07:36,800 --> 00:07:42,960
principles for example you can already

150
00:07:39,199 --> 00:07:46,400
get a very efficient AI accelerator.

151
00:07:42,960 --> 00:07:48,800
So to accelerate um those uh computing

152
00:07:46,400 --> 00:07:51,680
and in this case of course you use both

153
00:07:48,800 --> 00:07:54,080
analog and digital it's a mixed signal

154
00:07:51,680 --> 00:07:57,280
system.

155
00:07:54,080 --> 00:08:02,080
So um in the next few slides maybe we

156
00:07:57,280 --> 00:08:04,639
can um first look at uh why this type of

157
00:08:02,080 --> 00:08:08,560
approach the digital mainstream um

158
00:08:04,639 --> 00:08:12,000
platform is uh in trouble and how this

159
00:08:08,560 --> 00:08:15,039
can help and why this you know is the

160
00:08:12,000 --> 00:08:16,960
bright future. So what's wrong with the

161
00:08:15,039 --> 00:08:18,879
digital system? the digital system. The

162
00:08:16,960 --> 00:08:21,199
biggest trouble as everybody here

163
00:08:18,879 --> 00:08:24,960
probably know already the digital

164
00:08:21,199 --> 00:08:28,560
hardware is way behind the demand of um

165
00:08:24,960 --> 00:08:31,599
the AI the algorithm needs as you can

166
00:08:28,560 --> 00:08:35,599
see everybody use as CH GPT I assume

167
00:08:31,599 --> 00:08:40,080
right but do you know um CH GPT is so

168
00:08:35,599 --> 00:08:42,479
power hungry it use 45 gawatt hour daily

169
00:08:40,080 --> 00:08:44,720
for inference so you may say I don't

170
00:08:42,479 --> 00:08:48,240
have no idea what this means actually

171
00:08:44,720 --> 00:08:52,160
this means means 1.5 million household

172
00:08:48,240 --> 00:08:54,880
power consumption daily just okay that

173
00:08:52,160 --> 00:08:57,839
is actually more than 1% of the

174
00:08:54,880 --> 00:09:00,880
household in the US and that will need a

175
00:08:57,839 --> 00:09:03,279
two to three um nuclear plants to power

176
00:09:00,880 --> 00:09:05,760
up just for CHP you know Google has a

177
00:09:03,279 --> 00:09:09,440
Google version and other everybody so

178
00:09:05,760 --> 00:09:12,560
you can imagine um how much uh you know

179
00:09:09,440 --> 00:09:15,279
energy we're going to um put in just

180
00:09:12,560 --> 00:09:17,920
those things so basically in the Last

181
00:09:15,279 --> 00:09:19,600
few years you can see the computing

182
00:09:17,920 --> 00:09:22,320
demand

183
00:09:19,600 --> 00:09:24,800
increased by million times.

184
00:09:22,320 --> 00:09:27,839
But around the same time frame the

185
00:09:24,800 --> 00:09:31,600
hardware improvement represented by the

186
00:09:27,839 --> 00:09:34,560
CP GPU of Nvidia it has made a lot of

187
00:09:31,600 --> 00:09:36,399
improvement but even that allowed is

188
00:09:34,560 --> 00:09:39,440
about 100 you know compared to the

189
00:09:36,399 --> 00:09:43,120
million times demand you have a huge gap

190
00:09:39,440 --> 00:09:45,200
and that gap cannot be met by continuous

191
00:09:43,120 --> 00:09:48,160
increase uh you know the computing

192
00:09:45,200 --> 00:09:51,120
efficiency just along the same direction

193
00:09:48,160 --> 00:09:53,600
using digital system as you can see the

194
00:09:51,120 --> 00:09:56,640
trend Right? So you have to use some

195
00:09:53,600 --> 00:09:59,200
disruptive technology to do that. Okay.

196
00:09:56,640 --> 00:10:01,600
So question is what's wrong with our

197
00:09:59,200 --> 00:10:04,080
digital system and uh and probably

198
00:10:01,600 --> 00:10:06,480
everybody know as you know the the

199
00:10:04,080 --> 00:10:09,440
device level MOS law is coming to an

200
00:10:06,480 --> 00:10:12,800
end. We know that and also there is a

201
00:10:09,440 --> 00:10:17,440
memory war um because the the processor

202
00:10:12,800 --> 00:10:21,279
performance is better than uh memory and

203
00:10:17,440 --> 00:10:24,480
the gap between the processor and memory

204
00:10:21,279 --> 00:10:27,839
is increasing with time not decreasing.

205
00:10:24,480 --> 00:10:32,880
So this is called the memory war. Memory

206
00:10:27,839 --> 00:10:35,680
is the bottleneck. And in addition

207
00:10:32,880 --> 00:10:38,160
the dard scaling law uh has ended a long

208
00:10:35,680 --> 00:10:41,200
time ago. Basically, even if you can

209
00:10:38,160 --> 00:10:45,120
make the device smaller and smaller, you

210
00:10:41,200 --> 00:10:48,079
cram more devices on the chip, but the

211
00:10:45,120 --> 00:10:49,920
power density of the chip is not

212
00:10:48,079 --> 00:10:52,480
constant anymore. It increase

213
00:10:49,920 --> 00:10:54,560
dramatically, which means even you have

214
00:10:52,480 --> 00:10:56,560
a lot more devices on the chip, you

215
00:10:54,560 --> 00:10:58,880
cannot utilize them simultaneously

216
00:10:56,560 --> 00:11:01,120
anyway before you burn the chip. That's

217
00:10:58,880 --> 00:11:03,519
called the heat wall, right? So those

218
00:11:01,120 --> 00:11:06,320
are at the device level. at the

219
00:11:03,519 --> 00:11:08,399
architecture level the challenge is as

220
00:11:06,320 --> 00:11:10,160
big if not bigger that's the phenomenal

221
00:11:08,399 --> 00:11:13,760
bottleneck I think you know a lot of

222
00:11:10,160 --> 00:11:16,880
people um should know here so you know

223
00:11:13,760 --> 00:11:20,320
in your phone your laptop um everything

224
00:11:16,880 --> 00:11:22,959
the stream digital system is bu built

225
00:11:20,320 --> 00:11:25,040
based on the nomad architecture

226
00:11:22,959 --> 00:11:27,519
which means you have a separate

227
00:11:25,040 --> 00:11:30,480
processor from the memory these two are

228
00:11:27,519 --> 00:11:32,480
physically separated you store your data

229
00:11:30,480 --> 00:11:34,640
in the memory when you need a processor

230
00:11:32,480 --> 00:11:36,640
You send it to the processor, get it

231
00:11:34,640 --> 00:11:38,880
processed, you send the result back,

232
00:11:36,640 --> 00:11:41,200
store here, you take the next piece of

233
00:11:38,880 --> 00:11:43,760
information and to the processor to

234
00:11:41,200 --> 00:11:46,720
process, you do it in sequential. And

235
00:11:43,760 --> 00:11:48,800
that was perfectly fine in the past. Uh

236
00:11:46,720 --> 00:11:51,839
when the computing did not involve a lot

237
00:11:48,800 --> 00:11:54,480
of data, but now we know computing are

238
00:11:51,839 --> 00:11:56,560
all pretty much datacentric. So

239
00:11:54,480 --> 00:12:00,240
constantly shuttering data between the

240
00:11:56,560 --> 00:12:02,320
two units takes a lot of time and a lot

241
00:12:00,240 --> 00:12:04,320
of energy.

242
00:12:02,320 --> 00:12:06,480
It's just too much we cannot afford.

243
00:12:04,320 --> 00:12:08,959
That's a a bottleneck called of a no man

244
00:12:06,480 --> 00:12:12,720
bottleneck. And this is a digital

245
00:12:08,959 --> 00:12:14,480
system. Our world um is analog. Our data

246
00:12:12,720 --> 00:12:18,399
from the analog world. You have to

247
00:12:14,480 --> 00:12:20,959
digitize it first then uh process in the

248
00:12:18,399 --> 00:12:24,399
uh digital system. The analog digital

249
00:12:20,959 --> 00:12:27,360
conversion also is notoriously time and

250
00:12:24,399 --> 00:12:29,440
energy consuming. Right? So how to solve

251
00:12:27,360 --> 00:12:33,839
those problems um for the digital

252
00:12:29,440 --> 00:12:39,200
system? So we learn from um something we

253
00:12:33,839 --> 00:12:42,160
know um to be the most energy efficient

254
00:12:39,200 --> 00:12:44,160
computer that's the brain. So then if

255
00:12:42,160 --> 00:12:47,200
you look at the nature then you find a

256
00:12:44,160 --> 00:12:50,480
lot of our problem have been solved um

257
00:12:47,200 --> 00:12:53,040
by you know millions years of evolution

258
00:12:50,480 --> 00:12:56,000
in animals in human being and the

259
00:12:53,040 --> 00:12:59,040
answers um for many problems are already

260
00:12:56,000 --> 00:13:02,480
there right so uh how does the brain um

261
00:12:59,040 --> 00:13:06,800
perform computing it use neuron network

262
00:13:02,480 --> 00:13:09,360
and the network is uh is neurons

263
00:13:06,800 --> 00:13:12,240
connected by synapse and the synapse is

264
00:13:09,360 --> 00:13:16,639
a memory here So store the data in

265
00:13:12,240 --> 00:13:19,839
synapse and the computing is done where

266
00:13:16,639 --> 00:13:22,000
the data is stored. So it is inmemory

267
00:13:19,839 --> 00:13:24,959
computing. So you don't need to shutter

268
00:13:22,000 --> 00:13:27,360
data between the processor and uh um and

269
00:13:24,959 --> 00:13:30,079
uh uh the memory. So that is a very big

270
00:13:27,360 --> 00:13:32,079
deal. It's inmemory computing computing

271
00:13:30,079 --> 00:13:34,560
happens everywhere in parallel. It's a

272
00:13:32,079 --> 00:13:37,040
parallel computing and uh it can deal

273
00:13:34,560 --> 00:13:39,120
with analog data directly. It's analog

274
00:13:37,040 --> 00:13:42,639
computing. So with these three

275
00:13:39,120 --> 00:13:44,959
principles you can build AI accelerator

276
00:13:42,639 --> 00:13:46,480
okay to be very energy efficient high

277
00:13:44,959 --> 00:13:49,120
throughput.

278
00:13:46,480 --> 00:13:51,760
So there is more and the brain is also

279
00:13:49,120 --> 00:13:54,000
known to be much more efficient in

280
00:13:51,760 --> 00:13:56,240
learning. It use much less data much

281
00:13:54,000 --> 00:13:59,279
less energy for learning learn much

282
00:13:56,240 --> 00:14:01,680
faster. It is because brain is using the

283
00:13:59,279 --> 00:14:03,600
neuron science principles for learning.

284
00:14:01,680 --> 00:14:08,160
Okay very different from what we are

285
00:14:03,600 --> 00:14:10,399
using in AI. Um so with those principles

286
00:14:08,160 --> 00:14:12,639
included then you will have something

287
00:14:10,399 --> 00:14:17,839
more like a natural intelligence

288
00:14:12,639 --> 00:14:20,959
emulator right. So to include those

289
00:14:17,839 --> 00:14:24,720
principles um into artificial system we

290
00:14:20,959 --> 00:14:27,360
build we will need to have a new type of

291
00:14:24,720 --> 00:14:30,880
devices rather than just um you know

292
00:14:27,360 --> 00:14:34,160
semos silicon devices. Um so there are a

293
00:14:30,880 --> 00:14:35,920
lot of types u memorist is one of the uh

294
00:14:34,160 --> 00:14:38,720
leading candidate uh we have been

295
00:14:35,920 --> 00:14:41,440
working on um for those who do not know

296
00:14:38,720 --> 00:14:44,000
how memor work it is shown here simp

297
00:14:41,440 --> 00:14:46,160
simplified mechanism you have two

298
00:14:44,000 --> 00:14:48,800
electrode and in the middle you have a

299
00:14:46,160 --> 00:14:51,360
oxide material let's say titanium oxide

300
00:14:48,800 --> 00:14:55,199
which can be viewed as a semiconductor

301
00:14:51,360 --> 00:14:58,000
with um wide band gap okay as a

302
00:14:55,199 --> 00:15:00,800
semiconductor it has its dopend the

303
00:14:58,000 --> 00:15:03,440
dopend here are oxygen and vacancies. So

304
00:15:00,800 --> 00:15:06,399
the doped region are conductive. The

305
00:15:03,440 --> 00:15:09,360
undoped region had a metal semiconductor

306
00:15:06,399 --> 00:15:11,279
barrier shaky like a barrier has uh give

307
00:15:09,360 --> 00:15:14,720
you high resistance. That's why the

308
00:15:11,279 --> 00:15:18,240
entire device now is in the off state.

309
00:15:14,720 --> 00:15:20,720
So the beauty here is um the dopant are

310
00:15:18,240 --> 00:15:23,680
mobile electrically mobile. So you can

311
00:15:20,720 --> 00:15:26,880
use electrical field to move the dopen

312
00:15:23,680 --> 00:15:28,480
and also dope the top region and make

313
00:15:26,880 --> 00:15:31,360
everywhere conductive. then you get the

314
00:15:28,480 --> 00:15:33,120
on state, right? And uh so if you want

315
00:15:31,360 --> 00:15:35,440
to change it back, you reverse your

316
00:15:33,120 --> 00:15:37,440
voltage polarity, you move the oxygen

317
00:15:35,440 --> 00:15:39,760
vacancy dopen back, you get the off

318
00:15:37,440 --> 00:15:42,320
state and you can stop in the middle as

319
00:15:39,760 --> 00:15:44,480
well. Um so the oxygen vacancy is the

320
00:15:42,320 --> 00:15:47,760
doped here, but you can use many other

321
00:15:44,480 --> 00:15:51,120
type of dopend with other type of um

322
00:15:47,760 --> 00:15:54,399
dialectric material to make a memory. Um

323
00:15:51,120 --> 00:15:57,839
so as you can see here um the doped

324
00:15:54,399 --> 00:16:01,600
migration is critical uh for the um

325
00:15:57,839 --> 00:16:07,440
function of such device. So what determs

326
00:16:01,600 --> 00:16:09,920
um uh the doped behavior um largely is

327
00:16:07,440 --> 00:16:12,079
the activation energy. For example, if

328
00:16:09,920 --> 00:16:15,759
you have high activation energy like

329
00:16:12,079 --> 00:16:18,639
oxygen vacancies in oxide, the um

330
00:16:15,759 --> 00:16:21,600
activation energy is high and you need

331
00:16:18,639 --> 00:16:24,240
to pay more energy to move them to

332
00:16:21,600 --> 00:16:26,079
switch the device. And the good thing is

333
00:16:24,240 --> 00:16:28,480
after you switch the device, you move

334
00:16:26,079 --> 00:16:31,199
them, they like to stay there, you know,

335
00:16:28,480 --> 00:16:33,040
in a stable way. Um so that you get

336
00:16:31,199 --> 00:16:35,920
nonvolatility.

337
00:16:33,040 --> 00:16:37,920
So then when you do reset you switch it

338
00:16:35,920 --> 00:16:40,480
back move it back you need to reverse

339
00:16:37,920 --> 00:16:42,880
your voltage polarity you move the ions

340
00:16:40,480 --> 00:16:45,600
back by drift. So that's why we call

341
00:16:42,880 --> 00:16:47,519
this drift memory right. So this type of

342
00:16:45,600 --> 00:16:52,320
device has very stable conductance

343
00:16:47,519 --> 00:16:55,519
levels it can be used to um serve as AI

344
00:16:52,320 --> 00:16:58,000
accelerator I talk about.

345
00:16:55,519 --> 00:17:01,279
So instead if you have a low activation

346
00:16:58,000 --> 00:17:05,600
energy such as silver in

347
00:17:01,279 --> 00:17:07,520
ox and the activation energy is so low

348
00:17:05,600 --> 00:17:10,480
you need to use very little energy to

349
00:17:07,520 --> 00:17:13,520
move them to switch them but easy come

350
00:17:10,480 --> 00:17:15,919
easy goes. So after you switch it, you

351
00:17:13,520 --> 00:17:18,319
remove the electrical bias, the ions can

352
00:17:15,919 --> 00:17:20,559
easily diffuse back to give you the

353
00:17:18,319 --> 00:17:22,160
reset automatically

354
00:17:20,559 --> 00:17:24,799
without electrical bias. That's a

355
00:17:22,160 --> 00:17:27,760
diffusion process for reset. That's why

356
00:17:24,799 --> 00:17:30,400
we call it a diffusive memory. So this

357
00:17:27,760 --> 00:17:32,880
is a volatile type of device. It has a

358
00:17:30,400 --> 00:17:35,200
very interesting ion dynamics. Remember

359
00:17:32,880 --> 00:17:37,280
at the first slide I talk about ion

360
00:17:35,200 --> 00:17:39,919
dynamics is very important for

361
00:17:37,280 --> 00:17:41,919
biological neuronet network. So this is

362
00:17:39,919 --> 00:17:43,600
very useful for something like a spike

363
00:17:41,919 --> 00:17:45,840
neural network and I'm going to talk

364
00:17:43,600 --> 00:17:48,400
more about that later. There's something

365
00:17:45,840 --> 00:17:51,679
in the middle if you have a activation

366
00:17:48,400 --> 00:17:53,520
energy not too high not too low and uh

367
00:17:51,679 --> 00:17:55,760
that give you also very interesting

368
00:17:53,520 --> 00:17:57,360
behavior. Uh for example this is a

369
00:17:55,760 --> 00:18:00,640
traditional device with a high

370
00:17:57,360 --> 00:18:03,200
activation energy oxygen vacancy as the

371
00:18:00,640 --> 00:18:05,840
dopen. So typically you need a high

372
00:18:03,200 --> 00:18:08,480
switching current and the switching can

373
00:18:05,840 --> 00:18:10,640
be abrupt. So those behavior are not

374
00:18:08,480 --> 00:18:14,240
something you lack when you use a device

375
00:18:10,640 --> 00:18:17,600
for training in your network. And uh

376
00:18:14,240 --> 00:18:19,760
interestingly if you rem replace this

377
00:18:17,600 --> 00:18:22,480
tantelum with something else like

378
00:18:19,760 --> 00:18:25,200
ruinium immediately you see switching

379
00:18:22,480 --> 00:18:27,440
behavior different and you can see now

380
00:18:25,200 --> 00:18:29,360
the switching need much lower two orders

381
00:18:27,440 --> 00:18:33,120
of magnitude lower current and the

382
00:18:29,360 --> 00:18:35,360
switching on and off are much gradual.

383
00:18:33,120 --> 00:18:37,679
So those low current gradual switching

384
00:18:35,360 --> 00:18:40,080
is something you like to have when you

385
00:18:37,679 --> 00:18:46,080
use a device for um neuronet network

386
00:18:40,080 --> 00:18:48,640
training and uh this is oxide agnostic

387
00:18:46,080 --> 00:18:50,559
um as long as you keep lucinium

388
00:18:48,640 --> 00:18:52,880
electrode you change your oxide it

389
00:18:50,559 --> 00:18:55,440
doesn't matter you can see um in this

390
00:18:52,880 --> 00:18:58,559
case it's get similar switching behavior

391
00:18:55,440 --> 00:19:01,200
local and gradual switching so it can be

392
00:18:58,559 --> 00:19:04,640
do um done with something else uh like

393
00:19:01,200 --> 00:19:06,240
in paravsk material Um uh I'm not an

394
00:19:04,640 --> 00:19:08,000
expert of this but we have a

395
00:19:06,240 --> 00:19:11,280
collaborator they make this type of

396
00:19:08,000 --> 00:19:13,840
device and it suppose uh in such

397
00:19:11,280 --> 00:19:16,559
materials mobile species is iodan

398
00:19:13,840 --> 00:19:19,440
vacancy which has a media activation

399
00:19:16,559 --> 00:19:22,799
energy that give you low current um

400
00:19:19,440 --> 00:19:25,120
switching and uh can have gradual

401
00:19:22,799 --> 00:19:27,039
switching behavior if you um do it

402
00:19:25,120 --> 00:19:30,640
right.

403
00:19:27,039 --> 00:19:35,039
Okay. So there are actually a lot of

404
00:19:30,640 --> 00:19:37,919
fantastic um study uh along those land

405
00:19:35,039 --> 00:19:40,880
in um this camp on this campus you know

406
00:19:37,919 --> 00:19:44,400
such as in Belg group um they use

407
00:19:40,880 --> 00:19:46,559
different ion proton and make uh in the

408
00:19:44,400 --> 00:19:49,679
three terminal device and show very nice

409
00:19:46,559 --> 00:19:53,120
potentiation depreciation and show very

410
00:19:49,679 --> 00:19:55,919
fast switching behavior and a lot more

411
00:19:53,120 --> 00:19:57,600
and also in Joan's group they use some

412
00:19:55,919 --> 00:20:00,960
very interesting

413
00:19:57,600 --> 00:20:04,000
um mechanism to control the ion

414
00:20:00,960 --> 00:20:07,840
dynamics. So they use dislocation and

415
00:20:04,000 --> 00:20:11,600
you use alloying approach to

416
00:20:07,840 --> 00:20:13,679
make much more uh reproducible device

417
00:20:11,600 --> 00:20:16,240
and control the variability. So there

418
00:20:13,679 --> 00:20:18,960
are a lot more I know today you know I'm

419
00:20:16,240 --> 00:20:22,400
supposed to talk about my work. So in

420
00:20:18,960 --> 00:20:24,400
our group um we have been uh in the last

421
00:20:22,400 --> 00:20:27,600
few years we have been uh trying to

422
00:20:24,400 --> 00:20:31,120
focus uh uh on using different type of

423
00:20:27,600 --> 00:20:34,000
memorist devices for different um

424
00:20:31,120 --> 00:20:37,280
computing applications and uh some of

425
00:20:34,000 --> 00:20:39,760
the main result are are listed here and

426
00:20:37,280 --> 00:20:42,320
it can be categorized into three groups.

427
00:20:39,760 --> 00:20:44,480
In the first group um as shown here

428
00:20:42,320 --> 00:20:47,760
we're trying to build the AI accelerator

429
00:20:44,480 --> 00:20:51,200
I talk about and in the second group um

430
00:20:47,760 --> 00:20:55,919
we are trying to build a more faithful

431
00:20:51,200 --> 00:20:59,440
device uh to um biological so um that

432
00:20:55,919 --> 00:21:02,640
trying to make um natural intelligence

433
00:20:59,440 --> 00:21:04,400
emulators and in the third category

434
00:21:02,640 --> 00:21:06,640
we're trying to interface it with

435
00:21:04,400 --> 00:21:10,480
sensors I will talk about that that is

436
00:21:06,640 --> 00:21:12,720
also important direction. Um I I will

437
00:21:10,480 --> 00:21:14,480
mention about that at the end. So you

438
00:21:12,720 --> 00:21:17,440
know due to the time limit I'm going to

439
00:21:14,480 --> 00:21:20,400
pick a few examples to talk about in

440
00:21:17,440 --> 00:21:23,600
each category. Um so the first category

441
00:21:20,400 --> 00:21:26,880
is to build AI accelerator in this case

442
00:21:23,600 --> 00:21:29,280
with drift memory and then I will talk

443
00:21:26,880 --> 00:21:32,559
about to build a natural intelligence

444
00:21:29,280 --> 00:21:34,480
emulator with diffusive memory and then

445
00:21:32,559 --> 00:21:37,679
last I'm going to talk about how do we

446
00:21:34,480 --> 00:21:41,360
interface it with sensors um using both

447
00:21:37,679 --> 00:21:44,880
drift and diffusive memory. Okay.

448
00:21:41,360 --> 00:21:47,280
So um the AI accelerator I assume a lot

449
00:21:44,880 --> 00:21:51,600
of you already know um the the trick

450
00:21:47,280 --> 00:21:54,159
here this is um basically our

451
00:21:51,600 --> 00:21:56,720
accelerator is accelerating one thing

452
00:21:54,159 --> 00:21:59,919
vector matrix multiplication which may

453
00:21:56,720 --> 00:22:03,600
sound trivial but it is a big deal for

454
00:21:59,919 --> 00:22:06,240
uh AI machine learning because over 85%

455
00:22:03,600 --> 00:22:09,120
of computing are pretty much nothing but

456
00:22:06,240 --> 00:22:13,360
vector matrix multiplications. So how we

457
00:22:09,120 --> 00:22:17,200
accelerate that is we can um map a

458
00:22:13,360 --> 00:22:20,159
mathematic matrix into the conductance

459
00:22:17,200 --> 00:22:22,880
um matrix of a memora array by switching

460
00:22:20,159 --> 00:22:26,559
each device to a certain conductance and

461
00:22:22,880 --> 00:22:29,280
then we can use uh a vector of voltage

462
00:22:26,559 --> 00:22:32,720
to represent the input data and the

463
00:22:29,280 --> 00:22:34,720
voltage is applied to the rows and we

464
00:22:32,720 --> 00:22:38,240
collect the current at the end of the

465
00:22:34,720 --> 00:22:42,080
column. So the current is the dotproduct

466
00:22:38,240 --> 00:22:46,159
of the input vector with uh the vector

467
00:22:42,080 --> 00:22:48,400
in the matrix. So this is because um you

468
00:22:46,159 --> 00:22:51,039
are using actually Ohm's law for

469
00:22:48,400 --> 00:22:54,080
multiplication everywhere and you are

470
00:22:51,039 --> 00:22:56,559
using Kokoh's current law for summation

471
00:22:54,080 --> 00:22:59,200
also everywhere simultaneously. So

472
00:22:56,559 --> 00:23:01,840
basically if you think about this it is

473
00:22:59,200 --> 00:23:04,720
a in-memory computing it's a parallel

474
00:23:01,840 --> 00:23:07,440
computing it's analog computing it

475
00:23:04,720 --> 00:23:09,919
implement the three principles the um

476
00:23:07,440 --> 00:23:12,480
the brain actually is using so that's

477
00:23:09,919 --> 00:23:14,480
why it can lead to all of the magnitude

478
00:23:12,480 --> 00:23:18,080
improvement in speed and energy

479
00:23:14,480 --> 00:23:20,799
efficiency and to um demonstrate this in

480
00:23:18,080 --> 00:23:22,960
hardware we did it a while ago we did

481
00:23:20,799 --> 00:23:25,919
show order of the magnitude improvement

482
00:23:22,960 --> 00:23:27,600
in throughput and energy efficiency

483
00:23:25,919 --> 00:23:30,000
However,

484
00:23:27,600 --> 00:23:33,200
for such analog computing, it also have

485
00:23:30,000 --> 00:23:36,400
its weakness. So the weakness is the

486
00:23:33,200 --> 00:23:38,159
precision. Okay. Um, so in order to

487
00:23:36,400 --> 00:23:40,240
improve the precision, we have done

488
00:23:38,159 --> 00:23:43,440
something at the device level and also

489
00:23:40,240 --> 00:23:46,320
the circuit level. At the device level,

490
00:23:43,440 --> 00:23:49,200
it turned out that a memory is not a

491
00:23:46,320 --> 00:23:53,760
resistor and after all it is noisier

492
00:23:49,200 --> 00:23:56,080
than a resistor. And if uh you um switch

493
00:23:53,760 --> 00:23:58,720
the device to a fresh state then you

494
00:23:56,080 --> 00:24:01,440
read it with a constant voltage then you

495
00:23:58,720 --> 00:24:03,840
can see uh the current reading is quite

496
00:24:01,440 --> 00:24:06,960
noisy. You have the random telegraph

497
00:24:03,840 --> 00:24:09,280
noise. With such big noise you cannot

498
00:24:06,960 --> 00:24:12,240
have many conductance levels very close

499
00:24:09,280 --> 00:24:15,279
to each other and still distinguishable.

500
00:24:12,240 --> 00:24:18,240
So fortunately actually work with uh

501
00:24:15,279 --> 00:24:21,679
Julie, professor Julie from MIT group um

502
00:24:18,240 --> 00:24:24,880
they did a theory analysis for us and we

503
00:24:21,679 --> 00:24:27,840
understand um the origin of the RTA and

504
00:24:24,880 --> 00:24:31,039
we derived a simple electrical operation

505
00:24:27,840 --> 00:24:34,159
protocol to den noiseise it and after D

506
00:24:31,039 --> 00:24:36,559
noiseise you can see um the resistance

507
00:24:34,159 --> 00:24:38,960
uh noise is now much smaller like the

508
00:24:36,559 --> 00:24:41,840
red curve now you can have two

509
00:24:38,960 --> 00:24:44,480
conductance levels very close to each

510
00:24:41,840 --> 00:24:46,480
and still distinguishable. So with that

511
00:24:44,480 --> 00:24:49,120
we can get a thousands of conductance

512
00:24:46,480 --> 00:24:51,200
levels from each device that is

513
00:24:49,120 --> 00:24:55,200
demonstrated um you know in a large

514
00:24:51,200 --> 00:24:57,360
array um memory fabricated on CMOS made

515
00:24:55,200 --> 00:25:00,480
in the foundry.

516
00:24:57,360 --> 00:25:03,120
So that 11 bit precision is good enough

517
00:25:00,480 --> 00:25:05,520
for inference application but not good

518
00:25:03,120 --> 00:25:08,159
enough for many other applications such

519
00:25:05,520 --> 00:25:10,320
as training of neuron networks such as

520
00:25:08,159 --> 00:25:13,360
signal processing such as scientific

521
00:25:10,320 --> 00:25:15,360
computing. To get even higher precision

522
00:25:13,360 --> 00:25:19,200
we need to do something at a circuit

523
00:25:15,360 --> 00:25:22,080
level. And uh to do that we designed

524
00:25:19,200 --> 00:25:24,960
architecture and algorithm allow us to

525
00:25:22,080 --> 00:25:27,360
in principle obtain arbitrarily high

526
00:25:24,960 --> 00:25:30,000
precision by we call it arrow

527
00:25:27,360 --> 00:25:32,720
compensation approach. Um so the concept

528
00:25:30,000 --> 00:25:35,120
is shown here. Um the circuit shown

529
00:25:32,720 --> 00:25:38,960
here. Um I'm not going to go into detail

530
00:25:35,120 --> 00:25:41,600
but rather um by saying that um the key

531
00:25:38,960 --> 00:25:44,159
is instead of using just a single device

532
00:25:41,600 --> 00:25:46,880
to represent a number we use the

533
00:25:44,159 --> 00:25:50,240
weighted sum of multiple devices to do

534
00:25:46,880 --> 00:25:53,840
that. For example, if you uh wanted to

535
00:25:50,240 --> 00:25:56,960
represent a number thousand microsemens,

536
00:25:53,840 --> 00:25:59,679
um you can program uh one device and

537
00:25:56,960 --> 00:26:01,840
because it's an analog always have arrow

538
00:25:59,679 --> 00:26:04,720
let's say 10% programming error for

539
00:26:01,840 --> 00:26:07,440
those devices then you end up with 900

540
00:26:04,720 --> 00:26:09,440
microsemen. Okay. So instead of keep

541
00:26:07,440 --> 00:26:12,400
programming the same device over over

542
00:26:09,440 --> 00:26:14,960
again hope you luckily hit on this

543
00:26:12,400 --> 00:26:17,600
target which is not guaranteed need many

544
00:26:14,960 --> 00:26:20,320
steps and you just program a second

545
00:26:17,600 --> 00:26:23,279
device to compensate that arrow you know

546
00:26:20,320 --> 00:26:25,600
the arrow is 100 microsemens you program

547
00:26:23,279 --> 00:26:30,400
the second device you give it a smaller

548
00:26:25,600 --> 00:26:32,799
weight um to make sure it uh converge so

549
00:26:30,400 --> 00:26:35,440
then with let's say the second device is

550
00:26:32,799 --> 00:26:36,320
happen to be programmed also with 10%

551
00:26:35,440 --> 00:26:39,039
arrow

552
00:26:36,320 --> 00:26:43,039
And then with this smaller weight um it

553
00:26:39,039 --> 00:26:45,360
end up representing 90 microsemens. Then

554
00:26:43,039 --> 00:26:46,960
then you add these two together you have

555
00:26:45,360 --> 00:26:49,679
99

556
00:26:46,960 --> 00:26:51,840
uh 10 microsemens away from your target.

557
00:26:49,679 --> 00:26:54,799
You can keep programming the third

558
00:26:51,840 --> 00:26:57,679
device with a even smaller weight and

559
00:26:54,799 --> 00:27:00,159
you you can get um uh this device

560
00:26:57,679 --> 00:27:02,320
represent nine microsemen. Then you add

561
00:27:00,159 --> 00:27:07,039
them together with their weight, you get

562
00:27:02,320 --> 00:27:11,039
999 microsemens. And that

563
00:27:07,039 --> 00:27:13,120
drops the arrow from 10% to 0.1% by just

564
00:27:11,039 --> 00:27:15,200
using three devices, right? You can keep

565
00:27:13,120 --> 00:27:19,039
doing that uh if you need a higher

566
00:27:15,200 --> 00:27:22,080
precision. Um so the circuit overhead

567
00:27:19,039 --> 00:27:25,120
turned out to be quite a affordable. Um

568
00:27:22,080 --> 00:27:30,000
and we made this demonstration

569
00:27:25,120 --> 00:27:34,400
um in Foundry made SOC. So here is a 12

570
00:27:30,000 --> 00:27:38,080
inch wafer and um there are a lot of

571
00:27:34,400 --> 00:27:40,480
dice on it. Each d is So obviously I

572
00:27:38,080 --> 00:27:43,520
cannot bring the

573
00:27:40,480 --> 00:27:47,120
wafer there. I hope I have something

574
00:27:43,520 --> 00:27:49,600
here that that is one of the dice. I

575
00:27:47,120 --> 00:27:52,080
think you can pass around actually it's

576
00:27:49,600 --> 00:27:57,120
one of the the chip we made u with

577
00:27:52,080 --> 00:28:01,520
meister on cmos. Um so and each of the

578
00:27:57,120 --> 00:28:08,320
SOC uh has uh 10 neuron processing unit

579
00:28:01,520 --> 00:28:11,520
and uh each MPU has um 256 by 256 array

580
00:28:08,320 --> 00:28:13,279
and um then each of the device is

581
00:28:11,520 --> 00:28:18,640
capable of a thousand conductance

582
00:28:13,279 --> 00:28:20,640
levels. Um so with onchip uh CPU SRAMM

583
00:28:18,640 --> 00:28:24,399
and everything you you basically have a

584
00:28:20,640 --> 00:28:27,760
computer you can program um the chip to

585
00:28:24,399 --> 00:28:29,840
different neural network or different

586
00:28:27,760 --> 00:28:34,320
other things for different applications

587
00:28:29,840 --> 00:28:37,360
right so um first uh we use it for high

588
00:28:34,320 --> 00:28:40,720
precision scientific computing um in

589
00:28:37,360 --> 00:28:44,240
this case we use it to do the magneto

590
00:28:40,720 --> 00:28:47,440
hydrodnamic system simulation And you

591
00:28:44,240 --> 00:28:50,880
can see the comparison between a digital

592
00:28:47,440 --> 00:28:53,520
system and analog system. The result are

593
00:28:50,880 --> 00:28:55,520
quite similar. Um the difference is you

594
00:28:53,520 --> 00:28:59,520
have order the magnitude higher energy

595
00:28:55,520 --> 00:29:01,360
efficiency for the analog chip. Okay. Um

596
00:28:59,520 --> 00:29:05,200
and because now it you have high

597
00:29:01,360 --> 00:29:07,360
precision you can use it to also process

598
00:29:05,200 --> 00:29:10,799
signal not just neuron network. For

599
00:29:07,360 --> 00:29:14,000
example, you can program the array into

600
00:29:10,799 --> 00:29:18,880
uh DFT which is discrete for transform

601
00:29:14,000 --> 00:29:22,640
coefficient and then this um matrix can

602
00:29:18,880 --> 00:29:25,600
be used to um process the input signal.

603
00:29:22,640 --> 00:29:30,159
The input signal here is for example a

604
00:29:25,600 --> 00:29:34,000
spectra spectra uh the the signal is in

605
00:29:30,159 --> 00:29:36,799
time domain. Okay. Then um this DFT can

606
00:29:34,000 --> 00:29:40,000
convert the signal from time domain to

607
00:29:36,799 --> 00:29:42,559
frequency domain within one shot. And

608
00:29:40,000 --> 00:29:44,960
then the frequency domain spectra

609
00:29:42,559 --> 00:29:47,760
represent the fingerprint of the input

610
00:29:44,960 --> 00:29:50,799
signal and that fingerprint can be sent

611
00:29:47,760 --> 00:29:53,840
to a neuron network also made of memora

612
00:29:50,799 --> 00:29:58,080
array to do the classification. So

613
00:29:53,840 --> 00:30:00,480
basically um what you can do is to use

614
00:29:58,080 --> 00:30:03,039
the signal processing layer as a first

615
00:30:00,480 --> 00:30:06,000
layer of your network. You form a fuse

616
00:30:03,039 --> 00:30:09,600
network um to do the signal processing

617
00:30:06,000 --> 00:30:11,679
to the classification um all u on the

618
00:30:09,600 --> 00:30:13,919
same chip. Right?

619
00:30:11,679 --> 00:30:16,080
So in the next few slides I'm going to

620
00:30:13,919 --> 00:30:19,440
show you how we experimentally

621
00:30:16,080 --> 00:30:21,760
demonstrate the DFT and then the uh

622
00:30:19,440 --> 00:30:25,279
neuron network then the fuse network for

623
00:30:21,760 --> 00:30:27,919
signal processing in real time and um I

624
00:30:25,279 --> 00:30:30,399
apologize uh we cannot hear the sound

625
00:30:27,919 --> 00:30:32,799
here basically student is playing a

626
00:30:30,399 --> 00:30:36,720
music here and you can see the music

627
00:30:32,799 --> 00:30:41,120
temporal spectra uh here and that is

628
00:30:36,720 --> 00:30:43,600
converted by the DFT on tip here into

629
00:30:41,120 --> 00:30:46,720
spectra domain. That's a red curve you

630
00:30:43,600 --> 00:30:50,159
see uh here. And to compare we also use

631
00:30:46,720 --> 00:30:52,960
the software to uh convert it. Um you

632
00:30:50,159 --> 00:30:55,440
can see the software and hardware result

633
00:30:52,960 --> 00:30:57,360
quite comparable. So you're supposed to

634
00:30:55,440 --> 00:30:59,919
hear the sound but you can hear uh you

635
00:30:57,360 --> 00:31:02,000
can see it from there. So this shows we

636
00:30:59,919 --> 00:31:05,120
can use it to do signal processing right

637
00:31:02,000 --> 00:31:09,360
DFT and then to show the neuron network

638
00:31:05,120 --> 00:31:12,080
on chip. Um and we do uh uh real time

639
00:31:09,360 --> 00:31:15,039
video uh edge detection. This is the

640
00:31:12,080 --> 00:31:18,399
video student is holding and this is a

641
00:31:15,039 --> 00:31:21,360
processed uh by software. This is the

642
00:31:18,399 --> 00:31:24,480
process result by the chip here. And you

643
00:31:21,360 --> 00:31:27,919
can see they are quite comparable. So

644
00:31:24,480 --> 00:31:29,679
now we have the signal processing DFT.

645
00:31:27,919 --> 00:31:32,320
We have the neuron network. We can

646
00:31:29,679 --> 00:31:35,360
combine them together to form a fuse

647
00:31:32,320 --> 00:31:39,600
network also programmed on the chip. And

648
00:31:35,360 --> 00:31:41,440
then um we can use it to classify a

649
00:31:39,600 --> 00:31:42,559
temporal signal.

650
00:31:41,440 --> 00:31:45,279
>> Um

651
00:31:42,559 --> 00:31:49,120
>> the sound is converted into spectra

652
00:31:45,279 --> 00:31:51,039
signal by the DFT layer and then the

653
00:31:49,120 --> 00:31:53,760
spectra is classified by the neuron

654
00:31:51,039 --> 00:31:55,919
network all done in real time on the

655
00:31:53,760 --> 00:31:58,080
chip and the monitor and the computer

656
00:31:55,919 --> 00:32:00,640
here is just to visualize the input and

657
00:31:58,080 --> 00:32:05,039
output. So as you can see here the real

658
00:32:00,640 --> 00:32:07,120
time um you know processing is um

659
00:32:05,039 --> 00:32:09,120
something already you know demonstrated

660
00:32:07,120 --> 00:32:12,159
on the chip even though it's not large

661
00:32:09,120 --> 00:32:15,440
enough to do large language model yet at

662
00:32:12,159 --> 00:32:19,120
this stage but uh it is coming you know

663
00:32:15,440 --> 00:32:22,080
um as you see um there are quite a lot

664
00:32:19,120 --> 00:32:24,000
memorist effort now um is going

665
00:32:22,080 --> 00:32:27,200
commercialization

666
00:32:24,000 --> 00:32:28,720
um as shown by this recent perspective

667
00:32:27,200 --> 00:32:32,080
paper

668
00:32:28,720 --> 00:32:34,960
So that is something very close. Okay.

669
00:32:32,080 --> 00:32:37,679
Now let's talk about something still far

670
00:32:34,960 --> 00:32:40,080
at the you know early stage that is

671
00:32:37,679 --> 00:32:43,120
something like the natural intelligence

672
00:32:40,080 --> 00:32:46,000
emulator. Um

673
00:32:43,120 --> 00:32:48,320
for example you by by diffusing meister.

674
00:32:46,000 --> 00:32:50,960
So you know you would say we don't know

675
00:32:48,320 --> 00:32:54,080
much about na natural intelligence yet.

676
00:32:50,960 --> 00:32:57,360
How can we emulate it right? And so the

677
00:32:54,080 --> 00:33:00,399
approach here is one of the approach

678
00:32:57,360 --> 00:33:02,480
which is bottom down is a understanding

679
00:33:00,399 --> 00:33:04,000
by building. So we're trying to build

680
00:33:02,480 --> 00:33:06,480
something then we play with it. We're

681
00:33:04,000 --> 00:33:09,519
trying to use it to help us understand

682
00:33:06,480 --> 00:33:12,159
some of the bio principles. That's a um

683
00:33:09,519 --> 00:33:15,279
in order to do that we need to have a

684
00:33:12,159 --> 00:33:18,399
lot of faithful building blocks um to

685
00:33:15,279 --> 00:33:22,000
the bios system and actually uh we

686
00:33:18,399 --> 00:33:25,600
published this in the special issue

687
00:33:22,000 --> 00:33:28,399
organized by Billy in chemical uh review

688
00:33:25,600 --> 00:33:30,880
uh it's a summary of our effort um in

689
00:33:28,399 --> 00:33:34,720
this direction

690
00:33:30,880 --> 00:33:38,399
so if you look at the biological network

691
00:33:34,720 --> 00:33:41,840
um and the major component capab apps um

692
00:33:38,399 --> 00:33:44,880
s dendrites and immediately you will see

693
00:33:41,840 --> 00:33:47,840
the ions and the ion migration the ion

694
00:33:44,880 --> 00:33:51,679
channels are playing the critical role

695
00:33:47,840 --> 00:33:53,039
in generate their functions. So um like

696
00:33:51,679 --> 00:33:55,440
you know in the first slides that we

697
00:33:53,039 --> 00:33:59,200
talk about we wanted to emulate the ion

698
00:33:55,440 --> 00:34:03,760
dynamics okay you know to build those um

699
00:33:59,200 --> 00:34:05,840
component faithfully. Um so take a

700
00:34:03,760 --> 00:34:08,639
neuron as an example. You know the

701
00:34:05,840 --> 00:34:11,280
neuron uh key function is to generate

702
00:34:08,639 --> 00:34:13,599
action potential like this. And if you

703
00:34:11,280 --> 00:34:16,000
look closely how the action potential is

704
00:34:13,599 --> 00:34:18,639
generated, you would find that you know

705
00:34:16,000 --> 00:34:22,960
first you have a ATP pump that is

706
00:34:18,639 --> 00:34:26,000
pumping sodium ion out and um potassium

707
00:34:22,960 --> 00:34:30,320
ion in and to build the concentration

708
00:34:26,000 --> 00:34:32,240
gradient of um potassium ion and sodium

709
00:34:30,320 --> 00:34:37,839
ion just in the opposite direction the

710
00:34:32,240 --> 00:34:40,800
gradient and then um the this neuron me

711
00:34:37,839 --> 00:34:44,000
uh start to integrate input signal. Once

712
00:34:40,800 --> 00:34:47,280
the integration is over threshold then

713
00:34:44,000 --> 00:34:49,839
the um voltage controlled ion channel is

714
00:34:47,280 --> 00:34:51,919
open. Once this channel is open remember

715
00:34:49,839 --> 00:34:55,679
we have the concentration gradient built

716
00:34:51,919 --> 00:34:58,560
by the ATP pump. Now the sodium ion

717
00:34:55,679 --> 00:35:00,880
flood in once the gate is open the

718
00:34:58,560 --> 00:35:04,000
motion is driven by the concentration

719
00:35:00,880 --> 00:35:08,160
gradient and that will increase the

720
00:35:04,000 --> 00:35:10,560
polarization and uh the then the sodium

721
00:35:08,160 --> 00:35:12,720
ion channel will close by itself. Even

722
00:35:10,560 --> 00:35:15,520
before this close the potassium ion

723
00:35:12,720 --> 00:35:18,160
channel will open then the potassium ion

724
00:35:15,520 --> 00:35:20,400
will flood out also driven by its own

725
00:35:18,160 --> 00:35:23,440
concentration gradient and that will

726
00:35:20,400 --> 00:35:25,599
decrease the polarization and then the

727
00:35:23,440 --> 00:35:28,320
whole thing will go to a resting state

728
00:35:25,599 --> 00:35:31,359
and the ATP pump is keep working you

729
00:35:28,320 --> 00:35:34,079
know maintain this resting state.

730
00:35:31,359 --> 00:35:36,160
So in this whole process um the iron

731
00:35:34,079 --> 00:35:40,079
channel open and close is very

732
00:35:36,160 --> 00:35:42,880
important. So in order to um build a ion

733
00:35:40,079 --> 00:35:46,320
channel emulator by a nano device we use

734
00:35:42,880 --> 00:35:50,160
a diffusive memory and uh uh here you

735
00:35:46,320 --> 00:35:53,359
looking at a snapshots of INC2 TEM

736
00:35:50,160 --> 00:35:55,359
showing at each moment um when you apply

737
00:35:53,359 --> 00:35:58,079
electrical bias between the two

738
00:35:55,359 --> 00:36:00,400
electrode of a diffusing memory you can

739
00:35:58,079 --> 00:36:03,920
see in the gap you have a silver doped

740
00:36:00,400 --> 00:36:05,839
silicon silicon nitri oxide at the very

741
00:36:03,920 --> 00:36:08,240
beginning when you apply electrical bias

742
00:36:05,839 --> 00:36:10,480
between the two electron control and uh

743
00:36:08,240 --> 00:36:13,839
because you have a gap here the current

744
00:36:10,480 --> 00:36:17,680
remain low this process mimic the ion

745
00:36:13,839 --> 00:36:19,760
channel closed state and but it it's

746
00:36:17,680 --> 00:36:22,079
even though uh the current is low

747
00:36:19,760 --> 00:36:24,480
actually something is happening inside

748
00:36:22,079 --> 00:36:27,920
the device you can see the silver are

749
00:36:24,480 --> 00:36:30,800
actually moving and this is more and

750
00:36:27,920 --> 00:36:33,119
more silver accumulated here this mimic

751
00:36:30,800 --> 00:36:35,440
the integration process of the neuron

752
00:36:33,119 --> 00:36:37,920
before firing

753
00:36:35,440 --> 00:36:39,520
and then uh Once it reaches threshold

754
00:36:37,920 --> 00:36:41,680
you build a complete channel bridging

755
00:36:39,520 --> 00:36:44,880
the two electrode then the current shoot

756
00:36:41,680 --> 00:36:47,760
up. Okay. So that mimic the process it

757
00:36:44,880 --> 00:36:50,160
generate action potential the spike and

758
00:36:47,760 --> 00:36:53,280
the channel size is about 4 nanometer in

759
00:36:50,160 --> 00:36:57,119
diameter. So interestingly

760
00:36:53,280 --> 00:37:00,480
um after firing um there's no electrical

761
00:36:57,119 --> 00:37:04,640
signal anymore and zero electrical bias

762
00:37:00,480 --> 00:37:08,800
you can see this filament reshape itself

763
00:37:04,640 --> 00:37:12,240
spontaneously into a sphere. So break

764
00:37:08,800 --> 00:37:14,640
this channel and this mimic the process

765
00:37:12,240 --> 00:37:17,760
uh the channel the neuron goes back to

766
00:37:14,640 --> 00:37:20,400
resting state after firing automatically

767
00:37:17,760 --> 00:37:22,800
doesn't take any energy right so this is

768
00:37:20,400 --> 00:37:26,320
a diffusion process from here to here

769
00:37:22,800 --> 00:37:30,240
because there are no electrical bias and

770
00:37:26,320 --> 00:37:34,160
so this give us a faceful ion channel

771
00:37:30,240 --> 00:37:36,560
and then we can also use a capacitor to

772
00:37:34,160 --> 00:37:39,440
mimic the membrane capacitor put them

773
00:37:36,560 --> 00:37:42,640
together you have a simple leak,

774
00:37:39,440 --> 00:37:46,160
integrate and fire neuron. Okay, so as

775
00:37:42,640 --> 00:37:49,599
shown by this experimental data here, it

776
00:37:46,160 --> 00:37:52,800
does show the leak, integrate and fire.

777
00:37:49,599 --> 00:37:55,520
Um, so with this, it's a more capable

778
00:37:52,800 --> 00:37:58,400
neuron. It's a dynamic neuron. Um, now

779
00:37:55,520 --> 00:38:02,640
you can combine with synapse and you

780
00:37:58,400 --> 00:38:05,440
build a neuron network full of memory.

781
00:38:02,640 --> 00:38:07,920
And uh so you only have eight neurons

782
00:38:05,440 --> 00:38:10,640
here, small neuron network, but because

783
00:38:07,920 --> 00:38:12,720
the neuron itself is much more capable

784
00:38:10,640 --> 00:38:14,960
and it can perform unsupervised

785
00:38:12,720 --> 00:38:17,520
learning. And we were very excited to

786
00:38:14,960 --> 00:38:21,119
show for the first time you can um

787
00:38:17,520 --> 00:38:23,680
experimentally demonstrate unsupervised

788
00:38:21,119 --> 00:38:26,800
learning with the um memoristive

789
00:38:23,680 --> 00:38:29,040
network. And so this is because now you

790
00:38:26,800 --> 00:38:31,440
have a more capable neuron. But even

791
00:38:29,040 --> 00:38:34,960
this neuron like I said it just have

792
00:38:31,440 --> 00:38:37,680
very simple function like a leak

793
00:38:34,960 --> 00:38:39,280
integrate and fire. But we know in the

794
00:38:37,680 --> 00:38:42,640
real neuron there are a lot more

795
00:38:39,280 --> 00:38:44,800
functions. Okay for example it can also

796
00:38:42,640 --> 00:38:48,240
cascade signal from one layer to the

797
00:38:44,800 --> 00:38:50,800
other. Neuron is active device. Um so it

798
00:38:48,240 --> 00:38:53,920
also have intrinsic plasticity. It have

799
00:38:50,800 --> 00:38:57,520
refractory period. It has stochasticity.

800
00:38:53,920 --> 00:39:00,400
So in order to capture all these six key

801
00:38:57,520 --> 00:39:03,359
functions, if you build your neuron with

802
00:39:00,400 --> 00:39:06,000
a traditional CMOS, it will be a bulky

803
00:39:03,359 --> 00:39:09,440
circuit like this. You will need 24

804
00:39:06,000 --> 00:39:11,680
transistors and three capacitors. And uh

805
00:39:09,440 --> 00:39:14,160
we know we have so many 10 to the 11th

806
00:39:11,680 --> 00:39:16,880
neurons in the brain. You cannot afford

807
00:39:14,160 --> 00:39:19,440
to have such bulky circuit to represent

808
00:39:16,880 --> 00:39:23,200
just one. And it will cost a lot of

809
00:39:19,440 --> 00:39:26,400
energy and and uh area.

810
00:39:23,200 --> 00:39:30,079
So fortunately now we have um the

811
00:39:26,400 --> 00:39:34,320
dynamic device like diffusing meister we

812
00:39:30,079 --> 00:39:37,680
can build a neuron with uh only three

813
00:39:34,320 --> 00:39:41,440
component but still capture all the six

814
00:39:37,680 --> 00:39:45,280
key functions. Okay. So in this um

815
00:39:41,440 --> 00:39:47,680
neurons you have a transistor and you

816
00:39:45,280 --> 00:39:50,560
have a drift memory and a diffusing

817
00:39:47,680 --> 00:39:53,200
memory because memoristas are typically

818
00:39:50,560 --> 00:39:56,640
much smaller than the transistor. So you

819
00:39:53,200 --> 00:39:59,359
can build the memory on top of the gate

820
00:39:56,640 --> 00:40:02,000
build a 3D neuron and the entire

821
00:39:59,359 --> 00:40:04,960
footprint of this neuron is no bigger

822
00:40:02,000 --> 00:40:07,680
than a single transistor. Okay. So that

823
00:40:04,960 --> 00:40:11,520
easy to say than done. Actually uh it

824
00:40:07,680 --> 00:40:15,119
took a student almost a year to um f to

825
00:40:11,520 --> 00:40:17,599
successfully integrate the um two uh

826
00:40:15,119 --> 00:40:20,480
trans uh memorist one drift memory one

827
00:40:17,599 --> 00:40:23,839
diffuser memory on top of a transistor

828
00:40:20,480 --> 00:40:28,079
and show you can get this very nice um

829
00:40:23,839 --> 00:40:30,560
you know neuron behavior. And if you um

830
00:40:28,079 --> 00:40:33,839
look at in detail we experimentally show

831
00:40:30,560 --> 00:40:36,560
that such 3D neuron can show the leaky

832
00:40:33,839 --> 00:40:39,280
integration firing threshold cascade

833
00:40:36,560 --> 00:40:42,000
propagation intrinsic plasticity

834
00:40:39,280 --> 00:40:45,520
refractory period and stoasticity. The

835
00:40:42,000 --> 00:40:47,680
the six key functions all um come from

836
00:40:45,520 --> 00:40:50,960
you know area with just a single

837
00:40:47,680 --> 00:40:54,000
transistor. The reason is you utilize um

838
00:40:50,960 --> 00:40:56,000
the ion dynamics in um you know simple

839
00:40:54,000 --> 00:40:59,119
devices.

840
00:40:56,000 --> 00:41:01,520
So we do not have a a large array of

841
00:40:59,119 --> 00:41:04,160
this yet. And in order to understand the

842
00:41:01,520 --> 00:41:07,280
array level performance, we build a

843
00:41:04,160 --> 00:41:10,160
model that can faithfully reproduce uh

844
00:41:07,280 --> 00:41:14,079
the neuron behavior. And with that model

845
00:41:10,160 --> 00:41:17,440
uh we are able to um in simulation build

846
00:41:14,079 --> 00:41:21,119
a large recurrent spiking neuron network

847
00:41:17,440 --> 00:41:25,119
with thousands of neurons and multiple

848
00:41:21,119 --> 00:41:27,200
layers. So with this um

849
00:41:25,119 --> 00:41:30,880
recurrent spiking neuron network we are

850
00:41:27,200 --> 00:41:32,800
able to um classify spiking hedinber

851
00:41:30,880 --> 00:41:37,680
digits. basically it's a spoken version

852
00:41:32,800 --> 00:41:39,760
of amnest. Um so we can see um the

853
00:41:37,680 --> 00:41:43,680
neuronet network can do a decent

854
00:41:39,760 --> 00:41:47,119
classification with good accuracy of

855
00:41:43,680 --> 00:41:50,640
those spoken digits. Um but we also show

856
00:41:47,119 --> 00:41:53,200
if you remove one of the six neuronal

857
00:41:50,640 --> 00:41:55,839
functions from your neuron um your

858
00:41:53,200 --> 00:41:58,880
network performance is going to be

859
00:41:55,839 --> 00:42:01,680
affected um negatively. So that shows

860
00:41:58,880 --> 00:42:06,079
all those functions are actually useful

861
00:42:01,680 --> 00:42:09,760
for the neuron. Okay. So we um also

862
00:42:06,079 --> 00:42:12,079
compare um the memoristive neuron with

863
00:42:09,760 --> 00:42:14,160
the neurons in the living monkey. And

864
00:42:12,079 --> 00:42:17,599
with our collaborators of course they

865
00:42:14,160 --> 00:42:20,480
did the experiment to measure the neuron

866
00:42:17,599 --> 00:42:23,040
firing behavior uh in different regions

867
00:42:20,480 --> 00:42:25,599
in the monkey and as you can see these

868
00:42:23,040 --> 00:42:28,400
are the memoristive neuron and uh

869
00:42:25,599 --> 00:42:31,200
qualitative quantitative qualitatively

870
00:42:28,400 --> 00:42:32,960
they they are comparable. Um so

871
00:42:31,200 --> 00:42:35,599
basically this all due to the ion

872
00:42:32,960 --> 00:42:38,000
dynamics you have.

873
00:42:35,599 --> 00:42:41,359
Okay. Now let me switch gear talk about

874
00:42:38,000 --> 00:42:44,480
the last part um is um integration with

875
00:42:41,359 --> 00:42:47,520
the sensor. So if you think about um you

876
00:42:44,480 --> 00:42:50,079
know the natural intelligence system

877
00:42:47,520 --> 00:42:53,839
they evolved

878
00:42:50,079 --> 00:42:56,319
um by better deal with their

879
00:42:53,839 --> 00:42:59,280
environment. Okay. Right. So that means

880
00:42:56,319 --> 00:43:03,520
they have to be interact with the

881
00:42:59,280 --> 00:43:07,119
environment very um uh efficiently. Um

882
00:43:03,520 --> 00:43:10,560
so this comes from the fact that the

883
00:43:07,119 --> 00:43:14,480
natural intelligence usually seamlessly

884
00:43:10,560 --> 00:43:17,440
integrate sensing encoding and learning

885
00:43:14,480 --> 00:43:20,079
all together. And for example uh you

886
00:43:17,440 --> 00:43:22,800
have a different sensors and uh they

887
00:43:20,079 --> 00:43:27,599
generate sensing signal. Then the sense

888
00:43:22,800 --> 00:43:30,720
signal will be uh encoded into spiking

889
00:43:27,599 --> 00:43:33,119
signal either many spikes with a

890
00:43:30,720 --> 00:43:35,440
different frequency or a single spike

891
00:43:33,119 --> 00:43:38,160
with a different spike timing to

892
00:43:35,440 --> 00:43:40,880
represent the data. And such a spike

893
00:43:38,160 --> 00:43:42,800
signal is going to be sent to the brain

894
00:43:40,880 --> 00:43:45,280
which have the neuron network to do the

895
00:43:42,800 --> 00:43:48,160
sensor fusion and also the learning

896
00:43:45,280 --> 00:43:52,000
typically unsupervised learning. To

897
00:43:48,160 --> 00:43:54,319
mimic this system in artificial um

898
00:43:52,000 --> 00:43:57,760
system uh you will need to have a

899
00:43:54,319 --> 00:44:00,560
sensors then you will have a spike

900
00:43:57,760 --> 00:44:03,119
encoder and then you have a neuron

901
00:44:00,560 --> 00:44:07,680
network to do the sensor fusion and

902
00:44:03,119 --> 00:44:10,480
learning. So if again you do it with the

903
00:44:07,680 --> 00:44:13,200
SMOS circuit because SMOS circuit is not

904
00:44:10,480 --> 00:44:15,359
designed optimized for this purpose and

905
00:44:13,200 --> 00:44:17,680
you are going to need a bulky circuit

906
00:44:15,359 --> 00:44:22,720
like this just for encoding okay

907
00:44:17,680 --> 00:44:26,560
encoding signal and again um using uh

908
00:44:22,720 --> 00:44:30,880
the new type of memora diffuser memora

909
00:44:26,560 --> 00:44:34,560
we show that we can uh realize uh spike

910
00:44:30,880 --> 00:44:37,599
encoder with just a single diffusive

911
00:44:34,560 --> 00:44:41,440
memory. Okay, rather than such a bulky

912
00:44:37,599 --> 00:44:44,160
circuit and as experimentally shown here

913
00:44:41,440 --> 00:44:46,400
um you can have a sensor in this case

914
00:44:44,160 --> 00:44:48,880
let's say a pressure sensor then a

915
00:44:46,400 --> 00:44:51,680
diffusing meister circuit wise it looks

916
00:44:48,880 --> 00:44:54,560
like this you have a sensor when you

917
00:44:51,680 --> 00:44:56,480
have a pressure it generate a voltage

918
00:44:54,560 --> 00:44:58,800
that voltage is going to turn on the

919
00:44:56,480 --> 00:45:03,119
diffusing memory generate a current

920
00:44:58,800 --> 00:45:05,200
spike okay if the pressure is high then

921
00:45:03,119 --> 00:45:07,760
the voltage here is high then the

922
00:45:05,200 --> 00:45:11,119
merista is turned on quickly, it

923
00:45:07,760 --> 00:45:14,560
generate a spike within a very short um

924
00:45:11,119 --> 00:45:17,440
delay time. That's a spike timing. If

925
00:45:14,560 --> 00:45:20,079
the pressure is low, then the voltage is

926
00:45:17,440 --> 00:45:23,119
low. Then the memory is still turned on

927
00:45:20,079 --> 00:45:26,400
but only after a long delay time. That's

928
00:45:23,119 --> 00:45:28,880
a long spiking time. So basically this

929
00:45:26,400 --> 00:45:33,680
diffusing meister

930
00:45:28,880 --> 00:45:36,000
convert the pressure amplitude into a

931
00:45:33,680 --> 00:45:38,480
spike timing right that's a spike

932
00:45:36,000 --> 00:45:41,440
encoder and if you think about this you

933
00:45:38,480 --> 00:45:44,240
don't need uh external power and the

934
00:45:41,440 --> 00:45:47,200
pressure sensor itself power the memory

935
00:45:44,240 --> 00:45:49,280
it's a self-powered system and it's

936
00:45:47,200 --> 00:45:51,760
event driven only if you have event

937
00:45:49,280 --> 00:45:54,400
something happen and it's very energy

938
00:45:51,760 --> 00:45:56,000
efficient so this is not just for

939
00:45:54,400 --> 00:45:58,560
pressure sensor. It works for many

940
00:45:56,000 --> 00:46:02,160
sensors such as temperature sensor, such

941
00:45:58,560 --> 00:46:04,079
as light sensor as we shown here. For

942
00:46:02,160 --> 00:46:07,280
the light sensor case, a single

943
00:46:04,079 --> 00:46:10,400
diffusing memory, it convert the light

944
00:46:07,280 --> 00:46:13,680
intensity into spike timing. Okay, it's

945
00:46:10,400 --> 00:46:16,240
self-powered event driven. So now we can

946
00:46:13,680 --> 00:46:19,520
combine those component together to

947
00:46:16,240 --> 00:46:22,800
build a sensing and learning system as

948
00:46:19,520 --> 00:46:25,200
shown here. Um experimentally we have a

949
00:46:22,800 --> 00:46:28,720
pressure sensor. Oh that's a pressure

950
00:46:25,200 --> 00:46:31,200
sensor. We have a light sensor and we

951
00:46:28,720 --> 00:46:34,000
have a diffusive memory. So two of them

952
00:46:31,200 --> 00:46:37,119
serve as a spike encoder. We have a

953
00:46:34,000 --> 00:46:41,040
drift memory here serve as the synapse

954
00:46:37,119 --> 00:46:43,839
um for the learning part. And um then um

955
00:46:41,040 --> 00:46:46,400
we use this experimentally demonstrate

956
00:46:43,839 --> 00:46:49,520
some learning like input timing

957
00:46:46,400 --> 00:46:52,400
dependent plasticity. This is a learning

958
00:46:49,520 --> 00:46:54,560
row commonly observed in the bios system

959
00:46:52,400 --> 00:46:56,800
like STDP.

960
00:46:54,560 --> 00:47:01,200
So this learning row actually generate

961
00:46:56,800 --> 00:47:03,839
the biggest um weight change um if the

962
00:47:01,200 --> 00:47:06,960
two spike signal are synchronized. So

963
00:47:03,839 --> 00:47:10,000
use something like this. Uh we can find

964
00:47:06,960 --> 00:47:13,920
many applications such as we use it as a

965
00:47:10,000 --> 00:47:16,000
lightning um detector. Um so because a

966
00:47:13,920 --> 00:47:18,640
lightning event you have light you have

967
00:47:16,000 --> 00:47:23,680
sound and sound is like the pressure. So

968
00:47:18,640 --> 00:47:27,119
you can um use such a system to um um in

969
00:47:23,680 --> 00:47:29,680
simulation we show we can use it to uh

970
00:47:27,119 --> 00:47:32,319
distribute it in a large area to detect

971
00:47:29,680 --> 00:47:34,240
the lightning event which may not be

972
00:47:32,319 --> 00:47:35,839
important here but it's important for

973
00:47:34,240 --> 00:47:38,000
where I come from. You know in

974
00:47:35,839 --> 00:47:40,880
California each year we have a lot of

975
00:47:38,000 --> 00:47:42,560
wildfire caused by lightning event. We

976
00:47:40,880 --> 00:47:45,200
want to know the lightning you know

977
00:47:42,560 --> 00:47:47,599
distribution. So but anyway if you look

978
00:47:45,200 --> 00:47:51,359
at this system we don't need external

979
00:47:47,599 --> 00:47:54,160
power. It is used the sensor itself to

980
00:47:51,359 --> 00:47:57,200
drive the system. It's self powered and

981
00:47:54,160 --> 00:48:00,560
it naturally do the sensor fusion with

982
00:47:57,200 --> 00:48:03,599
multimodel sensor and the learning is

983
00:48:00,560 --> 00:48:05,839
unsupervised learning and also you know

984
00:48:03,599 --> 00:48:08,960
um realized inside the system with only

985
00:48:05,839 --> 00:48:13,920
few component right.

986
00:48:08,960 --> 00:48:17,440
Okay. So uh to summarize um I talk about

987
00:48:13,920 --> 00:48:20,000
um you know for uh the the memory the

988
00:48:17,440 --> 00:48:22,800
activation energy of the mobile ions are

989
00:48:20,000 --> 00:48:27,040
important um it largely determines the

990
00:48:22,800 --> 00:48:29,200
memoristive behavior and if you have um

991
00:48:27,040 --> 00:48:31,760
large activation energy you get a drift

992
00:48:29,200 --> 00:48:35,520
memory it's nonvolatile reset has to be

993
00:48:31,760 --> 00:48:38,079
done by drift but it shows stable dense

994
00:48:35,520 --> 00:48:40,480
conductance levels with high precision

995
00:48:38,079 --> 00:48:43,040
it is good for AI accelerator

996
00:48:40,480 --> 00:48:45,359
application. If you have low activation

997
00:48:43,040 --> 00:48:48,160
energy, then you get a volatile device.

998
00:48:45,359 --> 00:48:51,520
The reset has to be done by diffusion

999
00:48:48,160 --> 00:48:55,040
and uh it ex shows desirable ion

1000
00:48:51,520 --> 00:48:58,240
dynamics. It will be very efficient for

1001
00:48:55,040 --> 00:49:00,880
a natural intelligence emulator and both

1002
00:48:58,240 --> 00:49:03,760
diffusive memory drift memory can be

1003
00:49:00,880 --> 00:49:06,160
used directly interface with the sensors

1004
00:49:03,760 --> 00:49:09,839
to enable efficient and intelligent

1005
00:49:06,160 --> 00:49:12,480
sensing system. So after all I think um

1006
00:49:09,839 --> 00:49:15,680
neomorphic computing will be long lived.

1007
00:49:12,480 --> 00:49:18,800
Okay that's another takeaway message if

1008
00:49:15,680 --> 00:49:21,599
you don't remember anything else. So I

1009
00:49:18,800 --> 00:49:26,640
like to um thank my co-workers post

1010
00:49:21,599 --> 00:49:29,760
doctors and uh as shown here and I also

1011
00:49:26,640 --> 00:49:33,720
like to um thank the sponsors um for the

1012
00:49:29,760 --> 00:49:33,720
work. Thank you for your attention

1013
00:49:34,559 --> 00:49:38,000
>> for the great talk. Thank you. Wonderful

1014
00:49:37,520 --> 00:49:38,559
talk.

1015
00:49:38,000 --> 00:49:41,760
>> Thank you.

1016
00:49:38,559 --> 00:49:44,880
>> So we have time for questions. Um anyone

1017
00:49:41,760 --> 00:49:46,240
wants to go first? Let me there also

1018
00:49:44,880 --> 00:49:48,880
>> go ahead.

1019
00:49:46,240 --> 00:49:51,040
>> Okay. Thank you Joh. Uh thank you very

1020
00:49:48,880 --> 00:49:53,839
nice talk and I have a question about uh

1021
00:49:51,040 --> 00:49:55,599
the capacity capacity of the member

1022
00:49:53,839 --> 00:49:58,480
company uh elements. Mhm.

1023
00:49:55,599 --> 00:50:01,440
>> So if the capacity of uh this technology

1024
00:49:58,480 --> 00:50:04,720
is not that large than the uh large

1025
00:50:01,440 --> 00:50:06,720
language models or media scale models is

1026
00:50:04,720 --> 00:50:07,680
hard to convince ourself that's a

1027
00:50:06,720 --> 00:50:08,240
inmemory computing.

1028
00:50:07,680 --> 00:50:11,599
>> Yeah.

1029
00:50:08,240 --> 00:50:14,160
>> So uh I want to ask what's the uh ideal

1030
00:50:11,599 --> 00:50:16,559
scale or ideal capacity of

1031
00:50:14,160 --> 00:50:18,400
>> uh single chip level and also single

1032
00:50:16,559 --> 00:50:19,359
array level computing.

1033
00:50:18,400 --> 00:50:21,520
>> Right. Yeah.

1034
00:50:19,359 --> 00:50:25,839
>> Um okay then that's a very good

1035
00:50:21,520 --> 00:50:29,760
question. So um so far we are um each

1036
00:50:25,839 --> 00:50:33,920
cell is one T1R it has one transistor

1037
00:50:29,760 --> 00:50:36,640
one um memory um so the density is

1038
00:50:33,920 --> 00:50:38,960
limited by the T by the transistor you

1039
00:50:36,640 --> 00:50:41,280
know as everybody know even if you use a

1040
00:50:38,960 --> 00:50:43,599
three nanometer transistor tech 3

1041
00:50:41,280 --> 00:50:46,240
nanometer technology node a transistor

1042
00:50:43,599 --> 00:50:50,079
you make is three is still going to be

1043
00:50:46,240 --> 00:50:52,640
3,000 nanome square but the memory

1044
00:50:50,079 --> 00:50:55,280
itself can be as small as 3 nanometer by

1045
00:50:52,640 --> 00:50:58,079
3 nanometer. We even showed a 2

1046
00:50:55,280 --> 00:51:00,480
nanometer by 2 nanometer area for a

1047
00:50:58,079 --> 00:51:02,880
single memory. So memory will be much

1048
00:51:00,480 --> 00:51:06,240
smaller than the transistor. So the

1049
00:51:02,880 --> 00:51:08,800
density now is um limited by the

1050
00:51:06,240 --> 00:51:11,680
transistor. Then you can think um at

1051
00:51:08,800 --> 00:51:16,240
this stage it will be a DM density. DM

1052
00:51:11,680 --> 00:51:19,040
is one T1C right and u um so DM you can

1053
00:51:16,240 --> 00:51:22,079
stack right you can stack and the high

1054
00:51:19,040 --> 00:51:24,960
bandwidth memory but uh uh the issue

1055
00:51:22,079 --> 00:51:27,680
there is once you stack more then the

1056
00:51:24,960 --> 00:51:30,160
energy consumption will be too high the

1057
00:51:27,680 --> 00:51:33,599
heat dissipation will bother you you

1058
00:51:30,160 --> 00:51:36,240
cannot go probably more than 16 layers

1059
00:51:33,599 --> 00:51:38,240
so those devices they they will have a

1060
00:51:36,240 --> 00:51:40,319
much lower energy consumption so they

1061
00:51:38,240 --> 00:51:41,200
probably afford more you know um

1062
00:51:40,319 --> 00:51:46,720
stacking

1063
00:51:41,200 --> 00:51:49,040
layers and um also people trying to uh

1064
00:51:46,720 --> 00:51:52,800
um develop

1065
00:51:49,040 --> 00:51:55,200
um one T multiple R. Okay. So in

1066
00:51:52,800 --> 00:51:59,119
addition to that um you should also

1067
00:51:55,200 --> 00:52:01,280
think one R you know you can have 11 bit

1068
00:51:59,119 --> 00:52:02,800
precision maybe even higher. So that

1069
00:52:01,280 --> 00:52:06,800
equivalently

1070
00:52:02,800 --> 00:52:09,440
um you know one one T1C

1071
00:52:06,800 --> 00:52:11,680
that is only one bit. This is a one cell

1072
00:52:09,440 --> 00:52:13,680
one transistor region you have 11 bit

1073
00:52:11,680 --> 00:52:16,400
right that density. So if you think

1074
00:52:13,680 --> 00:52:20,480
about that actually the density is a

1075
00:52:16,400 --> 00:52:23,520
pretty promising. We we can do uh larger

1076
00:52:20,480 --> 00:52:27,040
one and we can scale it uh you know with

1077
00:52:23,520 --> 00:52:30,160
two down to 5 nanometer 3 nanometer and

1078
00:52:27,040 --> 00:52:34,640
eventually I think a single chip we can

1079
00:52:30,160 --> 00:52:38,079
probably have um you know um

1080
00:52:34,640 --> 00:52:40,160
uh let's see

1081
00:52:38,079 --> 00:52:42,640
>> gig giga will be something for sure.

1082
00:52:40,160 --> 00:52:44,480
Yeah we can do yeah yeah yeah giga.

1083
00:52:42,640 --> 00:52:46,160
Yeah. Yeah. But you know still like the

1084
00:52:44,480 --> 00:52:48,480
languages model we will need a more than

1085
00:52:46,160 --> 00:52:54,319
that. Yeah. So we we will need a multi-

1086
00:52:48,480 --> 00:52:58,240
layers. Um but uh uh for um

1087
00:52:54,319 --> 00:53:00,079
many edge applications um probably we we

1088
00:52:58,240 --> 00:53:02,160
yeah we do not need a large language

1089
00:53:00,079 --> 00:53:04,319
model. I I think that will be far but

1090
00:53:02,160 --> 00:53:07,920
there are a lot of lowhanging fruit we

1091
00:53:04,319 --> 00:53:09,200
need to pick first. Yeah.

1092
00:53:07,920 --> 00:53:12,760
>> Yeah.

1093
00:53:09,200 --> 00:53:12,760
>> Any other questions?

1094
00:53:13,040 --> 00:53:17,520
Hi, thanks. Um, for the LAF transistor,

1095
00:53:16,079 --> 00:53:19,920
you showed that the area decreased by

1096
00:53:17,520 --> 00:53:22,079
about order of magnitude. Do you have a

1097
00:53:19,920 --> 00:53:22,800
metric for how much power decreases from

1098
00:53:22,079 --> 00:53:23,680
CMOS?

1099
00:53:22,800 --> 00:53:24,640
>> For the neuron, right?

1100
00:53:23,680 --> 00:53:27,280
>> Yeah. For a neuron,

1101
00:53:24,640 --> 00:53:31,839
>> right? Yeah. So if you look at the paper

1102
00:53:27,280 --> 00:53:34,079
I think uh um for per neuron even now um

1103
00:53:31,839 --> 00:53:36,400
for our experimental demo is still a

1104
00:53:34,079 --> 00:53:39,200
micron two mic four micrometer by four

1105
00:53:36,400 --> 00:53:42,960
micrometer neuron that was already go to

1106
00:53:39,200 --> 00:53:45,839
a um femto something like that and we

1107
00:53:42,960 --> 00:53:48,319
also did the uh calculation to project

1108
00:53:45,839 --> 00:53:50,400
when you use advanced technology node go

1109
00:53:48,319 --> 00:53:52,559
to three nanometer technology node we

1110
00:53:50,400 --> 00:53:55,040
can go down to

1111
00:53:52,559 --> 00:53:58,160
yeah so that that will be actually

1112
00:53:55,040 --> 00:54:01,720
comparable or even lower than the bio.

1113
00:53:58,160 --> 00:54:01,720
Yeah. Mhm.

1114
00:54:04,319 --> 00:54:07,599
>> How do you perform the plasticity

1115
00:54:06,240 --> 00:54:11,280
experiment?

1116
00:54:07,599 --> 00:54:13,760
>> Um for the last part or which part?

1117
00:54:11,280 --> 00:54:15,839
>> Uh the second part?

1118
00:54:13,760 --> 00:54:17,520
>> So you showed some six different

1119
00:54:15,839 --> 00:54:20,400
principles of neurons which are

1120
00:54:17,520 --> 00:54:21,040
satisfied by the by the artificial

1121
00:54:20,400 --> 00:54:24,640
neuron.

1122
00:54:21,040 --> 00:54:25,440
>> Right. So the intrinsic plasticity you

1123
00:54:24,640 --> 00:54:28,880
mean

1124
00:54:25,440 --> 00:54:31,440
>> the intrinsic plasticity or

1125
00:54:28,880 --> 00:54:32,559
um so you

1126
00:54:31,440 --> 00:54:35,760
>> after this

1127
00:54:32,559 --> 00:54:37,520
>> this one this one right so the the you

1128
00:54:35,760 --> 00:54:39,839
talk about this

1129
00:54:37,520 --> 00:54:42,559
>> right so that's the intrinsic plasticity

1130
00:54:39,839 --> 00:54:45,040
that that is neuron not synapse we know

1131
00:54:42,559 --> 00:54:47,200
synapse can have you know the plasticity

1132
00:54:45,040 --> 00:54:50,160
synapse is just learning so this is a

1133
00:54:47,200 --> 00:54:52,559
neuron meaning um you know it fire from

1134
00:54:50,160 --> 00:54:56,079
one fire cycle to the next fire cycle.

1135
00:54:52,559 --> 00:54:58,480
If the next fire cycle is close and then

1136
00:54:56,079 --> 00:55:00,880
the firing behavior threshold everything

1137
00:54:58,480 --> 00:55:03,920
will be slightly different. Basically

1138
00:55:00,880 --> 00:55:05,760
the neuron if it fire frequently it has

1139
00:55:03,920 --> 00:55:10,240
the history

1140
00:55:05,760 --> 00:55:13,200
uh yate but if you um next firing event

1141
00:55:10,240 --> 00:55:15,200
is after a long time then it forget that

1142
00:55:13,200 --> 00:55:17,680
effect. So that is the intrinsic

1143
00:55:15,200 --> 00:55:20,319
plasticity we talk about. It's intrinsic

1144
00:55:17,680 --> 00:55:22,559
inside of the device. We don't need to

1145
00:55:20,319 --> 00:55:25,119
do anything to get it. We just need to

1146
00:55:22,559 --> 00:55:27,280
build a device like you know our bion

1147
00:55:25,119 --> 00:55:29,599
neuron using ions. It will be

1148
00:55:27,280 --> 00:55:33,119
automatically there. So that is the way

1149
00:55:29,599 --> 00:55:35,599
you know we say um understanding by

1150
00:55:33,119 --> 00:55:38,319
building because if you have something

1151
00:55:35,599 --> 00:55:41,440
like a diffusive meister they share the

1152
00:55:38,319 --> 00:55:43,920
physical mechanism uh at a physics level

1153
00:55:41,440 --> 00:55:46,400
with the biological component then

1154
00:55:43,920 --> 00:55:48,319
automatically you get multiple

1155
00:55:46,400 --> 00:55:51,200
properties you're looking for all

1156
00:55:48,319 --> 00:55:53,359
together without have having to engineer

1157
00:55:51,200 --> 00:55:55,760
one by one engineer this one lose

1158
00:55:53,359 --> 00:55:58,400
another one right so that that is the

1159
00:55:55,760 --> 00:56:03,040
philosophy if you make it similar you

1160
00:55:58,400 --> 00:56:04,319
will get similar function multiple.

1161
00:56:03,040 --> 00:56:05,440
>> Great. Thank you for the talk. So you

1162
00:56:04,319 --> 00:56:06,640
did a great job of showing the

1163
00:56:05,440 --> 00:56:08,079
applications when you have a mature

1164
00:56:06,640 --> 00:56:10,000
technology that you can fabricate at

1165
00:56:08,079 --> 00:56:12,000
scale and what you can do with like two

1166
00:56:10,000 --> 00:56:13,280
of an emerging technology. Do you have a

1167
00:56:12,000 --> 00:56:14,880
sense for what the lowhanging fruits are

1168
00:56:13,280 --> 00:56:17,280
once that emerging technology gets to

1169
00:56:14,880 --> 00:56:19,520
like 10, 100, a thousand? Like what is

1170
00:56:17,280 --> 00:56:20,960
the kind of opportunities and the gap

1171
00:56:19,520 --> 00:56:23,440
between this brand new technology and

1172
00:56:20,960 --> 00:56:26,079
something that's kind of more mature?

1173
00:56:23,440 --> 00:56:29,440
Um so you are talking about more like

1174
00:56:26,079 --> 00:56:30,799
the AI accelerator or the

1175
00:56:29,440 --> 00:56:32,799
>> whatever you think like what is the best

1176
00:56:30,799 --> 00:56:36,480
thing you can do once you have 10 to 100

1177
00:56:32,799 --> 00:56:38,480
of some new emerging technology

1178
00:56:36,480 --> 00:56:41,280
>> um

1179
00:56:38,480 --> 00:56:46,640
so this is not specific I don't know how

1180
00:56:41,280 --> 00:56:49,680
to um so for if if we think about the AI

1181
00:56:46,640 --> 00:56:51,599
accelerator right so we we can show uh

1182
00:56:49,680 --> 00:56:52,640
orders of the magnitude higher energy

1183
00:56:51,599 --> 00:56:55,599
efficiency

1184
00:56:52,640 --> 00:56:57,760
Um then GPU for example right then then

1185
00:56:55,599 --> 00:57:00,000
then you can find a lot of edge

1186
00:56:57,760 --> 00:57:03,359
applications you use it in the Google

1187
00:57:00,000 --> 00:57:06,079
glass you use it in XR and you use in

1188
00:57:03,359 --> 00:57:09,680
variables and those are all the places

1189
00:57:06,079 --> 00:57:11,839
you require two things one is a low

1190
00:57:09,680 --> 00:57:14,000
energy because it's energy constraint

1191
00:57:11,839 --> 00:57:15,839
and second you want to process

1192
00:57:14,000 --> 00:57:18,480
information in real time you know high

1193
00:57:15,839 --> 00:57:20,480
throughput and th those fit such

1194
00:57:18,480 --> 00:57:23,520
technology you know perfectly well

1195
00:57:20,480 --> 00:57:27,599
because that's its advantage age for the

1196
00:57:23,520 --> 00:57:30,240
uh for the um a longer term. If you also

1197
00:57:27,599 --> 00:57:34,799
have um the natural intelligence

1198
00:57:30,240 --> 00:57:38,160
emulator part um and capability then um

1199
00:57:34,799 --> 00:57:40,480
the robotics you know much smarter

1200
00:57:38,160 --> 00:57:42,559
robotics that can learn from the

1201
00:57:40,480 --> 00:57:45,599
environment by interacting with the

1202
00:57:42,559 --> 00:57:48,160
envir environment and uh it can learn

1203
00:57:45,599 --> 00:57:52,160
without using backup propagation. those

1204
00:57:48,160 --> 00:57:55,200
will all be possible and uh um so but

1205
00:57:52,160 --> 00:57:59,200
that still I think is a little bit far

1206
00:57:55,200 --> 00:58:03,920
but even with the AI accelerator

1207
00:57:59,200 --> 00:58:07,119
um we actually start to um use our chips

1208
00:58:03,920 --> 00:58:11,040
the SOC chip I showed you to develop

1209
00:58:07,119 --> 00:58:13,920
some algorithm that is simple enough but

1210
00:58:11,040 --> 00:58:18,079
also general enough and it can be

1211
00:58:13,920 --> 00:58:22,240
applied on a a single SOC see and

1212
00:58:18,079 --> 00:58:25,680
perform um learning and uh uh planning

1213
00:58:22,240 --> 00:58:28,799
all on chip so that it can already you

1214
00:58:25,680 --> 00:58:32,160
know fly a drone or something. Um so th

1215
00:58:28,799 --> 00:58:33,599
those are all you know the things um can

1216
00:58:32,160 --> 00:58:34,160
be used. Yeah.

1217
00:58:33,599 --> 00:58:36,559
>> Thank you.

1218
00:58:34,160 --> 00:58:38,799
>> Yeah.

1219
00:58:36,559 --> 00:58:40,319
>> Okay. One last question.

1220
00:58:38,799 --> 00:58:41,680
uh given you did a really good job

1221
00:58:40,319 --> 00:58:43,599
motivating the problem that you're

1222
00:58:41,680 --> 00:58:45,280
trying to solve the energy consumption

1223
00:58:43,599 --> 00:58:46,880
also just the growth of AI and I was

1224
00:58:45,280 --> 00:58:48,640
wondering um you covered briefly some of

1225
00:58:46,880 --> 00:58:50,400
the alternative approaches and I assume

1226
00:58:48,640 --> 00:58:51,920
there there are many um I was wondering

1227
00:58:50,400 --> 00:58:55,200
if you could maybe shine a light on what

1228
00:58:51,920 --> 00:58:56,400
are some other approaches to uh inmemory

1229
00:58:55,200 --> 00:58:57,920
computing that could also be very

1230
00:58:56,400 --> 00:58:58,720
promising and should be investigated

1231
00:58:57,920 --> 00:59:01,599
more

1232
00:58:58,720 --> 00:59:04,000
>> so other technology you mean um the

1233
00:59:01,599 --> 00:59:05,920
technology to use inmemory computing or

1234
00:59:04,000 --> 00:59:07,440
something outside of inmemory computing

1235
00:59:05,920 --> 00:59:08,960
>> I think better described as like other

1236
00:59:07,440 --> 00:59:10,480
approaches is to doing it outside of

1237
00:59:08,960 --> 00:59:12,640
Memorist or

1238
00:59:10,480 --> 00:59:14,400
>> right. Yeah. Okay. So outside of

1239
00:59:12,640 --> 00:59:16,720
Memorista we can use other type of

1240
00:59:14,400 --> 00:59:20,480
devices. I I know there are many

1241
00:59:16,720 --> 00:59:24,559
promising candidates uh you know like

1242
00:59:20,480 --> 00:59:28,079
Pharaoh electric um M RAM phase change

1243
00:59:24,559 --> 00:59:31,599
and um some of the new devices like um

1244
00:59:28,079 --> 00:59:34,880
Belgi working on EC RAM um you know um

1245
00:59:31,599 --> 00:59:38,079
so those are all very interesting um

1246
00:59:34,880 --> 00:59:41,920
possibilities to as a building blocks

1247
00:59:38,079 --> 00:59:44,640
and but outside of a in-memory computing

1248
00:59:41,920 --> 00:59:49,040
um I know there are you know other types

1249
00:59:44,640 --> 00:59:51,040
the quantum and uh and using photonics

1250
00:59:49,040 --> 00:59:53,440
though those those are all you know very

1251
00:59:51,040 --> 00:59:56,240
interesting as well it's out of my

1252
00:59:53,440 --> 00:59:58,720
expertise I don't want to you know to

1253
00:59:56,240 --> 01:00:00,640
make say too much on that something I

1254
00:59:58,720 --> 01:00:02,240
don't really know much yeah

1255
01:00:00,640 --> 01:00:05,200
>> thank you

1256
01:00:02,240 --> 01:00:10,440
>> great thanks a lot for the great talk

1257
01:00:05,200 --> 01:00:10,440
let's uh help me thank speaker again

