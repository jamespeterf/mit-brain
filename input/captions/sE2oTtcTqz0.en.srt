1
00:00:00,320 --> 00:00:05,720
welcome to the AI generation from

2
00:00:02,399 --> 00:00:07,560
research to impactful use cases panel

3
00:00:05,720 --> 00:00:10,320
and we will address today with you

4
00:00:07,560 --> 00:00:12,960
technical advancement uh the challenge

5
00:00:10,320 --> 00:00:15,000
of the mod of the curent Innovation and

6
00:00:12,960 --> 00:00:17,880
any question you will have so I will be

7
00:00:15,000 --> 00:00:20,080
the moderator my name is OD Oliva very

8
00:00:17,880 --> 00:00:24,160
French apology for my accent I've been

9
00:00:20,080 --> 00:00:26,320
at 20 years and I have two lives two

10
00:00:24,160 --> 00:00:28,880
their lives first I'm a senior research

11
00:00:26,320 --> 00:00:31,119
scientist I have a lab uh that actually

12
00:00:28,880 --> 00:00:33,079
focus on human machine intelligence for

13
00:00:31,119 --> 00:00:35,960
the past 20 years and this is really

14
00:00:33,079 --> 00:00:38,239
very sci-fi thinking for me like right

15
00:00:35,960 --> 00:00:40,920
now we are thinking about how can an AI

16
00:00:38,239 --> 00:00:43,440
with your minds but then the day today

17
00:00:40,920 --> 00:00:46,360
my other life is actually to take care

18
00:00:43,440 --> 00:00:49,760
for the schatzman College of computing

19
00:00:46,360 --> 00:00:52,719
of how the research done at MIT can

20
00:00:49,760 --> 00:00:56,920
Bridge with our industry and with all of

21
00:00:52,719 --> 00:00:58,960
you and in that uh role um I lead uh in

22
00:00:56,920 --> 00:01:00,719
the leadership of the college so I'm the

23
00:00:58,960 --> 00:01:03,440
director of the Strategic industry

24
00:01:00,719 --> 00:01:05,799
engagement I'm also the MIT director of

25
00:01:03,440 --> 00:01:08,119
the MIT IBM Watson AI lab we are going

26
00:01:05,799 --> 00:01:12,320
to talk today about it and I call it the

27
00:01:08,119 --> 00:01:14,680
MIT AI Hardware program so uh when I was

28
00:01:12,320 --> 00:01:16,720
invited to put this panel together I was

29
00:01:14,680 --> 00:01:18,439
really thrilled because we are going to

30
00:01:16,720 --> 00:01:20,840
have three

31
00:01:18,439 --> 00:01:25,000
formidable panelists today you're going

32
00:01:20,840 --> 00:01:29,960
to know how to say in French some of the

33
00:01:25,000 --> 00:01:32,880
English words um and uh really you they

34
00:01:29,960 --> 00:01:37,399
all have life that I would like to have

35
00:01:32,880 --> 00:01:41,399
so I would like to welcome intra deep

36
00:01:37,399 --> 00:01:44,759
gosh who is the Chief Executive Officer

37
00:01:41,399 --> 00:01:47,680
of fujit research of America thank you

38
00:01:44,759 --> 00:01:47,680
please come join

39
00:01:48,920 --> 00:01:54,680
us

40
00:01:51,000 --> 00:01:58,360
oops um I would like to David to David

41
00:01:54,680 --> 00:02:01,119
welcome David Cox vice president of AI

42
00:01:58,360 --> 00:02:04,399
model at IBM research and the IBM

43
00:02:01,119 --> 00:02:06,479
director of the MIT IBM Watson AI lab Hi

44
00:02:04,399 --> 00:02:09,719
how are you since

45
00:02:06,479 --> 00:02:09,719
yesterday dear

46
00:02:10,360 --> 00:02:16,519
colleagues and um I would like to

47
00:02:12,760 --> 00:02:20,440
welcome Ramin asani who co-founder and

48
00:02:16,519 --> 00:02:23,000
CEO of liquid AI a startup that you

49
00:02:20,440 --> 00:02:25,640
founded with the famous Cel director

50
00:02:23,000 --> 00:02:28,680
Daniel arus and you're also a machine

51
00:02:25,640 --> 00:02:31,840
learning research affiliate at Cel very

52
00:02:28,680 --> 00:02:31,840
very happy to have the three

53
00:02:34,560 --> 00:02:40,920
so we are going to have about 45 minutes

54
00:02:37,480 --> 00:02:44,280
uh together and I took from the previous

55
00:02:40,920 --> 00:02:47,319
talk that the audience um will walk away

56
00:02:44,280 --> 00:02:50,360
ready to ask the right questions so we

57
00:02:47,319 --> 00:02:54,560
have the right panelist for this and uh

58
00:02:50,360 --> 00:02:56,560
so I'll start with uh so few question

59
00:02:54,560 --> 00:02:59,120
for you and your introduction but what I

60
00:02:56,560 --> 00:03:01,000
was thinking is that I think I see here

61
00:02:59,120 --> 00:03:03,040
oh no why not I think myself then I'm

62
00:03:01,000 --> 00:03:05,959
going to see your question and maybe we

63
00:03:03,040 --> 00:03:08,400
can make this more lively and have you

64
00:03:05,959 --> 00:03:12,120
asking question during those 45 minutes

65
00:03:08,400 --> 00:03:15,560
but first let's know about our three um

66
00:03:12,120 --> 00:03:19,000
panelist here so a would you tell about

67
00:03:15,560 --> 00:03:23,799
yourself and what you're doing thank you

68
00:03:19,000 --> 00:03:27,319
OD uh so um uh I am Indra Kos my people

69
00:03:23,799 --> 00:03:31,040
called me IG and I had Fujitsu research

70
00:03:27,319 --> 00:03:33,319
of America so Fujitsu as you know is a

71
00:03:31,040 --> 00:03:36,799
Global Information Technology Company

72
00:03:33,319 --> 00:03:40,319
and U they have a big research Wing So

73
00:03:36,799 --> 00:03:42,640
currently we are 1,000 strong and uh

74
00:03:40,319 --> 00:03:45,879
they are as like the company spread all

75
00:03:42,640 --> 00:03:48,239
over the world and uh so we have lab in

76
00:03:45,879 --> 00:03:51,599
Japan of course and then we spread to

77
00:03:48,239 --> 00:03:54,840
North America and then Europe and the

78
00:03:51,599 --> 00:03:58,480
latest one is in the hotbed um of

79
00:03:54,840 --> 00:04:00,720
Bangalore so so we uh do research in all

80
00:03:58,480 --> 00:04:04,040
the cting age it techn techologies I am

81
00:04:00,720 --> 00:04:06,519
in charge of U the research activities

82
00:04:04,040 --> 00:04:07,760
in North America which comprises Canada

83
00:04:06,519 --> 00:04:11,640
and United

84
00:04:07,760 --> 00:04:14,360
States and uh AI is a huge part of our

85
00:04:11,640 --> 00:04:16,560
uh work right now in fact I would say

86
00:04:14,360 --> 00:04:19,680
the total amount of budget that we have

87
00:04:16,560 --> 00:04:23,400
in research more than 50% is now devoted

88
00:04:19,680 --> 00:04:26,400
to research in Ai and uh out of that uh

89
00:04:23,400 --> 00:04:28,600
generative AI of course is the new kid

90
00:04:26,400 --> 00:04:30,800
in the block I've been there for two

91
00:04:28,600 --> 00:04:35,240
years and we are doing active research

92
00:04:30,800 --> 00:04:37,840
in all of that and uh Fujitsu being a um

93
00:04:35,240 --> 00:04:40,520
businessto business company mainly uh

94
00:04:37,840 --> 00:04:43,720
our generative AI research focus is also

95
00:04:40,520 --> 00:04:45,600
on Enterprise AI so we are going to

96
00:04:43,720 --> 00:04:48,759
discuss a little bit more about that we

97
00:04:45,600 --> 00:04:51,479
have some uh tools uh some resources uh

98
00:04:48,759 --> 00:04:54,639
some new research areas that we target

99
00:04:51,479 --> 00:04:58,360
towards uh kind of helping Enterprises

100
00:04:54,639 --> 00:05:02,759
uh as we go through the panel thank you

101
00:04:58,360 --> 00:05:05,039
David great so I'm sorry David Cox I'm

102
00:05:02,759 --> 00:05:07,160
the BP for AI models at IBM research I

103
00:05:05,039 --> 00:05:11,000
actually wear two hats at IBM research

104
00:05:07,160 --> 00:05:13,479
one is I lead IBM's own internal llm

105
00:05:11,000 --> 00:05:15,000
development efforts so we're training a

106
00:05:13,479 --> 00:05:16,560
model series called granite and we're

107
00:05:15,000 --> 00:05:19,280
actually going to have a big release uh

108
00:05:16,560 --> 00:05:21,639
next month so stay tuned for that uh and

109
00:05:19,280 --> 00:05:24,039
then I also am the MIT director 's my

110
00:05:21,639 --> 00:05:25,199
counterpart where we have a research lab

111
00:05:24,039 --> 00:05:28,280
where IBM is investing close to a

112
00:05:25,199 --> 00:05:30,440
quarter billion dollars over 10 years to

113
00:05:28,280 --> 00:05:31,600
uh co-develop sort of Leading Edge Ai

114
00:05:30,440 --> 00:05:33,960
and I've been doing that for about 6

115
00:05:31,600 --> 00:05:36,720
years and as a confession I'm a

116
00:05:33,960 --> 00:05:39,280
recovering academic uh so before I

117
00:05:36,720 --> 00:05:41,600
joined IBM I was a professor at Harvard

118
00:05:39,280 --> 00:05:43,440
for about 10 years and my mom's very

119
00:05:41,600 --> 00:05:45,800
proud of me that I finally got out of

120
00:05:43,440 --> 00:05:47,720
school um I don't have the heart to tell

121
00:05:45,800 --> 00:05:50,000
her that my office is still on the MIT

122
00:05:47,720 --> 00:05:52,680
campus only about 20t across the street

123
00:05:50,000 --> 00:05:54,400
from where I did my PhD here but uh it's

124
00:05:52,680 --> 00:05:56,600
a it's a really exciting environment and

125
00:05:54,400 --> 00:05:59,080
it's a really exciting time to be in AI

126
00:05:56,600 --> 00:06:01,080
IBM does a lot of things in the AI space

127
00:05:59,080 --> 00:06:03,400
we have a Consulting arm that helps

128
00:06:01,080 --> 00:06:05,280
develop Solutions we have software we're

129
00:06:03,400 --> 00:06:06,840
infusing generative AI into all of our

130
00:06:05,280 --> 00:06:09,599
products and have been for quite some

131
00:06:06,840 --> 00:06:11,039
time and uh so we we really Spread

132
00:06:09,599 --> 00:06:13,440
spread the gamma and including also

133
00:06:11,039 --> 00:06:16,000
Hardware uh and everything from

134
00:06:13,440 --> 00:06:19,639
traditional Computing like mainframes to

135
00:06:16,000 --> 00:06:22,800
AI accelerators and also Quantum

136
00:06:19,639 --> 00:06:25,720
Computing yeah so hi everyone Ramen

137
00:06:22,800 --> 00:06:28,120
Hassani i u I'm the CEO and co-founder

138
00:06:25,720 --> 00:06:30,680
of liquid AI I started this company

139
00:06:28,120 --> 00:06:34,080
about a year and a half ago spawn out

140
00:06:30,680 --> 00:06:36,880
out of MIT the company

141
00:06:34,080 --> 00:06:39,039
is building found new type of foundation

142
00:06:36,880 --> 00:06:43,960
models building on a technology that I

143
00:06:39,039 --> 00:06:48,080
invented uh during my PhD um also at MIT

144
00:06:43,960 --> 00:06:50,360
and um so the technology itself was

145
00:06:48,080 --> 00:06:52,599
basic it's called liquid neural networks

146
00:06:50,360 --> 00:06:54,319
it's like a alternative type of neural

147
00:06:52,599 --> 00:06:57,560
networks we started looking into brain

148
00:06:54,319 --> 00:06:59,560
of little animals and then we

149
00:06:57,560 --> 00:07:01,599
identified what are the core comp

150
00:06:59,560 --> 00:07:04,280
computational blocks that are missing

151
00:07:01,599 --> 00:07:06,440
from today's AI systems such as the

152
00:07:04,280 --> 00:07:09,240
Transformers that powers today's gen

153
00:07:06,440 --> 00:07:12,440
systems and then uh we thought that okay

154
00:07:09,240 --> 00:07:14,319
so let's build these kind of neuronal

155
00:07:12,440 --> 00:07:16,639
circuits you know like this new type of

156
00:07:14,319 --> 00:07:19,360
AI systems and then see how powerful

157
00:07:16,639 --> 00:07:21,639
they are and then study their properties

158
00:07:19,360 --> 00:07:24,440
so we started building these um liquid

159
00:07:21,639 --> 00:07:27,120
neural networks liquid for flexibility

160
00:07:24,440 --> 00:07:28,759
okay the the whole system is going to be

161
00:07:27,120 --> 00:07:32,520
a very different system than a

162
00:07:28,759 --> 00:07:34,759
Transformer and a GPT system and it has

163
00:07:32,520 --> 00:07:38,039
a lot of attractive properties so

164
00:07:34,759 --> 00:07:40,800
property number one is that it is going

165
00:07:38,039 --> 00:07:43,840
to give you the best and goes beyond a a

166
00:07:40,800 --> 00:07:45,319
little bit beyond learning theory so

167
00:07:43,840 --> 00:07:47,960
what that means that means like you're

168
00:07:45,319 --> 00:07:51,159
going to get a very good out of

169
00:07:47,960 --> 00:07:52,800
distribution kind of um performance what

170
00:07:51,159 --> 00:07:55,240
does that mean that means if you let's

171
00:07:52,800 --> 00:07:57,919
say you're driving a car and you have an

172
00:07:55,240 --> 00:07:59,759
AI actually drive that car autonomously

173
00:07:57,919 --> 00:08:02,440
you if you deploy these kind of system

174
00:07:59,759 --> 00:08:04,159
on top of this you know car let's say

175
00:08:02,440 --> 00:08:06,520
all of a sudden there is raining like

176
00:08:04,159 --> 00:08:08,840
happen like you know it starts raining

177
00:08:06,520 --> 00:08:10,800
the system would be a lot more robust to

178
00:08:08,840 --> 00:08:14,039
these kind of properties so liquid is

179
00:08:10,800 --> 00:08:16,039
for flexibility and robustness okay and

180
00:08:14,039 --> 00:08:18,479
then uh we started with the field of

181
00:08:16,039 --> 00:08:21,199
Robotics we then applied this technology

182
00:08:18,479 --> 00:08:24,520
into let's say modeling Financial

183
00:08:21,199 --> 00:08:27,479
systems and modeling uh uh sequential

184
00:08:24,520 --> 00:08:30,159
data like in the space of uh Healthcare

185
00:08:27,479 --> 00:08:32,320
and user behavior and a lot of other

186
00:08:30,159 --> 00:08:34,080
space and and we soon realize that we

187
00:08:32,320 --> 00:08:36,000
have like a general purpose computer

188
00:08:34,080 --> 00:08:38,399
that gives us a very high quality and

189
00:08:36,000 --> 00:08:39,919
robustness kind of properties and then

190
00:08:38,399 --> 00:08:41,839
this was quality number one quality

191
00:08:39,919 --> 00:08:43,919
number two is that these systems are

192
00:08:41,839 --> 00:08:46,640
brain inspired so they are very

193
00:08:43,919 --> 00:08:48,000
efficient So based on the mathematics

194
00:08:46,640 --> 00:08:50,480
that we designed these kind of systems

195
00:08:48,000 --> 00:08:53,000
we built very efficient algorithms that

196
00:08:50,480 --> 00:08:55,120
you do not need to spend that much power

197
00:08:53,000 --> 00:08:58,079
for example today at the company we

198
00:08:55,120 --> 00:09:00,640
built uh Foundation models language

199
00:08:58,079 --> 00:09:02,399
Foundation models this same quality as

200
00:09:00,640 --> 00:09:06,240
what meta actually released like let's

201
00:09:02,399 --> 00:09:10,240
say llama 370 billion parameter with the

202
00:09:06,240 --> 00:09:12,360
same kind of capacity llama spent 48,000

203
00:09:10,240 --> 00:09:15,760
gpus to develop that kind of quality of

204
00:09:12,360 --> 00:09:18,880
models we spent 300 gpus to develop them

205
00:09:15,760 --> 00:09:21,600
all so the massive reduction in the cost

206
00:09:18,880 --> 00:09:24,160
of development of gen if you change kind

207
00:09:21,600 --> 00:09:26,920
of the substrate uh with this new type

208
00:09:24,160 --> 00:09:29,120
of technology and then the third and the

209
00:09:26,920 --> 00:09:31,440
last thing that I wanted to mention is

210
00:09:29,120 --> 00:09:33,399
we are coming back coming from a control

211
00:09:31,440 --> 00:09:36,600
theory background and and and dynamical

212
00:09:33,399 --> 00:09:39,320
systems and Robotics you know my I I my

213
00:09:36,600 --> 00:09:42,320
supervisor P Supervisor was Daniela ruse

214
00:09:39,320 --> 00:09:44,680
who is the director of Cel and um you

215
00:09:42,320 --> 00:09:46,519
know like she's a roboticist and and you

216
00:09:44,680 --> 00:09:48,440
know like we have been thinking about

217
00:09:46,519 --> 00:09:50,320
real world and implications of AI in

218
00:09:48,440 --> 00:09:52,240
real world when you want to deploy a

219
00:09:50,320 --> 00:09:53,920
system an intelligent system in real

220
00:09:52,240 --> 00:09:55,959
world you need to make sure that you

221
00:09:53,920 --> 00:09:57,920
have a lot of control on this system so

222
00:09:55,959 --> 00:09:59,480
it doesn't go Rogue so you need to have

223
00:09:57,920 --> 00:10:02,839
reliability you need to ensure that

224
00:09:59,480 --> 00:10:04,399
system is safe so that is basically

225
00:10:02,839 --> 00:10:06,839
another angle that we have we have a lot

226
00:10:04,399 --> 00:10:09,680
of control into the design and

227
00:10:06,839 --> 00:10:11,240
deployment of gen systems which is kind

228
00:10:09,680 --> 00:10:13,640
of sometimes you can call it

229
00:10:11,240 --> 00:10:16,040
explainability interpretability and

230
00:10:13,640 --> 00:10:18,040
aspects of like having visibility into

231
00:10:16,040 --> 00:10:20,079
how networks are coming into decision

232
00:10:18,040 --> 00:10:22,519
making so with this three kind of value

233
00:10:20,079 --> 00:10:24,360
proposition we started a company and we

234
00:10:22,519 --> 00:10:26,680
are a headquartered actually in the IBM

235
00:10:24,360 --> 00:10:28,480
building here right next to uh on the

236
00:10:26,680 --> 00:10:31,000
other side of the street uh the pi

237
00:10:28,480 --> 00:10:34,360
building actually and um we are 40

238
00:10:31,000 --> 00:10:36,720
people Massachusetts base we have um

239
00:10:34,360 --> 00:10:40,320
onethird of the team is also in palato

240
00:10:36,720 --> 00:10:43,880
but um happy to take whoa whoa thank you

241
00:10:40,320 --> 00:10:46,320
for uh that um very deep uh description

242
00:10:43,880 --> 00:10:48,639
so actually on this topic let's go to

243
00:10:46,320 --> 00:10:50,880
the technical advancement and challenges

244
00:10:48,639 --> 00:10:55,279
that you saw ra me you you you talk

245
00:10:50,880 --> 00:10:58,839
about a few but for IG and David um so

246
00:10:55,279 --> 00:11:02,079
in your in your business uh what are the

247
00:10:58,839 --> 00:11:05,760
challenges you see in

248
00:11:02,079 --> 00:11:09,720
training deploying at scale and

249
00:11:05,760 --> 00:11:12,720
maintaining uh those large model David

250
00:11:09,720 --> 00:11:14,959
yeah there's a lot of challenges uh in

251
00:11:12,720 --> 00:11:16,920
every part of this ecosystem and then

252
00:11:14,959 --> 00:11:18,240
there's another layer of challenges if

253
00:11:16,920 --> 00:11:20,279
you're an Enterprise if you're a big

254
00:11:18,240 --> 00:11:21,560
business on how you do that responsibly

255
00:11:20,279 --> 00:11:23,839
and then if you're in a regulated

256
00:11:21,560 --> 00:11:27,440
industry there's yet another sort of

257
00:11:23,839 --> 00:11:30,600
layer of challenges so um you know the

258
00:11:27,440 --> 00:11:34,279
today's uh technology requires enormous

259
00:11:30,600 --> 00:11:36,440
compute to train so we we typically have

260
00:11:34,279 --> 00:11:38,600
thousands of gpus burning for months at

261
00:11:36,440 --> 00:11:40,639
a time that's just standard practice

262
00:11:38,600 --> 00:11:42,240
with the technology we have today uh

263
00:11:40,639 --> 00:11:43,839
that's obviously a huge energy problem

264
00:11:42,240 --> 00:11:46,160
it's also a huge supply chain problem

265
00:11:43,839 --> 00:11:48,160
Nvidia you probably have seen nvidia's

266
00:11:46,160 --> 00:11:50,880
market cap growing and there's a

267
00:11:48,160 --> 00:11:53,560
worldwide shortage and you know like my

268
00:11:50,880 --> 00:11:55,680
CEO has to talk to somebody to Jensen to

269
00:11:53,560 --> 00:11:57,399
like strike a deal and good things so

270
00:11:55,680 --> 00:11:59,240
it's a it's it's an interesting moment

271
00:11:57,399 --> 00:12:00,839
we're in where it's there's you know any

272
00:11:59,240 --> 00:12:03,399
any technology we can get that could

273
00:12:00,839 --> 00:12:05,399
reduce that would be very very welcome

274
00:12:03,399 --> 00:12:07,600
the amounts of data we're using are

275
00:12:05,399 --> 00:12:08,639
astonishing um you know a year ago we

276
00:12:07,600 --> 00:12:11,240
trained some of the models we use

277
00:12:08,639 --> 00:12:13,440
internally on one and a qu trillion

278
00:12:11,240 --> 00:12:16,360
tokens so tokens like a word or a word

279
00:12:13,440 --> 00:12:18,079
part uh that felt like a lot at the time

280
00:12:16,360 --> 00:12:20,600
and when I was a PhD student working

281
00:12:18,079 --> 00:12:22,399
with little neural networks unimaginable

282
00:12:20,600 --> 00:12:25,279
quantity now we routinely are in the

283
00:12:22,399 --> 00:12:27,160
tens of you know well over 10 trillion

284
00:12:25,279 --> 00:12:29,360
tokens of data that means we're

285
00:12:27,160 --> 00:12:31,880
basically slurping up the entire

286
00:12:29,360 --> 00:12:33,680
internet and all kinds of data coming in

287
00:12:31,880 --> 00:12:35,399
and that's evolving and that introduces

288
00:12:33,680 --> 00:12:38,399
all kinds of interesting governance

289
00:12:35,399 --> 00:12:40,440
risks right because in many cases this

290
00:12:38,399 --> 00:12:42,720
is more data than a human could read in

291
00:12:40,440 --> 00:12:44,839
a lifetime in fact if you read the data

292
00:12:42,720 --> 00:12:47,920
in a typical llm training set it would

293
00:12:44,839 --> 00:12:49,120
take you 350,000 years to read it all

294
00:12:47,920 --> 00:12:51,519
just sitting there if you just sat down

295
00:12:49,120 --> 00:12:54,639
and tried to read it so we can't read it

296
00:12:51,519 --> 00:12:56,639
um so in some cases there are companies

297
00:12:54,639 --> 00:12:58,440
that have sort of played fast and loose

298
00:12:56,639 --> 00:13:00,000
and just said we're just going to take

299
00:12:58,440 --> 00:13:02,079
it and let the lawyers figure it out

300
00:13:00,000 --> 00:13:03,600
later that doesn't serve our customers

301
00:13:02,079 --> 00:13:04,839
so well so we put quite a bit of effort

302
00:13:03,600 --> 00:13:06,839
into making sure that we understand

303
00:13:04,839 --> 00:13:09,600
exactly what's in our models we're using

304
00:13:06,839 --> 00:13:10,920
AI based filtering to remove things that

305
00:13:09,600 --> 00:13:13,120
could be objectionable this is an

306
00:13:10,920 --> 00:13:14,480
ongoing process and we have we built a

307
00:13:13,120 --> 00:13:16,639
platform that we can have traceability

308
00:13:14,480 --> 00:13:19,240
back from from any model we produced to

309
00:13:16,639 --> 00:13:20,600
back to every last individual word that

310
00:13:19,240 --> 00:13:22,839
went into that model or image in the

311
00:13:20,600 --> 00:13:24,199
case of our multimodal models um so

312
00:13:22,839 --> 00:13:26,240
that's a big Challenge and then when you

313
00:13:24,199 --> 00:13:27,680
deploy these things um you know

314
00:13:26,240 --> 00:13:29,480
everybody's excited about the

315
00:13:27,680 --> 00:13:32,560
capabilities of the things and it's

316
00:13:29,480 --> 00:13:34,160
amazing um I I didn't think I don't know

317
00:13:32,560 --> 00:13:36,079
about you I I didn't think we would be

318
00:13:34,160 --> 00:13:38,240
here I've been doing like AI research in

319
00:13:36,079 --> 00:13:40,680
one form or another my entire adult life

320
00:13:38,240 --> 00:13:42,560
and I didn't think we'd be where we are

321
00:13:40,680 --> 00:13:44,199
this fast so it's amazing and everyone

322
00:13:42,560 --> 00:13:45,800
sees the potential but one of the

323
00:13:44,199 --> 00:13:47,279
defining characteristics of large

324
00:13:45,800 --> 00:13:49,040
language models is that they're

325
00:13:47,279 --> 00:13:51,959
unreliable uh we're in this new

326
00:13:49,040 --> 00:13:53,279
Computing regime where it's really hard

327
00:13:51,959 --> 00:13:55,880
to be really

328
00:13:53,279 --> 00:13:57,240
certain that the system will do what you

329
00:13:55,880 --> 00:13:59,440
need it to do and that it won't go off

330
00:13:57,240 --> 00:14:00,920
the rails hallucinate which is you know

331
00:13:59,440 --> 00:14:02,240
produce a response that it sort of

332
00:14:00,920 --> 00:14:05,120
generated out of thin air it's not the

333
00:14:02,240 --> 00:14:07,399
right answer or produce bias or negative

334
00:14:05,120 --> 00:14:10,160
responses this is a new attack factor

335
00:14:07,399 --> 00:14:13,519
for adversarial players so a malicious

336
00:14:10,160 --> 00:14:16,040
actor can uh do all kinds of different

337
00:14:13,519 --> 00:14:18,399
attacks against an LM um there was a

338
00:14:16,040 --> 00:14:20,120
comment in the previous talk don't put

339
00:14:18,399 --> 00:14:21,800
private data in something that has a

340
00:14:20,120 --> 00:14:23,279
public interface that's because

341
00:14:21,800 --> 00:14:26,600
increasingly researchers are figuring

342
00:14:23,279 --> 00:14:27,920
out how to uh sort of pull out the data

343
00:14:26,600 --> 00:14:29,600
like trick the model into giving up

344
00:14:27,920 --> 00:14:30,600
something it shouldn't give up um

345
00:14:29,600 --> 00:14:32,480
there's lots of ways to break the

346
00:14:30,600 --> 00:14:34,839
alignment do this things called prompt

347
00:14:32,480 --> 00:14:36,240
injection attacks so that's an

348
00:14:34,839 --> 00:14:38,759
interesting evolving space that

349
00:14:36,240 --> 00:14:41,040
companies are having to to to to deal

350
00:14:38,759 --> 00:14:43,880
with and it just amplifies all the

351
00:14:41,040 --> 00:14:44,880
issues that you had with AI before if

352
00:14:43,880 --> 00:14:46,160
you're going to take a model and

353
00:14:44,880 --> 00:14:48,320
customize it if you're going to take a

354
00:14:46,160 --> 00:14:50,399
model and have it do a retrieval

355
00:14:48,320 --> 00:14:52,720
augmented generation or rag sort of

356
00:14:50,399 --> 00:14:55,360
pattern pulling in data you have to

357
00:14:52,720 --> 00:14:57,720
validate it somehow so a lot of this

358
00:14:55,360 --> 00:15:00,360
landscape is evolving uh this is the

359
00:14:57,720 --> 00:15:02,320
place where we play at I BM all the time

360
00:15:00,360 --> 00:15:04,480
so we have governance products that will

361
00:15:02,320 --> 00:15:05,240
help you manage those things and and we

362
00:15:04,480 --> 00:15:08,600
think that's going to be really

363
00:15:05,240 --> 00:15:10,720
important particularly in in regulated

364
00:15:08,600 --> 00:15:13,480
Industries and I think it's safe to say

365
00:15:10,720 --> 00:15:15,800
there are places where we are seeing AI

366
00:15:13,480 --> 00:15:18,160
deployed at scale we deploy it already

367
00:15:15,800 --> 00:15:19,920
in many of our products but many

368
00:15:18,160 --> 00:15:23,240
companies and I'm sure many of you in

369
00:15:19,920 --> 00:15:26,720
the audience are kind of at that uh MVP

370
00:15:23,240 --> 00:15:28,360
po kind of stage like like a little

371
00:15:26,720 --> 00:15:30,800
tiger team went and said hey look what

372
00:15:28,360 --> 00:15:31,959
we did we built prototype but then and

373
00:15:30,800 --> 00:15:33,880
uh there was a mentioned of banks as

374
00:15:31,959 --> 00:15:35,240
well banks have lots of these and then

375
00:15:33,880 --> 00:15:38,000
you get to their model Risk Management

376
00:15:35,240 --> 00:15:39,920
Department which in a big bank is like a

377
00:15:38,000 --> 00:15:42,279
reporting directly down from the CEO

378
00:15:39,920 --> 00:15:44,199
independent arm that has to like say is

379
00:15:42,279 --> 00:15:45,160
this okay or not and I think we're in a

380
00:15:44,199 --> 00:15:46,560
phase now where everyone's trying to

381
00:15:45,160 --> 00:15:48,560
figure out how do we deal with that how

382
00:15:46,560 --> 00:15:50,880
do we be confident how do we build the

383
00:15:48,560 --> 00:15:53,680
right guard rails and and and models

384
00:15:50,880 --> 00:15:55,120
around that and that on top of the fact

385
00:15:53,680 --> 00:15:56,680
that the technology is changing all the

386
00:15:55,120 --> 00:15:58,440
time you know right now it's

387
00:15:56,680 --> 00:15:59,480
Transformers you have an alternative

388
00:15:58,440 --> 00:16:00,680
architecture we have a couple

389
00:15:59,480 --> 00:16:03,759
alternative architectures that we're

390
00:16:00,680 --> 00:16:04,839
working on as well and like nobody is

391
00:16:03,759 --> 00:16:06,360
quite sure what's going to happen but

392
00:16:04,839 --> 00:16:08,800
the world is changing so fast and

393
00:16:06,360 --> 00:16:10,199
companies are trying to build you know

394
00:16:08,800 --> 00:16:11,480
do something sensible with this power

395
00:16:10,199 --> 00:16:13,480
and everyone feels like they're being

396
00:16:11,480 --> 00:16:16,199
left behind so it's it's a really

397
00:16:13,480 --> 00:16:19,240
interesting environment I've never seen

398
00:16:16,199 --> 00:16:22,120
either in Academia or in industry I've

399
00:16:19,240 --> 00:16:23,880
never seen anything move this fast uh so

400
00:16:22,120 --> 00:16:27,079
it's exciting and hopefully you're all

401
00:16:23,880 --> 00:16:29,480
excited by it um but also really

402
00:16:27,079 --> 00:16:33,000
uncertain and and interesting and

403
00:16:29,480 --> 00:16:34,040
exciting time yeah the pace keep all of

404
00:16:33,000 --> 00:16:39,040
us

405
00:16:34,040 --> 00:16:41,319
Young no it's making me older don't um I

406
00:16:39,040 --> 00:16:43,920
I want to add basically the other things

407
00:16:41,319 --> 00:16:47,120
so he already uh mentioned David about

408
00:16:43,920 --> 00:16:49,800
all these challenges of putting it u in

409
00:16:47,120 --> 00:16:53,079
the private setting we had an additional

410
00:16:49,800 --> 00:16:55,040
challenge about language so all these LM

411
00:16:53,079 --> 00:16:57,440
models initially were all mostly trained

412
00:16:55,040 --> 00:17:00,079
on English and so when you put that

413
00:16:57,440 --> 00:17:02,480
model in a context of Japanese language

414
00:17:00,079 --> 00:17:04,520
the accuracy was really poor so

415
00:17:02,480 --> 00:17:07,039
initially Fujitsu did a lot of work

416
00:17:04,520 --> 00:17:09,640
fine-tuning these models on Japanese

417
00:17:07,039 --> 00:17:11,679
language so that the accuracy becomes

418
00:17:09,640 --> 00:17:13,880
much better so that was one of the

419
00:17:11,679 --> 00:17:16,400
challenges that had to be addressed and

420
00:17:13,880 --> 00:17:18,079
on top of that whenever you are talking

421
00:17:16,400 --> 00:17:20,319
about Enterprise the first thing they

422
00:17:18,079 --> 00:17:23,160
say was okay we want everything on

423
00:17:20,319 --> 00:17:25,480
premise okay we don't want to send send

424
00:17:23,160 --> 00:17:27,880
our data outside in the cloud we are a

425
00:17:25,480 --> 00:17:30,240
sensitive company we don't want and so

426
00:17:27,880 --> 00:17:31,799
you if you have to train an llm on

427
00:17:30,240 --> 00:17:34,320
premise I mean with the current

428
00:17:31,799 --> 00:17:36,919
technology the amount of investment that

429
00:17:34,320 --> 00:17:39,679
you have to do to create such a huge

430
00:17:36,919 --> 00:17:41,240
system that you just tell that to the

431
00:17:39,679 --> 00:17:44,640
customer and then immediately they

432
00:17:41,240 --> 00:17:47,080
switch off we cannot justify so then we

433
00:17:44,640 --> 00:17:49,799
had to create some other kind of

434
00:17:47,080 --> 00:17:52,400
solutions so one as David mentioned was

435
00:17:49,799 --> 00:17:54,760
rag so rag is retrieval augmented

436
00:17:52,400 --> 00:17:57,520
generation where you create some kind of

437
00:17:54,760 --> 00:18:00,520
a knowledge graph of the private data

438
00:17:57,520 --> 00:18:02,840
which is on premise and then you use

439
00:18:00,520 --> 00:18:05,679
your uh llm model which has been

440
00:18:02,840 --> 00:18:07,679
pre-trained but it takes input from

441
00:18:05,679 --> 00:18:10,400
those kind of models which are on

442
00:18:07,679 --> 00:18:12,760
premise to enhance your answers in the

443
00:18:10,400 --> 00:18:16,520
private setting so in that way you kind

444
00:18:12,760 --> 00:18:18,840
of um have the best of both words but

445
00:18:16,520 --> 00:18:20,640
it's not without issues and there are

446
00:18:18,840 --> 00:18:23,159
some things that some research is still

447
00:18:20,640 --> 00:18:25,640
going on to make this uh rag more

448
00:18:23,159 --> 00:18:28,240
intelligent and to give more accuracy to

449
00:18:25,640 --> 00:18:31,080
the kind of uh uh answers that you're

450
00:18:28,240 --> 00:18:33,840
are getting yeah so so I would say these

451
00:18:31,080 --> 00:18:37,120
two main challenges how do you keep it

452
00:18:33,840 --> 00:18:39,799
within the uh organization uh not

453
00:18:37,120 --> 00:18:42,080
disclosing the data to train the model

454
00:18:39,799 --> 00:18:44,400
outside and then of course uh the

455
00:18:42,080 --> 00:18:46,760
language barrier has to be

456
00:18:44,400 --> 00:18:48,559
crossed thank you so I have a question

457
00:18:46,760 --> 00:18:51,440
for Ramin and then we will start looking

458
00:18:48,559 --> 00:18:54,679
at the question from the audience so uh

459
00:18:51,440 --> 00:18:58,039
Ramin in your explanation of liquid uh

460
00:18:54,679 --> 00:19:00,360
Ai and uh the innovation of the startup

461
00:18:58,039 --> 00:19:02,480
tell us about energy consumption because

462
00:19:00,360 --> 00:19:05,159
I think we're all thinking about the

463
00:19:02,480 --> 00:19:08,600
planet so yeah

464
00:19:05,159 --> 00:19:10,679
absolutely well as David also noted like

465
00:19:08,600 --> 00:19:12,440
these systems like they require like

466
00:19:10,679 --> 00:19:14,559
different aspects of scale you have to

467
00:19:12,440 --> 00:19:16,400
have in mind like okay so everything

468
00:19:14,559 --> 00:19:17,320
started by the fact that you know you

469
00:19:16,400 --> 00:19:20,360
make them

470
00:19:17,320 --> 00:19:21,679
larger they get better and better you

471
00:19:20,360 --> 00:19:23,320
know by better and better I mean they

472
00:19:21,679 --> 00:19:25,400
become more reliable and they give you

473
00:19:23,320 --> 00:19:26,799
like more reliable answers you know and

474
00:19:25,400 --> 00:19:29,640
when I say bigger there are three

475
00:19:26,799 --> 00:19:32,039
dimensions of scale you know that we

476
00:19:29,640 --> 00:19:34,440
talking about one is the size of these

477
00:19:32,039 --> 00:19:36,720
models the other thing is that how much

478
00:19:34,440 --> 00:19:39,000
information they can process at the same

479
00:19:36,720 --> 00:19:41,480
time or in other word working memory of

480
00:19:39,000 --> 00:19:44,200
the device so that would be a context

481
00:19:41,480 --> 00:19:46,159
length and the third angle is the data

482
00:19:44,200 --> 00:19:48,760
that they consume the amount of time

483
00:19:46,159 --> 00:19:50,600
that you you you the number of tokens

484
00:19:48,760 --> 00:19:53,080
basically you you spend to actually

485
00:19:50,600 --> 00:19:55,240
train these Tok train these systems so

486
00:19:53,080 --> 00:19:57,320
across this tree when you start like

487
00:19:55,240 --> 00:19:59,679
scaling the models you get better and

488
00:19:57,320 --> 00:20:03,159
better and more reliable systems so that

489
00:19:59,679 --> 00:20:05,320
directly Demands a lot of um you know

490
00:20:03,159 --> 00:20:07,520
energy to consume and as I mentioned

491
00:20:05,320 --> 00:20:10,840
like uh meta just spend like you know

492
00:20:07,520 --> 00:20:12,799
two clusters of 24,000 gpus each like

493
00:20:10,840 --> 00:20:15,400
for development of the Llama series you

494
00:20:12,799 --> 00:20:17,280
know and that's a huge kind of system

495
00:20:15,400 --> 00:20:19,400
that we we seeing and it's and the trend

496
00:20:17,280 --> 00:20:21,039
is actually not slowing down you know

497
00:20:19,400 --> 00:20:23,039
about three months ago like Kevin Scott

498
00:20:21,039 --> 00:20:25,679
of Microsoft actually went on stage and

499
00:20:23,039 --> 00:20:27,320
showed like a scaling graph as long as

500
00:20:25,679 --> 00:20:29,400
you're scaling across the three

501
00:20:27,320 --> 00:20:31,240
dimensions that I mentioned it is there

502
00:20:29,400 --> 00:20:33,919
is going to be power consumption of this

503
00:20:31,240 --> 00:20:36,080
system and then the underlying system so

504
00:20:33,919 --> 00:20:37,799
far the Transformer architecture or

505
00:20:36,080 --> 00:20:40,400
attention is actually something that is

506
00:20:37,799 --> 00:20:43,200
working you know it's it's it's elegant

507
00:20:40,400 --> 00:20:45,200
it's simple you can scale it so there is

508
00:20:43,200 --> 00:20:47,240
very hard to beat the performance of

509
00:20:45,200 --> 00:20:48,880
this elegant architecture that's why

510
00:20:47,240 --> 00:20:51,360
just the the matter that people are

511
00:20:48,880 --> 00:20:53,559
scaling it it just seems to be solving

512
00:20:51,360 --> 00:20:55,679
problems like Beyond complexity Theory

513
00:20:53,559 --> 00:20:57,520
even you know now people were saying

514
00:20:55,679 --> 00:20:59,159
like Transformers from the mathematical

515
00:20:57,520 --> 00:21:02,000
formula it's a simple mathematical

516
00:20:59,159 --> 00:21:04,280
formula that that can do like it's

517
00:21:02,000 --> 00:21:05,960
literally matrix multiplication you know

518
00:21:04,280 --> 00:21:08,840
like when when we think about it and we

519
00:21:05,960 --> 00:21:10,720
scaling that you know and then so

520
00:21:08,840 --> 00:21:12,720
there's there there's not going to be

521
00:21:10,720 --> 00:21:14,960
like stopping of the consumption of

522
00:21:12,720 --> 00:21:17,360
energy there this trend is going to be

523
00:21:14,960 --> 00:21:20,360
continuing the thing is we just have to

524
00:21:17,360 --> 00:21:22,159
get a little bit more creative and the

525
00:21:20,360 --> 00:21:24,000
fact that a lot of people worked on

526
00:21:22,159 --> 00:21:26,440
alternative models you know from the

527
00:21:24,000 --> 00:21:28,720
base like changing like the landscape

528
00:21:26,440 --> 00:21:30,799
like into Transformers or something

529
00:21:28,720 --> 00:21:32,880
called computationally or quadratic in

530
00:21:30,799 --> 00:21:35,440
computational costs that means the more

531
00:21:32,880 --> 00:21:37,520
information they process it is becoming

532
00:21:35,440 --> 00:21:40,760
like quadratically harder to process

533
00:21:37,520 --> 00:21:44,440
those information so you can build

534
00:21:40,760 --> 00:21:46,760
systems that H that have linear time

535
00:21:44,440 --> 00:21:48,840
complexity these are like even one of

536
00:21:46,760 --> 00:21:51,120
the questions there is like State space

537
00:21:48,840 --> 00:21:52,760
models you know or ssms right so you're

538
00:21:51,120 --> 00:21:54,320
talking about this structural State

539
00:21:52,760 --> 00:21:55,880
space models that are out there these

540
00:21:54,320 --> 00:21:58,720
are linear kind of models you're

541
00:21:55,880 --> 00:22:00,520
reducing the complexity of attention

542
00:21:58,720 --> 00:22:02,880
that's why like you can consume a lot

543
00:22:00,520 --> 00:22:05,440
less especially as you're processing as

544
00:22:02,880 --> 00:22:07,799
you're scaling the models you are

545
00:22:05,440 --> 00:22:10,600
consuming a lot less kind of energy

546
00:22:07,799 --> 00:22:12,320
compared to the Transformer architecture

547
00:22:10,600 --> 00:22:14,240
the drawback of this alternative

548
00:22:12,320 --> 00:22:17,039
architecture is that in computer science

549
00:22:14,240 --> 00:22:19,080
there is no free launch right so that's

550
00:22:17,039 --> 00:22:21,279
that means if you're making something

551
00:22:19,080 --> 00:22:24,480
simpler or making a linear kind of

552
00:22:21,279 --> 00:22:26,279
system you're losing on expressivity so

553
00:22:24,480 --> 00:22:28,760
these alternative architectures they had

554
00:22:26,279 --> 00:22:29,960
a hard time to catch up with with the

555
00:22:28,760 --> 00:22:32,000
performance of a transformer

556
00:22:29,960 --> 00:22:34,279
architecture so you might actually get

557
00:22:32,000 --> 00:22:35,799
into like more more energy efficient

558
00:22:34,279 --> 00:22:38,840
Solutions as a result of like

559
00:22:35,799 --> 00:22:41,120
architecture research better kind of uh

560
00:22:38,840 --> 00:22:42,840
arrangement of the hardware status and

561
00:22:41,120 --> 00:22:44,720
everything so just to control the

562
00:22:42,840 --> 00:22:46,279
technology itself but what I want to

563
00:22:44,720 --> 00:22:49,240
tell you is that you're losing also

564
00:22:46,279 --> 00:22:50,679
accuracy so there has to be a balance or

565
00:22:49,240 --> 00:22:53,000
if you want to enable the next

566
00:22:50,679 --> 00:22:54,559
generation of kind of AI systems or the

567
00:22:53,000 --> 00:22:56,960
foundation models you really have to

568
00:22:54,559 --> 00:22:58,679
think about how can I maximize also for

569
00:22:56,960 --> 00:23:00,600
expressivity because quality at the end

570
00:22:58,679 --> 00:23:02,559
of the day is going to be the winner you

571
00:23:00,600 --> 00:23:04,679
know and these AI systems are getting

572
00:23:02,559 --> 00:23:06,760
better and better again as I said as you

573
00:23:04,679 --> 00:23:08,559
scale them you need to get into the

574
00:23:06,760 --> 00:23:11,120
point where you know efficiency and

575
00:23:08,559 --> 00:23:13,400
quality can can come together that was

576
00:23:11,120 --> 00:23:17,000
the idea where we wanted to combine kind

577
00:23:13,400 --> 00:23:19,600
of um ideas from dynamical systems like

578
00:23:17,000 --> 00:23:21,640
core 200 years of kind of mathematics

579
00:23:19,600 --> 00:23:23,600
that enables dynamical systems to bring

580
00:23:21,640 --> 00:23:25,600
in into learning theory to design

581
00:23:23,600 --> 00:23:28,760
something new and this became kind of

582
00:23:25,600 --> 00:23:31,039
liquid foundation models so on the on

583
00:23:28,760 --> 00:23:32,600
our kind of technology and our impact on

584
00:23:31,039 --> 00:23:35,400
on these things so I can tell you that

585
00:23:32,600 --> 00:23:38,520
the systems are in the order of 10 to 20

586
00:23:35,400 --> 00:23:40,480
times more energy efficient compared to

587
00:23:38,520 --> 00:23:42,720
uh uh Transformers on the deployment

588
00:23:40,480 --> 00:23:45,400
side on the development side and on the

589
00:23:42,720 --> 00:23:48,679
deployment side they could be between 10

590
00:23:45,400 --> 00:23:51,679
to 100 times faster and cheaper kind of

591
00:23:48,679 --> 00:23:53,960
to to to to use them but on the and on

592
00:23:51,679 --> 00:23:55,760
the quality they're going to surpass the

593
00:23:53,960 --> 00:23:58,200
quality of Transformers that's the

594
00:23:55,760 --> 00:24:00,640
difference between liquid and and ssms

595
00:23:58,200 --> 00:24:03,120
or any other alternative or because it's

596
00:24:00,640 --> 00:24:05,559
not just also an architecture it is the

597
00:24:03,120 --> 00:24:07,600
the the the entire kind of package that

598
00:24:05,559 --> 00:24:09,600
you're using in order to really like

599
00:24:07,600 --> 00:24:11,039
build a gen system you know there is the

600
00:24:09,600 --> 00:24:12,679
matter of what kind of algorithm do you

601
00:24:11,039 --> 00:24:14,520
use for training what kind of

602
00:24:12,679 --> 00:24:16,159
posttraining kind of strategies do you

603
00:24:14,520 --> 00:24:18,919
have what kind of how much how do you

604
00:24:16,159 --> 00:24:20,760
Cate data how do you uh think about

605
00:24:18,919 --> 00:24:22,159
causality you know like in in your data

606
00:24:20,760 --> 00:24:24,520
so there's a lot of things that you can

607
00:24:22,159 --> 00:24:28,399
do to really improve the quality of the

608
00:24:24,520 --> 00:24:30,559
final system that that comes out and at

609
00:24:28,399 --> 00:24:32,919
would we do that so in a week from now

610
00:24:30,559 --> 00:24:34,520
we're going to announce the first series

611
00:24:32,919 --> 00:24:36,919
of we have we have never actually

612
00:24:34,520 --> 00:24:39,559
published a liquid foundation model or

613
00:24:36,919 --> 00:24:42,360
lfm which is going to be a replacement

614
00:24:39,559 --> 00:24:43,679
for gpts basically and these are for the

615
00:24:42,360 --> 00:24:45,679
first time in a week from now you're

616
00:24:43,679 --> 00:24:46,760
going to read about it we're going to

617
00:24:45,679 --> 00:24:49,080
release to the board there is going to

618
00:24:46,760 --> 00:24:52,480
be API access through perplexity if how

619
00:24:49,080 --> 00:24:52,480
many of you guys knows what is

620
00:24:53,039 --> 00:24:57,600
perplexity so perplexity is like another

621
00:24:55,559 --> 00:24:59,200
AI startup that is working on a search

622
00:24:57,600 --> 00:25:01,799
algorithm so it's going to be on the

623
00:24:59,200 --> 00:25:03,640
perplexity and Lambda and and and we are

624
00:25:01,799 --> 00:25:05,039
also working with a couple of others to

625
00:25:03,640 --> 00:25:07,240
really get confirmed and we're going to

626
00:25:05,039 --> 00:25:09,760
give access to everyone to test this new

627
00:25:07,240 --> 00:25:12,240
generation of models that even on a

628
00:25:09,760 --> 00:25:14,440
smaller kind of size they maximize kind

629
00:25:12,240 --> 00:25:16,279
of their capacity of learning it's

630
00:25:14,440 --> 00:25:18,039
higher quality and at the same time

631
00:25:16,279 --> 00:25:20,799
being that efficient so reducing the

632
00:25:18,039 --> 00:25:23,000
energy cost sorry I went oh no no this

633
00:25:20,799 --> 00:25:25,799
is this is fascinating uh Innovation is

634
00:25:23,000 --> 00:25:28,360
really uh everywhere okay we'll have you

635
00:25:25,799 --> 00:25:31,720
uh coming back um after a few weeks

636
00:25:28,360 --> 00:25:33,440
weeks um so all of you actually we have

637
00:25:31,720 --> 00:25:35,679
excellent question here on the board

638
00:25:33,440 --> 00:25:37,919
let's go with the first one and it's

639
00:25:35,679 --> 00:25:41,679
related to uh what you were saying Rin

640
00:25:37,919 --> 00:25:43,679
but let's go with David and um IG so

641
00:25:41,679 --> 00:25:47,240
what's the best way to measure the

642
00:25:43,679 --> 00:25:50,760
impact uh of model uh in production on

643
00:25:47,240 --> 00:25:53,880
the business yeah um

644
00:25:50,760 --> 00:25:55,240
so one thing that I think we sometimes

645
00:25:53,880 --> 00:25:58,799
have a tendency to do when there's an

646
00:25:55,240 --> 00:26:01,399
exciting shiny new object is we

647
00:25:58,799 --> 00:26:04,919
we believe that it requires radically

648
00:26:01,399 --> 00:26:07,039
new rules and it it for for measuring it

649
00:26:04,919 --> 00:26:08,320
for thinking about how to use it and

650
00:26:07,039 --> 00:26:10,080
that can certainly be true and there's

651
00:26:08,320 --> 00:26:11,679
elements of that but the other thing to

652
00:26:10,080 --> 00:26:14,600
remember is you're still running a

653
00:26:11,679 --> 00:26:15,760
business and you know what is your

654
00:26:14,600 --> 00:26:17,120
return on investment what is your

655
00:26:15,760 --> 00:26:20,000
investment what is your return on that

656
00:26:17,120 --> 00:26:21,480
investment um on the on the power

657
00:26:20,000 --> 00:26:23,320
question actually just get back to that

658
00:26:21,480 --> 00:26:26,840
for a moment uh I don't know if you know

659
00:26:23,320 --> 00:26:28,520
this but like even a decade ago we were

660
00:26:26,840 --> 00:26:31,039
projecting that our year-over-year

661
00:26:28,520 --> 00:26:33,080
energy cost energy usage for computing

662
00:26:31,039 --> 00:26:35,880
was increasing at a rate such that by

663
00:26:33,080 --> 00:26:37,720
the year 2040 we would exceed the power

664
00:26:35,880 --> 00:26:40,440
budget of the planet Earth we're still

665
00:26:37,720 --> 00:26:42,480
on track we're going to run out of power

666
00:26:40,440 --> 00:26:44,000
there won't be enough solar radiation

667
00:26:42,480 --> 00:26:45,919
from the sun we can collect nuclear

668
00:26:44,000 --> 00:26:47,840
power plants we can commission stuff we

669
00:26:45,919 --> 00:26:51,720
can dig up out of the earth and burn to

670
00:26:47,840 --> 00:26:54,559
keep up with this habit now that's bad

671
00:26:51,720 --> 00:26:56,240
um we don't want to do that uh and of

672
00:26:54,559 --> 00:26:58,240
course we won't you know we won't we

673
00:26:56,240 --> 00:26:59,880
we'll just top out at some point um but

674
00:26:58,240 --> 00:27:01,760
but this is actually a nice case where

675
00:26:59,880 --> 00:27:03,399
the negative externalities with respect

676
00:27:01,760 --> 00:27:05,080
to client are actually climate are

677
00:27:03,399 --> 00:27:07,919
actually perfectly aligned with the

678
00:27:05,080 --> 00:27:11,080
negative of the internal uh issue of

679
00:27:07,919 --> 00:27:12,440
cost so if you're if you're specking

680
00:27:11,080 --> 00:27:14,559
data centers I don't know if any of you

681
00:27:12,440 --> 00:27:16,360
are uh specking out data centers these

682
00:27:14,559 --> 00:27:18,760
days it's all about megawatts like where

683
00:27:16,360 --> 00:27:20,000
can I get this many megawatts of power

684
00:27:18,760 --> 00:27:22,480
everything is about power power

685
00:27:20,000 --> 00:27:24,880
basically is like the clearest um

686
00:27:22,480 --> 00:27:26,440
surrogate for cost and many of these

687
00:27:24,880 --> 00:27:27,640
systems are actually people are doing

688
00:27:26,440 --> 00:27:28,960
poc's and they go straight to the

689
00:27:27,640 --> 00:27:31,720
biggest model

690
00:27:28,960 --> 00:27:32,840
and then somebody eventually like this

691
00:27:31,720 --> 00:27:34,640
you know they and then they come to me

692
00:27:32,840 --> 00:27:36,440
and say oh my God like how are we

693
00:27:34,640 --> 00:27:39,159
supposed to afford this is is this

694
00:27:36,440 --> 00:27:42,760
actually even cheaper than the human

695
00:27:39,159 --> 00:27:46,000
labor that we're replacing with this so

696
00:27:42,760 --> 00:27:48,080
I think there's um you know there is an

697
00:27:46,000 --> 00:27:50,279
element of understanding the costs of

698
00:27:48,080 --> 00:27:53,760
the system are you targeting a high

699
00:27:50,279 --> 00:27:56,000
enough value application and and do you

700
00:27:53,760 --> 00:27:58,320
have the metrics in place to measure the

701
00:27:56,000 --> 00:28:01,159
cost takeout you're doing or the new OP

702
00:27:58,320 --> 00:28:04,880
Topline opportunity that you're bringing

703
00:28:01,159 --> 00:28:06,399
um and this is really tricky also for um

704
00:28:04,880 --> 00:28:08,200
any of you are some of you making

705
00:28:06,399 --> 00:28:11,799
decisions about what to do with

706
00:28:08,200 --> 00:28:15,559
generative AI in your businesses right

707
00:28:11,799 --> 00:28:19,640
now none of you are doing that okay or

708
00:28:15,559 --> 00:28:22,559
you're shy um you know the benchmarks

709
00:28:19,640 --> 00:28:25,519
that are available in the community are

710
00:28:22,559 --> 00:28:27,840
largely driven by academics and they're

711
00:28:25,519 --> 00:28:30,240
also driven by a small number of players

712
00:28:27,840 --> 00:28:31,760
so so like you like how do you choose

713
00:28:30,240 --> 00:28:33,279
what model do I need a 70 billion

714
00:28:31,760 --> 00:28:35,640
parameter model do I need an 8 billion

715
00:28:33,279 --> 00:28:37,799
parameter model do I need a 405 billion

716
00:28:35,640 --> 00:28:38,880
parameter model for this task um and

717
00:28:37,799 --> 00:28:41,279
then you look at these benchmarks and

718
00:28:38,880 --> 00:28:43,120
you're like oh well this MML thing that

719
00:28:41,279 --> 00:28:45,360
seems to be high and people usually use

720
00:28:43,120 --> 00:28:46,720
that as a circuit for for knowledge like

721
00:28:45,360 --> 00:28:48,880
how much knowledge does the model have

722
00:28:46,720 --> 00:28:49,919
but if you want to score well on mlu

723
00:28:48,880 --> 00:28:53,240
that means you need to be able to do

724
00:28:49,919 --> 00:28:56,000
college level physics and does your HR

725
00:28:53,240 --> 00:29:00,159
chatbot really need to know Advanced

726
00:28:56,000 --> 00:29:02,600
physics like is that important or not um

727
00:29:00,159 --> 00:29:05,039
you know Mt bench this is a multi-turn

728
00:29:02,600 --> 00:29:06,880
chat like how good is it at chatting if

729
00:29:05,039 --> 00:29:09,279
your model cannot talk like a pirate you

730
00:29:06,880 --> 00:29:10,320
will not get a good empty bench score

731
00:29:09,279 --> 00:29:12,159
role playing is one of the major

732
00:29:10,320 --> 00:29:14,640
categories of empty bench I would argue

733
00:29:12,159 --> 00:29:17,159
that it's undesirable for your chatbot

734
00:29:14,640 --> 00:29:19,919
for HR issues to to speak like a pirate

735
00:29:17,159 --> 00:29:23,200
so the benchmarks we have right now are

736
00:29:19,919 --> 00:29:25,320
woefully misaligned with the needs of of

737
00:29:23,200 --> 00:29:27,640
businesses um we and others are working

738
00:29:25,320 --> 00:29:28,960
to try and help correct that so like you

739
00:29:27,640 --> 00:29:31,840
give you something that you can latch on

740
00:29:28,960 --> 00:29:33,039
to but the number one thing I would say

741
00:29:31,840 --> 00:29:34,440
as you're doing model selection as

742
00:29:33,039 --> 00:29:36,360
you're scoping out your applications is

743
00:29:34,440 --> 00:29:40,039
to actually first and foremost get good

744
00:29:36,360 --> 00:29:42,000
at evaluation like Define define the

745
00:29:40,039 --> 00:29:44,080
business case Define what the

746
00:29:42,000 --> 00:29:46,480
opportunity is like if you if you

747
00:29:44,080 --> 00:29:47,720
succeeded beyond your expectations how

748
00:29:46,480 --> 00:29:49,720
much cost would this take out what

749
00:29:47,720 --> 00:29:51,679
Topline opportunity was give and then

750
00:29:49,720 --> 00:29:53,159
what's the cost and then how would you

751
00:29:51,679 --> 00:29:54,760
measure it to know that you're getting

752
00:29:53,159 --> 00:29:57,320
there along the way and one of the

753
00:29:54,760 --> 00:29:59,440
things we find that many of our clients

754
00:29:57,320 --> 00:30:01,159
of of IBM and then even our own internal

755
00:29:59,440 --> 00:30:02,440
teams struggle with is even just

756
00:30:01,159 --> 00:30:04,480
understanding like having some data that

757
00:30:02,440 --> 00:30:07,600
says like am I doing the thing I want to

758
00:30:04,480 --> 00:30:09,159
do um the other thing you have to in

759
00:30:07,600 --> 00:30:11,039
that evaluation piece which I think is

760
00:30:09,159 --> 00:30:15,960
super important everyone needs to like

761
00:30:11,039 --> 00:30:17,440
develop a competency in evaluation um

762
00:30:15,960 --> 00:30:18,840
and or you know work with ecosystem

763
00:30:17,440 --> 00:30:20,720
players to provide you products to give

764
00:30:18,840 --> 00:30:22,760
you that capability is also

765
00:30:20,720 --> 00:30:26,399
understanding what's the risk of Black

766
00:30:22,760 --> 00:30:29,519
Swan errors um because in some

767
00:30:26,399 --> 00:30:31,279
Industries you know if if it's a fun you

768
00:30:29,519 --> 00:30:33,840
know it's helping you know your kid

769
00:30:31,279 --> 00:30:35,000
write their college essay you know if it

770
00:30:33,840 --> 00:30:38,320
makes a mistake no big deal you're going

771
00:30:35,000 --> 00:30:40,600
to edit it if this is giving advice on

772
00:30:38,320 --> 00:30:41,760
policies for your company um the courts

773
00:30:40,600 --> 00:30:43,360
are already starting to rule that you're

774
00:30:41,760 --> 00:30:46,120
liable for that so there was an airline

775
00:30:43,360 --> 00:30:48,440
in Canada that had a chatbot and

776
00:30:46,120 --> 00:30:51,640
somebody asked it what's your refund

777
00:30:48,440 --> 00:30:53,679
policy and the model just blissfully

778
00:30:51,640 --> 00:30:55,960
hallucinated a policy which is not the

779
00:30:53,679 --> 00:30:58,320
correct policy and the Court ruled that

780
00:30:55,960 --> 00:31:00,039
no your representative you can't just

781
00:30:58,320 --> 00:31:02,399
say the AI did it that's like the dog

782
00:31:00,039 --> 00:31:04,880
ate my my homework but for business and

783
00:31:02,399 --> 00:31:08,200
for AI so you need to understand all of

784
00:31:04,880 --> 00:31:10,320
those things so I think um you know you

785
00:31:08,200 --> 00:31:11,840
really just need to be very strategic

786
00:31:10,320 --> 00:31:13,120
and holistic about understanding

787
00:31:11,840 --> 00:31:15,320
understanding how to measure and that's

788
00:31:13,120 --> 00:31:17,440
all going to decide you know in the near

789
00:31:15,320 --> 00:31:18,960
term how this technology is going to is

790
00:31:17,440 --> 00:31:20,559
going to impact your business you really

791
00:31:18,960 --> 00:31:22,039
think about that you know it's it's

792
00:31:20,559 --> 00:31:22,960
really simple it's like how much how

793
00:31:22,039 --> 00:31:24,440
much money you're going to get how much

794
00:31:22,960 --> 00:31:26,559
money is it going to cost do the

795
00:31:24,440 --> 00:31:30,600
balancing yeah and go from I just wanted

796
00:31:26,559 --> 00:31:32,519
to add like a 30 second kind of um idea

797
00:31:30,600 --> 00:31:33,960
about these benchmarks as well like how

798
00:31:32,519 --> 00:31:36,000
how do you Benchmark these things it's

799
00:31:33,960 --> 00:31:37,720
also important like to understand these

800
00:31:36,000 --> 00:31:39,960
benchmarks are designed for general

801
00:31:37,720 --> 00:31:42,799
intelligence we're talking about future

802
00:31:39,960 --> 00:31:45,120
versus today so if you say value of gen

803
00:31:42,799 --> 00:31:46,760
today it's not going to be measured by

804
00:31:45,120 --> 00:31:48,120
those by those benchmarks that you're

805
00:31:46,760 --> 00:31:49,760
actually putting together we talking

806
00:31:48,120 --> 00:31:52,039
about general intelligence right like

807
00:31:49,760 --> 00:31:54,240
with the test of mmu for example it's

808
00:31:52,039 --> 00:31:56,360
like for for for systems that want to be

809
00:31:54,240 --> 00:31:58,880
generally good you know solving the the

810
00:31:56,360 --> 00:32:00,480
homeworks of your kids and uh giving you

811
00:31:58,880 --> 00:32:02,720
Financial advice at the same time so

812
00:32:00,480 --> 00:32:04,559
this is basically the area where uh you

813
00:32:02,720 --> 00:32:06,679
know like like the big big companies

814
00:32:04,559 --> 00:32:08,519
like openai and and Tropics of the world

815
00:32:06,679 --> 00:32:10,000
are actually moving towards you know so

816
00:32:08,519 --> 00:32:11,799
that's why like The Benchmark design and

817
00:32:10,000 --> 00:32:13,919
everything is kind of driven by that

818
00:32:11,799 --> 00:32:15,399
right so but today you want to have like

819
00:32:13,919 --> 00:32:17,919
more special like if you want to use

820
00:32:15,399 --> 00:32:19,799
today as action systems like system that

821
00:32:17,919 --> 00:32:21,960
you want to put them in production not

822
00:32:19,799 --> 00:32:23,519
system for productivity for productivity

823
00:32:21,960 --> 00:32:25,399
you can use any of these chat Bots but

824
00:32:23,519 --> 00:32:27,240
if you want to have an action system A

825
00:32:25,399 --> 00:32:29,639
system that takes action in especially

826
00:32:27,240 --> 00:32:31,919
safety critical applications you want to

827
00:32:29,639 --> 00:32:34,279
have like dedicated kind of benchmarks

828
00:32:31,919 --> 00:32:36,159
internal kind of evaluations set so that

829
00:32:34,279 --> 00:32:37,799
you can make them reliable and you would

830
00:32:36,159 --> 00:32:39,360
want to have like safety mechanisms in

831
00:32:37,799 --> 00:32:41,600
place so short there's a short-term

832
00:32:39,360 --> 00:32:45,799
long-term play in gen you need to always

833
00:32:41,600 --> 00:32:48,279
be aware of yeah um I actually looking

834
00:32:45,799 --> 00:32:51,519
at the question and related to uh the

835
00:32:48,279 --> 00:32:53,480
one we just had um do you have example

836
00:32:51,519 --> 00:32:56,279
of use cases in your

837
00:32:53,480 --> 00:32:59,559
organization where the model or AI had

838
00:32:56,279 --> 00:33:02,600
an impact yes yes of course so basically

839
00:32:59,559 --> 00:33:05,240
uh in Enterprise AI we have been

840
00:33:02,600 --> 00:33:09,519
targeting various use cases that

841
00:33:05,240 --> 00:33:12,559
enterprises want one of them is uh uh

842
00:33:09,519 --> 00:33:15,720
understanding uh internal knowledge so

843
00:33:12,559 --> 00:33:19,480
so you have uh these millions of pages

844
00:33:15,720 --> 00:33:22,760
of uh manuals and uh kind of written

845
00:33:19,480 --> 00:33:24,880
statements uh maybe legal documents and

846
00:33:22,760 --> 00:33:27,039
many people who were working with them

847
00:33:24,880 --> 00:33:29,200
might have left the company and then

848
00:33:27,039 --> 00:33:34,320
somebody want a query answer that you

849
00:33:29,200 --> 00:33:37,120
know can uh can I do this and in order

850
00:33:34,320 --> 00:33:39,159
to do that I mean you you need somebody

851
00:33:37,120 --> 00:33:41,480
previously would have to go through all

852
00:33:39,159 --> 00:33:44,000
those relevant documents and basically

853
00:33:41,480 --> 00:33:46,440
come and after maybe one week give you

854
00:33:44,000 --> 00:33:49,360
an answer yes this was done in so and so

855
00:33:46,440 --> 00:33:52,159
date on so and so time but now with this

856
00:33:49,360 --> 00:33:54,760
kind of uh generative AI models you can

857
00:33:52,159 --> 00:33:58,360
actually train them or use this kind of

858
00:33:54,760 --> 00:34:00,919
rag type uh schema inside your company

859
00:33:58,360 --> 00:34:03,399
and they can actually provide you with

860
00:34:00,919 --> 00:34:06,480
an answer and also a reference as to

861
00:34:03,399 --> 00:34:08,960
which uh place they got that answer from

862
00:34:06,480 --> 00:34:11,119
in just a few seconds so that's a huge

863
00:34:08,960 --> 00:34:13,200
increase in productivity so that's one

864
00:34:11,119 --> 00:34:15,800
of the things that people want they want

865
00:34:13,200 --> 00:34:18,679
to distill all this uh internal

866
00:34:15,800 --> 00:34:21,200
knowledge uh quickly uh uh so that they

867
00:34:18,679 --> 00:34:24,800
can get answers the other thing uh that

868
00:34:21,200 --> 00:34:27,480
people were uh uh looking at is this uh

869
00:34:24,800 --> 00:34:30,240
uh Network logs so there is a network

870
00:34:27,480 --> 00:34:33,000
out outage and there is an expert who

871
00:34:30,240 --> 00:34:35,399
looks at all these logs at various nodes

872
00:34:33,000 --> 00:34:37,839
in the network to figure out exactly

873
00:34:35,399 --> 00:34:40,520
what happened and now if you can train a

874
00:34:37,839 --> 00:34:42,879
model on those logs and the incident

875
00:34:40,520 --> 00:34:45,720
that happened before then now it can

876
00:34:42,879 --> 00:34:47,760
actually tell you oh in in maybe a split

877
00:34:45,720 --> 00:34:49,399
second that this is what actually

878
00:34:47,760 --> 00:34:51,480
happened you need to change change this

879
00:34:49,399 --> 00:34:53,480
node in the network or you need to make

880
00:34:51,480 --> 00:34:56,000
this kind of corrective action so these

881
00:34:53,480 --> 00:34:57,880
are some of the things that um uh

882
00:34:56,000 --> 00:35:01,160
actually people people use in the

883
00:34:57,880 --> 00:35:02,800
Enterprise yeah thank you um I see at

884
00:35:01,160 --> 00:35:05,520
the top one of the question I want to

885
00:35:02,800 --> 00:35:08,640
know David what is your prediction for

886
00:35:05,520 --> 00:35:11,359
the impact of quantum Computing on AI in

887
00:35:08,640 --> 00:35:13,880
the short and long term okay that was M

888
00:35:11,359 --> 00:35:16,400
my bad I said the word

889
00:35:13,880 --> 00:35:19,359
Quantum um so Quantum Computing is

890
00:35:16,400 --> 00:35:21,480
coming I mean IBM we have uh some of the

891
00:35:19,359 --> 00:35:24,280
very best and largest quantum computers

892
00:35:21,480 --> 00:35:25,920
in the world today um Quantum general

893
00:35:24,280 --> 00:35:28,079
purpose Quantum Computing isn't here yet

894
00:35:25,920 --> 00:35:30,000
and but it but we are getting into this

895
00:35:28,079 --> 00:35:31,240
age of what they call Quantum utility

896
00:35:30,000 --> 00:35:32,440
where we can actually do things with a

897
00:35:31,240 --> 00:35:35,599
quantum computer that you couldn't

898
00:35:32,440 --> 00:35:38,839
possibly do with classical computers now

899
00:35:35,599 --> 00:35:40,920
one misapprehension I want to just

900
00:35:38,839 --> 00:35:43,240
disabuse every one of all at once here

901
00:35:40,920 --> 00:35:45,079
on mass I'm going to do do my part for

902
00:35:43,240 --> 00:35:47,560
this is that a quantum computer isn't

903
00:35:45,079 --> 00:35:50,240
simply a faster computer it's a

904
00:35:47,560 --> 00:35:53,359
different kind of computer so computers

905
00:35:50,240 --> 00:35:55,760
compete with bits zeros and ones um

906
00:35:53,359 --> 00:35:57,079
quantum computers compute with cubits

907
00:35:55,760 --> 00:35:59,760
and they just have fundamentally

908
00:35:57,079 --> 00:36:01,839
different properties to them so they're

909
00:35:59,760 --> 00:36:04,119
amazing for things like if you want to

910
00:36:01,839 --> 00:36:06,560
simulate a material you can't beat a

911
00:36:04,119 --> 00:36:09,040
quantum computer because materials at

912
00:36:06,560 --> 00:36:12,680
the level of sort of atoms and electrons

913
00:36:09,040 --> 00:36:14,560
and and interacting obey Quantum you

914
00:36:12,680 --> 00:36:16,720
know quantum mechanical laws it's not

915
00:36:14,560 --> 00:36:20,000
like the physics we're used to at the

916
00:36:16,720 --> 00:36:21,520
length scales that we as humans occupy

917
00:36:20,000 --> 00:36:23,480
and we're going to have huge

918
00:36:21,520 --> 00:36:25,319
implications in the field of Material

919
00:36:23,480 --> 00:36:27,720
Science for instance from that you know

920
00:36:25,319 --> 00:36:29,160
new materials that can sequester carbon

921
00:36:27,720 --> 00:36:31,400
and all kinds of things are going to be

922
00:36:29,160 --> 00:36:32,599
amazing from that um they can also do

923
00:36:31,400 --> 00:36:35,400
machine learning you can do Quantum

924
00:36:32,599 --> 00:36:37,480
machine learning um but I would say at

925
00:36:35,400 --> 00:36:39,400
the stage we're at right now like don't

926
00:36:37,480 --> 00:36:40,720
just assume that when we get quantum

927
00:36:39,400 --> 00:36:43,839
computers that's going to magically

928
00:36:40,720 --> 00:36:45,599
somehow speed up what we do with AI it

929
00:36:43,839 --> 00:36:49,440
it'll work in certain very particular

930
00:36:45,599 --> 00:36:50,680
domains and be interesting but um we're

931
00:36:49,440 --> 00:36:52,400
not going to we don't see a world at

932
00:36:50,680 --> 00:36:53,720
least not at IBM where our clown

933
00:36:52,400 --> 00:36:55,880
computer is just like the solution

934
00:36:53,720 --> 00:36:58,319
actually what we're building is hybrid

935
00:36:55,880 --> 00:37:01,119
data centers that mix classical

936
00:36:58,319 --> 00:37:04,240
Computing Quantum Computing and then

937
00:37:01,119 --> 00:37:05,680
increasingly optimized AI acceleration

938
00:37:04,240 --> 00:37:08,880
Hardware because it's just different

939
00:37:05,680 --> 00:37:10,920
enough um you know gpus are the game are

940
00:37:08,880 --> 00:37:12,319
are s of the dominant architecture today

941
00:37:10,920 --> 00:37:14,119
we actually have something internal with

942
00:37:12,319 --> 00:37:17,240
an IBM which is actually a nonv noan

943
00:37:14,119 --> 00:37:19,680
architecture that can can run uh llms

944
00:37:17,240 --> 00:37:21,200
and other deep learning extremely fast

945
00:37:19,680 --> 00:37:22,400
um and we'll see continuing Innovation

946
00:37:21,200 --> 00:37:24,640
there but what Quantum is really going

947
00:37:22,400 --> 00:37:26,560
to win is something where it's like no

948
00:37:24,640 --> 00:37:29,160
matter how many computers you have it's

949
00:37:26,560 --> 00:37:31,920
just the problem the algorithms we have

950
00:37:29,160 --> 00:37:33,480
won't get you there so a classic example

951
00:37:31,920 --> 00:37:36,200
of something when we do have a general

952
00:37:33,480 --> 00:37:37,400
purpose on a computer is uh factoring

953
00:37:36,200 --> 00:37:39,440
prime numbers so a lot of our

954
00:37:37,400 --> 00:37:41,960
cryptography depends on taking two

955
00:37:39,440 --> 00:37:43,960
numbers very large numbers they prime

956
00:37:41,960 --> 00:37:46,640
numbers multiplying them together and

957
00:37:43,960 --> 00:37:47,960
then it's very easy to verify if you

958
00:37:46,640 --> 00:37:49,839
have those two original numbers if

959
00:37:47,960 --> 00:37:50,960
that's right but it's it's very hard to

960
00:37:49,839 --> 00:37:52,599
go the other way with a classical

961
00:37:50,960 --> 00:37:55,599
computer like the algorithms that we

962
00:37:52,599 --> 00:37:57,800
have for that scale in an unpleasant way

963
00:37:55,599 --> 00:37:59,680
like it like and it's to the level of if

964
00:37:57,800 --> 00:38:01,760
you had the fastest Computing chips and

965
00:37:59,680 --> 00:38:03,000
you tiled the Earth with them it

966
00:38:01,760 --> 00:38:04,920
wouldn't be enough compute because it

967
00:38:03,000 --> 00:38:06,359
just scales so poorly and what quantum

968
00:38:04,920 --> 00:38:08,200
computers do is they can they can

969
00:38:06,359 --> 00:38:10,240
actually they obey different rules and

970
00:38:08,200 --> 00:38:12,000
they can have a different scaling so all

971
00:38:10,240 --> 00:38:15,000
of those kinds of cryptography aths are

972
00:38:12,000 --> 00:38:16,520
going to be broken uh eventually

973
00:38:15,000 --> 00:38:18,520
thankfully we have postquantum

974
00:38:16,520 --> 00:38:20,040
cryptography actually IBM contributed

975
00:38:18,520 --> 00:38:21,280
several of the algorithms to this we

976
00:38:20,040 --> 00:38:22,920
have other ways to do it that aren't

977
00:38:21,280 --> 00:38:26,079
that are resistant to that kind of

978
00:38:22,920 --> 00:38:28,119
quantum algorithm but um but I I don't

979
00:38:26,079 --> 00:38:29,640
think we're going to see some like bang

980
00:38:28,119 --> 00:38:31,240
moment where it's like oh now we have P

981
00:38:29,640 --> 00:38:32,760
computers now we're just going to do the

982
00:38:31,240 --> 00:38:35,240
AI thing with that now the other

983
00:38:32,760 --> 00:38:36,720
direction is even more interesting

984
00:38:35,240 --> 00:38:39,119
though because a Quon computer is a

985
00:38:36,720 --> 00:38:40,200
really complicated analog system there's

986
00:38:39,119 --> 00:38:42,000
lots of different ways to do a quic

987
00:38:40,200 --> 00:38:44,400
computer but like kind we have you have

988
00:38:42,000 --> 00:38:46,760
like a cryostat which is a really really

989
00:38:44,400 --> 00:38:48,280
really cold refrigerator it's got heal

990
00:38:46,760 --> 00:38:49,839
it's like the cold like close to

991
00:38:48,280 --> 00:38:52,280
absolute zero and you're putting

992
00:38:49,839 --> 00:38:54,319
microwave pulses into it AI is actually

993
00:38:52,280 --> 00:38:55,920
really great for controlling that analog

994
00:38:54,319 --> 00:38:59,119
system see people are using AI

995
00:38:55,920 --> 00:39:02,079
Technologies to to came the Quant the

996
00:38:59,119 --> 00:39:04,560
cubits and that's that's a very

997
00:39:02,079 --> 00:39:06,119
effective and interesting place and we

998
00:39:04,560 --> 00:39:07,440
and we will we are starting to see Niche

999
00:39:06,119 --> 00:39:09,440
applications particularly the financial

1000
00:39:07,440 --> 00:39:12,760
sector is really interested in certain

1001
00:39:09,440 --> 00:39:16,119
kinds of AI algorithms not like whole

1002
00:39:12,760 --> 00:39:17,720
llms but things more like Quantum kernel

1003
00:39:16,119 --> 00:39:20,160
machines like for classification and

1004
00:39:17,720 --> 00:39:21,480
doing very very high value kind of

1005
00:39:20,160 --> 00:39:24,599
things that maybe a quantum computer

1006
00:39:21,480 --> 00:39:25,560
could be useful for but um so lots of

1007
00:39:24,599 --> 00:39:27,400
things are going to change and be

1008
00:39:25,560 --> 00:39:30,440
interesting but I don't think it's a one

1009
00:39:27,400 --> 00:39:33,000
+ 1 equals you know five kind of thing

1010
00:39:30,440 --> 00:39:35,760
that we're going to see anytime soon but

1011
00:39:33,000 --> 00:39:38,200
I've had to eat my words about l so I I

1012
00:39:35,760 --> 00:39:40,480
reserve yeah so so at Fujitsu actually

1013
00:39:38,200 --> 00:39:43,079
we do uh a lot of research in Quantum

1014
00:39:40,480 --> 00:39:45,000
machine learning so there's a team under

1015
00:39:43,079 --> 00:39:48,560
me who actually does some research on

1016
00:39:45,000 --> 00:39:52,200
that and uh yeah as uh David said I mean

1017
00:39:48,560 --> 00:39:54,000
the the speed up that you get in uh U

1018
00:39:52,200 --> 00:39:57,720
Quantum machine learning is not

1019
00:39:54,000 --> 00:40:00,680
exponential so you don't uh hope to uh

1020
00:39:57,720 --> 00:40:03,280
get to anywhere with a few number of

1021
00:40:00,680 --> 00:40:04,920
cubits so you are probably looking at

1022
00:40:03,280 --> 00:40:07,520
maybe hundreds of thousands or even

1023
00:40:04,920 --> 00:40:10,079
millions of cubits before you can see

1024
00:40:07,520 --> 00:40:11,960
the impact of that speed up on like AI

1025
00:40:10,079 --> 00:40:14,160
training or AI inference and that kind

1026
00:40:11,960 --> 00:40:17,200
of stuff so we are currently at around

1027
00:40:14,160 --> 00:40:19,400
a, noisy cubits so we are nowhere near

1028
00:40:17,200 --> 00:40:20,920
that so in the short term I would say I

1029
00:40:19,400 --> 00:40:23,680
mean the applications would be quite

1030
00:40:20,920 --> 00:40:26,119
limited and the impact on AI but in the

1031
00:40:23,680 --> 00:40:27,800
longer term yeah there will be uh

1032
00:40:26,119 --> 00:40:30,280
definitely an impact into training

1033
00:40:27,800 --> 00:40:32,440
because currently the llm models I mean

1034
00:40:30,280 --> 00:40:35,280
training from scratch you're looking at

1035
00:40:32,440 --> 00:40:38,359
months sometimes and so if you can even

1036
00:40:35,280 --> 00:40:40,480
speed that up uh by a polinomial time uh

1037
00:40:38,359 --> 00:40:43,000
in computer science jargon then you will

1038
00:40:40,480 --> 00:40:45,359
be bringing that down to maybe weeks or

1039
00:40:43,000 --> 00:40:48,280
days so that would be a huge

1040
00:40:45,359 --> 00:40:50,839
impact so we just have a few minutes uh

1041
00:40:48,280 --> 00:40:53,160
but I would like to jump into the future

1042
00:40:50,839 --> 00:40:55,599
um and actually so I spend the

1043
00:40:53,160 --> 00:40:58,160
weekend try reading and trying to find

1044
00:40:55,599 --> 00:41:00,240
information about strawberry

1045
00:40:58,160 --> 00:41:03,079
so if you don't know what strawberry is

1046
00:41:00,240 --> 00:41:06,040
it's the nickname of uh the the new

1047
00:41:03,079 --> 00:41:08,480
model that openai put available for a

1048
00:41:06,040 --> 00:41:11,960
lot of us to play with it's also called

1049
00:41:08,480 --> 00:41:15,599
thing o1 but what intrigued me is that

1050
00:41:11,960 --> 00:41:17,480
so I'm a I work on cognition human uh

1051
00:41:15,599 --> 00:41:20,520
cognition and it seems that one of the

1052
00:41:17,480 --> 00:41:23,000
old Mark of intelligence is actually

1053
00:41:20,520 --> 00:41:26,560
reasoning so I would like to ask the

1054
00:41:23,000 --> 00:41:27,800
three of you uh final words on what will

1055
00:41:26,560 --> 00:41:31,760
be the next

1056
00:41:27,800 --> 00:41:34,680
next breakthrough that you think we

1057
00:41:31,760 --> 00:41:35,680
should see or AI should do in order to

1058
00:41:34,680 --> 00:41:39,480
move

1059
00:41:35,680 --> 00:41:42,440
towards system that are more

1060
00:41:39,480 --> 00:41:46,400
intelligent any you want to start two

1061
00:41:42,440 --> 00:41:49,599
minute each yeah I can I can getar this

1062
00:41:46,400 --> 00:41:50,960
so this is not the the the most uh uh

1063
00:41:49,599 --> 00:41:52,680
interesting thing that came out like

1064
00:41:50,960 --> 00:41:54,880
there is a there is another model after

1065
00:41:52,680 --> 00:41:56,880
this one strawberry from open AI it's

1066
00:41:54,880 --> 00:41:58,400
like gbd 5 actually the one that

1067
00:41:56,880 --> 00:42:01,839
actually coming out that one is like

1068
00:41:58,400 --> 00:42:05,040
pretty impressive this one is still um a

1069
00:42:01,839 --> 00:42:07,359
good system it's a reason it's a f first

1070
00:42:05,040 --> 00:42:09,440
uh you know like you can you can do

1071
00:42:07,359 --> 00:42:11,280
computation as I told you like when you

1072
00:42:09,440 --> 00:42:14,000
train these models you train them on

1073
00:42:11,280 --> 00:42:15,720
enormous amount of data right so you can

1074
00:42:14,000 --> 00:42:19,119
you can spend that amount of data

1075
00:42:15,720 --> 00:42:22,079
training like training a model or you

1076
00:42:19,119 --> 00:42:25,359
can take like a well- trained model not

1077
00:42:22,079 --> 00:42:29,119
that large and then try

1078
00:42:25,359 --> 00:42:30,880
to search opportunities that this system

1079
00:42:29,119 --> 00:42:34,079
from the knowledge base that it actually

1080
00:42:30,880 --> 00:42:35,760
created at inference time to actually

1081
00:42:34,079 --> 00:42:38,079
create like more reasoning more

1082
00:42:35,760 --> 00:42:40,160
scenarios you know if this happens then

1083
00:42:38,079 --> 00:42:43,319
that happens you know and then this and

1084
00:42:40,160 --> 00:42:46,160
then try to actually encode all of this

1085
00:42:43,319 --> 00:42:48,960
you know input reasoning and thinking

1086
00:42:46,160 --> 00:42:50,359
kind of passes and then try to make a

1087
00:42:48,960 --> 00:42:52,880
decision so that that kind of

1088
00:42:50,359 --> 00:42:55,200
exploration and then trying to find out

1089
00:42:52,880 --> 00:42:57,720
the answer to a question by exploration

1090
00:42:55,200 --> 00:42:59,680
like deploying search and you know and

1091
00:42:57,720 --> 00:43:01,160
this this kind of thing at test time

1092
00:42:59,680 --> 00:43:03,440
this is something that the strawberry is

1093
00:43:01,160 --> 00:43:06,000
actually doing right so they're doing

1094
00:43:03,440 --> 00:43:07,520
test time uh kind of computation you

1095
00:43:06,000 --> 00:43:11,000
know so they're increasing the capacity

1096
00:43:07,520 --> 00:43:12,760
of compute on the test side and you know

1097
00:43:11,000 --> 00:43:14,319
like basically balancing between the two

1098
00:43:12,760 --> 00:43:16,640
you know you can spend a lot of time

1099
00:43:14,319 --> 00:43:18,480
training a Model A lot of data or you

1100
00:43:16,640 --> 00:43:20,599
can change the narrative and and and

1101
00:43:18,480 --> 00:43:22,839
just you know train at test time you

1102
00:43:20,599 --> 00:43:25,079
know like that's that's basically the

1103
00:43:22,839 --> 00:43:27,800
the the discovery that they had so this

1104
00:43:25,079 --> 00:43:30,480
seems to be a nice way for for AI

1105
00:43:27,800 --> 00:43:33,280
systems to memor you see before that AI

1106
00:43:30,480 --> 00:43:36,160
systems were memorizing uh data input

1107
00:43:33,280 --> 00:43:39,400
outputs now they're memorizing reasoning

1108
00:43:36,160 --> 00:43:41,319
so it's still in in distribution so it's

1109
00:43:39,400 --> 00:43:43,640
it's still what I would say is not like

1110
00:43:41,319 --> 00:43:45,599
something that goes beyond distribution

1111
00:43:43,640 --> 00:43:47,359
kind of learning you need something more

1112
00:43:45,599 --> 00:43:49,920
than that and that could happen in the

1113
00:43:47,359 --> 00:43:51,400
future thank you David your two minutes

1114
00:43:49,920 --> 00:43:52,680
so I mean anytime one of these models

1115
00:43:51,400 --> 00:43:54,599
comes out I have a team that goes and

1116
00:43:52,680 --> 00:43:56,359
tries to reverse engineer what what

1117
00:43:54,599 --> 00:43:58,000
exactly they did and and what we should

1118
00:43:56,359 --> 00:43:59,960
do about it and and what we should think

1119
00:43:58,000 --> 00:44:03,400
about it and and I think that piece

1120
00:43:59,960 --> 00:44:05,599
around being smarter about what you do

1121
00:44:03,400 --> 00:44:07,480
at inference time I mean that's

1122
00:44:05,599 --> 00:44:09,720
obviously a big part of it but I think

1123
00:44:07,480 --> 00:44:11,920
it's part of actually a larger Trend

1124
00:44:09,720 --> 00:44:13,720
which is don't assume that we're going

1125
00:44:11,920 --> 00:44:15,440
to have this Magic model that just is

1126
00:44:13,720 --> 00:44:17,359
all knowing and all being and all

1127
00:44:15,440 --> 00:44:20,280
thinking the pattern we're seeing much

1128
00:44:17,359 --> 00:44:24,040
more and more is actually embedding the

1129
00:44:20,280 --> 00:44:26,040
llm in a system of other tools and um

1130
00:44:24,040 --> 00:44:27,280
you might remember alphago this was a

1131
00:44:26,040 --> 00:44:29,240
reinforcement learning system and they

1132
00:44:27,280 --> 00:44:31,520
could play go it beat the the champion

1133
00:44:29,240 --> 00:44:33,400
in the in go and everyone said oh this

1134
00:44:31,520 --> 00:44:35,240
is fantastic reinforcement learning has

1135
00:44:33,400 --> 00:44:36,319
has really gotten us there a big part of

1136
00:44:35,240 --> 00:44:38,760
that system was something called Monte

1137
00:44:36,319 --> 00:44:41,520
Carlo treesearch which is actually a

1138
00:44:38,760 --> 00:44:44,000
classical uh you know AI like old school

1139
00:44:41,520 --> 00:44:45,480
AI kind of technique and you know

1140
00:44:44,000 --> 00:44:46,880
there's there's there's a lot of that

1141
00:44:45,480 --> 00:44:49,000
happening I think behind the scenes with

1142
00:44:46,880 --> 00:44:50,359
a lot of these these models and I think

1143
00:44:49,000 --> 00:44:53,760
even more than that what we're going to

1144
00:44:50,359 --> 00:44:55,480
see is treat an llm as a new tool that's

1145
00:44:53,760 --> 00:44:57,319
embedded in a larger system and when you

1146
00:44:55,480 --> 00:44:59,480
get to the issue of control this becomes

1147
00:44:57,319 --> 00:45:00,920
really important because I might not

1148
00:44:59,480 --> 00:45:02,680
just want to rely on a Model just to

1149
00:45:00,920 --> 00:45:05,040
give me the answer I might have a policy

1150
00:45:02,680 --> 00:45:07,280
that says hey if some employee asks this

1151
00:45:05,040 --> 00:45:08,920
question about this HR issue I I

1152
00:45:07,280 --> 00:45:10,480
shouldn't take what the model says I

1153
00:45:08,920 --> 00:45:13,040
have a thing that the lawyers wrote and

1154
00:45:10,480 --> 00:45:14,880
I need to I need to show them that and

1155
00:45:13,040 --> 00:45:16,240
sometimes we have this posture that as

1156
00:45:14,880 --> 00:45:17,839
all knowing model is going to know

1157
00:45:16,240 --> 00:45:19,440
everything and the reality is it's going

1158
00:45:17,839 --> 00:45:21,319
to be part of a system that includes

1159
00:45:19,440 --> 00:45:23,640
classical traditional Computing

1160
00:45:21,319 --> 00:45:25,760
components going to include classical AI

1161
00:45:23,640 --> 00:45:27,359
methods things like Monte Carlo

1162
00:45:25,760 --> 00:45:29,200
treesearch but also probably things like

1163
00:45:27,359 --> 00:45:31,119
planning and other kinds of reasoning

1164
00:45:29,200 --> 00:45:33,599
and we're going to have hybrid systems

1165
00:45:31,119 --> 00:45:35,160
that are are are more controllable and

1166
00:45:33,599 --> 00:45:38,119
and more powerful that harness the magic

1167
00:45:35,160 --> 00:45:42,200
of the LM but but don't don't rely on it

1168
00:45:38,119 --> 00:45:44,160
exclusively thank you so uh basically uh

1169
00:45:42,200 --> 00:45:46,480
the reasoning is very important in case

1170
00:45:44,160 --> 00:45:49,079
of auditing and governance so whenever

1171
00:45:46,480 --> 00:45:51,839
you have an AI system in in your uh

1172
00:45:49,079 --> 00:45:54,040
product and it makes make some kind of a

1173
00:45:51,839 --> 00:45:56,240
decision you would like to know if the

1174
00:45:54,040 --> 00:45:58,520
decision is wrong where actually did it

1175
00:45:56,240 --> 00:46:00,960
go wrong and that is very important

1176
00:45:58,520 --> 00:46:04,319
because you are liable right like like

1177
00:46:00,960 --> 00:46:07,160
he said about that GP uh that chatbot

1178
00:46:04,319 --> 00:46:09,680
which gave some hallucinated answer you

1179
00:46:07,160 --> 00:46:12,040
you don't want to get to that uh point

1180
00:46:09,680 --> 00:46:15,400
so whenever there is this kind of poor

1181
00:46:12,040 --> 00:46:17,480
reasoning or the the training is not

1182
00:46:15,400 --> 00:46:20,000
enough so that the llm has some

1183
00:46:17,480 --> 00:46:22,200
different uh Notions of knowledge or

1184
00:46:20,000 --> 00:46:24,079
there is bias in the data you want to

1185
00:46:22,200 --> 00:46:26,200
know all these kind of things so that

1186
00:46:24,079 --> 00:46:28,520
you can correct for that so the so you

1187
00:46:26,200 --> 00:46:30,880
don't run into all these real life

1188
00:46:28,520 --> 00:46:33,599
issues so these are the reasons why I

1189
00:46:30,880 --> 00:46:36,559
think this reasoning part of that llm is

1190
00:46:33,599 --> 00:46:39,800
very important all right well that's the

1191
00:46:36,559 --> 00:46:43,359
end of our session uh thank you everyone

1192
00:46:39,800 --> 00:46:46,880
for your question thank you uh for um to

1193
00:46:43,359 --> 00:46:50,880
be here it was uh very Illuminating okay

1194
00:46:46,880 --> 00:46:50,880
well see you next time

