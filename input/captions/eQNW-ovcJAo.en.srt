1
00:00:00,000 --> 00:00:02,475
[MUSIC PLAYING]

2
00:00:02,475 --> 00:00:17,227

3
00:00:17,227 --> 00:00:18,310
MEGAN MITCHELL: All right.

4
00:00:18,310 --> 00:00:22,210
Hello and good morning to
those of you joining us.

5
00:00:22,210 --> 00:00:27,390
And welcome to part two of
our AI Innovation Series

6
00:00:27,390 --> 00:00:29,940
here at Jameel
World Education Lab.

7
00:00:29,940 --> 00:00:31,750
For those of you
who don't know me,

8
00:00:31,750 --> 00:00:36,780
I am Megan Mitchell and
Acting Director for Operations

9
00:00:36,780 --> 00:00:38,200
and Strategy at J-WEL.

10
00:00:38,200 --> 00:00:40,920
And I'm here to serve
as a moderator today.

11
00:00:40,920 --> 00:00:43,650
But what we're hopeful is
that this will be really

12
00:00:43,650 --> 00:00:49,740
a lively, interactive discussion
with folks, chatting with one

13
00:00:49,740 --> 00:00:51,480
another, asking questions.

14
00:00:51,480 --> 00:00:55,000
Of course, I have questions
prepped and prepared.

15
00:00:55,000 --> 00:01:00,300
But I am hoping that I become
almost irrelevant as people

16
00:01:00,300 --> 00:01:01,150
start diving in.

17
00:01:01,150 --> 00:01:05,519
So don't hesitate, please,
to speak up and share

18
00:01:05,519 --> 00:01:07,560
your perspectives.

19
00:01:07,560 --> 00:01:12,070
This actually builds on our
first event of the series,

20
00:01:12,070 --> 00:01:14,580
which happened last
Tuesday on August 6th,

21
00:01:14,580 --> 00:01:16,830
and the video is
available for that,

22
00:01:16,830 --> 00:01:23,110
where we hosted MIT Dean of Open
Learning and Digital Learning,

23
00:01:23,110 --> 00:01:24,450
Cynthia Breazeal.

24
00:01:24,450 --> 00:01:28,770
She's also a professor at the
Media Lab to talk about her work

25
00:01:28,770 --> 00:01:32,850
and the work that's
happening at MIT on applying

26
00:01:32,850 --> 00:01:37,650
AI for intelligent agents to
serve as learning companions

27
00:01:37,650 --> 00:01:42,390
and mentors and
really the great work

28
00:01:42,390 --> 00:01:47,910
that's being done on
leveraging social and emotional

29
00:01:47,910 --> 00:01:50,910
artificial intelligence
is key to optimizing

30
00:01:50,910 --> 00:01:54,210
learning for
particularly younger

31
00:01:54,210 --> 00:01:57,250
learners with the promise
of this transferring

32
00:01:57,250 --> 00:02:00,160
to older learners as well.

33
00:02:00,160 --> 00:02:02,510
And really, these
AI powered robots,

34
00:02:02,510 --> 00:02:05,330
which are exciting
and fun to see,

35
00:02:05,330 --> 00:02:10,240
can be used as peers
and learning companions,

36
00:02:10,240 --> 00:02:13,690
because it's really critical
that that social activity stays

37
00:02:13,690 --> 00:02:16,540
in as a part of the
learning process.

38
00:02:16,540 --> 00:02:20,170
And beyond that, Dean
Breazeal also really spent

39
00:02:20,170 --> 00:02:22,850
a lot of time talking
about not just AI literacy,

40
00:02:22,850 --> 00:02:28,640
but AI fluency, and what does it
mean to not just learn about AI,

41
00:02:28,640 --> 00:02:31,270
but learn how to use
AI and how to use it

42
00:02:31,270 --> 00:02:37,900
as a responsible actor,
and really driving it

43
00:02:37,900 --> 00:02:40,780
forward as a learner as well.

44
00:02:40,780 --> 00:02:46,480
So with that setup,
we have two colleagues

45
00:02:46,480 --> 00:02:49,120
of Cynthia Breazeal's,
of Professor Breazeal's

46
00:02:49,120 --> 00:02:53,270
here, Hae Won Park,
and Sharifa Alghowinem,

47
00:02:53,270 --> 00:02:56,290
who will be speaking
about the work that's

48
00:02:56,290 --> 00:03:00,160
being done they're doing also
as part of the MIT Media Lab's

49
00:03:00,160 --> 00:03:03,580
Personal Robotics Group
and really building

50
00:03:03,580 --> 00:03:07,090
on the discussion
that happened in what

51
00:03:07,090 --> 00:03:09,490
Professor Breazeal presented.

52
00:03:09,490 --> 00:03:11,200
And then from there,
we've actually

53
00:03:11,200 --> 00:03:13,150
asked two of our
members to join us

54
00:03:13,150 --> 00:03:20,000
as well from the National
Autonomous University of Mexico.

55
00:03:20,000 --> 00:03:22,510
We have Maura Pompa Mansilla.

56
00:03:22,510 --> 00:03:28,060
And from University of Sao
Paulo we have Seiji Isotani.

57
00:03:28,060 --> 00:03:34,600
So they will be responding
to what they hear also

58
00:03:34,600 --> 00:03:36,290
from Hae Won and Sharifa.

59
00:03:36,290 --> 00:03:40,730
So this is hopefully going
to be an engaging dialogue

60
00:03:40,730 --> 00:03:46,198
and gives you all a chance
to really engage and react

61
00:03:46,198 --> 00:03:47,990
and respond and share
what's happening also

62
00:03:47,990 --> 00:03:50,000
at your universities.

63
00:03:50,000 --> 00:03:51,620
This is being
recorded so that we

64
00:03:51,620 --> 00:03:54,740
can share it with our members.

65
00:03:54,740 --> 00:03:57,830
But wanted to lay
that groundwork

66
00:03:57,830 --> 00:03:59,440
and excited to have you here.

67
00:03:59,440 --> 00:04:05,990
So just a little bit of
background first on Hae Won.

68
00:04:05,990 --> 00:04:09,620
Hae Won is a research scientist
with the Personal Robotics

69
00:04:09,620 --> 00:04:10,760
Group at MIT.

70
00:04:10,760 --> 00:04:14,510
Her PhD is from Georgia
Tech, and she actually

71
00:04:14,510 --> 00:04:17,300
is the co-founder of
XAI Robotics, which

72
00:04:17,300 --> 00:04:21,060
is a spin off from some of the
work she did at Georgia Tech.

73
00:04:21,060 --> 00:04:23,750
And she really
focuses on developing

74
00:04:23,750 --> 00:04:27,170
interactive social machines
that really personalize

75
00:04:27,170 --> 00:04:32,510
for their users over long term
interactions and keeping in mind

76
00:04:32,510 --> 00:04:36,380
the unique needs and
goals of a learner.

77
00:04:36,380 --> 00:04:38,750
And her research
agenda is centered

78
00:04:38,750 --> 00:04:42,110
around developing those
personal robots and agents that

79
00:04:42,110 --> 00:04:45,860
are capable of transforming
learning from one interaction

80
00:04:45,860 --> 00:04:50,300
to another and utilizing
collective knowledge

81
00:04:50,300 --> 00:04:54,020
and sensing across intelligent
and intuitive interfaces

82
00:04:54,020 --> 00:04:56,720
that are embedded
in everyday objects.

83
00:04:56,720 --> 00:05:00,710
And then we'll hear
from her colleague

84
00:05:00,710 --> 00:05:05,390
as well, Dr. Alghowinem, who's
also a research scientist

85
00:05:05,390 --> 00:05:07,670
with the personal robotics lab.

86
00:05:07,670 --> 00:05:11,000
And her PhD is in multimodal AI.

87
00:05:11,000 --> 00:05:14,210
And that's from the Australian
National University.

88
00:05:14,210 --> 00:05:19,880
And with this expertise that
she has in multimodal AI,

89
00:05:19,880 --> 00:05:25,440
she really focuses on
modeling human behavior,

90
00:05:25,440 --> 00:05:27,620
using these frameworks.

91
00:05:27,620 --> 00:05:32,360
And issues like mood and
behavior recognition,

92
00:05:32,360 --> 00:05:35,450
using speech and gaze
and body movements

93
00:05:35,450 --> 00:05:40,340
to detect conditions like
depression or suicide

94
00:05:40,340 --> 00:05:42,380
and deception among learners.

95
00:05:42,380 --> 00:05:45,790
So those types of really
maybe more subtle--

96
00:05:45,790 --> 00:05:48,860
things we think of
as more subtle cues.

97
00:05:48,860 --> 00:05:50,830
And she's also an
Associate Director

98
00:05:50,830 --> 00:05:54,950
of Postgraduate Programs at
Prince Sultan University.

99
00:05:54,950 --> 00:05:56,830
So she has eight
years of experience

100
00:05:56,830 --> 00:06:03,310
teaching computer science topics
and working with students,

101
00:06:03,310 --> 00:06:05,410
graduate students as well.

102
00:06:05,410 --> 00:06:08,140
So excited to have
them both here.

103
00:06:08,140 --> 00:06:11,050
Before we turn it
over to them, they've

104
00:06:11,050 --> 00:06:12,880
prepared a little
bit of a presentation

105
00:06:12,880 --> 00:06:15,160
to lay the groundwork
and to build

106
00:06:15,160 --> 00:06:18,400
on what Cynthia,
Professor Breazeal

107
00:06:18,400 --> 00:06:20,690
had done in our last session.

108
00:06:20,690 --> 00:06:28,150
I would be remiss if I didn't
say a few words about Bill

109
00:06:28,150 --> 00:06:33,160
Bonvillian, who is
really the person who's

110
00:06:33,160 --> 00:06:38,360
been the driving force behind
this AI Innovation Series.

111
00:06:38,360 --> 00:06:41,980
He leads a lot of our
thinking series here at J-WEL

112
00:06:41,980 --> 00:06:44,240
and has been doing a lot
of work with MIT Open

113
00:06:44,240 --> 00:06:45,780
Learning over the years.

114
00:06:45,780 --> 00:06:49,880
He's actually been teaching
at the intersection

115
00:06:49,880 --> 00:06:55,310
of policy and technology at
MIT for quite a long time.

116
00:06:55,310 --> 00:06:57,650
And he has a unique perspective,
because he's actually

117
00:06:57,650 --> 00:07:01,080
the director of MIT's
Washington, DC office.

118
00:07:01,080 --> 00:07:07,220
So advocating for MIT and in
the science and technology space

119
00:07:07,220 --> 00:07:09,620
and with our government
and policymakers.

120
00:07:09,620 --> 00:07:11,540
Because he spent a
lot of his career

121
00:07:11,540 --> 00:07:15,470
in Congress working, supporting
our congressional leaders

122
00:07:15,470 --> 00:07:18,710
to make sure they were
informed and prepared

123
00:07:18,710 --> 00:07:22,290
for the types of transformation
that was happening.

124
00:07:22,290 --> 00:07:25,350
So with that said, Bill
would like to say a few--

125
00:07:25,350 --> 00:07:28,800
I would like Bill to say a
few words to set the stage,

126
00:07:28,800 --> 00:07:32,148
and then we can turn it
over to Hae Won and Sharifa.

127
00:07:32,148 --> 00:07:33,190
WILLIAM BONVILLIAN: Sure.

128
00:07:33,190 --> 00:07:33,880
Thanks.

129
00:07:33,880 --> 00:07:35,530
Thanks, Megan.

130
00:07:35,530 --> 00:07:38,590
Thanks for that good summary.

131
00:07:38,590 --> 00:07:41,890
I'll just say a couple of things
for about a minute as kind

132
00:07:41,890 --> 00:07:48,520
of backgrounders for
all of us to think from.

133
00:07:48,520 --> 00:07:52,240
I mean, Cynthia's, obviously her
big message was how personalized

134
00:07:52,240 --> 00:07:57,100
education is, how social
it is, and how much

135
00:07:57,100 --> 00:08:00,040
learning is really
social learning

136
00:08:00,040 --> 00:08:02,920
in the context of other people.

137
00:08:02,920 --> 00:08:07,090
And if AI is going to be
successful and add a new tool

138
00:08:07,090 --> 00:08:11,020
set, the social and
emotional features of AI

139
00:08:11,020 --> 00:08:14,020
really need to be made optimal.

140
00:08:14,020 --> 00:08:17,740
So that includes emotional
engagement with learners

141
00:08:17,740 --> 00:08:23,020
and that includes treating AI
really as a collaborating ally.

142
00:08:23,020 --> 00:08:26,980
And AI, as Megan
noted, will need

143
00:08:26,980 --> 00:08:31,210
to be designed to perceive
interpersonal cues,

144
00:08:31,210 --> 00:08:35,950
to develop a form of rapport
and social inferences

145
00:08:35,950 --> 00:08:39,820
and pick up emotional cues
that reveal the kind of learner

146
00:08:39,820 --> 00:08:42,070
state of mind.

147
00:08:42,070 --> 00:08:46,480
So that means it's going to have
to be good at social observation

148
00:08:46,480 --> 00:08:52,000
and picking up guidance
from that observation.

149
00:08:52,000 --> 00:08:59,380
So social AI powered
robotics, which Sharif and Hae

150
00:08:59,380 --> 00:09:01,390
Won have both worked
on in turn can

151
00:09:01,390 --> 00:09:06,160
be used really as kind of
peer learning companions,

152
00:09:06,160 --> 00:09:07,960
if they're optimized.

153
00:09:07,960 --> 00:09:10,410
But she also emphasized
that because learning

154
00:09:10,410 --> 00:09:15,240
is a social activity for people,
teacher engagement is critical.

155
00:09:15,240 --> 00:09:17,940
AI is in no way a
real replacement

156
00:09:17,940 --> 00:09:21,270
for that social engagement
and mentoring from teachers.

157
00:09:21,270 --> 00:09:23,250
And then the other
big theme of her talk

158
00:09:23,250 --> 00:09:27,630
was really around AI
literacy and fluency.

159
00:09:27,630 --> 00:09:30,120
Educating students
in the first case

160
00:09:30,120 --> 00:09:32,880
as how to be literate
so they understand

161
00:09:32,880 --> 00:09:36,010
the new technology and
its potential uses,

162
00:09:36,010 --> 00:09:39,150
but also its limits and the
problems that it can cause.

163
00:09:39,150 --> 00:09:41,370
So to be good users,
they've got to understand

164
00:09:41,370 --> 00:09:45,090
both sides of this technology.

165
00:09:45,090 --> 00:09:46,950
And she argued
that we really need

166
00:09:46,950 --> 00:09:50,130
to strive towards AI fluency,
where students really

167
00:09:50,130 --> 00:09:52,110
learn how to make AI.

168
00:09:52,110 --> 00:09:55,030
And this is, essentially
in education terms,

169
00:09:55,030 --> 00:09:57,750
this is a
constructivist approach

170
00:09:57,750 --> 00:10:03,400
advanced by educators like
at MIT Seymour Papert.

171
00:10:03,400 --> 00:10:05,590
It's really learning by doing.

172
00:10:05,590 --> 00:10:09,240
And she discussed how we
begin to introducing this

173
00:10:09,240 --> 00:10:13,530
and discussed a lot of
tools that MIT has developed

174
00:10:13,530 --> 00:10:16,350
for both students and
teachers to help bring AI

175
00:10:16,350 --> 00:10:17,590
into classrooms.

176
00:10:17,590 --> 00:10:19,590
So with that, let me
turn it back to Megan,

177
00:10:19,590 --> 00:10:21,540
and we'll go to the next stage.

178
00:10:21,540 --> 00:10:22,740
MEGAN MITCHELL: Absolutely.

179
00:10:22,740 --> 00:10:25,020
I'm just here to
turn it now, we're

180
00:10:25,020 --> 00:10:27,450
going to hear from
our two colleagues

181
00:10:27,450 --> 00:10:31,330
from the Personal Robotics
Lab at the Media Lab.

182
00:10:31,330 --> 00:10:35,310
So I know Hae Won has
some slides to share,

183
00:10:35,310 --> 00:10:42,488
and we will jump into
their presentation, please.

184
00:10:42,488 --> 00:10:44,030
HAE WON PARK: Yeah,
thank you, Megan.

185
00:10:44,030 --> 00:10:47,290
Thank you, Bill,
for the staging.

186
00:10:47,290 --> 00:10:51,650
Yes, I have my background
Media Lab space.

187
00:10:51,650 --> 00:10:56,650
This is where people
hang out and we really

188
00:10:56,650 --> 00:11:00,100
emphasize the power of
collaboration and just people

189
00:11:00,100 --> 00:11:02,950
with collaborations
being birthed from just

190
00:11:02,950 --> 00:11:04,340
people mingling together.

191
00:11:04,340 --> 00:11:07,210
So here's our fun space.

192
00:11:07,210 --> 00:11:09,140
Yeah, great to be here.

193
00:11:09,140 --> 00:11:10,960
Thanks for the invitation.

194
00:11:10,960 --> 00:11:14,270
I'm also a co-director
of MIT RAISE,

195
00:11:14,270 --> 00:11:17,180
which is Responsible AI
for Social Empowerment.

196
00:11:17,180 --> 00:11:20,560
And I particularly
lead the AI augmented

197
00:11:20,560 --> 00:11:24,070
learning component
of MIT RAISE, which

198
00:11:24,070 --> 00:11:30,680
is about how to integrate AI in
education to empower learners.

199
00:11:30,680 --> 00:11:36,940
So we have also Hal and Eric,
who are also other co-directors,

200
00:11:36,940 --> 00:11:41,380
Cynthia being our overarching
director of MIT RAISE.

201
00:11:41,380 --> 00:11:45,260
We also have [INAUDIBLE]
how to bring AI education,

202
00:11:45,260 --> 00:11:49,520
teach about AI, and how
to empower teachers,

203
00:11:49,520 --> 00:11:51,450
professional development.

204
00:11:51,450 --> 00:11:54,740
And AI augmented
learning focuses

205
00:11:54,740 --> 00:11:57,450
on how AI can support learning.

206
00:11:57,450 --> 00:11:59,790
So how can we use
AI for education?

207
00:11:59,790 --> 00:12:04,520
So that's the big picture of MIT
RAISE I'm going to point out.

208
00:12:04,520 --> 00:12:07,410
And also apologize for my voice.

209
00:12:07,410 --> 00:12:10,040
I'm just recovering
from COVID, and I still

210
00:12:10,040 --> 00:12:11,680
have a stuffed voice.

211
00:12:11,680 --> 00:12:15,170
So you might hear my
voice breaking at times.

212
00:12:15,170 --> 00:12:19,130
That being said, I
bring you a presentation

213
00:12:19,130 --> 00:12:22,100
that goes a little bit more
deeper than what probably

214
00:12:22,100 --> 00:12:25,070
Cynthia shared, like
our overarching vision,

215
00:12:25,070 --> 00:12:26,840
last Tuesday.

216
00:12:26,840 --> 00:12:30,410
Going a little bit deeper
into basically the components

217
00:12:30,410 --> 00:12:31,800
that Bill pointed out.

218
00:12:31,800 --> 00:12:35,630
That was just a
fascinating summary, Bill.

219
00:12:35,630 --> 00:12:37,620
So yeah, let's get right to it.

220
00:12:37,620 --> 00:12:42,650
So we've been working on this
personalized AI companion

221
00:12:42,650 --> 00:12:49,320
agent for over 10 years,
over 15 years, I would say.

222
00:12:49,320 --> 00:12:58,100
And where we really focus on is
how can we bring the powerful AI

223
00:12:58,100 --> 00:13:03,260
tools, but not as just the
tool, but as a companion that

224
00:13:03,260 --> 00:13:08,240
can really integrated
in your everyday life

225
00:13:08,240 --> 00:13:12,890
and be that companion that
personalizes to you over time,

226
00:13:12,890 --> 00:13:16,310
understands how this student
or what kind of a learner

227
00:13:16,310 --> 00:13:19,530
this student is, what
motivates the student.

228
00:13:19,530 --> 00:13:25,370
And really leverage those social
power to support the learner's

229
00:13:25,370 --> 00:13:28,130
motivation to learn,
to really build

230
00:13:28,130 --> 00:13:31,250
a foundation over a long
term, to really set up

231
00:13:31,250 --> 00:13:33,870
for student success over time.

232
00:13:33,870 --> 00:13:39,470
So we are not particularly
too interested

233
00:13:39,470 --> 00:13:42,650
in trying to instill
the actual knowledge

234
00:13:42,650 --> 00:13:45,920
part of the curriculum
versus we've

235
00:13:45,920 --> 00:13:48,980
been working a lot on
how to support students

236
00:13:48,980 --> 00:13:52,920
to have a mindset,
have an attitude,

237
00:13:52,920 --> 00:13:56,690
have a motivation so that
they have this intrinsic power

238
00:13:56,690 --> 00:13:59,820
to support and propel
their learning forward.

239
00:13:59,820 --> 00:14:04,400
Because if the students actually
have those intrinsic motivation

240
00:14:04,400 --> 00:14:08,430
to learn, you don't
really have to instill,

241
00:14:08,430 --> 00:14:09,960
embed knowledge in them.

242
00:14:09,960 --> 00:14:13,740
They will self learn,
self teach them,

243
00:14:13,740 --> 00:14:16,210
or find resources to learn.

244
00:14:16,210 --> 00:14:18,100
So that has been our focus.

245
00:14:18,100 --> 00:14:21,060
And so that being
said, we will learn

246
00:14:21,060 --> 00:14:23,940
more how social
AI agents support

247
00:14:23,940 --> 00:14:27,180
those kind of
development in especially

248
00:14:27,180 --> 00:14:29,290
young early childhood.

249
00:14:29,290 --> 00:14:33,150
Because we think the school
readiness and diversity

250
00:14:33,150 --> 00:14:34,830
in classrooms needs
to be supported

251
00:14:34,830 --> 00:14:36,270
in the right direction.

252
00:14:36,270 --> 00:14:43,130
And worldwide now,
every learner when

253
00:14:43,130 --> 00:14:46,410
they enter the public
school system in the US,

254
00:14:46,410 --> 00:14:49,130
depending on the state
and district that

255
00:14:49,130 --> 00:14:53,630
happens in preschool,
sometimes in kindergarten,

256
00:14:53,630 --> 00:14:57,000
some districts might not
even support kindergarten.

257
00:14:57,000 --> 00:15:01,190
The public school system
starts at first grade.

258
00:15:01,190 --> 00:15:04,410
But when they first enter
the public school system,

259
00:15:04,410 --> 00:15:09,620
they are a unique distribution
of their cultural and linguistic

260
00:15:09,620 --> 00:15:12,180
background.

261
00:15:12,180 --> 00:15:15,410
They might not
come from a family

262
00:15:15,410 --> 00:15:19,040
where English is their first
language, which then that might

263
00:15:19,040 --> 00:15:23,480
actually be a factor for them
to have a harder time being

264
00:15:23,480 --> 00:15:27,470
integrated into the classroom
learning and interacting

265
00:15:27,470 --> 00:15:28,640
with others.

266
00:15:28,640 --> 00:15:30,780
And because at that
early of an age,

267
00:15:30,780 --> 00:15:33,170
interaction is key to learn.

268
00:15:33,170 --> 00:15:37,490
No one starts to learn
and read and write

269
00:15:37,490 --> 00:15:39,680
by just studying the book.

270
00:15:39,680 --> 00:15:43,950
They learn by social interacting
with the teachers, the peers,

271
00:15:43,950 --> 00:15:46,620
people around them.

272
00:15:46,620 --> 00:15:49,660
So that's why what we are
really highlighting here,

273
00:15:49,660 --> 00:15:52,210
learning is not just about
cognitive engagement,

274
00:15:52,210 --> 00:15:56,040
it's about social and affective
and relational engagement.

275
00:15:56,040 --> 00:15:59,310
I'm sure you all had experience
where you studied harder

276
00:15:59,310 --> 00:16:02,280
for a subject when you
liked the teacher more.

277
00:16:02,280 --> 00:16:08,100
So we're trying to really
highlight these aspects of what

278
00:16:08,100 --> 00:16:11,000
we can bring with social AI.

279
00:16:11,000 --> 00:16:16,120
So before a lot of the
intelligent tutoring system,

280
00:16:16,120 --> 00:16:19,620
they primarily focused
on the cognitive part

281
00:16:19,620 --> 00:16:21,130
of students learning.

282
00:16:21,130 --> 00:16:24,820
But learning is not just
about cognitive engagement.

283
00:16:24,820 --> 00:16:27,060
Let's bring in social,
affective, and relational

284
00:16:27,060 --> 00:16:30,340
aspects to learning as well.

285
00:16:30,340 --> 00:16:36,370
So the video is essential to
any kind of presentation we do,

286
00:16:36,370 --> 00:16:41,890
because we actually install
a physical embodied agent

287
00:16:41,890 --> 00:16:44,200
in the scene.

288
00:16:44,200 --> 00:16:46,960
So take a look at
how the children

289
00:16:46,960 --> 00:16:51,020
is engaging with this physical
embodied social agents.

290
00:16:51,020 --> 00:16:52,469
[VIDEO PLAYBACK]

291
00:16:52,469 --> 00:16:54,401
[MUSIC PLAYING]

292
00:16:54,401 --> 00:17:05,530

293
00:17:05,530 --> 00:17:07,510
- Nice to meet you.

294
00:17:07,510 --> 00:17:10,630
Want to play a story game?

295
00:17:10,630 --> 00:17:12,220
- Yes.

296
00:17:12,220 --> 00:17:15,819
- Once upon a time, a
little penguin named George

297
00:17:15,819 --> 00:17:17,980
lived in the South Pole.

298
00:17:17,980 --> 00:17:18,819
Go ahead.

299
00:17:18,819 --> 00:17:20,680
You tell a story.

300
00:17:20,680 --> 00:17:24,400
- I want it to be
about a snowman.

301
00:17:24,400 --> 00:17:25,720
- Ooh.

302
00:17:25,720 --> 00:17:30,230
- Once upon a time,
there was a snowman.

303
00:17:30,230 --> 00:17:32,000
And he was alive.

304
00:17:32,000 --> 00:17:33,740
- I'm a baby dragon.

305
00:17:33,740 --> 00:17:35,814
See my wings?

306
00:17:35,814 --> 00:17:37,140
- Yeah.

307
00:17:37,140 --> 00:17:39,870
- Will you play with me again?

308
00:17:39,870 --> 00:17:40,720
- Yes.

309
00:17:40,720 --> 00:17:42,664
Yeah.

310
00:17:42,664 --> 00:17:44,620
- Ooh, hi.

311
00:17:44,620 --> 00:17:46,210
- Hi.

312
00:17:46,210 --> 00:17:47,560
- Remember me?

313
00:17:47,560 --> 00:17:49,160
I'm Green.

314
00:17:49,160 --> 00:17:50,160
- I remember.

315
00:17:50,160 --> 00:17:58,453

316
00:17:58,453 --> 00:18:01,400
- Is it your favorite game too?

317
00:18:01,400 --> 00:18:01,900
- Yeah.

318
00:18:01,900 --> 00:18:08,150

319
00:18:08,150 --> 00:18:08,650
- Your turn.

320
00:18:08,650 --> 00:18:15,790

321
00:18:15,790 --> 00:18:18,810
- The penguins took
back the hat and put it

322
00:18:18,810 --> 00:18:20,121
on the snowman's head.

323
00:18:20,121 --> 00:18:23,070

324
00:18:23,070 --> 00:18:24,840
The end.

325
00:18:24,840 --> 00:18:25,670
Your turn.

326
00:18:25,670 --> 00:18:28,870

327
00:18:28,870 --> 00:18:34,006
- I'm gonna tell a
story about a snowman.

328
00:18:34,006 --> 00:18:34,506
- Ooh.

329
00:18:34,506 --> 00:18:37,500

330
00:18:37,500 --> 00:18:42,690
- Once upon a time there was--

331
00:18:42,690 --> 00:18:45,065
there were two penguin sisters.

332
00:18:45,065 --> 00:18:47,639

333
00:18:47,639 --> 00:18:48,842
[END PLAYBACK]

334
00:18:48,842 --> 00:18:49,550
HAE WON PARK: OK.

335
00:18:49,550 --> 00:18:53,505
So I'm not sure if you
noticed, but this work

336
00:18:53,505 --> 00:18:56,020
in the video, the particular
video I showed you,

337
00:18:56,020 --> 00:18:58,640
was done when Frozen
was a bit hit.

338
00:18:58,640 --> 00:19:01,240
So you could notice
actually children

339
00:19:01,240 --> 00:19:04,300
telling a story about two
sisters, penguin with two

340
00:19:04,300 --> 00:19:05,750
sisters, [INAUDIBLE] snowman.

341
00:19:05,750 --> 00:19:09,700
So the reason I'm
bringing up this old work

342
00:19:09,700 --> 00:19:13,900
is because we built a
lot of foundational work

343
00:19:13,900 --> 00:19:17,510
in our early years of
this investigation.

344
00:19:17,510 --> 00:19:20,230
And what you just
saw in the video,

345
00:19:20,230 --> 00:19:23,650
I think if you focused on
how the children is treating

346
00:19:23,650 --> 00:19:26,650
the robot, you could also see
their development of theory

347
00:19:26,650 --> 00:19:28,240
of mind.

348
00:19:28,240 --> 00:19:31,030
Placing the tablet
or reorienting it

349
00:19:31,030 --> 00:19:34,340
so that it's on the correct
orientation for the robot.

350
00:19:34,340 --> 00:19:36,790
And then later on, actually
being on the same side

351
00:19:36,790 --> 00:19:39,950
with the robot and
sharing these experiences.

352
00:19:39,950 --> 00:19:42,920
So why do they
engage in such a way?

353
00:19:42,920 --> 00:19:46,250
Because this is also
not an observation

354
00:19:46,250 --> 00:19:50,000
we can take when
children are just

355
00:19:50,000 --> 00:19:54,570
interacting with a tablet
or a smartphone or a TV.

356
00:19:54,570 --> 00:19:56,940
They're actually interacting
like a social being.

357
00:19:56,940 --> 00:19:59,030
And what's actually
happening inside

358
00:19:59,030 --> 00:20:04,280
them is there are papers that
actually saw why people engage

359
00:20:04,280 --> 00:20:08,510
with social robots, these
physical embodied agents,

360
00:20:08,510 --> 00:20:11,390
in such a different
way than they interact

361
00:20:11,390 --> 00:20:12,630
with other technologies.

362
00:20:12,630 --> 00:20:16,010
And they found out
that the actual brain

363
00:20:16,010 --> 00:20:17,900
area that lights up
when people interact

364
00:20:17,900 --> 00:20:20,600
with a social embodied
agent, they're

365
00:20:20,600 --> 00:20:22,820
the same regions in
the brain light up

366
00:20:22,820 --> 00:20:26,630
when they are socially
engaged with another agent

367
00:20:26,630 --> 00:20:28,040
or human being.

368
00:20:28,040 --> 00:20:33,200
So we are just leveraging
this kind of impact

369
00:20:33,200 --> 00:20:34,760
the social agent have.

370
00:20:34,760 --> 00:20:37,640
Because we also
get into, oh, isn't

371
00:20:37,640 --> 00:20:42,060
installing robots expensive
in classrooms and in homes?

372
00:20:42,060 --> 00:20:44,890
So can there be other more
scalable and less expensive

373
00:20:44,890 --> 00:20:45,390
ways?

374
00:20:45,390 --> 00:20:48,170
So we will actually get
to that discussion later.

375
00:20:48,170 --> 00:20:50,930
But that's why we
are emphasizing

376
00:20:50,930 --> 00:20:53,900
these physical embodied
agents, because we actually

377
00:20:53,900 --> 00:20:55,790
see in the field
of difference it

378
00:20:55,790 --> 00:20:58,490
makes in terms of
students' engagement.

379
00:20:58,490 --> 00:21:01,310
That also leads to
the learning outcome.

380
00:21:01,310 --> 00:21:04,590
So yeah, so in our
early investigation,

381
00:21:04,590 --> 00:21:07,910
we focused quite a bit on
social emotive perception

382
00:21:07,910 --> 00:21:09,240
and expression.

383
00:21:09,240 --> 00:21:14,340
So recognizing how the
children is engaging,

384
00:21:14,340 --> 00:21:15,800
what are their
interpersonal cues

385
00:21:15,800 --> 00:21:18,560
that they display to the
robot, because then you have

386
00:21:18,560 --> 00:21:21,000
to react to it accordingly.

387
00:21:21,000 --> 00:21:25,040
Also use that information to
personalize the interaction.

388
00:21:25,040 --> 00:21:29,040
The social emotive expression,
as you saw in the video,

389
00:21:29,040 --> 00:21:32,540
we also put a lot of
effort in designing

390
00:21:32,540 --> 00:21:35,150
the cues that is
displayed by the agent,

391
00:21:35,150 --> 00:21:38,870
because it needs to be
transparent and interpretable

392
00:21:38,870 --> 00:21:41,310
by the user interacting
with the agent.

393
00:21:41,310 --> 00:21:48,420
And also that becomes the factor
for engagement for the user.

394
00:21:48,420 --> 00:21:52,990
And then combining those
understanding and the expressive

395
00:21:52,990 --> 00:21:57,010
cues of the agent, we leveraged
them to actually learn

396
00:21:57,010 --> 00:21:58,810
during the interaction.

397
00:21:58,810 --> 00:22:03,140
Well, what kind of a learner
is this particular student?

398
00:22:03,140 --> 00:22:06,580
Do they welcome
challenges or do they

399
00:22:06,580 --> 00:22:10,810
like more hand-holding
and support over time

400
00:22:10,810 --> 00:22:14,510
propelling them to be a more
challenged, focused learner?

401
00:22:14,510 --> 00:22:18,070
And in all of this
picture, we really

402
00:22:18,070 --> 00:22:20,860
have this long term
relationship building.

403
00:22:20,860 --> 00:22:24,790
So we've seen in the
field that students

404
00:22:24,790 --> 00:22:28,000
build a rapport and emotional
bond with the agent.

405
00:22:28,000 --> 00:22:31,130
And how do we actually
leverage an engineer for that

406
00:22:31,130 --> 00:22:33,730
so that it can actually
propel the learning forward?

407
00:22:33,730 --> 00:22:36,050
And also in a ethical way.

408
00:22:36,050 --> 00:22:40,310
So a lot of great discussion
points that we can get into.

409
00:22:40,310 --> 00:22:44,110
And these are all little
projects and capabilities

410
00:22:44,110 --> 00:22:45,670
that we did.

411
00:22:45,670 --> 00:22:47,630
Not to overwhelm
you with a text,

412
00:22:47,630 --> 00:22:53,040
but building a social agent is
not just about machine learning.

413
00:22:53,040 --> 00:22:57,060
It has to also combine
human psychology education.

414
00:22:57,060 --> 00:22:59,970
So we had a lot of
collaborators along the way,

415
00:22:59,970 --> 00:23:02,810
a lot of sponsors,
including J-WEL,

416
00:23:02,810 --> 00:23:05,210
that had been working
with us to really

417
00:23:05,210 --> 00:23:07,380
bring this kind of
technology forward.

418
00:23:07,380 --> 00:23:09,870
So I'll speed up
a little bit here.

419
00:23:09,870 --> 00:23:13,460
So this is about learning how
the robot should be expressing

420
00:23:13,460 --> 00:23:16,580
and displaying the right
sentiment as it interacts

421
00:23:16,580 --> 00:23:19,400
with the students.

422
00:23:19,400 --> 00:23:21,530
We also tried to
understand, because we

423
00:23:21,530 --> 00:23:26,250
are working with young learners,
like five and six-year-olds.

424
00:23:26,250 --> 00:23:29,990
And we noticed that their
encoding and decoding

425
00:23:29,990 --> 00:23:32,510
of social cues is actually
quite different from how

426
00:23:32,510 --> 00:23:34,320
adults encode and decode.

427
00:23:34,320 --> 00:23:36,650
Like Megan right
there, you are nodding.

428
00:23:36,650 --> 00:23:38,670
That's a great
backchanneling cue.

429
00:23:38,670 --> 00:23:42,900
We actually noticed that kids
this young of an age rarely nod.

430
00:23:42,900 --> 00:23:47,360
It's not one of their
encoded cues, a cue

431
00:23:47,360 --> 00:23:50,130
that they use to communicate
that they are engaged.

432
00:23:50,130 --> 00:23:52,370
So we need to utilize
different cues.

433
00:23:52,370 --> 00:23:53,110
So take a look.

434
00:23:53,110 --> 00:23:53,777
[VIDEO PLAYBACK]

435
00:23:53,777 --> 00:23:54,606
- Broke the window.

436
00:23:54,606 --> 00:23:55,540
[LAUGHS]

437
00:23:55,540 --> 00:24:01,060
And then it ran through the
school and peeked inside.

438
00:24:01,060 --> 00:24:03,610
He saw-- they saw--

439
00:24:03,610 --> 00:24:06,140
[CHILDREN CHATTERING]

440
00:24:06,140 --> 00:24:07,030
[END PLAYBACK]

441
00:24:07,030 --> 00:24:08,710
HAE WON PARK: So we
collected this data

442
00:24:08,710 --> 00:24:13,690
to train to
recognize and develop

443
00:24:13,690 --> 00:24:20,500
expressive cues of the robot to
make sure that when the robot is

444
00:24:20,500 --> 00:24:23,710
listening to a children's
story, they can feel the robot

445
00:24:23,710 --> 00:24:27,050
is engaged to their
story and vice versa.

446
00:24:27,050 --> 00:24:28,820
When the robot is
telling a story,

447
00:24:28,820 --> 00:24:32,780
we also want to recognize
the right cues from the kids

448
00:24:32,780 --> 00:24:35,630
to recognize when they
are engaged and disengaged

449
00:24:35,630 --> 00:24:37,660
so that the robot
can also then perform

450
00:24:37,660 --> 00:24:43,990
different actions as reactive to
and responsive to the student's

451
00:24:43,990 --> 00:24:46,570
cues.

452
00:24:46,570 --> 00:24:49,000
So we did a bunch
of studies and we

453
00:24:49,000 --> 00:24:52,150
recognized that we found that
when the robot is actually

454
00:24:52,150 --> 00:24:59,230
able to recognize and encode and
decode the engagement cues, then

455
00:24:59,230 --> 00:25:00,910
they are more engaged
with the robot

456
00:25:00,910 --> 00:25:03,730
who can actually
backchannel accordingly

457
00:25:03,730 --> 00:25:08,470
to when they are talking to
the robot, telling a story.

458
00:25:08,470 --> 00:25:13,010
And then the highlight part of
our research is personalization.

459
00:25:13,010 --> 00:25:16,480
Because no one
learner is the same.

460
00:25:16,480 --> 00:25:19,180
No two learner are the same.

461
00:25:19,180 --> 00:25:23,890
Every student learns and they
get motivated differently.

462
00:25:23,890 --> 00:25:27,850
So how can we develop this
personalization strategies

463
00:25:27,850 --> 00:25:29,440
for robot?

464
00:25:29,440 --> 00:25:32,890
We can, for example, adapt
the robot's personality

465
00:25:32,890 --> 00:25:34,270
to the learner.

466
00:25:34,270 --> 00:25:37,370
We've done a ton of work on
adapting the robot's role.

467
00:25:37,370 --> 00:25:41,090
So should the robot
be always this expert,

468
00:25:41,090 --> 00:25:45,250
like more knowledgeable peer
that always offers information

469
00:25:45,250 --> 00:25:46,670
and knowledge to the student?

470
00:25:46,670 --> 00:25:50,510
Versus can we actually design
a vulnerable robot that

471
00:25:50,510 --> 00:25:53,000
asks for help from the student?

472
00:25:53,000 --> 00:25:58,010
And then can it actually
be a dynamic, adaptive role

473
00:25:58,010 --> 00:26:02,390
that is like a very natural
peer to peer interaction?

474
00:26:02,390 --> 00:26:06,920
You can be more knowledgeable in
one part or less knowledgeable

475
00:26:06,920 --> 00:26:07,710
in the other.

476
00:26:07,710 --> 00:26:10,160
So that really
brings the building

477
00:26:10,160 --> 00:26:13,670
of the reciprocal
interaction and relationship.

478
00:26:13,670 --> 00:26:18,230
And can we actually leverage
that to engage the student

479
00:26:18,230 --> 00:26:22,140
and propel their
learning further?

480
00:26:22,140 --> 00:26:24,930
We had so much fun
doing these projects.

481
00:26:24,930 --> 00:26:30,020
So this was a work that we did
to really adapt the robot's role

482
00:26:30,020 --> 00:26:30,930
to the learner.

483
00:26:30,930 --> 00:26:33,030
So when the student
is struggling,

484
00:26:33,030 --> 00:26:34,650
the robot offers support.

485
00:26:34,650 --> 00:26:37,340
Oh, I think lavender means this.

486
00:26:37,340 --> 00:26:43,160
Versus when the
student has knowledge

487
00:26:43,160 --> 00:26:49,630
that they want to demonstrate,
kids love to teach others.

488
00:26:49,630 --> 00:26:52,450
And then that's a point
where the robot is actually

489
00:26:52,450 --> 00:26:54,620
asking, hey, do you
know what this means?

490
00:26:54,620 --> 00:26:56,120
Or can you help me with this?

491
00:26:56,120 --> 00:26:59,830
And kids just jump in and
they offer their knowledge

492
00:26:59,830 --> 00:27:00,590
to the robot.

493
00:27:00,590 --> 00:27:03,190
And for the robot,
that becomes a moment

494
00:27:03,190 --> 00:27:06,080
to learn what the child
knows and doesn't know.

495
00:27:06,080 --> 00:27:10,780
And also that becomes a factor
for personalizing interaction

496
00:27:10,780 --> 00:27:13,700
and personalizing the learning
for this particular student.

497
00:27:13,700 --> 00:27:15,980
[VIDEO PLAYBACK]

498
00:27:15,980 --> 00:27:18,350
- Lavender?

499
00:27:18,350 --> 00:27:18,980
I don't know.

500
00:27:18,980 --> 00:27:21,820
Lavender.

501
00:27:21,820 --> 00:27:24,180
- What are we trying to find?

502
00:27:24,180 --> 00:27:29,240
- We are trying to find
lavender color stuff.

503
00:27:29,240 --> 00:27:31,700
- OK.

504
00:27:31,700 --> 00:27:34,380
The word was lavender.

505
00:27:34,380 --> 00:27:36,848
- Yes.

506
00:27:36,848 --> 00:27:40,670
And what is lavender, though?

507
00:27:40,670 --> 00:27:42,770
What's lavender?

508
00:27:42,770 --> 00:27:43,645
No, that's blue.

509
00:27:43,645 --> 00:27:46,160

510
00:27:46,160 --> 00:27:47,375
What is lavender?

511
00:27:47,375 --> 00:27:51,440

512
00:27:51,440 --> 00:27:54,051
Girl jumping.

513
00:27:54,051 --> 00:27:55,374
[CHIMING]

514
00:27:55,374 --> 00:27:57,140
[END PLAYBACK]

515
00:27:57,140 --> 00:27:59,930
HAE WON PARK: Just to pause
the video at that point.

516
00:27:59,930 --> 00:28:01,880
So what they are
actually doing is

517
00:28:01,880 --> 00:28:05,400
they are playing a word
vocabulary learning game

518
00:28:05,400 --> 00:28:06,600
like I-spy.

519
00:28:06,600 --> 00:28:08,850
So find something
that is lavender.

520
00:28:08,850 --> 00:28:13,610
So they need to collaborate to
find objects that are lavender.

521
00:28:13,610 --> 00:28:17,790
In the meanwhile, we have
these cameras on the tablet

522
00:28:17,790 --> 00:28:20,850
and on the robot that
monitors the child's

523
00:28:20,850 --> 00:28:24,300
expressive and engagement cues.

524
00:28:24,300 --> 00:28:27,440
And also the robot,
if you noticed,

525
00:28:27,440 --> 00:28:30,390
gazes at the child,
gazes at the tablet.

526
00:28:30,390 --> 00:28:32,700
It's also signaling
its engagement

527
00:28:32,700 --> 00:28:35,873
and also creating opportunities
to observe the child's cues.

528
00:28:35,873 --> 00:28:36,540
[VIDEO PLAYBACK]

529
00:28:36,540 --> 00:28:40,000
- I'm sure you will
do better next time.

530
00:28:40,000 --> 00:28:43,620
I believe in you.

531
00:28:43,620 --> 00:28:45,900
- Robot's turn.

532
00:28:45,900 --> 00:28:49,080
- Time to perform.

533
00:28:49,080 --> 00:28:51,700
Lavender is purple.

534
00:28:51,700 --> 00:28:53,580
- Yes.

535
00:28:53,580 --> 00:28:54,497
Just like my--

536
00:28:54,497 --> 00:28:55,080
[END PLAYBACK]

537
00:28:55,080 --> 00:28:56,288
HAE WON PARK: So there we go.

538
00:28:56,288 --> 00:29:00,090
So the robot decided that the
child would need a demonstration

539
00:29:00,090 --> 00:29:01,990
and offers its knowledge.

540
00:29:01,990 --> 00:29:04,320
And when the child
is confident, they

541
00:29:04,320 --> 00:29:07,350
know what lavender means and the
robot will actually back off.

542
00:29:07,350 --> 00:29:10,510
And when it's turn, it's
going to ask for help.

543
00:29:10,510 --> 00:29:11,970
And that gives
another opportunity

544
00:29:11,970 --> 00:29:14,110
for the child to
demonstrate their knowledge,

545
00:29:14,110 --> 00:29:15,660
practice their knowledge.

546
00:29:15,660 --> 00:29:19,500
And over time, we are
making this reinforcement,

547
00:29:19,500 --> 00:29:22,620
leveraging reinforcement
learning, which basically means

548
00:29:22,620 --> 00:29:26,970
the robot gets rewarded for
how the child is engaging well

549
00:29:26,970 --> 00:29:31,200
and propelling their learning
forward, thereby understanding

550
00:29:31,200 --> 00:29:35,800
that, OK, this child now is like
a learner that loves challenges.

551
00:29:35,800 --> 00:29:37,810
So even if the
child is struggling,

552
00:29:37,810 --> 00:29:41,490
let me back off, because this
worked better for this child.

553
00:29:41,490 --> 00:29:43,950
Versus if the child is--

554
00:29:43,950 --> 00:29:46,950
they need like a little bit more
hand-holding at the beginning,

555
00:29:46,950 --> 00:29:50,190
then it learns that, oh,
this child does better when

556
00:29:50,190 --> 00:29:51,940
I actually offer more support.

557
00:29:51,940 --> 00:29:54,570
So let me give more
support to the child.

558
00:29:54,570 --> 00:29:58,590
So thereby over time,
the agent can actually

559
00:29:58,590 --> 00:30:02,580
personalize its role and
actually adapt its behavior

560
00:30:02,580 --> 00:30:05,860
in real time along the way.

561
00:30:05,860 --> 00:30:07,780
So graphs.

562
00:30:07,780 --> 00:30:09,780
But this is basically
showing how powerful

563
00:30:09,780 --> 00:30:14,520
the personalized and adaptive
learning agent supports

564
00:30:14,520 --> 00:30:16,170
children's learning.

565
00:30:16,170 --> 00:30:19,760
So it shows that over time,
and especially over long term,

566
00:30:19,760 --> 00:30:23,480
that we see the
personalized robot offered

567
00:30:23,480 --> 00:30:26,390
much better support
in terms of propelling

568
00:30:26,390 --> 00:30:29,220
the learning forward and
also in terms of engagement.

569
00:30:29,220 --> 00:30:32,660
We also went back after
three weeks to a month

570
00:30:32,660 --> 00:30:35,990
to do a post-test on if
the learning actually

571
00:30:35,990 --> 00:30:37,190
stuck with them.

572
00:30:37,190 --> 00:30:40,550
We actually found that with
the personalized robot,

573
00:30:40,550 --> 00:30:43,280
more knowledge actually
stuck with the child

574
00:30:43,280 --> 00:30:45,610
even after the
interaction had ended.

575
00:30:45,610 --> 00:30:48,940

576
00:30:48,940 --> 00:30:51,970
So we've been applying
this kind of mechanism

577
00:30:51,970 --> 00:30:56,170
for adapting robot's role
in vocabulary learning

578
00:30:56,170 --> 00:31:00,590
and syntax learning.

579
00:31:00,590 --> 00:31:03,650
A lot of literacy and
language education,

580
00:31:03,650 --> 00:31:07,730
because that is a foundation to
any kind of learning, including

581
00:31:07,730 --> 00:31:09,700
STEM education along the way.

582
00:31:09,700 --> 00:31:17,420
We also applied it to how we can
foster creativity and curiosity

583
00:31:17,420 --> 00:31:18,980
in children.

584
00:31:18,980 --> 00:31:22,760
So this all results points to
how personalization actually

585
00:31:22,760 --> 00:31:23,720
works.

586
00:31:23,720 --> 00:31:25,550
This is one of my
favorite projects

587
00:31:25,550 --> 00:31:28,820
where we worked on not
actually teaching a vocabulary

588
00:31:28,820 --> 00:31:32,870
or anything like that, but
really growing their growth

589
00:31:32,870 --> 00:31:35,840
mindset, which means
growth mindset means

590
00:31:35,840 --> 00:31:38,390
it's a belief of your
ability that when

591
00:31:38,390 --> 00:31:43,190
you put in enough effort and
try hard, you grow over time.

592
00:31:43,190 --> 00:31:47,360
Versus a fixed mindset is,
oh, I'm born with my skills.

593
00:31:47,360 --> 00:31:48,990
My parents aren't smart.

594
00:31:48,990 --> 00:31:51,860
Probably I'm not smart.

595
00:31:51,860 --> 00:31:58,560
So how can we actually intervene
with those internal beliefs

596
00:31:58,560 --> 00:32:00,340
about their own ability?

597
00:32:00,340 --> 00:32:04,270
If the robot is actually showing
these beliefs about itself,

598
00:32:04,270 --> 00:32:08,850
can it actually transfer and
affect the student's belief

599
00:32:08,850 --> 00:32:10,120
about themselves?

600
00:32:10,120 --> 00:32:13,350
And we actually saw
a very strong impact

601
00:32:13,350 --> 00:32:15,870
of this interaction.

602
00:32:15,870 --> 00:32:19,950
And this work was
really well recognized

603
00:32:19,950 --> 00:32:23,640
across our community,
education community.

604
00:32:23,640 --> 00:32:28,890
Because before the growth
mindset was studied mostly

605
00:32:28,890 --> 00:32:30,850
between parent and
child relationship,

606
00:32:30,850 --> 00:32:34,500
teacher child relationship, but
not a peer to peer relationship,

607
00:32:34,500 --> 00:32:37,080
because you can't have
a five-year-old actor

608
00:32:37,080 --> 00:32:38,640
act out to another
five-year-old.

609
00:32:38,640 --> 00:32:41,510
But we can because
we have a robot.

610
00:32:41,510 --> 00:32:46,170
So the robot is demonstrating
how it deals-- sorry.

611
00:32:46,170 --> 00:32:49,070
How it deals with
failure and success.

612
00:32:49,070 --> 00:32:54,110
And when it's failing,
it's not saying I'm dumb

613
00:32:54,110 --> 00:32:56,570
or I'm not good at this.

614
00:32:56,570 --> 00:32:59,630
It thinks of it as a
learning opportunity.

615
00:32:59,630 --> 00:33:01,820
Same as when it succeeds.

616
00:33:01,820 --> 00:33:03,660
It doesn't say,
oh, I'm so smart.

617
00:33:03,660 --> 00:33:06,870
It says, I put in a lot
of thinking into this.

618
00:33:06,870 --> 00:33:08,473
And this time, I succeeded.

619
00:33:08,473 --> 00:33:09,140
[VIDEO PLAYBACK]

620
00:33:09,140 --> 00:33:11,880
- So I will try even more.

621
00:33:11,880 --> 00:33:13,195
Let's move this piece.

622
00:33:13,195 --> 00:33:15,885

623
00:33:15,885 --> 00:33:17,298
[CHIMING]

624
00:33:17,298 --> 00:33:18,240

625
00:33:18,240 --> 00:33:21,650
I never gave up even
when it was hard.

626
00:33:21,650 --> 00:33:27,380

627
00:33:27,380 --> 00:33:29,000
You're next.

628
00:33:29,000 --> 00:33:30,607
Try hard and you will--

629
00:33:30,607 --> 00:33:31,190
[END PLAYBACK]

630
00:33:31,190 --> 00:33:34,350
HAE WON PARK: So actually
doing our pilot test,

631
00:33:34,350 --> 00:33:36,365
we actually found out
the fixed mindset,

632
00:33:36,365 --> 00:33:38,990
because we had a fixed mindset
robot and a growth mindset robot

633
00:33:38,990 --> 00:33:40,380
condition at the beginning.

634
00:33:40,380 --> 00:33:43,220
And we found how quickly
kids actually grasped

635
00:33:43,220 --> 00:33:47,310
the fixed mindset that we
had to bring those kids back.

636
00:33:47,310 --> 00:33:50,210
So when the robot is saying,
oh, this is too hard for me,

637
00:33:50,210 --> 00:33:55,280
this is beyond my level, kids
were adapting that so quickly

638
00:33:55,280 --> 00:33:57,650
that we had to bring
those kids back

639
00:33:57,650 --> 00:34:02,210
and actually do several sessions
of growth mindset sessions.

640
00:34:02,210 --> 00:34:06,140
And then in our main study, we
had ditched the fixed mindset

641
00:34:06,140 --> 00:34:08,610
condition because
of that very reason,

642
00:34:08,610 --> 00:34:10,520
and we replaced it
with a neutral mindset

643
00:34:10,520 --> 00:34:13,380
where the robot is just,
oh, I failed, I succeeded.

644
00:34:13,380 --> 00:34:18,770
Just talks about its
factual information

645
00:34:18,770 --> 00:34:21,739
versus displaying the mindset.

646
00:34:21,739 --> 00:34:25,550
So we found out that
even in a single session,

647
00:34:25,550 --> 00:34:29,719
after a single session, that
this impact was so significant

648
00:34:29,719 --> 00:34:34,760
that we thought of bringing this
result to a longer term semester

649
00:34:34,760 --> 00:34:37,010
long classroom.

650
00:34:37,010 --> 00:34:40,190
So here we installed
robots in classrooms

651
00:34:40,190 --> 00:34:42,719
and did a similar
puzzle activity.

652
00:34:42,719 --> 00:34:45,290
Some puzzles are really
challenging to solve,

653
00:34:45,290 --> 00:34:50,420
but we measure what kind
of difficulty level puzzle

654
00:34:50,420 --> 00:34:53,989
the children were selecting
over time, what kind of behavior

655
00:34:53,989 --> 00:34:55,350
they are adapting.

656
00:34:55,350 --> 00:34:57,118
[VIDEO PLAYBACK]

657
00:34:57,118 --> 00:34:59,387
- If my pronunciation
is a bit wrong.

658
00:34:59,387 --> 00:34:59,970
[END PLAYBACK]

659
00:34:59,970 --> 00:35:01,928
HAE WON PARK: And eventually
what they actually

660
00:35:01,928 --> 00:35:03,560
believe about their ability.

661
00:35:03,560 --> 00:35:07,343
So I'll skip the first
interaction part.

662
00:35:07,343 --> 00:35:08,010
[VIDEO PLAYBACK]

663
00:35:08,010 --> 00:35:11,026
- As we become friends
with-- rotate pieces with--

664
00:35:11,026 --> 00:35:11,960
[END PLAYBACK]

665
00:35:11,960 --> 00:35:16,980
HAE WON PARK: Then we later
on went back and interacted--

666
00:35:16,980 --> 00:35:18,115
wait.

667
00:35:18,115 --> 00:35:19,240
This video doesn't have it.

668
00:35:19,240 --> 00:35:19,740
Sorry.

669
00:35:19,740 --> 00:35:22,660
We went back and actually
interviewed the children.

670
00:35:22,660 --> 00:35:25,800
What do you think about--

671
00:35:25,800 --> 00:35:27,787
why do you select
a difficult puzzle?

672
00:35:27,787 --> 00:35:29,620
Because when they select
a difficult puzzle,

673
00:35:29,620 --> 00:35:32,200
the robot will ask, why did you
select the difficult puzzle?

674
00:35:32,200 --> 00:35:36,060
Then kids will actually say,
because it helps my brain grow.

675
00:35:36,060 --> 00:35:38,310
It won't get me more
stickers, because we gave out

676
00:35:38,310 --> 00:35:41,680
stickers as a reward when they
were able to solve the puzzle.

677
00:35:41,680 --> 00:35:44,280
But we were actually fascinated.

678
00:35:44,280 --> 00:35:47,400
Kids with stickers, you
know their relationship.

679
00:35:47,400 --> 00:35:50,520
But they were not dwelling
on stickers anymore.

680
00:35:50,520 --> 00:35:55,980
They understand that
selecting more challenges

681
00:35:55,980 --> 00:36:01,500
and being more welcoming
of difficult challenges,

682
00:36:01,500 --> 00:36:06,000
it actually helps them with
growing their brain over time.

683
00:36:06,000 --> 00:36:09,330
What we also found out is that
when growth mindset was paired

684
00:36:09,330 --> 00:36:12,010
with curiosity
fostering interaction,

685
00:36:12,010 --> 00:36:16,740
they stuck longer after
the robot was removed.

686
00:36:16,740 --> 00:36:20,750
So we really understood
that power of growth mindset

687
00:36:20,750 --> 00:36:26,510
and the persistence of the
effort that you put in.

688
00:36:26,510 --> 00:36:28,070
That will eventually--
and the belief

689
00:36:28,070 --> 00:36:30,630
that it will eventually
come back to them,

690
00:36:30,630 --> 00:36:35,160
reward them in a longer term.

691
00:36:35,160 --> 00:36:35,670
Amazing.

692
00:36:35,670 --> 00:36:36,800
Oh, here's the interview.

693
00:36:36,800 --> 00:36:37,300
OK.

694
00:36:37,300 --> 00:36:37,967
[VIDEO PLAYBACK]

695
00:36:37,967 --> 00:36:40,860
- If you were playing this
game, which game would you pick?

696
00:36:40,860 --> 00:36:42,120
- This one.

697
00:36:42,120 --> 00:36:43,260
- This one?

698
00:36:43,260 --> 00:36:44,580
Why would you pick that one?

699
00:36:44,580 --> 00:36:50,040
- Because it looks like a
challenge and I love challenges.

700
00:36:50,040 --> 00:36:52,080
- So if you had to
choose which one you

701
00:36:52,080 --> 00:36:54,950
were going to try to play,
which one would you pick?

702
00:36:54,950 --> 00:36:56,220
Which treasure chest?

703
00:36:56,220 --> 00:36:57,540
The mystery one?

704
00:36:57,540 --> 00:36:58,710
Why?

705
00:36:58,710 --> 00:37:00,235
- I like mysteries.

706
00:37:00,235 --> 00:37:01,110
- You like mysteries?

707
00:37:01,110 --> 00:37:03,120
- One, two, three.

708
00:37:03,120 --> 00:37:04,625
Which would you choose?

709
00:37:04,625 --> 00:37:07,900

710
00:37:07,900 --> 00:37:09,010
Three?

711
00:37:09,010 --> 00:37:10,000
Why is that?

712
00:37:10,000 --> 00:37:12,020
- Because it looks mysterious.

713
00:37:12,020 --> 00:37:16,430
And I wanna do something
that will help my brain grow.

714
00:37:16,430 --> 00:37:18,230
[END PLAYBACK]

715
00:37:18,230 --> 00:37:19,220
HAE WON PARK: Amazing.

716
00:37:19,220 --> 00:37:22,280
It gives me chills
every time I see this.

717
00:37:22,280 --> 00:37:29,810
So I want to end this talk with
one of the most important aspect

718
00:37:29,810 --> 00:37:35,870
that we found across
our investigation.

719
00:37:35,870 --> 00:37:40,460
We actually found that when
children rated how close they

720
00:37:40,460 --> 00:37:43,310
are with the robot,
those children who

721
00:37:43,310 --> 00:37:45,980
rated the robot as
being close actually

722
00:37:45,980 --> 00:37:49,040
had a very strong positive
correlation to their learning

723
00:37:49,040 --> 00:37:51,110
outcomes.

724
00:37:51,110 --> 00:37:56,480
We also see this not
just in vocabulary

725
00:37:56,480 --> 00:37:59,420
learning and things like that,
but we see the same phenomena

726
00:37:59,420 --> 00:38:04,040
for instilling growth
mindset and other behavior,

727
00:38:04,040 --> 00:38:05,910
other kind of behaviors too.

728
00:38:05,910 --> 00:38:08,090
And this correlation
was even stronger

729
00:38:08,090 --> 00:38:10,380
when the robot was
personalized to them.

730
00:38:10,380 --> 00:38:13,520
So it actually brought
that personalization.

731
00:38:13,520 --> 00:38:15,110
Kids actually feel
that the robot

732
00:38:15,110 --> 00:38:18,690
knows them, understands them,
can match their learning

733
00:38:18,690 --> 00:38:20,470
pace, their learning style.

734
00:38:20,470 --> 00:38:26,880
So with that, I want to
open up for questions

735
00:38:26,880 --> 00:38:29,160
or I will actually
hand it off to Sharifa

736
00:38:29,160 --> 00:38:32,850
to talk about our
AI fluency work.

737
00:38:32,850 --> 00:38:35,430
And that will be
a nice transition

738
00:38:35,430 --> 00:38:38,400
to how we engage
with older students

739
00:38:38,400 --> 00:38:42,730
beyond this early childhood
stage of their education.

740
00:38:42,730 --> 00:38:43,230
OK.

741
00:38:43,230 --> 00:38:44,132
Thank you so much.

742
00:38:44,132 --> 00:38:45,340
MEGAN MITCHELL: That's great.

743
00:38:45,340 --> 00:38:47,700
And for those of you
who've just joined us,

744
00:38:47,700 --> 00:38:53,040
once we hear from our guests
from the MIT Media Lab Personal

745
00:38:53,040 --> 00:38:56,700
Robotics Group, we've invited
two of our member organizations

746
00:38:56,700 --> 00:39:00,480
also to share their perspectives
on what they've heard

747
00:39:00,480 --> 00:39:03,100
and really launch a dialogue.

748
00:39:03,100 --> 00:39:06,550
So just I know a few people
joined us after we started,

749
00:39:06,550 --> 00:39:07,720
so wanted to reset.

750
00:39:07,720 --> 00:39:10,102
But please, Sharifa, would
love to hear from you.

751
00:39:10,102 --> 00:39:11,560
SHARIFA ALGHOWINEM:
Yes, thank you.

752
00:39:11,560 --> 00:39:12,850
So my piece is short.

753
00:39:12,850 --> 00:39:16,650
I just wanted to touch
base on the AI literacy

754
00:39:16,650 --> 00:39:21,630
and AI fluency that Cynthia
mentioned in the last week

755
00:39:21,630 --> 00:39:23,020
session.

756
00:39:23,020 --> 00:39:26,610
So we do also a
lot of curriculum

757
00:39:26,610 --> 00:39:28,950
like design
curriculums that allow

758
00:39:28,950 --> 00:39:33,030
the kids to interact
and play with AI

759
00:39:33,030 --> 00:39:35,410
and learn about it
and, for example,

760
00:39:35,410 --> 00:39:37,530
creating models and such.

761
00:39:37,530 --> 00:39:42,370
So when it comes to if there
is an overlap between what

762
00:39:42,370 --> 00:39:47,310
Hae Won mentioned and what
the AI literacy is designing,

763
00:39:47,310 --> 00:39:50,610
we design two curriculum
that actually use the robots

764
00:39:50,610 --> 00:39:53,680
into the AI development.

765
00:39:53,680 --> 00:39:58,020
So we had created a curriculum
for Jibo, one of the robots

766
00:39:58,020 --> 00:40:01,080
that Hae Won also
showed in the videos,

767
00:40:01,080 --> 00:40:03,910
and attach it to scratch blocks.

768
00:40:03,910 --> 00:40:07,380
So the kids will
create an interaction

769
00:40:07,380 --> 00:40:08,980
with a conversational agent.

770
00:40:08,980 --> 00:40:11,700
They will create the
AI models for NLPs,

771
00:40:11,700 --> 00:40:15,480
like a Natural
Language Processing.

772
00:40:15,480 --> 00:40:19,860
If Jibo ask how are you
and then your response

773
00:40:19,860 --> 00:40:21,990
is a positive or
negative, so they

774
00:40:21,990 --> 00:40:24,210
would create the
model to classify

775
00:40:24,210 --> 00:40:27,510
what is the person have said
and then how the robot should

776
00:40:27,510 --> 00:40:28,060
interact.

777
00:40:28,060 --> 00:40:31,050
So it's a
conversational dialogue

778
00:40:31,050 --> 00:40:33,550
that they create
through scratch blocks.

779
00:40:33,550 --> 00:40:38,550
And also it has elements
of AI, either NLP or we

780
00:40:38,550 --> 00:40:41,070
have the blocks for
vision, and then

781
00:40:41,070 --> 00:40:43,470
we teach them all
of these things.

782
00:40:43,470 --> 00:40:46,050
And the other curriculum
that also integrate robots

783
00:40:46,050 --> 00:40:47,230
is the Doodle Bot.

784
00:40:47,230 --> 00:40:50,970
But this is one of the new
robots that we're creating

785
00:40:50,970 --> 00:40:53,070
and we're hoping that
will be cheap enough

786
00:40:53,070 --> 00:40:56,350
to be deployed and be
sent to the schools.

787
00:40:56,350 --> 00:41:01,590
And for that robot, also
kids will use scratch blocks

788
00:41:01,590 --> 00:41:03,630
to control its movement.

789
00:41:03,630 --> 00:41:04,630
They will draw together.

790
00:41:04,630 --> 00:41:07,260

791
00:41:07,260 --> 00:41:10,085
And the robot will be using
its vision to navigate.

792
00:41:10,085 --> 00:41:12,210
And then one of the
curriculums that we've designed

793
00:41:12,210 --> 00:41:15,390
is automatic navigation
with these robots.

794
00:41:15,390 --> 00:41:18,420
If the robots-- the kids
will create algorithms

795
00:41:18,420 --> 00:41:20,350
that finds the coins.

796
00:41:20,350 --> 00:41:23,490
And so they could compete
with the other child robot

797
00:41:23,490 --> 00:41:25,850
to who would collect more coins.

798
00:41:25,850 --> 00:41:29,583
And that's a specific curriculum
design for automatic navigation,

799
00:41:29,583 --> 00:41:31,000
search algorithms,
and all of this

800
00:41:31,000 --> 00:41:33,900
and where the kids would
learn these things.

801
00:41:33,900 --> 00:41:35,910
When we're not
using robots, and I

802
00:41:35,910 --> 00:41:38,070
think this is one of
the questions that

803
00:41:38,070 --> 00:41:42,960
was asked last week,
how could people

804
00:41:42,960 --> 00:41:48,210
at schools who doesn't have
enough resources play with AI

805
00:41:48,210 --> 00:41:50,680
or use AI into their own thing?

806
00:41:50,680 --> 00:41:54,700
So we create curriculums for
using Scratch, for example,

807
00:41:54,700 --> 00:41:55,930
or App Inventor.

808
00:41:55,930 --> 00:42:00,060
App Inventor is a mobile,
also block based programming

809
00:42:00,060 --> 00:42:01,570
for mobile applications.

810
00:42:01,570 --> 00:42:04,380
And they use these
with the sensors

811
00:42:04,380 --> 00:42:06,900
like cameras or
microphone or other things

812
00:42:06,900 --> 00:42:13,050
to know what is perception and
how to create these models.

813
00:42:13,050 --> 00:42:15,420
And that is the AI literacy.

814
00:42:15,420 --> 00:42:19,060
In each of these curriculums,
we integrate policy and ethics.

815
00:42:19,060 --> 00:42:25,000
So kids learn about what are
the ethical implications?

816
00:42:25,000 --> 00:42:26,380
What are the harms of AI?

817
00:42:26,380 --> 00:42:28,660
And they so they will be aware.

818
00:42:28,660 --> 00:42:30,660
One of the new curriculum
that we're designing

819
00:42:30,660 --> 00:42:36,040
is the digital
citizenship or how the AI

820
00:42:36,040 --> 00:42:38,930
could influence their voting.

821
00:42:38,930 --> 00:42:41,390
And that is specifically
designed for high school,

822
00:42:41,390 --> 00:42:44,290
since they're very
close to voting

823
00:42:44,290 --> 00:42:48,460
in the polls in the
next year or so.

824
00:42:48,460 --> 00:42:50,930
And then when we talk
about AI fluency,

825
00:42:50,930 --> 00:42:55,420
it's where how could they use
these curriculums or these AI

826
00:42:55,420 --> 00:42:58,150
into projects.

827
00:42:58,150 --> 00:42:59,770
With these, we
have a few things.

828
00:42:59,770 --> 00:43:05,585
Like we have the future
makers where the focus is not

829
00:43:05,585 --> 00:43:07,960
the curriculum itself, but
the focus is with the projects

830
00:43:07,960 --> 00:43:10,190
that they create that
has social impact.

831
00:43:10,190 --> 00:43:13,180
So they design with
their community

832
00:43:13,180 --> 00:43:17,620
some apps, mobile apps that
would help their community where

833
00:43:17,620 --> 00:43:23,830
they could have an impact either
for the financial or social

834
00:43:23,830 --> 00:43:26,530
or for their own community.

835
00:43:26,530 --> 00:43:28,810
Kids come up with
amazing projects.

836
00:43:28,810 --> 00:43:31,550
And I think they presented
their work last Friday.

837
00:43:31,550 --> 00:43:34,710
It's a six week project
during the summer.

838
00:43:34,710 --> 00:43:38,100
And it's amazing what
you could see from those.

839
00:43:38,100 --> 00:43:42,270
And this is where we're
thinking about how they use AI

840
00:43:42,270 --> 00:43:44,640
as, again, we're not
designing curriculum,

841
00:43:44,640 --> 00:43:47,490
but we're designing the
entrepreneurship mindset

842
00:43:47,490 --> 00:43:49,330
with this program.

843
00:43:49,330 --> 00:43:52,860
And they have mentors that
will take their teams week

844
00:43:52,860 --> 00:43:56,130
by week into designing
their final project.

845
00:43:56,130 --> 00:43:59,950
With this we do, I mean,
we mostly focus on K-12,

846
00:43:59,950 --> 00:44:05,500
but we have also a few
curriculums that are for adults.

847
00:44:05,500 --> 00:44:12,460
But we don't do undergrad or
algorithmic side of things.

848
00:44:12,460 --> 00:44:16,290
But it's more like, for example,
one of the things we did

849
00:44:16,290 --> 00:44:20,910
is a learning machines
curriculum where it is designed

850
00:44:20,910 --> 00:44:25,830
and focused to the leaders
who have to implement AI

851
00:44:25,830 --> 00:44:28,080
into their workplace,
but they might not

852
00:44:28,080 --> 00:44:31,980
know enough technical background
to ask the right questions

853
00:44:31,980 --> 00:44:37,250
before they integrate
AI into their workplace.

854
00:44:37,250 --> 00:44:41,410
So we do these
workshops or curriculums

855
00:44:41,410 --> 00:44:44,980
that are specifically designed
for them to understand

856
00:44:44,980 --> 00:44:48,880
policy and ethics and
harms and benefits of AI

857
00:44:48,880 --> 00:44:50,720
and how they could
create it themselves.

858
00:44:50,720 --> 00:44:54,380
They see it, they navigate
with it, and they build it.

859
00:44:54,380 --> 00:44:56,840
So they would see
what's the error

860
00:44:56,840 --> 00:44:58,450
rate that comes
with these models

861
00:44:58,450 --> 00:45:02,720
and how could they avoid them.

862
00:45:02,720 --> 00:45:06,900
Hae Won, you wanted to talk
about the elderly with robots,

863
00:45:06,900 --> 00:45:08,030
so I'll leave that to you.

864
00:45:08,030 --> 00:45:10,963

865
00:45:10,963 --> 00:45:12,880
HAE WON PARK: Do we have
time for that, Megan,

866
00:45:12,880 --> 00:45:14,765
or do we want to
move on to questions?

867
00:45:14,765 --> 00:45:16,390
MEGAN MITCHELL: Let
me see if that gets

868
00:45:16,390 --> 00:45:19,840
integrated into the questions.

869
00:45:19,840 --> 00:45:22,750
I would like to give our
discussants some time.

870
00:45:22,750 --> 00:45:24,800
But thank you both very much.

871
00:45:24,800 --> 00:45:28,120
I think this has
really opened open up

872
00:45:28,120 --> 00:45:31,550
a lot more of the
depth of the work.

873
00:45:31,550 --> 00:45:33,350
We had a wonderful
overview from Cynthia,

874
00:45:33,350 --> 00:45:38,090
but the real depth of the
research and the nuances.

875
00:45:38,090 --> 00:45:39,940
So thank you very much.

876
00:45:39,940 --> 00:45:42,460
Now I'd like to turn to we
have two discussants here

877
00:45:42,460 --> 00:45:44,290
from two of our
member universities

878
00:45:44,290 --> 00:45:48,880
who are going to share a
little bit kind of response

879
00:45:48,880 --> 00:45:53,480
and how their work is reflected
or not in some of this.

880
00:45:53,480 --> 00:45:56,740
So first I'd like to turn
it to Maura Pompa, who's

881
00:45:56,740 --> 00:46:00,910
with the National Autonomous
University of Mexico,

882
00:46:00,910 --> 00:46:03,280
UNAM, which is a little
bit easier to-- it rolls

883
00:46:03,280 --> 00:46:05,770
off the tongue a
little bit more easily.

884
00:46:05,770 --> 00:46:12,370
And Maura, her background
is in pedagogy and really

885
00:46:12,370 --> 00:46:15,940
has worked a lot in
the conducting research

886
00:46:15,940 --> 00:46:21,280
on education issues related to
teacher training and performance

887
00:46:21,280 --> 00:46:24,970
identity and the inclusion
of ICT in the classroom.

888
00:46:24,970 --> 00:46:27,160
And currently works
as a coordinator

889
00:46:27,160 --> 00:46:29,920
of management and
systematization of knowledge

890
00:46:29,920 --> 00:46:32,530
in the education,
in the coordination

891
00:46:32,530 --> 00:46:35,050
of assessment,
education, innovation,

892
00:46:35,050 --> 00:46:36,260
and development at UNAM.

893
00:46:36,260 --> 00:46:39,130
So, Maura, can I ask
you for your thoughts

894
00:46:39,130 --> 00:46:41,500
and just hear a
little bit from you

895
00:46:41,500 --> 00:46:44,350
to bring the member voice
into this conversation?

896
00:46:44,350 --> 00:46:46,100
MAURA POMPA MANSILLA:
Hi thank you, Megan.

897
00:46:46,100 --> 00:46:47,570
And it's great to be here.

898
00:46:47,570 --> 00:46:51,610
Thank you for the opportunity
to share this really insightful

899
00:46:51,610 --> 00:46:54,070
presentations that
they both have made

900
00:46:54,070 --> 00:46:58,660
and connecting it with
Dr. Cynthia's talk.

901
00:46:58,660 --> 00:47:04,690
And what Hae Won just
showed us is so powerful.

902
00:47:04,690 --> 00:47:09,730
The way AI is introduced
is very powerful.

903
00:47:09,730 --> 00:47:12,150
And how it's used
in the classroom.

904
00:47:12,150 --> 00:47:15,810
And also not only
in the classroom,

905
00:47:15,810 --> 00:47:19,010
but in the
educational practices.

906
00:47:19,010 --> 00:47:20,340
It's really powerful.

907
00:47:20,340 --> 00:47:25,850
And that is regardless
of the educational level

908
00:47:25,850 --> 00:47:27,260
that is being introduced.

909
00:47:27,260 --> 00:47:31,040
The approach of
the social robots

910
00:47:31,040 --> 00:47:35,960
allows a whole spectrum of
the social emotional aspect

911
00:47:35,960 --> 00:47:39,180
of learning as well as
the student engagement.

912
00:47:39,180 --> 00:47:42,270
So I think with the
videos, of course,

913
00:47:42,270 --> 00:47:44,510
it's really powerful how
to see the interaction

914
00:47:44,510 --> 00:47:49,460
and how everything there's
behind those interactions

915
00:47:49,460 --> 00:47:50,640
gets to work.

916
00:47:50,640 --> 00:47:55,010
And all the back and forth
you have with all the data

917
00:47:55,010 --> 00:47:55,820
that you collect.

918
00:47:55,820 --> 00:47:59,090
And you go back and then
you see some of the results

919
00:47:59,090 --> 00:47:59,990
and the interaction.

920
00:47:59,990 --> 00:48:03,560
That for a research project
must be really mind blowing

921
00:48:03,560 --> 00:48:08,390
and really exciting also for the
things that are next to come.

922
00:48:08,390 --> 00:48:14,420
And as Dr. Breazeal pointed
out in her talk also,

923
00:48:14,420 --> 00:48:16,940
well, the importance of
being able to make mistakes

924
00:48:16,940 --> 00:48:20,990
as part of the learning
process and this role switching

925
00:48:20,990 --> 00:48:22,140
with the robot.

926
00:48:22,140 --> 00:48:27,590
And it's also very revealing
as well as the growth mindset

927
00:48:27,590 --> 00:48:29,660
that it allows the
students to do.

928
00:48:29,660 --> 00:48:33,270
It's different when even if
it's a social interaction,

929
00:48:33,270 --> 00:48:37,160
it's different when you are
with a peer or with a teacher,

930
00:48:37,160 --> 00:48:40,500
as it's been pointed out,
as doing it with a robot.

931
00:48:40,500 --> 00:48:41,130
I don't know.

932
00:48:41,130 --> 00:48:44,150
You kind of relax that part
of the social interaction

933
00:48:44,150 --> 00:48:47,060
and you don't feel as judged.

934
00:48:47,060 --> 00:48:49,880
That's what I get from
what you were just sharing.

935
00:48:49,880 --> 00:48:53,550
And it allows room
for that interaction.

936
00:48:53,550 --> 00:48:56,040
And as the robot
itself makes mistakes,

937
00:48:56,040 --> 00:49:00,270
that puts mistakes as part of
the learning process, as I said.

938
00:49:00,270 --> 00:49:03,890
And that's really,
really interesting.

939
00:49:03,890 --> 00:49:07,460
And what Sharifa
just said is also

940
00:49:07,460 --> 00:49:12,380
about literacy and
this term of fluency.

941
00:49:12,380 --> 00:49:16,620
I really like that, since the
talk of Dr. Cynthia Breazeal,

942
00:49:16,620 --> 00:49:20,820
because I hadn't heard
that term in particular.

943
00:49:20,820 --> 00:49:26,010
And it really reveals--

944
00:49:26,010 --> 00:49:28,250
the concept really
resonates with me.

945
00:49:28,250 --> 00:49:32,680
And because it's more than being
literature or knowledgeable

946
00:49:32,680 --> 00:49:34,040
about AI.

947
00:49:34,040 --> 00:49:36,190
I think it brings
more to what it

948
00:49:36,190 --> 00:49:40,390
is to know what AI and
generative AI really is

949
00:49:40,390 --> 00:49:43,030
and how to interact in
a way that you really

950
00:49:43,030 --> 00:49:46,850
understand what is happening
and take advantage of it.

951
00:49:46,850 --> 00:49:50,960
I think it also relates with
affordances, not only literacy.

952
00:49:50,960 --> 00:49:56,710
So it's a broader way
of seeing what AI really

953
00:49:56,710 --> 00:49:58,210
means, because
you can understand

954
00:49:58,210 --> 00:50:02,270
that you have to have critical
thinking in order to use it.

955
00:50:02,270 --> 00:50:06,040
And also the answers that
it gets to you when you're

956
00:50:06,040 --> 00:50:13,340
using something like ChatGPT or
another not robot related AI,

957
00:50:13,340 --> 00:50:16,030
but also to understand
how is it built.

958
00:50:16,030 --> 00:50:21,010
And also by this
using these apps

959
00:50:21,010 --> 00:50:25,300
and creating apps
focused on the greater

960
00:50:25,300 --> 00:50:28,660
good for these
communities and what's

961
00:50:28,660 --> 00:50:31,930
been shared about the
work you're doing,

962
00:50:31,930 --> 00:50:34,990
I think it's amazing,
because it transcends

963
00:50:34,990 --> 00:50:40,630
the educational process to being
put in practice towards society

964
00:50:40,630 --> 00:50:42,250
and the greater good.

965
00:50:42,250 --> 00:50:45,200
So it's really good.

966
00:50:45,200 --> 00:50:50,470
And regarding what
we do at UNAM,

967
00:50:50,470 --> 00:50:55,370
well, we're also not prescribing
what we do to teachers.

968
00:50:55,370 --> 00:50:59,290
What we're doing
really is just the

969
00:50:59,290 --> 00:51:05,530
focusing on teacher and faculty
development and also in student.

970
00:51:05,530 --> 00:51:10,240
It's not literacy, but it's
also this fluency, as you say.

971
00:51:10,240 --> 00:51:14,650
Because it's important
for us to establish

972
00:51:14,650 --> 00:51:16,810
that they are the
ones that are using

973
00:51:16,810 --> 00:51:20,450
AI and they are the ones
that are introducing AI

974
00:51:20,450 --> 00:51:22,470
to the classroom
in a certain way.

975
00:51:22,470 --> 00:51:26,930
So as you just pointed out, that
is really powerful in the way

976
00:51:26,930 --> 00:51:29,330
that not only
faculty but students

977
00:51:29,330 --> 00:51:33,810
engage with this
type of interaction.

978
00:51:33,810 --> 00:51:37,610
So it opens the way or
it really determines

979
00:51:37,610 --> 00:51:42,200
on how AI is going to work
in favor of the learning

980
00:51:42,200 --> 00:51:44,670
that process that is
going to take place.

981
00:51:44,670 --> 00:51:50,420
So I'm going to stop there to
give time to other comments

982
00:51:50,420 --> 00:51:53,080
and what's next to come
in the menu for today.

983
00:51:53,080 --> 00:51:54,127
Thank you, Megan.

984
00:51:54,127 --> 00:51:55,710
MEGAN MITCHELL: Yes,
thank you, Maura.

985
00:51:55,710 --> 00:51:56,960
Thank you.

986
00:51:56,960 --> 00:52:02,750
I'd like to now turn it
over to Seiji Isotani, who

987
00:52:02,750 --> 00:52:05,720
he's a professor of computer
science at University of Sao

988
00:52:05,720 --> 00:52:09,350
Paulo, and spent some
time in the recent past

989
00:52:09,350 --> 00:52:12,350
also at Harvard Graduate
School of Education

990
00:52:12,350 --> 00:52:15,830
and the Berkman Klein Center
for Internet and Society.

991
00:52:15,830 --> 00:52:18,710
And his research
really in mission

992
00:52:18,710 --> 00:52:22,490
focuses on the social
impact of these types

993
00:52:22,490 --> 00:52:27,080
of educational innovations and
educational practices and AI

994
00:52:27,080 --> 00:52:29,090
technologies and policy.

995
00:52:29,090 --> 00:52:31,520
And with particular
emphasis on what

996
00:52:31,520 --> 00:52:33,860
this means in resource
constrained environments

997
00:52:33,860 --> 00:52:37,610
and for low and middle
income communities.

998
00:52:37,610 --> 00:52:41,690
He has a passion also for
this every student receives

999
00:52:41,690 --> 00:52:44,840
that personalized
support they need.

1000
00:52:44,840 --> 00:52:47,090
So I'm sure he was watching
the different interactions

1001
00:52:47,090 --> 00:52:48,950
with great interest.

1002
00:52:48,950 --> 00:52:52,620
And how to best personalize
that learning at scale,

1003
00:52:52,620 --> 00:52:55,860
which is obviously kind of
maybe the next frontier.

1004
00:52:55,860 --> 00:52:59,370
But with that, please,
can I invite you, Seiji?

1005
00:52:59,370 --> 00:53:03,530

1006
00:53:03,530 --> 00:53:05,810
SEIJI ISOTANI: Thank
you, Megan, Bill,

1007
00:53:05,810 --> 00:53:11,840
and guys for the kind
invitation and Hae Won

1008
00:53:11,840 --> 00:53:14,960
and Sharifa for
the presentation.

1009
00:53:14,960 --> 00:53:23,620
So what really stick to me
in this presentations as well

1010
00:53:23,620 --> 00:53:28,660
as synthesis
presentations was how

1011
00:53:28,660 --> 00:53:33,160
you are able to design
the technology, thinking

1012
00:53:33,160 --> 00:53:36,880
about both the pedagogical
approach as well

1013
00:53:36,880 --> 00:53:38,240
as the technical part.

1014
00:53:38,240 --> 00:53:44,860
So combining the two, I
think it's a really good way

1015
00:53:44,860 --> 00:53:50,170
to reduce potential bias that
people from technology use

1016
00:53:50,170 --> 00:53:51,440
usually have.

1017
00:53:51,440 --> 00:53:54,130
They're trying to just
design a technology

1018
00:53:54,130 --> 00:53:57,070
and see how it plays
out in education.

1019
00:53:57,070 --> 00:54:02,620
So if you see many
people just using ChatGPT

1020
00:54:02,620 --> 00:54:05,480
without any pedagogical
consideration,

1021
00:54:05,480 --> 00:54:09,250
this is one of the
concerns I have.

1022
00:54:09,250 --> 00:54:14,980
And many people in the
community is speaking out

1023
00:54:14,980 --> 00:54:18,710
about these challenges of
combining pedagogical approach

1024
00:54:18,710 --> 00:54:22,640
with AI technology in
a way that we actually

1025
00:54:22,640 --> 00:54:26,780
enhance human capabilities
to learn and teach

1026
00:54:26,780 --> 00:54:30,890
rather than just using
in any other way.

1027
00:54:30,890 --> 00:54:36,500
So one thing that
when I'm listening

1028
00:54:36,500 --> 00:54:40,490
to this presentation
is, OK, so how

1029
00:54:40,490 --> 00:54:42,860
can I use all this
knowledge that

1030
00:54:42,860 --> 00:54:50,160
has been built with robots
if we don't have robots

1031
00:54:50,160 --> 00:54:52,720
or if we don't have
infrastructure?

1032
00:54:52,720 --> 00:54:56,430
So the challenge for the
developing countries like Brazil

1033
00:54:56,430 --> 00:54:58,770
and other countries
in the Global South

1034
00:54:58,770 --> 00:55:02,750
is that infrastructure
is not there.

1035
00:55:02,750 --> 00:55:05,150
And how can we bring--

1036
00:55:05,150 --> 00:55:07,010
the question is,
how can we bring

1037
00:55:07,010 --> 00:55:17,060
the benefits of AI technologies
into places and for people when

1038
00:55:17,060 --> 00:55:24,440
infrastructure
and, how can I say,

1039
00:55:24,440 --> 00:55:31,940
and teachers are not ready
to use AI technologies?

1040
00:55:31,940 --> 00:55:35,000
So this is a big challenge.

1041
00:55:35,000 --> 00:55:39,650
For example, we've
been working in places

1042
00:55:39,650 --> 00:55:44,390
like in the Amazon with several
different cities over there.

1043
00:55:44,390 --> 00:55:53,470
And you don't see internet
connection at schools.

1044
00:55:53,470 --> 00:55:57,340
They do have some
internet connectivity

1045
00:55:57,340 --> 00:56:00,530
with their cell phones,
so it's not a strong one,

1046
00:56:00,530 --> 00:56:01,750
but they do have.

1047
00:56:01,750 --> 00:56:04,100
They don't have
computers in classroom.

1048
00:56:04,100 --> 00:56:08,680
Teachers do not have access
to computers in classrooms.

1049
00:56:08,680 --> 00:56:13,620
And then I keep asking
myself, is there

1050
00:56:13,620 --> 00:56:18,510
a way how we can bring
the benefits of all

1051
00:56:18,510 --> 00:56:26,440
these real interactions and
experiences into those places?

1052
00:56:26,440 --> 00:56:32,170
And if we think that
a cell phone could

1053
00:56:32,170 --> 00:56:39,230
be a possible infrastructure,
can we mimic the same results

1054
00:56:39,230 --> 00:56:43,550
that Hae Won and
Sharifa presented to us

1055
00:56:43,550 --> 00:56:50,610
into this particular device
or creating a low cost device

1056
00:56:50,610 --> 00:56:58,620
that you can actually add scale
to use in classrooms where

1057
00:56:58,620 --> 00:57:04,260
every student possibly have
access to this technology?

1058
00:57:04,260 --> 00:57:11,580
Maybe asking questions or
trying to design interactions

1059
00:57:11,580 --> 00:57:12,890
in the same way.

1060
00:57:12,890 --> 00:57:17,250
So when I'm trying
to listen and trying

1061
00:57:17,250 --> 00:57:25,830
to extract all this important
research into the topic.

1062
00:57:25,830 --> 00:57:28,740
These are one of the
questions I do have.

1063
00:57:28,740 --> 00:57:32,880
And thinking about AI
literacy and AI fluency,

1064
00:57:32,880 --> 00:57:34,870
that pose the same challenge.

1065
00:57:34,870 --> 00:57:41,140
So in managing
students and teachers

1066
00:57:41,140 --> 00:57:44,470
that do not have
access to a computer.

1067
00:57:44,470 --> 00:57:49,570
So can we teach AI literacy
and AI fluency just

1068
00:57:49,570 --> 00:57:53,326
with computer science
unplugged approaches?

1069
00:57:53,326 --> 00:57:56,220

1070
00:57:56,220 --> 00:57:58,270
I'm skeptical about this.

1071
00:57:58,270 --> 00:58:04,260
So I don't know if you have
any comments or ideas about how

1072
00:58:04,260 --> 00:58:09,390
we can explore the potential
of these technologies

1073
00:58:09,390 --> 00:58:11,590
in those regions.

1074
00:58:11,590 --> 00:58:18,030
But I really do believe that
to reduce the inequalities

1075
00:58:18,030 --> 00:58:22,800
and to reduce the
gap, we can think

1076
00:58:22,800 --> 00:58:27,660
about redesigning all
these tools and initiatives

1077
00:58:27,660 --> 00:58:30,520
to actually support
these regions.

1078
00:58:30,520 --> 00:58:35,070
Because if we can do that,
then at the discussion

1079
00:58:35,070 --> 00:58:42,060
that Cynthia and
Hae Won offer to us

1080
00:58:42,060 --> 00:58:45,870
to reduce inequalities
actually will

1081
00:58:45,870 --> 00:58:50,910
happen if we can actually
find a way to co-design

1082
00:58:50,910 --> 00:58:53,200
with AI in those places.

1083
00:58:53,200 --> 00:58:58,330
So these are my
first impressions.

1084
00:58:58,330 --> 00:59:06,280
And just a final
comment to summarize.

1085
00:59:06,280 --> 00:59:10,690
What I love about
this presentation was

1086
00:59:10,690 --> 00:59:16,930
it connects well
with a framework

1087
00:59:16,930 --> 00:59:22,540
that project 0 developed
a while ago where

1088
00:59:22,540 --> 00:59:28,010
to understand or to
improve human learning,

1089
00:59:28,010 --> 00:59:34,480
first we need to think
about who are my students?

1090
00:59:34,480 --> 00:59:38,710
And thinking about
personalizations,

1091
00:59:38,710 --> 00:59:44,770
social components, creativity,
growth mindset, and so on.

1092
00:59:44,770 --> 00:59:47,440
It goes exactly
to this direction.

1093
00:59:47,440 --> 00:59:53,110
Then we can think about
what students are learning,

1094
00:59:53,110 --> 00:59:55,780
the components, the
content component,

1095
00:59:55,780 --> 00:59:59,740
and how they are learning,
which is all the process

1096
00:59:59,740 --> 01:00:01,160
and the interactions.

1097
01:00:01,160 --> 01:00:11,080
And finally, what students can
do with what they've learned.

1098
01:00:11,080 --> 01:00:16,300
And I think this
component is still

1099
01:00:16,300 --> 01:00:17,660
missing in the presentation.

1100
01:00:17,660 --> 01:00:21,410
But I'm sure that you all
were thinking about this.

1101
01:00:21,410 --> 01:00:25,570
So after students
have learned and have

1102
01:00:25,570 --> 01:00:28,940
acquired those skills
and those knowledges,

1103
01:00:28,940 --> 01:00:30,470
what they can do with it?

1104
01:00:30,470 --> 01:00:35,560
And I think somebody,
I think it was Sharifa,

1105
01:00:35,560 --> 01:00:38,770
talking about digital
citizenship I think goes

1106
01:00:38,770 --> 01:00:40,790
directly into this direction.

1107
01:00:40,790 --> 01:00:42,410
So thank you very much.

1108
01:00:42,410 --> 01:00:44,830
So it was really
wonderful to listen.

1109
01:00:44,830 --> 01:00:46,570
And hopefully you
have some ideas

1110
01:00:46,570 --> 01:00:50,920
about how we can actually
use all this knowledge here

1111
01:00:50,920 --> 01:00:52,937
in the Global South.

1112
01:00:52,937 --> 01:00:55,020
MEGAN MITCHELL: Well, I
certainly saw both Hae Won

1113
01:00:55,020 --> 01:00:58,980
and Sharifa giving
the adult social cue

1114
01:00:58,980 --> 01:01:01,210
of nodding to that question.

1115
01:01:01,210 --> 01:01:04,770
So do you have a response
in thinking about

1116
01:01:04,770 --> 01:01:07,170
in those low resourced
environments,

1117
01:01:07,170 --> 01:01:09,840
how do we transfer
this great work?

1118
01:01:09,840 --> 01:01:11,850
Or what is the
time horizon maybe

1119
01:01:11,850 --> 01:01:18,370
for these types of things to
come from a lab to fruition

1120
01:01:18,370 --> 01:01:20,890
in a variety of settings?

1121
01:01:20,890 --> 01:01:27,000
HAE WON PARK: So let me first
lay out the landscape of robots

1122
01:01:27,000 --> 01:01:28,950
are expensive.

1123
01:01:28,950 --> 01:01:33,210
But for example, Jibo, the
white robot that appeared

1124
01:01:33,210 --> 01:01:38,250
in my presentation, that at
the time when it was launched

1125
01:01:38,250 --> 01:01:46,800
in 2018-ish time frame, the
full price was $899, US dollars.

1126
01:01:46,800 --> 01:01:50,220
Often went on sale
until the company,

1127
01:01:50,220 --> 01:01:53,500
the Jibo Inc. had closed.

1128
01:01:53,500 --> 01:01:56,380
So at that time,
if we compare that

1129
01:01:56,380 --> 01:01:59,780
to how much like iPads
cost, the cheapest iPad,

1130
01:01:59,780 --> 01:02:02,920
I was just looking up
apple.com, and it's currently

1131
01:02:02,920 --> 01:02:06,760
like $349, US dollars.

1132
01:02:06,760 --> 01:02:10,180
So just to give you a
comparison price point

1133
01:02:10,180 --> 01:02:14,830
that even for these social
robots, they did have come to,

1134
01:02:14,830 --> 01:02:16,370
and that was, what, 2018?

1135
01:02:16,370 --> 01:02:18,310
So that's already six years ago.

1136
01:02:18,310 --> 01:02:21,910
The price has
quite significantly

1137
01:02:21,910 --> 01:02:25,240
come down to the level of
these other smart devices

1138
01:02:25,240 --> 01:02:26,240
like a tablet.

1139
01:02:26,240 --> 01:02:28,700
So just laying
that landscape out.

1140
01:02:28,700 --> 01:02:36,550
And yeah, so when
we approach and I

1141
01:02:36,550 --> 01:02:40,960
work with countries that
have lower resourcing

1142
01:02:40,960 --> 01:02:43,120
than in a typical
setting in the US

1143
01:02:43,120 --> 01:02:46,420
where we are expecting
that when we are installing

1144
01:02:46,420 --> 01:02:51,550
these devices in schools, we
expect the schools have Wi-Fi.

1145
01:02:51,550 --> 01:02:54,580
That's the one stable
network and Wi-Fi

1146
01:02:54,580 --> 01:03:01,210
is the minimum necessity that
we currently perform our work.

1147
01:03:01,210 --> 01:03:05,080
And that is where
we like also use,

1148
01:03:05,080 --> 01:03:11,320
because these robots, the
lower end cost robots,

1149
01:03:11,320 --> 01:03:14,570
they don't come with a
super computer inside.

1150
01:03:14,570 --> 01:03:18,470
So we leverage a lot of
it on the cloud compute.

1151
01:03:18,470 --> 01:03:21,770
And that's why that Wi-Fi
connection is so important.

1152
01:03:21,770 --> 01:03:27,100
We are using latest, the best
automatic speech recognition.

1153
01:03:27,100 --> 01:03:31,340
API, for example, for especially
for children's speech.

1154
01:03:31,340 --> 01:03:34,190
Then that is all
cloud calls, right.

1155
01:03:34,190 --> 01:03:39,220
So oftentimes in the
current technology

1156
01:03:39,220 --> 01:03:43,040
that we are working on,
we use a lot of those.

1157
01:03:43,040 --> 01:03:46,570
We leverage a lot of those
internet connections and cloud

1158
01:03:46,570 --> 01:03:47,260
compute.

1159
01:03:47,260 --> 01:03:50,980
And cloud compute is not free.

1160
01:03:50,980 --> 01:03:54,010
Every call is a cost.

1161
01:03:54,010 --> 01:03:56,350
Now, let's rewind back,
because I just told you

1162
01:03:56,350 --> 01:04:00,590
that we've been working on
this field for about 15,

1163
01:04:00,590 --> 01:04:03,130
maybe a little
more than 15 years.

1164
01:04:03,130 --> 01:04:08,474
If we just rewind back to about
10 years ago, where were we?

1165
01:04:08,474 --> 01:04:12,350
Back then we were developing
this personalization

1166
01:04:12,350 --> 01:04:17,420
reinforcement models that
run locally on board.

1167
01:04:17,420 --> 01:04:20,350
Because there
weren't any powerful

1168
01:04:20,350 --> 01:04:22,660
LLMs back then that we
need to make cloud calls

1169
01:04:22,660 --> 01:04:23,860
and things like that.

1170
01:04:23,860 --> 01:04:27,820
We are also leveraging
because some of the schools

1171
01:04:27,820 --> 01:04:30,430
didn't have stable network.

1172
01:04:30,430 --> 01:04:34,360
Especially oftentimes we were
given rooms in the basement

1173
01:04:34,360 --> 01:04:37,820
and we're like even
if they had Wi-Fi,

1174
01:04:37,820 --> 01:04:39,680
there was a very low
network connection.

1175
01:04:39,680 --> 01:04:43,390
And then we were leveraging
offline ASR models

1176
01:04:43,390 --> 01:04:44,840
on the phones.

1177
01:04:44,840 --> 01:04:48,140
Is the performance on
par with cloud APIs?

1178
01:04:48,140 --> 01:04:51,700
No, but we still
were able to develop

1179
01:04:51,700 --> 01:04:53,980
personalized interactions
and models that

1180
01:04:53,980 --> 01:04:56,770
can learn run locally.

1181
01:04:56,770 --> 01:05:01,590
And we had to be prepared when
suddenly the school network goes

1182
01:05:01,590 --> 01:05:04,630
down but our study
interaction had to go on.

1183
01:05:04,630 --> 01:05:09,660
So we do have, because
we worked on this project

1184
01:05:09,660 --> 01:05:13,110
for so long, we do have--

1185
01:05:13,110 --> 01:05:16,450
the first video I showed
you, you won't believe it,

1186
01:05:16,450 --> 01:05:19,020
but that was completely offline.

1187
01:05:19,020 --> 01:05:23,190
The ASR was performed
on a local ASR model.

1188
01:05:23,190 --> 01:05:25,740
The robot interaction,
yes, it was a little bit

1189
01:05:25,740 --> 01:05:27,850
more prescriptive of the robot.

1190
01:05:27,850 --> 01:05:31,720
Like saying you should like say
this when the child say this.

1191
01:05:31,720 --> 01:05:35,640
You should say this at this
point in the interaction

1192
01:05:35,640 --> 01:05:36,850
and things like that.

1193
01:05:36,850 --> 01:05:41,670
But still, it is a interaction
that we can provide.

1194
01:05:41,670 --> 01:05:45,750
We are also always
thinking about scalability.

1195
01:05:45,750 --> 01:05:49,350
Because still, we show the
power of physical embodiment

1196
01:05:49,350 --> 01:05:51,810
of the social robot
and how it brings

1197
01:05:51,810 --> 01:05:55,500
this affective and relational
engagement from the child.

1198
01:05:55,500 --> 01:05:58,580
Yes, we believe that it's
the most effective way,

1199
01:05:58,580 --> 01:06:03,240
form of AI agent to use
in this kind of project.

1200
01:06:03,240 --> 01:06:05,840
But at the same time,
there can be trade off,

1201
01:06:05,840 --> 01:06:08,510
because for
scalability, we probably

1202
01:06:08,510 --> 01:06:11,930
want to put something
on mobile devices.

1203
01:06:11,930 --> 01:06:14,600
So we also have works
that we developed

1204
01:06:14,600 --> 01:06:19,640
a replica of a physical
agent as a virtual agent that

1205
01:06:19,640 --> 01:06:21,480
lives on the screen.

1206
01:06:21,480 --> 01:06:23,940
We've also done work
where there's a hybrid.

1207
01:06:23,940 --> 01:06:26,700
The kids have access
to the robot at school,

1208
01:06:26,700 --> 01:06:29,520
but also have the virtual
agent when they go home.

1209
01:06:29,520 --> 01:06:34,760
So to provide that
continued experience.

1210
01:06:34,760 --> 01:06:40,470
So there are many
ways to also engage.

1211
01:06:40,470 --> 01:06:44,840
And Julia, I did saw your
comment on the chat message

1212
01:06:44,840 --> 01:06:45,630
as well.

1213
01:06:45,630 --> 01:06:51,320
And yeah, we also had a
collaboration with organizations

1214
01:06:51,320 --> 01:06:52,930
like curious learning people.

1215
01:06:52,930 --> 01:06:57,610
They were also involved with the
one laptop per child project.

1216
01:06:57,610 --> 01:07:03,880
And these devices go to areas
where there's just 0 Wi-Fi.

1217
01:07:03,880 --> 01:07:06,340
There's no connection
to the internet.

1218
01:07:06,340 --> 01:07:10,200
But the interesting thing
about building interactions

1219
01:07:10,200 --> 01:07:13,350
on those devices and what
our curious learning also

1220
01:07:13,350 --> 01:07:20,050
had pushed through was
developing these offline apps

1221
01:07:20,050 --> 01:07:23,660
that you can't really
play it by yourself.

1222
01:07:23,660 --> 01:07:27,200
You have to play
with someone else.

1223
01:07:27,200 --> 01:07:30,980
So that kind of provides
setting for that.

1224
01:07:30,980 --> 01:07:33,640
You are actually socially
learning with another peer

1225
01:07:33,640 --> 01:07:36,340
teacher, because you
have to work together

1226
01:07:36,340 --> 01:07:37,550
to figure something out.

1227
01:07:37,550 --> 01:07:41,170
Even turning on the
device, they have

1228
01:07:41,170 --> 01:07:44,710
to think together because
there's no online manual

1229
01:07:44,710 --> 01:07:45,920
that they access.

1230
01:07:45,920 --> 01:07:48,110
But they also spread
their learning.

1231
01:07:48,110 --> 01:07:50,050
They develop
different forms of how

1232
01:07:50,050 --> 01:07:53,240
to engage and use these
devices amongst themselves.

1233
01:07:53,240 --> 01:07:56,270
So when Curious Learning
went back after a year,

1234
01:07:56,270 --> 01:07:58,960
they observed that
the students there

1235
01:07:58,960 --> 01:08:01,810
were using these apps
that they had put

1236
01:08:01,810 --> 01:08:04,640
about 20 apps on the tablets.

1237
01:08:04,640 --> 01:08:09,880
But kids were actually, they
invented their own way of how

1238
01:08:09,880 --> 01:08:12,190
to use those apps
amongst themselves

1239
01:08:12,190 --> 01:08:16,640
and created their own mechanisms
of how to collaborate,

1240
01:08:16,640 --> 01:08:20,810
build this challenging
competitive interactions

1241
01:08:20,810 --> 01:08:22,140
amongst themselves.

1242
01:08:22,140 --> 01:08:25,939
Use that as a mechanism to
also propel physical learning

1243
01:08:25,939 --> 01:08:29,359
outside the apps and all
different creative ways

1244
01:08:29,359 --> 01:08:31,290
that they actually
utilize the technology.

1245
01:08:31,290 --> 01:08:36,470
So it's also about
designing these technologies

1246
01:08:36,470 --> 01:08:40,800
in cases where we can't
have this ideal setting.

1247
01:08:40,800 --> 01:08:43,700
The social robots can
be purchased, embedded,

1248
01:08:43,700 --> 01:08:47,899
connect to the cloud
compute, or everything.

1249
01:08:47,899 --> 01:08:50,750
There's always a
design and component

1250
01:08:50,750 --> 01:08:55,580
that we can adapt, take pieces
of the important aspects,

1251
01:08:55,580 --> 01:08:59,960
and instill and design for
that given environment.

1252
01:08:59,960 --> 01:09:06,080
So Sharifa and I also has got
this J-WEL grant this year

1253
01:09:06,080 --> 01:09:09,830
to transfer our developed
technology that's

1254
01:09:09,830 --> 01:09:12,109
all developed for
English learners

1255
01:09:12,109 --> 01:09:14,310
to Arabic speaking children.

1256
01:09:14,310 --> 01:09:18,210
So we can actually install
them in refugee camps,

1257
01:09:18,210 --> 01:09:21,560
support their social
emotional learning, and also

1258
01:09:21,560 --> 01:09:23,670
the school curriculum.

1259
01:09:23,670 --> 01:09:26,149
And in those settings,
we are expecting

1260
01:09:26,149 --> 01:09:28,080
that in sometimes
like those cases,

1261
01:09:28,080 --> 01:09:31,430
we won't have the same
setting as in the Media Lab

1262
01:09:31,430 --> 01:09:34,490
or at public school
systems in the US.

1263
01:09:34,490 --> 01:09:36,380
Sharifa, if you want
to add anything.

1264
01:09:36,380 --> 01:09:37,797
SHARIFA ALGHOWINEM:
No, thank you.

1265
01:09:37,797 --> 01:09:42,350
I wanted to add this statement,
but you did, so thank you.

1266
01:09:42,350 --> 01:09:44,100
MEGAN MITCHELL: Well,
I'm mindful of time,

1267
01:09:44,100 --> 01:09:45,475
and I'm going to
make you promise

1268
01:09:45,475 --> 01:09:48,260
that you will come back
and talk about that

1269
01:09:48,260 --> 01:09:54,680
at all points in the study and
keep us up to date, because I do

1270
01:09:54,680 --> 01:09:58,310
think that is of particular
interest to our members

1271
01:09:58,310 --> 01:09:59,630
who work across--

1272
01:09:59,630 --> 01:10:01,790
I mean, I know there's
been a lot of work that's

1273
01:10:01,790 --> 01:10:06,000
gone into working across
different socioeconomic groups

1274
01:10:06,000 --> 01:10:08,450
and settings in the
US, but then looking

1275
01:10:08,450 --> 01:10:11,900
at the next level
of accessibility

1276
01:10:11,900 --> 01:10:17,210
from a connectivity perspective,
and doing this at scale.

1277
01:10:17,210 --> 01:10:18,660
I wish we had more time.

1278
01:10:18,660 --> 01:10:20,870
I have all sorts of questions.

1279
01:10:20,870 --> 01:10:23,900
I could have dove more into
that entrepreneurial mindset

1280
01:10:23,900 --> 01:10:26,920
and the growth mindset and
how you carry that forward.

1281
01:10:26,920 --> 01:10:31,010
I think there was lots here
that we could have unpacked

1282
01:10:31,010 --> 01:10:32,550
in a lot of different ways.

1283
01:10:32,550 --> 01:10:35,760
But time is precious,
and we are at time.

1284
01:10:35,760 --> 01:10:39,435
Bill, is there anything you'd
like to say to close us out?

1285
01:10:39,435 --> 01:10:41,810
Because we didn't get to hear
much from you, and I'm sure

1286
01:10:41,810 --> 01:10:43,775
you had lots of
good questions too.

1287
01:10:43,775 --> 01:10:46,150
WILLIAM BONVILLIAN: Seiji,
you raised a crucial question,

1288
01:10:46,150 --> 01:10:49,068
and I'm delighted to hear, Hae
Won and Sharifa, that you're

1289
01:10:49,068 --> 01:10:50,610
definitely putting
this on the agenda

1290
01:10:50,610 --> 01:10:52,680
and at a high point
in the agenda.

1291
01:10:52,680 --> 01:10:54,390
Because these are
crucial questions

1292
01:10:54,390 --> 01:10:56,517
for how this technology
can really scale.

1293
01:10:56,517 --> 01:10:57,850
MEGAN MITCHELL: Well, thank you.

1294
01:10:57,850 --> 01:11:01,180
Thank you all very much for
your time, your contributions,

1295
01:11:01,180 --> 01:11:05,140
your attention, and have
a wonderful day, evening,

1296
01:11:05,140 --> 01:11:07,090
afternoon, wherever you are.

1297
01:11:07,090 --> 01:11:07,830
Thank you all.

1298
01:11:07,830 --> 01:11:08,830
HAE WON PARK: Thank you.

1299
01:11:08,830 --> 01:11:11,330
[MUSIC PLAYING]

1300
01:11:11,330 --> 01:11:18,000

