1
00:00:04,759 --> 00:00:10,320
Our next speaker is Manish

2
00:00:12,200 --> 00:00:19,840
uh Reagan and he is

3
00:00:15,400 --> 00:00:23,199
a J Houston career development professor

4
00:00:19,840 --> 00:00:26,000
and also assistant professor in MIT

5
00:00:23,199 --> 00:00:28,160
school management with joint appointment

6
00:00:26,000 --> 00:00:32,120
in school of engineering.

7
00:00:28,160 --> 00:00:36,320
uh he's going to explore the

8
00:00:32,120 --> 00:00:39,760
AI's social impact, societal impact from

9
00:00:36,320 --> 00:00:42,040
the lens of computer science, economics,

10
00:00:39,760 --> 00:00:47,079
and law. Let's

11
00:00:42,040 --> 00:00:47,079
welcome Manish. Thank you.

12
00:00:48,960 --> 00:00:51,920
All right. Great. Thanks for having me

13
00:00:50,160 --> 00:00:54,079
here. Uh it's good to see some familiar

14
00:00:51,920 --> 00:00:56,160
faces in in the audience. So, my name is

15
00:00:54,079 --> 00:00:58,559
Manish. I am a computer scientist but I

16
00:00:56,160 --> 00:01:00,879
also sit in the business school and

17
00:00:58,559 --> 00:01:03,199
broadly I do a lot of research on trying

18
00:01:00,879 --> 00:01:05,439
to understand the societal implications

19
00:01:03,199 --> 00:01:07,920
that AI is going to have for us. So this

20
00:01:05,439 --> 00:01:09,520
can be at the small scale so individual

21
00:01:07,920 --> 00:01:11,520
decisions here and there or it could be

22
00:01:09,520 --> 00:01:13,439
at the policy level sort of macro across

23
00:01:11,520 --> 00:01:14,799
an entire society. And so I'm going to

24
00:01:13,439 --> 00:01:16,640
try to give you a few vignettes of what

25
00:01:14,799 --> 00:01:18,840
this research looks like and where I

26
00:01:16,640 --> 00:01:20,799
think we're headed as AI

27
00:01:18,840 --> 00:01:23,119
progresses. This is a picture that I

28
00:01:20,799 --> 00:01:24,720
show my MBA students some of whom are in

29
00:01:23,119 --> 00:01:26,240
this room right now.

30
00:01:24,720 --> 00:01:30,320
uh in one of the first few lectures I

31
00:01:26,240 --> 00:01:32,479
give and it's I always return to it

32
00:01:30,320 --> 00:01:35,920
because in my computer science education

33
00:01:32,479 --> 00:01:37,759
we very much focused on the sort of

34
00:01:35,920 --> 00:01:38,960
isolating AI from the rest of the world.

35
00:01:37,759 --> 00:01:40,320
So when you take a machine learning

36
00:01:38,960 --> 00:01:41,840
class as an undergraduate in many

37
00:01:40,320 --> 00:01:43,600
computer science departments they hand

38
00:01:41,840 --> 00:01:45,520
you data sets they tell you make some

39
00:01:43,600 --> 00:01:46,720
good predictions build AI models they

40
00:01:45,520 --> 00:01:48,320
don't really tell you where that data

41
00:01:46,720 --> 00:01:50,079
came from or what you're supposed to

42
00:01:48,320 --> 00:01:51,360
actually be doing with it. And I find

43
00:01:50,079 --> 00:01:52,880
that those are often the more

44
00:01:51,360 --> 00:01:54,320
instructive questions, at least from my

45
00:01:52,880 --> 00:01:56,320
perspective, when we're trying to

46
00:01:54,320 --> 00:01:58,240
understand what AI is going to do to the

47
00:01:56,320 --> 00:02:00,320
world. And so I find these blue arrows

48
00:01:58,240 --> 00:02:02,240
that I've drawn in here to be really the

49
00:02:00,320 --> 00:02:03,360
generative pieces of research for me.

50
00:02:02,240 --> 00:02:05,040
What is the stuff that we're putting

51
00:02:03,360 --> 00:02:07,119
into these AI models? And what are we

52
00:02:05,040 --> 00:02:09,440
doing with them once we've built them?

53
00:02:07,119 --> 00:02:11,280
And lots of people spend a ton of time

54
00:02:09,440 --> 00:02:13,760
optimizing the AI models themselves.

55
00:02:11,280 --> 00:02:15,840
That's not really me in particular. Uh I

56
00:02:13,760 --> 00:02:18,080
love that those people exist. I'm really

57
00:02:15,840 --> 00:02:19,360
interested in the connections, the

58
00:02:18,080 --> 00:02:20,400
interfaces between AI and the world,

59
00:02:19,360 --> 00:02:22,680
which that's where where we're going to

60
00:02:20,400 --> 00:02:26,000
try to live

61
00:02:22,680 --> 00:02:27,599
today. One of the real difficulties and

62
00:02:26,000 --> 00:02:29,520
and this I think I came across pretty

63
00:02:27,599 --> 00:02:31,280
early in my PhD of trying to think about

64
00:02:29,520 --> 00:02:33,519
well how what is a good general recipe

65
00:02:31,280 --> 00:02:34,959
for thinking about AI in society, right?

66
00:02:33,519 --> 00:02:36,959
Are there some broad principles that we

67
00:02:34,959 --> 00:02:39,120
can apply everywhere? Learn a few things

68
00:02:36,959 --> 00:02:40,360
and then just sort of repackage them and

69
00:02:39,120 --> 00:02:43,360
repurpose them for every different

70
00:02:40,360 --> 00:02:44,879
domain. This is what a computer science

71
00:02:43,360 --> 00:02:46,319
computer scientist is trained to do. You

72
00:02:44,879 --> 00:02:48,000
abstract away the problems. You solve

73
00:02:46,319 --> 00:02:50,720
them at abstract levels and then you try

74
00:02:48,000 --> 00:02:52,560
to propagate them all over the place.

75
00:02:50,720 --> 00:02:55,040
But the problem here is every domain is

76
00:02:52,560 --> 00:02:56,640
just so different. If we try to put AI

77
00:02:55,040 --> 00:02:58,400
in medical domains, that's going to look

78
00:02:56,640 --> 00:03:00,239
very different from putting it in

79
00:02:58,400 --> 00:03:01,720
employment domains. Very different from

80
00:03:00,239 --> 00:03:03,840
building code assistance for software

81
00:03:01,720 --> 00:03:05,760
engineers. There's different risks.

82
00:03:03,840 --> 00:03:07,920
There's idiosyncratic problems that you

83
00:03:05,760 --> 00:03:08,959
run into. Of course, there is common

84
00:03:07,920 --> 00:03:10,720
technology and we're going to try to

85
00:03:08,959 --> 00:03:12,319
pull out that thread a little bit, but

86
00:03:10,720 --> 00:03:13,840
we really need to incorporate a lot of

87
00:03:12,319 --> 00:03:15,840
domain expertise in order to make good

88
00:03:13,840 --> 00:03:18,400
decisions here. And so, I'm going to try

89
00:03:15,840 --> 00:03:20,319
to squeeze in a few examples in very

90
00:03:18,400 --> 00:03:22,000
different settings of where I think it's

91
00:03:20,319 --> 00:03:23,200
interesting to think about AI and its

92
00:03:22,000 --> 00:03:24,920
social impact. So, we're going to talk

93
00:03:23,200 --> 00:03:26,560
about decisions being made in medical

94
00:03:24,920 --> 00:03:28,000
triage. We're going to think about

95
00:03:26,560 --> 00:03:29,760
employment decision-m and how

96
00:03:28,000 --> 00:03:31,519
discrimination and discrimination law

97
00:03:29,760 --> 00:03:33,599
should govern machine learning. And then

98
00:03:31,519 --> 00:03:35,120
we'll think a little bit about where the

99
00:03:33,599 --> 00:03:36,799
future of creative production is going

100
00:03:35,120 --> 00:03:39,720
to look like with the rise of generative

101
00:03:36,799 --> 00:03:42,480
AI and how we might think about

102
00:03:39,720 --> 00:03:43,680
that. A general recipe that I try to

103
00:03:42,480 --> 00:03:46,000
employ, I'm not going to say that this

104
00:03:43,680 --> 00:03:48,480
works all the time. This this is not by

105
00:03:46,000 --> 00:03:50,239
any means the only thing you have to do,

106
00:03:48,480 --> 00:03:51,680
but it's basically two questions that

107
00:03:50,239 --> 00:03:54,319
you need to keep asking yourselves over

108
00:03:51,680 --> 00:03:55,680
and over again. What are the risks of

109
00:03:54,319 --> 00:03:57,439
building and deploying AI in a

110
00:03:55,680 --> 00:03:59,599
particular system or in a particular

111
00:03:57,439 --> 00:04:01,120
setting? And how should I measure those

112
00:03:59,599 --> 00:04:03,120
risks? and what could I what steps could

113
00:04:01,120 --> 00:04:04,239
I take to mitigate them? Okay, and so

114
00:04:03,120 --> 00:04:06,000
that's the the process that we're going

115
00:04:04,239 --> 00:04:07,680
to go through here. Again, there's more

116
00:04:06,000 --> 00:04:09,439
comprehensive frameworks you can find if

117
00:04:07,680 --> 00:04:10,879
you want to, but this is a really good

118
00:04:09,439 --> 00:04:12,959
first pass just trying to understand

119
00:04:10,879 --> 00:04:14,640
what do I think AI is going to do in my

120
00:04:12,959 --> 00:04:16,359
setting, in my business, in my uh

121
00:04:14,640 --> 00:04:19,040
domain, whatever it might

122
00:04:16,359 --> 00:04:20,639
be. Okay, so let's start by trying to

123
00:04:19,040 --> 00:04:22,800
think about the use of AI in medical

124
00:04:20,639 --> 00:04:24,400
triage decisions.

125
00:04:22,800 --> 00:04:26,080
I'll give you a canonical scenario that

126
00:04:24,400 --> 00:04:27,520
we we run into a lot and our our

127
00:04:26,080 --> 00:04:30,639
collaborators at the Yale School of

128
00:04:27,520 --> 00:04:33,280
Medicine run into a lot. Uh a patient

129
00:04:30,639 --> 00:04:34,639
shows up into an emergency room. Okay,

130
00:04:33,280 --> 00:04:35,840
they have some symptoms. They're going

131
00:04:34,639 --> 00:04:37,360
to be waiting there for a long time.

132
00:04:35,840 --> 00:04:39,320
It's going to take them a while to see a

133
00:04:37,360 --> 00:04:41,919
a doctor. Doctors are short staffed. So

134
00:04:39,320 --> 00:04:44,639
on eventually once they do see a doctor,

135
00:04:41,919 --> 00:04:46,080
that doctor has a decision to make.

136
00:04:44,639 --> 00:04:47,840
Should I send this patient home or

137
00:04:46,080 --> 00:04:49,120
should I keep them in the ER? This is of

138
00:04:47,840 --> 00:04:51,520
course not the only decision they have

139
00:04:49,120 --> 00:04:52,720
to make. But in the particular setting

140
00:04:51,520 --> 00:04:53,720
that we're going to be thinking about

141
00:04:52,720 --> 00:04:55,680
here, which happens to be

142
00:04:53,720 --> 00:04:56,960
gastrointestinal uh gastrointestinal

143
00:04:55,680 --> 00:04:58,639
bleeding, these are the types of

144
00:04:56,960 --> 00:05:00,320
decisions that doctors are making.

145
00:04:58,639 --> 00:05:01,840
Should I is this patient in good enough

146
00:05:00,320 --> 00:05:04,320
condition to be sent home or should they

147
00:05:01,840 --> 00:05:05,840
be kept in the ER? And so doctors have

148
00:05:04,320 --> 00:05:07,360
to make these decisions every day. These

149
00:05:05,840 --> 00:05:08,960
are going to be idiosyncratic decisions

150
00:05:07,360 --> 00:05:10,320
that they make and there's some

151
00:05:08,960 --> 00:05:11,600
standards of care around this. But by

152
00:05:10,320 --> 00:05:13,360
and large, doctors have a lot of

153
00:05:11,600 --> 00:05:14,639
discretion.

154
00:05:13,360 --> 00:05:16,320
But increasingly in these types of

155
00:05:14,639 --> 00:05:18,160
settings, we're also starting to build

156
00:05:16,320 --> 00:05:19,759
algorithms, build AI models, predictive

157
00:05:18,160 --> 00:05:21,360
models, whatever you want to call them,

158
00:05:19,759 --> 00:05:23,280
to help doctors make these decisions.

159
00:05:21,360 --> 00:05:25,840
And so these often take the form of risk

160
00:05:23,280 --> 00:05:27,199
scores. So you the doctor plugs in some

161
00:05:25,840 --> 00:05:29,440
data about the patient's vital

162
00:05:27,199 --> 00:05:31,759
statistics into some calculator. Uh

163
00:05:29,440 --> 00:05:34,160
medcalc or mdcalc is one of the ones

164
00:05:31,759 --> 00:05:35,919
that people use quite frequently. And

165
00:05:34,160 --> 00:05:38,240
this gives them a score. It says this

166
00:05:35,919 --> 00:05:40,880
person we think is a seven on a risk

167
00:05:38,240 --> 00:05:42,240
scale from 0 to 23. Okay. Hey, now if

168
00:05:40,880 --> 00:05:44,479
you're a doctor, what do you do with

169
00:05:42,240 --> 00:05:45,600
this information? All right, you talk to

170
00:05:44,479 --> 00:05:47,199
the patient. You have a lot of

171
00:05:45,600 --> 00:05:48,400
information that you know you got to

172
00:05:47,199 --> 00:05:49,680
have a conversation with them. They said

173
00:05:48,400 --> 00:05:52,800
their stomach hurts, whatever it might

174
00:05:49,680 --> 00:05:54,639
be. And this algorithm gave you a score.

175
00:05:52,800 --> 00:05:56,160
The real challenge here is how do you

176
00:05:54,639 --> 00:05:58,639
take these two very different sources of

177
00:05:56,160 --> 00:05:59,919
information and use them productively.

178
00:05:58,639 --> 00:06:02,680
Okay. And so that's the the challenge

179
00:05:59,919 --> 00:06:05,440
that we were trying to tackle here.

180
00:06:02,680 --> 00:06:07,840
uh the there's if we think about what

181
00:06:05,440 --> 00:06:09,360
are the risks involved in this scenario.

182
00:06:07,840 --> 00:06:10,560
This is one where the doctors actually

183
00:06:09,360 --> 00:06:12,880
have a really good sense of what the

184
00:06:10,560 --> 00:06:14,160
risks are. If you talk to them, the

185
00:06:12,880 --> 00:06:15,840
primary thing that they're worried about

186
00:06:14,160 --> 00:06:18,080
is sending home patients who really need

187
00:06:15,840 --> 00:06:19,120
care. Okay? They do not want to send

188
00:06:18,080 --> 00:06:20,680
people home who are going to suffer

189
00:06:19,120 --> 00:06:23,039
adverse

190
00:06:20,680 --> 00:06:25,000
consequences. But beyond this primary

191
00:06:23,039 --> 00:06:27,840
cost, there are also secondary costs as

192
00:06:25,000 --> 00:06:29,520
well. We have limited resources. We have

193
00:06:27,840 --> 00:06:31,440
limited ER beds. We have limited

194
00:06:29,520 --> 00:06:33,120
physician time. And we need to allocate

195
00:06:31,440 --> 00:06:35,360
those responsibly. Right? If you only

196
00:06:33,120 --> 00:06:37,120
cared about this failure to admit

197
00:06:35,360 --> 00:06:38,479
patients who need immediate attention,

198
00:06:37,120 --> 00:06:40,400
what would you do? You'd admit every

199
00:06:38,479 --> 00:06:42,000
single patient, right? You will never

200
00:06:40,400 --> 00:06:43,840
make any mistakes on our primary risk

201
00:06:42,000 --> 00:06:45,199
category. But that is not the only thing

202
00:06:43,840 --> 00:06:46,639
that we care about. And so we have

203
00:06:45,199 --> 00:06:48,560
complex trade-offs at play that we need

204
00:06:46,639 --> 00:06:50,080
to deal with here. And what we want to

205
00:06:48,560 --> 00:06:51,520
do is come up with a recipe or a

206
00:06:50,080 --> 00:06:53,919
framework that allows us to express

207
00:06:51,520 --> 00:06:57,360
those trade-offs and allow physicians to

208
00:06:53,919 --> 00:06:58,960
make good decisions within them. Okay.

209
00:06:57,360 --> 00:07:01,639
So what is the right way to combine

210
00:06:58,960 --> 00:07:04,240
these sources of

211
00:07:01,639 --> 00:07:05,440
information? At a high level, I'll I'll

212
00:07:04,240 --> 00:07:07,039
tell you the framework that we developed

213
00:07:05,440 --> 00:07:09,680
and I'll uh give you a sense of some of

214
00:07:07,039 --> 00:07:12,319
the the capabilities that we have here.

215
00:07:09,680 --> 00:07:14,240
Our high level approach is this. We are

216
00:07:12,319 --> 00:07:16,560
going to build the best models we can

217
00:07:14,240 --> 00:07:18,720
that group patients together by risk.

218
00:07:16,560 --> 00:07:19,840
Okay, so they say from the algorithms

219
00:07:18,720 --> 00:07:21,759
perspective, from our models

220
00:07:19,840 --> 00:07:23,680
perspective, we're going to put together

221
00:07:21,759 --> 00:07:25,360
all the patients who look identical. We

222
00:07:23,680 --> 00:07:27,440
can't tell the difference between them.

223
00:07:25,360 --> 00:07:28,639
Sometimes they are truly identical and

224
00:07:27,440 --> 00:07:30,000
this is because you can have patients

225
00:07:28,639 --> 00:07:31,680
who show up with exactly the same

226
00:07:30,000 --> 00:07:33,680
characteristics. So if you plug all

227
00:07:31,680 --> 00:07:35,280
their data into MD Calc, you actually

228
00:07:33,680 --> 00:07:37,039
get exactly the same information that

229
00:07:35,280 --> 00:07:37,919
you're putting in. No algorithm is going

230
00:07:37,039 --> 00:07:39,599
to be able to tell you the difference

231
00:07:37,919 --> 00:07:41,199
between those patients. But the crucial

232
00:07:39,599 --> 00:07:43,360
piece of information here is a doctor

233
00:07:41,199 --> 00:07:45,199
can, right? Because they don't just see

234
00:07:43,360 --> 00:07:46,880
the eight numbers that we passed into an

235
00:07:45,199 --> 00:07:48,639
algorithm. They talked to the patient.

236
00:07:46,880 --> 00:07:50,479
They saw the, you know, that their skin

237
00:07:48,639 --> 00:07:52,400
tone had changed. They saw how much pain

238
00:07:50,479 --> 00:07:55,039
they're in. Maybe they know them. They

239
00:07:52,400 --> 00:07:56,479
have a history with them. With all this

240
00:07:55,039 --> 00:07:58,400
information, doctors might actually be

241
00:07:56,479 --> 00:08:00,240
able to distinguish between them. And so

242
00:07:58,400 --> 00:08:02,160
what we're able to do is learn where are

243
00:08:00,240 --> 00:08:03,680
the groups of patients where doctors

244
00:08:02,160 --> 00:08:05,599
have lots of information that our AI

245
00:08:03,680 --> 00:08:08,720
models don't. And how do we allocate

246
00:08:05,599 --> 00:08:10,479
physician time towards them? And so what

247
00:08:08,720 --> 00:08:12,240
this lets us do is actually make

248
00:08:10,479 --> 00:08:14,639
decisions that are better than either

249
00:08:12,240 --> 00:08:16,560
algorithms or doctors in isolation.

250
00:08:14,639 --> 00:08:18,879
Okay, we make fewer mistakes. We take

251
00:08:16,560 --> 00:08:20,560
less time to do so.

252
00:08:18,879 --> 00:08:22,960
And one of the qualitative takeaways we

253
00:08:20,560 --> 00:08:24,639
found from this is algorithms are really

254
00:08:22,960 --> 00:08:27,120
good at identifying patients who are

255
00:08:24,639 --> 00:08:28,479
super high risk. And those are precisely

256
00:08:27,120 --> 00:08:30,560
the patients who should not be sitting

257
00:08:28,479 --> 00:08:32,080
in ER waiting rooms for hours to see a

258
00:08:30,560 --> 00:08:33,440
doctor before they're admitted. These

259
00:08:32,080 --> 00:08:35,279
are the types of patients who we would

260
00:08:33,440 --> 00:08:37,919
like to identify as soon as possible

261
00:08:35,279 --> 00:08:40,000
that this person needs to be in a uh in

262
00:08:37,919 --> 00:08:42,800
the ER. They don't need to see a triage

263
00:08:40,000 --> 00:08:44,399
doctor in order to tell you that. Where

264
00:08:42,800 --> 00:08:46,560
where doctors do particularly well

265
00:08:44,399 --> 00:08:48,560
though is for these ambiguous patients

266
00:08:46,560 --> 00:08:49,920
who the algorithms can't really

267
00:08:48,560 --> 00:08:52,240
distinguish between them. They all look

268
00:08:49,920 --> 00:08:53,440
kind of medium risk. Uh but it turns out

269
00:08:52,240 --> 00:08:54,880
that doctors actually have a lot of

270
00:08:53,440 --> 00:08:56,320
valuable information that is unavailable

271
00:08:54,880 --> 00:08:57,839
to our algorithms. And so what we're

272
00:08:56,320 --> 00:08:59,959
able to do is combine these sources of

273
00:08:57,839 --> 00:09:02,480
information to lead to better overall

274
00:08:59,959 --> 00:09:04,800
decisions. Now I mentioned that we're

275
00:09:02,480 --> 00:09:07,040
able to put these sort of the trade-offs

276
00:09:04,800 --> 00:09:08,399
at play uh and and try to figure out

277
00:09:07,040 --> 00:09:10,480
what are the right ways to make these

278
00:09:08,399 --> 00:09:11,680
types of decisions. I'll give you an

279
00:09:10,480 --> 00:09:13,600
example of the type of thing that you're

280
00:09:11,680 --> 00:09:16,880
able to do in this sort of framework. I

281
00:09:13,600 --> 00:09:18,080
mentioned to you that um true positives

282
00:09:16,880 --> 00:09:19,519
are really the thing that are the most

283
00:09:18,080 --> 00:09:21,760
important here, right? For all the

284
00:09:19,519 --> 00:09:22,880
patients uh who are really at high risk,

285
00:09:21,760 --> 00:09:25,760
those are the people that we want to be

286
00:09:22,880 --> 00:09:29,360
keeping in the ER. And so you should see

287
00:09:25,760 --> 00:09:31,279
each dot on this plot is a way that we

288
00:09:29,360 --> 00:09:33,279
can combine humans and algorithms to

289
00:09:31,279 --> 00:09:36,000
make decisions. It is a policy that a

290
00:09:33,279 --> 00:09:37,200
hospital could set. The the points over

291
00:09:36,000 --> 00:09:38,320
at the top are the ones that have really

292
00:09:37,200 --> 00:09:40,160
high true positive rates. These are the

293
00:09:38,320 --> 00:09:43,600
ones that hospitals are most likely to

294
00:09:40,160 --> 00:09:46,080
want to adopt. And what we can show is

295
00:09:43,600 --> 00:09:47,440
without making any additional mistakes

296
00:09:46,080 --> 00:09:49,040
with just matching exactly the

297
00:09:47,440 --> 00:09:50,800
performance that humans already have, we

298
00:09:49,040 --> 00:09:52,399
can automate some large fraction of c

299
00:09:50,800 --> 00:09:54,240
cases particularly the ones who are high

300
00:09:52,399 --> 00:09:55,680
risk. And what this allows us to do is

301
00:09:54,240 --> 00:09:56,959
maintain the quality of our decisions.

302
00:09:55,680 --> 00:09:58,800
We're not putting any additional

303
00:09:56,959 --> 00:10:01,800
patients at risk, but actually manage

304
00:09:58,800 --> 00:10:03,839
the trade-offs and resources

305
00:10:01,800 --> 00:10:05,839
effectively. Okay. As we go on, I'll

306
00:10:03,839 --> 00:10:08,000
I'll talk a little bit about employment

307
00:10:05,839 --> 00:10:09,839
decisions and how they differ from from

308
00:10:08,000 --> 00:10:11,040
medical triage decisions. And we'll talk

309
00:10:09,839 --> 00:10:12,839
a little bit about what are the the

310
00:10:11,040 --> 00:10:15,440
concerns at play

311
00:10:12,839 --> 00:10:16,959
here. Many of you may be using these

312
00:10:15,440 --> 00:10:19,279
types of algorithms. Maybe you're on the

313
00:10:16,959 --> 00:10:21,120
receiving end uh of of this pile of

314
00:10:19,279 --> 00:10:22,640
resumes that you have to sift through or

315
00:10:21,120 --> 00:10:24,320
you might be going through interviews

316
00:10:22,640 --> 00:10:26,320
yourself and you might be seeing AI

317
00:10:24,320 --> 00:10:28,079
interviewers uh on the other side. Okay,

318
00:10:26,320 --> 00:10:30,480
this is becoming increasingly common.

319
00:10:28,079 --> 00:10:31,600
And you know, it's a it's a cliche at

320
00:10:30,480 --> 00:10:32,880
this point in the business world.

321
00:10:31,600 --> 00:10:34,800
there's too many applications and yet

322
00:10:32,880 --> 00:10:36,880
it's impossible to find good talent. And

323
00:10:34,800 --> 00:10:38,320
on the candidate side, you go through

324
00:10:36,880 --> 00:10:39,920
hundreds of interviews and you never get

325
00:10:38,320 --> 00:10:41,360
a single job offer and you may not ever

326
00:10:39,920 --> 00:10:44,000
talk to a human in the first place,

327
00:10:41,360 --> 00:10:45,360
right? So somehow we've we've landed an

328
00:10:44,000 --> 00:10:46,920
equilibrium that nobody seems to be

329
00:10:45,360 --> 00:10:49,839
particularly happy

330
00:10:46,920 --> 00:10:51,839
with. So what are the risks in using AI

331
00:10:49,839 --> 00:10:54,560
for employment decisions? There are

332
00:10:51,839 --> 00:10:56,079
tons, right? This is me spending five

333
00:10:54,560 --> 00:10:57,600
minutes writing the ones I could think

334
00:10:56,079 --> 00:10:59,839
of off the top of my head on a slide.

335
00:10:57,600 --> 00:11:02,000
This is by no means exhaustive, right?

336
00:10:59,839 --> 00:11:03,519
Uh early on a lot of the concerns were

337
00:11:02,000 --> 00:11:05,839
around discrimination and that's still

338
00:11:03,519 --> 00:11:07,519
remains true. But there's also just

339
00:11:05,839 --> 00:11:08,720
quality considerations. Are candidates

340
00:11:07,519 --> 00:11:10,640
having bad experiences? Are you

341
00:11:08,720 --> 00:11:12,320
rejecting high quality candidates? Uh

342
00:11:10,640 --> 00:11:14,240
there's concerns of anti-competitive

343
00:11:12,320 --> 00:11:16,640
behavior as more and more firms adopt

344
00:11:14,240 --> 00:11:18,480
the same hiring algorithms. There's also

345
00:11:16,640 --> 00:11:20,399
just a lack of signal and AI generated

346
00:11:18,480 --> 00:11:22,640
resumes. Perhaps a lack of signal and AI

347
00:11:20,399 --> 00:11:24,320
conducted interviews. Today I'm going to

348
00:11:22,640 --> 00:11:26,959
talk about discrimination in our work

349
00:11:24,320 --> 00:11:28,880
there. But these are just there. The

350
00:11:26,959 --> 00:11:30,079
risks are vast here. Many of them may be

351
00:11:28,880 --> 00:11:33,000
small, but they're still worth thinking

352
00:11:30,079 --> 00:11:35,240
about and wondering if we can do

353
00:11:33,000 --> 00:11:37,120
better.

354
00:11:35,240 --> 00:11:39,680
Now, one of the things that you might

355
00:11:37,120 --> 00:11:41,440
want to measure here is whether people

356
00:11:39,680 --> 00:11:42,440
are discriminating in making employment

357
00:11:41,440 --> 00:11:44,640
decisions with

358
00:11:42,440 --> 00:11:45,680
AI. Now, there's a couple ways of

359
00:11:44,640 --> 00:11:47,279
thinking about this. I'm not going to

360
00:11:45,680 --> 00:11:49,040
give you the the the history of

361
00:11:47,279 --> 00:11:50,160
discrimination law in the United States,

362
00:11:49,040 --> 00:11:51,279
but you should think of there's

363
00:11:50,160 --> 00:11:54,480
basically two ways that you can

364
00:11:51,279 --> 00:11:55,920
discriminate as an employer. One is to

365
00:11:54,480 --> 00:11:57,120
explicitly treat people differently

366
00:11:55,920 --> 00:11:59,560
based on their demographic

367
00:11:57,120 --> 00:12:02,519
characteristics or protected uh

368
00:11:59,560 --> 00:12:04,720
characteristics and the other is to have

369
00:12:02,519 --> 00:12:07,440
unjustifiably different selection rates

370
00:12:04,720 --> 00:12:10,399
across different demographic groups um

371
00:12:07,440 --> 00:12:11,839
or to forego a less discrimin less

372
00:12:10,399 --> 00:12:14,880
discriminatory alternative when you

373
00:12:11,839 --> 00:12:16,560
could have used one. Okay. Now what

374
00:12:14,880 --> 00:12:18,880
we've started to do and what the law has

375
00:12:16,560 --> 00:12:20,160
started to require is that we audit AI

376
00:12:18,880 --> 00:12:21,920
models to see if they're going to

377
00:12:20,160 --> 00:12:23,440
discriminate. Right? One of the benefits

378
00:12:21,920 --> 00:12:25,040
of using AI, which you couldn't do with

379
00:12:23,440 --> 00:12:26,720
your human recruiters, is to actually

380
00:12:25,040 --> 00:12:27,760
just test them before you put them into

381
00:12:26,720 --> 00:12:29,360
practice. Right? When you have a new

382
00:12:27,760 --> 00:12:30,959
recruiter, you can't really predict what

383
00:12:29,360 --> 00:12:32,959
they're going to do until they actually

384
00:12:30,959 --> 00:12:34,560
make decisions on live candidates. This

385
00:12:32,959 --> 00:12:36,000
isn't necessarily true for AI models,

386
00:12:34,560 --> 00:12:37,600
though. We can stress test them before

387
00:12:36,000 --> 00:12:38,959
we even put them into production. So, we

388
00:12:37,600 --> 00:12:41,279
have some sense of what their operating

389
00:12:38,959 --> 00:12:43,839
characteristics are going to be. So,

390
00:12:41,279 --> 00:12:45,600
this is a new opportunity that we have,

391
00:12:43,839 --> 00:12:47,920
right? We can do these so-called

392
00:12:45,600 --> 00:12:49,519
pre-eployment audits. And in fact, some

393
00:12:47,920 --> 00:12:50,720
jurisdictions in the United States have

394
00:12:49,519 --> 00:12:52,040
started requiring them, includ in

395
00:12:50,720 --> 00:12:54,800
including New York

396
00:12:52,040 --> 00:12:56,959
City. But with this new opportunity also

397
00:12:54,800 --> 00:12:57,959
comes a new challenge. How should we

398
00:12:56,959 --> 00:12:59,680
measure

399
00:12:57,959 --> 00:13:01,440
discrimination? Is there just an

400
00:12:59,680 --> 00:13:03,120
off-the-shelf measure that we can use?

401
00:13:01,440 --> 00:13:04,320
What data sets should we use to evaluate

402
00:13:03,120 --> 00:13:05,680
them on? How should we conduct

403
00:13:04,320 --> 00:13:07,279
litigation around employment

404
00:13:05,680 --> 00:13:09,760
discrimination as it pertains to

405
00:13:07,279 --> 00:13:11,440
algorithms?

406
00:13:09,760 --> 00:13:13,519
One of the key observations that has

407
00:13:11,440 --> 00:13:14,959
come out in the recent years is that we

408
00:13:13,519 --> 00:13:16,560
often think of machine learning as

409
00:13:14,959 --> 00:13:17,839
producing the right model given the

410
00:13:16,560 --> 00:13:19,360
data. Right? The goal of machine

411
00:13:17,839 --> 00:13:20,800
learning process is you hand it some

412
00:13:19,360 --> 00:13:22,240
data, you get back a model, you don't

413
00:13:20,800 --> 00:13:24,560
really ask too many questions about how

414
00:13:22,240 --> 00:13:26,000
you got there. But it turns out there's

415
00:13:24,560 --> 00:13:27,920
actually lots of different ways to

416
00:13:26,000 --> 00:13:28,880
construct a good model. And one of the

417
00:13:27,920 --> 00:13:31,440
things that we've learned how to do in

418
00:13:28,880 --> 00:13:34,399
I'd say the past 3 to 5 years is search

419
00:13:31,440 --> 00:13:36,320
over the set of good models the sort the

420
00:13:34,399 --> 00:13:38,560
set of accurate models for ones that

421
00:13:36,320 --> 00:13:40,639
have the uh characteristics that you

422
00:13:38,560 --> 00:13:42,320
want on other domains. So for instance

423
00:13:40,639 --> 00:13:43,680
if you want an accurate model that also

424
00:13:42,320 --> 00:13:45,440
doesn't discriminate that is something

425
00:13:43,680 --> 00:13:48,480
you can specify in your model search

426
00:13:45,440 --> 00:13:50,240
that isn't just as simple as maximize uh

427
00:13:48,480 --> 00:13:52,800
the utility of the model just from an

428
00:13:50,240 --> 00:13:54,240
accuracy perspective. This has lots of

429
00:13:52,800 --> 00:13:57,839
legal implications which I'm not going

430
00:13:54,240 --> 00:13:59,440
to to bore you with too much. But we've

431
00:13:57,839 --> 00:14:00,639
been talking a lot to regulators

432
00:13:59,440 --> 00:14:03,199
specifically under the previous

433
00:14:00,639 --> 00:14:05,519
administration at the EEOC around what

434
00:14:03,199 --> 00:14:07,279
the standards should be for litigation,

435
00:14:05,519 --> 00:14:08,720
right? How do you construct policy

436
00:14:07,279 --> 00:14:10,160
around the capabilities of machine

437
00:14:08,720 --> 00:14:10,959
learning and how does that change the

438
00:14:10,160 --> 00:14:13,279
way that we should think about

439
00:14:10,959 --> 00:14:15,440
discrimination law? Can we use

440
00:14:13,279 --> 00:14:16,880
statistical tests in courtrooms and to

441
00:14:15,440 --> 00:14:18,320
what extent can we rely on them when

442
00:14:16,880 --> 00:14:20,399
we're talking about machine learning?

443
00:14:18,320 --> 00:14:22,000
These are all open questions just raised

444
00:14:20,399 --> 00:14:25,079
by the possibility of using machine

445
00:14:22,000 --> 00:14:27,120
learning in a previously human-driven

446
00:14:25,079 --> 00:14:29,279
setting. Okay, I'm going to end it on an

447
00:14:27,120 --> 00:14:30,560
example which hopefully will will give

448
00:14:29,279 --> 00:14:33,360
you guys all an opportunity to

449
00:14:30,560 --> 00:14:34,639
participate before you you go off. One

450
00:14:33,360 --> 00:14:36,160
of the things that's been really

451
00:14:34,639 --> 00:14:38,399
interesting to me just from a personal

452
00:14:36,160 --> 00:14:39,839
level uh over the past few years is that

453
00:14:38,399 --> 00:14:42,360
we've moved from this paradigm of using

454
00:14:39,839 --> 00:14:44,399
AI for prediction to using it for

455
00:14:42,360 --> 00:14:46,000
production. And the set of things that

456
00:14:44,399 --> 00:14:48,160
you can do from a production standpoint

457
00:14:46,000 --> 00:14:50,480
are just vast, right? there's so much

458
00:14:48,160 --> 00:14:52,480
room to build uh and to produce things

459
00:14:50,480 --> 00:14:53,839
with generative AI and I've been

460
00:14:52,480 --> 00:14:55,839
particularly interested in how this

461
00:14:53,839 --> 00:14:57,839
changes the landscape of creativity.

462
00:14:55,839 --> 00:14:59,839
Okay, now this is a very broad topic. I

463
00:14:57,839 --> 00:15:02,000
don't know enough about uh the sort of

464
00:14:59,839 --> 00:15:04,240
creativity literature to give a sort of

465
00:15:02,000 --> 00:15:07,279
expert informed view here. But one of

466
00:15:04,240 --> 00:15:09,199
the things that we've seen in empirical

467
00:15:07,279 --> 00:15:12,120
studies over the last couple of years is

468
00:15:09,199 --> 00:15:15,360
a surprisingly consistent

469
00:15:12,120 --> 00:15:16,800
fact. The the short version of it is

470
00:15:15,360 --> 00:15:19,680
when you get people in a room and you

471
00:15:16,800 --> 00:15:21,760
ask them to think creatively. Okay, you

472
00:15:19,680 --> 00:15:23,279
can do RCTs where you say this group of

473
00:15:21,760 --> 00:15:25,040
people does not get access to AI and

474
00:15:23,279 --> 00:15:26,560
that group of people does. And so you

475
00:15:25,040 --> 00:15:27,920
look at the people who who did not have

476
00:15:26,560 --> 00:15:30,320
access to AI and they gave you some

477
00:15:27,920 --> 00:15:32,959
ideas and those ideas look good. If you

478
00:15:30,320 --> 00:15:35,040
give people access uh to AI, it looks

479
00:15:32,959 --> 00:15:36,680
like they actually produce more ideas in

480
00:15:35,040 --> 00:15:39,120
isolation or they're more creative in

481
00:15:36,680 --> 00:15:40,880
isolation. But somehow when you take the

482
00:15:39,120 --> 00:15:43,440
set of all the things that they produce,

483
00:15:40,880 --> 00:15:45,120
that set is smaller. Essentially, what

484
00:15:43,440 --> 00:15:46,480
they're doing is every single person is

485
00:15:45,120 --> 00:15:47,839
just coming up with the same few ideas

486
00:15:46,480 --> 00:15:49,680
over and over again because that's what

487
00:15:47,839 --> 00:15:53,040
they asked ChatgBT for and they all got

488
00:15:49,680 --> 00:15:54,000
the same answers. Okay, now this isn't

489
00:15:53,040 --> 00:15:55,759
something that could have really

490
00:15:54,000 --> 00:15:57,360
happened before. We didn't have an

491
00:15:55,759 --> 00:15:59,360
artifact that would cause people to

492
00:15:57,360 --> 00:16:01,839
converge on their beliefs or converge on

493
00:15:59,360 --> 00:16:03,759
the things they were producing. But now

494
00:16:01,839 --> 00:16:06,000
we have this this sort of monoculture

495
00:16:03,759 --> 00:16:07,199
created by the use of AI, right?

496
00:16:06,000 --> 00:16:09,279
Everybody is going through the same

497
00:16:07,199 --> 00:16:10,959
centralized processes and so everybody

498
00:16:09,279 --> 00:16:13,680
is getting correlated or identical

499
00:16:10,959 --> 00:16:15,839
outputs from them. And so the question

500
00:16:13,680 --> 00:16:17,839
was to what extent should we expect this

501
00:16:15,839 --> 00:16:21,160
to happen in the wild and how is this

502
00:16:17,839 --> 00:16:24,320
going to affect the market for AI

503
00:16:21,160 --> 00:16:26,320
models? Okay, I'll give you a a really

504
00:16:24,320 --> 00:16:28,000
nice metaphor that I've been toying with

505
00:16:26,320 --> 00:16:29,759
for the last few years that's been sort

506
00:16:28,000 --> 00:16:32,320
of an interesting way for me to think

507
00:16:29,759 --> 00:16:33,839
about this. And that metaphor is a word

508
00:16:32,320 --> 00:16:36,079
game. It's called categories. Many of

509
00:16:33,839 --> 00:16:39,199
you may have played this. It is a game

510
00:16:36,079 --> 00:16:40,720
that re rewards both correctness and

511
00:16:39,199 --> 00:16:43,440
creativity. So effectively what happens

512
00:16:40,720 --> 00:16:45,279
is I give everybody in the room a letter

513
00:16:43,440 --> 00:16:47,600
and a category and say can you come up

514
00:16:45,279 --> 00:16:48,959
with an object that starts with the

515
00:16:47,600 --> 00:16:50,800
right letter and matches that category.

516
00:16:48,959 --> 00:16:52,560
So if I said you know the letter is L

517
00:16:50,800 --> 00:16:54,560
and the the category is things with

518
00:16:52,560 --> 00:16:55,759
tails. Somebody shout out a possible

519
00:16:54,560 --> 00:16:58,480
answer to this. What is a thing with a

520
00:16:55,759 --> 00:17:00,000
tail starting with an L? Great. Lion,

521
00:16:58,480 --> 00:17:02,639
leopard. These are all things that

522
00:17:00,000 --> 00:17:03,680
language models would say. Uh and so you

523
00:17:02,639 --> 00:17:05,439
it turns out you can just do this,

524
00:17:03,680 --> 00:17:07,120
right? And this is a way to assess the

525
00:17:05,439 --> 00:17:09,199
sort of creative capabilities of

526
00:17:07,120 --> 00:17:11,439
language models and a benchmark by which

527
00:17:09,199 --> 00:17:13,520
we can compare them. So I'll give you an

528
00:17:11,439 --> 00:17:15,760
example. If you try actually running a

529
00:17:13,520 --> 00:17:17,439
language model, many of you may be

530
00:17:15,760 --> 00:17:19,039
familiar with the there's sort of a a

531
00:17:17,439 --> 00:17:20,319
parameter. There's many parameters with

532
00:17:19,039 --> 00:17:21,439
language models that you get to tune.

533
00:17:20,319 --> 00:17:22,880
One of them is known as a temperature

534
00:17:21,439 --> 00:17:24,319
parameter which basically allows you to

535
00:17:22,880 --> 00:17:25,679
diversify the set of things that you're

536
00:17:24,319 --> 00:17:28,079
sampling from. It sort of increases the

537
00:17:25,679 --> 00:17:29,520
randomness in the model. So if you run a

538
00:17:28,079 --> 00:17:30,960
model at a temperature of zero, it is

539
00:17:29,520 --> 00:17:33,120
deterministic. It always gives you the

540
00:17:30,960 --> 00:17:34,480
same thing over and over again. It turns

541
00:17:33,120 --> 00:17:35,840
out for this particular language model,

542
00:17:34,480 --> 00:17:37,840
if you ask it this question, you

543
00:17:35,840 --> 00:17:39,600
deterministically get the answer lemur.

544
00:17:37,840 --> 00:17:42,799
That is the thing that starts with an L

545
00:17:39,600 --> 00:17:45,039
and has a tail. And as you start to turn

546
00:17:42,799 --> 00:17:47,520
this knob up, you get a little bit more

547
00:17:45,039 --> 00:17:49,760
creativity, right? So, lion starts to

548
00:17:47,520 --> 00:17:51,440
sneak into the mix here. And as you turn

549
00:17:49,760 --> 00:17:54,320
it up further and further, you get

550
00:17:51,440 --> 00:17:56,240
things like lizard and uh llamas and so

551
00:17:54,320 --> 00:17:58,320
on. And we can keep going with this. But

552
00:17:56,240 --> 00:18:00,200
what you'll notice is as you turn this

553
00:17:58,320 --> 00:18:02,400
temperature up, as you ask for more

554
00:18:00,200 --> 00:18:05,039
creativity, you lose quality. So the

555
00:18:02,400 --> 00:18:07,440
answers in red are the ones that uh my

556
00:18:05,039 --> 00:18:09,600
personal verifier uh said does not

557
00:18:07,440 --> 00:18:11,039
really have a tail like a ladybird. And

558
00:18:09,600 --> 00:18:13,600
so you see that there's this trade-off

559
00:18:11,039 --> 00:18:15,679
between quality and creativity, right?

560
00:18:13,600 --> 00:18:17,520
The more creative you try to get these

561
00:18:15,679 --> 00:18:20,480
models to be, the less accurate they

562
00:18:17,520 --> 00:18:21,760
are. All right? And this is okay, right?

563
00:18:20,480 --> 00:18:23,360
Sometimes we want a balance between

564
00:18:21,760 --> 00:18:25,520
creativity and accuracy. we're willing

565
00:18:23,360 --> 00:18:26,880
to to handle some mistakes because we

566
00:18:25,520 --> 00:18:29,200
actually want a broader set of things to

567
00:18:26,880 --> 00:18:31,120
come from it. Okay. And so that's the

568
00:18:29,200 --> 00:18:33,679
thing that we're going to try to to ask

569
00:18:31,120 --> 00:18:35,200
here. To what extent does this quality

570
00:18:33,679 --> 00:18:36,880
creativity trade-off manifest in

571
00:18:35,200 --> 00:18:39,280
practice? And is it perhaps different

572
00:18:36,880 --> 00:18:41,280
across different models? And without

573
00:18:39,280 --> 00:18:43,039
going too much into the the details, you

574
00:18:41,280 --> 00:18:44,880
can actually show that this sort of

575
00:18:43,039 --> 00:18:46,799
market has a naturally competitive

576
00:18:44,880 --> 00:18:50,000
structure. And what you should expect to

577
00:18:46,799 --> 00:18:52,320
see is creativity is rewarded as there

578
00:18:50,000 --> 00:18:54,559
is stronger competition. And you might

579
00:18:52,320 --> 00:18:56,080
find that a model that does really well

580
00:18:54,559 --> 00:18:57,840
according to your benchmark isn't

581
00:18:56,080 --> 00:18:59,919
actually the one that does better uh in

582
00:18:57,840 --> 00:19:02,559
a more sort of competitive world where

583
00:18:59,919 --> 00:19:04,240
more creativity is required. So without

584
00:19:02,559 --> 00:19:06,400
explaining too much what this plot says,

585
00:19:04,240 --> 00:19:07,799
here's a simulated world sim you can

586
00:19:06,400 --> 00:19:10,080
call it a game of

587
00:19:07,799 --> 00:19:13,120
categories where I pit two models

588
00:19:10,080 --> 00:19:15,200
against each other. And you should think

589
00:19:13,120 --> 00:19:17,600
of each of these axis representing the

590
00:19:15,200 --> 00:19:19,080
market share that each model gets. Okay?

591
00:19:17,600 --> 00:19:21,760
And what you can see

592
00:19:19,080 --> 00:19:23,840
is for this for a small number of

593
00:19:21,760 --> 00:19:25,360
competitors in this marketplace, you end

594
00:19:23,840 --> 00:19:26,640
up with one model dominating, right?

595
00:19:25,360 --> 00:19:29,440
This is the one that takes all the

596
00:19:26,640 --> 00:19:30,720
market share. But as there's a more and

597
00:19:29,440 --> 00:19:32,400
more competitive marketplace, a

598
00:19:30,720 --> 00:19:34,240
different model takes over. And what

599
00:19:32,400 --> 00:19:35,919
this speaks to is that we haven't really

600
00:19:34,240 --> 00:19:38,240
done a good job of benchmarking these

601
00:19:35,919 --> 00:19:40,400
models according to their quality uh in

602
00:19:38,240 --> 00:19:41,600
creative or competitive settings. We're

603
00:19:40,400 --> 00:19:43,600
really good at benchmarking them in

604
00:19:41,600 --> 00:19:45,520
isolation. And one of the things that

605
00:19:43,600 --> 00:19:47,600
we're working towards uh in in the near

606
00:19:45,520 --> 00:19:49,039
future is try to try to figure out how

607
00:19:47,600 --> 00:19:51,039
do you get these things to balance both

608
00:19:49,039 --> 00:19:52,400
quality and creativity simultaneously

609
00:19:51,039 --> 00:19:55,520
and to what extent is the market going

610
00:19:52,400 --> 00:19:57,280
to reward you for doing that. Okay. So I

611
00:19:55,520 --> 00:19:59,440
know I'm out of time. I'll stop here.

612
00:19:57,280 --> 00:20:01,440
But for me the takeaways of this sort of

613
00:19:59,440 --> 00:20:02,880
broad line of research are it's

614
00:20:01,440 --> 00:20:05,360
important to try to identify these

615
00:20:02,880 --> 00:20:07,919
highle risks or new phenomena that can

616
00:20:05,360 --> 00:20:09,760
occur when we put AI in society. Right?

617
00:20:07,919 --> 00:20:11,440
Some things occur just because when you

618
00:20:09,760 --> 00:20:13,360
use AI in a setting that you weren't

619
00:20:11,440 --> 00:20:15,200
before, stuff that was previously way

620
00:20:13,360 --> 00:20:16,960
too expensive now becomes feasible.

621
00:20:15,200 --> 00:20:18,880
Right? One example that has been on my

622
00:20:16,960 --> 00:20:21,200
mind recently is what happens when every

623
00:20:18,880 --> 00:20:22,960
child in the country gets access to both

624
00:20:21,200 --> 00:20:24,559
a homework aid and a personal tutor for

625
00:20:22,960 --> 00:20:25,760
for basically for free all the time.

626
00:20:24,559 --> 00:20:27,039
Right? Now, you could have said, well,

627
00:20:25,760 --> 00:20:28,240
this could have happened before if we

628
00:20:27,039 --> 00:20:29,600
had a lot of money. We could have paid

629
00:20:28,240 --> 00:20:31,200
for private tutors for everyone. But

630
00:20:29,600 --> 00:20:33,039
this wasn't happening at scale. It was

631
00:20:31,200 --> 00:20:34,640
previously too expensive. But now this

632
00:20:33,039 --> 00:20:36,240
new phenomena is possible and it's going

633
00:20:34,640 --> 00:20:38,720
to create social effects. how should we

634
00:20:36,240 --> 00:20:39,840
analyze those? There's also systemic

635
00:20:38,720 --> 00:20:41,600
effects that come with this sort of

636
00:20:39,840 --> 00:20:43,440
coordination at scale as we're all using

637
00:20:41,600 --> 00:20:45,919
the same AI models. Is it going to

638
00:20:43,440 --> 00:20:47,760
decrease our diversity of thought uh of

639
00:20:45,919 --> 00:20:50,240
our scientific production? And how do we

640
00:20:47,760 --> 00:20:52,480
maintain that diversity going forwards?

641
00:20:50,240 --> 00:20:53,919
And the last thing I'll end on is for me

642
00:20:52,480 --> 00:20:55,760
personally, it's been super critical to

643
00:20:53,919 --> 00:20:57,280
integrate domain expertise beyond

644
00:20:55,760 --> 00:20:59,039
computer science. As a computer

645
00:20:57,280 --> 00:21:00,640
scientist, I was only trained to think a

646
00:20:59,039 --> 00:21:03,440
certain way. And the more I've talked to

647
00:21:00,640 --> 00:21:04,880
economists, to doctors, to lawyers, I've

648
00:21:03,440 --> 00:21:07,200
learned different ways of looking at the

649
00:21:04,880 --> 00:21:08,640
world and as a result, different ways of

650
00:21:07,200 --> 00:21:10,080
looking at the way that AI is going to

651
00:21:08,640 --> 00:21:11,360
interact with it. All right, I'll stop

652
00:21:10,080 --> 00:21:13,919
there. Happy to take any questions if

653
00:21:11,360 --> 00:21:17,159
you have any. And thanks for coming.

654
00:21:13,919 --> 00:21:17,159
Thank you.

655
00:21:19,360 --> 00:21:23,840
How do you approach data privacy issues?

656
00:21:21,440 --> 00:21:25,280
Good question. So data privacy when

657
00:21:23,840 --> 00:21:29,760
you're using AI in a healthcare setting

658
00:21:25,280 --> 00:21:32,000
is obviously complicated. Thankfully, we

659
00:21:29,760 --> 00:21:34,240
we have basically two main protections

660
00:21:32,000 --> 00:21:36,000
uh that we have to follow. One is that

661
00:21:34,240 --> 00:21:37,760
there's federal law governing healthcare

662
00:21:36,000 --> 00:21:40,080
records and so we have to maintain HIPPA

663
00:21:37,760 --> 00:21:42,640
compliance with whatever we do. Usually

664
00:21:40,080 --> 00:21:44,240
that involves properly anonymizing data

665
00:21:42,640 --> 00:21:46,320
and stripping as much of it as we can

666
00:21:44,240 --> 00:21:48,400
before we start using it. Uh only

667
00:21:46,320 --> 00:21:49,919
accessing it on servers that belong to

668
00:21:48,400 --> 00:21:51,760
the hospital. And so for some of this

669
00:21:49,919 --> 00:21:53,360
work, we actually have to have our

670
00:21:51,760 --> 00:21:54,960
students hired by the hospitals that we

671
00:21:53,360 --> 00:21:56,240
work with. So they become become

672
00:21:54,960 --> 00:21:58,400
employees and they are subject to all

673
00:21:56,240 --> 00:22:00,159
the policies of the hospital. The second

674
00:21:58,400 --> 00:22:02,880
thing that we're we're governed by is uh

675
00:22:00,159 --> 00:22:04,799
IRBs. So when we run experiments, we

676
00:22:02,880 --> 00:22:06,159
submit IRBs that say here's what we plan

677
00:22:04,799 --> 00:22:07,840
to do. Here's why we think it's in the

678
00:22:06,159 --> 00:22:09,280
benefit of the patients. And we end up

679
00:22:07,840 --> 00:22:10,880
just being somewhat conservative with

680
00:22:09,280 --> 00:22:12,960
the things that we do as a result. And I

681
00:22:10,880 --> 00:22:14,559
think that's a good thing. There are

682
00:22:12,960 --> 00:22:16,320
techniques from a technical perspective

683
00:22:14,559 --> 00:22:17,760
to try to preserve privacy in people's

684
00:22:16,320 --> 00:22:19,600
data. Like you add some random noise

685
00:22:17,760 --> 00:22:21,600
into a data set uh that gives you

686
00:22:19,600 --> 00:22:23,360
additional protection beyond uh the

687
00:22:21,600 --> 00:22:24,720
anonymization. But for the applications

688
00:22:23,360 --> 00:22:26,880
that we have and the ones that I talked

689
00:22:24,720 --> 00:22:29,120
about, anonymized data is is actually

690
00:22:26,880 --> 00:22:29,120
good

691
00:22:29,799 --> 00:22:34,000
enough. Is the creativity problem a

692
00:22:32,320 --> 00:22:36,559
function of us not having the right

693
00:22:34,000 --> 00:22:39,760
theory of mind and philosophy how to for

694
00:22:36,559 --> 00:22:39,760
how to engage with these world

695
00:22:40,520 --> 00:22:45,360
models? It's a couple of things. So yes,

696
00:22:44,000 --> 00:22:46,360
I don't think we have the right theory

697
00:22:45,360 --> 00:22:48,640
of

698
00:22:46,360 --> 00:22:50,000
mind. The other thing I'll say is we

699
00:22:48,640 --> 00:22:51,520
don't really have the right objective

700
00:22:50,000 --> 00:22:54,400
functions by which to measure these

701
00:22:51,520 --> 00:22:56,559
models. And what I mean by that is the

702
00:22:54,400 --> 00:22:58,960
way that we build models, including all

703
00:22:56,559 --> 00:23:01,039
of the big AI labs, is you go out into

704
00:22:58,960 --> 00:23:03,360
the wild and you find here are examples

705
00:23:01,039 --> 00:23:05,200
of ways that people are using models,

706
00:23:03,360 --> 00:23:06,480
right? Maybe it's benchmark data that

707
00:23:05,200 --> 00:23:08,000
people have collected. Maybe it's your

708
00:23:06,480 --> 00:23:09,600
own chat transcripts from all the people

709
00:23:08,000 --> 00:23:12,480
that you've interacted with. And so

710
00:23:09,600 --> 00:23:13,919
those if you optimize for those, you get

711
00:23:12,480 --> 00:23:16,480
good performance on certain types of

712
00:23:13,919 --> 00:23:18,480
tasks. What's really hard to do is

713
00:23:16,480 --> 00:23:20,080
predict what are the future use cases

714
00:23:18,480 --> 00:23:21,600
that we haven't yet seen. what are

715
00:23:20,080 --> 00:23:23,679
people going to want to do or what would

716
00:23:21,600 --> 00:23:25,120
they do if we got really good at it and

717
00:23:23,679 --> 00:23:26,159
how would we design a model that's

718
00:23:25,120 --> 00:23:27,760
really good for those types of

719
00:23:26,159 --> 00:23:29,760
applications. So if you look at a lot of

720
00:23:27,760 --> 00:23:32,240
the benchmarks that we have today, they

721
00:23:29,760 --> 00:23:34,240
look like ELSAT questions and multiple

722
00:23:32,240 --> 00:23:36,000
choice questions. They're basically

723
00:23:34,240 --> 00:23:38,080
things that we've already created a lot

724
00:23:36,000 --> 00:23:39,360
of uh knowledge based questions to

725
00:23:38,080 --> 00:23:40,960
evaluate students. We're just going to

726
00:23:39,360 --> 00:23:42,799
use those to to evaluate language

727
00:23:40,960 --> 00:23:44,240
models. And that's a great start, but

728
00:23:42,799 --> 00:23:46,000
that's not really how people are going

729
00:23:44,240 --> 00:23:48,400
to be using these models, right? the

730
00:23:46,000 --> 00:23:51,760
primary use case uh for a large language

731
00:23:48,400 --> 00:23:53,360
model isn't to pass the LSAT for you. So

732
00:23:51,760 --> 00:23:54,880
I think a big problem that we have is

733
00:23:53,360 --> 00:23:57,200
just trying to figure out how do we

734
00:23:54,880 --> 00:23:59,120
measure what it means to perform well.

735
00:23:57,200 --> 00:24:01,360
And as I talk to my friends who work at

736
00:23:59,120 --> 00:24:03,039
the big AI labs, this has been a

737
00:24:01,360 --> 00:24:04,320
predominant concern that they have. How

738
00:24:03,039 --> 00:24:05,840
do we evaluate? How do we make sure

739
00:24:04,320 --> 00:24:07,360
these things are doing well? How do we

740
00:24:05,840 --> 00:24:09,360
make sure they're not going to go uh off

741
00:24:07,360 --> 00:24:11,360
the rails or or bypass our guardrails in

742
00:24:09,360 --> 00:24:13,440
some way? Evaluation for these models is

743
00:24:11,360 --> 00:24:14,960
really hard and it involves aligning

744
00:24:13,440 --> 00:24:16,559
these models with our own values, right?

745
00:24:14,960 --> 00:24:18,480
So what do we consider valuable in this

746
00:24:16,559 --> 00:24:21,279
world? That's really hard to write down.

747
00:24:18,480 --> 00:24:25,200
It's hard to specify. So in my mind that

748
00:24:21,279 --> 00:24:27,440
is the big obstacle uh beyond behind

749
00:24:25,200 --> 00:24:30,320
incentivizing the type of models that we

750
00:24:27,440 --> 00:24:34,760
want to see in the world. Thank you very

751
00:24:30,320 --> 00:24:34,760
much Manish. Thank you.

