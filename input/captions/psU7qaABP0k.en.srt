1
00:00:00,000 --> 00:00:02,988
[MUSIC PLAYING]

2
00:00:02,988 --> 00:00:12,960

3
00:00:12,960 --> 00:00:15,800
MEGAN MITCHELL: Hello, and
good morning, everyone.

4
00:00:15,800 --> 00:00:17,460
How are you?

5
00:00:17,460 --> 00:00:18,460
Good morning.

6
00:00:18,460 --> 00:00:19,418
Good afternoon.

7
00:00:19,418 --> 00:00:19,960
Good evening.

8
00:00:19,960 --> 00:00:22,902
I guess it really
depends on where you are.

9
00:00:22,902 --> 00:00:23,610
I should qualify.

10
00:00:23,610 --> 00:00:26,670
It is morning here,
in the Boston area.

11
00:00:26,670 --> 00:00:28,510
And my name is Megan Mitchell.

12
00:00:28,510 --> 00:00:31,870
I am the acting director of
the Jameel World Education Lab,

13
00:00:31,870 --> 00:00:34,420
which is probably
familiar to many of you.

14
00:00:34,420 --> 00:00:38,470
But for those of you who are
joining us for the first time,

15
00:00:38,470 --> 00:00:40,810
the Jameel World Education
Lab here, at MIT,

16
00:00:40,810 --> 00:00:42,940
which we refer to as J-WEL--

17
00:00:42,940 --> 00:00:45,360
it's much easier on the tongue--

18
00:00:45,360 --> 00:00:47,400
is the world's portal.

19
00:00:47,400 --> 00:00:49,110
We consider it
the world's portal

20
00:00:49,110 --> 00:00:53,460
to MIT expertise and research,
learning, experimentation

21
00:00:53,460 --> 00:00:57,870
and, really, a laboratory
of educational innovation.

22
00:00:57,870 --> 00:01:00,780
We collaborate with ambitious
and committed institutions

23
00:01:00,780 --> 00:01:02,760
around the world.

24
00:01:02,760 --> 00:01:05,440
Most of them are our members.

25
00:01:05,440 --> 00:01:07,300
And hopefully, you're
joining us-- many

26
00:01:07,300 --> 00:01:09,820
of you are joining us from
there-- to bring our shared

27
00:01:09,820 --> 00:01:14,240
vision to life of education
designed for everyone to thrive,

28
00:01:14,240 --> 00:01:16,790
which is, I hope and
believe, if you're here,

29
00:01:16,790 --> 00:01:19,780
that's a mission that
resonates with you.

30
00:01:19,780 --> 00:01:22,960
And we do this through
different pillars,

31
00:01:22,960 --> 00:01:25,780
like our Pathways for Talent,
Architecting Learning,

32
00:01:25,780 --> 00:01:28,930
Campus as a Catalyst, and that
really shapes who we work with

33
00:01:28,930 --> 00:01:30,110
and how we work.

34
00:01:30,110 --> 00:01:36,890
But you're here today as part
of our series in AI innovation.

35
00:01:36,890 --> 00:01:38,950
And we started this
series, actually,

36
00:01:38,950 --> 00:01:40,780
back in the summer,
where we heard

37
00:01:40,780 --> 00:01:44,260
from our dean of digital
learning, Cynthia Breazeal.

38
00:01:44,260 --> 00:01:46,660
And she spoke about the
work that she's doing

39
00:01:46,660 --> 00:01:48,340
and the work happening at MIT.

40
00:01:48,340 --> 00:01:52,030
But we've also explored
some of the spin-outs that

41
00:01:52,030 --> 00:01:56,110
are coming out that are in the
nonprofit space that are doing

42
00:01:56,110 --> 00:01:59,530
impact-driven work,
but still at scale,

43
00:01:59,530 --> 00:02:03,970
and really thinking about how
to use AI to reach more learners

44
00:02:03,970 --> 00:02:06,670
or to reach more learners
efficiently and effectively.

45
00:02:06,670 --> 00:02:11,800
But today, we're bringing
you some folks who are--

46
00:02:11,800 --> 00:02:15,340
SPEAKER: [NON-ENGLISH SPEECH]

47
00:02:15,340 --> 00:02:19,210
MEGAN MITCHELL: --more coming
from the corporate innovation

48
00:02:19,210 --> 00:02:20,110
side.

49
00:02:20,110 --> 00:02:23,390
And we will hear today from
three different panelists,

50
00:02:23,390 --> 00:02:27,910
each who have prepared
presentations for you.

51
00:02:27,910 --> 00:02:30,940
And they are going to share
with you the work that they've

52
00:02:30,940 --> 00:02:31,520
been doing.

53
00:02:31,520 --> 00:02:37,300
So first, we will hear from
Deepak Verma of EnglishHelper.

54
00:02:37,300 --> 00:02:42,510
And English Helper
is really focused

55
00:02:42,510 --> 00:02:49,290
on bringing English
language learning to many--

56
00:02:49,290 --> 00:02:51,540
English language translation
to many different folks

57
00:02:51,540 --> 00:02:52,900
around the world.

58
00:02:52,900 --> 00:02:55,350
But now, they're looking
at, what can they

59
00:02:55,350 --> 00:03:00,420
do in leveraging AI and
technology to expand their reach

60
00:03:00,420 --> 00:03:04,860
and really broaden the already
really impactful work they're

61
00:03:04,860 --> 00:03:05,540
doing.

62
00:03:05,540 --> 00:03:10,410
He also serves as a partner
at ANOVA Spark Ventures.

63
00:03:10,410 --> 00:03:12,730
And so he's an early
stage investor.

64
00:03:12,730 --> 00:03:15,120
So really looking across
the spectrum of what's

65
00:03:15,120 --> 00:03:17,700
happening in the
technology space.

66
00:03:17,700 --> 00:03:22,510
He is really an innovator
across his career,

67
00:03:22,510 --> 00:03:25,290
and we're excited
to hear from him.

68
00:03:25,290 --> 00:03:27,730
He does have his
MBA from MIT Sloan,

69
00:03:27,730 --> 00:03:30,630
and he's joining us
from Bangalore today.

70
00:03:30,630 --> 00:03:34,800
Then, we will we'll turn
to Joy Dasgupta, who

71
00:03:34,800 --> 00:03:39,780
is the CEO of Gyan AI, and
they're a deep tech startup,

72
00:03:39,780 --> 00:03:42,850
using proprietary and
explainable AI language

73
00:03:42,850 --> 00:03:45,890
model for research and
content development.

74
00:03:45,890 --> 00:03:49,550
I'm interested in that
explainable qualifier there.

75
00:03:49,550 --> 00:03:53,120
But their organization serves
international universities,

76
00:03:53,120 --> 00:03:55,070
so I'm sure of interest
to many of you,

77
00:03:55,070 --> 00:03:59,660
but publishing houses,
consulting firms, corporations,

78
00:03:59,660 --> 00:04:03,220
so a lot of different
companies and the support

79
00:04:03,220 --> 00:04:06,070
or different groups in the
business-to-business side

80
00:04:06,070 --> 00:04:07,360
of things.

81
00:04:07,360 --> 00:04:11,242
And Joy has deep
experience and played

82
00:04:11,242 --> 00:04:12,700
in many different
senior leadership

83
00:04:12,700 --> 00:04:15,890
roles at different
Fortune 500 companies.

84
00:04:15,890 --> 00:04:21,190
And then we'll turn it over
to our final panelist, Jean

85
00:04:21,190 --> 00:04:27,670
Hammond, another MBA or another
alum from the Sloan School.

86
00:04:27,670 --> 00:04:30,340
And actually, someone I served
on a board with and works

87
00:04:30,340 --> 00:04:35,090
diligently with MIT students
here, at here at MIT,

88
00:04:35,090 --> 00:04:39,040
but also, in her
role leading what

89
00:04:39,040 --> 00:04:42,650
she calls a micro-venture
capital firm, LearnLaunch,

90
00:04:42,650 --> 00:04:45,580
so you can see why we
have her here today,

91
00:04:45,580 --> 00:04:49,670
and invests in early stage
educational technology firms.

92
00:04:49,670 --> 00:04:51,760
So really has a
great perspective

93
00:04:51,760 --> 00:04:57,170
on how the ed tech landscape has
shaped and evolved over time.

94
00:04:57,170 --> 00:04:59,530
But in addition to just
funding, she really

95
00:04:59,530 --> 00:05:01,690
invests in the development
of those companies,

96
00:05:01,690 --> 00:05:05,080
running an accelerator to
help those companies scale

97
00:05:05,080 --> 00:05:09,260
and has invested in more than
86 companies around the world.

98
00:05:09,260 --> 00:05:11,440
So, as you can see, a
great, great group of

99
00:05:11,440 --> 00:05:15,850
panelists to give us their
perspective on what's

100
00:05:15,850 --> 00:05:18,490
happening with their
companies, and here

101
00:05:18,490 --> 00:05:20,340
to answer your questions.

102
00:05:20,340 --> 00:05:24,270
So before we turn it over
to our first presentation--

103
00:05:24,270 --> 00:05:26,870
each will do about a
15-minute presentation--

104
00:05:26,870 --> 00:05:30,750
and I ask you to put your
questions in the chat, please.

105
00:05:30,750 --> 00:05:34,220
We'll monitor that and
have time at the end of all

106
00:05:34,220 --> 00:05:37,260
the presentations for
some moderated Q&A.

107
00:05:37,260 --> 00:05:39,620
And I'll work in your
questions as well.

108
00:05:39,620 --> 00:05:43,040
I'd like to introduce my
colleague, Bill Bonvillian, who

109
00:05:43,040 --> 00:05:45,290
works with us here at
JWEL and has really

110
00:05:45,290 --> 00:05:51,800
been the real driving force
behind this AI innovator series.

111
00:05:51,800 --> 00:05:56,280
He has deep experience in the
science and technology space,

112
00:05:56,280 --> 00:05:58,910
particularly looking at
it from a policy lens,

113
00:05:58,910 --> 00:06:02,270
and the work that he's done
for the federal government

114
00:06:02,270 --> 00:06:07,340
here in the US, particularly
serving with our Senate leaders,

115
00:06:07,340 --> 00:06:08,280
working with them.

116
00:06:08,280 --> 00:06:11,720
But he also ran MIT's
Washington, DC office.

117
00:06:11,720 --> 00:06:16,400
So has a deep understanding
of the science and technology

118
00:06:16,400 --> 00:06:20,190
and how institutions need
to be thinking about this.

119
00:06:20,190 --> 00:06:23,690
But also just an incredible
contributor to the team

120
00:06:23,690 --> 00:06:24,510
here at JWEL.

121
00:06:24,510 --> 00:06:26,100
So, Bill, can I
turn it over to you,

122
00:06:26,100 --> 00:06:29,640
please, just to say a little
bit about this, as you were?

123
00:06:29,640 --> 00:06:32,780
It's your brainchild too.

124
00:06:32,780 --> 00:06:34,310
BILL BONVILLIAN: Thanks, Megan.

125
00:06:34,310 --> 00:06:36,720
I think today, you're
going to really enjoy--

126
00:06:36,720 --> 00:06:39,720
I mean, I think there's
an underlying issue.

127
00:06:39,720 --> 00:06:41,570
Why are we talking to companies?

128
00:06:41,570 --> 00:06:43,900
What do they have to
do with education?

129
00:06:43,900 --> 00:06:46,820
Doesn't that still
belong to educators?

130
00:06:46,820 --> 00:06:49,620
But I think the answer
is that companies,

131
00:06:49,620 --> 00:06:54,750
particularly smaller
firms and new startups,

132
00:06:54,750 --> 00:06:58,050
they're risking their futures
on the new technologies,

133
00:06:58,050 --> 00:07:02,100
and they can take a level of
risk and work in ways that

134
00:07:02,100 --> 00:07:03,310
educators can't.

135
00:07:03,310 --> 00:07:07,260
Companies can really play a
role in accelerating technology

136
00:07:07,260 --> 00:07:11,410
adoption, while established
educational institutions,

137
00:07:11,410 --> 00:07:15,330
schools, universities,
they can't take that risk,

138
00:07:15,330 --> 00:07:18,580
and they don't have that
single-minded kind of focus.

139
00:07:18,580 --> 00:07:21,120
So companies can really
play a useful role

140
00:07:21,120 --> 00:07:24,780
in experimenting with new
technologies, which is why we

141
00:07:24,780 --> 00:07:27,870
ought to talk to these folks.

142
00:07:27,870 --> 00:07:30,520
What are these firms
experimenting with?

143
00:07:30,520 --> 00:07:32,350
What can we expect to see?

144
00:07:32,350 --> 00:07:36,990
And I'll just build a
little bit on what Megan

145
00:07:36,990 --> 00:07:38,860
indicated in her introductions.

146
00:07:38,860 --> 00:07:41,730
So Deepak at English
Helper is now

147
00:07:41,730 --> 00:07:46,590
implementing a major AI based
digital tutoring project

148
00:07:46,590 --> 00:07:50,200
to help both teachers and
students acquire English.

149
00:07:50,200 --> 00:07:53,910
In other words, a
computer program that uses

150
00:07:53,910 --> 00:07:56,310
AI to create
personalized learning

151
00:07:56,310 --> 00:07:57,990
experiences for students.

152
00:07:57,990 --> 00:08:00,180
And that's a very
important early track

153
00:08:00,180 --> 00:08:04,890
for AI and probably
a principal role.

154
00:08:04,890 --> 00:08:11,580
Joy Dasgupta is with Guyana AI,
which really is a services firm.

155
00:08:11,580 --> 00:08:17,940
It's designed to help educators,
teachers, individual learners

156
00:08:17,940 --> 00:08:22,350
and bring AI
capabilities to them.

157
00:08:22,350 --> 00:08:25,650
So it can lead educators.

158
00:08:25,650 --> 00:08:27,960
It can help educators.

159
00:08:27,960 --> 00:08:31,000
If you're a school that
wants to start to apply AI,

160
00:08:31,000 --> 00:08:33,840
Guyana AI can help you.

161
00:08:33,840 --> 00:08:36,330
It also aids teachers
in developing

162
00:08:36,330 --> 00:08:39,690
AI based courses and teaching
materials from course content

163
00:08:39,690 --> 00:08:40,650
to video lectures.

164
00:08:40,650 --> 00:08:47,250
So I could give Guyana AI all
my teaching materials, my course

165
00:08:47,250 --> 00:08:52,750
readings, my lecture outlines,
and I can get back a syllabus,

166
00:08:52,750 --> 00:08:55,260
lecture notes, slides.

167
00:08:55,260 --> 00:08:58,990
And because it's using a
closed network of content,

168
00:08:58,990 --> 00:09:01,470
it can avoid the set of
hallucination processes

169
00:09:01,470 --> 00:09:03,970
that have afflicted AI.

170
00:09:03,970 --> 00:09:07,350
And it's not only
for teachers, it also

171
00:09:07,350 --> 00:09:10,590
affects individual learners in
designing their own AI based

172
00:09:10,590 --> 00:09:12,430
learning projects.

173
00:09:12,430 --> 00:09:19,870
And of course, schools
[AUDIO OUT] analyst

174
00:09:19,870 --> 00:09:28,030
Joy Hammond will help
us walk through what's

175
00:09:28,030 --> 00:09:33,710
happening in this world
of AI and education.

176
00:09:33,710 --> 00:09:35,890
So her firm is
both an accelerator

177
00:09:35,890 --> 00:09:40,430
that supports and helps
mature startup companies,

178
00:09:40,430 --> 00:09:42,490
but it also is a
micro venture firm

179
00:09:42,490 --> 00:09:45,100
that helps connect
them with financing

180
00:09:45,100 --> 00:09:50,390
and also provide financing for
the early stages of those firms.

181
00:09:50,390 --> 00:09:52,820
And Jean, who's been working
with these companies,

182
00:09:52,820 --> 00:09:56,020
can tell us about what some
of the most promising firms

183
00:09:56,020 --> 00:09:58,370
are doing that
she's working with.

184
00:09:58,370 --> 00:10:01,700
What are the AI approaches
they're adapting for education?

185
00:10:01,700 --> 00:10:04,780
And then secondly, she
can, from her vantage

186
00:10:04,780 --> 00:10:10,210
point of looking at this kind of
edtech AI and education world,

187
00:10:10,210 --> 00:10:13,720
give us ideas of what the
most promising approaches are

188
00:10:13,720 --> 00:10:15,580
for AI and education.

189
00:10:15,580 --> 00:10:18,880
So I think there's a lot we can
learn from what these firms are

190
00:10:18,880 --> 00:10:21,850
up to about the
kind of new pathways

191
00:10:21,850 --> 00:10:24,950
that they're trying to
create and how AI can

192
00:10:24,950 --> 00:10:27,932
assist in educational missions.

193
00:10:27,932 --> 00:10:29,390
MEGAN MITCHELL:
Deepak, are you all

194
00:10:29,390 --> 00:10:31,566
set to share your screen and--

195
00:10:31,566 --> 00:10:33,290
DEEPAK VERMA: I think I am.

196
00:10:33,290 --> 00:10:34,400
MEGAN MITCHELL: Amazing.

197
00:10:34,400 --> 00:10:35,720
DEEPAK VERMA: OK, great.

198
00:10:35,720 --> 00:10:37,520
Well, first of
all, I just wanted

199
00:10:37,520 --> 00:10:40,970
to say a big thank YOU to
Bill and the entire team

200
00:10:40,970 --> 00:10:44,750
here for having
me on this panel.

201
00:10:44,750 --> 00:10:49,800
It's an honor and really
looking forward to it.

202
00:10:49,800 --> 00:10:52,790
And also warm hello to all
of you from around the world.

203
00:10:52,790 --> 00:10:53,910
Great to connect.

204
00:10:53,910 --> 00:10:55,340
Great to meet.

205
00:10:55,340 --> 00:10:58,830
And with that, let's
get right to it.

206
00:10:58,830 --> 00:11:01,130
So English Helper is
actually a company

207
00:11:01,130 --> 00:11:05,220
that's been around for
some time, about 10 years.

208
00:11:05,220 --> 00:11:09,380
And we've been focused on
the literacy challenge.

209
00:11:09,380 --> 00:11:11,960
We started out
initially in India

210
00:11:11,960 --> 00:11:16,130
with a product that's
really a-- think of it

211
00:11:16,130 --> 00:11:20,870
as a classroom
product for students

212
00:11:20,870 --> 00:11:22,910
to learn how to
pronounce and how

213
00:11:22,910 --> 00:11:27,270
to build fluency in English
using their classroom textbooks.

214
00:11:27,270 --> 00:11:31,020
And that product scaled to
a fairly large footprint,

215
00:11:31,020 --> 00:11:33,690
as you can see on the
slide, over 100,000 schools,

216
00:11:33,690 --> 00:11:38,520
25 million kids, most of them in
India, some in other countries.

217
00:11:38,520 --> 00:11:40,730
But this was a
group product that

218
00:11:40,730 --> 00:11:44,040
was aimed at working with a
group of students in a class.

219
00:11:44,040 --> 00:11:46,310
It wasn't one to one.

220
00:11:46,310 --> 00:11:49,860
In the last 10 years, while we
built scale with this product,

221
00:11:49,860 --> 00:11:51,440
a few things have happened.

222
00:11:51,440 --> 00:11:55,250
AI has become far more
accessible and far more

223
00:11:55,250 --> 00:11:57,320
powerful.

224
00:11:57,320 --> 00:12:00,000
Computational power
has increased manifold,

225
00:12:00,000 --> 00:12:02,000
and now it's possible
to really think

226
00:12:02,000 --> 00:12:05,570
about doing large-scale systems,
which are highly, highly

227
00:12:05,570 --> 00:12:06,500
personalized.

228
00:12:06,500 --> 00:12:09,890
So with that thought
in mind, we have

229
00:12:09,890 --> 00:12:13,970
been building over the last
year our next-generation product

230
00:12:13,970 --> 00:12:16,080
or platform that we're
rolling out currently.

231
00:12:16,080 --> 00:12:18,890
It's called the Reading and
Comprehension Assistant.

232
00:12:18,890 --> 00:12:23,720
And what this is it's really a
foundational literacy platform.

233
00:12:23,720 --> 00:12:28,310
So it covers listening, reading,
listening, speaking, reading,

234
00:12:28,310 --> 00:12:29,300
and writing.

235
00:12:29,300 --> 00:12:35,900
And it works with learners from
absolute novices to experts,

236
00:12:35,900 --> 00:12:38,600
and it works with
learners of all types.

237
00:12:38,600 --> 00:12:46,390
So students in K-12, students
in colleges, young folks,

238
00:12:46,390 --> 00:12:49,720
or even moms in India that want
to learn English so that they

239
00:12:49,720 --> 00:12:51,290
can be more employable.

240
00:12:51,290 --> 00:12:54,560
So we work with all of
those different groups.

241
00:12:54,560 --> 00:12:57,220
And employability
is a big theme.

242
00:12:57,220 --> 00:13:01,000
Some of the things that this
product does, which I think

243
00:13:01,000 --> 00:13:06,310
are what makes it so
powerful, one of them

244
00:13:06,310 --> 00:13:08,030
is it provides instant feedback.

245
00:13:08,030 --> 00:13:12,070
So as you're speaking to the
product, it will listen to you,

246
00:13:12,070 --> 00:13:14,630
and it will score
your pronunciation.

247
00:13:14,630 --> 00:13:16,810
It will break down words
you're having difficulty

248
00:13:16,810 --> 00:13:18,910
with into their
syllables and kind

249
00:13:18,910 --> 00:13:23,060
allow you to hear the correct
pronunciation and then

250
00:13:23,060 --> 00:13:25,070
your pronunciation
and practice yours

251
00:13:25,070 --> 00:13:26,610
so that you can get better.

252
00:13:26,610 --> 00:13:28,350
It even goes to
the phoneme level.

253
00:13:28,350 --> 00:13:31,220
And all of this is with
the idea of learning

254
00:13:31,220 --> 00:13:33,110
how to pronounce correctly.

255
00:13:33,110 --> 00:13:37,380
But then even with passages
that it provides to you,

256
00:13:37,380 --> 00:13:39,118
you answer questions,
and it's assessing,

257
00:13:39,118 --> 00:13:41,160
are you really understanding
what you're reading?

258
00:13:41,160 --> 00:13:44,600
So that feedback is
coming back in real time.

259
00:13:44,600 --> 00:13:48,620
Another, I think, fairly
differentiating feature

260
00:13:48,620 --> 00:13:51,900
about this platform is
its content agnostic.

261
00:13:51,900 --> 00:13:58,790
And what I mean by that is it's
not gen AI LLM-based technology,

262
00:13:58,790 --> 00:14:02,600
which is trained on large,
large amounts of content.

263
00:14:02,600 --> 00:14:06,380
It's really an NLP or
computational linguistics

264
00:14:06,380 --> 00:14:13,550
model, which is, A, fairly small
in terms of footprint and B,

265
00:14:13,550 --> 00:14:15,720
it works on different
types of content.

266
00:14:15,720 --> 00:14:17,790
And so if you provide
content for young kids,

267
00:14:17,790 --> 00:14:19,650
it becomes an early
learning product.

268
00:14:19,650 --> 00:14:22,020
If you provide
content for adults,

269
00:14:22,020 --> 00:14:23,960
it's a product for adults.

270
00:14:23,960 --> 00:14:27,090
And as a result, we bring
our content to the table.

271
00:14:27,090 --> 00:14:29,880
But as we work with
partners and customers,

272
00:14:29,880 --> 00:14:32,000
we're also able to
host their content

273
00:14:32,000 --> 00:14:35,120
and deliver it to
their learners.

274
00:14:35,120 --> 00:14:38,120
It is adaptive, and
by that, I mean,

275
00:14:38,120 --> 00:14:40,740
it will adjust based
on how you're doing.

276
00:14:40,740 --> 00:14:42,900
If you're having difficulty
with the materials,

277
00:14:42,900 --> 00:14:44,450
it will step things down.

278
00:14:44,450 --> 00:14:46,910
If you're doing really
well, it will make things

279
00:14:46,910 --> 00:14:49,910
harder so that you're at the
level where you're actually

280
00:14:49,910 --> 00:14:52,170
able to learn effectively.

281
00:14:52,170 --> 00:14:54,530
And it has a bunch
of interactive tools

282
00:14:54,530 --> 00:14:58,130
like you can listen to the
instructions and even the text

283
00:14:58,130 --> 00:14:59,430
that you're supposed to read.

284
00:14:59,430 --> 00:15:01,920
You can see it in your
vernacular language.

285
00:15:01,920 --> 00:15:03,567
You can choose your accents.

286
00:15:03,567 --> 00:15:05,150
You can slow down
the pace, et cetera,

287
00:15:05,150 --> 00:15:07,355
just so that it's
a little bit more--

288
00:15:07,355 --> 00:15:09,630
you can tailor it a little bit.

289
00:15:09,630 --> 00:15:12,510
Each learner can tailor
it to what works for them.

290
00:15:12,510 --> 00:15:15,740
And then finally, we are
aligning to various curriculum

291
00:15:15,740 --> 00:15:18,420
around the world, so to
the Common Core in the US,

292
00:15:18,420 --> 00:15:22,210
to the CEFR, et cetera.

293
00:15:22,210 --> 00:15:26,793
So what I described to you is
these broad set of capabilities.

294
00:15:26,793 --> 00:15:28,210
But then the
question becomes, how

295
00:15:28,210 --> 00:15:31,090
does any specific
given learner engage?

296
00:15:31,090 --> 00:15:33,340
And so what we've done
on top of the platform

297
00:15:33,340 --> 00:15:36,280
is we've constructed--

298
00:15:36,280 --> 00:15:38,860
call them courses,
call them journeys--

299
00:15:38,860 --> 00:15:41,780
for specific outcomes.

300
00:15:41,780 --> 00:15:44,740
So we have a course that
we're calling "Introduction

301
00:15:44,740 --> 00:15:46,430
to Conversational Skills."

302
00:15:46,430 --> 00:15:49,930
And here the idea is for
blue or gray collar workers

303
00:15:49,930 --> 00:15:52,720
to develop enough
proficiency in the language

304
00:15:52,720 --> 00:15:55,540
to be able to communicate,
to be able to take

305
00:15:55,540 --> 00:15:58,150
an interview in English,
perhaps, to be more

306
00:15:58,150 --> 00:16:01,420
confident and comfortable
in their social environment

307
00:16:01,420 --> 00:16:02,120
with English.

308
00:16:02,120 --> 00:16:05,320
And so this is really focused
on the conversational side,

309
00:16:05,320 --> 00:16:10,330
and not so much on writing
and things like that.

310
00:16:10,330 --> 00:16:13,330
We have another course that
we're calling "Introduction

311
00:16:13,330 --> 00:16:15,670
to Foundational Skills,"
and this is more

312
00:16:15,670 --> 00:16:20,740
of if you're familiar with CEFR,
which is the Common European

313
00:16:20,740 --> 00:16:23,510
Standard Framework
for Languages,

314
00:16:23,510 --> 00:16:27,200
it starts with level A1, which
is a complete novice and then

315
00:16:27,200 --> 00:16:29,690
goes to A2 and B1
and B2 and C1 and C2.

316
00:16:29,690 --> 00:16:32,310
And C2 is for folks
who are experts.

317
00:16:32,310 --> 00:16:35,000
So think of Introduction
to Foundational Skills

318
00:16:35,000 --> 00:16:39,740
as a course that takes
you from A1 to B1, B2.

319
00:16:39,740 --> 00:16:44,120
And then we have an advanced
foundational skills course

320
00:16:44,120 --> 00:16:47,720
that takes you from B2 to C2.

321
00:16:47,720 --> 00:16:51,660
And so by the time you're done
with this across listening,

322
00:16:51,660 --> 00:16:53,510
speaking, reading,
and writing, you

323
00:16:53,510 --> 00:16:58,070
would be fairly proficient
in the language.

324
00:16:58,070 --> 00:17:04,470
Now, I did mention that the
platform is content agnostic,

325
00:17:04,470 --> 00:17:07,550
so we built a couple
of things on top of it

326
00:17:07,550 --> 00:17:12,810
that are more specific
to certain use cases.

327
00:17:12,810 --> 00:17:16,339
One of them is the IELTS prep.

328
00:17:16,339 --> 00:17:19,079
Again, I'm not sure how many of
you are familiar with the IELTS,

329
00:17:19,079 --> 00:17:20,329
but it's like a TOEFL.

330
00:17:20,329 --> 00:17:25,069
It's a test that
students who are looking

331
00:17:25,069 --> 00:17:27,020
to get admitted to
universities in the US

332
00:17:27,020 --> 00:17:28,910
typically have to
take, and they have

333
00:17:28,910 --> 00:17:31,040
to clear a certain
threshold in order

334
00:17:31,040 --> 00:17:33,380
to be considered admissible.

335
00:17:33,380 --> 00:17:36,290
What we're finding with
universities in the US

336
00:17:36,290 --> 00:17:39,140
that we're working with
that there are many

337
00:17:39,140 --> 00:17:41,420
qualified-- otherwise
qualified students

338
00:17:41,420 --> 00:17:44,090
who satisfy all the
criteria for admission

339
00:17:44,090 --> 00:17:47,900
but don't quite make it
on a subscore or a couple

340
00:17:47,900 --> 00:17:49,530
of sub scores of the IELTS.

341
00:17:49,530 --> 00:17:51,890
So this is a specific
program designed

342
00:17:51,890 --> 00:17:55,250
to help students who want
to pass the IELTS exam,

343
00:17:55,250 --> 00:17:58,470
practice for it, take
practice tests for it,

344
00:17:58,470 --> 00:18:01,440
and then be ready to take
the actual IELTS test.

345
00:18:01,440 --> 00:18:04,860
So that is the
IELTS prep course.

346
00:18:04,860 --> 00:18:10,980
And then we have a category that
we're calling academic prep.

347
00:18:10,980 --> 00:18:14,610
What we mean by
this is that it's

348
00:18:14,610 --> 00:18:20,080
one thing to learn foundational
English, generic English,

349
00:18:20,080 --> 00:18:22,260
but it's a whole
other thing if you

350
00:18:22,260 --> 00:18:28,370
are looking to become a nurse
or an OR assistant or a lawyer.

351
00:18:28,370 --> 00:18:31,500
You have to deal with the
language of the profession

352
00:18:31,500 --> 00:18:34,400
or the language of the course
or the language of the subject.

353
00:18:34,400 --> 00:18:39,870
And so the idea here is to build
specific content for learners

354
00:18:39,870 --> 00:18:42,630
that are either trying
to get proficient

355
00:18:42,630 --> 00:18:44,580
at the language of
a course, perhaps

356
00:18:44,580 --> 00:18:47,877
in college, or from an
employability perspective

357
00:18:47,877 --> 00:18:50,460
are trying to get proficient in
the language of the profession

358
00:18:50,460 --> 00:18:53,480
they're getting ready for.

359
00:18:53,480 --> 00:18:57,510
And so this is something
that we can add on,

360
00:18:57,510 --> 00:18:59,910
and we will continue
to add on as courses

361
00:18:59,910 --> 00:19:04,350
we offer for different
domains to get

362
00:19:04,350 --> 00:19:08,760
learners comfortable with
domain-specific vocabularies.

363
00:19:08,760 --> 00:19:12,880
So that is what I had in
terms of my prepared remarks.

364
00:19:12,880 --> 00:19:18,580
Happy to pause here and interact
more in the question and answer

365
00:19:18,580 --> 00:19:20,910
session.

366
00:19:20,910 --> 00:19:23,470
MEGAN MITCHELL:
Fantastic, Deepak.

367
00:19:23,470 --> 00:19:24,000
Excellent.

368
00:19:24,000 --> 00:19:25,710
I think that's
really interesting

369
00:19:25,710 --> 00:19:27,960
to see how you've
shifted or evolved

370
00:19:27,960 --> 00:19:30,550
and continue to grow
the company and adapt.

371
00:19:30,550 --> 00:19:32,830
And I think it will spark
some interesting questions.

372
00:19:32,830 --> 00:19:36,030
But why don't we stay
on our presentations?

373
00:19:36,030 --> 00:19:37,770
So we have lots of
time for questions,

374
00:19:37,770 --> 00:19:42,570
and we can assimilate all
the different information.

375
00:19:42,570 --> 00:19:46,020
Joy, are you ready?

376
00:19:46,020 --> 00:19:47,440
JOY DASGUPTA: Hello, everybody.

377
00:19:47,440 --> 00:19:49,710
My name is Joy Dasgupta.

378
00:19:49,710 --> 00:19:51,550
I'm based here in Boston.

379
00:19:51,550 --> 00:19:58,680
I'm the CEO of Gyan, which is
a language model company, an AI

380
00:19:58,680 --> 00:20:02,070
company, and I'll tell
you a little bit about it.

381
00:20:02,070 --> 00:20:05,550
I'm here to talk about
a specific product

382
00:20:05,550 --> 00:20:08,610
that we have built for
the higher education

383
00:20:08,610 --> 00:20:14,550
space on Gyan that is applicable
and usable by higher ed

384
00:20:14,550 --> 00:20:18,330
institutions, as well as
corporate L&D departments,

385
00:20:18,330 --> 00:20:23,410
to basically intelligently
discover content and auto curate

386
00:20:23,410 --> 00:20:29,510
it for rapidly generating
ready-to-use courses.

387
00:20:29,510 --> 00:20:31,090
So that's the focus.

388
00:20:31,090 --> 00:20:32,090
We do build.

389
00:20:32,090 --> 00:20:34,190
We have built many
other products

390
00:20:34,190 --> 00:20:36,410
for some other
verticals as well.

391
00:20:36,410 --> 00:20:38,330
That's out of scope
for this discussion.

392
00:20:38,330 --> 00:20:39,860
We'll just stay focused on this.

393
00:20:39,860 --> 00:20:42,490
And the target audience
for this product

394
00:20:42,490 --> 00:20:46,600
is essentially
faculty instructors

395
00:20:46,600 --> 00:20:50,710
who are in the business
of building courses.

396
00:20:50,710 --> 00:20:52,910
However, an adjunct
and adjacent use case,

397
00:20:52,910 --> 00:20:55,070
which I'll also talk about
in the course of this,

398
00:20:55,070 --> 00:20:59,830
is assessments, auto assessment,
personally setting up

399
00:20:59,830 --> 00:21:04,630
auto assessment for OpenText
responses, configuring it,

400
00:21:04,630 --> 00:21:07,330
and then actually scoring
it when students come in

401
00:21:07,330 --> 00:21:11,140
and respond to those questions.

402
00:21:11,140 --> 00:21:16,090
So you know everything
that's on this slide,

403
00:21:16,090 --> 00:21:18,470
but it's worth
stating one more time.

404
00:21:18,470 --> 00:21:20,380
Higher ed is at an
inflection point.

405
00:21:20,380 --> 00:21:21,850
It probably has
been for a while.

406
00:21:21,850 --> 00:21:24,190
There are these points
that you all well know.

407
00:21:24,190 --> 00:21:28,710
But I think the items here on
the left-hand side is what's key

408
00:21:28,710 --> 00:21:33,430
is now and I think
into the deep future,

409
00:21:33,430 --> 00:21:36,210
whatever we do or build
is going to have--

410
00:21:36,210 --> 00:21:38,730
going to need to have
these attributes.

411
00:21:38,730 --> 00:21:41,322
OK, I think stuff is going
to need to be flexible,

412
00:21:41,322 --> 00:21:43,030
whether it's technology,
whether it's us,

413
00:21:43,030 --> 00:21:45,960
whether how we
deliver training, how

414
00:21:45,960 --> 00:21:47,650
we prepare for it, et cetera.

415
00:21:47,650 --> 00:21:50,670
So the words that we
have come up with for--

416
00:21:50,670 --> 00:21:53,310
as design inputs
into our product

417
00:21:53,310 --> 00:21:55,260
is essentially
whatever we build has

418
00:21:55,260 --> 00:21:59,890
to be flexible, responsible,
responsive, affordable,

419
00:21:59,890 --> 00:22:05,910
adaptive, and personalized
and personalized ad quality.

420
00:22:05,910 --> 00:22:07,500
And then, of course,
the operative,

421
00:22:07,500 --> 00:22:09,480
the key word here is scale.

422
00:22:09,480 --> 00:22:11,020
You heard from Deepak as well.

423
00:22:11,020 --> 00:22:15,990
Like one of the things that
AI has enabled us to do

424
00:22:15,990 --> 00:22:19,020
is scale things
that we otherwise

425
00:22:19,020 --> 00:22:21,970
have to deliver in classrooms
or in face-to-face models,

426
00:22:21,970 --> 00:22:22,600
et cetera.

427
00:22:22,600 --> 00:22:24,730
This is beyond obviously
digital learning.

428
00:22:24,730 --> 00:22:25,760
This is AI enabled.

429
00:22:25,760 --> 00:22:28,640

430
00:22:28,640 --> 00:22:29,900
Whoops.

431
00:22:29,900 --> 00:22:33,530
So now the promise of AI.

432
00:22:33,530 --> 00:22:35,990
And I'm going to talk about a
specific branch of AI, which

433
00:22:35,990 --> 00:22:37,610
is large--

434
00:22:37,610 --> 00:22:42,980
excuse me-- language model
based, text-based AI.

435
00:22:42,980 --> 00:22:45,770
So obviously,
everybody on this call

436
00:22:45,770 --> 00:22:49,970
is well familiar with
the arrival of the LLMs

437
00:22:49,970 --> 00:22:54,710
back in 2022, and everything
changed after that.

438
00:22:54,710 --> 00:22:59,610
ChatGPT, Anthropic,
Meta, everything--

439
00:22:59,610 --> 00:23:03,140
all the very exciting
products that have come out

440
00:23:03,140 --> 00:23:07,250
have created a
tremendous opportunity.

441
00:23:07,250 --> 00:23:10,630
It has been backed by a
huge amount of marketing.

442
00:23:10,630 --> 00:23:13,820
It has also generated
a lot of noise.

443
00:23:13,820 --> 00:23:17,210
So opportunity and
confusion and what

444
00:23:17,210 --> 00:23:20,750
to do with it is basically we
go out and talk to many people,

445
00:23:20,750 --> 00:23:24,810
not just in higher ed, in
capital markets, in consulting,

446
00:23:24,810 --> 00:23:26,700
in large corporations.

447
00:23:26,700 --> 00:23:30,520
And the issue there is the
opportunity is overwhelming,

448
00:23:30,520 --> 00:23:32,250
but where to start,
how to start?

449
00:23:32,250 --> 00:23:35,460
And there are some
built-in challenges

450
00:23:35,460 --> 00:23:37,830
with the more
popular band of AI,

451
00:23:37,830 --> 00:23:42,420
the LLM based, and the key
one being that it is generally

452
00:23:42,420 --> 00:23:44,530
a black box.

453
00:23:44,530 --> 00:23:48,430
And it is therefore you cannot
double-click on it and get

454
00:23:48,430 --> 00:23:52,570
to the heart of the analysis and
the source of where the analysis

455
00:23:52,570 --> 00:23:53,840
came from.

456
00:23:53,840 --> 00:23:56,660
Now, in our view,
that is a showstopper

457
00:23:56,660 --> 00:24:02,580
for many use cases in higher
ed, in legal in capital markets,

458
00:24:02,580 --> 00:24:04,290
in publishing.

459
00:24:04,290 --> 00:24:06,680
Wherever explainability
is important,

460
00:24:06,680 --> 00:24:11,020
a black box approach
to AI is problematic.

461
00:24:11,020 --> 00:24:11,970
It has its uses.

462
00:24:11,970 --> 00:24:13,080
It has its applications.

463
00:24:13,080 --> 00:24:15,170
But there are places
where it just won't work.

464
00:24:15,170 --> 00:24:19,130
So I'm here to talk to you about
an explainable language model

465
00:24:19,130 --> 00:24:21,330
that we have built-in
Gyan that we have.

466
00:24:21,330 --> 00:24:24,330
Then that's too much
technical stuff there.

467
00:24:24,330 --> 00:24:26,990
But I'm going to talk about a
use case around it, which is how

468
00:24:26,990 --> 00:24:29,900
we use that to build a course.

469
00:24:29,900 --> 00:24:34,910
So just a very quick blurb on
Gyan so that you understand.

470
00:24:34,910 --> 00:24:38,390
Rest of it is clearer.

471
00:24:38,390 --> 00:24:41,030
Gyan basically means knowledge,
by the way, in Sanskrit.

472
00:24:41,030 --> 00:24:44,330
So there are two
pieces here that

473
00:24:44,330 --> 00:24:47,130
is applicable to the process
of course generation.

474
00:24:47,130 --> 00:24:50,820
So first piece is we built
Gyan as a research engine.

475
00:24:50,820 --> 00:24:56,550
And that is what we
apply to discover content

476
00:24:56,550 --> 00:24:59,820
and to auto-curate
content quickly

477
00:24:59,820 --> 00:25:02,970
from any source
anywhere, whether it

478
00:25:02,970 --> 00:25:07,320
is your private
source, Gyan provided

479
00:25:07,320 --> 00:25:09,430
sources, or the open internet.

480
00:25:09,430 --> 00:25:13,240
So a key point here is
that you have full agency--

481
00:25:13,240 --> 00:25:15,490
the instructor, the
course designer,

482
00:25:15,490 --> 00:25:19,230
instructional
designer, the faculty.

483
00:25:19,230 --> 00:25:23,880
You have full agency
over the content sources

484
00:25:23,880 --> 00:25:25,420
that you want to target.

485
00:25:25,420 --> 00:25:28,090
So Gyan will not draw
outside the lines.

486
00:25:28,090 --> 00:25:31,950
Gyan will not hallucinate
because again,

487
00:25:31,950 --> 00:25:35,670
just a little technical
point is Gyan's model is not

488
00:25:35,670 --> 00:25:37,300
trained on data.

489
00:25:37,300 --> 00:25:41,040
It is based on the rules
of the English language.

490
00:25:41,040 --> 00:25:43,863
We are not using heavy duty
math to understand English.

491
00:25:43,863 --> 00:25:45,780
We are using the rules
of the English language

492
00:25:45,780 --> 00:25:47,460
to understand English.

493
00:25:47,460 --> 00:25:50,610
So as a result, many
of the challenges

494
00:25:50,610 --> 00:25:53,190
that we have with large
language models that

495
00:25:53,190 --> 00:25:56,750
become a challenge
in adoption are

496
00:25:56,750 --> 00:25:59,090
absent because of that
architectural difference

497
00:25:59,090 --> 00:26:00,780
that we do not learn from data--

498
00:26:00,780 --> 00:26:04,320
no hallucination, no IP
infringement, no black box.

499
00:26:04,320 --> 00:26:05,925
You have full auditability.

500
00:26:05,925 --> 00:26:07,050
There are no cut-off dates.

501
00:26:07,050 --> 00:26:10,650
So when you run a query, you
get a response as of now,

502
00:26:10,650 --> 00:26:15,400
whatever is there out on
the web or in your stores.

503
00:26:15,400 --> 00:26:18,140
No inconsistency-- so every
time you ask Gyan a question,

504
00:26:18,140 --> 00:26:20,060
you get the same answer.

505
00:26:20,060 --> 00:26:22,960
Doesn't happen in some of
the big models out there.

506
00:26:22,960 --> 00:26:25,953
So content discovery is
very important, obviously,

507
00:26:25,953 --> 00:26:27,620
when you're trying
to generate a course.

508
00:26:27,620 --> 00:26:29,992
And then there's
this auto-assessment.

509
00:26:29,992 --> 00:26:31,450
So we take that
same language model

510
00:26:31,450 --> 00:26:33,340
and apply it to a
student response

511
00:26:33,340 --> 00:26:35,740
against fairly complex rubric.

512
00:26:35,740 --> 00:26:40,600
Rubric could have items like
rate the logical progression

513
00:26:40,600 --> 00:26:42,550
of this response.

514
00:26:42,550 --> 00:26:46,690
Give a score for the analytical
depth of this response.

515
00:26:46,690 --> 00:26:49,820
How closely is the conclusion
matching the thesis?

516
00:26:49,820 --> 00:26:51,860
So we can have some
fairly complex rubric.

517
00:26:51,860 --> 00:26:55,610
And you can double-click
to understand

518
00:26:55,610 --> 00:26:57,620
where the score is
coming from and why

519
00:26:57,620 --> 00:26:59,420
a student got a specific score.

520
00:26:59,420 --> 00:27:03,110
So these two things are embedded
in this course generator

521
00:27:03,110 --> 00:27:04,580
I'm about to show you.

522
00:27:04,580 --> 00:27:08,390
And on this graphic on
the right is basically

523
00:27:08,390 --> 00:27:11,760
saying the same point that you
have agency over the sources.

524
00:27:11,760 --> 00:27:13,370
That's a very important.

525
00:27:13,370 --> 00:27:15,840
And many versions of
AI that's out there,

526
00:27:15,840 --> 00:27:19,050
you don't know where the
answer is coming from.

527
00:27:19,050 --> 00:27:23,080
OK, you can do RAG, Retrieval
Augmented Generation.

528
00:27:23,080 --> 00:27:24,460
You can provide your content.

529
00:27:24,460 --> 00:27:27,430
It will try to draw inside
the box, but it will--

530
00:27:27,430 --> 00:27:30,540
but it tends to always
draw outside as well.

531
00:27:30,540 --> 00:27:33,580
So you can bring
your private content.

532
00:27:33,580 --> 00:27:37,230
Gyan provides some content as
well, like all the preprint

533
00:27:37,230 --> 00:27:41,020
servers, the archives, the bio
arcghive, chem archive, PubMed,

534
00:27:41,020 --> 00:27:42,610
USPTO, et cetera.

535
00:27:42,610 --> 00:27:46,120
We also have MIT
OCW preingested.

536
00:27:46,120 --> 00:27:48,940
We have a whole bunch of
other OERs preingested.

537
00:27:48,940 --> 00:27:51,420
So if you want to generate
a course using MIT OCW,

538
00:27:51,420 --> 00:27:54,510
you can do it today right away.

539
00:27:54,510 --> 00:27:57,210
And of course, the websites.

540
00:27:57,210 --> 00:27:59,560
OK, so that's the backdrop.

541
00:27:59,560 --> 00:28:03,210
This particular product
does these few things

542
00:28:03,210 --> 00:28:04,800
that you would
expect, I suppose,

543
00:28:04,800 --> 00:28:06,700
in any course
generation application.

544
00:28:06,700 --> 00:28:09,030
But some of the
differences is in how

545
00:28:09,030 --> 00:28:11,490
we do it, not in what we
do, because what kind of

546
00:28:11,490 --> 00:28:13,650
remains constant--

547
00:28:13,650 --> 00:28:16,930
so pedagogically
relevant course content.

548
00:28:16,930 --> 00:28:22,330
So intelligent discovery using
our method of AI from content

549
00:28:22,330 --> 00:28:26,800
everywhere, finding the most
relevant, discarding the rest.

550
00:28:26,800 --> 00:28:29,290
And then auto curation--

551
00:28:29,290 --> 00:28:33,940
we leverage the best
sources, MIT OCW included.

552
00:28:33,940 --> 00:28:37,370
We have support for
the autonomous learner.

553
00:28:37,370 --> 00:28:41,030
We also have an AI assistant
that will take you step by step,

554
00:28:41,030 --> 00:28:45,830
and that's the part of the
personalization happens there.

555
00:28:45,830 --> 00:28:48,150
So you are not in
a guided situation.

556
00:28:48,150 --> 00:28:50,630
You're in a
self-guided situation.

557
00:28:50,630 --> 00:28:53,850
Then the assessments that I
already talked to you about,

558
00:28:53,850 --> 00:28:56,930
another important
module in the product.

559
00:28:56,930 --> 00:29:04,010
And adaptive personalization--
OK, so here you basically--

560
00:29:04,010 --> 00:29:06,470
your progress
through the course.

561
00:29:06,470 --> 00:29:10,490
At a certain point,
if you are struggling,

562
00:29:10,490 --> 00:29:12,920
then you take an assessment.

563
00:29:12,920 --> 00:29:18,980
And the engine, looking at your
past performance and your scores

564
00:29:18,980 --> 00:29:21,540
and your performance on
the specific assessment,

565
00:29:21,540 --> 00:29:24,620
it is going to suggest
that you go two steps back,

566
00:29:24,620 --> 00:29:26,870
or you take this
remediation program that's

567
00:29:26,870 --> 00:29:29,900
somewhere else outside
of that specific course.

568
00:29:29,900 --> 00:29:33,960
So it tries to catch you up
and bring you up to that point

569
00:29:33,960 --> 00:29:37,400
so that then you can get the
most out of your learning.

570
00:29:37,400 --> 00:29:41,880
And this final bullet here is
continuous lifelong learning.

571
00:29:41,880 --> 00:29:45,360
So when we create
this course content,

572
00:29:45,360 --> 00:29:48,950
the word continuous basically
means that Gyan also

573
00:29:48,950 --> 00:29:51,530
keeps that course refreshed.

574
00:29:51,530 --> 00:29:54,960
So say you built a course on--

575
00:29:54,960 --> 00:29:58,010
I don't know-- introduction
to project management,

576
00:29:58,010 --> 00:29:59,360
and there's more data.

577
00:29:59,360 --> 00:30:03,590
Or let's pick a more topical
one, renewable energy, where

578
00:30:03,590 --> 00:30:05,300
stuff is changing every day.

579
00:30:05,300 --> 00:30:09,620
You could set an auto refresh on
your course content every month,

580
00:30:09,620 --> 00:30:13,940
and you would get the latest
stuff, which of course needs

581
00:30:13,940 --> 00:30:15,210
to be human adjudicated.

582
00:30:15,210 --> 00:30:18,300
Somebody needs to promote
that content into the course.

583
00:30:18,300 --> 00:30:20,450
But you will get a
good, rich collection

584
00:30:20,450 --> 00:30:22,790
of all the new stuff that
is not in your course that

585
00:30:22,790 --> 00:30:24,470
happened last month.

586
00:30:24,470 --> 00:30:29,050
So it's continuously refreshed.

587
00:30:29,050 --> 00:30:31,190
This is a busy slide,
but I'll just tell you,

588
00:30:31,190 --> 00:30:32,748
I think it might
be important just

589
00:30:32,748 --> 00:30:34,040
to share this briefly with you.

590
00:30:34,040 --> 00:30:36,980
This is how it
flows-- very simple.

591
00:30:36,980 --> 00:30:39,380
First, you come
in, and you provide

592
00:30:39,380 --> 00:30:41,570
Gyan learning objectives.

593
00:30:41,570 --> 00:30:43,250
From the learning
objectives, Gyan

594
00:30:43,250 --> 00:30:49,308
is going to suggest a
bunch of learning topics.

595
00:30:49,308 --> 00:30:51,850
At each point here, you can see
the instructor can intervene,

596
00:30:51,850 --> 00:30:54,100
so it is human in the
loop right throughout.

597
00:30:54,100 --> 00:30:56,830
So Gyan is going to suggest
a bunch of learning topics.

598
00:30:56,830 --> 00:30:59,720
Instructor then says, OK,
these topics look good.

599
00:30:59,720 --> 00:31:01,540
Then you trigger Gyan again.

600
00:31:01,540 --> 00:31:04,560
Gyan will then go and
look at all the sources

601
00:31:04,560 --> 00:31:09,660
that you have identified
and bring out content.

602
00:31:09,660 --> 00:31:12,060
Instructor goes and
scans the content

603
00:31:12,060 --> 00:31:13,420
and says, generally looks good.

604
00:31:13,420 --> 00:31:15,760
I like it and then pushes Go.

605
00:31:15,760 --> 00:31:19,200
Then Gyan from that will
generate lecture notes, reading

606
00:31:19,200 --> 00:31:21,960
list, a video, which
you can, of course, then

607
00:31:21,960 --> 00:31:23,860
everything you can go
in and edit of course,

608
00:31:23,860 --> 00:31:25,200
before you publish it.

609
00:31:25,200 --> 00:31:27,240
Assessments-- configuration
for assessment--

610
00:31:27,240 --> 00:31:31,560
like what are good
questions for this course?

611
00:31:31,560 --> 00:31:36,900
Set up the AI tutor and
also add to the learning

612
00:31:36,900 --> 00:31:39,910
path, which learning path
is not just for this course.

613
00:31:39,910 --> 00:31:42,108
Of course, it's across
a collection of courses,

614
00:31:42,108 --> 00:31:44,400
so it will then incrementally
add to the learning path.

615
00:31:44,400 --> 00:31:47,880
And then you finalize
the course and upload it.

616
00:31:47,880 --> 00:31:51,830
OK, so that's kind of how
it works schematically.

617
00:31:51,830 --> 00:31:53,880
Last thing, I'm just
going to show you what

618
00:31:53,880 --> 00:31:55,570
a compiled course looks like.

619
00:31:55,570 --> 00:31:57,960
So this is a
Gyan-generated course.

620
00:31:57,960 --> 00:32:01,630
It's the talking head
here is an avatar.

621
00:32:01,630 --> 00:32:04,820
So you've seen these.

622
00:32:04,820 --> 00:32:09,310
It's good enough, not the best,
but this is what it looks like.

623
00:32:09,310 --> 00:32:11,600
OK, so this is a course
on project management.

624
00:32:11,600 --> 00:32:14,660
It followed that entire
pipeline that I showed you.

625
00:32:14,660 --> 00:32:16,640
Yeah, where is my Play button?

626
00:32:16,640 --> 00:32:18,890
I hope we can hear this.

627
00:32:18,890 --> 00:32:21,230
- Project management for
information technology

628
00:32:21,230 --> 00:32:22,140
management.

629
00:32:22,140 --> 00:32:23,320
What is project management?

630
00:32:23,320 --> 00:32:24,737
JOY DASGUPTA: So
that's the start.

631
00:32:24,737 --> 00:32:28,170
Then I just go a little
further up to, let's say--

632
00:32:28,170 --> 00:32:29,003
I don't know-- here.

633
00:32:29,003 --> 00:32:34,010

634
00:32:34,010 --> 00:32:35,250
I play again.

635
00:32:35,250 --> 00:32:35,960
- --management?

636
00:32:35,960 --> 00:32:38,730
Given the dynamic nature
of IT environments,

637
00:32:38,730 --> 00:32:42,110
changes to project scope
requirements, or technology

638
00:32:42,110 --> 00:32:42,990
are common.

639
00:32:42,990 --> 00:32:46,020
JOY DASGUPTA: So what it's doing
is it took the lecture notes.

640
00:32:46,020 --> 00:32:50,690
It made slides out of these,
and then we get the talking head

641
00:32:50,690 --> 00:32:52,610
to read out the slides.

642
00:32:52,610 --> 00:32:54,180
So all of that is automated.

643
00:32:54,180 --> 00:32:56,690
You can intervene and
change and add more content,

644
00:32:56,690 --> 00:32:59,690
and then the talking head
will start saying those things

645
00:32:59,690 --> 00:33:01,800
as well and so on.

646
00:33:01,800 --> 00:33:03,620
And I'll just show
you in the end.

647
00:33:03,620 --> 00:33:07,820
There's a little summary that
happens, if I can get there,

648
00:33:07,820 --> 00:33:09,780
somewhere here.

649
00:33:09,780 --> 00:33:11,650
He'll try and summarize it.

650
00:33:11,650 --> 00:33:13,800
- IT management is
essential for delivering

651
00:33:13,800 --> 00:33:17,010
successful projects that meet
organizational objectives,

652
00:33:17,010 --> 00:33:18,028
satisfy--

653
00:33:18,028 --> 00:33:19,320
JOY DASGUPTA: You get the idea.

654
00:33:19,320 --> 00:33:25,470
So it's not automated
end to end, obviously,

655
00:33:25,470 --> 00:33:28,680
because you need human
in the loop throughout

656
00:33:28,680 --> 00:33:30,940
to promote it to the next stage.

657
00:33:30,940 --> 00:33:33,820
So that's, I think, all I
want to share at this point.

658
00:33:33,820 --> 00:33:35,700
And of course happy to
take questions later.

659
00:33:35,700 --> 00:33:37,890
Thank you.

660
00:33:37,890 --> 00:33:39,280
MEGAN MITCHELL: Thank you, Joy.

661
00:33:39,280 --> 00:33:41,290
I am not teaching this semester.

662
00:33:41,290 --> 00:33:43,140
And I'm really
bummed that I'm not

663
00:33:43,140 --> 00:33:47,190
because I think one of
the anxieties of teaching

664
00:33:47,190 --> 00:33:50,610
was it was the excitement,
but also like am I capturing

665
00:33:50,610 --> 00:33:51,760
all the new information?

666
00:33:51,760 --> 00:33:53,530
What needs to be updated?

667
00:33:53,530 --> 00:33:59,070
And having that
kind of by your side

668
00:33:59,070 --> 00:34:03,450
or to start fresh and see--
just fascinating, fascinating.

669
00:34:03,450 --> 00:34:05,370
So thank you very much.

670
00:34:05,370 --> 00:34:08,310
And now finally, we
will turn it over

671
00:34:08,310 --> 00:34:12,150
to Jean Hammond, who is the
founder of Learn Launch.

672
00:34:12,150 --> 00:34:13,150
JEAN HAMMOND: All right.

673
00:34:13,150 --> 00:34:19,679
Well, I'm also thrilled to be
here and very excited that JWEL

674
00:34:19,679 --> 00:34:24,449
is working on these
questions because I just

675
00:34:24,449 --> 00:34:28,780
think that education is
how we build the future.

676
00:34:28,780 --> 00:34:32,530
And if we don't build the
future to be ready for AI,

677
00:34:32,530 --> 00:34:36,090
we're not building
the right future.

678
00:34:36,090 --> 00:34:36,760
Let's see.

679
00:34:36,760 --> 00:34:39,885
I'm having some slide
moving trouble here.

680
00:34:39,885 --> 00:34:41,010
MEGAN MITCHELL: Here we go.

681
00:34:41,010 --> 00:34:42,010
JEAN HAMMOND: All right.

682
00:34:42,010 --> 00:34:44,730
So really quick just to
tell you a little bit more

683
00:34:44,730 --> 00:34:48,540
about Learn Launch so that you
just can visualize what we do.

684
00:34:48,540 --> 00:34:51,870
So we select a very
small number of companies

685
00:34:51,870 --> 00:34:56,340
to work with at a time, and then
we put them through a process

686
00:34:56,340 --> 00:35:00,790
where they get to learn some
things, learn from their peers.

687
00:35:00,790 --> 00:35:02,790
Of course, you always
learn more from your peers

688
00:35:02,790 --> 00:35:04,260
than from the instructors.

689
00:35:04,260 --> 00:35:08,940
And then put those
into their products

690
00:35:08,940 --> 00:35:14,800
as they get ready for more
interaction with the market.

691
00:35:14,800 --> 00:35:18,960
And so we're pretty interested
in supporting companies that

692
00:35:18,960 --> 00:35:21,180
are going to build
what we would think

693
00:35:21,180 --> 00:35:23,730
of as sustainable
businesses, i.e.

694
00:35:23,730 --> 00:35:27,630
they're going to be something
that school systems are

695
00:35:27,630 --> 00:35:29,550
interested in buying.

696
00:35:29,550 --> 00:35:35,950
And every day, school
systems are buying from--

697
00:35:35,950 --> 00:35:38,500
I don't know-- Pearson
and McGraw Hill.

698
00:35:38,500 --> 00:35:42,330
And so they definitely
were looking

699
00:35:42,330 --> 00:35:44,860
for the next big new things.

700
00:35:44,860 --> 00:35:48,930
So our process is to
really have this learning

701
00:35:48,930 --> 00:35:51,870
journey for the
companies and to have

702
00:35:51,870 --> 00:35:54,780
them think about what is
it that they're trying

703
00:35:54,780 --> 00:35:57,310
to measure in their product.

704
00:35:57,310 --> 00:35:59,340
And so just as an
example, to try

705
00:35:59,340 --> 00:36:05,070
to think a little bit about
the measurement of education,

706
00:36:05,070 --> 00:36:09,360
do we want to measure
whether that fifth grader can

707
00:36:09,360 --> 00:36:12,570
do an arithmetic
problem, or is it

708
00:36:12,570 --> 00:36:15,180
more interesting
in today's world

709
00:36:15,180 --> 00:36:17,730
if we can measure
at the same time

710
00:36:17,730 --> 00:36:20,430
their improvement in
executive function

711
00:36:20,430 --> 00:36:23,680
to take on advanced
math concepts?

712
00:36:23,680 --> 00:36:27,700
So what are we trying to measure
when we're looking at things?

713
00:36:27,700 --> 00:36:30,630
And for startups, of course,
you can't measure everything

714
00:36:30,630 --> 00:36:33,310
you're wanting to do with
your product day one,

715
00:36:33,310 --> 00:36:35,460
but over time, you're
looking for being

716
00:36:35,460 --> 00:36:37,610
able to create good outputs.

717
00:36:37,610 --> 00:36:39,940
So this is about
instrumenting your software

718
00:36:39,940 --> 00:36:43,930
and instrumenting your system
to fit into the environment

719
00:36:43,930 --> 00:36:46,580
that you're going to
deliver the product in.

720
00:36:46,580 --> 00:36:50,020
And then we have an
entrepreneur in residence for AI

721
00:36:50,020 --> 00:36:53,360
that focuses on how
to leverage that data.

722
00:36:53,360 --> 00:36:55,810
And all of this is sort
of around our philosophy

723
00:36:55,810 --> 00:37:03,010
that education systems need
to be increasingly data driven

724
00:37:03,010 --> 00:37:06,580
and when some of that
data can be appropriately

725
00:37:06,580 --> 00:37:09,310
automated to do that.

726
00:37:09,310 --> 00:37:12,170
So really, that's sort
of our philosophy.

727
00:37:12,170 --> 00:37:14,260
And then let's take
a step back and look

728
00:37:14,260 --> 00:37:15,860
at the education industry.

729
00:37:15,860 --> 00:37:17,360
Is it ready for AI?

730
00:37:17,360 --> 00:37:21,050
And I think the big
answer is not really.

731
00:37:21,050 --> 00:37:24,340
It's a very big global
market and continues

732
00:37:24,340 --> 00:37:27,700
to grow as more and
more people come into

733
00:37:27,700 --> 00:37:33,550
want to be a part of the
economies of their countries.

734
00:37:33,550 --> 00:37:36,700
And certainly, there
were a few things

735
00:37:36,700 --> 00:37:38,780
that happened when
COVID went through.

736
00:37:38,780 --> 00:37:41,530
Certainly, here in the
States, we could see the--

737
00:37:41,530 --> 00:37:45,380
I guess I would call it-- the
further connection of the pipes,

738
00:37:45,380 --> 00:37:50,380
putting in the ability
to deliver products

739
00:37:50,380 --> 00:37:56,210
to people's homes, and as well
as into the classroom digitally.

740
00:37:56,210 --> 00:37:58,300
But many of the things
that were coming

741
00:37:58,300 --> 00:38:02,770
through were not super
effective and were sort of based

742
00:38:02,770 --> 00:38:05,570
on a rote memory learning.

743
00:38:05,570 --> 00:38:10,150
So we increasingly see that
education is still going

744
00:38:10,150 --> 00:38:11,690
through a digital transition.

745
00:38:11,690 --> 00:38:15,230
At the same time, it's
interacting with AI,

746
00:38:15,230 --> 00:38:18,410
which is, it's a big bite.

747
00:38:18,410 --> 00:38:21,670
It's a big thing to
bite off all at once.

748
00:38:21,670 --> 00:38:27,770
And another aspect installed
systems in education--

749
00:38:27,770 --> 00:38:30,940
and remember, the bulk
of the money in education

750
00:38:30,940 --> 00:38:33,970
is spent in formal education.

751
00:38:33,970 --> 00:38:39,900
So that would include
private and public schools

752
00:38:39,900 --> 00:38:43,320
and colleges and universities.

753
00:38:43,320 --> 00:38:47,670
And there is some amount
of parents buying something

754
00:38:47,670 --> 00:38:49,180
directly for their students.

755
00:38:49,180 --> 00:38:52,090
But really, the bulk of the
money is in these locations.

756
00:38:52,090 --> 00:38:54,250
And so if we're going
to affect education,

757
00:38:54,250 --> 00:38:56,400
we probably need to
build things that

758
00:38:56,400 --> 00:38:59,490
will be able to fit into
the systems that are there.

759
00:38:59,490 --> 00:39:04,740
And in particular, in the US,
we have silos of information,

760
00:39:04,740 --> 00:39:07,170
like say in a K-12
system that don't even

761
00:39:07,170 --> 00:39:10,010
communicate to each other and
are owned by different parties.

762
00:39:10,010 --> 00:39:12,970

763
00:39:12,970 --> 00:39:16,090
So let's step back and think a
little bit about this relatively

764
00:39:16,090 --> 00:39:21,520
slow moving industry and
where AI can come in.

765
00:39:21,520 --> 00:39:25,390
It's really interesting
to see that in the first,

766
00:39:25,390 --> 00:39:37,610
18 months after ChatGPT was
announced, about $280 billion

767
00:39:37,610 --> 00:39:43,640
of additional spending was spent
on implementing generative AI

768
00:39:43,640 --> 00:39:46,380
into businesses.

769
00:39:46,380 --> 00:39:52,130
And of that, a very
tiny percent, around 2%,

770
00:39:52,130 --> 00:39:55,230
went into education.

771
00:39:55,230 --> 00:39:58,970
And so it's an industry
that's relatively slow moving.

772
00:39:58,970 --> 00:40:01,080
Often, people are
worried about, well,

773
00:40:01,080 --> 00:40:03,450
how will we be able
to use that fairly?

774
00:40:03,450 --> 00:40:06,090
How will that be
affected-- affect us?

775
00:40:06,090 --> 00:40:08,325
So let's think about
this a little bit.

776
00:40:08,325 --> 00:40:09,950
What are some places
that we can really

777
00:40:09,950 --> 00:40:11,930
do things that are different?

778
00:40:11,930 --> 00:40:15,710
I think we just saw two
really phenomenal examples

779
00:40:15,710 --> 00:40:20,460
of interesting ways to use AI.

780
00:40:20,460 --> 00:40:25,340
And obviously, the more that
we understand the learner

781
00:40:25,340 --> 00:40:29,360
and make sure that the learning
is tuned to what they already

782
00:40:29,360 --> 00:40:32,210
know and what they
need to learn next,

783
00:40:32,210 --> 00:40:35,760
that individualization
or personalized pathway

784
00:40:35,760 --> 00:40:37,800
is valuable.

785
00:40:37,800 --> 00:40:40,350
Looking for patterns--
do we actually

786
00:40:40,350 --> 00:40:44,400
understand what's going
on and for our learners

787
00:40:44,400 --> 00:40:46,360
in a more effective way?

788
00:40:46,360 --> 00:40:49,180
And then obviously,
as we just saw,

789
00:40:49,180 --> 00:40:54,180
the ability to use interactive
environments to practice

790
00:40:54,180 --> 00:40:58,020
and to build on what
you already know.

791
00:40:58,020 --> 00:41:03,090
And certainly, people who
have watched young kids

792
00:41:03,090 --> 00:41:08,580
interact with some of these
really easy-to-use AI systems.

793
00:41:08,580 --> 00:41:12,780
Hope that kids will be out there
doing inquiry-based learning

794
00:41:12,780 --> 00:41:16,680
and finding the next new
thing they want to dig into.

795
00:41:16,680 --> 00:41:19,710
Not sure every
youngster is thinking

796
00:41:19,710 --> 00:41:23,080
about the world that way, but
it certainly a beautiful one.

797
00:41:23,080 --> 00:41:25,710
It's happening.

798
00:41:25,710 --> 00:41:30,000
Data silos-- again, this idea
that we can get information out.

799
00:41:30,000 --> 00:41:33,270
AI is an aid to learning
materials development,

800
00:41:33,270 --> 00:41:35,950
and we just saw in this
course development product.

801
00:41:35,950 --> 00:41:41,010
It is going to make it much
faster to get what you need to

802
00:41:41,010 --> 00:41:43,060
into people's hands.

803
00:41:43,060 --> 00:41:47,340
And I loved the lifelong
learning component

804
00:41:47,340 --> 00:41:50,700
of our presenter
because the idea

805
00:41:50,700 --> 00:41:53,850
that you want to teach
something about what

806
00:41:53,850 --> 00:42:01,090
you can see in using telescopes
to your astronomy class is fine.

807
00:42:01,090 --> 00:42:05,110
But could you include what
Hubble looked at last week?

808
00:42:05,110 --> 00:42:09,990
Wow, now that would
be exciting and keep

809
00:42:09,990 --> 00:42:13,290
people interested and engaged.

810
00:42:13,290 --> 00:42:16,170
I think there's a number of
really exciting things going on

811
00:42:16,170 --> 00:42:16,870
here.

812
00:42:16,870 --> 00:42:20,890
One side of that's
coming out, so K-12.

813
00:42:20,890 --> 00:42:22,380
It's really about
matching learning

814
00:42:22,380 --> 00:42:24,570
differences and the content.

815
00:42:24,570 --> 00:42:29,070
And particularly as we're
looking at education

816
00:42:29,070 --> 00:42:31,440
for the purpose of
employment, we're

817
00:42:31,440 --> 00:42:34,290
seeing increasing amount
of conversation about,

818
00:42:34,290 --> 00:42:38,220
how could we actually understand
the skills of the learner

819
00:42:38,220 --> 00:42:42,820
and be able to map those
to the job requirements?

820
00:42:42,820 --> 00:42:46,740
So we have this skills-based
hiring conversation

821
00:42:46,740 --> 00:42:50,040
going on all over the
industry right now.

822
00:42:50,040 --> 00:42:51,850
So let's see.

823
00:42:51,850 --> 00:42:53,940
I need to watch my
time here a little bit.

824
00:42:53,940 --> 00:42:58,980
I think there's some
very detailed areas that

825
00:42:58,980 --> 00:43:03,790
are effective for different
parts of the education system.

826
00:43:03,790 --> 00:43:06,930
And I'm going to provide
a couple examples

827
00:43:06,930 --> 00:43:11,010
from our own portfolio
in the next slide that

828
00:43:11,010 --> 00:43:16,830
get to a number of these
little details here.

829
00:43:16,830 --> 00:43:19,560
So I've got four
companies that we've

830
00:43:19,560 --> 00:43:22,920
worked with closely pulled
out here just to say how

831
00:43:22,920 --> 00:43:26,940
each of them are utilizing
AI or what part of AI

832
00:43:26,940 --> 00:43:30,460
they think is important
for their future.

833
00:43:30,460 --> 00:43:32,350
And so I'll start
at the beginning.

834
00:43:32,350 --> 00:43:37,950
So if you had asked me
when I started Learn Lunch

835
00:43:37,950 --> 00:43:40,410
whether or not first and
second-grade teachers were going

836
00:43:40,410 --> 00:43:42,960
to sit on the floor
of their classroom

837
00:43:42,960 --> 00:43:45,960
and put logic blocks
together with their kids

838
00:43:45,960 --> 00:43:48,780
to make little
microrobots, I would

839
00:43:48,780 --> 00:43:51,910
have been kind of
suspicious that they

840
00:43:51,910 --> 00:43:54,440
would be uncomfortable
with tech,

841
00:43:54,440 --> 00:43:57,710
but that's exactly what's
happening for Robo Wunderkind.

842
00:43:57,710 --> 00:44:02,080
They're not only bringing
in robots very early,

843
00:44:02,080 --> 00:44:03,980
before the kids are scared.

844
00:44:03,980 --> 00:44:06,560
These little
six-year-olds saying,

845
00:44:06,560 --> 00:44:07,910
I'm going to be an engineer.

846
00:44:07,910 --> 00:44:11,690
And, because if you bring in the
robots in fifth or sixth grade,

847
00:44:11,690 --> 00:44:14,980
the kids from the advantaged
backgrounds know what to do,

848
00:44:14,980 --> 00:44:17,290
and the kids from the
disadvantaged backgrounds

849
00:44:17,290 --> 00:44:18,230
step back.

850
00:44:18,230 --> 00:44:19,850
They don't know what to do.

851
00:44:19,850 --> 00:44:21,200
They're not included.

852
00:44:21,200 --> 00:44:22,870
And so that's one
of the reasons why

853
00:44:22,870 --> 00:44:24,550
we're sort of excited
about the idea

854
00:44:24,550 --> 00:44:27,110
that this can be
going these ways.

855
00:44:27,110 --> 00:44:30,070
She's now building
in coursework to have

856
00:44:30,070 --> 00:44:32,170
students by third
and fourth grade

857
00:44:32,170 --> 00:44:36,380
be able to not only do some
medium-weight programming,

858
00:44:36,380 --> 00:44:41,380
but also be able to, in some
cases, use some AI modules

859
00:44:41,380 --> 00:44:45,640
or understand a tool called
AI as an additional way

860
00:44:45,640 --> 00:44:47,870
to think about
getting things done.

861
00:44:47,870 --> 00:44:49,600
And she's hoping to
build that out so

862
00:44:49,600 --> 00:44:51,040
that by fifth and
sixth grade, she

863
00:44:51,040 --> 00:44:54,980
has some fairly interactive
components in there.

864
00:44:54,980 --> 00:44:57,550
And again, that's one of
the challenges for education

865
00:44:57,550 --> 00:45:03,370
and AI is, how do we get people
Involved InScribe is working

866
00:45:03,370 --> 00:45:11,170
on helping with a sense of
belonging in colleges by keeping

867
00:45:11,170 --> 00:45:14,500
persistent answers to
questions that get asked over

868
00:45:14,500 --> 00:45:17,680
and over again, whether they
be in student life or they

869
00:45:17,680 --> 00:45:20,500
be in coursework,
helping make sure

870
00:45:20,500 --> 00:45:22,690
that the right questions
are getting answered

871
00:45:22,690 --> 00:45:29,080
and providing some other small
community-based supports.

872
00:45:29,080 --> 00:45:33,370
Edmond is an example of the
silos of information problem

873
00:45:33,370 --> 00:45:37,420
for in the US, we have
very, very heavy support

874
00:45:37,420 --> 00:45:40,210
of special needs
students, but it

875
00:45:40,210 --> 00:45:43,480
comes with phenomenal,
complex government regulation

876
00:45:43,480 --> 00:45:46,030
and the need to
look up and find out

877
00:45:46,030 --> 00:45:48,910
exactly what kind
of accommodation

878
00:45:48,910 --> 00:45:53,330
is allowed for which student
on which type of activity.

879
00:45:53,330 --> 00:45:59,510
And the complexity of looking
that up is almost impossible.

880
00:45:59,510 --> 00:46:02,410
And so now this is not only
keeping this accommodation

881
00:46:02,410 --> 00:46:07,570
database, but also providing
richer interactions

882
00:46:07,570 --> 00:46:11,110
with the teacher to
provide hints as well

883
00:46:11,110 --> 00:46:13,640
as collect information
of the student learning.

884
00:46:13,640 --> 00:46:17,360
And Julius is using
AI, as I mentioned,

885
00:46:17,360 --> 00:46:23,030
for this skills-based
view of the universe.

886
00:46:23,030 --> 00:46:25,690
They're doing deep
skill analysis that's

887
00:46:25,690 --> 00:46:27,490
supported by the
Department of Energy

888
00:46:27,490 --> 00:46:31,690
for the different jobs
for solar installers,

889
00:46:31,690 --> 00:46:34,420
or they're working with
the state of Minnesota

890
00:46:34,420 --> 00:46:37,450
on training all of the
people that will manage

891
00:46:37,450 --> 00:46:41,000
21 solar farms for the state.

892
00:46:41,000 --> 00:46:43,870
And so they're not only
building out the courseware

893
00:46:43,870 --> 00:46:46,600
by assembling courseware
based on exactly

894
00:46:46,600 --> 00:46:49,940
what the learner needs to
learn from lots of sources.

895
00:46:49,940 --> 00:46:51,530
They don't make much courseware.

896
00:46:51,530 --> 00:46:54,610
They assemble it and provide it.

897
00:46:54,610 --> 00:46:57,310
But they also have built
these really deep ways

898
00:46:57,310 --> 00:47:02,260
of analyzing a per industry
skill analysis that's

899
00:47:02,260 --> 00:47:06,280
maybe ahead of the
market in number

900
00:47:06,280 --> 00:47:09,610
of details that can support.

901
00:47:09,610 --> 00:47:14,530
I was just down at
New York EdTech Week

902
00:47:14,530 --> 00:47:18,100
and saw some more
companies working

903
00:47:18,100 --> 00:47:22,360
on support of building
curriculum, et cetera.

904
00:47:22,360 --> 00:47:29,580
And AI is moving
very, very quickly.

905
00:47:29,580 --> 00:47:33,550
So companies that
are already in market

906
00:47:33,550 --> 00:47:37,550
are trying to go back and
bring additional capabilities,

907
00:47:37,550 --> 00:47:40,890
automate, and enhance things
that they already had.

908
00:47:40,890 --> 00:47:44,110

909
00:47:44,110 --> 00:47:48,190
But also we're seeing brand new
companies come in, particularly

910
00:47:48,190 --> 00:47:52,180
around things like course
development or a developed

911
00:47:52,180 --> 00:47:54,150
specific course.

912
00:47:54,150 --> 00:47:55,810
WOMAN: [NON-ENGLISH SPEECH]

913
00:47:55,810 --> 00:48:02,310

914
00:48:02,310 --> 00:48:05,260
MEGAN MITCHELL: If we can,
please make sure we're muted.

915
00:48:05,260 --> 00:48:07,760
JEAN HAMMOND: So this is just
a picture of all the companies

916
00:48:07,760 --> 00:48:10,700
that are in our
active portfolio just

917
00:48:10,700 --> 00:48:13,670
to show the diversity
of the kinds of things

918
00:48:13,670 --> 00:48:16,910
that we'll be bumping
into and working with.

919
00:48:16,910 --> 00:48:18,780
Education is not a single thing.

920
00:48:18,780 --> 00:48:22,930
It's a complex mix of things.

921
00:48:22,930 --> 00:48:27,610
So maybe just to wrap
up here, we really

922
00:48:27,610 --> 00:48:32,230
have to think about how we
can bring the human inside

923
00:48:32,230 --> 00:48:37,000
into the world of
an AI-mediated world

924
00:48:37,000 --> 00:48:42,010
and to build great jobs
that do have a human inside

925
00:48:42,010 --> 00:48:48,070
and try to figure out what
that world will look like.

926
00:48:48,070 --> 00:48:50,740
I think the industry
as a whole is

927
00:48:50,740 --> 00:48:55,580
in-- is in a discussion
about measuring what matters.

928
00:48:55,580 --> 00:48:58,280
And so what matters?

929
00:48:58,280 --> 00:49:01,010
How are we going to measure the
things that really do matter?

930
00:49:01,010 --> 00:49:03,760
How can we make sure that our
assessments aren't measuring

931
00:49:03,760 --> 00:49:11,560
what was needed for
1987 and instead are

932
00:49:11,560 --> 00:49:15,490
measuring what is needed today?

933
00:49:15,490 --> 00:49:19,047
We certainly need
to experiment with--

934
00:49:19,047 --> 00:49:21,970

935
00:49:21,970 --> 00:49:22,510
let's see.

936
00:49:22,510 --> 00:49:25,060
This is my hope for
the education industry

937
00:49:25,060 --> 00:49:27,370
that we get much
more experimental.

938
00:49:27,370 --> 00:49:30,610
We just let people go
and do things that we not

939
00:49:30,610 --> 00:49:34,240
have really rigid environments
because if we're bringing up

940
00:49:34,240 --> 00:49:36,730
kids in rigid
environments and then

941
00:49:36,730 --> 00:49:44,560
they get to the range of options
that AI hopefully will hopefully

942
00:49:44,560 --> 00:49:47,380
ethically provide to us.

943
00:49:47,380 --> 00:49:50,050
We're going to need
some people who

944
00:49:50,050 --> 00:49:53,270
are interested in
the finding things.

945
00:49:53,270 --> 00:49:57,560
We certainly can develop
more personalized pathways.

946
00:49:57,560 --> 00:50:02,770
And I'm a big fan of education
being more researched.

947
00:50:02,770 --> 00:50:06,970
When we go to research
what's going on in the world,

948
00:50:06,970 --> 00:50:09,500
we end up in a
much deeper place.

949
00:50:09,500 --> 00:50:14,170
So if what we're doing is
we're helping youngsters

950
00:50:14,170 --> 00:50:16,660
set memories and
understandings that

951
00:50:16,660 --> 00:50:19,220
allow critical thinking
in their brain,

952
00:50:19,220 --> 00:50:21,960
then we need to understand
how that happens,

953
00:50:21,960 --> 00:50:23,930
what's going on in
learning science

954
00:50:23,930 --> 00:50:31,430
as well as what's going on
in the education industry.

955
00:50:31,430 --> 00:50:32,520
MEGAN MITCHELL: Fantastic.

956
00:50:32,520 --> 00:50:33,240
Thank you.

957
00:50:33,240 --> 00:50:34,770
And thank you to all--

958
00:50:34,770 --> 00:50:36,215
wow-- all of our presenters.

959
00:50:36,215 --> 00:50:39,260
I think we kind of
took a deep dive

960
00:50:39,260 --> 00:50:42,620
into two sets of-- two sets of
technologies or two companies

961
00:50:42,620 --> 00:50:49,460
and then had a beautiful kind
of landscape look from Jean.

962
00:50:49,460 --> 00:50:51,270
I think for the folks
here on the call,

963
00:50:51,270 --> 00:50:54,290
there's going to be a lot of
googling and searching and doing

964
00:50:54,290 --> 00:50:58,330
more research into the
different companies.

965
00:50:58,330 --> 00:51:02,240
There's something I heard
throughout that I would love

966
00:51:02,240 --> 00:51:05,060
to-- a string I'd
love to pull on,

967
00:51:05,060 --> 00:51:08,780
and that is in
Joy's presentation,

968
00:51:08,780 --> 00:51:11,450
you had kind on
one of the sides.

969
00:51:11,450 --> 00:51:16,560
Education needs to be flexible,
responsive, affordable,

970
00:51:16,560 --> 00:51:18,600
adaptive, adaptive.

971
00:51:18,600 --> 00:51:23,280
And Jean, at the very beginning,
you kind of-- is education,

972
00:51:23,280 --> 00:51:24,510
is it ready for AI?

973
00:51:24,510 --> 00:51:26,490
Is the industry ready for AI?

974
00:51:26,490 --> 00:51:29,330
You want to see it
be more experimental.

975
00:51:29,330 --> 00:51:31,710
I think the people on
this call, the people who

976
00:51:31,710 --> 00:51:35,240
show up for these types
of sessions, are there.

977
00:51:35,240 --> 00:51:37,220
They're the early adopters.

978
00:51:37,220 --> 00:51:41,310
They're the ones willing
to do some experiments

979
00:51:41,310 --> 00:51:42,910
and try some things new.

980
00:51:42,910 --> 00:51:46,980
But I from a long career in
higher education, the education

981
00:51:46,980 --> 00:51:50,460
industry isn't always the
most flexible and responsive.

982
00:51:50,460 --> 00:51:54,520
And so how do you think about
from your vantage point,

983
00:51:54,520 --> 00:51:56,820
shifting mindsets
or equipping people

984
00:51:56,820 --> 00:52:01,530
to help really leverage
this type of technology,

985
00:52:01,530 --> 00:52:05,580
whether that's something
is core to your business

986
00:52:05,580 --> 00:52:09,335
is sales in building
a sales funnel

987
00:52:09,335 --> 00:52:11,460
and a sales pipeline,
particularly on an enterprise

988
00:52:11,460 --> 00:52:16,530
side, to getting out and
getting that evidence based,

989
00:52:16,530 --> 00:52:18,900
those findings that will
help change mindsets?

990
00:52:18,900 --> 00:52:21,010
Or maybe there's
some other great way.

991
00:52:21,010 --> 00:52:22,920
Maybe it's a
grassroots approach.

992
00:52:22,920 --> 00:52:25,650
So I'm just curious, from
each of the panelists, how

993
00:52:25,650 --> 00:52:30,810
you think that we can move
so many people forward

994
00:52:30,810 --> 00:52:33,660
in the industry that
are a little bit less

995
00:52:33,660 --> 00:52:35,970
flexible and responsive.

996
00:52:35,970 --> 00:52:39,150
JEAN HAMMOND: I think that the
things that we've always known

997
00:52:39,150 --> 00:52:42,000
really work in education--

998
00:52:42,000 --> 00:52:44,650
learning from your peers,
being in small groups,

999
00:52:44,650 --> 00:52:48,060
project-based learning,
experiential learning--

1000
00:52:48,060 --> 00:52:51,570
I think the more that we can
say, hey, one of the things

1001
00:52:51,570 --> 00:52:54,750
AI is bringing us is the ability
to bring what we've always known

1002
00:52:54,750 --> 00:53:00,570
work into the
environment and maybe

1003
00:53:00,570 --> 00:53:05,190
have less expectation
of everybody

1004
00:53:05,190 --> 00:53:09,660
in this group of 35
people or 25 people

1005
00:53:09,660 --> 00:53:11,850
are sitting in a room
together doing the same thing

1006
00:53:11,850 --> 00:53:13,180
at the same time.

1007
00:53:13,180 --> 00:53:15,270
And so probably
there's a mix of--

1008
00:53:15,270 --> 00:53:18,840
there's a mix of issues
going on that looks

1009
00:53:18,840 --> 00:53:22,680
like that if we can
help the teachers

1010
00:53:22,680 --> 00:53:26,550
and the instructors feel that
they're getting to do what they

1011
00:53:26,550 --> 00:53:31,540
always wanted to do by utilizing
some new tech, that should help.

1012
00:53:31,540 --> 00:53:34,630
The internet didn't
kill education.

1013
00:53:34,630 --> 00:53:39,270
So AI probably won't either.

1014
00:53:39,270 --> 00:53:42,510
DEEPAK VERMA: Yeah, I
think we have a more

1015
00:53:42,510 --> 00:53:45,700
kind of narrow kind of offer.

1016
00:53:45,700 --> 00:53:47,200
In some ways, it's
broad but narrow.

1017
00:53:47,200 --> 00:53:50,730
And what we're looking
for is across three

1018
00:53:50,730 --> 00:53:56,200
broad categories of education
or learning-- there's the K-12.

1019
00:53:56,200 --> 00:53:59,550
There's higher ed,
and then there's

1020
00:53:59,550 --> 00:54:02,010
youth, adults, et cetera.

1021
00:54:02,010 --> 00:54:06,270
And in each of those,
what are the glaring

1022
00:54:06,270 --> 00:54:10,020
problems that everybody sees,
and everybody is tackling?

1023
00:54:10,020 --> 00:54:13,570
And they're getting hurt by it
within the education system.

1024
00:54:13,570 --> 00:54:17,110
So, for example, in India,
and also in the US, I mean,

1025
00:54:17,110 --> 00:54:20,160
when you look at tests of
fifth graders or sixth graders

1026
00:54:20,160 --> 00:54:22,660
and how are they doing compared
to how they should be doing,

1027
00:54:22,660 --> 00:54:24,460
are they reading at
grade level, et cetera,

1028
00:54:24,460 --> 00:54:26,670
I mean that there is
real data around that.

1029
00:54:26,670 --> 00:54:29,110
And so that's what
we try to focus on.

1030
00:54:29,110 --> 00:54:33,130
And in some places,
we've had, I think,

1031
00:54:33,130 --> 00:54:34,780
a fair amount of early success.

1032
00:54:34,780 --> 00:54:37,090
And in some places, we've
had no movement at all.

1033
00:54:37,090 --> 00:54:43,210
So in higher ed, when we
focus our conversation around,

1034
00:54:43,210 --> 00:54:46,720
are you trying to admit
more international students,

1035
00:54:46,720 --> 00:54:51,760
and are you being unable to
do so because of language,

1036
00:54:51,760 --> 00:54:53,270
lack of ability?

1037
00:54:53,270 --> 00:54:54,770
Well, we can help with that.

1038
00:54:54,770 --> 00:54:58,660
Now, let's say you admit a
student who clears the aisles

1039
00:54:58,660 --> 00:55:02,420
or whatever, and joins,
and semester one happens.

1040
00:55:02,420 --> 00:55:04,340
Semester two, the
courses get harder.

1041
00:55:04,340 --> 00:55:06,550
Semester three, they
get even harder.

1042
00:55:06,550 --> 00:55:07,660
Now what?

1043
00:55:07,660 --> 00:55:12,820
And so are you seeing a drop-off
problem because some of these

1044
00:55:12,820 --> 00:55:14,470
folks who are not--

1045
00:55:14,470 --> 00:55:17,080
I mean, they've got marginal
level of language skills

1046
00:55:17,080 --> 00:55:20,403
encounter tough courses
with tough language.

1047
00:55:20,403 --> 00:55:21,320
Are they dropping out?

1048
00:55:21,320 --> 00:55:22,760
Well, we got a
solution for that.

1049
00:55:22,760 --> 00:55:25,160
And employability
is very different.

1050
00:55:25,160 --> 00:55:29,270
I think there's economics
that drive that system.

1051
00:55:29,270 --> 00:55:30,770
So it's not so hard.

1052
00:55:30,770 --> 00:55:35,460
I think what's hard is from
my perspective is like K-12.

1053
00:55:35,460 --> 00:55:38,783

1054
00:55:38,783 --> 00:55:40,200
I mean, those are
complex systems,

1055
00:55:40,200 --> 00:55:42,783
and you've got to work your way
through those complex systems.

1056
00:55:42,783 --> 00:55:44,730
And it's time consuming,
and it's hard.

1057
00:55:44,730 --> 00:55:46,570
But we're there.

1058
00:55:46,570 --> 00:55:48,810
We're going to keep
trying until we figure out

1059
00:55:48,810 --> 00:55:50,652
what the right answer is here.

1060
00:55:50,652 --> 00:55:52,110
JOY DASGUPTA: I
think I'll chime in

1061
00:55:52,110 --> 00:55:53,652
with a slightly
different perspective

1062
00:55:53,652 --> 00:55:58,490
because I think it's the
rate of change question.

1063
00:55:58,490 --> 00:56:00,223
And the world is changing.

1064
00:56:00,223 --> 00:56:01,640
There's nothing
we can do about it

1065
00:56:01,640 --> 00:56:03,690
at a rate that is accelerating.

1066
00:56:03,690 --> 00:56:05,190
It's not the change
is accelerating.

1067
00:56:05,190 --> 00:56:06,750
The rate of change
is accelerating.

1068
00:56:06,750 --> 00:56:08,460
So that's a double there.

1069
00:56:08,460 --> 00:56:12,270
And then that's on the
receiving side of your students.

1070
00:56:12,270 --> 00:56:14,690
And on the other
side, the technology

1071
00:56:14,690 --> 00:56:18,150
is evolving even faster.

1072
00:56:18,150 --> 00:56:19,800
So here we are in between.

1073
00:56:19,800 --> 00:56:24,320
How do we prepare these children
and young adults and adults

1074
00:56:24,320 --> 00:56:26,790
for that, connect the dots?

1075
00:56:26,790 --> 00:56:29,810
So I'll give you a vendor's
perspective, our perspective.

1076
00:56:29,810 --> 00:56:32,760
So we are trying
to sell into this.

1077
00:56:32,760 --> 00:56:35,310
Just to put on my hat.

1078
00:56:35,310 --> 00:56:36,990
So what do we do?

1079
00:56:36,990 --> 00:56:38,780
OK, so everybody's
change resistant.

1080
00:56:38,780 --> 00:56:41,660
Adults don't change easily.

1081
00:56:41,660 --> 00:56:44,760
So we are looking
for early adopters.

1082
00:56:44,760 --> 00:56:47,510
We are looking for the
path of least resistance

1083
00:56:47,510 --> 00:56:50,750
to get in and make the
point and try to demonstrate

1084
00:56:50,750 --> 00:56:51,780
what's in it for me.

1085
00:56:51,780 --> 00:56:53,660
Ultimately, that's the question.

1086
00:56:53,660 --> 00:56:56,860
Whether you're an instructor,
faculty, dean, provost,

1087
00:56:56,860 --> 00:56:58,470
what's in it for me?

1088
00:56:58,470 --> 00:57:00,480
And of course, my
institution maybe--

1089
00:57:00,480 --> 00:57:05,500
whatever your expanded
sense of me is.

1090
00:57:05,500 --> 00:57:07,337
And so that we have to hit.

1091
00:57:07,337 --> 00:57:09,420
But I think the other thing
that we did-- and just

1092
00:57:09,420 --> 00:57:11,110
to put it out there.

1093
00:57:11,110 --> 00:57:12,540
So with this core
generator thing

1094
00:57:12,540 --> 00:57:14,860
actually was born
from this idea,

1095
00:57:14,860 --> 00:57:17,880
which is let's go to a
place where it's kind of--

1096
00:57:17,880 --> 00:57:19,210
there's nobody there.

1097
00:57:19,210 --> 00:57:23,190
So this is we are going to
the instructional designer

1098
00:57:23,190 --> 00:57:26,970
in a place where he or she
is working on Microsoft Word

1099
00:57:26,970 --> 00:57:27,580
and Google.

1100
00:57:27,580 --> 00:57:32,500
There's no major
application in this part.

1101
00:57:32,500 --> 00:57:35,080
So there's really no resistance.

1102
00:57:35,080 --> 00:57:37,860
So we can get in, and we
can show the value that this

1103
00:57:37,860 --> 00:57:38,905
is how you can get going.

1104
00:57:38,905 --> 00:57:41,280
And after that, of course,
when you get into the learning

1105
00:57:41,280 --> 00:57:43,930
delivery part, there's a lot
of other competing tools.

1106
00:57:43,930 --> 00:57:46,270
But by then, maybe we
have changed a few minds.

1107
00:57:46,270 --> 00:57:49,350
So net net, I think this
is a change management

1108
00:57:49,350 --> 00:57:53,360
problem, which is obviously
well known but hard to crack.

1109
00:57:53,360 --> 00:57:56,230

1110
00:57:56,230 --> 00:57:58,010
JEAN HAMMOND: I love
you saying going

1111
00:57:58,010 --> 00:57:59,900
to where there's an open space.

1112
00:57:59,900 --> 00:58:02,930
And maybe that's one of the
things that will come from AI

1113
00:58:02,930 --> 00:58:07,580
and education is that some
of the less rigid education

1114
00:58:07,580 --> 00:58:11,310
environments will be able
to adopt new things faster.

1115
00:58:11,310 --> 00:58:17,720
And we may see some of the
big new markets in education

1116
00:58:17,720 --> 00:58:22,260
are Latin America, and
particularly Africa,

1117
00:58:22,260 --> 00:58:25,670
because there's just such a huge
percent of the population that's

1118
00:58:25,670 --> 00:58:29,870
still under educated,
phenomenally undereducated, lots

1119
00:58:29,870 --> 00:58:31,070
of white space.

1120
00:58:31,070 --> 00:58:34,370
MEGAN MITCHELL: So beyond
the dollars and cents

1121
00:58:34,370 --> 00:58:40,500
or in the revenues, how
do we think about impact?

1122
00:58:40,500 --> 00:58:42,990
I mean, you are in an
in education space.

1123
00:58:42,990 --> 00:58:45,440
And yes, people often adopt
new things because of,

1124
00:58:45,440 --> 00:58:46,958
what's in it for me?

1125
00:58:46,958 --> 00:58:48,000
Like, how can I use this?

1126
00:58:48,000 --> 00:58:49,950
What problems does
it solve for me?

1127
00:58:49,950 --> 00:58:53,330
But impact is important.

1128
00:58:53,330 --> 00:58:56,520
And in this kind of
ever-changing and evolving

1129
00:58:56,520 --> 00:59:00,420
environment, the way
people think about impact

1130
00:59:00,420 --> 00:59:01,270
might be different.

1131
00:59:01,270 --> 00:59:07,220
Are there metrics
that Joy and Deepak,

1132
00:59:07,220 --> 00:59:10,230
you use to think
about the change,

1133
00:59:10,230 --> 00:59:13,080
beyond that revenue, the type
of educational impact you're

1134
00:59:13,080 --> 00:59:13,740
having?

1135
00:59:13,740 --> 00:59:17,460
Or I'll say, Jean, are there--
is a company kind of in or two

1136
00:59:17,460 --> 00:59:19,410
in your portfolio
that are thinking

1137
00:59:19,410 --> 00:59:21,810
in really interesting
ways about demonstrating

1138
00:59:21,810 --> 00:59:25,230
the power of the work they're
doing through particular impact

1139
00:59:25,230 --> 00:59:26,590
measurements and metrics?

1140
00:59:26,590 --> 00:59:29,160
JEAN HAMMOND: So we
had to figure this--

1141
00:59:29,160 --> 00:59:30,910
the answer out to this
question ourselves.

1142
00:59:30,910 --> 00:59:32,200
So we did some soul searching.

1143
00:59:32,200 --> 00:59:35,050
And what I can tell
you is, starting out,

1144
00:59:35,050 --> 00:59:36,283
we're a for-profit company.

1145
00:59:36,283 --> 00:59:37,450
We're very clear about that.

1146
00:59:37,450 --> 00:59:39,930
I mean, I actually
am a strong believer

1147
00:59:39,930 --> 00:59:43,540
that it's for-profit companies
that can sustain and scale.

1148
00:59:43,540 --> 00:59:47,200
And so if you want to do that,
you have to be for-profit.

1149
00:59:47,200 --> 00:59:54,870
Having said that, I think we
are really driven by the idea

1150
00:59:54,870 --> 00:59:59,640
that we can change life
trajectories of people

1151
00:59:59,640 --> 01:00:03,270
that we engage with and
can impart learning to.

1152
01:00:03,270 --> 01:00:05,290
Think about English.

1153
01:00:05,290 --> 01:00:07,390
You think about
foundational skills.

1154
01:00:07,390 --> 01:00:09,630
If I'm a little bit
better at English today

1155
01:00:09,630 --> 01:00:12,460
than I was yesterday, there's
more that's accessible to me.

1156
01:00:12,460 --> 01:00:13,660
That's more I can learn.

1157
01:00:13,660 --> 01:00:17,310
Perhaps I can get a better job.

1158
01:00:17,310 --> 01:00:21,010
And you see this in
several professions.

1159
01:00:21,010 --> 01:00:22,930
If you're not that
great at English,

1160
01:00:22,930 --> 01:00:25,360
you have a limit in terms
of how far you can go.

1161
01:00:25,360 --> 01:00:27,610
And then beyond that,
if you're not skilled,

1162
01:00:27,610 --> 01:00:29,050
you're going to get stuck.

1163
01:00:29,050 --> 01:00:34,020
So our view is
every learner we can

1164
01:00:34,020 --> 01:00:38,310
impact is going to change
that learner's trajectory

1165
01:00:38,310 --> 01:00:40,380
for the rest of their lives.

1166
01:00:40,380 --> 01:00:42,930
That's what we want to get to.

1167
01:00:42,930 --> 01:00:45,280
And we're doing
work, as I mentioned,

1168
01:00:45,280 --> 01:00:46,630
in India and in the US.

1169
01:00:46,630 --> 01:00:48,570
In India, we have this problem
that there are a lot of people

1170
01:00:48,570 --> 01:00:49,987
who want to learn,
but they can't.

1171
01:00:49,987 --> 01:00:50,650
They can't pay.

1172
01:00:50,650 --> 01:00:52,380
So what do you do?

1173
01:00:52,380 --> 01:00:54,450
Well, in those cases,
what we're looking at

1174
01:00:54,450 --> 01:00:58,990
is how do we find
funding from donors?

1175
01:00:58,990 --> 01:01:01,860
Or there's this program in
India, CSR, Corporate Social

1176
01:01:01,860 --> 01:01:04,570
Responsibility, which companies
kind of put money into.

1177
01:01:04,570 --> 01:01:07,900
How can we find money from
those sources to pay for this?

1178
01:01:07,900 --> 01:01:10,260
We're not going to do it
for free because we can't.

1179
01:01:10,260 --> 01:01:12,010
But how do we make
that whole system work

1180
01:01:12,010 --> 01:01:13,677
so that we're not
saying no to learners?

1181
01:01:13,677 --> 01:01:17,553
So that's impact, as you know,
from a mission perspective.

1182
01:01:17,553 --> 01:01:19,470
The second question is,
how do you measure it?

1183
01:01:19,470 --> 01:01:25,110
And I think if you think
about how our system works,

1184
01:01:25,110 --> 01:01:26,080
somebody comes in.

1185
01:01:26,080 --> 01:01:26,950
They do a placement.

1186
01:01:26,950 --> 01:01:28,450
We understand what
level they're at.

1187
01:01:28,450 --> 01:01:30,910
We provide lessons, excuse me.

1188
01:01:30,910 --> 01:01:33,630
And then we assess again, and
they're at a higher level.

1189
01:01:33,630 --> 01:01:35,050
And then we provide less.

1190
01:01:35,050 --> 01:01:36,940
So you can see
those trajectories.

1191
01:01:36,940 --> 01:01:41,520
And there are various
different curriculum standards,

1192
01:01:41,520 --> 01:01:43,930
and all of them specify
different levels.

1193
01:01:43,930 --> 01:01:45,960
And as long as we're
tracking to folks

1194
01:01:45,960 --> 01:01:49,900
and seeing their
journeys move forward,

1195
01:01:49,900 --> 01:01:51,510
I mean, the system
itself is measuring

1196
01:01:51,510 --> 01:01:55,090
the impact on the goal,
if that makes sense.

1197
01:01:55,090 --> 01:01:55,840
JEAN HAMMOND: Yes.

1198
01:01:55,840 --> 01:01:59,560
I think that you've
pointed to something that--

1199
01:01:59,560 --> 01:02:02,740
first of all, I think
changing life trajectories

1200
01:02:02,740 --> 01:02:09,230
is the long-term important,
and then to some extent,

1201
01:02:09,230 --> 01:02:12,280
we'll increase the
knowledge of humans

1202
01:02:12,280 --> 01:02:15,610
by our education institutions
doing research, too.

1203
01:02:15,610 --> 01:02:18,470
But that's what
we're here about.

1204
01:02:18,470 --> 01:02:23,080
But I do think there's an
interesting side going on

1205
01:02:23,080 --> 01:02:27,220
in AI right now because some
of the types of use of AI,

1206
01:02:27,220 --> 01:02:31,760
for example, for individual
personal feedback,

1207
01:02:31,760 --> 01:02:35,110
either as sort of a
tutor or a practice bot.

1208
01:02:35,110 --> 01:02:39,740
If you're running live to an
AI system, it's expensive.

1209
01:02:39,740 --> 01:02:45,970
And so there's some limits
to what can be done.

1210
01:02:45,970 --> 01:02:49,670
And again, we don't have to
have everything run live.

1211
01:02:49,670 --> 01:02:55,250
I mean, there's a lot of clever
ways to get around some of that.

1212
01:02:55,250 --> 01:03:00,130
But certainly, right
now, one of the things

1213
01:03:00,130 --> 01:03:04,450
that will hold back as
well as fear from the users

1214
01:03:04,450 --> 01:03:07,920
not understanding what they're
working with from the users.

1215
01:03:07,920 --> 01:03:12,650
So like some of these
products we saw today will--

1216
01:03:12,650 --> 01:03:16,710
as well as those holding back
things going into education--

1217
01:03:16,710 --> 01:03:21,240
I think the cost
will be an issue.

1218
01:03:21,240 --> 01:03:25,970
And so that's an aside
on the underlying desire

1219
01:03:25,970 --> 01:03:29,820
to get more people
more educated.

1220
01:03:29,820 --> 01:03:31,820
DEEPAK VERMA: If I may
just respond to one thing

1221
01:03:31,820 --> 01:03:33,110
that you said,
Jean, because I do

1222
01:03:33,110 --> 01:03:34,440
think it's an important point.

1223
01:03:34,440 --> 01:03:37,530
And Joy, sorry, keeping
you waiting for a bit here.

1224
01:03:37,530 --> 01:03:41,750
But so I think, in general,
your point is absolutely right.

1225
01:03:41,750 --> 01:03:46,760
I do want to make one
in full disclosure here.

1226
01:03:46,760 --> 01:03:50,960
A lot of our AI secret sauce
is being developed by this guy

1227
01:03:50,960 --> 01:03:52,650
also on this panel, Joy.

1228
01:03:52,650 --> 01:03:55,590
So we use Gyan
within the shelter,

1229
01:03:55,590 --> 01:04:02,180
and that is a very light,
very computationally light,

1230
01:04:02,180 --> 01:04:06,270
way compared to some of
these larger language models.

1231
01:04:06,270 --> 01:04:09,470
And that's what enables us to do
many of the things we're doing.

1232
01:04:09,470 --> 01:04:13,200
And Joy, maybe you can answer
that question some more,

1233
01:04:13,200 --> 01:04:15,505
but I wanted to mention
that to everybody.

1234
01:04:15,505 --> 01:04:16,380
JOY DASGUPTA: Thanks.

1235
01:04:16,380 --> 01:04:21,570
So and also, I think Jean
implied and said this also.

1236
01:04:21,570 --> 01:04:22,710
Like I think we are--

1237
01:04:22,710 --> 01:04:26,080
by being in this space and
putting our solutions out there,

1238
01:04:26,080 --> 01:04:30,220
I think we are part of a bigger
movement that is accelerating,

1239
01:04:30,220 --> 01:04:38,010
hopefully access and equity
across economic, social, global

1240
01:04:38,010 --> 01:04:39,490
south, et cetera, et cetera.

1241
01:04:39,490 --> 01:04:42,207
So that's one point
that we are part of.

1242
01:04:42,207 --> 01:04:43,290
I mean, we are not direct.

1243
01:04:43,290 --> 01:04:45,358
We are contributing in
some small way, hopefully.

1244
01:04:45,358 --> 01:04:47,400
And the other thing is
the point that Deepak just

1245
01:04:47,400 --> 01:04:50,243
made on the sustainability
side of things.

1246
01:04:50,243 --> 01:04:51,660
And our student
impact necessarily

1247
01:04:51,660 --> 01:04:56,180
is we use a very small energy
footprint as opposed to we

1248
01:04:56,180 --> 01:04:57,930
don't have to set up
a nuclear power plant

1249
01:04:57,930 --> 01:05:00,910
to train our language.

1250
01:05:00,910 --> 01:05:04,450
I think that's it's kind of
getting there for some of these.

1251
01:05:04,450 --> 01:05:06,010
So we don't use GPUs.

1252
01:05:06,010 --> 01:05:08,010
We use CPUs for those
who are learning this.

1253
01:05:08,010 --> 01:05:10,870
So that's for the movement part.

1254
01:05:10,870 --> 01:05:15,540
But I think at the company
level, I think we don't--

1255
01:05:15,540 --> 01:05:19,770
within quotes-- we don't
directly touch the students.

1256
01:05:19,770 --> 01:05:22,440
Our customers are institutions.

1257
01:05:22,440 --> 01:05:24,810
So our impact measure is--

1258
01:05:24,810 --> 01:05:26,907
to put it in a very
bluntly is-- so we

1259
01:05:26,907 --> 01:05:28,740
are trying to-- how are
we trying to impact?

1260
01:05:28,740 --> 01:05:30,370
OK, let's just use two threads.

1261
01:05:30,370 --> 01:05:33,700
Continuous learning, we talked
about and personalization.

1262
01:05:33,700 --> 01:05:36,300
Those are, I think, two
major vectors of impact.

1263
01:05:36,300 --> 01:05:38,050
I mean, think about
personalized medicine.

1264
01:05:38,050 --> 01:05:41,230
Think about personalized
finance, financial advising,

1265
01:05:41,230 --> 01:05:41,740
whatever.

1266
01:05:41,740 --> 01:05:44,740
So personalized learning--
that's what we do.

1267
01:05:44,740 --> 01:05:47,587
And our measure
of impact is, this

1268
01:05:47,587 --> 01:05:49,320
will sound really
crass, is like,

1269
01:05:49,320 --> 01:05:52,410
are we getting our
contracts renewed?

1270
01:05:52,410 --> 01:05:54,940
If there's a renewal
of our contract,

1271
01:05:54,940 --> 01:05:58,210
that means we are making a
difference to that institution.

1272
01:05:58,210 --> 01:06:00,610
But the point here is we don't
directly own the students.

1273
01:06:00,610 --> 01:06:02,610
So it's an indirect impact.

1274
01:06:02,610 --> 01:06:04,090
MEGAN MITCHELL: Yeah, very good.

1275
01:06:04,090 --> 01:06:06,150
So let's talk about--

1276
01:06:06,150 --> 01:06:08,730
it's a great
question in the chat

1277
01:06:08,730 --> 01:06:13,560
about benchmarking or
existing benchmarks or tools.

1278
01:06:13,560 --> 01:06:17,300
This is such a new space, and
there's so much happening.

1279
01:06:17,300 --> 01:06:19,720
And how does someone
who is very much

1280
01:06:19,720 --> 01:06:23,500
interested wants to
adopt these technologies,

1281
01:06:23,500 --> 01:06:25,270
listening to all of
you that you can see?

1282
01:06:25,270 --> 01:06:26,470
You can feel.

1283
01:06:26,470 --> 01:06:29,720
These are technologies
that I want to explore.

1284
01:06:29,720 --> 01:06:32,740
But like that trust
component in such

1285
01:06:32,740 --> 01:06:36,310
a evolving and emerging
space, how do people

1286
01:06:36,310 --> 01:06:39,700
think about building
tools or validating

1287
01:06:39,700 --> 01:06:43,970
that these tools that you
have and you're delivering

1288
01:06:43,970 --> 01:06:47,090
are worthwhile or worth
or up to the standard

1289
01:06:47,090 --> 01:06:49,700
that you're talking
about them being at

1290
01:06:49,700 --> 01:06:52,550
and can really drive
the types of change?

1291
01:06:52,550 --> 01:06:57,230
How is someone an informed
consumer in something

1292
01:06:57,230 --> 01:06:59,980
that's such a new area?

1293
01:06:59,980 --> 01:07:03,043
JOY DASGUPTA: It's sort of
an AI certification stamp,

1294
01:07:03,043 --> 01:07:04,460
MEGAN MITCHELL:
Seal of approval--

1295
01:07:04,460 --> 01:07:09,100
yes good housekeeping,
good AI seal of approval.

1296
01:07:09,100 --> 01:07:10,600
JOY DASGUPTA: I can
try to go first.

1297
01:07:10,600 --> 01:07:13,330
I think there's two
levels of benchmarking,

1298
01:07:13,330 --> 01:07:14,920
and I can talk about
the first part.

1299
01:07:14,920 --> 01:07:16,940
First part is the
language model itself.

1300
01:07:16,940 --> 01:07:19,250
And there are lots of
benchmarks out there actually,

1301
01:07:19,250 --> 01:07:22,160
that are put out by various
different groups, like Google,

1302
01:07:22,160 --> 01:07:23,030
like Microsoft.

1303
01:07:23,030 --> 01:07:24,320
So there's msmarco.

1304
01:07:24,320 --> 01:07:28,550
If you're in the medical
field, there's PubMed QA.

1305
01:07:28,550 --> 01:07:30,790
There's natural questions from--

1306
01:07:30,790 --> 01:07:31,520
I forget who.

1307
01:07:31,520 --> 01:07:32,600
Maybe it's from Google.

1308
01:07:32,600 --> 01:07:36,430
So these are large
data sets with lots

1309
01:07:36,430 --> 01:07:38,860
of questions on the one
side and correct answers

1310
01:07:38,860 --> 01:07:39,800
on the other side.

1311
01:07:39,800 --> 01:07:44,350
And if your model can take these
questions, look at the data set,

1312
01:07:44,350 --> 01:07:46,780
and answer these questions
accurately, that's the test.

1313
01:07:46,780 --> 01:07:48,530
OK, where do you stand
on the leaderboard?

1314
01:07:48,530 --> 01:07:49,960
But that's for the
language model.

1315
01:07:49,960 --> 01:07:52,150
That is like the
engine of your car

1316
01:07:52,150 --> 01:07:55,100
is still not going to give you
a sense of how the car drives.

1317
01:07:55,100 --> 01:07:57,590
So that's proof of the engine.

1318
01:07:57,590 --> 01:07:59,720
Next is proof of
the car, which is,

1319
01:07:59,720 --> 01:08:03,320
let's say, the course
generator, in my case

1320
01:08:03,320 --> 01:08:06,380
that we don't have a
benchmark for because this

1321
01:08:06,380 --> 01:08:07,830
is a relatively new product.

1322
01:08:07,830 --> 01:08:10,340
We don't know who to
compare it with necessarily,

1323
01:08:10,340 --> 01:08:12,060
but I think we
need to get there.

1324
01:08:12,060 --> 01:08:15,055
But the language model part,
there is stuff out there.

1325
01:08:15,055 --> 01:08:16,680
I mean, many of you
probably know that.

1326
01:08:16,680 --> 01:08:19,279
And you should go and
look it up, and we are--

1327
01:08:19,279 --> 01:08:20,990
Yan is going to publish.

1328
01:08:20,990 --> 01:08:23,930
We are in the process of
publishing our own benchmarks.

1329
01:08:23,930 --> 01:08:26,687
MEGAN MITCHELL:
Fantastic, fantastic.

1330
01:08:26,687 --> 01:08:27,479
DEEPAK VERMA: Yeah.

1331
01:08:27,479 --> 01:08:30,420
And I guess on the
education side,

1332
01:08:30,420 --> 01:08:33,470
what we've done in the past
with our earlier product

1333
01:08:33,470 --> 01:08:38,330
is done a number of
RCTs randomized studies

1334
01:08:38,330 --> 01:08:41,630
where products are
implemented with some students

1335
01:08:41,630 --> 01:08:44,300
in some classrooms
against control groups

1336
01:08:44,300 --> 01:08:45,270
and how they perform.

1337
01:08:45,270 --> 01:08:49,819
So we have all that data,
and it's very strong.

1338
01:08:49,819 --> 01:08:52,102
With our new-- this
whole new platform,

1339
01:08:52,102 --> 01:08:53,060
it's a whole new thing.

1340
01:08:53,060 --> 01:08:53,819
It's brand new.

1341
01:08:53,819 --> 01:08:57,740
We've just we've just started
giving out demo accounts,

1342
01:08:57,740 --> 01:08:59,760
and we'll be starting
our first pilots.

1343
01:08:59,760 --> 01:09:02,930
And so we're also looking
for early, not even

1344
01:09:02,930 --> 01:09:06,930
adopters, early experimenters
who are saying, you know,

1345
01:09:06,930 --> 01:09:07,979
it sounds interesting.

1346
01:09:07,979 --> 01:09:09,870
Let me try it and
see for myself.

1347
01:09:09,870 --> 01:09:12,433
And they'll bring on
10, 20, 30 learners.

1348
01:09:12,433 --> 01:09:13,850
We'll give it two
or three months,

1349
01:09:13,850 --> 01:09:14,939
and we'll see how it's going.

1350
01:09:14,939 --> 01:09:16,022
And then we go from there.

1351
01:09:16,022 --> 01:09:19,100
But I guess over time,
we'll be able to generate

1352
01:09:19,100 --> 01:09:21,149
real data on impact.

1353
01:09:21,149 --> 01:09:24,160
But as of now, it's early
days for the new product.

1354
01:09:24,160 --> 01:09:25,160
MEGAN MITCHELL: Amazing.

1355
01:09:25,160 --> 01:09:27,160
That brings us back to
the experimental approach

1356
01:09:27,160 --> 01:09:29,100
that Jean was talking about.

1357
01:09:29,100 --> 01:09:31,859
We need people that have that.

1358
01:09:31,859 --> 01:09:34,200
JEAN HAMMOND: But I
think in many places,

1359
01:09:34,200 --> 01:09:37,130
people try to measure
impact by trying

1360
01:09:37,130 --> 01:09:38,939
to understand what change.

1361
01:09:38,939 --> 01:09:41,630
The fact that this
product exists

1362
01:09:41,630 --> 01:09:47,340
is that this capability, and the
teachers exist, can be measured.

1363
01:09:47,340 --> 01:09:49,472
And so the so-called
logic models

1364
01:09:49,472 --> 01:09:52,014
are a way of thinking about what
are the inputs, the outputs,

1365
01:09:52,014 --> 01:09:53,790
and over time.

1366
01:09:53,790 --> 01:09:56,340
And we work with lots of
early-stage companies.

1367
01:09:56,340 --> 01:09:58,880
And so what we know is you
can't measure everything

1368
01:09:58,880 --> 01:10:00,900
that you want to do early on.

1369
01:10:00,900 --> 01:10:04,332
And so we sometimes think that
if a company has a product

1370
01:10:04,332 --> 01:10:06,540
roadmap, what they're going
to do with their product,

1371
01:10:06,540 --> 01:10:09,380
they should also have an
impact measurement roadmap that

1372
01:10:09,380 --> 01:10:12,740
goes with it, increasing
amounts of additional data

1373
01:10:12,740 --> 01:10:16,110
that they'll be able to access,
both from their own system,

1374
01:10:16,110 --> 01:10:17,850
from instrumenting
their own system,

1375
01:10:17,850 --> 01:10:21,530
but also from perhaps getting
the environment in which

1376
01:10:21,530 --> 01:10:25,200
the product is delivered, say
the school district or whatever,

1377
01:10:25,200 --> 01:10:26,450
to also--

1378
01:10:26,450 --> 01:10:29,720
to also be able
to share data out

1379
01:10:29,720 --> 01:10:32,090
of other parts of
the infrastructure

1380
01:10:32,090 --> 01:10:33,930
that's being used for education.

1381
01:10:33,930 --> 01:10:36,360
So do we know what
the right answer is?

1382
01:10:36,360 --> 01:10:43,370
No, I think we're in the middle
of a societal or worldwide

1383
01:10:43,370 --> 01:10:45,000
experiment to try
to figure out, well,

1384
01:10:45,000 --> 01:10:47,010
what is it that we're
trying to measure?

1385
01:10:47,010 --> 01:10:49,040
And I think that we're
trying to measure

1386
01:10:49,040 --> 01:10:50,790
better lives for more people.

1387
01:10:50,790 --> 01:10:54,890
We're trying to measure
that with equitable access.

1388
01:10:54,890 --> 01:10:56,540
How are we going to get there?

1389
01:10:56,540 --> 01:10:58,275
Let's work on it.

1390
01:10:58,275 --> 01:11:00,150
MEGAN MITCHELL: And OK,
so we're out of time,

1391
01:11:00,150 --> 01:11:01,320
but I want to ask one.

1392
01:11:01,320 --> 01:11:03,480
It's kind of pulling on
a question in the chat.

1393
01:11:03,480 --> 01:11:05,430
And so if you can just
answer really quickly.

1394
01:11:05,430 --> 01:11:08,940
I mean, the speed of
change, the pace of change,

1395
01:11:08,940 --> 01:11:10,440
the rate of change is shifting.

1396
01:11:10,440 --> 01:11:11,553
I mean, what's next?

1397
01:11:11,553 --> 01:11:13,220
Like you're already
talking about things

1398
01:11:13,220 --> 01:11:17,600
that feel like
we're on the edge.

1399
01:11:17,600 --> 01:11:21,270
We know that in two months,
three months, six months,

1400
01:11:21,270 --> 01:11:23,803
they're going to be new
features or additional AI tools.

1401
01:11:23,803 --> 01:11:25,220
Like what should
we be looking out

1402
01:11:25,220 --> 01:11:27,270
for just in a couple sentences?

1403
01:11:27,270 --> 01:11:30,590
Each of you, what's
the next horizon?

1404
01:11:30,590 --> 01:11:31,740
It's not even a horizon.

1405
01:11:31,740 --> 01:11:34,393
It's like, what's
around the next corner.

1406
01:11:34,393 --> 01:11:35,810
JEAN HAMMOND: Well,
I'll go first.

1407
01:11:35,810 --> 01:11:37,460
I think we have
to be looking out

1408
01:11:37,460 --> 01:11:41,490
for making sure that we build
the humans in the system.

1409
01:11:41,490 --> 01:11:43,400
I mean, education is social.

1410
01:11:43,400 --> 01:11:44,900
We learn from each
other much better

1411
01:11:44,900 --> 01:11:49,250
than we learn from sitting in
front of a boring old screen.

1412
01:11:49,250 --> 01:11:51,930
So, really, how do
we build in humans,

1413
01:11:51,930 --> 01:11:55,220
in the system across
all of these areas?

1414
01:11:55,220 --> 01:11:57,650
DEEPAK VERMA: And
answering more specifically

1415
01:11:57,650 --> 01:12:01,820
from my English
helper hat, we're

1416
01:12:01,820 --> 01:12:04,850
working on some very
interesting stuff that

1417
01:12:04,850 --> 01:12:10,490
will enable really proficient
users to better comprehend what

1418
01:12:10,490 --> 01:12:12,890
they're reading through our app.

1419
01:12:12,890 --> 01:12:15,690
So you think about a
nine-page research paper.

1420
01:12:15,690 --> 01:12:17,720
It's dense.

1421
01:12:17,720 --> 01:12:18,965
You guys are all from MIT.

1422
01:12:18,965 --> 01:12:21,090
Maybe you can read it once
and understand it fully,

1423
01:12:21,090 --> 01:12:22,440
but this is pretty hard.

1424
01:12:22,440 --> 01:12:24,770
But what we're doing
is we're building tools

1425
01:12:24,770 --> 01:12:26,960
to help even
advanced users really

1426
01:12:26,960 --> 01:12:29,880
be able to consume what they're
consuming through the app.

1427
01:12:29,880 --> 01:12:33,500
So ultimate aspiration is
for all of us, everyone,

1428
01:12:33,500 --> 01:12:36,090
to have this app in their
pocket and consume whatever

1429
01:12:36,090 --> 01:12:37,590
they're reading with it.

1430
01:12:37,590 --> 01:12:40,048
JOY DASGUPTA: And the degree
of noise is going to increase.

1431
01:12:40,048 --> 01:12:42,130
I think maybe that's the
basis of the question.

1432
01:12:42,130 --> 01:12:44,160
How do I navigate this noise?

1433
01:12:44,160 --> 01:12:47,490
I think maybe we can get
back to some of Jean's points

1434
01:12:47,490 --> 01:12:50,260
and 5 Ds on that last slide.

1435
01:12:50,260 --> 01:12:53,370
I think we have to get
grounded in what we want

1436
01:12:53,370 --> 01:12:55,260
and what we need to
do with our students

1437
01:12:55,260 --> 01:13:00,270
and let the tools just be
tools and not drive us.

1438
01:13:00,270 --> 01:13:05,310
Right now, the tail and the
dog is getting all mixed up.

1439
01:13:05,310 --> 01:13:08,700
So I think we just
need to be the dog.

1440
01:13:08,700 --> 01:13:11,430
And the future is
completely uncertain.

1441
01:13:11,430 --> 01:13:13,680
Nobody should even attempt
to predict it, particularly

1442
01:13:13,680 --> 01:13:14,860
in the AI world.

1443
01:13:14,860 --> 01:13:16,150
It's not possible.

1444
01:13:16,150 --> 01:13:18,630
Even the developers
can't predict it.

1445
01:13:18,630 --> 01:13:19,910
Sam Altman can't predict it.

1446
01:13:19,910 --> 01:13:20,910
I mean, he doesn't know.

1447
01:13:20,910 --> 01:13:24,060
He's surprised by the next
release of his own product.

1448
01:13:24,060 --> 01:13:28,200
OK, so maybe we get out of that
business and stay grounded on,

1449
01:13:28,200 --> 01:13:31,180
what are we trying to
accomplish here in education?

1450
01:13:31,180 --> 01:13:33,150
So that's maybe an
indirect answer.

1451
01:13:33,150 --> 01:13:36,460
MEGAN MITCHELL: That is the
perfect way to bring us back.

1452
01:13:36,460 --> 01:13:39,460
Like we're all what do we want
to accomplish in education?

1453
01:13:39,460 --> 01:13:43,210
And think of these as tools
and focus on the core mission.

1454
01:13:43,210 --> 01:13:45,288
So thank you all very much.

1455
01:13:45,288 --> 01:13:46,830
Thank you to our
audience for staying

1456
01:13:46,830 --> 01:13:48,580
and for allowing us to go.

1457
01:13:48,580 --> 01:13:51,030
Just indulge us to go just
a few minutes over so we

1458
01:13:51,030 --> 01:13:54,060
could get this look forward.

1459
01:13:54,060 --> 01:13:58,420
Thank you very much, Bill for
bringing this group together,

1460
01:13:58,420 --> 01:14:01,410
Joy, Deepak, Jean, for
all the great insights,

1461
01:14:01,410 --> 01:14:04,260
and for our folks in the
audience for your questions

1462
01:14:04,260 --> 01:14:09,360
and your attention, Amy has put
a link in this chat for a survey

1463
01:14:09,360 --> 01:14:12,750
that we would appreciate
input from our participants.

1464
01:14:12,750 --> 01:14:13,680
And thank you all.

1465
01:14:13,680 --> 01:14:15,250
This has been recorded.

1466
01:14:15,250 --> 01:14:17,440
We will be sharing
that on our connector.

1467
01:14:17,440 --> 01:14:18,310
But thank you.

1468
01:14:18,310 --> 01:14:18,940
Thank you.

1469
01:14:18,940 --> 01:14:22,050
Have a wonderful day,
evening, and rest

1470
01:14:22,050 --> 01:14:23,553
of the week, wherever you are.

1471
01:14:23,553 --> 01:14:24,970
DEEPAK VERMA: Thank
you very much.

1472
01:14:24,970 --> 01:14:25,970
JOY DASGUPTA: Thank you.

1473
01:14:25,970 --> 01:14:29,410
[MUSIC PLAYING]

1474
01:14:29,410 --> 01:14:34,000

