1
00:00:04,880 --> 00:00:09,400
R and Computing cir a social ethical

2
00:00:07,799 --> 00:00:12,080
responsibilities of computing we're

3
00:00:09,400 --> 00:00:15,000
going have a bunch of talks and um uh

4
00:00:12,080 --> 00:00:17,240
sort of discussions um on topics related

5
00:00:15,000 --> 00:00:21,400
this kicking off with kadim Nori who's a

6
00:00:17,240 --> 00:00:23,080
postto here at MIT in eccon right and um

7
00:00:21,400 --> 00:00:24,199
and going to tell us about whether or

8
00:00:23,080 --> 00:00:27,119
ask us the question of whether

9
00:00:24,199 --> 00:00:29,190
algorithms will make the world more fat

10
00:00:27,119 --> 00:00:35,270
give it up for

11
00:00:29,190 --> 00:00:35,270
[Applause]

12
00:00:35,840 --> 00:00:43,039
awesome thank you I really appreciate

13
00:00:37,800 --> 00:00:45,399
the introduction um so uh just uh so I'm

14
00:00:43,039 --> 00:00:48,559
a future of work postto in the econ

15
00:00:45,399 --> 00:00:51,320
Department um and generally what I do is

16
00:00:48,559 --> 00:00:53,039
I'm a labor Economist uh and you know

17
00:00:51,320 --> 00:00:55,120
specifically I'm interested in the

18
00:00:53,039 --> 00:00:58,840
economics of talent selection and talent

19
00:00:55,120 --> 00:01:01,160
allocation uh in the economy um the

20
00:00:58,840 --> 00:01:04,760
title will algorithms make the world

21
00:01:01,160 --> 00:01:07,000
more fair o sorry about that does anyone

22
00:01:04,760 --> 00:01:09,240
happen toh think they have an answer to

23
00:01:07,000 --> 00:01:12,040
that

24
00:01:09,240 --> 00:01:15,320
question yes no

25
00:01:12,040 --> 00:01:17,280
okay okay uh I think uh yes I think I

26
00:01:15,320 --> 00:01:22,880
heard a yes that's uh that's correct I

27
00:01:17,280 --> 00:01:24,759
guess I'm done here um no um no and and

28
00:01:22,880 --> 00:01:25,920
it depends that's actually pretty much

29
00:01:24,759 --> 00:01:28,880
going to be the answer that I'm going to

30
00:01:25,920 --> 00:01:31,680
come to but we'll get there um the

31
00:01:28,880 --> 00:01:33,759
subtitle uh you know a wide- ranging

32
00:01:31,680 --> 00:01:36,720
discussion I think also has some meaning

33
00:01:33,759 --> 00:01:38,560
here uh My Hope Is that uh as I go

34
00:01:36,720 --> 00:01:41,320
through this talk I'm actually going to

35
00:01:38,560 --> 00:01:43,360
be able to get some questions and

36
00:01:41,320 --> 00:01:45,200
thoughts from you all uh and so there's

37
00:01:43,360 --> 00:01:47,960
going to be prompting um there's going

38
00:01:45,200 --> 00:01:51,439
to be less uh maybe a lecture and more

39
00:01:47,960 --> 00:01:52,960
of a structured uh conversation about uh

40
00:01:51,439 --> 00:01:56,079
some of these Notions of algorithmic

41
00:01:52,960 --> 00:01:59,520
fairness and what they mean in practice

42
00:01:56,079 --> 00:02:00,640
uh okay so uh I you know I start I think

43
00:01:59,520 --> 00:02:02,479
a lot of people are thinking about this

44
00:02:00,640 --> 00:02:04,719
question it's a question that's uh in

45
00:02:02,479 --> 00:02:08,879
the culture RIT large and so it's not

46
00:02:04,719 --> 00:02:10,959
uncommon to see uh headlines like this

47
00:02:08,879 --> 00:02:15,040
you know the rise of AI puts Spotlight

48
00:02:10,959 --> 00:02:16,519
on bias and algorithms um or you know

49
00:02:15,040 --> 00:02:20,360
headlines that let me look something

50
00:02:16,519 --> 00:02:21,280
like this um you know AI machines they

51
00:02:20,360 --> 00:02:24,440
might be

52
00:02:21,280 --> 00:02:26,680
racist um maybe even areas like health

53
00:02:24,440 --> 00:02:29,560
or life insurance there's uh questions

54
00:02:26,680 --> 00:02:32,280
of bias and so for years I've been

55
00:02:29,560 --> 00:02:35,440
seeing headlines like these that suggest

56
00:02:32,280 --> 00:02:36,800
that you know things are bad potentially

57
00:02:35,440 --> 00:02:39,879
you know maybe algorithms are actually

58
00:02:36,800 --> 00:02:42,879
going to make things worse um the only

59
00:02:39,879 --> 00:02:44,840
challenge is that um you know it's

60
00:02:42,879 --> 00:02:47,040
almost equally common to see headlines

61
00:02:44,840 --> 00:02:49,560
that look something like this biased

62
00:02:47,040 --> 00:02:53,720
algorithms are easier to fix than biased

63
00:02:49,560 --> 00:02:56,519
people um or you know five ways AI is

64
00:02:53,720 --> 00:03:00,480
making the world a safer place um you

65
00:02:56,519 --> 00:03:02,599
know uh and you know Fair AI why why

66
00:03:00,480 --> 00:03:06,840
everyone should profit from the AI boom

67
00:03:02,599 --> 00:03:09,480
and so there's at least as much um you

68
00:03:06,840 --> 00:03:10,760
know for every concern there seems to be

69
00:03:09,480 --> 00:03:13,360
somebody that thinks that this is going

70
00:03:10,760 --> 00:03:15,720
to ultimately be a Panacea and so what

71
00:03:13,360 --> 00:03:17,959
are the different ways of thinking about

72
00:03:15,720 --> 00:03:20,640
um algorithmic fairness whether or not

73
00:03:17,959 --> 00:03:23,959
it'll make the world fairer or or better

74
00:03:20,640 --> 00:03:27,280
um in this discussion what I'm hoping to

75
00:03:23,959 --> 00:03:29,599
do is I'm going to walk through two

76
00:03:27,280 --> 00:03:31,920
concrete case studies of algorithm use

77
00:03:29,599 --> 00:03:34,400
that is are closely related to my

78
00:03:31,920 --> 00:03:38,120
research um and that's going to be using

79
00:03:34,400 --> 00:03:41,000
AI in recruiting for Tech workers and

80
00:03:38,120 --> 00:03:44,080
then also uh algorithmic cohort

81
00:03:41,000 --> 00:03:47,120
selection um with an application to

82
00:03:44,080 --> 00:03:49,239
admissions uh and you know through that

83
00:03:47,120 --> 00:03:52,599
I'm hoping to discuss what it means to

84
00:03:49,239 --> 00:03:55,439
make algorithms fair in these contexts

85
00:03:52,599 --> 00:03:57,280
um I'm going to the discussion part is

86
00:03:55,439 --> 00:03:58,640
underlined I've mentioned this uh I'm

87
00:03:57,280 --> 00:04:00,280
going to you know be asking questions

88
00:03:58,640 --> 00:04:02,319
and asking you guys to contribute there

89
00:04:00,280 --> 00:04:04,079
are going to be no wrong answers here as

90
00:04:02,319 --> 00:04:06,599
you've seen there's a wide range of

91
00:04:04,079 --> 00:04:09,200
views on whether or not um algorithms

92
00:04:06,599 --> 00:04:11,879
are are fair and so I really am hoping

93
00:04:09,200 --> 00:04:13,959
this will uh you know spur different

94
00:04:11,879 --> 00:04:17,040
people to give various

95
00:04:13,959 --> 00:04:19,720
ideas I'm also going to introduce four

96
00:04:17,040 --> 00:04:21,799
types of fairness that are discussed in

97
00:04:19,720 --> 00:04:24,639
the AL algorithmic fairness literature

98
00:04:21,799 --> 00:04:26,000
to give us um you know some common

99
00:04:24,639 --> 00:04:29,120
language to maybe talk about these

100
00:04:26,000 --> 00:04:31,880
things intelligently and then uh at the

101
00:04:29,120 --> 00:04:34,720
end I have a bonus where uh I would like

102
00:04:31,880 --> 00:04:36,759
to discuss what might what might what

103
00:04:34,720 --> 00:04:40,400
might make humans distinct from AI in

104
00:04:36,759 --> 00:04:41,880
the context of work um I can pretty much

105
00:04:40,400 --> 00:04:42,880
say right now I think that I'm probably

106
00:04:41,880 --> 00:04:47,800
not going to have time to get to that

107
00:04:42,880 --> 00:04:50,360
bonus but we'll see um okay so the first

108
00:04:47,800 --> 00:04:52,840
case which comes from a topic I'm

109
00:04:50,360 --> 00:04:56,080
researching uh is it's about considering

110
00:04:52,840 --> 00:05:00,120
humans versus generative AI in

111
00:04:56,080 --> 00:05:01,960
recruiting so in order to understand uh

112
00:05:00,120 --> 00:05:04,400
this application you need to know a

113
00:05:01,960 --> 00:05:06,440
little bit about the hiring process for

114
00:05:04,400 --> 00:05:10,000
Tech workers and for Tech workers

115
00:05:06,440 --> 00:05:13,120
specifically software Engineers um and

116
00:05:10,000 --> 00:05:16,840
so the first part of this process it

117
00:05:13,120 --> 00:05:19,880
starts with a recruiter screen um and

118
00:05:16,840 --> 00:05:22,000
there what happens is that generally a

119
00:05:19,880 --> 00:05:24,280
human is looking at a resume and they're

120
00:05:22,000 --> 00:05:25,800
asking is this resume good or is it good

121
00:05:24,280 --> 00:05:29,919
enough is this somebody that I should

122
00:05:25,800 --> 00:05:32,080
give an actual interview to

123
00:05:29,919 --> 00:05:34,319
roughly in the data that I've worked

124
00:05:32,080 --> 00:05:36,080
with roughly 50% of people pass these

125
00:05:34,319 --> 00:05:38,319
kind these kinds of uh screens and then

126
00:05:36,080 --> 00:05:41,160
the next part would be a technical

127
00:05:38,319 --> 00:05:44,319
interview where you actually do a coding

128
00:05:41,160 --> 00:05:47,120
test and um the interviewer

129
00:05:44,319 --> 00:05:48,639
decides can you code well enough to move

130
00:05:47,120 --> 00:05:51,479
move Beyond and for us to consider

131
00:05:48,639 --> 00:05:53,520
hiring you uh and then the final thing

132
00:05:51,479 --> 00:05:55,479
would be uh the actual hiring decision

133
00:05:53,520 --> 00:05:58,360
of the people who make it through these

134
00:05:55,479 --> 00:06:00,560
two screens do we uh make a decision to

135
00:05:58,360 --> 00:06:02,479
hire and

136
00:06:00,560 --> 00:06:07,319
a question that a number of companies

137
00:06:02,479 --> 00:06:09,440
are asking themselves is um should the

138
00:06:07,319 --> 00:06:12,919
recruiters uh be replaced with

139
00:06:09,440 --> 00:06:15,440
generative Ai and by this I mean systems

140
00:06:12,919 --> 00:06:18,639
either built on or similar to chat GPT

141
00:06:15,440 --> 00:06:20,880
clad Gro some of these uh large language

142
00:06:18,639 --> 00:06:23,440
learning um large language models that

143
00:06:20,880 --> 00:06:25,800
have that have been Incorporated

144
00:06:23,440 --> 00:06:28,240
and you know in particular so I'm going

145
00:06:25,800 --> 00:06:30,360
to be focusing this discussion around

146
00:06:28,240 --> 00:06:32,680
this first part where you're doing a

147
00:06:30,360 --> 00:06:36,199
recruiter screen one thing I should say

148
00:06:32,680 --> 00:06:37,960
is that um there are a lot of reasons

149
00:06:36,199 --> 00:06:40,319
that organizations are interested in

150
00:06:37,960 --> 00:06:43,680
using AI in this part of the hiring

151
00:06:40,319 --> 00:06:46,240
process oftentimes um recruiters don't

152
00:06:43,680 --> 00:06:47,639
necessarily have technical experience

153
00:06:46,240 --> 00:06:49,360
but they're looking at resumés and

154
00:06:47,639 --> 00:06:51,240
they're making judgments on who should

155
00:06:49,360 --> 00:06:52,840
get a technical interview and so there's

156
00:06:51,240 --> 00:06:55,120
reasons to think that there are mistakes

157
00:06:52,840 --> 00:06:57,080
made and so you know it's not it's it's

158
00:06:55,120 --> 00:06:59,240
I think there's some justification to

159
00:06:57,080 --> 00:07:01,800
try and use generative AI here it makes

160
00:06:59,240 --> 00:07:01,800
a lot of

161
00:07:01,919 --> 00:07:08,599
sense okay so let's think about if we

162
00:07:06,039 --> 00:07:10,599
were to use AI in this context what the

163
00:07:08,599 --> 00:07:13,680
hypothetical recruiting problem actually

164
00:07:10,599 --> 00:07:16,919
looks like so imagine that you have an

165
00:07:13,680 --> 00:07:18,479
applicant pool and uh you're what you're

166
00:07:16,919 --> 00:07:20,479
ultimately trying to do is Select which

167
00:07:18,479 --> 00:07:23,440
candidates deserve an

168
00:07:20,479 --> 00:07:26,680
interview you can also Imagine uh you

169
00:07:23,440 --> 00:07:29,319
know that AI can observe um in this case

170
00:07:26,680 --> 00:07:30,680
I'm just going to simplify things and

171
00:07:29,319 --> 00:07:32,360
there you know they're they're

172
00:07:30,680 --> 00:07:35,520
individuals that are orange and white

173
00:07:32,360 --> 00:07:37,319
I'm going to assume that those colors uh

174
00:07:35,520 --> 00:07:39,080
are representative of some identity

175
00:07:37,319 --> 00:07:41,720
characteristic so you can think of that

176
00:07:39,080 --> 00:07:44,360
as um a demographic characteristic it

177
00:07:41,720 --> 00:07:45,560
could be it could be gender uh it could

178
00:07:44,360 --> 00:07:49,319
also be

179
00:07:45,560 --> 00:07:50,800
race they also observe resumes in this

180
00:07:49,319 --> 00:07:53,599
case so and that's going to have your

181
00:07:50,800 --> 00:07:57,159
education your work history

182
00:07:53,599 --> 00:07:59,080
Etc and so you know the first discussion

183
00:07:57,159 --> 00:08:02,240
question that I'd like to ask the

184
00:07:59,080 --> 00:08:04,759
audience is what are the various ways

185
00:08:02,240 --> 00:08:07,840
that AI recruiting decisions might be

186
00:08:04,759 --> 00:08:11,000
considered unfair um at this at this

187
00:08:07,840 --> 00:08:12,560
stage um and you know what I originally

188
00:08:11,000 --> 00:08:15,960
was going to do is I was going to have

189
00:08:12,560 --> 00:08:17,960
you guys turn uh to audience members

190
00:08:15,960 --> 00:08:20,199
next to you and have a discussion about

191
00:08:17,960 --> 00:08:21,639
this given the distribution of the

192
00:08:20,199 --> 00:08:24,159
audience not everybody has someone

193
00:08:21,639 --> 00:08:26,919
sitting next to them um but I I would

194
00:08:24,159 --> 00:08:29,039
actually you know love to get answers to

195
00:08:26,919 --> 00:08:30,039
to this question if anyone has thoughts

196
00:08:29,039 --> 00:08:31,680
as the

197
00:08:30,039 --> 00:08:34,320
AI is making decisions about who to

198
00:08:31,680 --> 00:08:37,800
interview um what are ways in which you

199
00:08:34,320 --> 00:08:41,440
might imagine this this decision being

200
00:08:37,800 --> 00:08:41,440
unfair given these

201
00:08:46,360 --> 00:08:50,360
contexts um yeah sure actually that so

202
00:08:48,680 --> 00:08:53,279
this would this would be if you guys can

203
00:08:50,360 --> 00:08:56,399
um form into groups um you know how many

204
00:08:53,279 --> 00:08:57,760
MIT uh P people does it take to be able

205
00:08:56,399 --> 00:08:59,440
to figure out how to form into groups I

206
00:08:57,760 --> 00:09:01,040
don't know but if you guys can hand

207
00:08:59,440 --> 00:09:02,440
handle that problem then I'll give you

208
00:09:01,040 --> 00:09:05,440
guys five minutes to have a discussion

209
00:09:02,440 --> 00:09:05,440
here

210
00:09:48,240 --> 00:09:51,240
well

211
00:10:01,240 --> 00:10:04,240
still

212
00:10:33,959 --> 00:10:36,959
us

213
00:11:17,040 --> 00:11:20,040
some

214
00:11:37,639 --> 00:11:42,079
all right we have one one minute left

215
00:12:24,519 --> 00:12:28,519
Pro br

216
00:12:40,519 --> 00:12:46,519
okay we are we're at time it sounds like

217
00:12:44,240 --> 00:12:49,440
there are some uh pretty uh intense

218
00:12:46,519 --> 00:12:49,440
conversations going

219
00:12:49,680 --> 00:12:54,680
on so I'll give like 10 seconds

220
00:13:05,279 --> 00:13:11,760
no no this is great um the actually

221
00:13:08,680 --> 00:13:13,440
given your enthusiasm um do you guys

222
00:13:11,760 --> 00:13:14,959
want to share a little bit about what

223
00:13:13,440 --> 00:13:17,519
you guys talked about I'm just I'm

224
00:13:14,959 --> 00:13:21,160
curious the various elements of somebody

225
00:13:17,519 --> 00:13:21,160
has a sort of synthesis

226
00:13:33,600 --> 00:13:39,639
then um if the AI says well generally

227
00:13:38,160 --> 00:13:41,279
the orange people have more of the

228
00:13:39,639 --> 00:13:42,760
talent of the white people it's going to

229
00:13:41,279 --> 00:13:44,839
be insensitive to the particular

230
00:13:42,760 --> 00:13:47,040
distribution of talent within the orange

231
00:13:44,839 --> 00:13:49,440
people say than the white people because

232
00:13:47,040 --> 00:13:51,079
maybe you're a white person who's uh got

233
00:13:49,440 --> 00:13:55,040
lots of this kind of talent being looked

234
00:13:51,079 --> 00:13:56,600
for um and so um and then there was a a

235
00:13:55,040 --> 00:13:59,160
sort of push back to that as well what

236
00:13:56,600 --> 00:14:01,000
else can the AI do I mean because all

237
00:13:59,160 --> 00:14:03,800
the characteristics looking at are going

238
00:14:01,000 --> 00:14:07,639
to be characteristics so generally

239
00:14:03,800 --> 00:14:09,720
people um with this characteristic have

240
00:14:07,639 --> 00:14:13,240
talent it's not going to be able to look

241
00:14:09,720 --> 00:14:15,279
at the individual in the um full

242
00:14:13,240 --> 00:14:16,040
specificity so there there was a kind of

243
00:14:15,279 --> 00:14:19,120
push

244
00:14:16,040 --> 00:14:21,399
back that's um that's awesome so you

245
00:14:19,120 --> 00:14:24,519
know in that back and forth there

246
00:14:21,399 --> 00:14:25,880
actually are a couple of ideas um that

247
00:14:24,519 --> 00:14:29,079
are that are common in the fairness

248
00:14:25,880 --> 00:14:31,560
literature so one of them was being sens

249
00:14:29,079 --> 00:14:33,519
itive to the possibility that the AI

250
00:14:31,560 --> 00:14:35,399
could make different decisions across

251
00:14:33,519 --> 00:14:37,920
the orange and white groups that would

252
00:14:35,399 --> 00:14:39,600
be akin to what's called group fairness

253
00:14:37,920 --> 00:14:42,160
and there are different ways of thinking

254
00:14:39,600 --> 00:14:44,839
about group fairness but you know the

255
00:14:42,160 --> 00:14:48,079
tension associated with like say

256
00:14:44,839 --> 00:14:49,959
qualifications or ability across groups

257
00:14:48,079 --> 00:14:52,800
um the tensions the tension there gets

258
00:14:49,959 --> 00:14:54,160
towards uh individual fairness um or

259
00:14:52,800 --> 00:14:56,519
what's what's often times called

260
00:14:54,160 --> 00:14:59,079
individual fairness or is represented by

261
00:14:56,519 --> 00:15:01,320
the similar treatment principle people

262
00:14:59,079 --> 00:15:03,680
who are similar um should get the same

263
00:15:01,320 --> 00:15:05,320
sort of outcomes and so if you know two

264
00:15:03,680 --> 00:15:07,720
people are equally qualified you'd

265
00:15:05,320 --> 00:15:09,480
expect them to get sort of treated um

266
00:15:07,720 --> 00:15:11,040
similarly so so it sounds like you know

267
00:15:09,480 --> 00:15:13,800
some of those ideas were floating around

268
00:15:11,040 --> 00:15:16,880
there um does does any other group uh

269
00:15:13,800 --> 00:15:20,240
want to share maybe additional ideas

270
00:15:16,880 --> 00:15:20,240
that were raised

271
00:15:21,160 --> 00:15:25,079
um yeah in the back

272
00:15:27,040 --> 00:15:33,639
yeah separation orange and white people

273
00:15:30,920 --> 00:15:37,199
and people have um even with two people

274
00:15:33,639 --> 00:15:39,759
have different experiences or the same

275
00:15:37,199 --> 00:15:42,680
experiences people may have have

276
00:15:39,759 --> 00:15:44,120
different types of lived experiences

277
00:15:42,680 --> 00:15:47,440
that where they're bringing an extra

278
00:15:44,120 --> 00:15:49,600
store or their different different um

279
00:15:47,440 --> 00:15:52,440
perspective or they see a particular

280
00:15:49,600 --> 00:15:55,079
bias that's a glaring

281
00:15:52,440 --> 00:15:56,920
flaw that that people are not looking at

282
00:15:55,079 --> 00:15:59,079
and that could be I was telling him an

283
00:15:56,920 --> 00:16:01,319
example like in the the automotive

284
00:15:59,079 --> 00:16:03,839
industry where so many women and

285
00:16:01,319 --> 00:16:05,360
children were been killed by the airbags

286
00:16:03,839 --> 00:16:07,120
and they wondering why are they kill why

287
00:16:05,360 --> 00:16:09,519
are they dying and they b a female

288
00:16:07,120 --> 00:16:12,600
engineer in the first six because you

289
00:16:09,519 --> 00:16:14,480
know most women and children are not as

290
00:16:12,600 --> 00:16:17,120
tall as men and you're suffocating

291
00:16:14,480 --> 00:16:18,720
breaking the rases just suffocating and

292
00:16:17,120 --> 00:16:21,000
why don't you I it sounds something

293
00:16:18,720 --> 00:16:23,759
simple why don't you just adjust it to

294
00:16:21,000 --> 00:16:26,079
the fact of who who actually is in the

295
00:16:23,759 --> 00:16:28,399
car like oh you know we didn't think of

296
00:16:26,079 --> 00:16:31,079
that and it's not it's just an example

297
00:16:28,399 --> 00:16:33,040
of having from different viewpoints and

298
00:16:31,079 --> 00:16:36,560
different live experience that that can

299
00:16:33,040 --> 00:16:39,880
contribute to you know a a

300
00:16:36,560 --> 00:16:42,480
group that that's uh great as so that

301
00:16:39,880 --> 00:16:44,959
actually adds some additional Concepts

302
00:16:42,480 --> 00:16:47,800
that um I'll talk about a little bit um

303
00:16:44,959 --> 00:16:51,399
but that are also uh very uh important

304
00:16:47,800 --> 00:16:53,000
so um there could be a narrow sense in

305
00:16:51,399 --> 00:16:56,240
which you're thinking about hiring

306
00:16:53,000 --> 00:16:58,600
people based on like measures of ability

307
00:16:56,240 --> 00:17:01,120
but that might not take into account the

308
00:16:58,600 --> 00:17:02,959
fact that there are um other dimensions

309
00:17:01,120 --> 00:17:05,160
of ability that actually contribute to

310
00:17:02,959 --> 00:17:07,199
being able to do something better and so

311
00:17:05,160 --> 00:17:11,120
that would be um think generally in the

312
00:17:07,199 --> 00:17:13,720
area of uh of outcome fairness um and so

313
00:17:11,120 --> 00:17:18,439
you know one example that I I think of a

314
00:17:13,720 --> 00:17:21,160
lot in the um in the late 40s um the uh

315
00:17:18,439 --> 00:17:24,160
Coca-Cola uh actually uh pioneered in

316
00:17:21,160 --> 00:17:27,000
marketing they they hired a um uh a

317
00:17:24,160 --> 00:17:29,520
black marketing um manager into their

318
00:17:27,000 --> 00:17:32,200
company and um a at a when a lot of

319
00:17:29,520 --> 00:17:33,640
organizations weren't doing that um and

320
00:17:32,200 --> 00:17:36,200
the reason that they did it was because

321
00:17:33,640 --> 00:17:38,080
they were like hey I mean um uh we're

322
00:17:36,200 --> 00:17:40,039
trying to sell products now COC is not

323
00:17:38,080 --> 00:17:41,520
necessarily great for you so that's a

324
00:17:40,039 --> 00:17:43,000
caveat to this but we're trying to we're

325
00:17:41,520 --> 00:17:45,760
trying to sell products and we we don't

326
00:17:43,000 --> 00:17:48,480
really know that much about black

327
00:17:45,760 --> 00:17:50,400
communities and so maybe this person's

328
00:17:48,480 --> 00:17:52,880
perspective would add something to our

329
00:17:50,400 --> 00:17:54,760
team that would make it easier for us to

330
00:17:52,880 --> 00:17:56,600
you know Market to these groups and so

331
00:17:54,760 --> 00:17:58,880
that was a case where actually uh you

332
00:17:56,600 --> 00:18:01,400
know by I mean I don't know if I've seen

333
00:17:58,880 --> 00:18:03,880
quanitative uh exact analysis of this

334
00:18:01,400 --> 00:18:05,400
but at least by their reports um it was

335
00:18:03,880 --> 00:18:07,120
very helpful that they made this

336
00:18:05,400 --> 00:18:08,679
decision and they were able to actually

337
00:18:07,120 --> 00:18:11,120
sell their product more widely now yeah

338
00:18:08,679 --> 00:18:14,240
I like safety as a as a better example

339
00:18:11,120 --> 00:18:16,720
um just because you know Cocola but um

340
00:18:14,240 --> 00:18:19,200
is is are there any other ideas that uh

341
00:18:16,720 --> 00:18:20,600
you know seem additive to this

342
00:18:19,200 --> 00:18:23,320
discussion things that maybe haven't

343
00:18:20,600 --> 00:18:26,120
really been discussed so

344
00:18:23,320 --> 00:18:28,120
far really one identity characteristic

345
00:18:26,120 --> 00:18:31,120
but there's a lot of other hidden

346
00:18:28,120 --> 00:18:33,559
metadata like be observed by the AI that

347
00:18:31,120 --> 00:18:36,799
can also be used to make things less

348
00:18:33,559 --> 00:18:39,400
fair or more bias right just from uh you

349
00:18:36,799 --> 00:18:41,200
know things associated with where where

350
00:18:39,400 --> 00:18:43,280
you went to school where you work where

351
00:18:41,200 --> 00:18:44,880
you live uh there's a lot of stuff

352
00:18:43,280 --> 00:18:47,000
that's even though it's not explicitly

353
00:18:44,880 --> 00:18:49,320
called out it's going to be embedded in

354
00:18:47,000 --> 00:18:52,520
there and AI will definitely find it and

355
00:18:49,320 --> 00:18:54,200
use it uh in its decision making that

356
00:18:52,520 --> 00:18:56,919
that's that's a great point so yeah I've

357
00:18:54,200 --> 00:18:58,919
I've uh made this a simple example U you

358
00:18:56,919 --> 00:19:01,240
know about One identity characteristic

359
00:18:58,919 --> 00:19:03,679
and that's the focus but of course you

360
00:19:01,240 --> 00:19:06,480
can infer all sorts of different

361
00:19:03,679 --> 00:19:10,120
identity characteristics something as uh

362
00:19:06,480 --> 00:19:13,000
simple as a name could have information

363
00:19:10,120 --> 00:19:14,840
about both say your race and your gender

364
00:19:13,000 --> 00:19:17,799
um and so that's a that's also a great

365
00:19:14,840 --> 00:19:19,480
observation okay so this is is is anyone

366
00:19:17,799 --> 00:19:24,240
else want to want to contribute any

367
00:19:19,480 --> 00:19:24,240
ideas I don't want to cut anyone off

368
00:19:26,640 --> 00:19:32,480
yes divided physical attractiveness or

369
00:19:30,000 --> 00:19:37,000
not physically attractive that neither

370
00:19:32,480 --> 00:19:37,000
Abraham Lincoln nor Jack Maul would have

371
00:19:38,240 --> 00:19:44,039
jobs um okay so um I actually think that

372
00:19:42,039 --> 00:19:46,000
that does raise an important

373
00:19:44,039 --> 00:19:49,360
consideration what kind of

374
00:19:46,000 --> 00:19:50,679
characteristic is this um characteristic

375
00:19:49,360 --> 00:19:53,080
uh you know that separates between

376
00:19:50,679 --> 00:19:55,200
orange and white so you might wonder is

377
00:19:53,080 --> 00:19:57,200
it a protected characteristic so that

378
00:19:55,200 --> 00:19:59,120
would be like race and gender that might

379
00:19:57,200 --> 00:20:01,880
have different implications um

380
00:19:59,120 --> 00:20:03,720
particularly legally um or is it not a

381
00:20:01,880 --> 00:20:05,760
protected characteristic so I think

382
00:20:03,720 --> 00:20:07,880
technically um I don't believe that

383
00:20:05,760 --> 00:20:10,080
attractiveness is a legally protected

384
00:20:07,880 --> 00:20:13,799
characteristic though I could be wrong

385
00:20:10,080 --> 00:20:16,240
about this um and uh so and you know but

386
00:20:13,799 --> 00:20:17,400
still we have these intuitions that um

387
00:20:16,240 --> 00:20:20,320
you know maybe that's not the right way

388
00:20:17,400 --> 00:20:22,640
to be selecting people um but that's a

389
00:20:20,320 --> 00:20:26,039
that's a really that's a it's a good

390
00:20:22,640 --> 00:20:28,880
point um okay so I'm going to now go oh

391
00:20:26,039 --> 00:20:31,760
yes

392
00:20:28,880 --> 00:20:35,159
we're talking about uh AI survices uh

393
00:20:31,760 --> 00:20:38,360
here but the thing uh AI need to follow

394
00:20:35,159 --> 00:20:41,880
someone's instruction of doing uh this

395
00:20:38,360 --> 00:20:44,760
hiding processing so what happen is uh

396
00:20:41,880 --> 00:20:47,440
that particular individual or company

397
00:20:44,760 --> 00:20:50,360
actually is the another bias Source

398
00:20:47,440 --> 00:20:52,919
because it will give certain instruction

399
00:20:50,360 --> 00:20:56,640
how AI should behave how AI should

400
00:20:52,919 --> 00:20:59,280
select right mhm and that another source

401
00:20:56,640 --> 00:21:02,679
of like biases to totally so actually

402
00:20:59,280 --> 00:21:05,520
that um we've covered all of the now

403
00:21:02,679 --> 00:21:07,320
we've covered all the four um different

404
00:21:05,520 --> 00:21:09,400
sorts of fairness I wanted to talk about

405
00:21:07,320 --> 00:21:12,799
so previously we talked about individual

406
00:21:09,400 --> 00:21:15,320
fairness um group group fairness um and

407
00:21:12,799 --> 00:21:17,640
then outcome uh fairness but the last

408
00:21:15,320 --> 00:21:21,080
one is actually uh called procedural

409
00:21:17,640 --> 00:21:24,400
fairness and so um part of the procedure

410
00:21:21,080 --> 00:21:26,279
of deploying the a AI requires somebody

411
00:21:24,400 --> 00:21:29,559
giving some input into what the AI is

412
00:21:26,279 --> 00:21:32,880
doing um or what counts uh as you know

413
00:21:29,559 --> 00:21:36,360
you know um uh as as an outcome that

414
00:21:32,880 --> 00:21:38,039
would you know be good um and so there

415
00:21:36,360 --> 00:21:41,200
are other elements there but I I I I

416
00:21:38,039 --> 00:21:42,919
appreciate adding that as well um so you

417
00:21:41,200 --> 00:21:45,000
know right now I just wanted to make

418
00:21:42,919 --> 00:21:47,960
sure to get all of these Concepts on the

419
00:21:45,000 --> 00:21:49,159
table and and uh see whether or not uh

420
00:21:47,960 --> 00:21:51,720
the four buckets that I'm going to

421
00:21:49,159 --> 00:21:55,520
discuss seems to encapsulate um most of

422
00:21:51,720 --> 00:21:58,120
what we've said so first concept as I've

423
00:21:55,520 --> 00:22:00,360
mentioned individual fairness um and a

424
00:21:58,120 --> 00:22:02,279
lot of what people leverage here is

425
00:22:00,360 --> 00:22:05,360
something called the similar treatment

426
00:22:02,279 --> 00:22:06,760
principle um at the individual uh level

427
00:22:05,360 --> 00:22:08,600
you know if you have two individuals

428
00:22:06,760 --> 00:22:10,760
that are very similar in a number of

429
00:22:08,600 --> 00:22:12,960
characteristics um this could this could

430
00:22:10,760 --> 00:22:15,440
mean like sort of merit-based measures

431
00:22:12,960 --> 00:22:16,799
or measures of performance then uh you

432
00:22:15,440 --> 00:22:18,200
know one way of thinking about fairness

433
00:22:16,799 --> 00:22:21,880
is that those people that are similar

434
00:22:18,200 --> 00:22:25,200
should be treated similarly

435
00:22:21,880 --> 00:22:28,760
um but uh another aspect would be group

436
00:22:25,200 --> 00:22:30,960
fairness and so here there are a variety

437
00:22:28,760 --> 00:22:33,840
of Notions that actually often times

438
00:22:30,960 --> 00:22:35,520
conflict so the first one um that's

439
00:22:33,840 --> 00:22:37,120
often times used in the algorithmic

440
00:22:35,520 --> 00:22:39,720
space is what's called statistical

441
00:22:37,120 --> 00:22:42,640
parity and so what statistical parity

442
00:22:39,720 --> 00:22:44,880
says is that the um judgments made by

443
00:22:42,640 --> 00:22:47,799
the algorithm should be the same on

444
00:22:44,880 --> 00:22:49,640
average uh across the groups so if we

445
00:22:47,799 --> 00:22:52,159
had white and orange and it's uh

446
00:22:49,640 --> 00:22:54,880
interview decision um they should both

447
00:22:52,159 --> 00:22:56,799
get they should both be equally likely

448
00:22:54,880 --> 00:22:59,159
uh to be given an interview um

449
00:22:56,799 --> 00:23:01,520
regardless of their group now now you

450
00:22:59,159 --> 00:23:03,640
know in the first group that talked I

451
00:23:01,520 --> 00:23:05,360
think raised some things that are

452
00:23:03,640 --> 00:23:07,679
concerning about this what if there's

453
00:23:05,360 --> 00:23:10,480
differences in uh capability across the

454
00:23:07,679 --> 00:23:13,480
groups or qualifications this notion

455
00:23:10,480 --> 00:23:16,120
ignores ignores these kinds of concerns

456
00:23:13,480 --> 00:23:19,600
um a more refined notion is something

457
00:23:16,120 --> 00:23:24,400
called equalized odds and so here the

458
00:23:19,600 --> 00:23:27,720
idea is that um if you are qualified for

459
00:23:24,400 --> 00:23:31,760
the position or if you um uh are you

460
00:23:27,720 --> 00:23:34,799
know if you will be deemed as worth

461
00:23:31,760 --> 00:23:38,120
hiring in a technical interview um you

462
00:23:34,799 --> 00:23:40,320
should have the same likelihood of being

463
00:23:38,120 --> 00:23:42,720
given an interview regardless of which

464
00:23:40,320 --> 00:23:44,400
group you're in and so this is slightly

465
00:23:42,720 --> 00:23:47,760
different and it actually takes into

466
00:23:44,400 --> 00:23:49,559
account um you know your underlying uh

467
00:23:47,760 --> 00:23:51,080
qualifications for the position in this

468
00:23:49,559 --> 00:23:54,039
case

469
00:23:51,080 --> 00:23:56,240
um the this it's a related notion is

470
00:23:54,039 --> 00:23:58,679
this idea of counterfactual fairness

471
00:23:56,240 --> 00:24:00,440
which is the prediction that the

472
00:23:58,679 --> 00:24:02,559
algorithm makes or the Judgment it gives

473
00:24:00,440 --> 00:24:04,799
whether or not you get an interview

474
00:24:02,559 --> 00:24:07,039
should not be affected if you were to

475
00:24:04,799 --> 00:24:09,880
just shift the person's identity

476
00:24:07,039 --> 00:24:11,880
characteristic and so that's um also

477
00:24:09,880 --> 00:24:14,679
closely related to this equalized odds

478
00:24:11,880 --> 00:24:16,760
notion and then you know one other thing

479
00:24:14,679 --> 00:24:18,400
that I'll mention is this idea of

480
00:24:16,760 --> 00:24:21,120
intersectional fairness so we were

481
00:24:18,400 --> 00:24:22,720
talking about one characteristic but

482
00:24:21,120 --> 00:24:25,039
there could be many demographic

483
00:24:22,720 --> 00:24:28,039
characteristics and you might care about

484
00:24:25,039 --> 00:24:30,720
not just um you know having fairness

485
00:24:28,039 --> 00:24:32,559
across the two characteristics but

486
00:24:30,720 --> 00:24:35,799
fairness with those two characteristics

487
00:24:32,559 --> 00:24:38,320
interact and so um so it's basically

488
00:24:35,799 --> 00:24:39,520
applying the same Notions but let let's

489
00:24:38,320 --> 00:24:41,679
say you're thinking about race and

490
00:24:39,520 --> 00:24:44,559
gender you're doing it for all the race

491
00:24:41,679 --> 00:24:48,799
and gender categories

492
00:24:44,559 --> 00:24:51,480
um so the third um notion of fairness

493
00:24:48,799 --> 00:24:52,559
outcome fairness um a couple there a lot

494
00:24:51,480 --> 00:24:55,520
of different things that you could put

495
00:24:52,559 --> 00:24:57,640
in this category but often times uh

496
00:24:55,520 --> 00:25:00,360
things like diversity and representation

497
00:24:57,640 --> 00:25:03,440
are included into this so here um you

498
00:25:00,360 --> 00:25:05,799
have some goal of uh representing some

499
00:25:03,440 --> 00:25:07,640
group and that's a kind of outcome uh in

500
00:25:05,799 --> 00:25:09,200
in the organization for example you want

501
00:25:07,640 --> 00:25:11,240
to make sure you're like nationally

502
00:25:09,200 --> 00:25:12,880
representative or something like this um

503
00:25:11,240 --> 00:25:14,919
or you would like to achieve some sort

504
00:25:12,880 --> 00:25:16,399
of diversity there are lots of reasons

505
00:25:14,919 --> 00:25:18,399
that relate to further outcomes why you

506
00:25:16,399 --> 00:25:20,080
might want this like the example of

507
00:25:18,399 --> 00:25:23,159
somebody bringing in lived experience

508
00:25:20,080 --> 00:25:24,559
making you more productive um but then

509
00:25:23,159 --> 00:25:26,799
you know there are other kinds of

510
00:25:24,559 --> 00:25:29,159
Notions that are related to this so um

511
00:25:26,799 --> 00:25:31,120
we want to make decisions that um are

512
00:25:29,159 --> 00:25:32,960
beneficial to the people who we consider

513
00:25:31,120 --> 00:25:36,200
the worst off or the most disadvantaged

514
00:25:32,960 --> 00:25:38,520
so aian notion would also fall into this

515
00:25:36,200 --> 00:25:41,559
uh category um and then finally

516
00:25:38,520 --> 00:25:44,120
procedural fairness uh so here I'm

517
00:25:41,559 --> 00:25:47,440
thinking about things like the actual

518
00:25:44,120 --> 00:25:49,919
algorithm training process um or the

519
00:25:47,440 --> 00:25:51,919
data being used are are we using data

520
00:25:49,919 --> 00:25:54,760
that already has certain kinds of biases

521
00:25:51,919 --> 00:25:57,120
in it uh something like what was raised

522
00:25:54,760 --> 00:25:59,200
just a second ago the person who's

523
00:25:57,120 --> 00:26:01,159
making the decisions about how the

524
00:25:59,200 --> 00:26:03,399
algorithms deployed would also count as

525
00:26:01,159 --> 00:26:06,399
part of uh the process and we might want

526
00:26:03,399 --> 00:26:09,679
to make sure that that's um fair in some

527
00:26:06,399 --> 00:26:12,440
way are there any okay so are there any

528
00:26:09,679 --> 00:26:15,159
other things that um you know are

529
00:26:12,440 --> 00:26:16,880
outside of these four buckets that you

530
00:26:15,159 --> 00:26:20,159
know you guys maybe want to add because

531
00:26:16,880 --> 00:26:20,159
I don't necessarily think this is

532
00:26:21,480 --> 00:26:26,480
comprehensive posed it okay but I would

533
00:26:24,480 --> 00:26:29,200
think that the the PO problem is we

534
00:26:26,480 --> 00:26:32,000
posed a little bit too narrowly okay in

535
00:26:29,200 --> 00:26:35,080
the sense that you're assuming that uh

536
00:26:32,000 --> 00:26:37,120
the applican pool is the same is fixed

537
00:26:35,080 --> 00:26:39,159
right or you're assuming that the number

538
00:26:37,120 --> 00:26:41,960
of people who get chosen from the a

539
00:26:39,159 --> 00:26:45,120
African pool is fixed right you know for

540
00:26:41,960 --> 00:26:47,360
a given job or something and and in fact

541
00:26:45,120 --> 00:26:49,080
though you know these numbers are you

542
00:26:47,360 --> 00:26:51,640
know they're not fixed right because

543
00:26:49,080 --> 00:26:53,640
some disadvantaged people don't apply

544
00:26:51,640 --> 00:26:56,279
don't they won't even try to apply to

545
00:26:53,640 --> 00:26:58,440
something that you know so now they have

546
00:26:56,279 --> 00:27:00,080
they now there no disadvantage applic

547
00:26:58,440 --> 00:27:02,200
because none of them you discourage them

548
00:27:00,080 --> 00:27:04,000
all and they won't apply so I mean there

549
00:27:02,200 --> 00:27:08,159
are these feedback groups I think that

550
00:27:04,000 --> 00:27:11,559
go beyond the confines of the particular

551
00:27:08,159 --> 00:27:14,720
Pro problem that you posed in that uh in

552
00:27:11,559 --> 00:27:17,240
that way yeah just one example suppose

553
00:27:14,720 --> 00:27:19,720
you the goal of this whole process

554
00:27:17,240 --> 00:27:22,559
should also be that uh if if there's

555
00:27:19,720 --> 00:27:24,919
historically disadvantaged groups right

556
00:27:22,559 --> 00:27:27,480
you want the outcome for that

557
00:27:24,919 --> 00:27:30,760
historically disadvantaged group to be

558
00:27:27,480 --> 00:27:32,320
better you know in some sense overall

559
00:27:30,760 --> 00:27:34,279
okay so I mean that's a kind of group

560
00:27:32,320 --> 00:27:36,559
fairness the way you propos the drout

561
00:27:34,279 --> 00:27:39,720
group fairness okay so let's say you

562
00:27:36,559 --> 00:27:42,519
have some job which you're trying to um

563
00:27:39,720 --> 00:27:44,399
uh hire one out of a million people okay

564
00:27:42,519 --> 00:27:47,240
so what I'm saying is that the cut how

565
00:27:44,399 --> 00:27:49,840
sharp the cut is is going to determine

566
00:27:47,240 --> 00:27:51,240
some kind of fairness right because uh

567
00:27:49,840 --> 00:27:53,279
if you're trying to hire one out of a

568
00:27:51,240 --> 00:27:54,720
million people or let's say two out of a

569
00:27:53,279 --> 00:27:56,919
million people you're going to choose

570
00:27:54,720 --> 00:27:59,279
two candidates out of a million people

571
00:27:56,919 --> 00:28:02,000
okay and now you say

572
00:27:59,279 --> 00:28:04,399
I'm choosing one man and one woman okay

573
00:28:02,000 --> 00:28:05,159
so now it's perfectly fair with respect

574
00:28:04,399 --> 00:28:07,960
to

575
00:28:05,159 --> 00:28:11,200
gender right because you chose one man

576
00:28:07,960 --> 00:28:13,919
and one woman okay but the problem is if

577
00:28:11,200 --> 00:28:16,559
you're trying to overall uh improve the

578
00:28:13,919 --> 00:28:18,000
status of women kind of doesn't help I

579
00:28:16,559 --> 00:28:21,000
mean it's great for that one woman who

580
00:28:18,000 --> 00:28:25,080
got chosen but um you know what about

581
00:28:21,000 --> 00:28:28,360
the other 9999 you know uh people so you

582
00:28:25,080 --> 00:28:30,760
know it doesn't have a a a a a

583
00:28:28,360 --> 00:28:32,440
significant outcome for for the grp

584
00:28:30,760 --> 00:28:34,279
fairness so that's what I would say

585
00:28:32,440 --> 00:28:36,399
that's uh I appreciate uh that

586
00:28:34,279 --> 00:28:37,799
conversation actually is a natural

587
00:28:36,399 --> 00:28:40,919
transition to the next point that I want

588
00:28:37,799 --> 00:28:44,000
to make which is um about um whether or

589
00:28:40,919 --> 00:28:46,360
not these kinds of fairness um do they

590
00:28:44,000 --> 00:28:48,600
conflict with each other uh in in

591
00:28:46,360 --> 00:28:51,279
certain contexts and so I think you

592
00:28:48,600 --> 00:28:52,919
actually just posed a great example of

593
00:28:51,279 --> 00:28:56,200
um a case where you might have

594
00:28:52,919 --> 00:28:58,960
statistical parity amongst men and women

595
00:28:56,200 --> 00:29:00,840
um and you know you you hire 50/50 or

596
00:28:58,960 --> 00:29:03,880
you've achieved some sort of diversity

597
00:29:00,840 --> 00:29:05,919
there um but uh you know if it's the

598
00:29:03,880 --> 00:29:08,080
case that you're try you care about

599
00:29:05,919 --> 00:29:10,600
outcome fairness uh in in the sense that

600
00:29:08,080 --> 00:29:13,159
you're trying to improve the status of

601
00:29:10,600 --> 00:29:15,200
women through this decision um it might

602
00:29:13,159 --> 00:29:17,120
not have very much of an effect there

603
00:29:15,200 --> 00:29:19,760
are actually cases where it might have a

604
00:29:17,120 --> 00:29:22,760
negative effect so an example would be

605
00:29:19,760 --> 00:29:26,039
imagine if the uh the woman that gets

606
00:29:22,760 --> 00:29:28,640
selected um is uh through a want of

607
00:29:26,039 --> 00:29:30,159
fairness you make it you you have this

608
00:29:28,640 --> 00:29:32,880
equality across gender but the woman

609
00:29:30,159 --> 00:29:35,880
that gets selected um is uh less

610
00:29:32,880 --> 00:29:38,320
qualified and um let's say that that

611
00:29:35,880 --> 00:29:40,320
makes the company less likely to make

612
00:29:38,320 --> 00:29:41,640
that kind of decision in the future and

613
00:29:40,320 --> 00:29:43,840
so then it actually could potentially

614
00:29:41,640 --> 00:29:47,480
hurt uh women moving forward and that

615
00:29:43,840 --> 00:29:50,000
would be a case where you end up with um

616
00:29:47,480 --> 00:29:52,960
group fairness at the sacrifice of a

617
00:29:50,000 --> 00:29:55,080
version of outcome fairness um the these

618
00:29:52,960 --> 00:29:56,720
are just these are just hypothetical uh

619
00:29:55,080 --> 00:29:58,960
examples but people you know rais them

620
00:29:56,720 --> 00:30:01,440
in the real world in in General I think

621
00:29:58,960 --> 00:30:04,039
that there are lots of uh tensions

622
00:30:01,440 --> 00:30:06,720
between these um uh these different

623
00:30:04,039 --> 00:30:09,919
Notions and it's difficult to have

624
00:30:06,720 --> 00:30:12,799
processes that hit everyone's notion of

625
00:30:09,919 --> 00:30:15,559
each of them and so in general this uh

626
00:30:12,799 --> 00:30:17,240
part where you have overlapping um you

627
00:30:15,559 --> 00:30:19,679
know you you you have an algorithmic

628
00:30:17,240 --> 00:30:21,320
process that is fair in all of these

629
00:30:19,679 --> 00:30:24,480
Notions I'm not saying there actually

630
00:30:21,320 --> 00:30:26,320
are none but in general uh you're often

631
00:30:24,480 --> 00:30:27,760
times making sacrifices and you're not

632
00:30:26,320 --> 00:30:32,039
going to get all of these Notions at the

633
00:30:27,760 --> 00:30:34,600
same time um okay so keeping this in

634
00:30:32,039 --> 00:30:36,640
mind uh actually like to jump back to

635
00:30:34,600 --> 00:30:39,600
the example um that I've I've been

636
00:30:36,640 --> 00:30:42,200
running with which is that of recruiting

637
00:30:39,600 --> 00:30:45,000
and so um what I'm going to do now is

638
00:30:42,200 --> 00:30:47,320
I'm actually going to show you some uh

639
00:30:45,000 --> 00:30:50,159
information and I'm going to try and get

640
00:30:47,320 --> 00:30:52,399
reactions on whether or not uh you know

641
00:30:50,159 --> 00:30:54,559
it changes your thoughts on whether or

642
00:30:52,399 --> 00:30:57,600
not using an algorithm in this case is

643
00:30:54,559 --> 00:30:59,600
fair um and so going back you know you

644
00:30:57,600 --> 00:31:02,039
have the orange group you have the white

645
00:30:59,600 --> 00:31:04,840
group and imagine that you actually

646
00:31:02,039 --> 00:31:08,000
deploy AI to make recruiting decisions

647
00:31:04,840 --> 00:31:09,880
and the first thing um that you see is

648
00:31:08,000 --> 00:31:11,480
that you actually see the differences

649
00:31:09,880 --> 00:31:14,399
and the likelihood of being interviewed

650
00:31:11,480 --> 00:31:16,159
if you're in one of these groups and so

651
00:31:14,399 --> 00:31:19,799
here the white group is being

652
00:31:16,159 --> 00:31:21,080
interviewed uh you know 55% of the time

653
00:31:19,799 --> 00:31:25,559
uh here the orange group is being

654
00:31:21,080 --> 00:31:27,159
interviewed 50% of the time um you know

655
00:31:25,559 --> 00:31:28,679
what does this does this give you uh you

656
00:31:27,159 --> 00:31:31,840
know any information

657
00:31:28,679 --> 00:31:33,919
uh that's useful for assing the fairness

658
00:31:31,840 --> 00:31:37,279
of using the algorithm in this Cas does

659
00:31:33,919 --> 00:31:37,279
anyone have reactions to

660
00:31:41,720 --> 00:31:46,480
this could be something depending on how

661
00:31:44,440 --> 00:31:49,320
large your sample is if it's four is not

662
00:31:46,480 --> 00:31:51,159
going to be meaningful sure uh but it

663
00:31:49,320 --> 00:31:53,760
may be you may also want to look at all

664
00:31:51,159 --> 00:31:56,000
the other all the other fairness the the

665
00:31:53,760 --> 00:31:58,480
three other ones at least to see where

666
00:31:56,000 --> 00:32:02,559
you know how it's behaving sure s does

667
00:31:58,480 --> 00:32:04,360
trade off so yeah I think um uh based on

668
00:32:02,559 --> 00:32:06,919
the definitions that I've given you know

669
00:32:04,360 --> 00:32:08,639
this violates statistical parody but

670
00:32:06,919 --> 00:32:10,159
whether or not that's like you know the

671
00:32:08,639 --> 00:32:12,240
right thing or the wrong thing how does

672
00:32:10,159 --> 00:32:15,760
it relate to other Notions I think more

673
00:32:12,240 --> 00:32:17,200
more information is probably needed um

674
00:32:15,760 --> 00:32:18,600
so you know the next thing that you

675
00:32:17,200 --> 00:32:21,039
could reveal oh one thing I should

676
00:32:18,600 --> 00:32:23,120
mention is I've done four and four here

677
00:32:21,039 --> 00:32:25,240
but I'm actually not you know the groups

678
00:32:23,120 --> 00:32:27,039
could be much larger than this or not

679
00:32:25,240 --> 00:32:29,360
I'm I'm mainly doing that for graphical

680
00:32:27,039 --> 00:32:31,240
representation but um but I don't want

681
00:32:29,360 --> 00:32:35,159
people to over index on the fact that

682
00:32:31,240 --> 00:32:37,000
it's just eight people um okay so now

683
00:32:35,159 --> 00:32:40,200
imagine that you get U additional

684
00:32:37,000 --> 00:32:43,240
information so um what this is is it's a

685
00:32:40,200 --> 00:32:45,440
a resume score what the company has done

686
00:32:43,240 --> 00:32:47,519
is they've taken all of the

687
00:32:45,440 --> 00:32:49,279
characteristics of historical resumés of

688
00:32:47,519 --> 00:32:51,960
people who go through this recruiting

689
00:32:49,279 --> 00:32:54,760
Pro process um and what they what they

690
00:32:51,960 --> 00:32:56,600
do is they predict um whether or not

691
00:32:54,760 --> 00:32:58,519
you're going to actually pass your

692
00:32:56,600 --> 00:33:02,000
interview based on that uh that

693
00:32:58,519 --> 00:33:04,240
information and then they assign um uh

694
00:33:02,000 --> 00:33:06,399
people what this is is the the average

695
00:33:04,240 --> 00:33:10,360
percentile within the different groups

696
00:33:06,399 --> 00:33:11,840
of that score um and so you know the you

697
00:33:10,360 --> 00:33:15,840
know it looks like you hear the white

698
00:33:11,840 --> 00:33:18,240
group um has a significantly better

699
00:33:15,840 --> 00:33:20,320
resume along this Dimension than the

700
00:33:18,240 --> 00:33:22,519
orange group like does this change the

701
00:33:20,320 --> 00:33:26,480
way that we think about the initial

702
00:33:22,519 --> 00:33:26,480
difference in the decisions

703
00:33:35,279 --> 00:33:40,559
the I think the the sort of natural

704
00:33:38,799 --> 00:33:43,200
thing to me when I if I were to see

705
00:33:40,559 --> 00:33:45,600
something like this would be

706
00:33:43,200 --> 00:33:47,960
um maybe it's the case then that there

707
00:33:45,600 --> 00:33:50,120
are actually qualification differences

708
00:33:47,960 --> 00:33:52,360
across the groups and so the fact that

709
00:33:50,120 --> 00:33:54,279
you're seeing um a higher likelihood of

710
00:33:52,360 --> 00:33:57,840
being interviewed from the white group

711
00:33:54,279 --> 00:34:01,360
is in some sense Justified um at least

712
00:33:57,840 --> 00:34:03,080
that's I think a way of looking at it um

713
00:34:01,360 --> 00:34:06,039
the final thing that you you know you

714
00:34:03,080 --> 00:34:08,280
could reveal here is actually the true

715
00:34:06,039 --> 00:34:10,679
pass rate in the technical interview so

716
00:34:08,280 --> 00:34:12,720
remember the um recruiters are making

717
00:34:10,679 --> 00:34:15,800
decisions about who gets interviewed and

718
00:34:12,720 --> 00:34:18,800
then what if you knew the actual ground

719
00:34:15,800 --> 00:34:22,599
truth um likelihood of someone from each

720
00:34:18,800 --> 00:34:25,520
of these groups passing the interview um

721
00:34:22,599 --> 00:34:27,520
you know uh and here it actually the the

722
00:34:25,520 --> 00:34:29,919
the likelihoods look actually mapped on

723
00:34:27,520 --> 00:34:31,720
to uh are very similar exactly the same

724
00:34:29,919 --> 00:34:34,520
as um the likelihood of being

725
00:34:31,720 --> 00:34:36,679
interviewed does that change your uh you

726
00:34:34,520 --> 00:34:40,760
know your view of whether or not the

727
00:34:36,679 --> 00:34:40,760
decisions made by this AI are

728
00:34:45,240 --> 00:34:49,000
fair um among people who've already got

729
00:34:47,760 --> 00:34:51,159
part the first that's a good that's a

730
00:34:49,000 --> 00:34:55,320
good question so I should I should

731
00:34:51,159 --> 00:34:57,880
explain so um this uh this information

732
00:34:55,320 --> 00:35:00,480
is uh it's it's on exact numbers but

733
00:34:57,880 --> 00:35:03,640
it's based on a study that I'm um

734
00:35:00,480 --> 00:35:05,720
actually currently working on and here

735
00:35:03,640 --> 00:35:10,400
uh actually you know what we have access

736
00:35:05,720 --> 00:35:13,200
to is we have access to um interview uh

737
00:35:10,400 --> 00:35:16,480
the interview pass rate for everybody in

738
00:35:13,200 --> 00:35:21,079
the sample and then we um we go and we

739
00:35:16,480 --> 00:35:23,359
get uh AI to make judgments about um who

740
00:35:21,079 --> 00:35:25,320
would they interview um you know based

741
00:35:23,359 --> 00:35:27,240
on the resumés that they're given and so

742
00:35:25,320 --> 00:35:30,240
here you don't have this selection issue

743
00:35:27,240 --> 00:35:32,280
where you only have outcomes for uh

744
00:35:30,240 --> 00:35:33,760
people uh who were selected by a human

745
00:35:32,280 --> 00:35:36,040
to get an interview but there's no

746
00:35:33,760 --> 00:35:37,839
reason that's not a natural like usually

747
00:35:36,040 --> 00:35:39,599
that's not going to be the case but for

748
00:35:37,839 --> 00:35:41,000
this thought experiment you said that's

749
00:35:39,599 --> 00:35:45,160
that's actually the ground truth

750
00:35:41,000 --> 00:35:46,200
information for um Everybody um do you

751
00:35:45,160 --> 00:35:48,640
do you have a thought does that does

752
00:35:46,200 --> 00:35:51,240
that make a difference

753
00:35:48,640 --> 00:35:53,640
and the AI is performing the same way

754
00:35:51,240 --> 00:35:55,599
the human

755
00:35:53,640 --> 00:35:56,960
interviewers that the AI is making the

756
00:35:55,599 --> 00:35:58,000
same judgments as the human interviewers

757
00:35:56,960 --> 00:36:00,599
which doesn't necessarily mean that it's

758
00:35:58,000 --> 00:36:03,440
all Fab it just means

759
00:36:00,599 --> 00:36:07,200
that if it's if this bias is sheded by

760
00:36:03,440 --> 00:36:09,280
the AI ad sure yes and so I should I

761
00:36:07,200 --> 00:36:11,480
should be careful to so the AI is making

762
00:36:09,280 --> 00:36:13,599
decisions about who gets an interview

763
00:36:11,480 --> 00:36:15,200
and then um the interview pass rate

764
00:36:13,599 --> 00:36:17,319
tells you the actual interview that it's

765
00:36:15,200 --> 00:36:19,240
given whether or not humans pass it so

766
00:36:17,319 --> 00:36:21,920
what you're saying is right that the AI

767
00:36:19,240 --> 00:36:23,280
is making decisions that are similar to

768
00:36:21,920 --> 00:36:25,800
the downstream

769
00:36:23,280 --> 00:36:27,599
interviewers um and you know if you

770
00:36:25,800 --> 00:36:29,680
think about that is what the AI is tring

771
00:36:27,599 --> 00:36:31,200
trying to do like they're trying to pass

772
00:36:29,680 --> 00:36:33,079
people who will be passed in this actual

773
00:36:31,200 --> 00:36:35,640
interview yeah I think that you know

774
00:36:33,079 --> 00:36:38,240
maybe it does uh mean that this is um

775
00:36:35,640 --> 00:36:40,800
the AI is like performing well so to

776
00:36:38,240 --> 00:36:40,800
speak

777
00:36:42,760 --> 00:36:49,520
yes SC uh you are basically using AI to

778
00:36:46,920 --> 00:36:53,079
do this job uh what about if it's a

779
00:36:49,520 --> 00:36:55,839
human uh did the like a scolling is

780
00:36:53,079 --> 00:36:59,480
there any uh difference uh there that's

781
00:36:55,839 --> 00:37:01,560
another angle to look at yeah so it is

782
00:36:59,480 --> 00:37:03,640
so you know one thing that one could

783
00:37:01,560 --> 00:37:05,640
point out is like the seeming

784
00:37:03,640 --> 00:37:07,000
discrepancy between the differences in

785
00:37:05,640 --> 00:37:08,560
the rasme score and the differences in

786
00:37:07,000 --> 00:37:10,000
the interview pass rate which is not

787
00:37:08,560 --> 00:37:12,960
quite I'll get i'll get to your exact

788
00:37:10,000 --> 00:37:15,119
point but um what that would suggest is

789
00:37:12,960 --> 00:37:16,839
that you know you're using some sort of

790
00:37:15,119 --> 00:37:19,280
a tool that's telling you how good this

791
00:37:16,839 --> 00:37:20,839
resume is and you're using uh you know a

792
00:37:19,280 --> 00:37:22,640
population to make some sort of

793
00:37:20,839 --> 00:37:25,160
prediction here but in this particular

794
00:37:22,640 --> 00:37:27,680
sample it actually doesn't seem to you

795
00:37:25,160 --> 00:37:29,960
know uh be that effective you would

796
00:37:27,680 --> 00:37:31,680
expect that the white group would do

797
00:37:29,960 --> 00:37:34,079
even better relative to the orange group

798
00:37:31,680 --> 00:37:35,920
in the actual interviews um which that

799
00:37:34,079 --> 00:37:37,880
happens all the time you use historical

800
00:37:35,920 --> 00:37:40,000
data to make a judgment of the quality

801
00:37:37,880 --> 00:37:41,760
of the resumés but you know in a

802
00:37:40,000 --> 00:37:43,599
particular sample that you're looking at

803
00:37:41,760 --> 00:37:48,000
those predictions don't necessarily um

804
00:37:43,599 --> 00:37:50,640
pin out now um the uh so I'm so whether

805
00:37:48,000 --> 00:37:53,480
or not humans uh are scoring the resume

806
00:37:50,640 --> 00:37:55,520
versus whether or not AI is doing it is

807
00:37:53,480 --> 00:37:57,040
something that I'm actually not going to

808
00:37:55,520 --> 00:38:00,560
comment on I I think I think that could

809
00:37:57,040 --> 00:38:02,040
be an important um aspect of this but I

810
00:38:00,560 --> 00:38:05,560
do think that it's important to keep in

811
00:38:02,040 --> 00:38:07,680
mind um that all this information um

812
00:38:05,560 --> 00:38:10,800
this is information that's about using

813
00:38:07,680 --> 00:38:13,960
AI to make judgments uh on these

814
00:38:10,800 --> 00:38:17,079
applications but you can use AI or you

815
00:38:13,960 --> 00:38:19,440
can use humans and so there's actually

816
00:38:17,079 --> 00:38:21,400
another comparison um that might be

817
00:38:19,440 --> 00:38:25,560
important for understanding whether or

818
00:38:21,400 --> 00:38:28,160
not it's fair to adopt AI um how would

819
00:38:25,560 --> 00:38:30,400
humans make these kinds of decisions

820
00:38:28,160 --> 00:38:33,480
and so what I've done here these are

821
00:38:30,400 --> 00:38:35,400
exactly the same um because I'm assuming

822
00:38:33,480 --> 00:38:37,000
that there's the same resume score and

823
00:38:35,400 --> 00:38:38,839
that the actual ground truth hasn't

824
00:38:37,000 --> 00:38:41,680
changed this is exact the exact same

825
00:38:38,839 --> 00:38:44,839
sample that's being judged um but there

826
00:38:41,680 --> 00:38:48,079
are differences between how humans are

827
00:38:44,839 --> 00:38:49,960
making um you know interview decisions

828
00:38:48,079 --> 00:38:52,599
and how the AI is making interview

829
00:38:49,960 --> 00:38:57,560
decisions so I think perhaps the most

830
00:38:52,599 --> 00:38:59,160
notable difference is that um the uh the

831
00:38:57,560 --> 00:39:00,760
magnitude or the the sign of the

832
00:38:59,160 --> 00:39:02,920
difference is actually flipped between

833
00:39:00,760 --> 00:39:04,640
the white and orange groups humans are

834
00:39:02,920 --> 00:39:08,119
actually saying the orange groups should

835
00:39:04,640 --> 00:39:10,079
be interviewed at higher rates um and

836
00:39:08,119 --> 00:39:12,440
you know based on our previous

837
00:39:10,079 --> 00:39:14,400
discussion you know it's kind of it

838
00:39:12,440 --> 00:39:16,760
might be difficult to justify that based

839
00:39:14,400 --> 00:39:19,400
on the ground truth performance or the

840
00:39:16,760 --> 00:39:21,520
resume scores like um it would it would

841
00:39:19,400 --> 00:39:24,960
appear that the white group is sort of

842
00:39:21,520 --> 00:39:27,640
more qualified on both those Dimensions

843
00:39:24,960 --> 00:39:30,119
um and uh I before I move forward I want

844
00:39:27,640 --> 00:39:32,720
to see does anyone have any any thoughts

845
00:39:30,119 --> 00:39:35,359
um on how adding the human information

846
00:39:32,720 --> 00:39:39,359
maybe uh you know complicates things or

847
00:39:35,359 --> 00:39:40,960
changes the way we think about this yeah

848
00:39:39,359 --> 00:39:43,680
there's also might be in this whole

849
00:39:40,960 --> 00:39:47,000
problem an opportunity cost you know

850
00:39:43,680 --> 00:39:49,040
whereas if you decide you know that

851
00:39:47,000 --> 00:39:51,880
maybe the humans are figuring that they

852
00:39:49,040 --> 00:39:54,880
may get some Opportunity by interviewing

853
00:39:51,880 --> 00:39:57,400
more of the orange group and you know

854
00:39:54,880 --> 00:39:59,920
you can't measure opportunity cost it's

855
00:39:57,400 --> 00:40:02,400
you know so so there's kind of a bias in

856
00:39:59,920 --> 00:40:04,119
the statistical judgment that you're

857
00:40:02,400 --> 00:40:06,119
you're going on the information you have

858
00:40:04,119 --> 00:40:08,720
rather than the information you don't

859
00:40:06,119 --> 00:40:09,960
have sure um I I think that there are a

860
00:40:08,720 --> 00:40:13,680
a few ways that the notion of

861
00:40:09,960 --> 00:40:16,359
opportunity cost plays in here so um one

862
00:40:13,680 --> 00:40:18,119
is thinking about uh how costly is it

863
00:40:16,359 --> 00:40:20,640
actually to have humans review these

864
00:40:18,119 --> 00:40:23,280
resumés relative to the AI it takes time

865
00:40:20,640 --> 00:40:25,200
and humans could be doing something else

866
00:40:23,280 --> 00:40:27,319
and so if you use AI you it could be a

867
00:40:25,200 --> 00:40:30,880
cost-saving thing um I think what you

868
00:40:27,319 --> 00:40:32,839
you were referring to is um you know

869
00:40:30,880 --> 00:40:34,640
maybe there's a maybe there's a hiring

870
00:40:32,839 --> 00:40:36,000
initiative where what what you're trying

871
00:40:34,640 --> 00:40:37,960
to do is you're particularly trying to

872
00:40:36,000 --> 00:40:39,599
promote a particular kind of candidate

873
00:40:37,960 --> 00:40:41,119
and so you're more likely to be able to

874
00:40:39,599 --> 00:40:43,680
make an offer to a candidate from one

875
00:40:41,119 --> 00:40:45,079
group or another and so we don't observe

876
00:40:43,680 --> 00:40:48,000
that and so that's the kind of thing the

877
00:40:45,079 --> 00:40:50,920
human might care about um I think those

878
00:40:48,000 --> 00:40:52,480
are those are great uh observations I

879
00:40:50,920 --> 00:40:57,800
think the the last thing that I want to

880
00:40:52,480 --> 00:40:57,800
reveal about this is um yes

881
00:40:58,160 --> 00:41:04,040
question uh the initial grouping of what

882
00:41:01,079 --> 00:41:06,560
you uh do here like in terms of I don't

883
00:41:04,040 --> 00:41:10,119
know initially how do you cataliz into

884
00:41:06,560 --> 00:41:12,839
this two population of Orange versus

885
00:41:10,119 --> 00:41:15,839
white right but if you differentiate

886
00:41:12,839 --> 00:41:18,079
that into another uh angle like looking

887
00:41:15,839 --> 00:41:20,560
at this population divide into a

888
00:41:18,079 --> 00:41:23,680
different group then the data you

889
00:41:20,560 --> 00:41:26,599
generate uh could be uh different too so

890
00:41:23,680 --> 00:41:30,960
that's another thing I would uh look at

891
00:41:26,599 --> 00:41:33,680
how you uh do the division of grouping

892
00:41:30,960 --> 00:41:37,960
uh yeah yeah so I've left the grouping

893
00:41:33,680 --> 00:41:40,920
vague kind of um on on purpose um and

894
00:41:37,960 --> 00:41:42,760
and you'll see why in a second but um

895
00:41:40,920 --> 00:41:44,560
but yes if if you were to group people

896
00:41:42,760 --> 00:41:47,839
differently these numbers would look

897
00:41:44,560 --> 00:41:50,079
different um like let's say this is um a

898
00:41:47,839 --> 00:41:51,680
gender differentiator like the the the

899
00:41:50,079 --> 00:41:53,440
white versus orange that's going to lead

900
00:41:51,680 --> 00:41:55,920
to certain results but you'll say you

901
00:41:53,440 --> 00:41:58,000
then Group by um you know something like

902
00:41:55,920 --> 00:42:00,359
soci economic background or something

903
00:41:58,000 --> 00:42:02,440
like this um then it's going to give you

904
00:42:00,359 --> 00:42:03,880
uh you know different um uh different

905
00:42:02,440 --> 00:42:06,880
results that's a that's a good

906
00:42:03,880 --> 00:42:09,760
observation um the okay the last thing

907
00:42:06,880 --> 00:42:13,440
that I want to reveal on this is that um

908
00:42:09,760 --> 00:42:15,680
uh so imagine that uh now um the white

909
00:42:13,440 --> 00:42:18,079
group is what what I'm going to call an

910
00:42:15,680 --> 00:42:21,760
over represented group so in in my

911
00:42:18,079 --> 00:42:24,359
context um it's over represented um

912
00:42:21,760 --> 00:42:28,280
racial groups in Tech so that would be

913
00:42:24,359 --> 00:42:31,280
uh both um Asian and white uh applicants

914
00:42:28,280 --> 00:42:33,839
um but then the orange group actually

915
00:42:31,280 --> 00:42:35,359
represents um under an underrepresented

916
00:42:33,839 --> 00:42:37,680
group so this would be underrepresented

917
00:42:35,359 --> 00:42:41,359
races particularly Hispanic and black

918
00:42:37,680 --> 00:42:44,400
applicants um does that change um this

919
00:42:41,359 --> 00:42:46,000
picture at all once you know exactly

920
00:42:44,400 --> 00:42:50,000
what characteristic or what kind of

921
00:42:46,000 --> 00:42:50,000
characteristic you're looking at

922
00:42:59,240 --> 00:43:05,079
yes what kind of job are

923
00:43:02,640 --> 00:43:06,920
you what kind of job are they being is

924
00:43:05,079 --> 00:43:09,760
it this is a software engineering job is

925
00:43:06,920 --> 00:43:09,760
what you should think of

926
00:43:12,880 --> 00:43:17,880
yes it seems to me that actually human

927
00:43:15,760 --> 00:43:20,599
selection shows that they are trying to

928
00:43:17,880 --> 00:43:23,680
create more diversity than the AI

929
00:43:20,599 --> 00:43:25,720
selection try to create more inclusion

930
00:43:23,680 --> 00:43:28,119
than the AI

931
00:43:25,720 --> 00:43:31,280
selection yes if if I if I were if I

932
00:43:28,119 --> 00:43:33,440
were to have um uh you know tried to

933
00:43:31,280 --> 00:43:35,079
tried to plant someone to give like what

934
00:43:33,440 --> 00:43:38,079
I was looking for as an answer that's

935
00:43:35,079 --> 00:43:40,200
basically um the kind of intuition flip

936
00:43:38,079 --> 00:43:41,839
that I think U I also had so I think

937
00:43:40,200 --> 00:43:44,680
going to this project I think that

938
00:43:41,839 --> 00:43:48,240
there's um you know a thought uh what if

939
00:43:44,680 --> 00:43:51,240
AI can debias humans um and I think

940
00:43:48,240 --> 00:43:53,559
generally that's seen as a good thing um

941
00:43:51,240 --> 00:43:57,880
that debiasing is a good thing um in

942
00:43:53,559 --> 00:44:00,400
this context uh it would appear that hum

943
00:43:57,880 --> 00:44:02,640
actually do have a bias um and they have

944
00:44:00,400 --> 00:44:07,040
a bias that's in favor of an

945
00:44:02,640 --> 00:44:09,240
underrepresented group um and um in if

946
00:44:07,040 --> 00:44:11,040
you if you know you take these numbers

947
00:44:09,240 --> 00:44:12,960
to uh be real they're not exactly right

948
00:44:11,040 --> 00:44:14,280
but they're qualitatively correct if you

949
00:44:12,960 --> 00:44:17,960
were to make the decision to switch over

950
00:44:14,280 --> 00:44:21,559
to AI selection um it might actually

951
00:44:17,960 --> 00:44:25,520
hurt the underrepresented minority group

952
00:44:21,559 --> 00:44:28,000
but it eliminates bias actually um if if

953
00:44:25,520 --> 00:44:30,119
um you're thinking of bias as uh you

954
00:44:28,000 --> 00:44:33,680
know uh

955
00:44:30,119 --> 00:44:36,760
um you know different judgments relative

956
00:44:33,680 --> 00:44:38,040
to the ground truth of uh you know your

957
00:44:36,760 --> 00:44:41,319
your qualification on the thing that's

958
00:44:38,040 --> 00:44:42,760
being judged and so um to to me I was

959
00:44:41,319 --> 00:44:45,319
like when I you know when I saw this it

960
00:44:42,760 --> 00:44:47,119
was um actually pretty s it was pretty

961
00:44:45,319 --> 00:44:50,280
surprising it made me sort of reth

962
00:44:47,119 --> 00:44:54,359
rethink some of these um ideas um and I

963
00:44:50,280 --> 00:44:55,559
think uh so you know one question I'm

964
00:44:54,359 --> 00:44:57,160
I'm not going to make you guys go into

965
00:44:55,559 --> 00:44:59,760
groups again because actually this part

966
00:44:57,160 --> 00:45:01,839
has taken longer than I anticipated um

967
00:44:59,760 --> 00:45:04,040
but um one question that I I wanted to

968
00:45:01,839 --> 00:45:07,319
raise to the audience is can

969
00:45:04,040 --> 00:45:10,599
discrimination be fair or is it always

970
00:45:07,319 --> 00:45:12,520
unfair by definition um if you see you

971
00:45:10,599 --> 00:45:14,440
know evidence of a bias Can it can you

972
00:45:12,520 --> 00:45:17,079
know can it be fair what sorts of things

973
00:45:14,440 --> 00:45:18,319
might make it fair and so you know the

974
00:45:17,079 --> 00:45:21,160
second question that would go along with

975
00:45:18,319 --> 00:45:23,200
that if no why if yes you know what are

976
00:45:21,160 --> 00:45:25,119
some examples

977
00:45:23,200 --> 00:45:27,480
um

978
00:45:25,119 --> 00:45:29,520
uh I don't know if anyone happens to

979
00:45:27,480 --> 00:45:32,640
have thoughts uh on either of these

980
00:45:29,520 --> 00:45:32,640
questions right

981
00:45:33,599 --> 00:45:41,319
now sure this is your um I wonder if

982
00:45:38,760 --> 00:45:43,880
understand well I wonder what what your

983
00:45:41,319 --> 00:45:45,720
decisions or thought processes around

984
00:45:43,880 --> 00:45:47,760
the extensibility of the fairness

985
00:45:45,720 --> 00:45:50,319
concept versus viewing fairness as one

986
00:45:47,760 --> 00:45:52,720
element in a larger strategic set of

987
00:45:50,319 --> 00:45:54,359
things right because fairness winds

988
00:45:52,720 --> 00:45:56,760
interacting with other what I would have

989
00:45:54,359 --> 00:45:57,559
thought of as other strategic priorities

990
00:45:56,760 --> 00:46:00,000
right

991
00:45:57,559 --> 00:46:02,240
and so that liberates you from having to

992
00:46:00,000 --> 00:46:03,640
use spess to explain certain kinds of

993
00:46:02,240 --> 00:46:06,640
dynamics

994
00:46:03,640 --> 00:46:08,319
that maybe it might be hard to use those

995
00:46:06,640 --> 00:46:11,160
four categories of fairness to explain

996
00:46:08,319 --> 00:46:14,760
or yeah totally so I this is a talk

997
00:46:11,160 --> 00:46:17,440
that's focused on fairness um no no I I

998
00:46:14,760 --> 00:46:19,000
actually think but there's a totally um

999
00:46:17,440 --> 00:46:20,960
you know different thing so when I when

1000
00:46:19,000 --> 00:46:23,200
I talk about outcome fairness it's a

1001
00:46:20,960 --> 00:46:26,200
closest I get to talking about something

1002
00:46:23,200 --> 00:46:28,599
like just you know trying to do a good

1003
00:46:26,200 --> 00:46:32,680
job in the

1004
00:46:28,599 --> 00:46:35,240
yeah exact exactly and so I think like

1005
00:46:32,680 --> 00:46:36,440
productivity concerns concerns or like

1006
00:46:35,240 --> 00:46:39,000
you know what you're actually trying to

1007
00:46:36,440 --> 00:46:41,599
accomplish is another piece of this I

1008
00:46:39,000 --> 00:46:44,400
actually think um uh you can bring it

1009
00:46:41,599 --> 00:46:46,920
into a conversation about fairness um by

1010
00:46:44,400 --> 00:46:49,640
thinking about things like um overall

1011
00:46:46,920 --> 00:46:52,119
societal welfare so one place where this

1012
00:46:49,640 --> 00:46:56,240
oftentimes gets raised is in the context

1013
00:46:52,119 --> 00:46:58,720
of certain um very uh you know uh

1014
00:46:56,240 --> 00:47:01,400
physically demanding jobs that are like

1015
00:46:58,720 --> 00:47:03,480
uh that matter for safety for example so

1016
00:47:01,400 --> 00:47:06,400
um you know there might be certain parts

1017
00:47:03,480 --> 00:47:08,280
of the military where um it's actually

1018
00:47:06,400 --> 00:47:10,400
the most fair thing if you think about

1019
00:47:08,280 --> 00:47:12,079
protecting a country is to just have the

1020
00:47:10,400 --> 00:47:14,240
people that are physically the strongest

1021
00:47:12,079 --> 00:47:16,200
or something like this so I do think

1022
00:47:14,240 --> 00:47:18,880
that those uh kind of more strategic

1023
00:47:16,200 --> 00:47:21,720
concerns can come into a conversation of

1024
00:47:18,880 --> 00:47:23,280
fairness um I've excluded them primarily

1025
00:47:21,720 --> 00:47:27,119
uh in this discussion but I think that's

1026
00:47:23,280 --> 00:47:29,119
a good point um anyways maybe I'll I'll

1027
00:47:27,119 --> 00:47:31,880
leave these as questions to ponder as

1028
00:47:29,119 --> 00:47:33,800
you guys go about uh your day uh as

1029
00:47:31,880 --> 00:47:38,240
opposed to forcing you guys to give me

1030
00:47:33,800 --> 00:47:40,400
answers um okay um so I'm actually

1031
00:47:38,240 --> 00:47:43,319
pretty close to time so what I'm going

1032
00:47:40,400 --> 00:47:47,359
to do is I I I wanted to introduce an

1033
00:47:43,319 --> 00:47:50,480
area so so that was an example where um

1034
00:47:47,359 --> 00:47:52,160
using AI while it might be helpful in

1035
00:47:50,480 --> 00:47:55,079
some ways it's

1036
00:47:52,160 --> 00:47:58,680
ambiguous uh whether or not it's making

1037
00:47:55,079 --> 00:48:00,800
things more fair um based on the data

1038
00:47:58,680 --> 00:48:03,720
that I showed you guys and I want to

1039
00:48:00,800 --> 00:48:07,359
introduce a context where maybe it's a

1040
00:48:03,720 --> 00:48:10,480
little bit uh less ambiguous uh and a

1041
00:48:07,359 --> 00:48:12,440
case where actually maybe AI can help um

1042
00:48:10,480 --> 00:48:14,599
you know move us towards a fairer world

1043
00:48:12,440 --> 00:48:16,760
and so there um I'm thinking about

1044
00:48:14,599 --> 00:48:19,200
cohort selection diversity uh and

1045
00:48:16,760 --> 00:48:21,280
algorithms um this is really kind of a

1046
00:48:19,200 --> 00:48:23,599
project about admissions uh and I'm

1047
00:48:21,280 --> 00:48:27,960
going to I'm going to go through this uh

1048
00:48:23,599 --> 00:48:30,160
you know fairly quickly but um

1049
00:48:27,960 --> 00:48:31,559
you know and since the Banning of

1050
00:48:30,160 --> 00:48:32,960
affirmative action and college

1051
00:48:31,559 --> 00:48:34,839
admissions a lot of or a lot of

1052
00:48:32,960 --> 00:48:36,160
organizations schools in particular are

1053
00:48:34,839 --> 00:48:39,520
interested in figuring out how to

1054
00:48:36,160 --> 00:48:41,319
diversify their college classes without

1055
00:48:39,520 --> 00:48:44,760
um using

1056
00:48:41,319 --> 00:48:48,400
race um in the current climate there's

1057
00:48:44,760 --> 00:48:51,079
uh a lot of criticism of demographic uh

1058
00:48:48,400 --> 00:48:55,119
divers diversification uh and so

1059
00:48:51,079 --> 00:48:57,040
criticisms of um what's called Dei uh

1060
00:48:55,119 --> 00:48:58,799
but you know it's still something that I

1061
00:48:57,040 --> 00:49:00,880
think is quite popular around different

1062
00:48:58,799 --> 00:49:03,480
organizations so this is a concern that

1063
00:49:00,880 --> 00:49:06,520
maybe is worth trying to figure out if

1064
00:49:03,480 --> 00:49:08,760
algorithms can help uh the last thing

1065
00:49:06,520 --> 00:49:12,119
I'll say on this is that the people who

1066
00:49:08,760 --> 00:49:14,160
criticize uh demographic diversity of

1067
00:49:12,119 --> 00:49:16,000
oftentimes also have their own Notions

1068
00:49:14,160 --> 00:49:18,319
of diversity that they're interested in

1069
00:49:16,000 --> 00:49:20,520
so this is an example of an organization

1070
00:49:18,319 --> 00:49:22,720
that has a Libertarian leaning heterodox

1071
00:49:20,520 --> 00:49:24,520
Academy that uh is interested in

1072
00:49:22,720 --> 00:49:27,440
Viewpoint diversity or ideological

1073
00:49:24,520 --> 00:49:30,240
diversity on campuses which is a kind of

1074
00:49:27,440 --> 00:49:32,400
uh diversity and so um it seems like a

1075
00:49:30,240 --> 00:49:33,720
lot of different sorts of organizations

1076
00:49:32,400 --> 00:49:35,680
whether it's demographic diversity or

1077
00:49:33,720 --> 00:49:38,440
other forms of diversity are interested

1078
00:49:35,680 --> 00:49:42,079
in figuring out how to um select diverse

1079
00:49:38,440 --> 00:49:43,599
uh groups of people um and my thinking

1080
00:49:42,079 --> 00:49:46,359
about this started when I was working

1081
00:49:43,599 --> 00:49:48,920
with a a prestigious scholarship um

1082
00:49:46,359 --> 00:49:51,240
called uh actually I can't tell you guys

1083
00:49:48,920 --> 00:49:53,119
the name uh I have an NDA I can't I

1084
00:49:51,240 --> 00:49:56,160
can't share that information but I'm

1085
00:49:53,119 --> 00:49:58,359
going to refer to them as uh the program

1086
00:49:56,160 --> 00:50:01,520
for the sake of this and so they were a

1087
00:49:58,359 --> 00:50:05,079
scholarship that um was interested in

1088
00:50:01,520 --> 00:50:06,520
finding 15 to 18yearold uh Geniuses like

1089
00:50:05,079 --> 00:50:09,040
people who are right tailed Talent

1090
00:50:06,520 --> 00:50:11,079
across different C countries to support

1091
00:50:09,040 --> 00:50:13,119
uh them going to college uh they also

1092
00:50:11,079 --> 00:50:14,319
they give lifetime funding uh and

1093
00:50:13,119 --> 00:50:15,960
support if they want to start

1094
00:50:14,319 --> 00:50:18,319
organizations they have seed funds that

1095
00:50:15,960 --> 00:50:20,920
kind of thing and it's a the selection

1096
00:50:18,319 --> 00:50:23,000
is based on a competitive process um

1097
00:50:20,920 --> 00:50:26,040
that's based on applicant projects they

1098
00:50:23,000 --> 00:50:28,040
tend to be Tech oriented projects um and

1099
00:50:26,040 --> 00:50:29,200
I started working with this organization

1100
00:50:28,040 --> 00:50:31,079
uh because they were interested in

1101
00:50:29,200 --> 00:50:33,440
optimally trading off between diversity

1102
00:50:31,079 --> 00:50:35,200
and performance how do they how do they

1103
00:50:33,440 --> 00:50:38,119
select people that in a way that you

1104
00:50:35,200 --> 00:50:41,359
know accounts for both these elements um

1105
00:50:38,119 --> 00:50:44,440
and they also had Fairly clear Notions

1106
00:50:41,359 --> 00:50:46,319
of uh performance and diversity so for

1107
00:50:44,440 --> 00:50:49,000
performance they have these scores that

1108
00:50:46,319 --> 00:50:51,200
they give all the projects and uh that's

1109
00:50:49,000 --> 00:50:53,160
what they were thinking of as per as a

1110
00:50:51,200 --> 00:50:54,400
cohort's performance just sum all the

1111
00:50:53,160 --> 00:50:57,319
project scores of the people in the

1112
00:50:54,400 --> 00:50:59,480
cohort they select um forers University

1113
00:50:57,319 --> 00:51:02,720
they were interested in selecting a

1114
00:50:59,480 --> 00:51:06,200
cohort that was close to you know 50%

1115
00:51:02,720 --> 00:51:07,319
female 1 poor 1th first generation and

1116
00:51:06,200 --> 00:51:08,880
they they have a lot of different

1117
00:51:07,319 --> 00:51:10,880
countries there over 100 countries that

1118
00:51:08,880 --> 00:51:12,799
apply to the program and so they wanted

1119
00:51:10,880 --> 00:51:15,240
uh one person from each uh applicant

1120
00:51:12,799 --> 00:51:18,200
country as well uh and so this was this

1121
00:51:15,240 --> 00:51:19,760
was their sort of notion of diversity um

1122
00:51:18,200 --> 00:51:21,520
and so they were they were trying to

1123
00:51:19,760 --> 00:51:23,920
figure out

1124
00:51:21,520 --> 00:51:26,799
um if we were to move in the direction

1125
00:51:23,920 --> 00:51:28,799
of meeting our diversity goals um what's

1126
00:51:26,799 --> 00:51:30,960
the optimal way to do that how do we do

1127
00:51:28,799 --> 00:51:35,760
this in a way that minimizes um you know

1128
00:51:30,960 --> 00:51:38,280
losses in terms of uh performance uh and

1129
00:51:35,760 --> 00:51:39,760
um you know I ultimately uh you know

1130
00:51:38,280 --> 00:51:44,799
attempted to help them solve this

1131
00:51:39,760 --> 00:51:45,880
problem um I think the what you know

1132
00:51:44,799 --> 00:51:47,400
what I'm what I'm going to do is

1133
00:51:45,880 --> 00:51:52,680
actually I'm going to skip over this

1134
00:51:47,400 --> 00:51:52,680
this simple example for the sake of

1135
00:51:54,040 --> 00:51:59,520
time actually I think I'm G to let's do

1136
00:52:00,960 --> 00:52:06,040
this so the simple way of thinking about

1137
00:52:03,680 --> 00:52:08,960
what their problem actually is is they

1138
00:52:06,040 --> 00:52:11,920
want to select a cohort of some size

1139
00:52:08,960 --> 00:52:14,480
from a set of uh you know applications

1140
00:52:11,920 --> 00:52:18,000
they care about two characteristics both

1141
00:52:14,480 --> 00:52:20,559
performance and diversity um I've talked

1142
00:52:18,000 --> 00:52:24,040
performance it's a sum of their project

1143
00:52:20,559 --> 00:52:26,240
scores but diversity um what I'm

1144
00:52:24,040 --> 00:52:28,200
conceptualizing as a multi-dimensional

1145
00:52:26,240 --> 00:52:30,480
pro proximity to the organization's

1146
00:52:28,200 --> 00:52:34,640
compositional Target so if they want

1147
00:52:30,480 --> 00:52:37,799
50/50 gender um if they uh you know say

1148
00:52:34,640 --> 00:52:39,359
um have you know 6040 you can

1149
00:52:37,799 --> 00:52:41,440
characterize the distance from their

1150
00:52:39,359 --> 00:52:44,680
goal or the proximity to their goal or

1151
00:52:41,440 --> 00:52:46,799
if they want um you 20% to be poor let's

1152
00:52:44,680 --> 00:52:48,200
say they only have 10% that are poor in

1153
00:52:46,799 --> 00:52:50,920
a particular cohort you can you can

1154
00:52:48,200 --> 00:52:53,880
characterize that distance and create um

1155
00:52:50,920 --> 00:52:56,040
a multi-dimensional proximity uh score

1156
00:52:53,880 --> 00:52:58,079
um and then you know the organization

1157
00:52:56,040 --> 00:53:00,040
really should just be surveying all the

1158
00:52:58,079 --> 00:53:01,880
potential cohorts that they could

1159
00:53:00,040 --> 00:53:03,920
construct that gets closest to their

1160
00:53:01,880 --> 00:53:07,160
most preferred over these two

1161
00:53:03,920 --> 00:53:09,319
characteristics um the and so the way to

1162
00:53:07,160 --> 00:53:11,240
think about this kind of a problem is

1163
00:53:09,319 --> 00:53:13,359
that um you know there are lots of

1164
00:53:11,240 --> 00:53:15,880
different potential cohorts in diversity

1165
00:53:13,359 --> 00:53:17,480
performance space um the ones near the

1166
00:53:15,880 --> 00:53:18,760
top you know top left those are ones

1167
00:53:17,480 --> 00:53:20,720
that are high diversity but low

1168
00:53:18,760 --> 00:53:23,040
performance the ones bottom right those

1169
00:53:20,720 --> 00:53:24,920
are high performance low diversity um

1170
00:53:23,040 --> 00:53:26,559
what you can do is you construct you can

1171
00:53:24,920 --> 00:53:28,400
construct what's called a selection

1172
00:53:26,559 --> 00:53:30,839
possibilities FR an tier it's the

1173
00:53:28,400 --> 00:53:33,400
maximum achievable diversity for every

1174
00:53:30,839 --> 00:53:34,720
level of uh cohort performance and what

1175
00:53:33,400 --> 00:53:37,400
the organization what they're going to

1176
00:53:34,720 --> 00:53:40,280
want to do is they want to select their

1177
00:53:37,400 --> 00:53:41,520
favorite cohort on that Frontier and so

1178
00:53:40,280 --> 00:53:45,000
the nice thing about being on this

1179
00:53:41,520 --> 00:53:47,240
Frontier is that um there are no cohorts

1180
00:53:45,000 --> 00:53:48,960
that dominate if you're on the frontier

1181
00:53:47,240 --> 00:53:51,040
there aren't existing cohorts that are

1182
00:53:48,960 --> 00:53:53,440
both better in diversity and performance

1183
00:53:51,040 --> 00:53:55,119
which you certainly want to eliminate um

1184
00:53:53,440 --> 00:53:57,000
uh you do not want to select a dominated

1185
00:53:55,119 --> 00:53:59,799
cohort um

1186
00:53:57,000 --> 00:54:01,400
now the problem that we ran into and

1187
00:53:59,799 --> 00:54:04,799
this is where algorithms start to come

1188
00:54:01,400 --> 00:54:07,280
in um is that technically uh calculating

1189
00:54:04,799 --> 00:54:11,200
the selection possibilities Frontier is

1190
00:54:07,280 --> 00:54:12,960
uh a is complex it's a hard problem um

1191
00:54:11,200 --> 00:54:14,839
and the reason of the reason for that

1192
00:54:12,960 --> 00:54:18,000
actually has to do with this diversity

1193
00:54:14,839 --> 00:54:20,040
component um it turns out that each

1194
00:54:18,000 --> 00:54:21,960
person's contribution to diversity is

1195
00:54:20,040 --> 00:54:25,200
going to depend on the identity of every

1196
00:54:21,960 --> 00:54:27,799
other member in the Co in the group and

1197
00:54:25,200 --> 00:54:31,920
so there's no such thing is individual

1198
00:54:27,799 --> 00:54:33,880
diversity it's a group concept um and so

1199
00:54:31,920 --> 00:54:36,160
because of this in order to determine

1200
00:54:33,880 --> 00:54:38,240
which is the most diverse cohort or the

1201
00:54:36,160 --> 00:54:39,720
one that's closest to their ideal you

1202
00:54:38,240 --> 00:54:41,880
actually need to com you need to look at

1203
00:54:39,720 --> 00:54:43,720
every possible cohort you'd form and

1204
00:54:41,880 --> 00:54:47,559
figure out which one gets closest to

1205
00:54:43,720 --> 00:54:50,000
your um desired uh composition and um

1206
00:54:47,559 --> 00:54:51,720
the challenge with that is that there

1207
00:54:50,000 --> 00:54:54,760
even when you have small sample size

1208
00:54:51,720 --> 00:54:57,480
small numbers of applications um you end

1209
00:54:54,760 --> 00:54:58,720
up with effectively uh you know very

1210
00:54:57,480 --> 00:55:00,920
difficult to count number of

1211
00:54:58,720 --> 00:55:02,280
applications so imagine in our context

1212
00:55:00,920 --> 00:55:06,319
there are 2,000 applications you're

1213
00:55:02,280 --> 00:55:08,760
choosing a cohort of 500 um and it turns

1214
00:55:06,319 --> 00:55:12,359
out that this would require uh actually

1215
00:55:08,760 --> 00:55:13,720
evaluating comparing uh um you know 5.6

1216
00:55:12,359 --> 00:55:16,240
* 10^

1217
00:55:13,720 --> 00:55:19,040
486th um number of cohorts in order to

1218
00:55:16,240 --> 00:55:20,880
know which one's the closest now um

1219
00:55:19,040 --> 00:55:24,280
there are estimates of the total number

1220
00:55:20,880 --> 00:55:26,160
of atoms um in the known universe and

1221
00:55:24,280 --> 00:55:28,839
this number is about that number raised

1222
00:55:26,160 --> 00:55:31,640
to to the sixth um so it's an

1223
00:55:28,839 --> 00:55:34,520
intractable problem to say the least um

1224
00:55:31,640 --> 00:55:36,039
and so uh you know formally you know for

1225
00:55:34,520 --> 00:55:37,680
people that are interested in computer

1226
00:55:36,039 --> 00:55:41,839
science or complexity Theory the problem

1227
00:55:37,680 --> 00:55:43,680
is um NP hard uh and um which means that

1228
00:55:41,839 --> 00:55:45,760
there's no algorithm that can compute

1229
00:55:43,680 --> 00:55:48,400
the answer in a reasonable amount of

1230
00:55:45,760 --> 00:55:50,319
time or an exact answer um there are

1231
00:55:48,400 --> 00:55:53,319
some special cases so if you just have

1232
00:55:50,319 --> 00:55:55,359
two groups for example um then uh you

1233
00:55:53,319 --> 00:55:58,599
can solve this uh you know quite simply

1234
00:55:55,359 --> 00:56:01,280
but in general the problem is complex uh

1235
00:55:58,599 --> 00:56:03,480
and so just to I'm I'm now just going to

1236
00:56:01,280 --> 00:56:05,520
you know o give an overview of kind of

1237
00:56:03,480 --> 00:56:07,960
how algorithms help uh solve this

1238
00:56:05,520 --> 00:56:10,359
problem so really there's like a you

1239
00:56:07,960 --> 00:56:11,760
know effectively an infinite space of

1240
00:56:10,359 --> 00:56:13,200
different cohorts you can look for and

1241
00:56:11,760 --> 00:56:15,480
you have to search in order to get to

1242
00:56:13,200 --> 00:56:18,559
the frontier what an organization

1243
00:56:15,480 --> 00:56:21,240
typically will do in the wild is kind of

1244
00:56:18,559 --> 00:56:23,240
uh select the one that's uh seems best

1245
00:56:21,240 --> 00:56:25,599
to them but they're leaving a lot of

1246
00:56:23,240 --> 00:56:26,880
potentially better cohorts in the table

1247
00:56:25,599 --> 00:56:29,160
um

1248
00:56:26,880 --> 00:56:31,520
and what would be nice would be able to

1249
00:56:29,160 --> 00:56:33,640
estimate uh this Frontier to have some

1250
00:56:31,520 --> 00:56:35,920
sort of procedure that can get you

1251
00:56:33,640 --> 00:56:40,079
closer to this Frontier so that you

1252
00:56:35,920 --> 00:56:42,640
could do substantially better um and so

1253
00:56:40,079 --> 00:56:45,599
um what I ultimately do in this project

1254
00:56:42,640 --> 00:56:47,920
is I actually generate an algorithm that

1255
00:56:45,599 --> 00:56:51,160
attempts to estimate this Frontier um

1256
00:56:47,920 --> 00:56:53,359
that's based on what's called uh greedy

1257
00:56:51,160 --> 00:56:56,079
optimization um and it uses this

1258
00:56:53,359 --> 00:56:59,079
procedure uh you know um over and over

1259
00:56:56,079 --> 00:57:02,079
again again to in order to trace out a

1260
00:56:59,079 --> 00:57:02,079
frontier

1261
00:57:02,559 --> 00:57:08,799
um the just to give uh you know a brief

1262
00:57:06,119 --> 00:57:12,319
overview of what you know what is greedy

1263
00:57:08,799 --> 00:57:15,559
optimization um what what a greedy

1264
00:57:12,319 --> 00:57:17,280
optimization process is is it's you make

1265
00:57:15,559 --> 00:57:19,720
a series of iterative iterative

1266
00:57:17,280 --> 00:57:22,440
decisions where you try and maximize the

1267
00:57:19,720 --> 00:57:25,119
increase in some objective function at

1268
00:57:22,440 --> 00:57:26,720
each choice um and you don't you

1269
00:57:25,119 --> 00:57:28,920
basically don't care about Global

1270
00:57:26,720 --> 00:57:31,760
optimization and so the way that you can

1271
00:57:28,920 --> 00:57:34,079
apply it to this problem um is that you

1272
00:57:31,760 --> 00:57:38,680
can actually have an objective function

1273
00:57:34,079 --> 00:57:42,319
which is the weighted sum of performance

1274
00:57:38,680 --> 00:57:45,880
and diversity uh and you start with an

1275
00:57:42,319 --> 00:57:47,880
empty cohort and you add people um who

1276
00:57:45,880 --> 00:57:51,200
increase the subjective function the

1277
00:57:47,880 --> 00:57:53,119
most it's a it's a it's a very sort of

1278
00:57:51,200 --> 00:57:55,839
um

1279
00:57:53,119 --> 00:57:57,280
uh almost simplistic or naive approach

1280
00:57:55,839 --> 00:58:00,359
to trying to solve this problem but it

1281
00:57:57,280 --> 00:58:03,200
ends up working fairly well um and then

1282
00:58:00,359 --> 00:58:06,720
um once you keep adding people till you

1283
00:58:03,200 --> 00:58:08,559
get to the total um cohort number um you

1284
00:58:06,720 --> 00:58:11,200
know you finish once you actually you

1285
00:58:08,559 --> 00:58:13,119
know you get to end members which is the

1286
00:58:11,200 --> 00:58:15,799
number that you want to have you do this

1287
00:58:13,119 --> 00:58:19,079
process now waiting diversity and

1288
00:58:15,799 --> 00:58:21,720
performance differently so you um first

1289
00:58:19,079 --> 00:58:23,400
you initially wait only performance and

1290
00:58:21,720 --> 00:58:25,119
then you give a little bit of weight to

1291
00:58:23,400 --> 00:58:26,680
to diversity and then you give a little

1292
00:58:25,119 --> 00:58:28,559
bit more weight to diversity University

1293
00:58:26,680 --> 00:58:29,920
and you do this procedure over and over

1294
00:58:28,559 --> 00:58:32,799
again and that gives you different

1295
00:58:29,920 --> 00:58:36,000
points um on the frontier one way of

1296
00:58:32,799 --> 00:58:38,720
thinking about this um simply is to say

1297
00:58:36,000 --> 00:58:43,039
um essentially what this does is it

1298
00:58:38,720 --> 00:58:44,599
gives bonus points to people who are in

1299
00:58:43,039 --> 00:58:47,799
particular groups that you want to

1300
00:58:44,599 --> 00:58:50,400
represent but it adjusts those points as

1301
00:58:47,799 --> 00:58:52,680
you get close to the number of uh people

1302
00:58:50,400 --> 00:58:54,680
that you actually want to uh represent

1303
00:58:52,680 --> 00:58:56,839
in your group and so if you met the

1304
00:58:54,680 --> 00:58:59,760
target you no longer get th those

1305
00:58:56,839 --> 00:59:02,440
additional bonus points in your cohort

1306
00:58:59,760 --> 00:59:06,480
um and so you know what that ultimately

1307
00:59:02,440 --> 00:59:08,400
cashes out as is um you know you have uh

1308
00:59:06,480 --> 00:59:11,280
this estimate of the the frontier you

1309
00:59:08,400 --> 00:59:13,039
know how do you get this you um run this

1310
00:59:11,280 --> 00:59:14,880
greedy optimization procedure once you

1311
00:59:13,039 --> 00:59:16,760
get a point on the frontier you run it

1312
00:59:14,880 --> 00:59:18,960
again again you get another point and

1313
00:59:16,760 --> 00:59:21,720
this allows you to trace out the entire

1314
00:59:18,960 --> 00:59:23,799
Frontier um and the idea is that now you

1315
00:59:21,720 --> 00:59:26,480
can EST you can actually pick a cohort

1316
00:59:23,799 --> 00:59:28,520
that's closer um to the you know the

1317
00:59:26,480 --> 00:59:33,440
actual uh selection possibilities

1318
00:59:28,520 --> 00:59:36,119
Frontier um the you know to sort of

1319
00:59:33,440 --> 00:59:38,000
conclude um and and and show why this

1320
00:59:36,119 --> 00:59:40,039
actually mattered in an actual

1321
00:59:38,000 --> 00:59:42,680
organization what I could do is I can

1322
00:59:40,039 --> 00:59:45,039
estimate um this Frontier of diversity

1323
00:59:42,680 --> 00:59:46,640
vers performance for the scholarship

1324
00:59:45,039 --> 00:59:49,079
that I was looking at and I could

1325
00:59:46,640 --> 00:59:52,000
compare the actual cohorts that they had

1326
00:59:49,079 --> 00:59:55,039
been selecting and so you can see that

1327
00:59:52,000 --> 00:59:56,440
um for the you know first cycle they

1328
00:59:55,039 --> 00:59:59,520
selected someone well with within the

1329
00:59:56,440 --> 01:00:01,200
estimated Frontier that I that I had um

1330
00:59:59,520 --> 01:00:03,079
that I had calculated and so you could

1331
01:00:01,200 --> 01:00:05,520
have improved diversity substantially

1332
01:00:03,079 --> 01:00:07,119
without reducing performance um or you

1333
01:00:05,520 --> 01:00:10,559
could have improved performance without

1334
01:00:07,119 --> 01:00:12,920
changing diversity um you see the same

1335
01:00:10,559 --> 01:00:17,240
thing in the second cycle that they left

1336
01:00:12,920 --> 01:00:20,640
a lot of these gains um on the table and

1337
01:00:17,240 --> 01:00:23,079
uh what I ultimately do is I gave them

1338
01:00:20,640 --> 01:00:25,039
access to this technology that I had uh

1339
01:00:23,079 --> 01:00:27,799
developed with my co-author and in the

1340
01:00:25,039 --> 01:00:29,960
third cohort you see different story um

1341
01:00:27,799 --> 01:00:33,119
so in the third cohort you see them

1342
01:00:29,960 --> 01:00:35,640
select very very close to the frontier

1343
01:00:33,119 --> 01:00:37,960
and the reason why I'm you know this

1344
01:00:35,640 --> 01:00:40,359
example is one that I'm I'm getting

1345
01:00:37,960 --> 01:00:42,799
really excited about for research is

1346
01:00:40,359 --> 01:00:44,559
that it's a case where actually there

1347
01:00:42,799 --> 01:00:47,079
were improvements on what people would

1348
01:00:44,559 --> 01:00:49,440
consider Merit and what people would

1349
01:00:47,079 --> 01:00:51,280
consider diversity um you didn't

1350
01:00:49,440 --> 01:00:54,400
actually need to sacrifice one of those

1351
01:00:51,280 --> 01:00:56,240
dimensions and I think I'm I'll conclude

1352
01:00:54,400 --> 01:00:57,720
with this you know I asked or not

1353
01:00:56,240 --> 01:01:00,319
algorithms are going to make the world

1354
01:00:57,720 --> 01:01:03,440
more fair at the beginning uh I think in

1355
01:01:00,319 --> 01:01:04,839
some cases it's uh it's it's hard to see

1356
01:01:03,440 --> 01:01:07,200
whether or not they will I think the

1357
01:01:04,839 --> 01:01:09,240
context matters a lot but the thing that

1358
01:01:07,200 --> 01:01:11,559
I think makes me optimistic is that

1359
01:01:09,240 --> 01:01:14,599
there are a number of problems that are

1360
01:01:11,559 --> 01:01:16,559
simply intractable for people to be able

1361
01:01:14,599 --> 01:01:19,039
to figure out that are fundamentally

1362
01:01:16,559 --> 01:01:21,599
fairness problems that the kind of

1363
01:01:19,039 --> 01:01:23,799
characteristics algorithms have I think

1364
01:01:21,599 --> 01:01:25,720
make them um much better at trying to

1365
01:01:23,799 --> 01:01:27,720
solve those problems in cases you know

1366
01:01:25,720 --> 01:01:30,039
like like this um humans left to their

1367
01:01:27,720 --> 01:01:32,799
own devices uh maybe we're doing

1368
01:01:30,039 --> 01:01:36,280
something both less effective and less

1369
01:01:32,799 --> 01:01:38,799
Fair um so I'm going to wrap up with

1370
01:01:36,280 --> 01:01:41,280
that I know I'm I'm way over time there

1371
01:01:38,799 --> 01:01:45,039
were some th questions for you guys but

1372
01:01:41,280 --> 01:01:45,039
thanks for having me

