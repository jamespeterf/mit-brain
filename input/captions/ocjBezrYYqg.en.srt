1
00:00:02,680 --> 00:00:03,513
(knocking on door)

2
00:00:03,513 --> 00:00:04,800
- [Interviewer] Hey, Katie.
- Hey!

3
00:00:04,800 --> 00:00:05,790
- [Interviewer] Thanks for taking the time

4
00:00:05,790 --> 00:00:06,900
to hang out with us today.

5
00:00:06,900 --> 00:00:09,000
- Of course! I'm so excited to be here.

6
00:00:09,000 --> 00:00:10,560
- [Interviewer] Now,
you're getting your PhD

7
00:00:10,560 --> 00:00:11,910
here at MIT CSAIL.

8
00:00:11,910 --> 00:00:13,890
What inspired you to
work at the intersection

9
00:00:13,890 --> 00:00:16,200
of machine learning and mental health?

10
00:00:16,200 --> 00:00:18,240
- I was initially drawn
to machine learning

11
00:00:18,240 --> 00:00:20,250
because I think it's really interesting

12
00:00:20,250 --> 00:00:22,002
from an intellectual standpoint,

13
00:00:22,002 --> 00:00:24,630
and then I quickly grew
even more interested

14
00:00:24,630 --> 00:00:26,760
when I realized the
immense potential it has

15
00:00:26,760 --> 00:00:28,890
to make a societal impact.

16
00:00:28,890 --> 00:00:32,400
And I think one area
that is particularly ripe

17
00:00:32,400 --> 00:00:35,826
to have an impact in is mental health.

18
00:00:35,826 --> 00:00:38,280
Because a lot of people
struggle with mental health,

19
00:00:38,280 --> 00:00:40,350
and compared to other aspects of health,

20
00:00:40,350 --> 00:00:41,520
like physical health,

21
00:00:41,520 --> 00:00:43,740
our understanding is relatively limited.

22
00:00:43,740 --> 00:00:45,054
- [Interviewer] I completely agree.

23
00:00:45,054 --> 00:00:47,490
Now, for those of us not in the field,

24
00:00:47,490 --> 00:00:49,260
can you walk me through the challenge

25
00:00:49,260 --> 00:00:52,050
of accurately capturing
mental health metrics

26
00:00:52,050 --> 00:00:53,370
using machine learning?

27
00:00:53,370 --> 00:00:55,680
- Yeah, let me talk through an example.

28
00:00:55,680 --> 00:00:58,020
So some of my work has looked at

29
00:00:58,020 --> 00:01:00,900
how we can automatically
monitor mental health

30
00:01:00,900 --> 00:01:02,310
from speech data.

31
00:01:02,310 --> 00:01:04,500
You can imagine that
when someone's depressed,

32
00:01:04,500 --> 00:01:06,150
it changes the way they speak.

33
00:01:06,150 --> 00:01:09,780
They may speak more slowly
and in a more monotone manner.

34
00:01:09,780 --> 00:01:11,220
And so we can use machine learning

35
00:01:11,220 --> 00:01:12,990
to pick up on those changes.

36
00:01:12,990 --> 00:01:17,460
The main challenges that other
things also change speech,

37
00:01:17,460 --> 00:01:20,820
such as how tired someone
is, who they're speaking to,

38
00:01:20,820 --> 00:01:22,830
what recording device is being used.

39
00:01:22,830 --> 00:01:26,520
And so we have to disentangle
between these various factors.

40
00:01:26,520 --> 00:01:29,340
- [Interviewer] Now, what
specific mental health conditions

41
00:01:29,340 --> 00:01:30,960
are you focusing on?

42
00:01:30,960 --> 00:01:33,780
- I've worked on speech data

43
00:01:33,780 --> 00:01:36,330
from people with bipolar disorder.

44
00:01:36,330 --> 00:01:38,940
I've also worked on wearable sensor data

45
00:01:38,940 --> 00:01:41,700
that we've collected from
people who have depression.

46
00:01:41,700 --> 00:01:44,340
And then finally I've looked
at stress and anxiety.

47
00:01:44,340 --> 00:01:46,950
- [Interviewer] You've been
at this for about five years,

48
00:01:46,950 --> 00:01:49,470
so how do you see
personalized interventions

49
00:01:49,470 --> 00:01:52,500
improving mental healthcare
in say, the next decade?

50
00:01:52,500 --> 00:01:55,260
- One of the big challenges
with mental healthcare

51
00:01:55,260 --> 00:01:57,720
is that the best treatment
for one individual

52
00:01:57,720 --> 00:01:59,730
can be different from the best treatment

53
00:01:59,730 --> 00:02:01,050
for another individual.

54
00:02:01,050 --> 00:02:04,020
And it's hard to know which
treatment works best in advance.

55
00:02:04,020 --> 00:02:05,550
So machine learning can be used

56
00:02:05,550 --> 00:02:08,580
to help understand
interpersonal differences,

57
00:02:08,580 --> 00:02:11,310
to enable more tailored treatments.

58
00:02:11,310 --> 00:02:13,050
- [Interviewer] Now,
to deploy these models

59
00:02:13,050 --> 00:02:13,980
in the real world,

60
00:02:13,980 --> 00:02:16,590
we always hope that
they're safe and accurate,

61
00:02:16,590 --> 00:02:18,210
but how do we ensure that?

62
00:02:18,210 --> 00:02:20,520
And do you have any ethical concerns?

63
00:02:20,520 --> 00:02:21,420
- Absolutely.

64
00:02:21,420 --> 00:02:23,850
I think when we think about deploying

65
00:02:23,850 --> 00:02:26,760
machine learning based health technology,

66
00:02:26,760 --> 00:02:29,550
we need to consider how we evaluate

67
00:02:29,550 --> 00:02:32,640
other types of healthcare
interventions, like medications.

68
00:02:32,640 --> 00:02:35,550
We should apply the same
scrutiny when considering

69
00:02:35,550 --> 00:02:39,510
whether we should use mental
health technology on patients.

70
00:02:39,510 --> 00:02:40,350
- [Interviewer] Absolutely.

71
00:02:40,350 --> 00:02:42,510
Now a big part of research

72
00:02:42,510 --> 00:02:45,570
is dealing with difficult
challenges all day long

73
00:02:45,570 --> 00:02:46,680
and trying to solve them.

74
00:02:46,680 --> 00:02:49,410
So tell us about something
particularly challenging

75
00:02:49,410 --> 00:02:50,640
that you solved.

76
00:02:50,640 --> 00:02:54,900
- Yeah, so a problem we
see time and time again

77
00:02:54,900 --> 00:02:56,940
when working with mental health data

78
00:02:56,940 --> 00:02:58,590
is that when we visualize the data,

79
00:02:58,590 --> 00:03:01,260
so say we have speech here on the x-axis

80
00:03:01,260 --> 00:03:03,960
and heart rate on the y-axis,

81
00:03:03,960 --> 00:03:06,570
the data is highly
clustered by individual.

82
00:03:06,570 --> 00:03:08,040
So here we have person one,

83
00:03:08,040 --> 00:03:10,260
and over here we have person two.

84
00:03:10,260 --> 00:03:14,190
And these inter-person
differences are so pronounced

85
00:03:14,190 --> 00:03:17,910
that it can make it hard to
determine the differences

86
00:03:17,910 --> 00:03:20,310
we actually care about,
which are within individuals.

87
00:03:20,310 --> 00:03:22,680
So when is this person
depressed versus not?

88
00:03:22,680 --> 00:03:24,600
And so I developed algorithms

89
00:03:24,600 --> 00:03:26,880
that allow us to better
focus on the differences

90
00:03:26,880 --> 00:03:29,580
we care about and ignore
these other differences

91
00:03:29,580 --> 00:03:31,380
that make it difficult to learn.

92
00:03:31,380 --> 00:03:33,000
- [Interviewer] That
does sound challenging.

93
00:03:33,000 --> 00:03:35,490
Now, do you have a
favorite software or tool

94
00:03:35,490 --> 00:03:37,590
that's indispensable to your work?

95
00:03:37,590 --> 00:03:40,530
- One tool I've liked a lot recently

96
00:03:40,530 --> 00:03:42,360
is called Weights and Biases.

97
00:03:42,360 --> 00:03:45,180
It allows me to automatically
track the progress

98
00:03:45,180 --> 00:03:48,906
of my experiments and visualize
the results in real time.

99
00:03:48,906 --> 00:03:50,070
(elevator dinging)

100
00:03:50,070 --> 00:03:51,569
- Oh, hi Katie!
- Oh my gosh, hi!

101
00:03:51,569 --> 00:03:53,670
- So good to see you.
- You too.

102
00:03:53,670 --> 00:03:55,380
- So to put this in really simple terms,

103
00:03:55,380 --> 00:03:57,150
can you explain the
role of causal inference

104
00:03:57,150 --> 00:03:58,530
as if I was a 10-year-old?

105
00:03:58,530 --> 00:04:01,530
- Sure! So causal inference is a field

106
00:04:01,530 --> 00:04:03,630
that studies cause and effect.

107
00:04:03,630 --> 00:04:06,990
And it's important to
distinguish causal relationships

108
00:04:06,990 --> 00:04:08,850
from those that are just correlations.

109
00:04:08,850 --> 00:04:10,950
For instance, if I take vitamins,

110
00:04:10,950 --> 00:04:13,290
I wanna know if they will
actually improve my health,

111
00:04:13,290 --> 00:04:15,540
versus they're just something
that healthy people do,

112
00:04:15,540 --> 00:04:17,610
so they're correlated with health.

113
00:04:17,610 --> 00:04:19,470
- Oh yeah, that makes a lot of sense.

114
00:04:19,470 --> 00:04:21,630
So can you explain the
role of causal inference

115
00:04:21,630 --> 00:04:23,730
as it pertains in your research

116
00:04:23,730 --> 00:04:25,140
on machine learning and health?

117
00:04:25,140 --> 00:04:26,430
- Yeah, absolutely.

118
00:04:26,430 --> 00:04:29,940
So when we use machine
learning to, for instance,

119
00:04:29,940 --> 00:04:31,860
recommend treatments to patients,

120
00:04:31,860 --> 00:04:34,820
we need to make sure
that the relationships

121
00:04:34,820 --> 00:04:37,350
we're considering are
causal relationships,

122
00:04:37,350 --> 00:04:40,170
that the treatment will
actually have a causal effect

123
00:04:40,170 --> 00:04:41,370
on the patient's health,

124
00:04:41,370 --> 00:04:44,370
and isn't just something that
the machine learning model

125
00:04:44,370 --> 00:04:47,970
has picked up as co-occurring
with better health.

126
00:04:47,970 --> 00:04:49,440
- Thank you. That's very helpful.

127
00:04:49,440 --> 00:04:51,012
- [Katie] Of course. Good to see you!

128
00:04:51,012 --> 00:04:52,350
- You too. Hope you have a great day.

129
00:04:52,350 --> 00:04:53,183
- [Katie] You too.

130
00:04:53,183 --> 00:04:54,450
- [Interviewer] So a big part of research

131
00:04:54,450 --> 00:04:57,000
is balancing the theoretical
with the practical.

132
00:04:57,000 --> 00:04:58,380
How do you do that?

133
00:04:58,380 --> 00:05:02,070
- I like to use the practical
challenges that we encounter

134
00:05:02,070 --> 00:05:04,620
when thinking about
mental health applications

135
00:05:04,620 --> 00:05:08,040
to motivate my more general
machine learning work.

136
00:05:08,040 --> 00:05:09,660
- [Interviewer] Now,
what about limitations

137
00:05:09,660 --> 00:05:12,060
to current approaches to
mental health monitoring?

138
00:05:12,060 --> 00:05:13,440
How does your work address those?

139
00:05:13,440 --> 00:05:17,190
- One of the main limitations
is that monitoring right now

140
00:05:17,190 --> 00:05:20,640
is restricted primarily to
observations of patients

141
00:05:20,640 --> 00:05:21,810
in the clinic.

142
00:05:21,810 --> 00:05:25,590
My work examines how we
can use data from patients

143
00:05:25,590 --> 00:05:28,530
outside of the clinic as they
go about their everyday lives

144
00:05:28,530 --> 00:05:30,870
to get a more comprehensive
picture of health.

145
00:05:30,870 --> 00:05:32,070
- [Interviewer] Could you talk to me about

146
00:05:32,070 --> 00:05:33,724
what you find to be the most
fulfilling aspect of your work?

147
00:05:33,724 --> 00:05:34,650
(elevator dinging)

148
00:05:34,650 --> 00:05:37,290
- There are a couple
things. First is the people.

149
00:05:37,290 --> 00:05:40,320
I get to work with brilliant, motivated,

150
00:05:40,320 --> 00:05:42,270
just wonderful people.

151
00:05:42,270 --> 00:05:43,260
And then the second

152
00:05:43,260 --> 00:05:45,990
is that I'm constantly challenging myself

153
00:05:45,990 --> 00:05:47,220
and learning new things.

154
00:05:47,220 --> 00:05:48,540
- [Interviewer] Do you have any current

155
00:05:48,540 --> 00:05:51,210
favorite applications of ML in healthcare?

156
00:05:51,210 --> 00:05:53,580
- Obviously I'm excited
about mental healthcare.

157
00:05:53,580 --> 00:05:54,810
But in addition to that,

158
00:05:54,810 --> 00:05:57,390
I'm also really excited
about machine learning

159
00:05:57,390 --> 00:06:00,163
for helping people improve
their physical fitness.

160
00:06:00,163 --> 00:06:02,220
- [Interviewer] With all these wearables,

161
00:06:02,220 --> 00:06:04,800
I do tend to think about
navigating privacy concerns

162
00:06:04,800 --> 00:06:07,275
that are inherent in using
AI for health monitoring.

163
00:06:07,275 --> 00:06:08,400
- Definitely.

164
00:06:08,400 --> 00:06:11,610
That's a really important
thing that we think about.

165
00:06:11,610 --> 00:06:14,010
There's many ways we consider privacy.

166
00:06:14,010 --> 00:06:18,180
One is making sure we have
great security practices.

167
00:06:18,180 --> 00:06:21,330
Another is making sure we
only collect the data we need.

168
00:06:21,330 --> 00:06:23,070
And then finally, we also make sure

169
00:06:23,070 --> 00:06:25,140
that we're transparent with the peoples

170
00:06:25,140 --> 00:06:29,460
whose data we're collecting
and we obtain informed consent.

171
00:06:29,460 --> 00:06:30,780
- [Interviewer] Over
the past couple years,

172
00:06:30,780 --> 00:06:33,030
we've had increasingly more conversations

173
00:06:33,030 --> 00:06:35,610
around mental health and therapy.

174
00:06:35,610 --> 00:06:38,520
So how do you see the
integration of these AI tools

175
00:06:38,520 --> 00:06:41,550
in mental healthcare changing
societal perceptions?

176
00:06:41,550 --> 00:06:43,800
- Incorporating AI in mental healthcare

177
00:06:43,800 --> 00:06:45,540
can help make it more accessible.

178
00:06:45,540 --> 00:06:48,540
And I think in doing so, that
will help to reduce the stigma

179
00:06:48,540 --> 00:06:50,400
around accessing mental healthcare.

180
00:06:50,400 --> 00:06:51,600
- [Interviewer] When
you lie awake at night,

181
00:06:51,600 --> 00:06:53,850
do any philosophical questions come up,

182
00:06:53,850 --> 00:06:56,970
considering the role of AI in
understanding human emotions?

183
00:06:56,970 --> 00:06:58,680
- One question that comes up is,

184
00:06:58,680 --> 00:07:01,620
what does it even mean
to experience an emotion?

185
00:07:01,620 --> 00:07:04,620
AI may be able to simulate
having an emotion,

186
00:07:04,620 --> 00:07:07,800
but does that mean it
really experiences emotions?

187
00:07:07,800 --> 00:07:10,320
- [Interviewer] What about the
balance between human empathy

188
00:07:10,320 --> 00:07:12,360
and machine precision playing out

189
00:07:12,360 --> 00:07:13,710
in mental health interventions?

190
00:07:13,710 --> 00:07:15,540
- I think we definitely need both,

191
00:07:15,540 --> 00:07:18,720
and I don't think they
necessarily need to be intention.

192
00:07:18,720 --> 00:07:21,330
If we equip mental health professionals

193
00:07:21,330 --> 00:07:24,300
to improve their ability to help patients

194
00:07:24,300 --> 00:07:27,050
using machine learning tools,
I think we can have both.

195
00:07:28,020 --> 00:07:29,610
- [Interviewer] And what
about this potential

196
00:07:29,610 --> 00:07:32,640
future relationship between
human therapists and AI

197
00:07:32,640 --> 00:07:33,783
in mental healthcare?

198
00:07:34,800 --> 00:07:38,940
- Yeah, I think mental healthcare workers

199
00:07:38,940 --> 00:07:42,270
are already using AI tools
to help, for instance,

200
00:07:42,270 --> 00:07:44,010
summarize clinical notes.

201
00:07:44,010 --> 00:07:46,860
And I think we'll see this
continue into the future,

202
00:07:46,860 --> 00:07:50,580
where AI is used to enhance
the ability of therapists

203
00:07:50,580 --> 00:07:54,140
and other mental healthcare
professionals to do their jobs.

204
00:07:54,140 --> 00:07:55,560
- Oh, hey, Katie!
- Hey Gabe!

205
00:07:55,560 --> 00:07:57,240
- Can you tell me about
a moment in your research

206
00:07:57,240 --> 00:07:58,410
that changed your understanding

207
00:07:58,410 --> 00:07:59,820
of machine learning's potential?

208
00:07:59,820 --> 00:08:02,640
- Yeah, so actually, when I
first started doing research,

209
00:08:02,640 --> 00:08:03,960
I worked on a project

210
00:08:03,960 --> 00:08:06,960
where we were detecting
changes in the severity

211
00:08:06,960 --> 00:08:09,300
of bipolar disorder from speech data.

212
00:08:09,300 --> 00:08:12,390
And when I got my first model
working, it was so exciting,

213
00:08:12,390 --> 00:08:14,490
and it really demonstrated to me

214
00:08:14,490 --> 00:08:16,200
how machine learning has this potential

215
00:08:16,200 --> 00:08:18,809
to help in really important applications.

216
00:08:18,809 --> 00:08:20,580
- Cool, thanks!
- Yeah, of course.

217
00:08:20,580 --> 00:08:22,350
See you, Gabe!
- Yeah.

218
00:08:22,350 --> 00:08:23,580
- [Interviewer] You've
been at this for a while.

219
00:08:23,580 --> 00:08:25,860
I've asked you a lot of
questions about your research.

220
00:08:25,860 --> 00:08:27,030
So throughout all this,

221
00:08:27,030 --> 00:08:29,610
how do you maintain a sense
of curiosity and wonder,

222
00:08:29,610 --> 00:08:32,640
especially when things are
complex or frustrating?

223
00:08:32,640 --> 00:08:34,500
- I think for me, a big part of it

224
00:08:34,500 --> 00:08:37,260
is recognizing that when challenges occur,

225
00:08:37,260 --> 00:08:39,600
that's where most of the learning lies.

226
00:08:39,600 --> 00:08:42,630
I've found that I've made some
of my biggest breakthroughs

227
00:08:42,630 --> 00:08:44,493
at moments of peak frustration.

228
00:08:45,690 --> 00:08:47,910
- [Interviewer] Now since
we're in a different building,

229
00:08:47,910 --> 00:08:50,070
I thought it'd be a great
time to ask about life

230
00:08:50,070 --> 00:08:51,330
at MIT CSAIL.

231
00:08:51,330 --> 00:08:53,610
Tell me about your most
memorable experience so far.

232
00:08:53,610 --> 00:08:56,730
- One of my most memorable
experiences came early on

233
00:08:56,730 --> 00:09:00,567
when I was giving one of my
first lab meeting presentations.

234
00:09:00,567 --> 00:09:03,120
And I found that within just a few slides

235
00:09:03,120 --> 00:09:04,320
of my presentation,

236
00:09:04,320 --> 00:09:07,650
I was stopped and asked so many questions.

237
00:09:07,650 --> 00:09:08,940
And I think that really reflects

238
00:09:08,940 --> 00:09:10,980
how engaged everyone is here.

239
00:09:10,980 --> 00:09:12,930
Everyone was so invested in helping me

240
00:09:12,930 --> 00:09:16,203
and offering suggestions, and
that's really stuck with me.

241
00:09:16,203 --> 00:09:19,380
- [Interviewer] Now, what
about a typical day at CSAIL?

242
00:09:19,380 --> 00:09:21,180
- My days vary a lot.

243
00:09:21,180 --> 00:09:23,310
I do a number of different things,

244
00:09:23,310 --> 00:09:27,150
from coding to writing to
preparing presentations,

245
00:09:27,150 --> 00:09:30,750
to having brainstorming
sessions, whiteboarding.

246
00:09:30,750 --> 00:09:33,210
So it really just depends
on the project and the day.

247
00:09:33,210 --> 00:09:36,330
And I love the degree of
flexibility that I have.

248
00:09:36,330 --> 00:09:38,700
- [Interviewer] Yeah, like
hanging out with us all day.

249
00:09:38,700 --> 00:09:41,490
Now, I know community is a
really big part of research

250
00:09:41,490 --> 00:09:43,830
and it's a big part of the ethos at CSAIL.

251
00:09:43,830 --> 00:09:45,000
Talk to me about a collaboration

252
00:09:45,000 --> 00:09:47,070
that maybe brought a new
perspective to your work?

253
00:09:47,070 --> 00:09:50,790
- So partway through my
second year of my PhD,

254
00:09:50,790 --> 00:09:54,090
I started working closely with
my friend and collaborator,

255
00:09:54,090 --> 00:09:55,500
Rob Lewis.

256
00:09:55,500 --> 00:09:58,260
And we worked on a number
of different projects,

257
00:09:58,260 --> 00:10:01,440
ranging from more applied
in the mental health space

258
00:10:01,440 --> 00:10:03,960
to more general machine learning projects.

259
00:10:03,960 --> 00:10:05,670
And I've just loved having someone

260
00:10:05,670 --> 00:10:07,830
to go into the weeds with.

261
00:10:07,830 --> 00:10:10,830
We do a lot of brainstorming
sessions together,

262
00:10:10,830 --> 00:10:13,830
and some of my most
productive research moments

263
00:10:13,830 --> 00:10:15,450
have come through that collaboration,

264
00:10:15,450 --> 00:10:17,910
so it's something I've appreciated so much

265
00:10:17,910 --> 00:10:19,230
during my time here.

266
00:10:19,230 --> 00:10:21,960
- [Interviewer] What's it like
working with your advisors?

267
00:10:21,960 --> 00:10:25,140
- Working with my advisors
is a wonderful experience.

268
00:10:25,140 --> 00:10:27,450
Not only are they brilliant,

269
00:10:27,450 --> 00:10:31,530
they're also incredibly
supportive and engaged in my work.

270
00:10:31,530 --> 00:10:33,420
One thing I really admire about them

271
00:10:33,420 --> 00:10:37,560
is they both value doing really
high quality technical work

272
00:10:37,560 --> 00:10:40,350
and making sure that the
work has the potential

273
00:10:40,350 --> 00:10:42,150
for real world impact.

274
00:10:42,150 --> 00:10:44,250
- [Interviewer] And how do
you incorporate their feedback

275
00:10:44,250 --> 00:10:46,080
into your work?

276
00:10:46,080 --> 00:10:47,130
- That's a great question.

277
00:10:47,130 --> 00:10:50,820
I think about the advice
they've given me a lot

278
00:10:50,820 --> 00:10:53,130
as I'm going about my
research, week to week.

279
00:10:53,130 --> 00:10:56,205
But then I also have
meetings each week with them

280
00:10:56,205 --> 00:10:58,740
where we discuss together

281
00:10:58,740 --> 00:11:00,570
how I can incorporate their feedback

282
00:11:00,570 --> 00:11:02,940
and thinking about what
I'm going to do next.

283
00:11:02,940 --> 00:11:03,840
- [Interviewer] Okay.

284
00:11:03,840 --> 00:11:06,300
Now, do you have a
favorite spot on campus?

285
00:11:06,300 --> 00:11:10,440
- Yeah. I love going to the
river, at the Charles River,

286
00:11:10,440 --> 00:11:11,520
and walking along it.

287
00:11:11,520 --> 00:11:13,680
I find it, it's a great
way to clear my head,

288
00:11:13,680 --> 00:11:16,320
especially when I'm working on
a difficult research problem.

289
00:11:16,320 --> 00:11:18,330
- [Interviewer] It's
beautiful and peaceful.

290
00:11:18,330 --> 00:11:20,638
Is that your favorite spot in Cambridge?

291
00:11:20,638 --> 00:11:23,850
- It's among them. I also am a big foodie.

292
00:11:23,850 --> 00:11:28,590
So I love this Mediterranean
tapas restaurant called Oleana.

293
00:11:28,590 --> 00:11:29,700
- [Interviewer] Oleana is amazing.

294
00:11:29,700 --> 00:11:32,010
If you're watching this
and you haven't been, go.

295
00:11:32,010 --> 00:11:35,070
Now, what's your favorite aspect
of living in Massachusetts?

296
00:11:35,070 --> 00:11:37,650
- I love that Massachusetts has a mix of

297
00:11:37,650 --> 00:11:42,180
everything I like in a
state, in Boston, in a city.

298
00:11:42,180 --> 00:11:45,540
There's great food, there's fun breweries,

299
00:11:45,540 --> 00:11:48,720
there's pretty nature,
Boston's a pretty city.

300
00:11:48,720 --> 00:11:52,350
And then on top of that,
there's access to nature.

301
00:11:52,350 --> 00:11:53,940
I love skiing and hiking

302
00:11:53,940 --> 00:11:57,450
and it's great that I can do
both of those easily from here.

303
00:11:57,450 --> 00:11:58,710
- [Interviewer] Let's rewind a bit

304
00:11:58,710 --> 00:12:00,990
and talk about life before MIT.

305
00:12:00,990 --> 00:12:02,220
Talk to me about your experiences

306
00:12:02,220 --> 00:12:03,570
at the University of Michigan.

307
00:12:03,570 --> 00:12:06,510
- I loved my time at the
University of Michigan.

308
00:12:06,510 --> 00:12:08,100
I actually grew up in Ann Arbor,

309
00:12:08,100 --> 00:12:11,100
so the city has a really
special place in my heart.

310
00:12:11,100 --> 00:12:14,010
And my time at the University
of Michigan was so fun.

311
00:12:14,010 --> 00:12:16,320
I met so many of my best friends there.

312
00:12:16,320 --> 00:12:20,580
It has this really high
energy culture that I love.

313
00:12:20,580 --> 00:12:21,990
And then the other thing about it

314
00:12:21,990 --> 00:12:25,650
is it's where I really
cultivated my research interests.

315
00:12:25,650 --> 00:12:28,350
It's where I first started doing research

316
00:12:28,350 --> 00:12:30,120
on machine learning for mental health,

317
00:12:30,120 --> 00:12:34,200
and first thought about
applying to do my PhD,

318
00:12:34,200 --> 00:12:37,110
and so ultimately how I ended up at MIT.

319
00:12:37,110 --> 00:12:38,280
- [Interviewer] Wow. All right.

320
00:12:38,280 --> 00:12:40,260
So what are some of the differences then

321
00:12:40,260 --> 00:12:43,080
between living in Michigan and Cambridge?

322
00:12:43,080 --> 00:12:46,080
- Well, I was hoping that
when I moved to Cambridge,

323
00:12:46,080 --> 00:12:49,020
I would escape the Michigan
winters and have better weather.

324
00:12:49,020 --> 00:12:51,570
That hasn't really turned out to be true.

325
00:12:51,570 --> 00:12:53,220
It's maybe a little bit better.

326
00:12:53,220 --> 00:12:54,990
But what I like about living in Cambridge

327
00:12:54,990 --> 00:12:57,210
is it feels like a bigger
city than Ann Arbor,

328
00:12:57,210 --> 00:12:59,317
which is very much a college town.

329
00:12:59,317 --> 00:13:02,970
So there are more restaurants,
more people, more going on.

330
00:13:02,970 --> 00:13:04,650
- [Interviewer] Okay,
so the bar is really low

331
00:13:04,650 --> 00:13:06,450
for weather in Boston.

332
00:13:06,450 --> 00:13:08,400
But anyways, I wanna talk
about your internships.

333
00:13:08,400 --> 00:13:11,070
I know you interned at
Microsoft three times.

334
00:13:11,070 --> 00:13:12,480
Tell me a little bit about that.

335
00:13:12,480 --> 00:13:15,480
- I really liked my
internships at Microsoft.

336
00:13:15,480 --> 00:13:17,250
The first two were in
software engineering,

337
00:13:17,250 --> 00:13:18,600
and I was so excited

338
00:13:18,600 --> 00:13:20,970
because it was my first
time working at a company

339
00:13:20,970 --> 00:13:21,947
in general.

340
00:13:21,947 --> 00:13:25,920
But what I didn't like
as much is I felt like

341
00:13:25,920 --> 00:13:28,470
it was a little bit limited
in terms of creativity.

342
00:13:28,470 --> 00:13:31,599
I was told to code things
and then I'd code them.

343
00:13:31,599 --> 00:13:34,050
So I was actually very excited

344
00:13:34,050 --> 00:13:37,590
when I got to intern at Microsoft
Research this past summer.

345
00:13:37,590 --> 00:13:40,200
I had much more freedom in terms of

346
00:13:40,200 --> 00:13:42,270
directing the course of the project,

347
00:13:42,270 --> 00:13:45,180
and was able to work on
cutting-edge research.

348
00:13:45,180 --> 00:13:47,010
- [Interviewer] Now, did
any of that influence

349
00:13:47,010 --> 00:13:47,850
your current work?

350
00:13:47,850 --> 00:13:49,440
And either way, just tell me about

351
00:13:49,440 --> 00:13:51,450
some current published
papers you're working on.

352
00:13:51,450 --> 00:13:56,450
- Yeah, so one recent paper
was around detecting stress

353
00:13:57,240 --> 00:14:00,690
from a biosignal called
electrodermal activity,

354
00:14:00,690 --> 00:14:03,030
which sounds scary, but
really what that is,

355
00:14:03,030 --> 00:14:05,970
it's the electrical
conductance of your skin,

356
00:14:05,970 --> 00:14:07,740
which has to do with how much you sweat.

357
00:14:07,740 --> 00:14:11,370
So when your fight or flight
response is triggered,

358
00:14:11,370 --> 00:14:13,740
it changes sweat activity,

359
00:14:13,740 --> 00:14:16,140
and then we can actually measure that

360
00:14:16,140 --> 00:14:19,230
to track changes in stress.

361
00:14:19,230 --> 00:14:21,660
- [Interviewer] The field
is rapidly advancing.

362
00:14:21,660 --> 00:14:24,030
It sometimes feels like it's almost daily.

363
00:14:24,030 --> 00:14:26,790
So how do you stay current
with new advancements?

364
00:14:26,790 --> 00:14:28,020
- It can be tough,

365
00:14:28,020 --> 00:14:30,420
but I find there's a
few things that help me.

366
00:14:30,420 --> 00:14:32,190
One is Twitter.

367
00:14:32,190 --> 00:14:34,860
Academic Twitter has a lot of posts

368
00:14:34,860 --> 00:14:37,530
where people are sharing
the latest advancements.

369
00:14:37,530 --> 00:14:39,330
And then I read a lot of papers,

370
00:14:39,330 --> 00:14:41,160
and then finally I talk to people.

371
00:14:41,160 --> 00:14:44,190
I find it's helpful to know
people who are in the know.

372
00:14:44,190 --> 00:14:45,660
So I'm always talking to other people

373
00:14:45,660 --> 00:14:48,270
about what they've been reading about.

374
00:14:48,270 --> 00:14:50,610
- [Interviewer] How do you
prioritize all of your tasks

375
00:14:50,610 --> 00:14:52,680
and just manage your time between research

376
00:14:52,680 --> 00:14:54,300
and your personal life?

377
00:14:54,300 --> 00:14:57,120
- It can be tough, but I
find what's important for me

378
00:14:57,120 --> 00:14:59,610
is scheduling things in my personal life,

379
00:14:59,610 --> 00:15:02,220
so I hold myself
accountable to doing them.

380
00:15:02,220 --> 00:15:06,810
So I love playing soccer and I
have regular soccer practice.

381
00:15:06,810 --> 00:15:10,350
I love running and I book time for that.

382
00:15:10,350 --> 00:15:13,230
And then I also make a lot
of social plans with friends

383
00:15:13,230 --> 00:15:14,970
to get me out of the office.

384
00:15:14,970 --> 00:15:15,803
- [Interviewer] Okay.

385
00:15:15,803 --> 00:15:18,330
So now can you share a book or a paper

386
00:15:18,330 --> 00:15:20,460
that's greatly influenced you?

387
00:15:20,460 --> 00:15:22,500
- Yeah, so off the top of my head,

388
00:15:22,500 --> 00:15:24,780
one book that's influenced
my research a lot

389
00:15:24,780 --> 00:15:26,190
is called "The Book of Why."

390
00:15:26,190 --> 00:15:28,560
It's by Judea Pearl and Dana Mackenzie.

391
00:15:28,560 --> 00:15:31,800
And it's about causal
inference and its importance.

392
00:15:31,800 --> 00:15:33,930
- [Interviewer] What's it
like to mentor students

393
00:15:33,930 --> 00:15:35,760
or new researchers who come to your lab?

394
00:15:35,760 --> 00:15:37,470
- Getting to mentor other students

395
00:15:37,470 --> 00:15:40,560
is one of my favorite parts
about being a grad student.

396
00:15:40,560 --> 00:15:43,920
I find that it's so rewarding
to get to pay it forward

397
00:15:43,920 --> 00:15:46,770
and help other people
find the joys of research,

398
00:15:46,770 --> 00:15:49,050
and gain the confidence in their ability

399
00:15:49,050 --> 00:15:52,080
to take on challenging research problems.

400
00:15:52,080 --> 00:15:53,190
Oh, hey Rob!

401
00:15:53,190 --> 00:15:56,190
- Hey Katie. How's it going?
- Good. How are you?

402
00:15:56,190 --> 00:15:57,780
- Good, good. It's great to see you.

403
00:15:57,780 --> 00:16:01,050
What are your aspirations for
the next phase of your career?

404
00:16:01,050 --> 00:16:03,300
- I think a couple things come to mind.

405
00:16:03,300 --> 00:16:06,390
One is I would like to move more towards

406
00:16:06,390 --> 00:16:09,071
having a real impact on real patients.

407
00:16:09,071 --> 00:16:11,970
And then the other is I
would like to continue

408
00:16:11,970 --> 00:16:13,230
to mentor students,

409
00:16:13,230 --> 00:16:17,490
and I would like to increase
my scope in doing that

410
00:16:17,490 --> 00:16:20,733
and actually become a
professor and have my own lab.

411
00:16:20,733 --> 00:16:22,980
- These are all really admirable goals,

412
00:16:22,980 --> 00:16:24,990
and I'm sure you're gonna get there.

413
00:16:24,990 --> 00:16:27,300
I have one last question for you.

414
00:16:27,300 --> 00:16:30,450
What advice would you give
to aspiring young researchers

415
00:16:30,450 --> 00:16:33,613
who dream about getting
to a position like yours?

416
00:16:33,613 --> 00:16:36,596
- I think one thing that's
been really big for me

417
00:16:36,596 --> 00:16:41,596
has been reflecting on what
my genuine interests are

418
00:16:41,610 --> 00:16:42,930
and following those.

419
00:16:42,930 --> 00:16:45,990
I think machine learning has gotten so hot

420
00:16:45,990 --> 00:16:48,360
and there's all these things
like large language models

421
00:16:48,360 --> 00:16:50,190
that everyone's really excited about,

422
00:16:50,190 --> 00:16:51,507
and it's easy to get caught up

423
00:16:51,507 --> 00:16:54,060
in what other people are talking about.

424
00:16:54,060 --> 00:16:55,380
But it's important to think about

425
00:16:55,380 --> 00:16:58,740
what genuinely gets you
excited and go for that.

426
00:16:58,740 --> 00:17:01,050
And then the second thing
is to really go for it.

427
00:17:01,050 --> 00:17:03,570
I've found that by reaching out to people

428
00:17:03,570 --> 00:17:06,082
and expressing my interest
and making connections,

429
00:17:06,082 --> 00:17:08,220
that's been huge for my success,

430
00:17:08,220 --> 00:17:10,350
because so much of my success comes from

431
00:17:10,350 --> 00:17:12,150
the people that I get to work with.

432
00:17:12,150 --> 00:17:15,690
It was good to see you, Rob.
Nice hanging with you all.

433
00:17:15,690 --> 00:17:18,273
I'll see you next time. Take care!

