1
00:00:00,040 --> 00:00:04,720
and thank you all for being here today

2
00:00:01,760 --> 00:00:06,240
to attend this uh talk uh today's primer

3
00:00:04,720 --> 00:00:08,679
is going to be about large language

4
00:00:06,240 --> 00:00:10,960
models and biological Foundation models

5
00:00:08,679 --> 00:00:12,559
I'm going to give some background ideas

6
00:00:10,960 --> 00:00:16,120
about these models and then how they can

7
00:00:12,559 --> 00:00:16,120
be applied to biological

8
00:00:16,279 --> 00:00:23,240
data so 2024 is a very exciting time to

9
00:00:20,199 --> 00:00:26,000
be in AI research uh every day now there

10
00:00:23,240 --> 00:00:27,640
are more and more advanced AI models and

11
00:00:26,000 --> 00:00:29,599
applications and algorithms being

12
00:00:27,640 --> 00:00:31,880
released uh the ones on the slide here

13
00:00:29,599 --> 00:00:33,600
that I've listed are just a sample of

14
00:00:31,880 --> 00:00:35,920
what I've been seeing on my computer

15
00:00:33,600 --> 00:00:39,160
shared around Twitter uh in the past one

16
00:00:35,920 --> 00:00:41,559
or two years um in the past just one or

17
00:00:39,160 --> 00:00:43,840
two years we have had Advanced image

18
00:00:41,559 --> 00:00:46,760
generation models biological models and

19
00:00:43,840 --> 00:00:49,480
very Advanced AI models um generative AI

20
00:00:46,760 --> 00:00:51,399
models coming out of large companies and

21
00:00:49,480 --> 00:00:53,960
the general sense right now is that with

22
00:00:51,399 --> 00:00:57,280
these Advanced AI systems we are now

23
00:00:53,960 --> 00:01:00,039
closer than ever to building General uh

24
00:00:57,280 --> 00:01:01,840
intelligent AI systems and a large

25
00:01:00,039 --> 00:01:04,640
reason behind these recent developments

26
00:01:01,840 --> 00:01:07,040
in AI has been Recent research in large

27
00:01:04,640 --> 00:01:08,720
language models in today's primer I want

28
00:01:07,040 --> 00:01:11,320
to give some more details about the

29
00:01:08,720 --> 00:01:13,479
training architecture and capabilities

30
00:01:11,320 --> 00:01:15,159
of these large language models and then

31
00:01:13,479 --> 00:01:19,520
how these models and other Foundation

32
00:01:15,159 --> 00:01:19,520
models are being applied to biological

33
00:01:20,159 --> 00:01:25,320
data so I'll begin with the question

34
00:01:22,640 --> 00:01:27,439
generally what are large language models

35
00:01:25,320 --> 00:01:29,600
a large language model is a very large

36
00:01:27,439 --> 00:01:32,560
deep learning model that is trained on

37
00:01:29,600 --> 00:01:34,200
text from the internet so imagine all of

38
00:01:32,560 --> 00:01:36,720
the information that you see on the

39
00:01:34,200 --> 00:01:39,840
internet the social media sites user

40
00:01:36,720 --> 00:01:42,880
posts documents uh news articles and all

41
00:01:39,840 --> 00:01:44,799
other human generated information this

42
00:01:42,880 --> 00:01:46,759
is the data that is scraped from the

43
00:01:44,799 --> 00:01:49,640
internet and formed into textual data

44
00:01:46,759 --> 00:01:52,960
sets and this is the base of knowledge

45
00:01:49,640 --> 00:01:55,119
for these very large uh models on the

46
00:01:52,960 --> 00:01:57,600
slide here I have a recent timeline in

47
00:01:55,119 --> 00:01:59,920
the past two or three years of major

48
00:01:57,600 --> 00:02:02,520
large language model releases by large

49
00:01:59,920 --> 00:02:05,320
AI companies this is a very short list

50
00:02:02,520 --> 00:02:07,039
just from some uh top AI companies but

51
00:02:05,320 --> 00:02:10,160
this is a very crowded field so there's

52
00:02:07,039 --> 00:02:10,160
many more llms

53
00:02:11,599 --> 00:02:16,280
available so what can large language

54
00:02:14,560 --> 00:02:18,680
models do now that there are more of

55
00:02:16,280 --> 00:02:21,120
them available than ever before today's

56
00:02:18,680 --> 00:02:23,720
large language models are able to give

57
00:02:21,120 --> 00:02:26,560
accurate and structured responses to a

58
00:02:23,720 --> 00:02:28,440
very wide range of user queries on the

59
00:02:26,560 --> 00:02:30,519
left hand side here we can see that in

60
00:02:28,440 --> 00:02:33,160
response to my query about how can I

61
00:02:30,519 --> 00:02:36,120
improve my programming skills CH GPT

62
00:02:33,160 --> 00:02:38,319
very quickly uh generates a list that is

63
00:02:36,120 --> 00:02:40,680
detailed and structured uh giving a

64
00:02:38,319 --> 00:02:42,640
couple of steps and a couple of um tips

65
00:02:40,680 --> 00:02:45,519
that I can follow in order to improve my

66
00:02:42,640 --> 00:02:47,680
programming skills this generative

67
00:02:45,519 --> 00:02:50,360
ability of these large language models

68
00:02:47,680 --> 00:02:52,480
makes them very useful for tasks like

69
00:02:50,360 --> 00:02:55,080
writing and planning and summarizing

70
00:02:52,480 --> 00:02:57,280
things and it's amazing when you think

71
00:02:55,080 --> 00:02:59,480
that it only took chat gbt a couple of

72
00:02:57,280 --> 00:03:01,560
seconds to generate this response which

73
00:02:59,480 --> 00:03:04,319
is even cut off here and it would have

74
00:03:01,560 --> 00:03:06,319
taken a human like me at least a couple

75
00:03:04,319 --> 00:03:08,879
of minutes to generate this same kind of

76
00:03:06,319 --> 00:03:11,120
response so with this Advanced

77
00:03:08,879 --> 00:03:13,640
generation capability what can we do

78
00:03:11,120 --> 00:03:16,000
with large language models in recent

79
00:03:13,640 --> 00:03:18,319
years there's been a growing ecosystem

80
00:03:16,000 --> 00:03:20,879
of applications based on large language

81
00:03:18,319 --> 00:03:22,879
models and other generative AI systems

82
00:03:20,879 --> 00:03:25,239
now you can find AI companies offering

83
00:03:22,879 --> 00:03:28,000
services for writing assistance coding

84
00:03:25,239 --> 00:03:30,280
assistance um and other content creation

85
00:03:28,000 --> 00:03:33,480
tools uh a favorite one which came out

86
00:03:30,280 --> 00:03:35,239
recently of M is uh Google's notebook LM

87
00:03:33,480 --> 00:03:37,400
it actually lets you generate podcast

88
00:03:35,239 --> 00:03:40,319
audio given a couple of sources that you

89
00:03:37,400 --> 00:03:43,599
upload so we're still in an exploratory

90
00:03:40,319 --> 00:03:47,480
phase of the best uh interactions and

91
00:03:43,599 --> 00:03:47,480
applications for these AI

92
00:03:49,159 --> 00:03:54,360
systems okay so to begin digging deeper

93
00:03:52,239 --> 00:03:56,040
into large language models we need to

94
00:03:54,360 --> 00:03:58,079
understand the background architecture

95
00:03:56,040 --> 00:04:00,319
behind them which is the Transformer

96
00:03:58,079 --> 00:04:03,159
dating all the way back to 2017

97
00:04:00,319 --> 00:04:05,599
which seems like ages ago at this point

98
00:04:03,159 --> 00:04:08,159
um the Transformer is an architecture

99
00:04:05,599 --> 00:04:10,400
that is based almost entirely around two

100
00:04:08,159 --> 00:04:12,680
kinds of operations that is multi-head

101
00:04:10,400 --> 00:04:14,560
attention and then feed forward dense

102
00:04:12,680 --> 00:04:18,079
layers they are colored as orange and

103
00:04:14,560 --> 00:04:20,239
blue on here attention is a mechanism

104
00:04:18,079 --> 00:04:22,400
that highlights interactions between

105
00:04:20,239 --> 00:04:24,520
different parts of your input and then

106
00:04:22,400 --> 00:04:27,360
feed forward layers are standard neuron

107
00:04:24,520 --> 00:04:30,280
Network layers that add more trainable

108
00:04:27,360 --> 00:04:32,800
capacity to the architecture we can see

109
00:04:30,280 --> 00:04:35,720
here that this Transformer uh decoder

110
00:04:32,800 --> 00:04:38,080
architecture it consists of n blocks

111
00:04:35,720 --> 00:04:40,240
each block containing a tension layer

112
00:04:38,080 --> 00:04:42,080
and a feed forward layer and we stack

113
00:04:40,240 --> 00:04:44,680
these blocks one on top of another to

114
00:04:42,080 --> 00:04:47,520
form a very deep uh

115
00:04:44,680 --> 00:04:49,720
Network now aside from attention and

116
00:04:47,520 --> 00:04:51,440
feed forward layers there's only a

117
00:04:49,720 --> 00:04:53,440
couple of other operations happening

118
00:04:51,440 --> 00:04:55,400
inside of this Transformer there are

119
00:04:53,440 --> 00:04:57,720
normalization layers in yellow which

120
00:04:55,400 --> 00:05:00,440
happen after each operation there are

121
00:04:57,720 --> 00:05:02,960
skip connections which go around each of

122
00:05:00,440 --> 00:05:05,440
these operations these skip connections

123
00:05:02,960 --> 00:05:07,520
help gradients back propagate back

124
00:05:05,440 --> 00:05:09,680
through the network during training this

125
00:05:07,520 --> 00:05:11,880
helps us train these very deep networks

126
00:05:09,680 --> 00:05:14,520
uh stably and then the last thing is

127
00:05:11,880 --> 00:05:16,720
positional encoding positional encoding

128
00:05:14,520 --> 00:05:18,960
is added to the input of the Transformer

129
00:05:16,720 --> 00:05:21,720
and it helps tell the Transformer what

130
00:05:18,960 --> 00:05:24,160
order the input information

131
00:05:21,720 --> 00:05:25,840
came so over the next couple of slides

132
00:05:24,160 --> 00:05:27,639
I'll dive into a couple parts of the

133
00:05:25,840 --> 00:05:31,639
architecture which deserve a bit more

134
00:05:27,639 --> 00:05:31,639
detail uh starting with the input

135
00:05:33,240 --> 00:05:37,080
so we are modeling language with these

136
00:05:35,360 --> 00:05:40,720
models which means that we need to

137
00:05:37,080 --> 00:05:43,479
define a process for turning a raw input

138
00:05:40,720 --> 00:05:45,800
text into a list of numerical vectors

139
00:05:43,479 --> 00:05:47,759
which our model will be able to process

140
00:05:45,800 --> 00:05:50,000
the way that this is typically done with

141
00:05:47,759 --> 00:05:52,160
large language models is called text

142
00:05:50,000 --> 00:05:55,400
tokenization in tokenization we are

143
00:05:52,160 --> 00:05:57,479
going to break apart raw input text into

144
00:05:55,400 --> 00:06:00,080
smaller word pieces which are colored

145
00:05:57,479 --> 00:06:02,039
here these word pieces need to be small

146
00:06:00,080 --> 00:06:04,280
word pieces that the large language

147
00:06:02,039 --> 00:06:05,520
model will recognize and that they can

148
00:06:04,280 --> 00:06:07,479
represent

149
00:06:05,520 --> 00:06:10,919
numerically so every large language

150
00:06:07,479 --> 00:06:14,280
model has an Associated dictionary this

151
00:06:10,919 --> 00:06:17,479
dictionary is around 50,000 to

152
00:06:14,280 --> 00:06:20,039
250,000 word pieces which the large

153
00:06:17,479 --> 00:06:22,120
language model understands and the llm

154
00:06:20,039 --> 00:06:24,400
will learn an embedding table which has

155
00:06:22,120 --> 00:06:28,080
a vector corresponding to each word

156
00:06:24,400 --> 00:06:30,440
piece so if I ask for the word in that

157
00:06:28,080 --> 00:06:33,400
is corresponding to the

158
00:06:30,440 --> 00:06:35,960
818th word embedding in the lln

159
00:06:33,400 --> 00:06:38,199
dictionary so all we need to do is when

160
00:06:35,960 --> 00:06:40,560
we receive input text we need to break

161
00:06:38,199 --> 00:06:42,080
apart this text into word pieces and

162
00:06:40,560 --> 00:06:44,400
these word pieces just need to be inside

163
00:06:42,080 --> 00:06:47,880
of the dictionary and the llm will be

164
00:06:44,400 --> 00:06:50,880
able to represent it now at minimum this

165
00:06:47,880 --> 00:06:53,000
dictionary is going to include uh at

166
00:06:50,880 --> 00:06:55,840
least letters numbers and special

167
00:06:53,000 --> 00:06:57,680
characters so the at least what we can

168
00:06:55,840 --> 00:06:59,960
do is break apart the text into

169
00:06:57,680 --> 00:07:02,400
characters and we will be able to encod

170
00:06:59,960 --> 00:07:05,120
it but um we can do better than that

171
00:07:02,400 --> 00:07:07,160
because the tokens uh will also include

172
00:07:05,120 --> 00:07:09,800
words and so we'll be able to use less

173
00:07:07,160 --> 00:07:12,639
tokens to represent the entire

174
00:07:09,800 --> 00:07:15,199
sentence a typical algorithm that llms

175
00:07:12,639 --> 00:07:17,479
use uh to come up with this dictionary

176
00:07:15,199 --> 00:07:20,000
is called bite pair and coding bite

177
00:07:17,479 --> 00:07:22,479
paent coding is where we look at our raw

178
00:07:20,000 --> 00:07:24,560
text we look at which words and

179
00:07:22,479 --> 00:07:26,319
characters frequently occur next to each

180
00:07:24,560 --> 00:07:29,080
other and then we're just going to

181
00:07:26,319 --> 00:07:30,280
iteratively merge frequently incurring

182
00:07:29,080 --> 00:07:32,039
uh pairs

183
00:07:30,280 --> 00:07:33,720
when we merge a pair that is added to

184
00:07:32,039 --> 00:07:38,759
our dictionary and when we do enough

185
00:07:33,720 --> 00:07:38,759
merges we get up to our 50,000 uh

186
00:07:40,840 --> 00:07:45,599
dictionary okay so now we know about the

187
00:07:43,599 --> 00:07:47,759
input to our large language model now

188
00:07:45,599 --> 00:07:50,479
let's look at the output what comes out

189
00:07:47,759 --> 00:07:52,479
of the llm when we put a sequence of

190
00:07:50,479 --> 00:07:53,599
text into the large language model like

191
00:07:52,479 --> 00:07:56,360
on the bottom

192
00:07:53,599 --> 00:07:59,080
left these this list of vectors is going

193
00:07:56,360 --> 00:08:01,960
to go through our n layer Transformer

194
00:07:59,080 --> 00:08:05,080
and then the last linear layer and soft

195
00:08:01,960 --> 00:08:06,560
Max are going to take the last token of

196
00:08:05,080 --> 00:08:09,400
our input sequence and they're going to

197
00:08:06,560 --> 00:08:12,479
Output a probability distribution this

198
00:08:09,400 --> 00:08:14,639
probability distribution is over the

199
00:08:12,479 --> 00:08:16,840
dictionary or the vocabulary of the

200
00:08:14,639 --> 00:08:20,000
large language model and we're actually

201
00:08:16,840 --> 00:08:22,440
predicting here what is the probability

202
00:08:20,000 --> 00:08:24,520
um that any one dictionary entry is the

203
00:08:22,440 --> 00:08:26,639
next token in the sequence so we're

204
00:08:24,520 --> 00:08:28,840
doing self-supervised learning we know

205
00:08:26,639 --> 00:08:31,120
from the data itself what the next word

206
00:08:28,840 --> 00:08:33,200
is and we're going to have the llm

207
00:08:31,120 --> 00:08:35,479
predict what the next word should be

208
00:08:33,200 --> 00:08:38,360
given the input

209
00:08:35,479 --> 00:08:40,320
sentence um this is called Next token

210
00:08:38,360 --> 00:08:43,440
prediction and we usually optimize this

211
00:08:40,320 --> 00:08:43,440
through cross entropy

212
00:08:44,480 --> 00:08:48,200
loss the last thing that I want to

213
00:08:46,519 --> 00:08:50,200
highlight about the Transformer

214
00:08:48,200 --> 00:08:52,720
architecture is something called causal

215
00:08:50,200 --> 00:08:54,720
attention masking this is a detail about

216
00:08:52,720 --> 00:08:56,839
the attention operation that's happening

217
00:08:54,720 --> 00:08:59,040
inside of the

218
00:08:56,839 --> 00:09:00,920
Transformer so we see on the left hand

219
00:08:59,040 --> 00:09:02,279
side here I mentioned earlier that

220
00:09:00,920 --> 00:09:04,480
attention is an operation that

221
00:09:02,279 --> 00:09:06,360
highlights interactions between parts of

222
00:09:04,480 --> 00:09:08,959
your input if we look at these words

223
00:09:06,360 --> 00:09:11,800
here large language models predict the

224
00:09:08,959 --> 00:09:15,200
the first three words actually refer to

225
00:09:11,800 --> 00:09:17,399
the same object llms and so in a perfect

226
00:09:15,200 --> 00:09:19,440
world we would hope that these three

227
00:09:17,399 --> 00:09:22,040
that the llm would know that these three

228
00:09:19,440 --> 00:09:23,680
words mean the same object they they

229
00:09:22,040 --> 00:09:25,079
should interact more with each other

230
00:09:23,680 --> 00:09:27,839
compared to the rest of the words in the

231
00:09:25,079 --> 00:09:29,480
sentence so this is an example attention

232
00:09:27,839 --> 00:09:31,760
Matrix which we would expect where the

233
00:09:29,480 --> 00:09:33,560
first three words are interacting highly

234
00:09:31,760 --> 00:09:36,360
with each other when we look at

235
00:09:33,560 --> 00:09:38,680
interactions in this sort of uh Matrix

236
00:09:36,360 --> 00:09:40,440
format we call it an attention Matrix

237
00:09:38,680 --> 00:09:42,519
and each of the scores in the Matrix is

238
00:09:40,440 --> 00:09:45,839
an attention coefficient telling us

239
00:09:42,519 --> 00:09:48,200
interaction between tokens now when we

240
00:09:45,839 --> 00:09:50,640
do causal attention what's happening is

241
00:09:48,200 --> 00:09:52,680
that we are creating a mask which is a

242
00:09:50,640 --> 00:09:54,880
lower triangular Matrix and we are

243
00:09:52,680 --> 00:09:57,560
zeroing out the top half of this

244
00:09:54,880 --> 00:09:59,839
attention Matrix the reason why we do

245
00:09:57,560 --> 00:10:01,760
this is because our

246
00:09:59,839 --> 00:10:03,440
language modeling objective is next

247
00:10:01,760 --> 00:10:05,760
token prediction we are predicting the

248
00:10:03,440 --> 00:10:08,440
future and if we want to do this over

249
00:10:05,760 --> 00:10:10,200
the whole sequence we don't want a token

250
00:10:08,440 --> 00:10:11,800
to be able to look at the future

251
00:10:10,200 --> 00:10:13,600
otherwise it would know what token comes

252
00:10:11,800 --> 00:10:15,360
next in order to make it a meaningful

253
00:10:13,600 --> 00:10:18,440
objective we need to hide the future

254
00:10:15,360 --> 00:10:21,200
from each token and we do that by not

255
00:10:18,440 --> 00:10:22,440
allowing any one token in the rows to

256
00:10:21,200 --> 00:10:24,800
look at the

257
00:10:22,440 --> 00:10:27,120
future the really cool thing is that

258
00:10:24,800 --> 00:10:28,560
when we do this causal attention masking

259
00:10:27,120 --> 00:10:30,760
we can actually apply our language

260
00:10:28,560 --> 00:10:32,480
modeling objectives to the entire

261
00:10:30,760 --> 00:10:34,040
sequence so the first three words

262
00:10:32,480 --> 00:10:36,000
predict the fourth the first four

263
00:10:34,040 --> 00:10:38,560
predict the fifth but this is all

264
00:10:36,000 --> 00:10:40,880
happening on one forward pass and so

265
00:10:38,560 --> 00:10:43,360
you're getting multiple subsequences of

266
00:10:40,880 --> 00:10:45,519
training examples by doing one forward

267
00:10:43,360 --> 00:10:47,680
pass this actually makes the trans the

268
00:10:45,519 --> 00:10:50,800
training of these Transformer decoders

269
00:10:47,680 --> 00:10:50,800
very efficient which is

270
00:10:52,480 --> 00:10:56,639
nice now that we've gone through some of

271
00:10:54,720 --> 00:10:58,360
the details about the architecture I'm

272
00:10:56,639 --> 00:11:00,639
going to spend the next couple of slides

273
00:10:58,360 --> 00:11:03,160
talking about how these llms are trained

274
00:11:00,639 --> 00:11:06,600
at a large scale there's a lot of stuff

275
00:11:03,160 --> 00:11:08,560
happening on this um on this figure but

276
00:11:06,600 --> 00:11:11,000
I'm going to go through it part by part

277
00:11:08,560 --> 00:11:12,399
there are four main stages of training

278
00:11:11,000 --> 00:11:14,600
large language models there's

279
00:11:12,399 --> 00:11:16,519
pre-training supervised fine-tuning

280
00:11:14,600 --> 00:11:19,279
there's reward modeling where we start

281
00:11:16,519 --> 00:11:20,839
to uh model human preferences and then

282
00:11:19,279 --> 00:11:23,000
there's reinforcement learning where

283
00:11:20,839 --> 00:11:25,880
we're going to tune the llm and align it

284
00:11:23,000 --> 00:11:28,079
to human preferences each of these four

285
00:11:25,880 --> 00:11:30,480
training stages has an Associated data

286
00:11:28,079 --> 00:11:32,680
set which we collect and train on it has

287
00:11:30,480 --> 00:11:34,360
a modeling objective an algorithm that

288
00:11:32,680 --> 00:11:36,000
we train with and then there's a

289
00:11:34,360 --> 00:11:38,600
resulting model which will help

290
00:11:36,000 --> 00:11:40,560
initialize the further training stages

291
00:11:38,600 --> 00:11:43,560
I'll start with

292
00:11:40,560 --> 00:11:45,920
pre-training the pre-training stage uh

293
00:11:43,560 --> 00:11:48,360
is the stage of training which takes the

294
00:11:45,920 --> 00:11:50,200
majority of the compute and time in

295
00:11:48,360 --> 00:11:53,360
large language model development in

296
00:11:50,200 --> 00:11:55,680
pre-training we are our data set is the

297
00:11:53,360 --> 00:11:57,480
raw um the web scrape of the raw

298
00:11:55,680 --> 00:11:59,360
internet so we're the scale which we're

299
00:11:57,480 --> 00:12:01,320
talking about here is trillions of

300
00:11:59,360 --> 00:12:03,800
tokens and words gathered from the

301
00:12:01,320 --> 00:12:06,480
internet so it's a very large data set

302
00:12:03,800 --> 00:12:08,240
uh High quantity low quality because we

303
00:12:06,480 --> 00:12:11,160
cannot F we can only filter the internet

304
00:12:08,240 --> 00:12:13,360
so much we're doing our language

305
00:12:11,160 --> 00:12:17,199
modeling objective next token prediction

306
00:12:13,360 --> 00:12:19,639
over the internet and on the slide here

307
00:12:17,199 --> 00:12:21,519
I have an example of the Text corpus

308
00:12:19,639 --> 00:12:24,880
which was used to train

309
00:12:21,519 --> 00:12:27,040
gpt3 um common crawl is the web scraped

310
00:12:24,880 --> 00:12:29,560
data set which they use we can see here

311
00:12:27,040 --> 00:12:32,040
though that open AI actually included a

312
00:12:29,560 --> 00:12:34,079
couple of other sources of textual data

313
00:12:32,040 --> 00:12:36,079
the intuition here is that you want to

314
00:12:34,079 --> 00:12:39,360
have a large amount of training data but

315
00:12:36,079 --> 00:12:42,000
including some higher quality sources of

316
00:12:39,360 --> 00:12:44,639
information like Wikipedia articles or

317
00:12:42,000 --> 00:12:47,760
some textbook data sets those are higher

318
00:12:44,639 --> 00:12:49,000
quality textual data than random Json

319
00:12:47,760 --> 00:12:51,440
queries which you might find on the

320
00:12:49,000 --> 00:12:53,880
internet so some companies will actually

321
00:12:51,440 --> 00:12:56,920
upwe these smaller but higher quality

322
00:12:53,880 --> 00:12:59,240
sources of textual data during

323
00:12:56,920 --> 00:13:01,639
pre-training now the scale of

324
00:12:59,240 --> 00:13:03,639
pre-training is massive we have a very

325
00:13:01,639 --> 00:13:06,360
large data set and we need thousands of

326
00:13:03,639 --> 00:13:08,839
gpus over several months of training to

327
00:13:06,360 --> 00:13:10,560
actually train in this phase so only

328
00:13:08,839 --> 00:13:12,920
like large AI companies with a lot of

329
00:13:10,560 --> 00:13:14,639
backing can kind of attempt to this uh

330
00:13:12,920 --> 00:13:16,440
stage of

331
00:13:14,639 --> 00:13:18,839
training yeah do you know what the

332
00:13:16,440 --> 00:13:21,760
source of Comm and crawl is is it the

333
00:13:18,839 --> 00:13:25,120
internet yes it's the internet I believe

334
00:13:21,760 --> 00:13:26,839
there's a company which performed common

335
00:13:25,120 --> 00:13:28,880
crawl and they keep adding more textual

336
00:13:26,839 --> 00:13:32,160
data sets so it's an open- Source data

337
00:13:28,880 --> 00:13:33,880
set like I think we could download it

338
00:13:32,160 --> 00:13:37,399
wouldn't that already include

339
00:13:33,880 --> 00:13:39,760
Wikipedia it would but if they want to

340
00:13:37,399 --> 00:13:42,440
specifically process Wikipedia filter it

341
00:13:39,760 --> 00:13:43,880
a bit more and upway it's um like how

342
00:13:42,440 --> 00:13:45,959
prevalent it is in the data set they

343
00:13:43,880 --> 00:13:48,519
might have it separately yeah okay but

344
00:13:45,959 --> 00:13:50,440
um it brings up a good point that common

345
00:13:48,519 --> 00:13:52,560
crawl will include a lot of data sets

346
00:13:50,440 --> 00:13:54,360
that are on the internet so like things

347
00:13:52,560 --> 00:13:56,360
like you know test set contamination and

348
00:13:54,360 --> 00:14:00,320
data leakage is a big problem with LMS

349
00:13:56,360 --> 00:14:00,320
because it sees so much on the internet

350
00:14:01,279 --> 00:14:05,639
okay in the next stage which is

351
00:14:03,160 --> 00:14:09,320
supervised fine-tuning our data set is

352
00:14:05,639 --> 00:14:10,959
going to become uh lower quantity but

353
00:14:09,320 --> 00:14:12,639
higher quality we're going to shift from

354
00:14:10,959 --> 00:14:15,120
training on the entire internet and

355
00:14:12,639 --> 00:14:18,680
we're actually going to um if I'm a

356
00:14:15,120 --> 00:14:20,600
company I will hire a group of labelers

357
00:14:18,680 --> 00:14:23,920
I'll give them a set of prompts and I'll

358
00:14:20,600 --> 00:14:26,600
have those um labelers generate ideal

359
00:14:23,920 --> 00:14:29,240
responses to these prompts we want to

360
00:14:26,600 --> 00:14:31,600
supervise fine-tune this llm to start

361
00:14:29,240 --> 00:14:34,399
being better at answering questions the

362
00:14:31,600 --> 00:14:36,040
base model it's um it's like a

363
00:14:34,399 --> 00:14:37,839
autocomplete engine that's good at

364
00:14:36,040 --> 00:14:40,440
completing text but it's not good at

365
00:14:37,839 --> 00:14:42,519
conversation it now in supervised fine

366
00:14:40,440 --> 00:14:43,720
tuning we are going to actually train it

367
00:14:42,519 --> 00:14:48,160
with the same language modeling

368
00:14:43,720 --> 00:14:51,040
objective to um give better responses to

369
00:14:48,160 --> 00:14:52,959
input prompts this stage takes a couple

370
00:14:51,040 --> 00:14:56,639
of hundreds of gpus and a few days of

371
00:14:52,959 --> 00:14:59,600
training so this is sort of um a smaller

372
00:14:56,639 --> 00:15:01,959
fine-tuning stage and the result model

373
00:14:59,600 --> 00:15:04,920
is called the supervised fine tune model

374
00:15:01,959 --> 00:15:06,600
sft this model is Deployable and if you

375
00:15:04,920 --> 00:15:08,320
ask it questions it will give you some

376
00:15:06,600 --> 00:15:10,839
good responses because we're tuning it

377
00:15:08,320 --> 00:15:12,680
to be conversational now but it's not

378
00:15:10,839 --> 00:15:14,880
the best performing kind of large

379
00:15:12,680 --> 00:15:16,360
language models the best llm models are

380
00:15:14,880 --> 00:15:18,600
the ones that have gone through

381
00:15:16,360 --> 00:15:20,519
reinforcement learning which is the next

382
00:15:18,600 --> 00:15:22,680
two stages of

383
00:15:20,519 --> 00:15:25,560
training moving on to reinforcement

384
00:15:22,680 --> 00:15:28,120
learning this the next two stages is

385
00:15:25,560 --> 00:15:31,440
where we're going to align llns to

386
00:15:28,120 --> 00:15:33,519
generate more human preferred responses

387
00:15:31,440 --> 00:15:35,920
now that's a very tall task like how do

388
00:15:33,519 --> 00:15:39,319
we model what responses humans are going

389
00:15:35,920 --> 00:15:41,000
to like um what we're going to do is in

390
00:15:39,319 --> 00:15:43,639
the reward modeling stage which is the

391
00:15:41,000 --> 00:15:46,199
third stage we're going to shift our

392
00:15:43,639 --> 00:15:48,639
data set collection Focus from

393
00:15:46,199 --> 00:15:51,279
generating ideal responses to actually

394
00:15:48,639 --> 00:15:53,759
ranking existing responses we will have

395
00:15:51,279 --> 00:15:56,680
our llm generate a lot of example

396
00:15:53,759 --> 00:15:59,480
completions or responses to prompts and

397
00:15:56,680 --> 00:16:02,160
we're actually going to start collect in

398
00:15:59,480 --> 00:16:03,600
ranking information if you used Chad GPT

399
00:16:02,160 --> 00:16:04,839
before you might have actually done this

400
00:16:03,600 --> 00:16:06,600
where it gives you two or three

401
00:16:04,839 --> 00:16:08,920
completions and you select the best one

402
00:16:06,600 --> 00:16:10,880
whichever one you prefer when you do

403
00:16:08,920 --> 00:16:12,680
that you're actually providing future

404
00:16:10,880 --> 00:16:15,759
training data for them to train future

405
00:16:12,680 --> 00:16:18,360
reward models what we're going to do is

406
00:16:15,759 --> 00:16:21,079
out of let's say 10 responses to a given

407
00:16:18,360 --> 00:16:23,000
prompt uh they will have professional

408
00:16:21,079 --> 00:16:25,079
labelers rank them in order of

409
00:16:23,000 --> 00:16:27,880
preference so we're Gathering ranking

410
00:16:25,079 --> 00:16:30,720
data which is a a ground Truth For What

411
00:16:27,880 --> 00:16:32,480
kind of responses do humans prefer more

412
00:16:30,720 --> 00:16:34,440
maybe one describes the data better

413
00:16:32,480 --> 00:16:37,240
maybe one had the code commented more

414
00:16:34,440 --> 00:16:37,240
nicely and so

415
00:16:37,839 --> 00:16:42,120
on and what we're going to do in this

416
00:16:40,079 --> 00:16:45,079
reward modeling stage is we are going to

417
00:16:42,120 --> 00:16:47,639
train a reward model this reward model

418
00:16:45,079 --> 00:16:49,560
can be a smaller language model with the

419
00:16:47,639 --> 00:16:52,240
same architecture or it could be another

420
00:16:49,560 --> 00:16:55,600
system all it the only constraint is

421
00:16:52,240 --> 00:16:58,120
that the reward model needs to Output a

422
00:16:55,600 --> 00:16:59,680
score a reward for how much humans are

423
00:16:58,120 --> 00:17:02,079
going to like this

424
00:16:59,680 --> 00:17:05,319
response the way that we train this is

425
00:17:02,079 --> 00:17:07,079
if we will'll assemble an input prompt

426
00:17:05,319 --> 00:17:08,480
into our reward model we're going to lay

427
00:17:07,079 --> 00:17:10,600
out the same prompt and we're going to

428
00:17:08,480 --> 00:17:13,799
put different responses these candidate

429
00:17:10,600 --> 00:17:17,039
responses uh after it we will append a

430
00:17:13,799 --> 00:17:19,120
reward token at the end of the input and

431
00:17:17,039 --> 00:17:20,959
what the reward model is going to do is

432
00:17:19,120 --> 00:17:23,880
it's going to take that reward token and

433
00:17:20,959 --> 00:17:27,000
its job is to predict a scaler value a

434
00:17:23,880 --> 00:17:29,600
score a reward for that response for how

435
00:17:27,000 --> 00:17:32,200
good it is and our loss function is

436
00:17:29,600 --> 00:17:35,200
actually just binary cross entropy for

437
00:17:32,200 --> 00:17:37,080
the preferred response reward minus the

438
00:17:35,200 --> 00:17:39,120
dispreferred response reward we're

439
00:17:37,080 --> 00:17:41,840
actually just maximizing the difference

440
00:17:39,120 --> 00:17:43,880
in score between a good and a bad

441
00:17:41,840 --> 00:17:45,919
response the really cool thing about

442
00:17:43,880 --> 00:17:47,840
this process is that we're basically

443
00:17:45,919 --> 00:17:50,559
training this reward model to give

444
00:17:47,840 --> 00:17:53,919
scores for responses but we're training

445
00:17:50,559 --> 00:17:56,880
it to tell which responses humans are

446
00:17:53,919 --> 00:17:58,559
going to like more when we have a reward

447
00:17:56,880 --> 00:18:00,600
model that can actually model human

448
00:17:58,559 --> 00:18:03,240
prefer references then you can optimize

449
00:18:00,600 --> 00:18:05,559
your llm against it to then get more

450
00:18:03,240 --> 00:18:08,080
human preferred outputs so this this

451
00:18:05,559 --> 00:18:09,960
reward model is really telling um is

452
00:18:08,080 --> 00:18:12,200
really modeling human preferences in

453
00:18:09,960 --> 00:18:12,200
this

454
00:18:13,520 --> 00:18:18,360
pipeline okay now the fourth stage is

455
00:18:16,840 --> 00:18:20,280
reinforcement learning from Human

456
00:18:18,360 --> 00:18:23,120
feedback you might have heard this as

457
00:18:20,280 --> 00:18:25,000
rlf what we're going to do is we're

458
00:18:23,120 --> 00:18:27,000
going to use a reinforcement learning

459
00:18:25,000 --> 00:18:29,640
algorithm to Now update the weights of

460
00:18:27,000 --> 00:18:32,640
our llm and we're going to intuitively

461
00:18:29,640 --> 00:18:34,760
try to get the llm to generate responses

462
00:18:32,640 --> 00:18:37,799
that humans like more this is where the

463
00:18:34,760 --> 00:18:38,840
llm start to be very good at generating

464
00:18:37,799 --> 00:18:42,760
good

465
00:18:38,840 --> 00:18:44,559
responses now in reinforcement learning

466
00:18:42,760 --> 00:18:47,400
usually the terminology we use is we're

467
00:18:44,559 --> 00:18:49,720
trying to optimize a policy to maximize

468
00:18:47,400 --> 00:18:52,120
some sort of reward the reward model we

469
00:18:49,720 --> 00:18:54,120
have because we had trained our reward

470
00:18:52,120 --> 00:18:56,360
model in the previous step and the

471
00:18:54,120 --> 00:18:59,360
policy is actually just our large

472
00:18:56,360 --> 00:19:01,039
language model that's generating text so

473
00:18:59,360 --> 00:19:04,360
the action we're taking is generating

474
00:19:01,039 --> 00:19:06,120
text the space of all generation is

475
00:19:04,360 --> 00:19:08,919
actually just the llm vocabulary that's

476
00:19:06,120 --> 00:19:11,280
our possible actions we can take and all

477
00:19:08,919 --> 00:19:13,559
we're going to do is we are going to uh

478
00:19:11,280 --> 00:19:16,600
train our llm to generate better

479
00:19:13,559 --> 00:19:19,000
responses according to the reward model

480
00:19:16,600 --> 00:19:21,159
now specifically what's happening is

481
00:19:19,000 --> 00:19:23,440
that we are using a reinforcement

482
00:19:21,159 --> 00:19:26,760
learning algorithm typically this is

483
00:19:23,440 --> 00:19:29,440
proximal policy optimization this PO

484
00:19:26,760 --> 00:19:31,240
algorithm is going to update the weights

485
00:19:29,440 --> 00:19:34,799
of our large language model and it's

486
00:19:31,240 --> 00:19:37,360
going to try and boost the

487
00:19:34,799 --> 00:19:39,720
probabilities of next token prediction

488
00:19:37,360 --> 00:19:41,600
of generating good responses so the

489
00:19:39,720 --> 00:19:44,400
first prompt and response here in the

490
00:19:41,600 --> 00:19:46,320
top row it was assigned a reward of 1.1

491
00:19:44,400 --> 00:19:49,280
this is pretty high relative to the rest

492
00:19:46,320 --> 00:19:51,200
of the batch since this reward is high

493
00:19:49,280 --> 00:19:53,159
Poo is going to update the weights so

494
00:19:51,200 --> 00:19:54,679
that the First Response the

495
00:19:53,159 --> 00:19:57,559
probabilities of generating that

496
00:19:54,679 --> 00:19:59,799
response are increased and it'll downway

497
00:19:57,559 --> 00:20:02,520
the other responses that have less

498
00:19:59,799 --> 00:20:04,840
reward so we're literally training the

499
00:20:02,520 --> 00:20:06,760
llm to boost the probabilities of

500
00:20:04,840 --> 00:20:08,520
generating human preferred responses

501
00:20:06,760 --> 00:20:10,880
according to the reward model from the

502
00:20:08,520 --> 00:20:10,880
previous

503
00:20:12,440 --> 00:20:17,440
stud so the last two stages with reward

504
00:20:15,919 --> 00:20:18,840
modeling reinforcement learning were a

505
00:20:17,440 --> 00:20:20,320
little bit more complicated than the

506
00:20:18,840 --> 00:20:22,159
first two steps and so we have a

507
00:20:20,320 --> 00:20:25,559
question on why do we do this

508
00:20:22,159 --> 00:20:27,679
reinforcement learning now in practice

509
00:20:25,559 --> 00:20:29,039
what papers have found is that

510
00:20:27,679 --> 00:20:30,520
reinforcement learning model models they

511
00:20:29,039 --> 00:20:31,960
tend to generate better responses

512
00:20:30,520 --> 00:20:34,080
they're more aligned to the kind of

513
00:20:31,960 --> 00:20:36,039
responses that humans like the figure on

514
00:20:34,080 --> 00:20:39,120
the right hand side is from an open AI

515
00:20:36,039 --> 00:20:41,240
paper in 2022 and they basically showed

516
00:20:39,120 --> 00:20:44,039
that the top two models in yellow and

517
00:20:41,240 --> 00:20:45,840
red uh generate more preferred outputs

518
00:20:44,039 --> 00:20:48,000
compared to supervised fun model in

519
00:20:45,840 --> 00:20:50,600
green and that one in turn is better

520
00:20:48,000 --> 00:20:53,559
than the base model so in practice this

521
00:20:50,600 --> 00:20:55,640
seems to work better and an analogy

522
00:20:53,559 --> 00:20:57,679
which I kind of like to think about here

523
00:20:55,640 --> 00:21:00,000
is that when we're doing pre-training

524
00:20:57,679 --> 00:21:02,200
and supervised f tuning it's sort of

525
00:21:00,000 --> 00:21:04,799
like a student studying from textbooks

526
00:21:02,200 --> 00:21:07,080
you're learning by looking and trying to

527
00:21:04,799 --> 00:21:08,720
study textbooks when you switch to

528
00:21:07,080 --> 00:21:10,640
reinforcement learning that is like when

529
00:21:08,720 --> 00:21:11,919
you go to a teacher and they're grading

530
00:21:10,640 --> 00:21:13,440
your essay or they're giving you

531
00:21:11,919 --> 00:21:16,440
feedback you're getting some

532
00:21:13,440 --> 00:21:18,320
discriminative input from a teacher

533
00:21:16,440 --> 00:21:20,840
that's helping you improve your output

534
00:21:18,320 --> 00:21:22,360
you're modeling to their preferences so

535
00:21:20,840 --> 00:21:24,960
that's sort of like an analogy for

536
00:21:22,360 --> 00:21:26,919
reinforcement learning another sort of

537
00:21:24,960 --> 00:21:29,080
way to think about this is that if

538
00:21:26,919 --> 00:21:31,760
you're trying to generate data yourself

539
00:21:29,080 --> 00:21:34,360
for these stages of llm training

540
00:21:31,760 --> 00:21:36,320
sometimes it's easier to rank between

541
00:21:34,360 --> 00:21:38,320
different responses to generate ranking

542
00:21:36,320 --> 00:21:41,320
data rather than trying to generate

543
00:21:38,320 --> 00:21:43,159
perfect ideal responses to train the llm

544
00:21:41,320 --> 00:21:47,159
so the data collection may be a little

545
00:21:43,159 --> 00:21:47,159
bit easier with um ranking

546
00:21:49,279 --> 00:21:54,440
data okay now that we've gone through

547
00:21:51,679 --> 00:21:55,880
some ideas for uh developing large

548
00:21:54,440 --> 00:21:57,919
language models I'm going to spend the

549
00:21:55,880 --> 00:22:00,520
next couple of slides going through some

550
00:21:57,919 --> 00:22:02,919
ideas for for how llms how do we make

551
00:22:00,520 --> 00:22:06,240
llms uh generate better outputs how do

552
00:22:02,919 --> 00:22:06,240
we make them more efficient more

553
00:22:06,360 --> 00:22:10,679
useful the first major idea I want to

554
00:22:08,520 --> 00:22:12,880
talk about is scaling laws for large

555
00:22:10,679 --> 00:22:14,400
language models this is a big idea we've

556
00:22:12,880 --> 00:22:15,919
developed this large framework for a

557
00:22:14,400 --> 00:22:18,559
training and developing large langage

558
00:22:15,919 --> 00:22:20,919
models now how does this scale how does

559
00:22:18,559 --> 00:22:23,679
performance improve as we throw more

560
00:22:20,919 --> 00:22:25,000
data and compute at it what this study

561
00:22:23,679 --> 00:22:26,919
found is that if you look at the

562
00:22:25,000 --> 00:22:29,039
performance of large language models

563
00:22:26,919 --> 00:22:32,440
which is cross entropy loss on the y-

564
00:22:29,039 --> 00:22:34,559
axis here and you put either compute

565
00:22:32,440 --> 00:22:37,360
data set size or the number of model

566
00:22:34,559 --> 00:22:38,840
parameters on the x-axis as you increase

567
00:22:37,360 --> 00:22:41,039
these things there's actually a very

568
00:22:38,840 --> 00:22:42,880
predictable increase in the performance

569
00:22:41,039 --> 00:22:44,200
of large language models the really

570
00:22:42,880 --> 00:22:46,080
significant thing that they found was

571
00:22:44,200 --> 00:22:48,360
that you can actually fit equations and

572
00:22:46,080 --> 00:22:50,799
really predict how the performance is

573
00:22:48,360 --> 00:22:53,480
going to improve if you train with more

574
00:22:50,799 --> 00:22:55,240
compute or if you give it more data now

575
00:22:53,480 --> 00:22:58,600
some of the general takeaways that they

576
00:22:55,240 --> 00:23:00,919
found was that the relationship between

577
00:22:58,600 --> 00:23:03,720
performance and compute was a power law

578
00:23:00,919 --> 00:23:06,760
as they did more training runs with more

579
00:23:03,720 --> 00:23:09,360
and more um higher capacity models the

580
00:23:06,760 --> 00:23:11,440
loss was uh increasing along this sort

581
00:23:09,360 --> 00:23:13,080
of power law curve and then the other

582
00:23:11,440 --> 00:23:14,919
thing which they found is that there's a

583
00:23:13,080 --> 00:23:17,120
relationship between the amount of data

584
00:23:14,919 --> 00:23:19,279
that you train on and then the number of

585
00:23:17,120 --> 00:23:21,240
parameters of the model basically when

586
00:23:19,279 --> 00:23:23,440
you increase your data set size by a

587
00:23:21,240 --> 00:23:24,919
certain amount you should also increase

588
00:23:23,440 --> 00:23:26,799
the number of parameters you're training

589
00:23:24,919 --> 00:23:30,440
with so that your performance is not

590
00:23:26,799 --> 00:23:30,440
bottlenecked by data or model

591
00:23:32,480 --> 00:23:37,640
capacity the next really big idea is

592
00:23:34,880 --> 00:23:39,720
about Chain of Thought reasoning steps

593
00:23:37,640 --> 00:23:41,880
so these large language models they have

594
00:23:39,720 --> 00:23:43,640
very strong generation capabilities but

595
00:23:41,880 --> 00:23:47,080
if we think for a moment about how

596
00:23:43,640 --> 00:23:49,600
humans how we generate output when I am

597
00:23:47,080 --> 00:23:53,440
sort of generating output either speech

598
00:23:49,600 --> 00:23:55,760
or written text I'm at the same time as

599
00:23:53,440 --> 00:23:57,880
I'm generating I'm also reasoning about

600
00:23:55,760 --> 00:23:59,360
what I've just said or written I can

601
00:23:57,880 --> 00:24:01,240
backtrack I can go back and correct

602
00:23:59,360 --> 00:24:02,919
myself or I can think about what I've

603
00:24:01,240 --> 00:24:05,480
said and so I can have some sort of

604
00:24:02,919 --> 00:24:07,520
intermediate reasoning process but if

605
00:24:05,480 --> 00:24:10,039
we're just giving large language models

606
00:24:07,520 --> 00:24:11,960
input and output like responses it

607
00:24:10,039 --> 00:24:14,159
doesn't get the chance to do that

608
00:24:11,960 --> 00:24:17,000
reasoning process so the idea about

609
00:24:14,159 --> 00:24:18,960
chain of thoughts is we ask the llms or

610
00:24:17,000 --> 00:24:21,080
we encourage them to generate

611
00:24:18,960 --> 00:24:23,880
intermediate outputs this is especially

612
00:24:21,080 --> 00:24:26,840
apparent on mathematical questions where

613
00:24:23,880 --> 00:24:29,399
if we ask the model some math questions

614
00:24:26,840 --> 00:24:31,919
the performance

615
00:24:29,399 --> 00:24:33,640
uh steadily improves if we have the

616
00:24:31,919 --> 00:24:36,120
model output some intermediate

617
00:24:33,640 --> 00:24:37,880
mathematical steps before generating its

618
00:24:36,120 --> 00:24:40,080
final response this was shown pretty

619
00:24:37,880 --> 00:24:40,919
robustly on mathematical and reasoning

620
00:24:40,080 --> 00:24:44,000
data

621
00:24:40,919 --> 00:24:45,720
sets now this first paper which

622
00:24:44,000 --> 00:24:47,799
generated intermediate reasoning steps

623
00:24:45,720 --> 00:24:50,640
was called chain of thoughts this is not

624
00:24:47,799 --> 00:24:53,240
the only way to do chain of thoughts or

625
00:24:50,640 --> 00:24:55,039
re intermediate reasoning processes um

626
00:24:53,240 --> 00:24:57,440
on the right hand side is just a

627
00:24:55,039 --> 00:24:58,880
intuition for we can actually generate

628
00:24:57,440 --> 00:25:01,159
multiple chains

629
00:24:58,880 --> 00:25:02,960
of reasoning processes and we can

630
00:25:01,159 --> 00:25:04,440
backtrack and have the llm go through

631
00:25:02,960 --> 00:25:07,240
different reasoning paths and pick the

632
00:25:04,440 --> 00:25:09,440
best one so this is sort of forms a tree

633
00:25:07,240 --> 00:25:11,960
of thoughts which was another idea

634
00:25:09,440 --> 00:25:14,880
recently and this performs better than

635
00:25:11,960 --> 00:25:18,440
just um no intermediate reasoning

636
00:25:14,880 --> 00:25:20,159
steps is is Tri of f basically what

637
00:25:18,440 --> 00:25:22,919
theyve released with o1 or is that

638
00:25:20,159 --> 00:25:24,919
something else yes it is it is okay so

639
00:25:22,919 --> 00:25:26,919
uh very recently open AI came out with

640
00:25:24,919 --> 00:25:29,159
the 01 models and we don't know the

641
00:25:26,919 --> 00:25:30,919
exact details about how they are

642
00:25:29,159 --> 00:25:32,799
implementing chain of thoughts they you

643
00:25:30,919 --> 00:25:34,720
know keep it within their team but they

644
00:25:32,799 --> 00:25:36,720
are doing some sort of intermediate

645
00:25:34,720 --> 00:25:38,840
reasoning steps and that's why if you

646
00:25:36,720 --> 00:25:41,159
ask 01 a question like I did here how

647
00:25:38,840 --> 00:25:42,600
many houses can we build on Mars it's

648
00:25:41,159 --> 00:25:43,960
there are some intermediate steps that

649
00:25:42,600 --> 00:25:45,880
are very helpful to know what's the

650
00:25:43,960 --> 00:25:47,480
surface area of Mars how many you know

651
00:25:45,880 --> 00:25:51,200
how much space does a usual house take

652
00:25:47,480 --> 00:25:53,520
up and this 01 model it'll spend like

653
00:25:51,200 --> 00:25:55,520
you know 20 30 seconds thinking about it

654
00:25:53,520 --> 00:25:57,480
and you actually have the option to

655
00:25:55,520 --> 00:26:00,039
expand some of the intermediate steps

656
00:25:57,480 --> 00:26:01,760
that 01 thought about before it gave you

657
00:26:00,039 --> 00:26:05,000
the final answer so it was calculating a

658
00:26:01,760 --> 00:26:06,760
couple of these things so oh so we don't

659
00:26:05,000 --> 00:26:08,080
know how the tree of thought works but

660
00:26:06,760 --> 00:26:10,159
for the Chain of Thought like the

661
00:26:08,080 --> 00:26:12,480
earlier version of this idea are they

662
00:26:10,159 --> 00:26:15,600
basically is like part of the model

663
00:26:12,480 --> 00:26:18,840
basically breaking down the prompts into

664
00:26:15,600 --> 00:26:21,600
other prompts sorry other questions and

665
00:26:18,840 --> 00:26:24,440
then generating results for that or do

666
00:26:21,600 --> 00:26:26,440
you know how that works yes so in some

667
00:26:24,440 --> 00:26:28,640
problems like mathematical questions

668
00:26:26,440 --> 00:26:30,520
like for some data sets we do have the

669
00:26:28,640 --> 00:26:32,880
intermediate States and we can really

670
00:26:30,520 --> 00:26:34,559
train the llm that the intermediate stat

671
00:26:32,880 --> 00:26:37,440
should be this and we can train it using

672
00:26:34,559 --> 00:26:39,760
our language modeling objective um we

673
00:26:37,440 --> 00:26:42,919
don't know too much about how like the

674
00:26:39,760 --> 00:26:45,520
01 models do it but we do know that they

675
00:26:42,919 --> 00:26:46,919
use reinforcement learning to do that so

676
00:26:45,520 --> 00:26:49,919
they either do some sort of reward

677
00:26:46,919 --> 00:26:52,399
modeling or reinforcement learning on

678
00:26:49,919 --> 00:26:55,320
example outputs where humans either

679
00:26:52,399 --> 00:26:56,880
humans or well that we've generated the

680
00:26:55,320 --> 00:26:59,039
intermediate reasoning steps to

681
00:26:56,880 --> 00:27:00,559
encourage the model to put out

682
00:26:59,039 --> 00:27:02,600
intermediate reasoning steps before it

683
00:27:00,559 --> 00:27:04,640
generates the final answer so they

684
00:27:02,600 --> 00:27:06,000
likely gathered some data where they put

685
00:27:04,640 --> 00:27:08,120
out a lot of intermediate reasoning

686
00:27:06,000 --> 00:27:10,640
steps and trained it using reinforcement

687
00:27:08,120 --> 00:27:10,640
learning for

688
00:27:12,840 --> 00:27:18,080
that okay some other major

689
00:27:16,159 --> 00:27:20,840
ideas

690
00:27:18,080 --> 00:27:22,520
oh I had a question about um there was

691
00:27:20,840 --> 00:27:25,720
like a recent study where they were

692
00:27:22,520 --> 00:27:28,240
taking these um kind of math questions

693
00:27:25,720 --> 00:27:30,159
and then they showed that if you change

694
00:27:28,240 --> 00:27:32,240
the names of the people in in the

695
00:27:30,159 --> 00:27:34,880
question or you just change the actual

696
00:27:32,240 --> 00:27:37,200
numbers that accuracy just starts to

697
00:27:34,880 --> 00:27:41,399
drop somewhere between a little and a

698
00:27:37,200 --> 00:27:42,720
lot so could you comment on if you're

699
00:27:41,399 --> 00:27:44,880
familiar with that if you could comment

700
00:27:42,720 --> 00:27:47,399
on that or what do you think would be

701
00:27:44,880 --> 00:27:49,399
like the analogous issue that we would

702
00:27:47,399 --> 00:27:51,799
have when translating these to um

703
00:27:49,399 --> 00:27:53,799
biological sequence data type things as

704
00:27:51,799 --> 00:27:57,000
well like where are What are the similar

705
00:27:53,799 --> 00:27:59,120
brittleness that you would expect given

706
00:27:57,000 --> 00:28:01,720
that type of behavior thank you wow

707
00:27:59,120 --> 00:28:04,240
that's a that's a very good question um

708
00:28:01,720 --> 00:28:06,640
if the performance of a large language

709
00:28:04,240 --> 00:28:08,799
model dropped very significantly when

710
00:28:06,640 --> 00:28:11,440
you just swap the name or swap out the

711
00:28:08,799 --> 00:28:13,440
numbers then it might be an indication

712
00:28:11,440 --> 00:28:14,840
if it dropped if the drop was very

713
00:28:13,440 --> 00:28:16,799
drastic it might be an indication the

714
00:28:14,840 --> 00:28:18,600
llm saw that data set before on the

715
00:28:16,799 --> 00:28:20,720
Internet it's recalling rather than

716
00:28:18,600 --> 00:28:23,200
generalizing to the reasoning steps and

717
00:28:20,720 --> 00:28:24,760
so maybe when you throw it out of what

718
00:28:23,200 --> 00:28:28,120
it's already seen on the internet then

719
00:28:24,760 --> 00:28:30,600
it sort of collapses although if the

720
00:28:28,120 --> 00:28:33,320
performance is like not too drastic of a

721
00:28:30,600 --> 00:28:35,480
drop then maybe it's just like um maybe

722
00:28:33,320 --> 00:28:38,039
if you gave the llm more generation

723
00:28:35,480 --> 00:28:40,840
tries it will still do okay on the data

724
00:28:38,039 --> 00:28:43,240
set the analogous problem in biology I

725
00:28:40,840 --> 00:28:45,399
think it might be where the large

726
00:28:43,240 --> 00:28:47,399
language model has seen a lot of

727
00:28:45,399 --> 00:28:49,559
biological Concepts on the internet

728
00:28:47,399 --> 00:28:51,320
maybe it saw certain molecules or drug

729
00:28:49,559 --> 00:28:53,840
names very commonly represented in

730
00:28:51,320 --> 00:28:55,360
online databases when you ask it

731
00:28:53,840 --> 00:28:57,559
questions or you try to do Downstream

732
00:28:55,360 --> 00:28:59,440
tasks with those familiar molecules or

733
00:28:57,559 --> 00:29:01,640
biological compounds it might do very

734
00:28:59,440 --> 00:29:03,399
good and then when you start to use very

735
00:29:01,640 --> 00:29:05,519
rare data or something that's not seen

736
00:29:03,399 --> 00:29:06,960
commonly on the internet then the llm

737
00:29:05,519 --> 00:29:08,480
might be thrown out of what it's

738
00:29:06,960 --> 00:29:10,880
familiar with and then it might see a

739
00:29:08,480 --> 00:29:13,279
similar drop in performance then you

740
00:29:10,880 --> 00:29:16,760
might need more data on that rare kind

741
00:29:13,279 --> 00:29:19,120
of um biological like a sample to make

742
00:29:16,760 --> 00:29:19,120
up for

743
00:29:20,519 --> 00:29:25,799
that

744
00:29:22,880 --> 00:29:27,600
okay so another major idea in large

745
00:29:25,799 --> 00:29:30,640
language models is really important is

746
00:29:27,600 --> 00:29:32,440
long context modeling so we saw before

747
00:29:30,640 --> 00:29:34,760
that we are putting in a sequence of

748
00:29:32,440 --> 00:29:36,279
texts into our large language model and

749
00:29:34,760 --> 00:29:39,519
we are embedding it into some word

750
00:29:36,279 --> 00:29:42,240
vectors now we add positional encoding

751
00:29:39,519 --> 00:29:45,080
to this um to these words and that tells

752
00:29:42,240 --> 00:29:47,679
us the length of our input large

753
00:29:45,080 --> 00:29:49,840
language models have a maximum length a

754
00:29:47,679 --> 00:29:51,600
maximum number of word pieces that they

755
00:29:49,840 --> 00:29:53,840
can accept as input and that's called

756
00:29:51,600 --> 00:29:55,960
the context length this context length

757
00:29:53,840 --> 00:29:58,200
is one of the really big limitations of

758
00:29:55,960 --> 00:30:00,679
large language models because the longer

759
00:29:58,200 --> 00:30:02,919
your context L the more information you

760
00:30:00,679 --> 00:30:04,600
can put into the input prompt you can

761
00:30:02,919 --> 00:30:06,960
imagine if you're doing medical question

762
00:30:04,600 --> 00:30:09,640
answering it helps a lot if you put

763
00:30:06,960 --> 00:30:12,000
relevant medical or biological text in

764
00:30:09,640 --> 00:30:14,240
the context before you ask the llm the

765
00:30:12,000 --> 00:30:16,000
question it helps it generalized um and

766
00:30:14,240 --> 00:30:17,640
give a good answer but if you do not

767
00:30:16,000 --> 00:30:20,679
have enough context length you cannot do

768
00:30:17,640 --> 00:30:22,880
that so llms are limited by their

769
00:30:20,679 --> 00:30:25,519
context length there's quite a lot of

770
00:30:22,880 --> 00:30:28,000
work on how do we extend this context

771
00:30:25,519 --> 00:30:31,919
length and let llms take more things as

772
00:30:28,000 --> 00:30:34,159
input now there are too many um there's

773
00:30:31,919 --> 00:30:36,279
too much research in the idea in the

774
00:30:34,159 --> 00:30:38,760
long context modeling field to go over

775
00:30:36,279 --> 00:30:41,039
but I'll just go over one idea for some

776
00:30:38,760 --> 00:30:43,440
intuition here let's say that we are

777
00:30:41,039 --> 00:30:45,559
doing positional encoding using a

778
00:30:43,440 --> 00:30:48,600
popular positional encoding method maybe

779
00:30:45,559 --> 00:30:51,000
rotational positional encoding or Rope

780
00:30:48,600 --> 00:30:55,240
Rope does positional encoding by

781
00:30:51,000 --> 00:30:57,320
rotating vectors if two word pieces are

782
00:30:55,240 --> 00:30:59,240
very far apart then they will be rotated

783
00:30:57,320 --> 00:31:01,240
far apart it's a continuous rotation

784
00:30:59,240 --> 00:31:03,799
that's applied to the input vectors this

785
00:31:01,240 --> 00:31:07,039
is one way of doing relative positional

786
00:31:03,799 --> 00:31:09,799
encoding now if we trained our large

787
00:31:07,039 --> 00:31:12,880
language model to have a sequence input

788
00:31:09,799 --> 00:31:16,000
length of 4,000 and then at inference we

789
00:31:12,880 --> 00:31:17,679
have some unseen range going up to 8,000

790
00:31:16,000 --> 00:31:19,000
what we can do and what some people have

791
00:31:17,679 --> 00:31:21,080
shown is that you can actually

792
00:31:19,000 --> 00:31:24,159
interpolate and kind of squeeze your

793
00:31:21,080 --> 00:31:26,880
input uh sequence to fit within the

794
00:31:24,159 --> 00:31:29,720
normal range of positional encoding you

795
00:31:26,880 --> 00:31:31,840
can interpolate your rotations and just

796
00:31:29,720 --> 00:31:33,639
uh put more points in between and

797
00:31:31,840 --> 00:31:35,559
because these rotations are continuous

798
00:31:33,639 --> 00:31:38,200
the llm is actually still able to

799
00:31:35,559 --> 00:31:40,000
discern what tokens are in what order

800
00:31:38,200 --> 00:31:41,880
this works for certain positional

801
00:31:40,000 --> 00:31:44,080
encoding methods like rotational

802
00:31:41,880 --> 00:31:46,399
positional encoding um and so this is

803
00:31:44,080 --> 00:31:48,080
one way people have sort of um gone

804
00:31:46,399 --> 00:31:49,840
around this issue there are other

805
00:31:48,080 --> 00:31:52,000
methods like compressive methods but in

806
00:31:49,840 --> 00:31:54,880
general we're trying to find ways that

807
00:31:52,000 --> 00:31:57,519
the llm is able to um accurately

808
00:31:54,880 --> 00:32:00,559
represent what order tokens came in when

809
00:31:57,519 --> 00:32:00,559
it receives longer

810
00:32:01,679 --> 00:32:07,480
cont is the rotation position L coding

811
00:32:04,559 --> 00:32:10,360
something people work Ed for natural

812
00:32:07,480 --> 00:32:12,120
language yes um do you have any

813
00:32:10,360 --> 00:32:13,720
intuition for why

814
00:32:12,120 --> 00:32:17,600
rotational

815
00:32:13,720 --> 00:32:19,880
uh modeling might be helpful here yeah

816
00:32:17,600 --> 00:32:21,720
so with positional encoding what you're

817
00:32:19,880 --> 00:32:25,159
really trying to do is you're trying to

818
00:32:21,720 --> 00:32:26,919
tell the llm somehow how far apart

819
00:32:25,159 --> 00:32:29,279
different vectors are so that when you

820
00:32:26,919 --> 00:32:31,360
do a tension if there's two similar

821
00:32:29,279 --> 00:32:33,120
words right next to each other they're

822
00:32:31,360 --> 00:32:34,840
they have very high interaction score

823
00:32:33,120 --> 00:32:36,480
but if they're very far apart they may

824
00:32:34,840 --> 00:32:38,080
be in completely different documents you

825
00:32:36,480 --> 00:32:41,480
don't want them to attend highly to each

826
00:32:38,080 --> 00:32:44,720
other so a good positional encoding

827
00:32:41,480 --> 00:32:46,240
method should change or should do

828
00:32:44,720 --> 00:32:48,480
something to the vectors so that the

829
00:32:46,240 --> 00:32:51,279
very far apart words don't attend very

830
00:32:48,480 --> 00:32:53,559
highly to each other when you rotate the

831
00:32:51,279 --> 00:32:55,399
input word embedding vectors if you

832
00:32:53,559 --> 00:32:57,559
rotate them far enough in that attention

833
00:32:55,399 --> 00:32:59,360
Matrix they won't be as similar because

834
00:32:57,559 --> 00:33:01,679
you're literally moving them farther

835
00:32:59,360 --> 00:33:03,760
apart in in this rotation space so by

836
00:33:01,679 --> 00:33:05,799
applying rotation matrices so that's

837
00:33:03,760 --> 00:33:08,519
sort of how for the far apart vectors

838
00:33:05,799 --> 00:33:10,559
when they rotated far apart um the

839
00:33:08,519 --> 00:33:12,080
attention score will not be as high if

840
00:33:10,559 --> 00:33:14,240
you're able to change the attention

841
00:33:12,080 --> 00:33:15,760
scores to reflect distance then you'll

842
00:33:14,240 --> 00:33:18,120
the llm will understand these vectors

843
00:33:15,760 --> 00:33:18,120
are far

844
00:33:21,760 --> 00:33:27,360
apart do sorry do these Transformations

845
00:33:24,880 --> 00:33:29,880
work for like biological sequence models

846
00:33:27,360 --> 00:33:31,799
cuz like I like if you have a protein

847
00:33:29,880 --> 00:33:34,120
sequence or a DNA sequence your distance

848
00:33:31,799 --> 00:33:35,840
is a literal molecular distance and if

849
00:33:34,120 --> 00:33:38,519
you're squeezing it in half you're like

850
00:33:35,840 --> 00:33:41,240
doing something physically so have

851
00:33:38,519 --> 00:33:43,039
people tried these on those domains and

852
00:33:41,240 --> 00:33:45,080
do they seem to work that's that's a

853
00:33:43,039 --> 00:33:47,320
really good question if you're modeling

854
00:33:45,080 --> 00:33:48,519
textual biological data then most of

855
00:33:47,320 --> 00:33:50,840
this will still work because you're

856
00:33:48,519 --> 00:33:53,000
modeling language when you start to

857
00:33:50,840 --> 00:33:55,080
model like numerical or other kinds of

858
00:33:53,000 --> 00:33:59,120
biological data I'll like show a few

859
00:33:55,080 --> 00:34:01,519
examples in some further slides um

860
00:33:59,120 --> 00:34:03,399
what you need to do is you need to uh

861
00:34:01,519 --> 00:34:05,039
change your architecture and maybe your

862
00:34:03,399 --> 00:34:07,399
positional coding strategy to reflect

863
00:34:05,039 --> 00:34:09,480
your biological data so some things that

864
00:34:07,399 --> 00:34:11,359
people have tried is that rather than

865
00:34:09,480 --> 00:34:13,800
doing like this language positional

866
00:34:11,359 --> 00:34:16,240
encoding you might if you're working

867
00:34:13,800 --> 00:34:20,399
with genomics data for example you might

868
00:34:16,240 --> 00:34:22,760
add a positional encoding um Vector that

869
00:34:20,399 --> 00:34:24,359
tells the model What gene this is or

870
00:34:22,760 --> 00:34:27,000
what position on the chromosome are you

871
00:34:24,359 --> 00:34:29,280
want if you can devide some sort of um

872
00:34:27,000 --> 00:34:31,240
system for just telling the model how

873
00:34:29,280 --> 00:34:33,280
far things are apart in chromosome space

874
00:34:31,240 --> 00:34:34,919
or like you know what Gene is it as long

875
00:34:33,280 --> 00:34:39,119
as you can identify it to the model then

876
00:34:34,919 --> 00:34:39,119
it can learn on it thank

877
00:34:39,720 --> 00:34:46,839
you okay in the spirit of um efficiency

878
00:34:45,200 --> 00:34:48,839
because now we're getting longer and

879
00:34:46,839 --> 00:34:50,839
longer sequence lens we need to worry

880
00:34:48,839 --> 00:34:54,280
about how does the attention mechanism

881
00:34:50,839 --> 00:34:55,720
itself uh scale the attention mechanism

882
00:34:54,280 --> 00:34:57,720
which we've been talking about has

883
00:34:55,720 --> 00:34:59,880
quadratic complexity and we can see that

884
00:34:57,720 --> 00:35:02,359
when you have an input sequence it's

885
00:34:59,880 --> 00:35:04,160
creating a n byn Matrix for the

886
00:35:02,359 --> 00:35:06,440
attention so this has quadratic

887
00:35:04,160 --> 00:35:08,800
complexity which scales very poorly when

888
00:35:06,440 --> 00:35:11,839
we get to very large models very long

889
00:35:08,800 --> 00:35:14,000
sequence Lens two main ideas which uh

890
00:35:11,839 --> 00:35:16,280
people have tried to improve this

891
00:35:14,000 --> 00:35:18,320
complexity problem is to improve

892
00:35:16,280 --> 00:35:20,880
efficiency with GPU optimization and

893
00:35:18,320 --> 00:35:22,480
then improve the scalability with GPU

894
00:35:20,880 --> 00:35:24,839
optimization what we're really trying to

895
00:35:22,480 --> 00:35:27,720
do is we're trying to utilize our gpus

896
00:35:24,839 --> 00:35:30,839
more efficiently so we're trying to

897
00:35:27,720 --> 00:35:33,920
manage how do we read and write data

898
00:35:30,839 --> 00:35:36,040
from the GPU memory back to the CPU or

899
00:35:33,920 --> 00:35:37,359
back to this um in order to speed up

900
00:35:36,040 --> 00:35:38,359
computations we want to be more

901
00:35:37,359 --> 00:35:39,960
efficient with these Matrix

902
00:35:38,359 --> 00:35:41,720
multiplications so that things happen

903
00:35:39,960 --> 00:35:43,720
faster on the GPU so that there's no

904
00:35:41,720 --> 00:35:45,400
bottlenecks uh flash attention on the

905
00:35:43,720 --> 00:35:47,760
left-and side is an example of this it's

906
00:35:45,400 --> 00:35:50,680
a industry standard by now people use

907
00:35:47,760 --> 00:35:53,040
Flash attention to make the GPU

908
00:35:50,680 --> 00:35:54,920
optimization a bit more efficient on the

909
00:35:53,040 --> 00:35:57,000
right hand side the idea of sub

910
00:35:54,920 --> 00:35:58,200
quadratic attention this is another

911
00:35:57,000 --> 00:36:01,079
subfield where where there's a lot of

912
00:35:58,200 --> 00:36:03,800
research going on the idea is to

913
00:36:01,079 --> 00:36:06,599
approximate this quadratic attention

914
00:36:03,800 --> 00:36:08,520
with other with approximation algorithms

915
00:36:06,599 --> 00:36:10,400
that are sub quadratic we still want to

916
00:36:08,520 --> 00:36:12,760
calculate attention coefficients it's

917
00:36:10,400 --> 00:36:14,880
very useful to highlight interactions

918
00:36:12,760 --> 00:36:16,400
between parts of your input but we don't

919
00:36:14,880 --> 00:36:18,880
want something that has quadratic

920
00:36:16,400 --> 00:36:20,640
complexity because computer scientist in

921
00:36:18,880 --> 00:36:22,640
computer science fighting quadratic

922
00:36:20,640 --> 00:36:25,280
complexity is a losing battle so we try

923
00:36:22,640 --> 00:36:27,040
to get sub quadratic methods and by now

924
00:36:25,280 --> 00:36:28,599
like um the example on the right hand

925
00:36:27,040 --> 00:36:30,800
side is former attention this is a

926
00:36:28,599 --> 00:36:32,680
linear attention method which changes

927
00:36:30,800 --> 00:36:35,599
the Matrix multiplications to calculate

928
00:36:32,680 --> 00:36:35,599
attention in linear

929
00:36:36,319 --> 00:36:41,480
time okay moving on with our theme about

930
00:36:39,440 --> 00:36:43,720
efficiency there are some methods for

931
00:36:41,480 --> 00:36:45,200
doing parameter efficient fine-tuning

932
00:36:43,720 --> 00:36:47,079
the idea here is that when you have a

933
00:36:45,200 --> 00:36:49,079
downstream application maybe you have a

934
00:36:47,079 --> 00:36:50,880
specific biological task or maybe you

935
00:36:49,079 --> 00:36:53,599
have a specific data set which you're

936
00:36:50,880 --> 00:36:56,640
interested in you want to fine-tune the

937
00:36:53,599 --> 00:36:58,640
lln to do this Downstream task however

938
00:36:56,640 --> 00:37:00,880
usually fine tune all of the weights of

939
00:36:58,640 --> 00:37:02,760
the llm for every Downstream task you

940
00:37:00,880 --> 00:37:04,400
want to do is not very efficient and

941
00:37:02,760 --> 00:37:06,800
it's not very feasible because there's

942
00:37:04,400 --> 00:37:09,520
billions of parameters so the idea here

943
00:37:06,800 --> 00:37:12,680
is to tune only a fraction of your llm

944
00:37:09,520 --> 00:37:15,640
weights or your Downstream tasks now

945
00:37:12,680 --> 00:37:17,400
what fraction of weights do you uh tune

946
00:37:15,640 --> 00:37:18,839
and where do you put them is something

947
00:37:17,400 --> 00:37:21,160
that different parameter efficient

948
00:37:18,839 --> 00:37:23,480
methods try to deal with uh the one I

949
00:37:21,160 --> 00:37:25,079
have here Laura is also is a very

950
00:37:23,480 --> 00:37:27,560
popular method that came up in recent

951
00:37:25,079 --> 00:37:30,520
years the idea with Laura is basically

952
00:37:27,560 --> 00:37:33,240
to freeze the pre-trained weight Matrix

953
00:37:30,520 --> 00:37:35,160
in blue here we freeze that and instead

954
00:37:33,240 --> 00:37:36,760
of training all of those weights we're

955
00:37:35,160 --> 00:37:39,200
going to train a smaller subset of

956
00:37:36,760 --> 00:37:41,480
Weights in Orange we're going to project

957
00:37:39,200 --> 00:37:43,839
the data down into a lower Dimension or

958
00:37:41,480 --> 00:37:45,960
a lower Rank and then we'll project it

959
00:37:43,839 --> 00:37:47,400
back up these two matrices they take up

960
00:37:45,960 --> 00:37:49,720
less parameters than the original

961
00:37:47,400 --> 00:37:53,800
weights and we're basically fine-tuning

962
00:37:49,720 --> 00:37:55,440
the llm in a lower rank space this uh

963
00:37:53,800 --> 00:37:57,720
decreases the amount of parameters we

964
00:37:55,440 --> 00:38:01,599
need to tune and in practice it's

965
00:37:57,720 --> 00:38:01,599
performs decently well

966
00:38:03,079 --> 00:38:06,960
um okay and the final thing about

967
00:38:05,480 --> 00:38:10,079
efficiency which I want to mention here

968
00:38:06,960 --> 00:38:12,560
is about quantization so usually large

969
00:38:10,079 --> 00:38:15,200
language models are trained with 32-bit

970
00:38:12,560 --> 00:38:17,640
or 16bit Precision this means that every

971
00:38:15,200 --> 00:38:21,119
single weight in the large language

972
00:38:17,640 --> 00:38:23,200
model is represented using 32 or 16 bits

973
00:38:21,119 --> 00:38:25,720
but what people have shown is that if

974
00:38:23,200 --> 00:38:28,200
you can actually get away with

975
00:38:25,720 --> 00:38:31,280
representing the weights with less bits

976
00:38:28,200 --> 00:38:34,480
if you can go down to 168 or four this

977
00:38:31,280 --> 00:38:37,040
actually lets you use up less GPU memory

978
00:38:34,480 --> 00:38:39,520
it also speeds up your computations and

979
00:38:37,040 --> 00:38:41,160
so people have done studies where they

980
00:38:39,520 --> 00:38:43,160
take different weights three different

981
00:38:41,160 --> 00:38:45,560
weights on the bottom here um in the

982
00:38:43,160 --> 00:38:47,280
subplots and then they try to represent

983
00:38:45,560 --> 00:38:49,960
these weight matrices using less and

984
00:38:47,280 --> 00:38:52,440
less bits starting with 16 bit 8 bit

985
00:38:49,960 --> 00:38:54,160
four bit and what they basically show is

986
00:38:52,440 --> 00:38:56,240
that the performance on this natural

987
00:38:54,160 --> 00:38:59,119
language understanding test on the

988
00:38:56,240 --> 00:39:01,160
y-axis it does not suffer too much

989
00:38:59,119 --> 00:39:05,560
performance degradation until you get

990
00:39:01,160 --> 00:39:08,319
down to 4 32bit quantization so you can

991
00:39:05,560 --> 00:39:11,000
get quite a lot of um efficiency gains

992
00:39:08,319 --> 00:39:13,680
in terms of GPU memory um up until maybe

993
00:39:11,000 --> 00:39:15,480
four bit quantization the cost of doing

994
00:39:13,680 --> 00:39:18,160
this quantization is that you might lose

995
00:39:15,480 --> 00:39:18,160
a little bit of

996
00:39:18,520 --> 00:39:22,119
performance okay the last thing I want

997
00:39:20,560 --> 00:39:23,680
to talk about with large language models

998
00:39:22,119 --> 00:39:26,160
is about multimodal large language

999
00:39:23,680 --> 00:39:28,160
models there's been a this is a very hot

1000
00:39:26,160 --> 00:39:30,599
topic of research recently

1001
00:39:28,160 --> 00:39:33,119
the general idea when we want to align

1002
00:39:30,599 --> 00:39:35,040
another modality of data it may be

1003
00:39:33,119 --> 00:39:36,560
images it may be some sort of biological

1004
00:39:35,040 --> 00:39:39,119
data or so

1005
00:39:36,560 --> 00:39:42,040
on at a high level what we need to do is

1006
00:39:39,119 --> 00:39:44,079
we need to one bring the other modality

1007
00:39:42,040 --> 00:39:45,800
of data into the space of the large

1008
00:39:44,079 --> 00:39:48,079
language model and then the other thing

1009
00:39:45,800 --> 00:39:50,280
we need to do is when the llm is

1010
00:39:48,079 --> 00:39:52,119
generating we need to provide a way for

1011
00:39:50,280 --> 00:39:54,880
it to generate the other modality of

1012
00:39:52,119 --> 00:39:57,480
data so input and output if we can do

1013
00:39:54,880 --> 00:40:00,000
this then we can give these llms the

1014
00:39:57,480 --> 00:40:02,040
ility to interact and input modalities

1015
00:40:00,000 --> 00:40:04,000
in other in other modalities other than

1016
00:40:02,040 --> 00:40:06,160
language which would be very

1017
00:40:04,000 --> 00:40:09,200
powerful there's been a lot of methods

1018
00:40:06,160 --> 00:40:11,200
for how do you bring the other modality

1019
00:40:09,200 --> 00:40:12,599
into language space the easiest thing

1020
00:40:11,200 --> 00:40:15,240
you can do is learn different

1021
00:40:12,599 --> 00:40:17,000
projections um which people have tried

1022
00:40:15,240 --> 00:40:19,000
um but at a high level we're just trying

1023
00:40:17,000 --> 00:40:21,160
to align the two modalities of

1024
00:40:19,000 --> 00:40:24,560
information using paired language and

1025
00:40:21,160 --> 00:40:26,160
other modality data um and we learn the

1026
00:40:24,560 --> 00:40:27,920
encoder and then potentially the

1027
00:40:26,160 --> 00:40:31,520
generator if you also want want to

1028
00:40:27,920 --> 00:40:31,520
Output the other modalities of

1029
00:40:33,480 --> 00:40:37,760
information me move on to the biology

1030
00:40:36,200 --> 00:40:40,319
side can I bring up two questions from

1031
00:40:37,760 --> 00:40:43,480
Zoom sure um someone's asking if you

1032
00:40:40,319 --> 00:40:45,599
could comment on um Good Luck long

1033
00:40:43,480 --> 00:40:48,319
context Window performance of the state

1034
00:40:45,599 --> 00:40:51,040
space models like Mamba and then there

1035
00:40:48,319 --> 00:40:53,200
is one clarification question uh asking

1036
00:40:51,040 --> 00:40:58,960
you Flora is similar to fine-tuning

1037
00:40:53,200 --> 00:41:01,119
models like with uh let's say imet uh

1038
00:40:58,960 --> 00:41:03,920
we can ask for more clarification yeah

1039
00:41:01,119 --> 00:41:07,200
okay um so for the first question Mambo

1040
00:41:03,920 --> 00:41:08,960
was a very interesting um uh model which

1041
00:41:07,200 --> 00:41:11,240
came out recently for modeling longrange

1042
00:41:08,960 --> 00:41:14,119
interaction so the Mamba architecture

1043
00:41:11,240 --> 00:41:17,240
it's based on stat spased models which

1044
00:41:14,119 --> 00:41:19,640
have a little bit more theory behind um

1045
00:41:17,240 --> 00:41:22,000
how do you how do you parameterize the

1046
00:41:19,640 --> 00:41:24,079
previous context in the models in order

1047
00:41:22,000 --> 00:41:26,079
to keep enough information in recurrent

1048
00:41:24,079 --> 00:41:29,599
architectures as you keep processing

1049
00:41:26,079 --> 00:41:31,480
more and more input um I'm not too like

1050
00:41:29,599 --> 00:41:33,640
I'm not deeply familiar with like the

1051
00:41:31,480 --> 00:41:35,680
theory behind State space models they do

1052
00:41:33,640 --> 00:41:37,560
seem to have very good capabilities for

1053
00:41:35,680 --> 00:41:39,359
modeling longrange interactions I think

1054
00:41:37,560 --> 00:41:40,960
they've been shown to do pretty good on

1055
00:41:39,359 --> 00:41:42,319
like book summarization tasks where

1056
00:41:40,960 --> 00:41:45,400
you're trying to summarize an entire

1057
00:41:42,319 --> 00:41:47,040
book that's put into context um but it

1058
00:41:45,400 --> 00:41:50,319
remains to be seen whether those

1059
00:41:47,040 --> 00:41:52,040
architectures will kind of um scale past

1060
00:41:50,319 --> 00:41:54,200
Transformers in terms of in terms of

1061
00:41:52,040 --> 00:41:55,520
scale and performance um these large

1062
00:41:54,200 --> 00:41:58,640
language models they do have an

1063
00:41:55,520 --> 00:42:00,400
advantage that they're very very good at

1064
00:41:58,640 --> 00:42:02,800
you're it's very easy to parallelize

1065
00:42:00,400 --> 00:42:04,520
their computations across gpus and so

1066
00:42:02,800 --> 00:42:06,640
companies have been able to really scale

1067
00:42:04,520 --> 00:42:08,599
up large language model training and so

1068
00:42:06,640 --> 00:42:10,839
it remains to be seen whether the state

1069
00:42:08,599 --> 00:42:13,400
spased models can be scaled up like that

1070
00:42:10,839 --> 00:42:15,480
and what was the second question asking

1071
00:42:13,400 --> 00:42:18,760
if Laura can be used for fine tuning the

1072
00:42:15,480 --> 00:42:20,240
way that um people fine-tune imag net

1073
00:42:18,760 --> 00:42:22,920
I'm not sure if the question is like

1074
00:42:20,240 --> 00:42:24,839
application to image net or do you mean

1075
00:42:22,920 --> 00:42:27,559
if they mean like just relating Laura

1076
00:42:24,839 --> 00:42:29,880
with fine tuning and uh image models in

1077
00:42:27,559 --> 00:42:31,559
general um that's a little unclear to me

1078
00:42:29,880 --> 00:42:32,640
too I don't know if they will comment

1079
00:42:31,559 --> 00:42:35,640
for more

1080
00:42:32,640 --> 00:42:37,760
clarification right in general um Lura

1081
00:42:35,640 --> 00:42:40,720
fine tuning is a very good way to

1082
00:42:37,760 --> 00:42:42,599
fine-tune uh llms in general and models

1083
00:42:40,720 --> 00:42:44,480
in general like um I don't think it can

1084
00:42:42,599 --> 00:42:46,520
be applied to other Transformers as well

1085
00:42:44,480 --> 00:42:49,680
so if you are fine-tuning on something

1086
00:42:46,520 --> 00:42:51,240
like imet it is a it is a valid option

1087
00:42:49,680 --> 00:42:53,400
um we're just fine-tuning a smaller

1088
00:42:51,240 --> 00:42:55,880
fraction of weights to kind of change

1089
00:42:53,400 --> 00:42:57,960
the activations inside of the model so

1090
00:42:55,880 --> 00:43:00,760
it is a good way to train even if you're

1091
00:42:57,960 --> 00:43:02,280
thinking about training multimodal llms

1092
00:43:00,760 --> 00:43:03,880
where you're freezing the original

1093
00:43:02,280 --> 00:43:06,240
language llm and then you're training

1094
00:43:03,880 --> 00:43:08,480
another modality encoder it is a very

1095
00:43:06,240 --> 00:43:10,240
good way to kind of um train a little

1096
00:43:08,480 --> 00:43:11,960
bit of parameters in the large language

1097
00:43:10,240 --> 00:43:14,520
model while you're learning the other

1098
00:43:11,960 --> 00:43:16,760
modality encoder the nice thing about

1099
00:43:14,520 --> 00:43:19,160
things like Laura specifically is that

1100
00:43:16,760 --> 00:43:21,640
it actually leaves the original llm

1101
00:43:19,160 --> 00:43:24,599
weights intact with some finetuning

1102
00:43:21,640 --> 00:43:26,400
methods you have the concern that you're

1103
00:43:24,599 --> 00:43:28,119
changing too many weights and shifting

1104
00:43:26,400 --> 00:43:30,160
away from the original weights which

1105
00:43:28,119 --> 00:43:31,640
were good at modeling language already

1106
00:43:30,160 --> 00:43:32,920
but with Laura you're actually phrasing

1107
00:43:31,640 --> 00:43:35,520
the original weights and you're just

1108
00:43:32,920 --> 00:43:37,599
learning some new adap adaptation

1109
00:43:35,520 --> 00:43:40,040
weights so you leave the original model

1110
00:43:37,599 --> 00:43:42,839
intact which is

1111
00:43:40,040 --> 00:43:44,119
good okay for the rest of this talk I

1112
00:43:42,839 --> 00:43:46,040
don't think I have too much time but I'm

1113
00:43:44,119 --> 00:43:48,079
going to talk about how these large

1114
00:43:46,040 --> 00:43:49,839
language models and other Foundation

1115
00:43:48,079 --> 00:43:51,359
models can be used for modeling

1116
00:43:49,839 --> 00:43:53,880
biological

1117
00:43:51,359 --> 00:43:55,559
dat so just like I mentioned on the

1118
00:43:53,880 --> 00:43:58,000
first slide right now is a very exciting

1119
00:43:55,559 --> 00:44:00,000
time to be in AI Arch there's just as

1120
00:43:58,000 --> 00:44:02,640
many biological Foundation models coming

1121
00:44:00,000 --> 00:44:04,960
out now on the very short list of them

1122
00:44:02,640 --> 00:44:07,319
is Alpha fold which was really recently

1123
00:44:04,960 --> 00:44:09,280
just um awarded a Nobel Prize which is

1124
00:44:07,319 --> 00:44:11,400
amazing so there's a lot of really

1125
00:44:09,280 --> 00:44:13,920
exciting biological Foundation models

1126
00:44:11,400 --> 00:44:16,160
also coming out and in general

1127
00:44:13,920 --> 00:44:18,720
biological Foundation models they also

1128
00:44:16,160 --> 00:44:21,160
train on a large amount of biological

1129
00:44:18,720 --> 00:44:23,000
and medical data um it may not always be

1130
00:44:21,160 --> 00:44:25,599
language it can be numerical data or

1131
00:44:23,000 --> 00:44:27,559
other kinds of data as well um and we

1132
00:44:25,599 --> 00:44:29,520
just want to build a base Foundation

1133
00:44:27,559 --> 00:44:31,520
model that can be adapted to diverse

1134
00:44:29,520 --> 00:44:34,880
Downstream

1135
00:44:31,520 --> 00:44:37,559
tasks now in terms of public biomedical

1136
00:44:34,880 --> 00:44:39,440
data that we have um available to us to

1137
00:44:37,559 --> 00:44:42,000
build these Foundation models there are

1138
00:44:39,440 --> 00:44:45,240
some common sources of data that people

1139
00:44:42,000 --> 00:44:47,640
use there's PubMed pumm Hub cem there's

1140
00:44:45,240 --> 00:44:50,359
the CDC guidelines archive has quite a

1141
00:44:47,640 --> 00:44:52,079
lot of quite a large range of papers as

1142
00:44:50,359 --> 00:44:53,920
well there are also some really nice

1143
00:44:52,079 --> 00:44:57,119
data sets generated here at the broad

1144
00:44:53,920 --> 00:44:59,160
Institute like the link X and the link

1145
00:44:57,119 --> 00:45:01,520
study and then GTE for single cell

1146
00:44:59,160 --> 00:45:03,400
expression data um for protein modeling

1147
00:45:01,520 --> 00:45:06,480
people usually look at protein databases

1148
00:45:03,400 --> 00:45:09,240
like unit Proto um and in general when

1149
00:45:06,480 --> 00:45:10,880
we're developing or we're fine-tuning

1150
00:45:09,240 --> 00:45:12,800
large language models and Foundation

1151
00:45:10,880 --> 00:45:15,000
models for biological applications

1152
00:45:12,800 --> 00:45:17,280
people usually Define some sort of data

1153
00:45:15,000 --> 00:45:19,520
mixture which they're interested in um I

1154
00:45:17,280 --> 00:45:22,880
think for Metatron which was an llm

1155
00:45:19,520 --> 00:45:25,079
released by epfl and meta uh they chose

1156
00:45:22,880 --> 00:45:28,960
to do PubMed um as well as some other

1157
00:45:25,079 --> 00:45:28,960
General textual data sets

1158
00:45:30,119 --> 00:45:34,240
so there's been a growing um there's

1159
00:45:32,480 --> 00:45:37,839
been a growing group of models that have

1160
00:45:34,240 --> 00:45:39,559
been trying to tune language models in

1161
00:45:37,839 --> 00:45:41,400
order to be better at biomedical

1162
00:45:39,559 --> 00:45:43,079
applications Metatron which I just

1163
00:45:41,400 --> 00:45:45,000
mentioned is a very good example of this

1164
00:45:43,079 --> 00:45:47,760
one this is sort of a graphic from their

1165
00:45:45,000 --> 00:45:51,079
paper about the development of bi

1166
00:45:47,760 --> 00:45:51,960
biology and biomedical focused llms um

1167
00:45:51,079 --> 00:45:55,760
since

1168
00:45:51,960 --> 00:45:57,960
2020 now the performance on this graph

1169
00:45:55,760 --> 00:46:00,200
is in terms of met question answering

1170
00:45:57,960 --> 00:46:02,319
data sets Med QA it's a good Benchmark

1171
00:46:00,200 --> 00:46:04,640
task for how well these language models

1172
00:46:02,319 --> 00:46:06,520
respond on medical questioning and it's

1173
00:46:04,640 --> 00:46:10,400
a data set which the performance has

1174
00:46:06,520 --> 00:46:13,000
been steadily increasing on in 2023 the

1175
00:46:10,400 --> 00:46:15,240
closed Source models like GPT 4 and palm

1176
00:46:13,000 --> 00:46:17,760
to from Google managed to get very high

1177
00:46:15,240 --> 00:46:19,800
scores on these data sets so over 80%

1178
00:46:17,760 --> 00:46:22,720
performance in medical question and

1179
00:46:19,800 --> 00:46:24,640
answering uh just last year in 2023 the

1180
00:46:22,720 --> 00:46:27,680
open source models mostly based on the

1181
00:46:24,640 --> 00:46:30,040
Llama llm architecture started to catch

1182
00:46:27,680 --> 00:46:32,559
up but in terms of like fine-tuning

1183
00:46:30,040 --> 00:46:35,000
these biological llms the open source

1184
00:46:32,559 --> 00:46:37,960
models still lag behind the industry

1185
00:46:35,000 --> 00:46:37,960
private models a little

1186
00:46:38,079 --> 00:46:43,160
bit I'm going to switch gears now and

1187
00:46:40,680 --> 00:46:46,359
get more into single cell RNA sequencing

1188
00:46:43,160 --> 00:46:49,280
data which our lab is interested in so

1189
00:46:46,359 --> 00:46:50,880
with single cell data um I'm sure we're

1190
00:46:49,280 --> 00:46:53,559
all familiar with the Single Cell

1191
00:46:50,880 --> 00:46:55,440
sequencing process where we disassociate

1192
00:46:53,559 --> 00:46:58,200
cells and then we sequence them to get

1193
00:46:55,440 --> 00:47:00,440
gene expression profiles at single cell

1194
00:46:58,200 --> 00:47:02,960
resolution the developments in single

1195
00:47:00,440 --> 00:47:05,480
cell uh sequencing technology in the

1196
00:47:02,960 --> 00:47:07,640
past decade have really increased the

1197
00:47:05,480 --> 00:47:09,559
throughput and the amount of data that

1198
00:47:07,640 --> 00:47:12,200
people are able to generate and in the

1199
00:47:09,559 --> 00:47:14,520
recent few years there's been large

1200
00:47:12,200 --> 00:47:16,720
scale data atlases generated such as

1201
00:47:14,520 --> 00:47:19,280
human cell Atlas and the cell by Gene by

1202
00:47:16,720 --> 00:47:21,640
The czi Institute these single cell

1203
00:47:19,280 --> 00:47:23,680
Atlas data is perfect for building

1204
00:47:21,640 --> 00:47:26,000
Foundation models it has a very large

1205
00:47:23,680 --> 00:47:27,640
diversity of uh expression information

1206
00:47:26,000 --> 00:47:30,559
from a lot of tissu

1207
00:47:27,640 --> 00:47:32,640
um for multiple species and the first

1208
00:47:30,559 --> 00:47:34,440
generation of single cell Foundation

1209
00:47:32,640 --> 00:47:36,880
models has been developed in the past

1210
00:47:34,440 --> 00:47:37,720
one or two years the first one of these

1211
00:47:36,880 --> 00:47:40,839
was

1212
00:47:37,720 --> 00:47:44,280
scpt now GPT is in the name so we know

1213
00:47:40,839 --> 00:47:45,960
that this is a llm inspired architecture

1214
00:47:44,280 --> 00:47:47,559
but they're actually not modeling

1215
00:47:45,960 --> 00:47:50,319
language here they're modeling gene

1216
00:47:47,559 --> 00:47:52,440
expression information so rather than a

1217
00:47:50,319 --> 00:47:55,520
sentence here the input sequence for

1218
00:47:52,440 --> 00:47:57,839
sgbt will be a list of gene expression

1219
00:47:55,520 --> 00:48:00,880
vectors we will encode gene expression

1220
00:47:57,839 --> 00:48:04,280
and then the genee identity into a token

1221
00:48:00,880 --> 00:48:06,720
and we'll train this scpt model to model

1222
00:48:04,280 --> 00:48:08,960
this expression information instead of

1223
00:48:06,720 --> 00:48:12,520
next token prediction we're going to do

1224
00:48:08,960 --> 00:48:15,839
Gene um Mass gene expression

1225
00:48:12,520 --> 00:48:19,319
prediction so GPT was trained on data

1226
00:48:15,839 --> 00:48:22,000
from the cell by Gene U public data

1227
00:48:19,319 --> 00:48:25,000
Atlas so it was around 33 million cells

1228
00:48:22,000 --> 00:48:26,520
from Mostly humans and mice and they

1229
00:48:25,000 --> 00:48:28,319
after pre-training they came up with

1230
00:48:26,520 --> 00:48:30,480
their Foundation model for modeling gene

1231
00:48:28,319 --> 00:48:32,599
expression data they applied their model

1232
00:48:30,480 --> 00:48:34,760
to a lot of Downstream tasks in single

1233
00:48:32,599 --> 00:48:37,160
cell analysis they did things like cell

1234
00:48:34,760 --> 00:48:39,960
type annotation batch integration and

1235
00:48:37,160 --> 00:48:42,319
perturbation response

1236
00:48:39,960 --> 00:48:44,319
prediction now around the same time

1237
00:48:42,319 --> 00:48:46,640
another Foundation model came out this

1238
00:48:44,319 --> 00:48:50,079
was Gene former developed by researchers

1239
00:48:46,640 --> 00:48:51,680
also from the broad Institute um this uh

1240
00:48:50,079 --> 00:48:54,960
was another Transformer based

1241
00:48:51,680 --> 00:48:57,000
architecture it also um modeled gene

1242
00:48:54,960 --> 00:49:00,440
expression profiles by modeling a list

1243
00:48:57,000 --> 00:49:01,920
of Gene um tokens however they differed

1244
00:49:00,440 --> 00:49:03,880
a little bit in the way that they

1245
00:49:01,920 --> 00:49:05,920
pre-processed and tokenized the gene

1246
00:49:03,880 --> 00:49:07,319
expression information they normalized

1247
00:49:05,920 --> 00:49:09,920
the data a bit differently and then they

1248
00:49:07,319 --> 00:49:12,160
did a rank encoding of the genes by

1249
00:49:09,920 --> 00:49:14,160
their expression information so this

1250
00:49:12,160 --> 00:49:16,799
sort of goes to show that multiple

1251
00:49:14,160 --> 00:49:19,520
Foundation models have been developed

1252
00:49:16,799 --> 00:49:22,160
however there is no consensus on the way

1253
00:49:19,520 --> 00:49:24,559
to tokenize and normalize this gene

1254
00:49:22,160 --> 00:49:26,160
expression data um to build these large

1255
00:49:24,559 --> 00:49:28,720
scale Foundation models that's still in

1256
00:49:26,160 --> 00:49:28,720
active area

1257
00:49:30,400 --> 00:49:35,280
research now in general when we're

1258
00:49:32,839 --> 00:49:37,520
trying to model single cell data at

1259
00:49:35,280 --> 00:49:40,040
scale there's a couple of big challenges

1260
00:49:37,520 --> 00:49:42,119
which we come up with um a lot of the

1261
00:49:40,040 --> 00:49:43,839
challenges in natural language have been

1262
00:49:42,119 --> 00:49:46,119
not completely figured out but they're

1263
00:49:43,839 --> 00:49:47,480
much more developed and matured um in

1264
00:49:46,119 --> 00:49:49,359
single cell data we're still dealing

1265
00:49:47,480 --> 00:49:52,880
with a couple of challenges the first

1266
00:49:49,359 --> 00:49:55,480
one is multimodality multimodality means

1267
00:49:52,880 --> 00:49:58,440
when we look at a single cell sequencing

1268
00:49:55,480 --> 00:50:00,559
study there is numerical information

1269
00:49:58,440 --> 00:50:02,960
from the gene expression values but

1270
00:50:00,559 --> 00:50:05,520
there's also a lot of multimodal

1271
00:50:02,960 --> 00:50:06,880
information associated with the study

1272
00:50:05,520 --> 00:50:09,040
that gives a lot of important

1273
00:50:06,880 --> 00:50:10,599
information about that data you have the

1274
00:50:09,040 --> 00:50:12,440
text from the manuscript and the

1275
00:50:10,599 --> 00:50:14,760
abstract itself you have cell type

1276
00:50:12,440 --> 00:50:17,799
annotations maybe disease labels and

1277
00:50:14,760 --> 00:50:19,960
then there's also prior knowledge about

1278
00:50:17,799 --> 00:50:21,799
biological Concepts on the internet

1279
00:50:19,960 --> 00:50:23,440
these might take the form of gen set

1280
00:50:21,799 --> 00:50:25,640
databases or they may be knowledge

1281
00:50:23,440 --> 00:50:27,960
graphs on the internet about single cell

1282
00:50:25,640 --> 00:50:30,599
Concepts so there's quite a lot of

1283
00:50:27,960 --> 00:50:32,839
sources of information which is hard for

1284
00:50:30,599 --> 00:50:35,000
one model to integrate by

1285
00:50:32,839 --> 00:50:36,920
itself some other challenges which are

1286
00:50:35,000 --> 00:50:38,880
more well established in single cell

1287
00:50:36,920 --> 00:50:41,119
sequencing data is the sparsity and

1288
00:50:38,880 --> 00:50:43,480
noise that's inherent in the sequencing

1289
00:50:41,119 --> 00:50:45,920
process and then also the batch effects

1290
00:50:43,480 --> 00:50:45,920
between

1291
00:50:46,960 --> 00:50:51,160
samples and then the last idea which

1292
00:50:49,319 --> 00:50:54,079
I'll kind of leave everyone with today

1293
00:50:51,160 --> 00:50:55,640
is about the benefit of pre-training on

1294
00:50:54,079 --> 00:50:57,480
text when we're thinking about language

1295
00:50:55,640 --> 00:50:59,680
models so when we're thinking about

1296
00:50:57,480 --> 00:51:01,480
building Foundation models should you

1297
00:50:59,680 --> 00:51:03,480
start from scratch and learn on your

1298
00:51:01,480 --> 00:51:05,799
data or should you try to initialize

1299
00:51:03,480 --> 00:51:08,400
from a pre-trained model like a language

1300
00:51:05,799 --> 00:51:10,000
model um in in natural language

1301
00:51:08,400 --> 00:51:12,440
processing it's been pretty well

1302
00:51:10,000 --> 00:51:14,680
established for many years now that

1303
00:51:12,440 --> 00:51:17,200
building a general model by pre-training

1304
00:51:14,680 --> 00:51:19,319
it on textual data and then fine-tuning

1305
00:51:17,200 --> 00:51:21,520
it for a specific application it works

1306
00:51:19,319 --> 00:51:24,119
better than creating narrow test

1307
00:51:21,520 --> 00:51:26,720
specific models the earliest example of

1308
00:51:24,119 --> 00:51:28,839
this I found was in 2020 with bioert

1309
00:51:26,720 --> 00:51:30,839
bioert took the strongest language model

1310
00:51:28,839 --> 00:51:33,119
at the time which was Bert it fine-tuned

1311
00:51:30,839 --> 00:51:36,119
it for some biological data sets and it

1312
00:51:33,119 --> 00:51:38,160
was able to outperform both Bert and the

1313
00:51:36,119 --> 00:51:40,960
state-of-the-art test specific models at

1314
00:51:38,160 --> 00:51:42,960
the time for their biological tasks so

1315
00:51:40,960 --> 00:51:45,559
this was in 2020 and the language models

1316
00:51:42,960 --> 00:51:47,760
have only gotten stronger since then so

1317
00:51:45,559 --> 00:51:49,920
it's um in natural language processing

1318
00:51:47,760 --> 00:51:53,280
it's pretty well established that

1319
00:51:49,920 --> 00:51:56,119
pre-training helps and some intuition

1320
00:51:53,280 --> 00:51:58,559
which I kind of hold for this is when we

1321
00:51:56,119 --> 00:52:00,760
pre-ra on natural language and when we

1322
00:51:58,559 --> 00:52:03,839
train on data from the internet there's

1323
00:52:00,760 --> 00:52:06,559
a very large amount of biological and

1324
00:52:03,839 --> 00:52:08,319
medical Concepts on the Internet or the

1325
00:52:06,559 --> 00:52:10,200
model to learn from on the right hand

1326
00:52:08,319 --> 00:52:11,880
side's a visualization of PubMed

1327
00:52:10,200 --> 00:52:14,319
literature just one data source on the

1328
00:52:11,880 --> 00:52:17,359
internet there is a lot of information

1329
00:52:14,319 --> 00:52:19,760
which the model can learn so can can I

1330
00:52:17,359 --> 00:52:19,760
just listen

1331
00:52:23,000 --> 00:52:28,119
to

1332
00:52:24,520 --> 00:52:30,079
sorry okay so there's a lot of general

1333
00:52:28,119 --> 00:52:32,079
concepts in information which the model

1334
00:52:30,079 --> 00:52:33,680
can learn from on the internet and

1335
00:52:32,079 --> 00:52:35,640
building representations about these

1336
00:52:33,680 --> 00:52:37,240
general concepts during pre-training

1337
00:52:35,640 --> 00:52:41,160
helps the model when it's trying to find

1338
00:52:37,240 --> 00:52:41,160
to an adapt for Downstream

1339
00:52:41,400 --> 00:52:45,480
tasks okay and with that thank you very

1340
00:52:43,640 --> 00:52:47,680
much for listening and I'm happy to take

1341
00:52:45,480 --> 00:52:49,839
any more

1342
00:52:47,680 --> 00:52:52,720
questions first of all thank you so much

1343
00:52:49,839 --> 00:52:54,280
for inviting us uh and uh giving us a

1344
00:52:52,720 --> 00:52:57,160
chance to talk about our

1345
00:52:54,280 --> 00:53:00,359
work um yeah so

1346
00:52:57,160 --> 00:53:04,040
uh my name is David Van djk and uh I run

1347
00:53:00,359 --> 00:53:05,920
a lab at Yale both in at the med school

1348
00:53:04,040 --> 00:53:09,839
and in computer

1349
00:53:05,920 --> 00:53:12,640
science and uh in our lab we build

1350
00:53:09,839 --> 00:53:15,280
algorithms to to decode the rules of

1351
00:53:12,640 --> 00:53:17,480
life we work with all kinds of large

1352
00:53:15,280 --> 00:53:19,640
biomedical data uh but we have a special

1353
00:53:17,480 --> 00:53:21,520
interest uh in single cell genomics and

1354
00:53:19,640 --> 00:53:23,680
that's what I'm going to talk about uh

1355
00:53:21,520 --> 00:53:28,240
today

1356
00:53:23,680 --> 00:53:30,480
primarily um yeah so single cell

1357
00:53:28,240 --> 00:53:33,720
analysis in the age of LM so sad just

1358
00:53:30,480 --> 00:53:34,960
gave an amazing prior uh primer on large

1359
00:53:33,720 --> 00:53:39,119
language models and biological

1360
00:53:34,960 --> 00:53:41,920
Foundation models and um so we all know

1361
00:53:39,119 --> 00:53:45,160
that llms are kind of revolutionizing

1362
00:53:41,920 --> 00:53:46,960
the way we interact with information um

1363
00:53:45,160 --> 00:53:48,280
and S presented a bit about biological

1364
00:53:46,960 --> 00:53:50,760
Foundation models applying these

1365
00:53:48,280 --> 00:53:53,280
Technologies the into

1366
00:53:50,760 --> 00:53:55,280
genomics um and today I'm going to talk

1367
00:53:53,280 --> 00:53:57,839
about several of our projects where we

1368
00:53:55,280 --> 00:54:01,640
kind of trying to attempt to do

1369
00:53:57,839 --> 00:54:04,839
this um specifically today I would like

1370
00:54:01,640 --> 00:54:07,760
to argue that biology has a language or

1371
00:54:04,839 --> 00:54:11,079
even multiple languages and we can use

1372
00:54:07,760 --> 00:54:14,000
algorithms we can use computation to

1373
00:54:11,079 --> 00:54:15,680
understand to decipher this language so

1374
00:54:14,000 --> 00:54:19,160
one very good example is the immune

1375
00:54:15,680 --> 00:54:22,839
system so uh as you know the immune

1376
00:54:19,160 --> 00:54:25,640
system uh is composed of many different

1377
00:54:22,839 --> 00:54:28,760
cell types many cells that are

1378
00:54:25,640 --> 00:54:31,079
interacting in complex ways and one way

1379
00:54:28,760 --> 00:54:33,040
in which cells communicate as you

1380
00:54:31,079 --> 00:54:35,319
probably know is using cyto or

1381
00:54:33,040 --> 00:54:38,400
chemokines so we can think of those in

1382
00:54:35,319 --> 00:54:41,359
some sense as words um but generally

1383
00:54:38,400 --> 00:54:43,880
they occur in concerts many different

1384
00:54:41,359 --> 00:54:46,400
cyto kindes Chemin together and together

1385
00:54:43,880 --> 00:54:48,760
these signals uh you can imagine form

1386
00:54:46,400 --> 00:54:51,119
sort of sentences through which cells

1387
00:54:48,760 --> 00:54:53,960
communicate with one another so you can

1388
00:54:51,119 --> 00:54:56,040
imagine that if we were to understand

1389
00:54:53,960 --> 00:55:00,079
this language that means we could listen

1390
00:54:56,040 --> 00:55:01,960
to cells we can even talk to them so the

1391
00:55:00,079 --> 00:55:03,200
ability to decipher decipher this

1392
00:55:01,960 --> 00:55:04,960
language is really important because it

1393
00:55:03,200 --> 00:55:06,640
allows us potentially to control the

1394
00:55:04,960 --> 00:55:08,040
system right imagine you can talk to

1395
00:55:06,640 --> 00:55:10,559
cells and tell them what to do or they

1396
00:55:08,040 --> 00:55:13,480
can tell you how they're feeling or

1397
00:55:10,559 --> 00:55:16,640
what's wrong right that would be the

1398
00:55:13,480 --> 00:55:18,799
dream um so the first project I want to

1399
00:55:16,640 --> 00:55:21,119
talk about today is an algorithm that we

1400
00:55:18,799 --> 00:55:23,720
developed cot is where we actually

1401
00:55:21,119 --> 00:55:28,319
attempt to learn this immune language

1402
00:55:23,720 --> 00:55:28,319
using a causal inference framework

1403
00:55:29,000 --> 00:55:35,200
um so let me start with a question so

1404
00:55:32,039 --> 00:55:37,680
imagine you encounter an on contacted

1405
00:55:35,200 --> 00:55:40,680
tribe uh so you're the first to to to

1406
00:55:37,680 --> 00:55:42,520
make contact and uh you don't know their

1407
00:55:40,680 --> 00:55:43,760
language they don't know your language

1408
00:55:42,520 --> 00:55:45,200
and you have to learn to communicate

1409
00:55:43,760 --> 00:55:48,039
with them how would you go about doing

1410
00:55:45,200 --> 00:55:50,920
this so perhaps first you would listen

1411
00:55:48,039 --> 00:55:53,440
right you would try to see kind of are

1412
00:55:50,920 --> 00:55:56,480
there any patterns in their in the way

1413
00:55:53,440 --> 00:55:58,119
they speak certain sounds um then

1414
00:55:56,480 --> 00:55:59,599
perhaps you would try to interact with

1415
00:55:58,119 --> 00:56:02,359
them you would make some simple gestures

1416
00:55:59,599 --> 00:56:04,680
or words or even simple phrases um you

1417
00:56:02,359 --> 00:56:06,559
would make guesses uh and and and

1418
00:56:04,680 --> 00:56:08,079
initially and then observe the outcome

1419
00:56:06,559 --> 00:56:10,160
and then from there you would improve

1420
00:56:08,079 --> 00:56:13,000
you kind of make a model and you would

1421
00:56:10,160 --> 00:56:14,640
adapt based on feedback and gradually

1422
00:56:13,000 --> 00:56:15,640
potentially you would allow would allow

1423
00:56:14,640 --> 00:56:18,440
you to

1424
00:56:15,640 --> 00:56:20,280
communicate um we we essentially apply

1425
00:56:18,440 --> 00:56:22,760
these same principles to biology so

1426
00:56:20,280 --> 00:56:25,760
biology has been on contacted if you

1427
00:56:22,760 --> 00:56:27,680
will for billions of years and only very

1428
00:56:25,760 --> 00:56:30,559
recently have we started to contact it

1429
00:56:27,680 --> 00:56:33,160
and started to talk with it and I would

1430
00:56:30,559 --> 00:56:35,720
say in very primitive ways

1431
00:56:33,160 --> 00:56:37,760
still um but we apply the same

1432
00:56:35,720 --> 00:56:39,760
principles to decoding for example cell

1433
00:56:37,760 --> 00:56:42,440
communication we listen we do

1434
00:56:39,760 --> 00:56:44,640
measurements right we collect data then

1435
00:56:42,440 --> 00:56:46,039
we talk to them we do perturbations we

1436
00:56:44,640 --> 00:56:48,680
stimulate the cells in different ways

1437
00:56:46,039 --> 00:56:50,799
and see how they react and then with

1438
00:56:48,680 --> 00:56:52,559
this feedback we develop models we try

1439
00:56:50,799 --> 00:56:54,720
to understand the language in the hope

1440
00:56:52,559 --> 00:56:56,880
to communicate with these cells and and

1441
00:56:54,720 --> 00:56:59,079
and understand what they're doing and

1442
00:56:56,880 --> 00:57:02,119
potentially to to manipulate them or I

1443
00:56:59,079 --> 00:57:06,520
mean manipulate sounds bad but to maybe

1444
00:57:02,119 --> 00:57:10,359
convince them to change in different

1445
00:57:06,520 --> 00:57:13,359
ways so um yeah so so I guess I'm

1446
00:57:10,359 --> 00:57:14,799
arguing that perturbing cells is like

1447
00:57:13,359 --> 00:57:17,119
talking to them or if you want to talk

1448
00:57:14,799 --> 00:57:20,760
to cells one way is to kind of perturb

1449
00:57:17,119 --> 00:57:23,280
them stimulate treat them um and this is

1450
00:57:20,760 --> 00:57:25,680
something that we do a lot uh especially

1451
00:57:23,280 --> 00:57:27,200
uh in an Institute like this uh where

1452
00:57:25,680 --> 00:57:29,480
you do large L scale experiments where

1453
00:57:27,200 --> 00:57:31,520
you do all kinds of drug genetic

1454
00:57:29,480 --> 00:57:33,920
perturbations all kinds of ways to

1455
00:57:31,520 --> 00:57:35,720
perturb to stimulate cells and see how

1456
00:57:33,920 --> 00:57:37,880
they react and then build your models

1457
00:57:35,720 --> 00:57:40,520
and try to learn this

1458
00:57:37,880 --> 00:57:43,559
language however there is a challenge

1459
00:57:40,520 --> 00:57:46,720
there is a problem because if we take

1460
00:57:43,559 --> 00:57:49,520
cells and we stimulate them and we try

1461
00:57:46,720 --> 00:57:51,400
to observe their

1462
00:57:49,520 --> 00:57:53,559
response and let's say you would want to

1463
00:57:51,400 --> 00:57:55,839
do this at the Single Cell level because

1464
00:57:53,559 --> 00:57:58,039
there's a lot of heterogenity and every

1465
00:57:55,839 --> 00:58:00,440
cell is different so you would want to

1466
00:57:58,039 --> 00:58:02,440
know exactly how an individual cell

1467
00:58:00,440 --> 00:58:05,319
reacts to a stimulation a drug or

1468
00:58:02,440 --> 00:58:07,599
genetic perturbation the problem is we

1469
00:58:05,319 --> 00:58:09,400
cannot really track individual cells

1470
00:58:07,599 --> 00:58:11,480
across perturbations because

1471
00:58:09,400 --> 00:58:13,720
fundamentally the measurement is

1472
00:58:11,480 --> 00:58:16,920
destructive the moment you measure a

1473
00:58:13,720 --> 00:58:18,359
cell at least you know fairly High

1474
00:58:16,920 --> 00:58:20,359
dimensional there's there's of course

1475
00:58:18,359 --> 00:58:21,760
you know life cell Imaging but it's very

1476
00:58:20,359 --> 00:58:23,440
low dimensional measurements but a high

1477
00:58:21,760 --> 00:58:25,799
dimensional measurements such as singles

1478
00:58:23,440 --> 00:58:27,920
sna sequencing the moment you measure

1479
00:58:25,799 --> 00:58:29,240
the cell C you destroy it so you can

1480
00:58:27,920 --> 00:58:32,200
only measure it

1481
00:58:29,240 --> 00:58:33,640
once but ideally what you would want is

1482
00:58:32,200 --> 00:58:36,119
you want to measure a cell let's say in

1483
00:58:33,640 --> 00:58:37,960
a control condition without simulation

1484
00:58:36,119 --> 00:58:39,839
and then you want to perturb it with the

1485
00:58:37,960 --> 00:58:41,520
drug or genetic perturbation or whatever

1486
00:58:39,839 --> 00:58:43,440
it is then you measure it again because

1487
00:58:41,520 --> 00:58:45,480
then you truly know the sort of causal

1488
00:58:43,440 --> 00:58:47,440
difference causal effect what was the

1489
00:58:45,480 --> 00:58:49,920
true effect of that perturbation on that

1490
00:58:47,440 --> 00:58:52,200
cell that's not possible what you need

1491
00:58:49,920 --> 00:58:55,000
in this case really is a time machine

1492
00:58:52,200 --> 00:58:56,640
you measure the cell in the process you

1493
00:58:55,000 --> 00:58:58,280
destroy it you take your time machine

1494
00:58:56,640 --> 00:58:59,200
back when it was still alive you measure

1495
00:58:58,280 --> 00:59:01,520
it

1496
00:58:59,200 --> 00:59:03,680
again we don't have a physical time

1497
00:59:01,520 --> 00:59:06,079
machine but through inference to

1498
00:59:03,680 --> 00:59:07,839
computation we can build if you will a

1499
00:59:06,079 --> 00:59:10,920
computational time machine that's what

1500
00:59:07,839 --> 00:59:10,920
we set out to do in this

1501
00:59:12,160 --> 00:59:20,200
project so what is the solution the

1502
00:59:16,160 --> 00:59:22,079
solution is to infer so-called

1503
00:59:20,200 --> 00:59:24,640
counterfactuals so

1504
00:59:22,079 --> 00:59:27,200
counterfactuals this is idea from causal

1505
00:59:24,640 --> 00:59:29,480
inference and it's essentially asking

1506
00:59:27,200 --> 00:59:33,280
what would have happened happened

1507
00:59:29,480 --> 00:59:34,760
if so through causal inference this

1508
00:59:33,280 --> 00:59:37,960
framework which is called the potential

1509
00:59:34,760 --> 00:59:41,119
outcome framework we can match cells

1510
00:59:37,960 --> 00:59:43,520
across conditions based on confounder

1511
00:59:41,119 --> 00:59:46,079
signals and I'll explain in a bit what

1512
00:59:43,520 --> 00:59:47,440
that means to identify these

1513
00:59:46,079 --> 00:59:50,920
counterfactual

1514
00:59:47,440 --> 00:59:54,039
pairs uh essentially pairs of

1515
00:59:50,920 --> 00:59:56,359
cells where you can say it's the same

1516
00:59:54,039 --> 00:59:59,200
cell or it's essentially the same cell

1517
00:59:56,359 --> 00:59:59,200
but in a different

1518
01:00:00,160 --> 01:00:06,280
condition so to do

1519
01:00:02,319 --> 01:00:08,400
this we focus on separating um

1520
01:00:06,280 --> 01:00:13,000
confounder from treatment

1521
01:00:08,400 --> 01:00:15,200
signals so there are many sources of

1522
01:00:13,000 --> 01:00:19,680
variability in a population of

1523
01:00:15,200 --> 01:00:23,119
cells um a subset of

1524
01:00:19,680 --> 01:00:24,720
those uh sources of variability are due

1525
01:00:23,119 --> 01:00:27,559
to the treatment you know whatever you

1526
01:00:24,720 --> 01:00:29,760
perturb the cells with and they change

1527
01:00:27,559 --> 01:00:32,000
but some signals or a lot of them have

1528
01:00:29,760 --> 01:00:33,559
nothing to do don't correlate with the

1529
01:00:32,000 --> 01:00:36,480
treatment for example you have different

1530
01:00:33,559 --> 01:00:38,920
cell types there's cell cycle there's

1531
01:00:36,480 --> 01:00:40,760
all kind of sources of variability that

1532
01:00:38,920 --> 01:00:43,280
have nothing to do with the with the

1533
01:00:40,760 --> 01:00:45,760
response to the treatment we call those

1534
01:00:43,280 --> 01:00:48,960
confounders so you can kind of imagine

1535
01:00:45,760 --> 01:00:51,280
the data uh comprises of confounder

1536
01:00:48,960 --> 01:00:54,680
signals and outcome signal so where the

1537
01:00:51,280 --> 01:00:57,000
outcome signals um correlate or a result

1538
01:00:54,680 --> 01:00:59,319
of the treatment

1539
01:00:57,000 --> 01:01:01,599
so the idea is that we want to remove

1540
01:00:59,319 --> 01:01:03,359
the outcome signals before we do the

1541
01:01:01,599 --> 01:01:05,079
matching because otherwise there's a

1542
01:01:03,359 --> 01:01:07,480
bias because when we try to match cells

1543
01:01:05,079 --> 01:01:09,160
between conditions ideally we want to

1544
01:01:07,480 --> 01:01:11,799
have the same distribution we want to be

1545
01:01:09,160 --> 01:01:14,160
able to say okay this is the same cell

1546
01:01:11,799 --> 01:01:16,400
across the two conditions but

1547
01:01:14,160 --> 01:01:18,240
fundamentally the cells have changed in

1548
01:01:16,400 --> 01:01:20,880
response to the pertubation so we

1549
01:01:18,240 --> 01:01:21,839
temporarily remove those effects as

1550
01:01:20,880 --> 01:01:24,319
outcome

1551
01:01:21,839 --> 01:01:26,599
effects and we look at the cells what we

1552
01:01:24,319 --> 01:01:27,839
call the confounder space there we do

1553
01:01:26,599 --> 01:01:29,559
the matching now we can do correct

1554
01:01:27,839 --> 01:01:31,880
matching and specifically you use

1555
01:01:29,559 --> 01:01:34,119
optimal transport so now it's

1556
01:01:31,880 --> 01:01:36,559
unbiased now we have our counterfactual

1557
01:01:34,119 --> 01:01:38,640
cell pairs we can do our individual

1558
01:01:36,559 --> 01:01:40,760
treatment analysis so we can ask for

1559
01:01:38,640 --> 01:01:44,000
every individual cell how does it

1560
01:01:40,760 --> 01:01:46,240
respond so this is a you know a visual

1561
01:01:44,000 --> 01:01:48,359
overview of the algorithm so you do your

1562
01:01:46,240 --> 01:01:50,960
experiment for example you you have your

1563
01:01:48,359 --> 01:01:52,760
control cells you do some

1564
01:01:50,960 --> 01:01:55,680
perturbation you do your traditional

1565
01:01:52,760 --> 01:01:57,520
single cell analysis then we actually

1566
01:01:55,680 --> 01:01:59,839
use independent component analysis to

1567
01:01:57,520 --> 01:02:02,279
separate different um sources of

1568
01:01:59,839 --> 01:02:05,039
variability we split those in confounder

1569
01:02:02,279 --> 01:02:06,960
and outcome signals then as I mentioned

1570
01:02:05,039 --> 01:02:08,839
before we temporarily remove the

1571
01:02:06,960 --> 01:02:10,960
outcomes we just look at the confounders

1572
01:02:08,839 --> 01:02:13,440
we call the confounder space in that

1573
01:02:10,960 --> 01:02:15,319
space we perform optimal transport we

1574
01:02:13,440 --> 01:02:17,599
match cells we obtain our counterfactual

1575
01:02:15,319 --> 01:02:19,640
cell Pairs and now we can perform

1576
01:02:17,599 --> 01:02:21,240
treatment effect analysis but truly at

1577
01:02:19,640 --> 01:02:23,760
the Single Cell level we can ask for

1578
01:02:21,240 --> 01:02:25,559
every individual cell how did it respond

1579
01:02:23,760 --> 01:02:27,920
as opposed to asking it for how does the

1580
01:02:25,559 --> 01:02:31,760
whole population

1581
01:02:27,920 --> 01:02:33,279
respond so the outcome the result are

1582
01:02:31,760 --> 01:02:35,680
these counterfactual cell pairs so we

1583
01:02:33,279 --> 01:02:39,520
can ask for every individual cell how

1584
01:02:35,680 --> 01:02:41,279
does it respond to simulation because

1585
01:02:39,520 --> 01:02:44,640
different cells will resp respond

1586
01:02:41,279 --> 01:02:44,640
differently or maybe

1587
01:02:45,640 --> 01:02:50,640
not now this is where things get

1588
01:02:48,160 --> 01:02:52,240
interesting so now that we have this

1589
01:02:50,640 --> 01:02:54,480
framework where we

1590
01:02:52,240 --> 01:02:56,960
can ask questions about how do

1591
01:02:54,480 --> 01:02:59,160
individual cells respond to a treatment

1592
01:02:56,960 --> 01:03:00,839
we can ask questions about what happens

1593
01:02:59,160 --> 01:03:03,559
when you have multiple

1594
01:03:00,839 --> 01:03:05,920
treatments um specifically about Synergy

1595
01:03:03,559 --> 01:03:07,920
for example if you have two different

1596
01:03:05,920 --> 01:03:09,359
perturbations when you combine those two

1597
01:03:07,920 --> 01:03:11,079
perturbations does something different

1598
01:03:09,359 --> 01:03:12,920
happen than when you apply those

1599
01:03:11,079 --> 01:03:14,960
perturbations separately is there an

1600
01:03:12,920 --> 01:03:17,319
interaction and the reason why that I

1601
01:03:14,960 --> 01:03:19,200
bring this up is that because is is that

1602
01:03:17,319 --> 01:03:21,039
there is a fundamental relationship

1603
01:03:19,200 --> 01:03:23,920
between this and

1604
01:03:21,039 --> 01:03:26,640
language um in fact I would like to

1605
01:03:23,920 --> 01:03:28,799
argue that Synergy some form of a protol

1606
01:03:26,640 --> 01:03:30,880
language so let's take natural language

1607
01:03:28,799 --> 01:03:33,680
right we have a sentence with words the

1608
01:03:30,880 --> 01:03:36,680
meaning of a word is highly dependent on

1609
01:03:33,680 --> 01:03:39,160
the context in fact two words when they

1610
01:03:36,680 --> 01:03:41,760
occur together might have completely

1611
01:03:39,160 --> 01:03:44,119
different meaning when they do not occur

1612
01:03:41,760 --> 01:03:46,119
together or occur with other words right

1613
01:03:44,119 --> 01:03:48,359
so there's interactions there's

1614
01:03:46,119 --> 01:03:50,520
nonlinear effects and of course in

1615
01:03:48,359 --> 01:03:52,720
natural language we have you know many

1616
01:03:50,520 --> 01:03:54,520
like higher order effects when you have

1617
01:03:52,720 --> 01:03:56,520
interactions when you have many words

1618
01:03:54,520 --> 01:03:58,440
and it's more complicated but I would

1619
01:03:56,520 --> 01:04:01,000
say that this sort of nonlinear

1620
01:03:58,440 --> 01:04:02,839
interaction between words to Synergy is

1621
01:04:01,000 --> 01:04:06,119
the fundamental building block of

1622
01:04:02,839 --> 01:04:09,160
language and that's why we are studying

1623
01:04:06,119 --> 01:04:12,160
it here we wanted to ask are there such

1624
01:04:09,160 --> 01:04:13,559
interactions in biology um specifically

1625
01:04:12,160 --> 01:04:17,039
um in immune

1626
01:04:13,559 --> 01:04:18,240
responses so in this case we can measure

1627
01:04:17,039 --> 01:04:20,799
treatment effects to individual

1628
01:04:18,240 --> 01:04:23,319
perturbations but also combinations and

1629
01:04:20,799 --> 01:04:25,400
we can use this idea to quantify what we

1630
01:04:23,319 --> 01:04:28,160
call Synergy score so what we simply do

1631
01:04:25,400 --> 01:04:31,720
is ask when you take perturbation a and

1632
01:04:28,160 --> 01:04:33,880
perturbation B and you add them up do

1633
01:04:31,720 --> 01:04:37,440
you get the same as when

1634
01:04:33,880 --> 01:04:39,279
you um simulate the cells with

1635
01:04:37,440 --> 01:04:41,960
perturbation A and B combined or does

1636
01:04:39,279 --> 01:04:43,680
something special happen for example is

1637
01:04:41,960 --> 01:04:45,200
a certain gene expression higher than

1638
01:04:43,680 --> 01:04:49,359
you would expect or is lower than you

1639
01:04:45,200 --> 01:04:52,520
expect any deviation means that there is

1640
01:04:49,359 --> 01:04:54,920
an interaction there's a Synergy effect

1641
01:04:52,520 --> 01:04:57,520
and again that idea is fundamental to

1642
01:04:54,920 --> 01:04:59,119
language sort of a building block I call

1643
01:04:57,520 --> 01:05:02,559
it sort of a Proto

1644
01:04:59,119 --> 01:05:04,240
language okay so now let's apply this

1645
01:05:02,559 --> 01:05:07,839
idea

1646
01:05:04,240 --> 01:05:09,440
so we aim to understand the language you

1647
01:05:07,839 --> 01:05:12,359
know immune language Lang language of

1648
01:05:09,440 --> 01:05:15,640
cyto kindes um and my amazing

1649
01:05:12,359 --> 01:05:18,240
collaborator Jeff shisuka um carried out

1650
01:05:15,640 --> 01:05:22,520
this experiment while he took um healthy

1651
01:05:18,240 --> 01:05:24,680
donor um pbmc and simulated the cells

1652
01:05:22,520 --> 01:05:26,359
both acutely for two days or chronically

1653
01:05:24,680 --> 01:05:28,920
for seven days

1654
01:05:26,359 --> 01:05:31,520
uh with a number of different cyto

1655
01:05:28,920 --> 01:05:33,359
kindes both individually as well as

1656
01:05:31,520 --> 01:05:35,680
combinations and then measured the

1657
01:05:33,359 --> 01:05:38,119
effect through single cell

1658
01:05:35,680 --> 01:05:40,640
sequencing and the question there was

1659
01:05:38,119 --> 01:05:42,400
how do the cells how do different cells

1660
01:05:40,640 --> 01:05:43,920
respond to different cyto and how do

1661
01:05:42,400 --> 01:05:45,599
they respond to combinations if there's

1662
01:05:43,920 --> 01:05:49,319
something special going

1663
01:05:45,599 --> 01:05:52,640
on so we measured individual conditions

1664
01:05:49,319 --> 01:05:56,920
as you can see here in AER in aerons Al

1665
01:05:52,640 --> 01:05:59,400
Stakes Etc but also several combinations

1666
01:05:56,920 --> 01:06:02,279
to specifically ask are there nonlinear

1667
01:05:59,400 --> 01:06:02,279
are there synergistic

1668
01:06:02,680 --> 01:06:09,760
effects so first of all when you just

1669
01:06:06,839 --> 01:06:12,240
look at individual perturbations our

1670
01:06:09,760 --> 01:06:16,119
framework our cinemo framework is able

1671
01:06:12,240 --> 01:06:19,039
to quantify these individual treatment

1672
01:06:16,119 --> 01:06:20,720
effects so the plots on the right each

1673
01:06:19,039 --> 01:06:22,160
point is a cell but it's not your

1674
01:06:20,720 --> 01:06:23,760
traditional umap we're not looking at

1675
01:06:22,160 --> 01:06:27,240
gene expression patterns we're looking

1676
01:06:23,760 --> 01:06:29,000
at changes so every cell actually is is

1677
01:06:27,240 --> 01:06:31,400
a cell pair is a counterfactual cell

1678
01:06:29,000 --> 01:06:32,559
pair so it shows how cells change so

1679
01:06:31,400 --> 01:06:34,440
here on the left we see the different

1680
01:06:32,559 --> 01:06:37,960
cell typs you're used to seeing the spot

1681
01:06:34,440 --> 01:06:40,319
there's B cells D cells monocytes Etc

1682
01:06:37,960 --> 01:06:43,480
okay nothing special but here we're

1683
01:06:40,319 --> 01:06:46,079
looking at the response space and what

1684
01:06:43,480 --> 01:06:48,400
you can see here is

1685
01:06:46,079 --> 01:06:51,760
that while there are many different cell

1686
01:06:48,400 --> 01:06:54,119
types there are only two classes of

1687
01:06:51,760 --> 01:06:55,640
response so there's only two ways in

1688
01:06:54,119 --> 01:06:57,440
which a cells can respond in fact

1689
01:06:55,640 --> 01:06:59,960
there's one uh cluster where you have

1690
01:06:57,440 --> 01:07:02,200
the the T cells and then K cells and

1691
01:06:59,960 --> 01:07:04,960
another cluster which is largely

1692
01:07:02,200 --> 01:07:06,319
Moniz so this shows and by the way this

1693
01:07:04,960 --> 01:07:08,520
is one specific treatment this is

1694
01:07:06,319 --> 01:07:10,960
inferent beta treatment but this

1695
01:07:08,520 --> 01:07:13,559
demonstrates that we can use our

1696
01:07:10,960 --> 01:07:15,440
individual um treatment analysis using

1697
01:07:13,559 --> 01:07:17,880
our causal inference framework to ask

1698
01:07:15,440 --> 01:07:21,160
how do individual cells

1699
01:07:17,880 --> 01:07:22,640
respond um and are there do they respond

1700
01:07:21,160 --> 01:07:24,039
a similar different ways and here you

1701
01:07:22,640 --> 01:07:27,400
can see there's really only two ways in

1702
01:07:24,039 --> 01:07:27,400
which they respond

1703
01:07:30,920 --> 01:07:37,200
um but now

1704
01:07:33,640 --> 01:07:39,079
um so there we we we you know Quantified

1705
01:07:37,200 --> 01:07:41,039
the effect of individual

1706
01:07:39,079 --> 01:07:43,079
perturbations um but as I mentioned

1707
01:07:41,039 --> 01:07:44,480
we're really interested here in sort of

1708
01:07:43,079 --> 01:07:46,319
deciphering this language we're

1709
01:07:44,480 --> 01:07:49,319
interested in combinations so let's look

1710
01:07:46,319 --> 01:07:51,240
at these uh combos so I introduce this

1711
01:07:49,319 --> 01:07:53,240
idea of the Synergy score we can

1712
01:07:51,240 --> 01:07:58,160
quantify

1713
01:07:53,240 --> 01:07:59,599
how uh combining two perturbations is

1714
01:07:58,160 --> 01:08:01,640
different or the effect is different

1715
01:07:59,599 --> 01:08:04,400
from stimulating itself separately with

1716
01:08:01,640 --> 01:08:07,000
these perturbations or how much deviates

1717
01:08:04,400 --> 01:08:09,160
from that and we device a Synergy score

1718
01:08:07,000 --> 01:08:11,319
as you can see here on the y-axis here's

1719
01:08:09,160 --> 01:08:14,319
broken down by cell type and you can see

1720
01:08:11,319 --> 01:08:16,480
very clearly that monocytes monocytes

1721
01:08:14,319 --> 01:08:18,480
show very strong Synergy score so in

1722
01:08:16,480 --> 01:08:21,600
this case especially monocytes were

1723
01:08:18,480 --> 01:08:23,520
synergistic meaning that they had a

1724
01:08:21,600 --> 01:08:27,440
different response to cyto kind

1725
01:08:23,520 --> 01:08:27,440
combinations than individual cyto Kines

1726
01:08:29,080 --> 01:08:35,120
um in fact they particularly showed this

1727
01:08:32,239 --> 01:08:37,719
synergistic effect for interferon beta

1728
01:08:35,120 --> 01:08:41,359
plus uh these three different cyto such

1729
01:08:37,719 --> 01:08:43,880
as tnf Alpha interferon gamma and

1730
01:08:41,359 --> 01:08:46,719
L6 and it turns out these so these

1731
01:08:43,880 --> 01:08:50,480
combinations essentially trigger um a

1732
01:08:46,719 --> 01:08:54,159
synergistic response to multi- Signal

1733
01:08:50,480 --> 01:08:55,759
information um and we found that uh this

1734
01:08:54,159 --> 01:08:58,719
showed an inhibition of actually

1735
01:08:55,759 --> 01:09:01,839
nutrifil chemotactic signaling as well

1736
01:08:58,719 --> 01:09:02,960
as induction of monoy chemotactic

1737
01:09:01,839 --> 01:09:05,960
programs

1738
01:09:02,960 --> 01:09:08,080
so these synergistic effects uh cause

1739
01:09:05,960 --> 01:09:11,159
some kind of Downstream uh

1740
01:09:08,080 --> 01:09:13,319
chemotactic um

1741
01:09:11,159 --> 01:09:16,400
expression

1742
01:09:13,319 --> 01:09:19,839
um so what this shows is that our

1743
01:09:16,400 --> 01:09:22,239
approach not only can identify responses

1744
01:09:19,839 --> 01:09:24,679
of individual cells of perturbations but

1745
01:09:22,239 --> 01:09:25,960
it can identify synergistic nonlinear

1746
01:09:24,679 --> 01:09:28,920
effects

1747
01:09:25,960 --> 01:09:31,560
and in some sense you could call this a

1748
01:09:28,920 --> 01:09:34,520
combinatorial logic these

1749
01:09:31,560 --> 01:09:37,199
nonlinearity and that is the first step

1750
01:09:34,520 --> 01:09:40,040
to understanding a language and we only

1751
01:09:37,199 --> 01:09:41,960
measured you know up to two

1752
01:09:40,040 --> 01:09:43,600
combinations but there might be higher

1753
01:09:41,960 --> 01:09:44,839
order interactions maybe some different

1754
01:09:43,600 --> 01:09:47,560
things will go on when you have three

1755
01:09:44,839 --> 01:09:49,400
four or five cyto packs of course the

1756
01:09:47,560 --> 01:09:52,560
challenge there is that you have this

1757
01:09:49,400 --> 01:09:54,920
combinatorial huge comori

1758
01:09:52,560 --> 01:09:56,719
space um but I think it would be super

1759
01:09:54,920 --> 01:09:58,400
interesting to kind of scale up this

1760
01:09:56,719 --> 01:10:01,320
experiment maybe in some clever ways

1761
01:09:58,400 --> 01:10:03,840
where we can scale it to really start to

1762
01:10:01,320 --> 01:10:06,560
infer the logic the language uh because

1763
01:10:03,840 --> 01:10:09,159
I think we just scratched the surface

1764
01:10:06,560 --> 01:10:11,360
here

1765
01:10:09,159 --> 01:10:13,679
okay so now that we have kind of

1766
01:10:11,360 --> 01:10:18,480
establish this idea of biology as a

1767
01:10:13,679 --> 01:10:20,400
language why not use state-of-the-art

1768
01:10:18,480 --> 01:10:24,239
language tools or natural language

1769
01:10:20,400 --> 01:10:25,560
processing to tackle this problem so in

1770
01:10:24,239 --> 01:10:28,880
the second project I want to talk talk

1771
01:10:25,560 --> 01:10:31,920
about um it's called selto sentence

1772
01:10:28,880 --> 01:10:34,280
where we teach large language models the

1773
01:10:31,920 --> 01:10:38,560
language um of

1774
01:10:34,280 --> 01:10:42,360
biology so sa

1775
01:10:38,560 --> 01:10:44,239
just um gave uh terrific introduction

1776
01:10:42,360 --> 01:10:45,760
into large language models and

1777
01:10:44,239 --> 01:10:47,320
biological Foundation models so I don't

1778
01:10:45,760 --> 01:10:50,320
have to convince you I mean even before

1779
01:10:47,320 --> 01:10:53,920
that I'm sure you were already convinced

1780
01:10:50,320 --> 01:10:55,800
um so the question is can large language

1781
01:10:53,920 --> 01:10:58,320
models model biology

1782
01:10:55,800 --> 01:11:00,239
well clearly llms are good at language I

1783
01:10:58,320 --> 01:11:02,440
mean they're really fantastic at at

1784
01:11:00,239 --> 01:11:04,159
modeling language we also know that

1785
01:11:02,440 --> 01:11:05,679
they're pretty good at learning new

1786
01:11:04,159 --> 01:11:09,159
languages

1787
01:11:05,679 --> 01:11:10,520
so an llm that's been trained on English

1788
01:11:09,159 --> 01:11:12,400
has a much easier time learning a

1789
01:11:10,520 --> 01:11:14,840
completely new language than if you

1790
01:11:12,400 --> 01:11:16,760
would train it from scratch because

1791
01:11:14,840 --> 01:11:18,199
there are similarities between languages

1792
01:11:16,760 --> 01:11:19,640
right I mean of course you might be

1793
01:11:18,199 --> 01:11:23,760
familiar with this idea of the universal

1794
01:11:19,640 --> 01:11:25,360
grammar chsky um there are similarities

1795
01:11:23,760 --> 01:11:27,120
between languages right they have some

1796
01:11:25,360 --> 01:11:29,320
fundamental building blocks that is the

1797
01:11:27,120 --> 01:11:31,520
same so if you know one language it's

1798
01:11:29,320 --> 01:11:33,440
easier also for humans if you know long

1799
01:11:31,520 --> 01:11:38,800
one language it's easier to know another

1800
01:11:33,440 --> 01:11:39,719
language um so if we assume that biology

1801
01:11:38,800 --> 01:11:42,360
has a

1802
01:11:39,719 --> 01:11:45,280
language why not use perhaps we can use

1803
01:11:42,360 --> 01:11:47,239
llms to learn the language so what does

1804
01:11:45,280 --> 01:11:48,800
a biological language look like so in

1805
01:11:47,239 --> 01:11:51,000
the previous project I was kind of

1806
01:11:48,800 --> 01:11:53,159
talking about cell communications where

1807
01:11:51,000 --> 01:11:56,040
the cyto chemokines are kind of the

1808
01:11:53,159 --> 01:11:57,679
words they occur together into sentences

1809
01:11:56,040 --> 01:11:59,480
but another way to look at biological

1810
01:11:57,679 --> 01:12:03,080
language in terms of more sort of

1811
01:11:59,480 --> 01:12:06,679
genomic language in the sense that genes

1812
01:12:03,080 --> 01:12:10,040
the units of biology um are the

1813
01:12:06,679 --> 01:12:11,920
words individual cells you can think of

1814
01:12:10,040 --> 01:12:14,199
as sentences right where different words

1815
01:12:11,920 --> 01:12:17,080
occur together in a certain

1816
01:12:14,199 --> 01:12:21,280
context tissues perhaps we can think of

1817
01:12:17,080 --> 01:12:25,480
those as paragraphs or documents Etc so

1818
01:12:21,280 --> 01:12:28,080
that might be a natural way of modeling

1819
01:12:25,480 --> 01:12:28,080
biological

1820
01:12:29,360 --> 01:12:35,199
language so why would we actually use

1821
01:12:32,360 --> 01:12:39,159
large language models to model single

1822
01:12:35,199 --> 01:12:40,320
cell data so pre-trained large language

1823
01:12:39,159 --> 01:12:42,600
models so they been pre-trained on

1824
01:12:40,320 --> 01:12:44,600
natural language they are inherently

1825
01:12:42,600 --> 01:12:46,080
powerful we know that they're smart

1826
01:12:44,600 --> 01:12:47,760
right to a certain

1827
01:12:46,080 --> 01:12:51,560
degree

1828
01:12:47,760 --> 01:12:53,719
um they are very disappointing so far

1829
01:12:51,560 --> 01:12:55,639
incredibly uh

1830
01:12:53,719 --> 01:12:58,239
capable and

1831
01:12:55,639 --> 01:12:59,760
even though they're trained on language

1832
01:12:58,239 --> 01:13:01,400
they're much more than language models

1833
01:12:59,760 --> 01:13:04,520
they can do all kinds of Downstream

1834
01:13:01,400 --> 01:13:07,480
diverse Downstream tasks right such as

1835
01:13:04,520 --> 01:13:11,199
math or you know whatever it is in

1836
01:13:07,480 --> 01:13:12,960
addition um the advantage of using large

1837
01:13:11,199 --> 01:13:15,679
language models to model biology is that

1838
01:13:12,960 --> 01:13:17,440
it would allow for seamless integration

1839
01:13:15,679 --> 01:13:19,679
of biological data and textual data so I

1840
01:13:17,440 --> 01:13:23,320
had already mentioned briefly that

1841
01:13:19,679 --> 01:13:25,760
there's huge amount of sort of metadata

1842
01:13:23,320 --> 01:13:28,560
about biology in in the text

1843
01:13:25,760 --> 01:13:30,960
form this would potentially allow

1844
01:13:28,560 --> 01:13:34,600
integrating all that metadata that we

1845
01:13:30,960 --> 01:13:36,679
have with actual genomic data to have

1846
01:13:34,600 --> 01:13:39,120
sort of a model that kind of understands

1847
01:13:36,679 --> 01:13:40,840
both and can sort of translate or go

1848
01:13:39,120 --> 01:13:44,000
back and forth between the

1849
01:13:40,840 --> 01:13:46,360
two and another huge advantage of using

1850
01:13:44,000 --> 01:13:48,960
LMS is there's that is that we can

1851
01:13:46,360 --> 01:13:52,800
leverage incredibly rapid advancements

1852
01:13:48,960 --> 01:13:54,199
in LM infrastructure so if you're at

1853
01:13:52,800 --> 01:13:55,679
least somewhat following this Fields

1854
01:13:54,199 --> 01:13:57,320
you'll know that there's like every week

1855
01:13:55,679 --> 01:14:00,760
There's a new model coming out or a new

1856
01:13:57,320 --> 01:14:03,280
framework or or you know larger context

1857
01:14:00,760 --> 01:14:05,639
or you know billions trillions of

1858
01:14:03,280 --> 01:14:08,040
parameters if you were to build your own

1859
01:14:05,639 --> 01:14:09,800
models you could never keep up with it

1860
01:14:08,040 --> 01:14:12,880
so if we get to leverage these

1861
01:14:09,800 --> 01:14:15,719
incredible sort of exponential you know

1862
01:14:12,880 --> 01:14:18,480
advancements in LM infrastructure that

1863
01:14:15,719 --> 01:14:22,639
would be an advantage I would

1864
01:14:18,480 --> 01:14:25,840
think so I would like to argue that you

1865
01:14:22,639 --> 01:14:28,880
know instead of thinking about model

1866
01:14:25,840 --> 01:14:31,400
design so traditionally we have a

1867
01:14:28,880 --> 01:14:36,719
problem uh we have data and then we come

1868
01:14:31,400 --> 01:14:39,320
up with a model new model to to to to to

1869
01:14:36,719 --> 01:14:42,199
that's designed to capture whatever your

1870
01:14:39,320 --> 01:14:45,000
you know data is

1871
01:14:42,199 --> 01:14:46,679
about how you want to you know approach

1872
01:14:45,000 --> 01:14:49,040
your problem how you to answer that your

1873
01:14:46,679 --> 01:14:51,639
questions you create a new a new

1874
01:14:49,040 --> 01:14:53,679
system instead what if we turn this into

1875
01:14:51,639 --> 01:14:55,280
a data engineering problem we say the

1876
01:14:53,679 --> 01:14:56,880
model is already there it's just in

1877
01:14:55,280 --> 01:14:59,560
large language mob we're not going to

1878
01:14:56,880 --> 01:15:01,800
mess with it instead we're going to look

1879
01:14:59,560 --> 01:15:04,760
at our data and we're going to modify

1880
01:15:01,800 --> 01:15:06,880
our data engineer it in such a way that

1881
01:15:04,760 --> 01:15:09,600
we can feed it to the llm we're kind of

1882
01:15:06,880 --> 01:15:11,800
turning the problem around because then

1883
01:15:09,600 --> 01:15:14,000
we can use all of this we don't have to

1884
01:15:11,800 --> 01:15:16,600
create a new model if tomorrow a new

1885
01:15:14,000 --> 01:15:17,920
model comes out great we can just

1886
01:15:16,600 --> 01:15:19,800
immediately use it we don't have to

1887
01:15:17,920 --> 01:15:21,840
change anything because our data is

1888
01:15:19,800 --> 01:15:26,360
already designed to go to to work with

1889
01:15:21,840 --> 01:15:26,360
that model okay

1890
01:15:27,679 --> 01:15:33,440
so we have started doing this in our

1891
01:15:30,800 --> 01:15:36,920
framework is called cellto sentence and

1892
01:15:33,440 --> 01:15:39,400
we want to redefine single cell

1893
01:15:36,920 --> 01:15:42,480
analysis by using large language models

1894
01:15:39,400 --> 01:15:45,080
and this is not just one method it's a

1895
01:15:42,480 --> 01:15:48,719
framework that in principle allows you

1896
01:15:45,080 --> 01:15:50,639
to do every single cell analysis task

1897
01:15:48,719 --> 01:15:53,199
just through

1898
01:15:50,639 --> 01:15:55,560
llms and we would like to you know

1899
01:15:53,199 --> 01:15:58,199
convince you that this is

1900
01:15:55,560 --> 01:16:01,239
better um and there's many advantages to

1901
01:15:58,199 --> 01:16:04,040
it and it's an incredibly flexible

1902
01:16:01,239 --> 01:16:06,760
framework so just to give you a brief

1903
01:16:04,040 --> 01:16:10,360
overview we started with huge amount of

1904
01:16:06,760 --> 01:16:12,440
data more data is more better uh we

1905
01:16:10,360 --> 01:16:14,280
collect lots of single cell data but not

1906
01:16:12,440 --> 01:16:16,280
just single cell data our framework can

1907
01:16:14,280 --> 01:16:18,480
work with any kind of data as long as

1908
01:16:16,280 --> 01:16:20,360
you can convert into text we also look

1909
01:16:18,480 --> 01:16:23,880
at bulk RNA sequencing

1910
01:16:20,360 --> 01:16:26,320
data we collect any kind of annotation

1911
01:16:23,880 --> 01:16:29,000
that is available you know cell types

1912
01:16:26,320 --> 01:16:31,360
tissues drug perturbations as well as

1913
01:16:29,000 --> 01:16:34,560
sort of more elaborate natural language

1914
01:16:31,360 --> 01:16:36,719
annotations so I think Gene sets is a

1915
01:16:34,560 --> 01:16:39,040
great example of like a lot of our

1916
01:16:36,719 --> 01:16:41,199
biological knowledge is in the form of

1917
01:16:39,040 --> 01:16:44,159
Gene sets right Gene sets is a natural

1918
01:16:41,199 --> 01:16:45,480
bridge between biological language and

1919
01:16:44,159 --> 01:16:47,480
natural language because literally you

1920
01:16:45,480 --> 01:16:50,120
have a set of genes let's say these 50

1921
01:16:47,480 --> 01:16:52,000
genes are associated with Parkin dises

1922
01:16:50,120 --> 01:16:53,280
that's a bridge between biological

1923
01:16:52,000 --> 01:16:55,600
language and natural language and we've

1924
01:16:53,280 --> 01:16:57,159
done that literally to translate right

1925
01:16:55,600 --> 01:16:58,719
humans have created this because we

1926
01:16:57,159 --> 01:17:00,760
don't understand biology genes don't

1927
01:16:58,719 --> 01:17:03,480
mean anything to us so we've created

1928
01:17:00,760 --> 01:17:05,960
these Gene sets through experiments to

1929
01:17:03,480 --> 01:17:07,880
translate between bi biological language

1930
01:17:05,960 --> 01:17:09,719
and human language we can all feed this

1931
01:17:07,880 --> 01:17:11,760
into our model and then our model can

1932
01:17:09,719 --> 01:17:14,239
learn to translate we don't have to do

1933
01:17:11,760 --> 01:17:17,239
it manually anymore and of course we

1934
01:17:14,239 --> 01:17:19,840
have the papers themselves right if

1935
01:17:17,239 --> 01:17:22,480
someone if there's a a data set out

1936
01:17:19,840 --> 01:17:24,440
there is usually comes with a paper

1937
01:17:22,480 --> 01:17:26,400
where they did the analysis and there's

1938
01:17:24,440 --> 01:17:29,199
a lot of information in that paper in

1939
01:17:26,400 --> 01:17:32,480
textual form right so that seems like a

1940
01:17:29,199 --> 01:17:34,600
natural way to kind of uh see make sense

1941
01:17:32,480 --> 01:17:37,280
to kind of to leverage that data so this

1942
01:17:34,600 --> 01:17:39,400
is how we collect you know the

1943
01:17:37,280 --> 01:17:41,520
data and then the idea of sell to

1944
01:17:39,400 --> 01:17:44,719
sentence again it's a data engineering

1945
01:17:41,520 --> 01:17:46,239
problem we convert all this data into a

1946
01:17:44,719 --> 01:17:49,760
textual

1947
01:17:46,239 --> 01:17:51,679
format we train the model multitask it's

1948
01:17:49,760 --> 01:17:54,000
a foundation model so just like GPT is

1949
01:17:51,679 --> 01:17:55,560
trained on all kinds of different types

1950
01:17:54,000 --> 01:17:59,480
of data

1951
01:17:55,560 --> 01:18:02,800
um we do the same thing for example we

1952
01:17:59,480 --> 01:18:05,480
train our model on um you know gene

1953
01:18:02,800 --> 01:18:08,239
expression data plus cell type labels

1954
01:18:05,480 --> 01:18:11,000
tissue labels diseases

1955
01:18:08,239 --> 01:18:13,560
species everything this is the

1956
01:18:11,000 --> 01:18:16,920
pre-training phase and then we have an

1957
01:18:13,560 --> 01:18:19,440
inference phase where the user uses this

1958
01:18:16,920 --> 01:18:20,719
model to do whatever they want to do for

1959
01:18:19,440 --> 01:18:23,199
example they give it a cell and it

1960
01:18:20,719 --> 01:18:25,560
predicts label of a cell the cell type

1961
01:18:23,199 --> 01:18:27,560
or it interprets a cell or it translates

1962
01:18:25,560 --> 01:18:29,239
a Cell between Mouse and human or

1963
01:18:27,560 --> 01:18:32,280
predicts the response of perturbation

1964
01:18:29,239 --> 01:18:34,639
whatever you want to do you can do that

1965
01:18:32,280 --> 01:18:37,320
in fact you can use the system in a zero

1966
01:18:34,639 --> 01:18:39,120
shot way you know when you use gep chat

1967
01:18:37,320 --> 01:18:40,360
GPT it's zero shot you don't find tun

1968
01:18:39,120 --> 01:18:42,280
you just prompt it and it gives you an

1969
01:18:40,360 --> 01:18:45,520
answer but sometimes when you have a

1970
01:18:42,280 --> 01:18:47,159
more Specialized or more nuanced uh

1971
01:18:45,520 --> 01:18:49,159
difficult task you may want to fine-tune

1972
01:18:47,159 --> 01:18:50,639
your model on the test we can do that as

1973
01:18:49,159 --> 01:18:54,520
well we can find you the model on

1974
01:18:50,639 --> 01:18:58,040
whatever task you're interested in Okay

1975
01:18:54,520 --> 01:19:01,239
so at the core of selta sentence is uh

1976
01:18:58,040 --> 01:19:04,120
converting gene expression into textual

1977
01:19:01,239 --> 01:19:06,920
format and it's actually very simple we

1978
01:19:04,120 --> 01:19:09,560
simply uh create cell sentences we call

1979
01:19:06,920 --> 01:19:12,560
them cell sentences by rank ordering G

1980
01:19:09,560 --> 01:19:14,880
names by expression from high to low

1981
01:19:12,560 --> 01:19:18,800
that's it so every

1982
01:19:14,880 --> 01:19:20,679
cell becomes a cell sentence um in fact

1983
01:19:18,800 --> 01:19:23,000
we encode only the express

1984
01:19:20,679 --> 01:19:24,199
genes uh single cell data is generally

1985
01:19:23,000 --> 01:19:26,480
very sparse so let's say there's

1986
01:19:24,199 --> 01:19:28,719
thousand 2,000 Express genes we take

1987
01:19:26,480 --> 01:19:31,480
those genes and we rank them from high

1988
01:19:28,719 --> 01:19:32,679
to low so s explained that we have

1989
01:19:31,480 --> 01:19:35,159
positional encoding which is an

1990
01:19:32,679 --> 01:19:36,600
essential part of large language models

1991
01:19:35,159 --> 01:19:40,280
now in natural language the positional

1992
01:19:36,600 --> 01:19:44,760
encoding exists to

1993
01:19:40,280 --> 01:19:46,920
um to to to um encode where different

1994
01:19:44,760 --> 01:19:48,920
words occur in in in a sentence whether

1995
01:19:46,920 --> 01:19:53,320
they're close together far apart what

1996
01:19:48,920 --> 01:19:55,920
order they have Etc in our case

1997
01:19:53,320 --> 01:19:57,920
implicitly the positional encoding is

1998
01:19:55,920 --> 01:19:59,480
encoding expression because we don't

1999
01:19:57,920 --> 01:20:01,360
encode expression explicitly we just

2000
01:19:59,480 --> 01:20:03,639
rank order the genes but then the

2001
01:20:01,360 --> 01:20:06,320
positional encoding is encoding the

2002
01:20:03,639 --> 01:20:07,719
ordering and thus implicitly the rank

2003
01:20:06,320 --> 01:20:09,159
ordering is encoding expression so it's

2004
01:20:07,719 --> 01:20:11,239
a very elegant way of encoding

2005
01:20:09,159 --> 01:20:14,520
expression with these language models

2006
01:20:11,239 --> 01:20:16,280
can ask question here um so say talked

2007
01:20:14,520 --> 01:20:17,600
about two different Foundation models

2008
01:20:16,280 --> 01:20:21,400
one of them was doing the binning one of

2009
01:20:17,600 --> 01:20:23,080
them was doing the ranking yeah um so

2010
01:20:21,400 --> 01:20:25,639
question one is do you do some sort of

2011
01:20:23,080 --> 01:20:27,560
normalization like Med based

2012
01:20:25,639 --> 01:20:30,120
normalization type of stuff that so so

2013
01:20:27,560 --> 01:20:31,840
yeah for input and and I kind of skipped

2014
01:20:30,120 --> 01:20:35,920
over that but we the input is sort of

2015
01:20:31,840 --> 01:20:38,840
your standard processing by BL and but

2016
01:20:35,920 --> 01:20:41,639
actually a when you think about it a lot

2017
01:20:38,840 --> 01:20:43,239
of these processing steps you know like

2018
01:20:41,639 --> 01:20:44,560
log transformation they don't really

2019
01:20:43,239 --> 01:20:46,440
change the ordering so it doesn't really

2020
01:20:44,560 --> 01:20:48,199
matter we do it anyway because we often

2021
01:20:46,440 --> 01:20:50,320
already take data that's been

2022
01:20:48,199 --> 01:20:53,239
pre-process right we take data from like

2023
01:20:50,320 --> 01:20:55,520
CCI cell by Gene or human cell outet

2024
01:20:53,239 --> 01:20:58,920
it's already processed so we do that

2025
01:20:55,520 --> 01:21:00,560
anyway um but yeah I know that answers

2026
01:20:58,920 --> 01:21:02,840
your question J former was doing

2027
01:21:00,560 --> 01:21:04,600
something like taking the median of a

2028
01:21:02,840 --> 01:21:06,560
gene across all the cells in their

2029
01:21:04,600 --> 01:21:09,199
training data yeah um and then

2030
01:21:06,560 --> 01:21:10,920
normalizing with respect to that yeah so

2031
01:21:09,199 --> 01:21:13,080
yeah that's a good question we've

2032
01:21:10,920 --> 01:21:15,280
experimented with sort of normalization

2033
01:21:13,080 --> 01:21:17,480
because we figured

2034
01:21:15,280 --> 01:21:19,440
okay certain genes are always highly

2035
01:21:17,480 --> 01:21:21,239
expressed right so this this ranking is

2036
01:21:19,440 --> 01:21:23,520
going to be dominated by uninteresting

2037
01:21:21,239 --> 01:21:24,639
signals like ribosomal gen right so we

2038
01:21:23,520 --> 01:21:27,239
thought okay if you do some kind of

2039
01:21:24,639 --> 01:21:29,360
normalization to it turns out it doesn't

2040
01:21:27,239 --> 01:21:30,880
really help and it just adds

2041
01:21:29,360 --> 01:21:33,239
complexity

2042
01:21:30,880 --> 01:21:35,920
so this simple approach just it just

2043
01:21:33,239 --> 01:21:38,480
works so we don't need to do anything

2044
01:21:35,920 --> 01:21:39,920
anything fancy I think internally the

2045
01:21:38,480 --> 01:21:41,800
model when it sees a lot of data is

2046
01:21:39,920 --> 01:21:43,320
doing those things it'll learn that

2047
01:21:41,800 --> 01:21:45,440
certain genes are always high and it's

2048
01:21:43,320 --> 01:21:48,440
fine it doesn't it doesn't really

2049
01:21:45,440 --> 01:21:50,560
matter um

2050
01:21:48,440 --> 01:21:51,920
yeah so the nice thing is that we

2051
01:21:50,560 --> 01:21:53,360
actually we can also go back so we can

2052
01:21:51,920 --> 01:21:55,400
take single cell data convert it to our

2053
01:21:53,360 --> 01:21:57,639
cell sentences but actually convert it

2054
01:21:55,400 --> 01:22:00,360
back into expression if that's what you

2055
01:21:57,639 --> 01:22:01,159
want and the reason why we can do this

2056
01:22:00,360 --> 01:22:03,280
is

2057
01:22:01,159 --> 01:22:05,960
because the

2058
01:22:03,280 --> 01:22:07,600
rank um of the genes is highly

2059
01:22:05,960 --> 01:22:10,760
correlated with the expression in other

2060
01:22:07,600 --> 01:22:12,639
words converting gene expression to rank

2061
01:22:10,760 --> 01:22:14,320
we lose very little information as you

2062
01:22:12,639 --> 01:22:16,159
can see by the r squ measure here and

2063
01:22:14,320 --> 01:22:17,480
this is just simply using a simple

2064
01:22:16,159 --> 01:22:19,159
linear regression you can imagine you

2065
01:22:17,480 --> 01:22:22,120
could do a fancier sort of nonlinear

2066
01:22:19,159 --> 01:22:23,679
model again it's not necessary the nice

2067
01:22:22,120 --> 01:22:25,080
thing about this approach is it's simple

2068
01:22:23,679 --> 01:22:26,520
it works

2069
01:22:25,080 --> 01:22:28,280
you know why make it more complicated

2070
01:22:26,520 --> 01:22:30,000
when it's not necessary and you can see

2071
01:22:28,280 --> 01:22:33,199
here on the right we see original

2072
01:22:30,000 --> 01:22:34,560
expression um in blue and and overlay on

2073
01:22:33,199 --> 01:22:36,520
top of that is the reconstructed

2074
01:22:34,560 --> 01:22:39,320
expression and you can see that there's

2075
01:22:36,520 --> 01:22:42,159
only you know very minor

2076
01:22:39,320 --> 01:22:44,360
differences so this rank ordering

2077
01:22:42,159 --> 01:22:46,719
converting to cell sentences is is is a

2078
01:22:44,360 --> 01:22:49,440
very it's a good way to encode gene

2079
01:22:46,719 --> 01:22:52,120
expression data you lose very little

2080
01:22:49,440 --> 01:22:54,560
information okay so once we have you

2081
01:22:52,120 --> 01:22:56,560
know encoded our gene expression to cell

2082
01:22:54,560 --> 01:22:58,199
sentences we can do all kinds of things

2083
01:22:56,560 --> 01:22:59,239
we can add textual annotation to it

2084
01:22:58,199 --> 01:23:01,679
because now we're dealing with text

2085
01:22:59,239 --> 01:23:03,320
right you can any kind of text um you

2086
01:23:01,679 --> 01:23:05,560
can fine-tune your models using

2087
01:23:03,320 --> 01:23:07,639
traditional autor regressive next token

2088
01:23:05,560 --> 01:23:10,120
prediction llms just predict the next

2089
01:23:07,639 --> 01:23:13,000
token as as site was was explaining we

2090
01:23:10,120 --> 01:23:14,719
change nothing about that as far as the

2091
01:23:13,000 --> 01:23:17,040
model is concerned it's just text like

2092
01:23:14,719 --> 01:23:19,920
any other text and it's just predicting

2093
01:23:17,040 --> 01:23:22,400
the next word in our case next Gene

2094
01:23:19,920 --> 01:23:24,560
which means it's predicting expression

2095
01:23:22,400 --> 01:23:26,719
implicitly and then we can do inference

2096
01:23:24,560 --> 01:23:29,560
can generate cells from text or generate

2097
01:23:26,719 --> 01:23:31,159
text from cells complete cells Etc and

2098
01:23:29,560 --> 01:23:32,920
then finally when we have generated cell

2099
01:23:31,159 --> 01:23:34,719
sentence we can ALS as I mentioned

2100
01:23:32,920 --> 01:23:36,199
before even convert them back into

2101
01:23:34,719 --> 01:23:37,760
expression if that's what you care about

2102
01:23:36,199 --> 01:23:40,080
if you want to go back to the original

2103
01:23:37,760 --> 01:23:43,960
space no

2104
01:23:40,080 --> 01:23:46,520
problem so really we can now think about

2105
01:23:43,960 --> 01:23:49,120
single cell analysis as

2106
01:23:46,520 --> 01:23:51,320
prompting any task that you want to do

2107
01:23:49,120 --> 01:23:53,239
generate cells predict perturbations

2108
01:23:51,320 --> 01:23:56,239
interpret cells label cells whatever you

2109
01:23:53,239 --> 01:23:58,679
want to do it becomes a prompt now just

2110
01:23:56,239 --> 01:24:00,520
like you interact with GPT for example

2111
01:23:58,679 --> 01:24:03,520
you want to generate a cell from a

2112
01:24:00,520 --> 01:24:06,560
certain cell type you say here I have a

2113
01:24:03,520 --> 01:24:08,639
uh long lived plasma cell generate one

2114
01:24:06,560 --> 01:24:11,480
and it generates the cell sentence the

2115
01:24:08,639 --> 01:24:13,639
the the G names in order or we can say

2116
01:24:11,480 --> 01:24:15,440
here is a cell what cell type is it and

2117
01:24:13,639 --> 01:24:17,800
it will predict in textual form of the

2118
01:24:15,440 --> 01:24:19,239
cell type or we can even do complete

2119
01:24:17,800 --> 01:24:20,880
natural language insights you give it

2120
01:24:19,239 --> 01:24:22,840
one or multiple cells and you ask the

2121
01:24:20,880 --> 01:24:24,800
model okay what's going on there what's

2122
01:24:22,840 --> 01:24:27,040
the biology what disease am I dealing

2123
01:24:24,800 --> 01:24:29,800
what cell types are in my data so it

2124
01:24:27,040 --> 01:24:31,920
provides a completely new way of

2125
01:24:29,800 --> 01:24:35,040
interacting with genomic

2126
01:24:31,920 --> 01:24:37,520
data so we've carried out a lot of these

2127
01:24:35,040 --> 01:24:39,920
tasks we have used our model to generate

2128
01:24:37,520 --> 01:24:44,960
cells from test text so in this

2129
01:24:39,920 --> 01:24:47,040
case uh we provided it with uh a cell

2130
01:24:44,960 --> 01:24:49,239
type in a textual format and as generate

2131
01:24:47,040 --> 01:24:51,119
the cell you can see here the ground

2132
01:24:49,239 --> 01:24:52,639
truth expression of the held out cell

2133
01:24:51,119 --> 01:24:54,679
types and on the right the generated

2134
01:24:52,639 --> 01:24:56,960
expression you can see it look very

2135
01:24:54,679 --> 01:25:00,320
similar um of course we've Quantified

2136
01:24:56,960 --> 01:25:02,800
this and we have extensively benchmarked

2137
01:25:00,320 --> 01:25:04,679
this with all against state-of-the-art

2138
01:25:02,800 --> 01:25:08,000
generative single cell models as well as

2139
01:25:04,679 --> 01:25:10,400
Foundation models and we outperform

2140
01:25:08,000 --> 01:25:12,199
them um we can also go the other way

2141
01:25:10,400 --> 01:25:14,040
around we can generate text from cells

2142
01:25:12,199 --> 01:25:17,040
so in this case we gave it a a cell

2143
01:25:14,040 --> 01:25:19,880
sentence and ask it to label it to

2144
01:25:17,040 --> 01:25:24,960
predict the cell type in text formats

2145
01:25:19,880 --> 01:25:27,639
again we uh extensively Benchmark this

2146
01:25:24,960 --> 01:25:29,320
now as I mentioned

2147
01:25:27,639 --> 01:25:33,520
before

2148
01:25:29,320 --> 01:25:36,119
um llms when they're you know trained on

2149
01:25:33,520 --> 01:25:38,679
natural language they have an easier

2150
01:25:36,119 --> 01:25:40,760
time learning a new language even though

2151
01:25:38,679 --> 01:25:42,000
theyve never seen that language before

2152
01:25:40,760 --> 01:25:44,080
they understand the principles of

2153
01:25:42,000 --> 01:25:45,679
language and I mentioned that that could

2154
01:25:44,080 --> 01:25:47,239
be an advantage for learning biological

2155
01:25:45,679 --> 01:25:50,000
language because learning biology

2156
01:25:47,239 --> 01:25:51,880
starting out with human language might

2157
01:25:50,000 --> 01:25:53,960
be easier than starting out from scratch

2158
01:25:51,880 --> 01:25:54,880
and we did this experiments we train

2159
01:25:53,960 --> 01:25:57,080
model

2160
01:25:54,880 --> 01:25:59,320
that were either pre-trained on natural

2161
01:25:57,080 --> 01:26:02,280
language or we trained them from scratch

2162
01:25:59,320 --> 01:26:04,719
and we very very clearly saw a huge

2163
01:26:02,280 --> 01:26:06,880
boost when you start out with natural

2164
01:26:04,719 --> 01:26:08,880
language so that raises an interesting

2165
01:26:06,880 --> 01:26:10,679
idea that you know natural language

2166
01:26:08,880 --> 01:26:12,080
helps understanding biological language

2167
01:26:10,679 --> 01:26:13,520
it means that there's some similarity

2168
01:26:12,080 --> 01:26:15,760
between our language and the language of

2169
01:26:13,520 --> 01:26:18,000
biology which I think is encouraging can

2170
01:26:15,760 --> 01:26:20,560
I quickly ask um yeah here is the

2171
01:26:18,000 --> 01:26:25,199
accuracy based on reconstructing the

2172
01:26:20,560 --> 01:26:26,880
order of the Gen um

2173
01:26:25,199 --> 01:26:29,840
I think this is a cell label prediction

2174
01:26:26,880 --> 01:26:31,960
task okay yeah but we see this across

2175
01:26:29,840 --> 01:26:33,920
the board in any kind of task there's a

2176
01:26:31,960 --> 01:26:35,440
huge advantage and we know that this is

2177
01:26:33,920 --> 01:26:37,760
the case between other modalities for

2178
01:26:35,440 --> 01:26:40,480
example pre-training on text and doing

2179
01:26:37,760 --> 01:26:42,400
fine-tuning on images or vice versa we

2180
01:26:40,480 --> 01:26:44,119
we we knew that this was the case but we

2181
01:26:42,400 --> 01:26:46,840
didn't know it that was the case in

2182
01:26:44,119 --> 01:26:49,960
biology so I think that's

2183
01:26:46,840 --> 01:26:51,360
encouraging so who here in the audience

2184
01:26:49,960 --> 01:26:54,480
is a bioinformatician or does

2185
01:26:51,360 --> 01:26:56,639
bioinformatics uh work

2186
01:26:54,480 --> 01:26:58,639
yeah so I have some bad news for you

2187
01:26:56,639 --> 01:27:00,119
because uh I'm going to take Delta send

2188
01:26:58,639 --> 01:27:04,199
is going to take your job

2189
01:27:00,119 --> 01:27:07,520
away because we kind of set out um with

2190
01:27:04,199 --> 01:27:10,239
the idea to um automate the B

2191
01:27:07,520 --> 01:27:13,440
informatician can we use this model to

2192
01:27:10,239 --> 01:27:14,840
go directly from raw data to natural

2193
01:27:13,440 --> 01:27:16,320
language insights because that's that's

2194
01:27:14,840 --> 01:27:17,760
really what we do as B informaticians

2195
01:27:16,320 --> 01:27:20,840
and I'm I'm a conversational biologist

2196
01:27:17,760 --> 01:27:22,679
myself right we have data use various

2197
01:27:20,840 --> 01:27:25,040
tools to analyze that data but

2198
01:27:22,679 --> 01:27:26,520
ultimately your goal to write a paper

2199
01:27:25,040 --> 01:27:28,679
get it published paper's natural

2200
01:27:26,520 --> 01:27:31,719
language right so the goal is what we do

2201
01:27:28,679 --> 01:27:36,040
is we go from data to papers right

2202
01:27:31,719 --> 01:27:37,960
that's really what we are um so can you

2203
01:27:36,040 --> 01:27:42,800
automate that with

2204
01:27:37,960 --> 01:27:45,239
llms and the answer is yes so what we

2205
01:27:42,800 --> 01:27:48,760
did is we trained our model fine- tuned

2206
01:27:45,239 --> 01:27:52,280
our model on data sets the form of cell

2207
01:27:48,760 --> 01:27:54,600
sentences combined with natural language

2208
01:27:52,280 --> 01:27:56,760
abstracts were associated with the study

2209
01:27:54,600 --> 01:27:58,719
but the model learns to associate the

2210
01:27:56,760 --> 01:28:00,840
data with the

2211
01:27:58,719 --> 01:28:02,880
interpretation and then at inference

2212
01:28:00,840 --> 01:28:04,920
what we did is we gave it a new data set

2213
01:28:02,880 --> 01:28:05,960
that it's never seen before and ask it

2214
01:28:04,920 --> 01:28:07,800
generate the

2215
01:28:05,960 --> 01:28:11,560
abstract and that's what we show here

2216
01:28:07,800 --> 01:28:13,960
this is one example um and it's is able

2217
01:28:11,560 --> 01:28:17,600
to interpret the data and it's able to

2218
01:28:13,960 --> 01:28:19,600
meaningfully um you know understand

2219
01:28:17,600 --> 01:28:22,000
what's going on in the data and and and

2220
01:28:19,600 --> 01:28:23,360
provide it in natural language so we

2221
01:28:22,000 --> 01:28:24,679
wanted to Benchmark this it's kind of

2222
01:28:23,360 --> 01:28:26,520
hard to Benchmark this because no one

2223
01:28:24,679 --> 01:28:27,239
has tried this before so what we did is

2224
01:28:26,520 --> 01:28:29,679
we

2225
01:28:27,239 --> 01:28:31,639
just uh gave ourselves sentences to Chad

2226
01:28:29,679 --> 01:28:33,880
gbt or other language models that were

2227
01:28:31,639 --> 01:28:35,400
not kind of trained on our data because

2228
01:28:33,880 --> 01:28:37,119
you can imagine that they probably

2229
01:28:35,400 --> 01:28:39,360
recognize some of the G names and could

2230
01:28:37,119 --> 01:28:42,800
do some kind of interpretation and they

2231
01:28:39,360 --> 01:28:45,760
were but it was really bad so this shows

2232
01:28:42,800 --> 01:28:48,080
that we can go directly from data to

2233
01:28:45,760 --> 01:28:50,080
natural language insights yeah and we

2234
01:28:48,080 --> 01:28:54,840
Benchmark this can I ask one more

2235
01:28:50,080 --> 01:28:57,440
question one examples from yeah um

2236
01:28:54,840 --> 01:28:59,719
was the originating tissue also part of

2237
01:28:57,440 --> 01:29:01,880
the training data when you're uh

2238
01:28:59,719 --> 01:29:06,440
fine-tuning the model

2239
01:29:01,880 --> 01:29:08,400
or I guess how does CS Salto sentence

2240
01:29:06,440 --> 01:29:10,400
able to tell that this is from fallopian

2241
01:29:08,400 --> 01:29:14,400
tube for example or was it already part

2242
01:29:10,400 --> 01:29:16,639
of the prompt it was most likely part of

2243
01:29:14,400 --> 01:29:20,159
the pre- training

2244
01:29:16,639 --> 01:29:22,920
uh uh well it it it didn't know so it

2245
01:29:20,159 --> 01:29:25,199
has it might have seen similar data but

2246
01:29:22,920 --> 01:29:27,600
remember we're starting out with a

2247
01:29:25,199 --> 01:29:30,159
natural language pre-rain model which

2248
01:29:27,600 --> 01:29:33,600
means it knows fallopian tube in natural

2249
01:29:30,159 --> 01:29:35,520
language right okay yeah but then in the

2250
01:29:33,600 --> 01:29:38,360
fine-tuning phase we're

2251
01:29:35,520 --> 01:29:39,800
associating natural language not NE not

2252
01:29:38,360 --> 01:29:42,639
this language but other natural to gene

2253
01:29:39,800 --> 01:29:44,800
expression it's able to generalize so

2254
01:29:42,639 --> 01:29:48,280
it's during training it's never seen

2255
01:29:44,800 --> 01:29:50,480
flop Tu data and but it's able to infer

2256
01:29:48,280 --> 01:29:52,040
that it's it's this is called

2257
01:29:50,480 --> 01:29:55,040
generalization right it has seen some

2258
01:29:52,040 --> 01:29:57,960
examples of natural language data and it

2259
01:29:55,040 --> 01:30:00,560
can generalize it to New to because it

2260
01:29:57,960 --> 01:30:02,280
understands biology it understands

2261
01:30:00,560 --> 01:30:04,520
natural language and it can it learns to

2262
01:30:02,280 --> 01:30:04,520
make the

2263
01:30:10,040 --> 01:30:13,639
connection this

2264
01:30:11,760 --> 01:30:18,000
knowledge

2265
01:30:13,639 --> 01:30:20,040
thanks so in this natural uh language

2266
01:30:18,000 --> 01:30:22,840
large language models there is all sorts

2267
01:30:20,040 --> 01:30:25,000
of stuff embedded in them you know

2268
01:30:22,840 --> 01:30:26,360
that's why we have dark web whatnot

2269
01:30:25,000 --> 01:30:28,679
that's why we have this this is the

2270
01:30:26,360 --> 01:30:31,400
control these models and even they're

2271
01:30:28,679 --> 01:30:34,960
way more powerful because we use gpt2

2272
01:30:31,400 --> 01:30:37,440
and this was I believe GPT 3.5 or

2273
01:30:34,960 --> 01:30:39,280
four these models were just changed on

2274
01:30:37,440 --> 01:30:42,320
texting that's why we we expected them

2275
01:30:39,280 --> 01:30:43,800
to do something not completely random

2276
01:30:42,320 --> 01:30:46,480
because they'll recognize some Gene

2277
01:30:43,800 --> 01:30:48,639
names and be able to interpret it but

2278
01:30:46,480 --> 01:30:50,480
this shows and we did many examples and

2279
01:30:48,639 --> 01:30:52,760
we Quantified this that they do a really

2280
01:30:50,480 --> 01:30:56,080
poor job at interpreting it and this is

2281
01:30:52,760 --> 01:30:58,159
gp4 versus G PT R gpt2 that's fine-tuned

2282
01:30:56,080 --> 01:31:00,520
on Cell

2283
01:30:58,159 --> 01:31:02,719
sentences um and and and we made sure

2284
01:31:00,520 --> 01:31:04,280
that our model has never seen this B

2285
01:31:02,719 --> 01:31:07,719
this gene expression data single cell

2286
01:31:04,280 --> 01:31:07,719
data so it was not in the training

2287
01:31:08,760 --> 01:31:15,639
center there wasn't even single cell

2288
01:31:11,400 --> 01:31:17,239
data that was like I mean it probably

2289
01:31:15,639 --> 01:31:18,760
was yeah but what is like that's that's

2290
01:31:17,239 --> 01:31:21,520
the whole point right yes it has seen

2291
01:31:18,760 --> 01:31:22,639
similar things similar cell types

2292
01:31:21,520 --> 01:31:25,040
similar panels but that's the whole

2293
01:31:22,639 --> 01:31:26,400
point right it can kind of

2294
01:31:25,040 --> 01:31:30,320
repurpose that information in a

2295
01:31:26,400 --> 01:31:32,520
meaningful way recombine it to make

2296
01:31:30,320 --> 01:31:35,520
these new

2297
01:31:32,520 --> 01:31:35,520
inferences

2298
01:31:35,560 --> 01:31:40,560
thanks so another advantage of our model

2299
01:31:38,440 --> 01:31:42,560
is that it's kind of species

2300
01:31:40,560 --> 01:31:44,280
agnostic it doesn't care if you give it

2301
01:31:42,560 --> 01:31:47,480
a cell sentence that comes from a mouse

2302
01:31:44,280 --> 01:31:50,199
from a human cell or a mouse cell it's

2303
01:31:47,480 --> 01:31:51,960
just Gene names rank ordered and that's

2304
01:31:50,199 --> 01:31:54,360
what we did we train it on Mouse data

2305
01:31:51,960 --> 01:31:57,440
and human data

2306
01:31:54,360 --> 01:31:58,800
but we gave it zero information on how

2307
01:31:57,440 --> 01:32:01,080
Mouse and human are

2308
01:31:58,800 --> 01:32:03,119
related so we didn't give it any

2309
01:32:01,080 --> 01:32:04,639
information on sort of homology between

2310
01:32:03,119 --> 01:32:06,159
genes or anything we just gave it

2311
01:32:04,639 --> 01:32:08,880
separately Mouse cells human cells

2312
01:32:06,159 --> 01:32:10,360
that's it and then we asked okay what

2313
01:32:08,880 --> 01:32:11,920
happens now if you take Mouse and human

2314
01:32:10,360 --> 01:32:13,159
cells and you kind of embed them in the

2315
01:32:11,920 --> 01:32:16,040
same space

2316
01:32:13,159 --> 01:32:20,199
because we can extract latent embeddings

2317
01:32:16,040 --> 01:32:21,679
from our model um simply by extracting

2318
01:32:20,199 --> 01:32:23,040
really a sentence embedding and this is

2319
01:32:21,679 --> 01:32:25,360
an idea that's been used in language

2320
01:32:23,040 --> 01:32:27,800
modeling is you encode a sentence and

2321
01:32:25,360 --> 01:32:29,679
you just extract the embedding from that

2322
01:32:27,800 --> 01:32:32,520
sentence in our case a sentence is a

2323
01:32:29,679 --> 01:32:35,280
cell and we can extract that embedding

2324
01:32:32,520 --> 01:32:36,600
of a cell regardless if it's a human or

2325
01:32:35,280 --> 01:32:40,760
Mouse cell and that's what we did here

2326
01:32:36,600 --> 01:32:43,360
so we took data uh this is banre data

2327
01:32:40,760 --> 01:32:45,480
that was uh I think measured in the same

2328
01:32:43,360 --> 01:32:47,360
lab so it was kind of controlled in a

2329
01:32:45,480 --> 01:32:50,280
sense that they use the same technology

2330
01:32:47,360 --> 01:32:51,639
to measure Mouse human bankas and we

2331
01:32:50,280 --> 01:32:53,199
co-embedded the data this is what it

2332
01:32:51,639 --> 01:32:54,880
looks like the first all you can see it

2333
01:32:53,199 --> 01:32:57,320
kind of has a similar shape but they're

2334
01:32:54,880 --> 01:32:59,159
not overlapping we don't expect them to

2335
01:32:57,320 --> 01:33:01,080
overlap because Mouse is not human right

2336
01:32:59,159 --> 01:33:02,679
they should be separate but there should

2337
01:33:01,080 --> 01:33:05,280
be some similarity and that's what we

2338
01:33:02,679 --> 01:33:06,920
see here it's colored by cell types you

2339
01:33:05,280 --> 01:33:09,280
can see that they kind of order in the

2340
01:33:06,920 --> 01:33:11,600
same way now what we can do is something

2341
01:33:09,280 --> 01:33:13,960
interesting because now Mouse and human

2342
01:33:11,600 --> 01:33:17,639
exist in the same space and we can think

2343
01:33:13,960 --> 01:33:20,800
of translation between species as a

2344
01:33:17,639 --> 01:33:22,920
batch correction or matching problem so

2345
01:33:20,800 --> 01:33:24,239
what we did is we took optimal transport

2346
01:33:22,920 --> 01:33:26,840
which is a way to match the

2347
01:33:24,239 --> 01:33:29,760
distributions right as I also explained

2348
01:33:26,840 --> 01:33:32,840
or presented in the previous project and

2349
01:33:29,760 --> 01:33:35,920
we ask okay try to map the human cells

2350
01:33:32,840 --> 01:33:37,960
to the mouse cells and then we ask does

2351
01:33:35,920 --> 01:33:41,760
it correctly line up the cell type and

2352
01:33:37,960 --> 01:33:44,960
for a large part it does this join space

2353
01:33:41,760 --> 01:33:46,960
allows you to align between different

2354
01:33:44,960 --> 01:33:48,560
species in a meaningful way and it

2355
01:33:46,960 --> 01:33:49,760
allows for a jointed analysis make

2356
01:33:48,560 --> 01:33:51,560
things really easy because you don't

2357
01:33:49,760 --> 01:33:53,960
have to get think anymore this is mouse

2358
01:33:51,560 --> 01:33:56,400
this is human how do you you know oh

2359
01:33:53,960 --> 01:33:57,760
these genes relate these don't now you

2360
01:33:56,400 --> 01:34:00,600
have a shared space where you can do

2361
01:33:57,760 --> 01:34:03,760
shared analysis and kind of derive your

2362
01:34:00,600 --> 01:34:06,480
your your insights in a much more

2363
01:34:03,760 --> 01:34:10,560
convenient

2364
01:34:06,480 --> 01:34:13,040
way um excuse me I have a question yeah

2365
01:34:10,560 --> 01:34:14,719
uh can you go back to the slide where uh

2366
01:34:13,040 --> 01:34:18,320
you had this gene

2367
01:34:14,719 --> 01:34:21,040
expression uh profile and you were

2368
01:34:18,320 --> 01:34:22,600
trying to predict the cell type um so

2369
01:34:21,040 --> 01:34:27,119
you had a sequence of

2370
01:34:22,600 --> 01:34:27,119
genes um um

2371
01:34:27,760 --> 01:34:33,280
back yeah this so for for example at

2372
01:34:30,800 --> 01:34:36,400
first time uh you would give a sequence

2373
01:34:33,280 --> 01:34:39,119
of genes and would the cell type be the

2374
01:34:36,400 --> 01:34:39,920
cell type which maximizes the likelihood

2375
01:34:39,119 --> 01:34:43,400
of that

2376
01:34:39,920 --> 01:34:44,760
sequence no we we simply do inference in

2377
01:34:43,400 --> 01:34:47,360
a generative sense so we give it the

2378
01:34:44,760 --> 01:34:49,320
self sentence and then ask the model to

2379
01:34:47,360 --> 01:34:53,040
complet sentence so it generates the

2380
01:34:49,320 --> 01:34:54,600
next tokens the next words and that's we

2381
01:34:53,040 --> 01:34:58,760
interpret that as a cell type so it'll

2382
01:34:54,600 --> 01:35:02,639
say oh this cell is likely is a cd4t

2383
01:34:58,760 --> 01:35:04,800
cell so it just produces text and then

2384
01:35:02,639 --> 01:35:07,760
we can look at that text and see is that

2385
01:35:04,800 --> 01:35:10,320
the correct cell type or

2386
01:35:07,760 --> 01:35:13,560
not just like when you use chat GPT you

2387
01:35:10,320 --> 01:35:15,360
give it a prompt and the model does next

2388
01:35:13,560 --> 01:35:17,679
token prediction it just completes your

2389
01:35:15,360 --> 01:35:20,199
it generates words uh to complete your

2390
01:35:17,679 --> 01:35:22,000
prompt which you know can be an answer

2391
01:35:20,199 --> 01:35:23,400
to a question or whatever it is we do in

2392
01:35:22,000 --> 01:35:26,080
the same way we give it a cell sentence

2393
01:35:23,400 --> 01:35:28,360
and then the completion is predicting

2394
01:35:26,080 --> 01:35:30,520
the cell type in this case so it's going

2395
01:35:28,360 --> 01:35:32,239
to complete the the sequence based on

2396
01:35:30,520 --> 01:35:35,440
the cell type that it has learned

2397
01:35:32,239 --> 01:35:35,440
correct exactly

2398
01:35:39,760 --> 01:35:44,719
exactly

2399
01:35:41,480 --> 01:35:46,920
okay we can also do perturbation

2400
01:35:44,719 --> 01:35:48,920
response prediction so this is actually

2401
01:35:46,920 --> 01:35:50,520
the same data set as I presented before

2402
01:35:48,920 --> 01:35:53,119
this is from the Cino paper so this is

2403
01:35:50,520 --> 01:35:55,560
the combinatorial cyto stimulation so we

2404
01:35:53,119 --> 01:35:57,800
were able to repurpose our own data but

2405
01:35:55,560 --> 01:36:00,080
in this case we're using it in a

2406
01:35:57,800 --> 01:36:01,800
generative sense so in cinemo we did

2407
01:36:00,080 --> 01:36:04,960
analysis right we just looked at the

2408
01:36:01,800 --> 01:36:07,239
data and and made some inferences here

2409
01:36:04,960 --> 01:36:09,000
we want to generate so what we did is we

2410
01:36:07,239 --> 01:36:11,719
train our model or find you in our

2411
01:36:09,000 --> 01:36:16,119
cendence model on a

2412
01:36:11,719 --> 01:36:17,679
subset of the perturbations and then ask

2413
01:36:16,119 --> 01:36:19,119
can you predict the Unseen perturbation

2414
01:36:17,679 --> 01:36:22,000
because really the question here is does

2415
01:36:19,119 --> 01:36:25,199
the model generalize can it make

2416
01:36:22,000 --> 01:36:27,280
meaningful inferences about data or

2417
01:36:25,199 --> 01:36:30,040
conditions it's never seen

2418
01:36:27,280 --> 01:36:32,119
before right so you might train it on

2419
01:36:30,040 --> 01:36:33,760
interferent beta in I6 and then you

2420
01:36:32,119 --> 01:36:35,920
would ask okay can you predict what

2421
01:36:33,760 --> 01:36:37,440
happens when you can combine them and

2422
01:36:35,920 --> 01:36:41,159
that's how we

2423
01:36:37,440 --> 01:36:44,159
did um and as you can see here first of

2424
01:36:41,159 --> 01:36:45,639
all in general the generated data is

2425
01:36:44,159 --> 01:36:47,639
very similar to the ground truth data

2426
01:36:45,639 --> 01:36:50,159
they overlap and we can even look at

2427
01:36:47,639 --> 01:36:54,199
specific perturbations or cell types

2428
01:36:50,159 --> 01:36:55,800
here uh forgot what that was um and we

2429
01:36:54,199 --> 01:36:57,760
can quantify it we can literally ask

2430
01:36:55,800 --> 01:36:59,520
okay when you predict these unseen

2431
01:36:57,760 --> 01:37:02,000
combinations again it was not models

2432
01:36:59,520 --> 01:37:04,080
never seen those do the cells look like

2433
01:37:02,000 --> 01:37:05,679
the ground truth and they do and we've

2434
01:37:04,080 --> 01:37:07,440
extensively benchmarked this and it

2435
01:37:05,679 --> 01:37:10,159
works really

2436
01:37:07,440 --> 01:37:12,719
well um we've also done this on a mouse

2437
01:37:10,159 --> 01:37:18,080
cocine data set so this is uh data

2438
01:37:12,719 --> 01:37:20,119
generated um from a paper in by near aen

2439
01:37:18,080 --> 01:37:22,440
um so this is a large number I think

2440
01:37:20,119 --> 01:37:24,320
more than a 100 cyto kind perturbations

2441
01:37:22,440 --> 01:37:25,760
in Mouse lymph noes

2442
01:37:24,320 --> 01:37:28,320
so this is not

2443
01:37:25,760 --> 01:37:30,480
combinatorial however the labels are

2444
01:37:28,320 --> 01:37:32,480
combinatorial because every cell has two

2445
01:37:30,480 --> 01:37:34,199
labels it has a cell type label and a

2446
01:37:32,480 --> 01:37:37,119
perturbation so we were still able to do

2447
01:37:34,199 --> 01:37:39,119
a combinatorial experiment where we

2448
01:37:37,119 --> 01:37:41,760
basically split the data these

2449
01:37:39,119 --> 01:37:43,440
perturbations into 50 in a 50/50 split

2450
01:37:41,760 --> 01:37:47,199
so we train the model on half of the

2451
01:37:43,440 --> 01:37:48,800
cell type U cyrine combinations and then

2452
01:37:47,199 --> 01:37:52,880
we do inference on the the ones that we

2453
01:37:48,800 --> 01:37:55,440
held out and as you can see um it works

2454
01:37:52,880 --> 01:37:55,440
incredibly well

2455
01:37:56,119 --> 01:38:02,480
well so when you think about language

2456
01:37:59,239 --> 01:38:05,679
models um they don't just model

2457
01:38:02,480 --> 01:38:08,199
individual sentences right usually you

2458
01:38:05,679 --> 01:38:10,679
provide it with at least a paragraph or

2459
01:38:08,199 --> 01:38:12,599
a document or S was saying that the test

2460
01:38:10,679 --> 01:38:13,880
long context models by summar

2461
01:38:12,599 --> 01:38:15,679
summarizing whole

2462
01:38:13,880 --> 01:38:17,760
books

2463
01:38:15,679 --> 01:38:19,599
context it's all about context the

2464
01:38:17,760 --> 01:38:22,159
meaning is in the context right the

2465
01:38:19,599 --> 01:38:24,080
meaning of a word depends on the context

2466
01:38:22,159 --> 01:38:26,239
of the sentence the meaning of a

2467
01:38:24,080 --> 01:38:29,000
sentence depends on the context of the

2468
01:38:26,239 --> 01:38:32,320
document you can have two identical

2469
01:38:29,000 --> 01:38:34,239
sentences occurring in different

2470
01:38:32,320 --> 01:38:35,520
documents or books where they have

2471
01:38:34,239 --> 01:38:38,880
different

2472
01:38:35,520 --> 01:38:41,840
meanings so

2473
01:38:38,880 --> 01:38:43,599
why uh shouldn't we model cells like

2474
01:38:41,840 --> 01:38:45,639
this the same

2475
01:38:43,599 --> 01:38:48,360
cell might or at least in terms of a

2476
01:38:45,639 --> 01:38:50,599
gene expression pattern two cells might

2477
01:38:48,360 --> 01:38:51,760
look identical but they occur in

2478
01:38:50,599 --> 01:38:53,880
different tissues and they might do

2479
01:38:51,760 --> 01:38:55,159
different things one one of the reasons

2480
01:38:53,880 --> 01:38:56,760
is because we don't measure everything

2481
01:38:55,159 --> 01:38:58,040
right single cell sequencing it's it's a

2482
01:38:56,760 --> 01:39:00,320
high dimensional measurement but you're

2483
01:38:58,040 --> 01:39:02,920
only measuring RNA right there's so much

2484
01:39:00,320 --> 01:39:07,000
you're not measuring

2485
01:39:02,920 --> 01:39:08,040
so the context really determines for a

2486
01:39:07,000 --> 01:39:10,360
large

2487
01:39:08,040 --> 01:39:14,920
part what the meaning what a cell does

2488
01:39:10,360 --> 01:39:17,400
the function of a cell now our model our

2489
01:39:14,920 --> 01:39:20,400
framework makes it really easy to model

2490
01:39:17,400 --> 01:39:22,599
cellular context because instead of

2491
01:39:20,400 --> 01:39:23,880
providing it one cell sentence we can

2492
01:39:22,599 --> 01:39:27,679
provide it five

2493
01:39:23,880 --> 01:39:29,159
or 10 or 10,000 cell sentences Al

2494
01:39:27,679 --> 01:39:33,080
together that were collected from the

2495
01:39:29,159 --> 01:39:34,679
same data set same tissue maybe a

2496
01:39:33,080 --> 01:39:37,880
spatial region when you're doing spatial

2497
01:39:34,679 --> 01:39:39,840
transcriptomics whatever your context is

2498
01:39:37,880 --> 01:39:42,440
you can encode those cells together in

2499
01:39:39,840 --> 01:39:44,440
our model really easily and then the

2500
01:39:42,440 --> 01:39:46,440
model will understand it will model the

2501
01:39:44,440 --> 01:39:48,560
context and it will learn

2502
01:39:46,440 --> 01:39:50,840
relationships between the cells between

2503
01:39:48,560 --> 01:39:53,760
the genes between different cells Etc

2504
01:39:50,840 --> 01:39:55,199
exactly like how natural language works

2505
01:39:53,760 --> 01:39:57,960
so we've tried this and actually we have

2506
01:39:55,199 --> 01:40:00,760
built the first

2507
01:39:57,960 --> 01:40:02,960
multi-cellular context Foundation model

2508
01:40:00,760 --> 01:40:05,639
where we encoded either one three or

2509
01:40:02,960 --> 01:40:08,280
five cells in one context and um this

2510
01:40:05,639 --> 01:40:10,199
task is a um it's an annotation I think

2511
01:40:08,280 --> 01:40:13,560
it's cell type and tissue tissue

2512
01:40:10,199 --> 01:40:15,960
prediction task and you can see that the

2513
01:40:13,560 --> 01:40:17,360
performance gets a huge boost when

2514
01:40:15,960 --> 01:40:19,360
you're modeling cont and this makes

2515
01:40:17,360 --> 01:40:21,480
sense right it's kind of obvious if you

2516
01:40:19,360 --> 01:40:23,320
give it five cells from a tissue versus

2517
01:40:21,480 --> 01:40:25,280
one obviously there's more information

2518
01:40:23,320 --> 01:40:27,960
information to five cells so it's

2519
01:40:25,280 --> 01:40:30,520
obvious on the one hand but

2520
01:40:27,960 --> 01:40:33,360
also um yeah no one has showed this

2521
01:40:30,520 --> 01:40:33,360
before and it

2522
01:40:34,159 --> 01:40:39,159
works

2523
01:40:36,320 --> 01:40:41,840
okay so

2524
01:40:39,159 --> 01:40:43,560
um yeah so today I talked about this

2525
01:40:41,840 --> 01:40:46,719
idea of bi biology as a language or

2526
01:40:43,560 --> 01:40:48,639
having multiple languages uh for example

2527
01:40:46,719 --> 01:40:51,480
you know cells communicating through

2528
01:40:48,639 --> 01:40:53,760
complex structured signals uh you know

2529
01:40:51,480 --> 01:40:56,159
the idea of cyto kindes chemokine

2530
01:40:53,760 --> 01:40:58,960
functioning as words and together their

2531
01:40:56,159 --> 01:41:01,440
sentences uh genes as words cells as

2532
01:40:58,960 --> 01:41:03,199
sentences tissues as kind of

2533
01:41:01,440 --> 01:41:06,080
documents and then first I talked about

2534
01:41:03,199 --> 01:41:08,880
cinemo this causal inference framework

2535
01:41:06,080 --> 01:41:10,239
where we can sort of uncover Synergy

2536
01:41:08,880 --> 01:41:11,840
which is kind of a fundamental building

2537
01:41:10,239 --> 01:41:13,960
block of

2538
01:41:11,840 --> 01:41:16,520
language uh by quantifying these

2539
01:41:13,960 --> 01:41:18,040
nonlinear interactions between the cyto

2540
01:41:16,520 --> 01:41:19,840
and then I talked about our framework

2541
01:41:18,040 --> 01:41:22,199
sell to sentence where we model cells as

2542
01:41:19,840 --> 01:41:24,520
text where instead of building models we

2543
01:41:22,199 --> 01:41:26,360
do data engineering

2544
01:41:24,520 --> 01:41:27,119
um and we're kind of redefining single

2545
01:41:26,360 --> 01:41:29,239
cell

2546
01:41:27,119 --> 01:41:31,560
analysis by using

2547
01:41:29,239 --> 01:41:33,480
llms um which allows us to interpret

2548
01:41:31,560 --> 01:41:35,520
cells generate cells predicts

2549
01:41:33,480 --> 01:41:38,920
perturbations perform cross species

2550
01:41:35,520 --> 01:41:41,280
multi-species analysis and even uh model

2551
01:41:38,920 --> 01:41:44,400
multicellular context as I just

2552
01:41:41,280 --> 01:41:47,520
showed so with that I want to thank you

2553
01:41:44,400 --> 01:41:50,119
for listening and uh thank the amazing

2554
01:41:47,520 --> 01:41:51,800
members of my lab collaborators uh

2555
01:41:50,119 --> 01:41:53,639
sources of funding we're now

2556
01:41:51,800 --> 01:41:55,119
collaborating with Google research to

2557
01:41:53,639 --> 01:41:58,280
scale up our models because you can

2558
01:41:55,119 --> 01:42:01,480
imagine if you want to go big S it shows

2559
01:41:58,280 --> 01:42:04,480
bigger is often better uh we need the

2560
01:42:01,480 --> 01:42:08,920
resources uh so we need partners for

2561
01:42:04,480 --> 01:42:11,159
that and um yeah code is on GitHub we

2562
01:42:08,920 --> 01:42:13,920
share our models and hugging face uh we

2563
01:42:11,159 --> 01:42:16,280
have tutorials API so this is not a

2564
01:42:13,920 --> 01:42:18,080
single project this is or a single

2565
01:42:16,280 --> 01:42:20,239
method it's a new framework so if you're

2566
01:42:18,080 --> 01:42:22,960
interested in contributing finding new

2567
01:42:20,239 --> 01:42:26,080
applications of our method or just you

2568
01:42:22,960 --> 01:42:28,000
know working with us or using it please

2569
01:42:26,080 --> 01:42:33,000
reach out we kind of want to build a

2570
01:42:28,000 --> 01:42:33,000
community around this technology thank

2571
01:42:52,119 --> 01:42:56,159
you e

