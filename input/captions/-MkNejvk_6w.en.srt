1
00:00:01,160 --> 00:00:05,400
all right good

2
00:00:03,480 --> 00:00:06,600
afternoon we we can do better than that

3
00:00:05,400 --> 00:00:11,040
good

4
00:00:06,600 --> 00:00:13,679
afternoon excellent all right so I want

5
00:00:11,040 --> 00:00:16,760
to talk to you today about sourcing

6
00:00:13,679 --> 00:00:19,080
Innovation around Ai and using AI as a

7
00:00:16,760 --> 00:00:21,279
tool for that and for those of you uh

8
00:00:19,080 --> 00:00:22,720
who've met with me I know I've met with

9
00:00:21,279 --> 00:00:25,680
number of groups over the last five or

10
00:00:22,720 --> 00:00:27,560
10 years a bunch of the work that I that

11
00:00:25,680 --> 00:00:30,000
I did at that point one of the things we

12
00:00:27,560 --> 00:00:32,320
saw was this increasing usage by firm

13
00:00:30,000 --> 00:00:34,480
terms of external Innovation sources for

14
00:00:32,320 --> 00:00:36,719
digital innovation in particular right

15
00:00:34,480 --> 00:00:38,239
we saw that historically right there was

16
00:00:36,719 --> 00:00:39,800
this much bigger mix where sometimes you

17
00:00:38,239 --> 00:00:42,200
were innovating internally you might

18
00:00:39,800 --> 00:00:44,600
have your um like your business units

19
00:00:42,200 --> 00:00:46,399
doing stuff but that as we got to very

20
00:00:44,600 --> 00:00:48,520
technical things it became increasingly

21
00:00:46,399 --> 00:00:50,760
important to Source Innovation from

22
00:00:48,520 --> 00:00:52,800
universities from third parties that had

23
00:00:50,760 --> 00:00:54,920
particular knowledge in this and so what

24
00:00:52,800 --> 00:00:57,079
I'm going to really talk about today is

25
00:00:54,920 --> 00:00:59,000
that in the context of AI right you you

26
00:00:57,079 --> 00:01:01,879
want to innovate you want to use AI as a

27
00:00:59,000 --> 00:01:04,119
tool to in your company but you run into

28
00:01:01,879 --> 00:01:06,240
this problem which is there's a whole

29
00:01:04,119 --> 00:01:08,200
bunch of expertise needed for that and

30
00:01:06,240 --> 00:01:09,799
so is it worth it should you invest

31
00:01:08,200 --> 00:01:12,000
should you build a bunch of capabilities

32
00:01:09,799 --> 00:01:13,799
internally to do that that might be

33
00:01:12,000 --> 00:01:15,320
expensive right should you adopt

34
00:01:13,799 --> 00:01:17,280
something off the shelf well there might

35
00:01:15,320 --> 00:01:18,520
be some drawbacks to that too and so I

36
00:01:17,280 --> 00:01:21,240
want to think about those kind of

37
00:01:18,520 --> 00:01:22,640
tradeoffs and give you some sense of how

38
00:01:21,240 --> 00:01:24,479
AI is developing and how you should

39
00:01:22,640 --> 00:01:27,600
think about that if you want to be using

40
00:01:24,479 --> 00:01:30,400
it to help do Innovation

41
00:01:27,600 --> 00:01:32,640
okay so I want to start with just a

42
00:01:30,400 --> 00:01:35,000
couple of examples of the remarkable

43
00:01:32,640 --> 00:01:37,360
success that AI has had in helping with

44
00:01:35,000 --> 00:01:38,320
Innovation and I think um this one is

45
00:01:37,360 --> 00:01:40,960
you know

46
00:01:38,320 --> 00:01:43,360
both uh is very important right protein

47
00:01:40,960 --> 00:01:45,240
folding this is the 2018 Discovery by

48
00:01:43,360 --> 00:01:47,880
the Deep Mind Group of being able to do

49
00:01:45,240 --> 00:01:49,719
incredibly good job with this this was

50
00:01:47,880 --> 00:01:50,840
sort of remarkable in the sense that

51
00:01:49,719 --> 00:01:52,600
this was a problem that had

52
00:01:50,840 --> 00:01:54,880
traditionally been enormously hard

53
00:01:52,600 --> 00:01:57,320
people had spent a lot of time and a lot

54
00:01:54,880 --> 00:01:59,000
of computer Cycles trying to do a good

55
00:01:57,320 --> 00:02:00,439
job at this and had really not been that

56
00:01:59,000 --> 00:02:02,880
successful

57
00:02:00,439 --> 00:02:04,520
then there was actually a whole bunch of

58
00:02:02,880 --> 00:02:06,079
initiatives that were out that said

59
00:02:04,520 --> 00:02:08,039
humans were actually still better at

60
00:02:06,079 --> 00:02:10,039
these things than computers let's use

61
00:02:08,039 --> 00:02:12,000
humans for this right and so there was

62
00:02:10,039 --> 00:02:13,879
something called folded at home and

63
00:02:12,000 --> 00:02:15,599
other techniques where people would get

64
00:02:13,879 --> 00:02:18,319
do that Innovation

65
00:02:15,599 --> 00:02:22,000
themselves they were still better not

66
00:02:18,319 --> 00:02:23,920
than this system right in this the Deep

67
00:02:22,000 --> 00:02:26,440
Mind crew did a number of specific

68
00:02:23,920 --> 00:02:28,599
things so first of all they brought in

69
00:02:26,440 --> 00:02:30,599
these these deep Learning Systems these

70
00:02:28,599 --> 00:02:32,640
new AI systems that were able ble to do

71
00:02:30,599 --> 00:02:35,080
things sort of more similarly to the way

72
00:02:32,640 --> 00:02:36,920
that humans had thought about that and

73
00:02:35,080 --> 00:02:38,440
also equally important they brought in

74
00:02:36,920 --> 00:02:40,519
something like a factor of a thousand

75
00:02:38,440 --> 00:02:42,360
times more compute than people had used

76
00:02:40,519 --> 00:02:44,040
to solve that problem before and that

77
00:02:42,360 --> 00:02:48,480
was also incredibly valuable for doing a

78
00:02:44,040 --> 00:02:50,239
good job on this okay so big success

79
00:02:48,480 --> 00:02:52,680
there we

80
00:02:50,239 --> 00:02:54,640
see this is just now last year the

81
00:02:52,680 --> 00:02:56,159
newest version of chat gbd passes the US

82
00:02:54,640 --> 00:02:58,239
medical licensing exam with flying

83
00:02:56,159 --> 00:03:00,879
colors and diagnosed a one in a thousand

84
00:02:58,239 --> 00:03:03,239
condition in seconds okay this is you

85
00:03:00,879 --> 00:03:05,959
know again in this sort of area of

86
00:03:03,239 --> 00:03:07,599
science um most recently AI achieves a

87
00:03:05,959 --> 00:03:09,920
silver medal standard solving

88
00:03:07,599 --> 00:03:12,360
International mathematical Olympiad

89
00:03:09,920 --> 00:03:14,440
problems okay so this is this is

90
00:03:12,360 --> 00:03:16,760
interesting this is kind of a part of a

91
00:03:14,440 --> 00:03:19,120
broader phenomenon that we're now seeing

92
00:03:16,760 --> 00:03:21,120
of spending a lot more time at imprints

93
00:03:19,120 --> 00:03:23,040
right so for those of you who know the

94
00:03:21,120 --> 00:03:26,280
um uh

95
00:03:23,040 --> 00:03:28,519
gp4 uh 01 right has this property that

96
00:03:26,280 --> 00:03:30,159
not only does it try and solve a problem

97
00:03:28,519 --> 00:03:31,400
right in a way that large language model

98
00:03:30,159 --> 00:03:34,120
do in the way that we're sort of used to

99
00:03:31,400 --> 00:03:35,480
them doing with chat GPT but it'll try

100
00:03:34,120 --> 00:03:37,439
an answer and then it might actually

101
00:03:35,480 --> 00:03:39,760
sort of reflect on it so it might ask

102
00:03:37,439 --> 00:03:42,360
this itself like does this seem like a

103
00:03:39,760 --> 00:03:44,280
good answer to the question you asked

104
00:03:42,360 --> 00:03:45,480
and it turns out that even though it

105
00:03:44,280 --> 00:03:47,159
just produced that answer sometimes

106
00:03:45,480 --> 00:03:49,400
it'll say like no that doesn't seem like

107
00:03:47,159 --> 00:03:51,200
a good answer like somehow by reflecting

108
00:03:49,400 --> 00:03:53,480
it does not notices that this is not a

109
00:03:51,200 --> 00:03:55,760
good answer and it can go back and say

110
00:03:53,480 --> 00:03:57,640
okay well you know why was it not a good

111
00:03:55,760 --> 00:03:59,879
answer and then sort of like give itself

112
00:03:57,640 --> 00:04:02,159
almost like another prompt to to try and

113
00:03:59,879 --> 00:04:04,120
to a better answer so that's a

114
00:04:02,159 --> 00:04:05,400
particular sort of narrow example of

115
00:04:04,120 --> 00:04:07,200
something that is coming in more and

116
00:04:05,400 --> 00:04:09,239
more with these mathematical problems

117
00:04:07,200 --> 00:04:10,599
where people put more structure on them

118
00:04:09,239 --> 00:04:12,720
and say like rather than trying to solve

119
00:04:10,599 --> 00:04:15,360
it all at once in one shot maybe I want

120
00:04:12,720 --> 00:04:16,959
to solve it in increments right maybe I

121
00:04:15,360 --> 00:04:19,040
know that a typical way to solve this is

122
00:04:16,959 --> 00:04:20,680
to have five steps and I try and solve

123
00:04:19,040 --> 00:04:23,160
the first step and then the second step

124
00:04:20,680 --> 00:04:25,520
and so on on long and what's been

125
00:04:23,160 --> 00:04:27,360
remarkable is we have seen in areas like

126
00:04:25,520 --> 00:04:29,720
math and science these systems doing

127
00:04:27,360 --> 00:04:31,199
better than they had done before so that

128
00:04:29,720 --> 00:04:32,000
aidea of having those steps might be

129
00:04:31,199 --> 00:04:34,080
very

130
00:04:32,000 --> 00:04:35,400
valuable okay and then this is not

131
00:04:34,080 --> 00:04:37,320
really a science example but I think

132
00:04:35,400 --> 00:04:39,639
it's a nice one just to think about it

133
00:04:37,320 --> 00:04:41,479
so this is a a beautiful example of a an

134
00:04:39,639 --> 00:04:44,560
image created by mid

135
00:04:41,479 --> 00:04:46,440
journey I bring this one up not so much

136
00:04:44,560 --> 00:04:49,000
as I say because it's a science thing

137
00:04:46,440 --> 00:04:51,039
but so much as it's a very sort of quick

138
00:04:49,000 --> 00:04:52,639
ability to produce something like this

139
00:04:51,039 --> 00:04:54,440
which we do see all over the place in

140
00:04:52,639 --> 00:04:57,360
terms of very Hands-On R&D that firms

141
00:04:54,440 --> 00:04:58,720
are doing we see them saying Oh like we

142
00:04:57,360 --> 00:05:02,000
want to design the next generation of

143
00:04:58,720 --> 00:05:03,759
car what should that car look like let's

144
00:05:02,000 --> 00:05:05,039
really rapidly prototype what different

145
00:05:03,759 --> 00:05:06,800
pictures could look like what's the next

146
00:05:05,039 --> 00:05:08,800
sneaker going to look like what are some

147
00:05:06,800 --> 00:05:10,600
rapid prototypes of that you can produce

148
00:05:08,800 --> 00:05:12,400
images with these systems so much more

149
00:05:10,600 --> 00:05:14,759
quickly that that can be very valuable

150
00:05:12,400 --> 00:05:17,880
for very Hands-On R&D that you're

151
00:05:14,759 --> 00:05:20,600
doing okay all right so with that is

152
00:05:17,880 --> 00:05:22,800
sort of in our mind of seeing this plan

153
00:05:20,600 --> 00:05:25,360
out goals for this session are going to

154
00:05:22,800 --> 00:05:27,800
be to understand why AI is being used in

155
00:05:25,360 --> 00:05:28,639
Innovation and sorry how it's being used

156
00:05:27,800 --> 00:05:31,080
and

157
00:05:28,639 --> 00:05:32,360
why and so I want to start you out with

158
00:05:31,080 --> 00:05:34,680
uh a graph that my lab has been

159
00:05:32,360 --> 00:05:37,240
producing about the use of foundation

160
00:05:34,680 --> 00:05:40,800
models in

161
00:05:37,240 --> 00:05:42,199
science so for those who um have been

162
00:05:40,800 --> 00:05:44,120
thinking about this Foundation models

163
00:05:42,199 --> 00:05:46,440
are these large AI models that have

164
00:05:44,120 --> 00:05:48,440
being built they're typically very

165
00:05:46,440 --> 00:05:50,240
expensive right and they're being used

166
00:05:48,440 --> 00:05:52,240
very broadly so chat GPT would be one

167
00:05:50,240 --> 00:05:53,840
example of this mid Journey the one I

168
00:05:52,240 --> 00:05:55,400
just talked about on the previous slide

169
00:05:53,840 --> 00:05:57,360
would be an example of this these are

170
00:05:55,400 --> 00:06:00,800
systems that you sort of say ah maybe I

171
00:05:57,360 --> 00:06:03,440
can use this off the shelf or do some

172
00:06:00,800 --> 00:06:05,360
adaptation and so what we're seeing here

173
00:06:03,440 --> 00:06:07,120
in the four different lines so this Top

174
00:06:05,360 --> 00:06:09,240
Line is probably the thing that if

175
00:06:07,120 --> 00:06:10,759
you've seen most discussions about Ai

176
00:06:09,240 --> 00:06:13,280
and science that's probably a version of

177
00:06:10,759 --> 00:06:15,599
what you've seen which is people citing

178
00:06:13,280 --> 00:06:17,319
these models so they'd say ah somebody

179
00:06:15,599 --> 00:06:19,520
cited chat gbt that must mean it's being

180
00:06:17,319 --> 00:06:21,440
used that is not true right we are

181
00:06:19,520 --> 00:06:23,199
living in an era where there's a ton of

182
00:06:21,440 --> 00:06:25,000
hype and a lot of interest around AI

183
00:06:23,199 --> 00:06:27,319
lots of people site these things without

184
00:06:25,000 --> 00:06:28,800
actually using them right so that Top

185
00:06:27,319 --> 00:06:30,919
Line is more of a reference for us to

186
00:06:28,800 --> 00:06:32,759
say okay well that's what's going on but

187
00:06:30,919 --> 00:06:35,039
the next one down which you can see is

188
00:06:32,759 --> 00:06:38,840
not being as much we're not quite at 1%

189
00:06:35,039 --> 00:06:41,639
yet but that is model use so that says I

190
00:06:38,840 --> 00:06:44,280
have I'm doing some area of Science and

191
00:06:41,639 --> 00:06:46,639
I'm going to actually draw on chat gbt

192
00:06:44,280 --> 00:06:48,280
right so maybe you have like in in your

193
00:06:46,639 --> 00:06:50,120
companies you're saying oh we're trying

194
00:06:48,280 --> 00:06:52,639
to do some drug design and we're trying

195
00:06:50,120 --> 00:06:54,280
to think about where might be there some

196
00:06:52,639 --> 00:06:55,800
other interactions that this drug might

197
00:06:54,280 --> 00:06:58,360
have and we say okay well we want to

198
00:06:55,800 --> 00:07:00,360
scan the literature to get some sense of

199
00:06:58,360 --> 00:07:01,240
does this does this article suggest that

200
00:07:00,360 --> 00:07:02,479
there might be a problem does this

201
00:07:01,240 --> 00:07:04,520
article suggest that there might be a

202
00:07:02,479 --> 00:07:06,879
problem right that would be an example

203
00:07:04,520 --> 00:07:09,479
you use the API from chat GPT and you

204
00:07:06,879 --> 00:07:11,919
could actually Analyze This okay so

205
00:07:09,479 --> 00:07:13,720
that's model uses the next one down is

206
00:07:11,919 --> 00:07:16,759
model customization so this is saying

207
00:07:13,720 --> 00:07:18,680
not take CH chat TPT off the shelf this

208
00:07:16,759 --> 00:07:20,120
is saying you take it then you train it

209
00:07:18,680 --> 00:07:21,919
on some of your own data maybe you do

210
00:07:20,120 --> 00:07:24,240
some prompt engineering to try and get

211
00:07:21,919 --> 00:07:25,960
it to perform better and then the last

212
00:07:24,240 --> 00:07:26,879
one down here is new actual new models

213
00:07:25,960 --> 00:07:29,160
being

214
00:07:26,879 --> 00:07:31,759
released so I want you to take several

215
00:07:29,160 --> 00:07:33,759
things from this graph so the first

216
00:07:31,759 --> 00:07:36,120
thing is just how big a difference there

217
00:07:33,759 --> 00:07:39,199
is between these different sources of

218
00:07:36,120 --> 00:07:40,440
things so citations very cheap right you

219
00:07:39,199 --> 00:07:43,560
can site it you can say like

220
00:07:40,440 --> 00:07:45,039
contextually this is important model use

221
00:07:43,560 --> 00:07:46,240
that's just an API that'll cost you a

222
00:07:45,039 --> 00:07:48,879
little bit of money because you can do

223
00:07:46,240 --> 00:07:51,479
it right considerably less of that but

224
00:07:48,879 --> 00:07:53,000
boy still growing very very fast and

225
00:07:51,479 --> 00:07:54,840
then model customizations that's even

226
00:07:53,000 --> 00:07:57,159
more expensive right then you actually

227
00:07:54,840 --> 00:07:59,159
have to have your own team helping to

228
00:07:57,159 --> 00:08:01,280
customize this model make it right and

229
00:07:59,159 --> 00:08:03,400
you can see again quite a drop in the

230
00:08:01,280 --> 00:08:06,199
the percentage of papers that are doing

231
00:08:03,400 --> 00:08:07,400
that okay so that's number one there are

232
00:08:06,199 --> 00:08:09,479
these big differences between these

233
00:08:07,400 --> 00:08:11,680
things the second thing is you can see

234
00:08:09,479 --> 00:08:13,319
that on the y- axis here we have the

235
00:08:11,680 --> 00:08:15,520
share of papers that are are actually

236
00:08:13,319 --> 00:08:17,240
doing these things and we can see these

237
00:08:15,520 --> 00:08:18,680
this scale is an exponential SC a

238
00:08:17,240 --> 00:08:20,000
logarithmic scale so these are

239
00:08:18,680 --> 00:08:23,240
exponential curves even though they look

240
00:08:20,000 --> 00:08:25,440
linear this is an enormous adoption

241
00:08:23,240 --> 00:08:27,240
happening within science right and so

242
00:08:25,440 --> 00:08:29,759
when I say why is it important to be

243
00:08:27,240 --> 00:08:31,520
thinking about AI in science and an R&D

244
00:08:29,759 --> 00:08:33,399
this is why right it is going up

245
00:08:31,520 --> 00:08:35,279
incredibly rapidly and then the last

246
00:08:33,399 --> 00:08:37,320
thing is to notice this distinction

247
00:08:35,279 --> 00:08:39,360
between all of those lines and this line

248
00:08:37,320 --> 00:08:41,599
about new models being released that's

249
00:08:39,360 --> 00:08:42,880
growing much much more slowly that's

250
00:08:41,599 --> 00:08:45,000
because these models are so expensive to

251
00:08:42,880 --> 00:08:46,080
build right and so that's something

252
00:08:45,000 --> 00:08:47,760
that's going to be important to think

253
00:08:46,080 --> 00:08:49,080
about as you think about the choice

254
00:08:47,760 --> 00:08:50,680
about do you want to design your own

255
00:08:49,080 --> 00:08:51,959
model or do you want to just use someone

256
00:08:50,680 --> 00:08:54,959
else's model for

257
00:08:51,959 --> 00:08:54,959
it

258
00:08:55,800 --> 00:09:01,079
okay okay so it's also interesting to

259
00:08:59,320 --> 00:09:05,560
look look at this picture across

260
00:09:01,079 --> 00:09:05,560
different fields uh within the within

261
00:09:05,880 --> 00:09:12,360
science and what we can see

262
00:09:08,800 --> 00:09:13,640
there is that I think not surprisingly

263
00:09:12,360 --> 00:09:15,640
computer science is the one that is

264
00:09:13,640 --> 00:09:17,920
leading the leading the thing in fact

265
00:09:15,640 --> 00:09:22,480
more than 1% of all computer science

266
00:09:17,920 --> 00:09:25,320
papers are already using TR gbt excuse

267
00:09:22,480 --> 00:09:26,800
me using one of these AI tools in the

268
00:09:25,320 --> 00:09:28,480
way that it's actually been done and

269
00:09:26,800 --> 00:09:30,320
here the these lines represent either

270
00:09:28,480 --> 00:09:32,120
using or adapting for yourself both of

271
00:09:30,320 --> 00:09:36,480
those are included in these

272
00:09:32,120 --> 00:09:39,079
lines mathematics next engineering next

273
00:09:36,480 --> 00:09:42,360
then medicine then biology and then

274
00:09:39,079 --> 00:09:44,040
physics okay so this is again just more

275
00:09:42,360 --> 00:09:45,040
of a reference for I thought might be

276
00:09:44,040 --> 00:09:46,320
useful for all of you when you're

277
00:09:45,040 --> 00:09:48,160
thinking about your particular companies

278
00:09:46,320 --> 00:09:49,440
and what you work in right you can say

279
00:09:48,160 --> 00:09:53,440
well if the main thing we're doing is

280
00:09:49,440 --> 00:09:55,720
computer science right we you are we in

281
00:09:53,440 --> 00:09:58,120
an era where already 1% are using it if

282
00:09:55,720 --> 00:09:59,640
you're in more than an area like physics

283
00:09:58,120 --> 00:10:03,160
right where you'd say okay well we're

284
00:09:59,640 --> 00:10:05,480
still at you know 0 1% quite a bit lower

285
00:10:03,160 --> 00:10:09,839
those models are probably not as

286
00:10:05,480 --> 00:10:11,600
developed okay so huge exponential

287
00:10:09,839 --> 00:10:14,760
increase in the use in

288
00:10:11,600 --> 00:10:17,399
science what's actually driving this

289
00:10:14,760 --> 00:10:20,120
change and for this I want to show a

290
00:10:17,399 --> 00:10:24,839
probably my favorite example uh so this

291
00:10:20,120 --> 00:10:27,040
is from a 2002 excuse me 2022 paper by

292
00:10:24,839 --> 00:10:28,320
Google about the power of scaling up

293
00:10:27,040 --> 00:10:29,920
neural networks so they trained this

294
00:10:28,320 --> 00:10:32,040
model this is going to be model that you

295
00:10:29,920 --> 00:10:34,360
give it some kind of verbal prompt and

296
00:10:32,040 --> 00:10:36,200
it produces an image right so you know

297
00:10:34,360 --> 00:10:37,800
modern chbd you just type this in it

298
00:10:36,200 --> 00:10:40,600
produces an image for you it's going to

299
00:10:37,800 --> 00:10:43,760
be like that and so the prompt they gave

300
00:10:40,600 --> 00:10:45,079
it was a portrait photo of a kangaroo

301
00:10:43,760 --> 00:10:47,040
wearing an orange hoodie and blue

302
00:10:45,079 --> 00:10:48,839
sunglasses standing on the grass in

303
00:10:47,040 --> 00:10:51,200
front of the Sydney Opera House holding

304
00:10:48,839 --> 00:10:54,680
a sign on the chest that says Welcome

305
00:10:51,200 --> 00:10:56,800
Friends okay it's kind of fun chosen

306
00:10:54,680 --> 00:10:58,360
because this is going to require

307
00:10:56,800 --> 00:11:00,639
interpolation right there was no such

308
00:10:58,360 --> 00:11:01,959
picture in the on the internet that it

309
00:11:00,639 --> 00:11:05,160
could just recapitulate right it's got

310
00:11:01,959 --> 00:11:07,320
to try and figure this out so what

311
00:11:05,160 --> 00:11:09,680
happens if you train this on a model

312
00:11:07,320 --> 00:11:11,360
with 350 million

313
00:11:09,680 --> 00:11:12,920
parameters you get this kind of

314
00:11:11,360 --> 00:11:15,320
Chihuahua looking

315
00:11:12,920 --> 00:11:17,360
thing right that that this is not very

316
00:11:15,320 --> 00:11:20,040
good right there's something that looks

317
00:11:17,360 --> 00:11:21,279
a little bit like an orange hoodie like

318
00:11:20,040 --> 00:11:22,839
that building in the background is not

319
00:11:21,279 --> 00:11:25,800
the city the Opera House the sign is

320
00:11:22,839 --> 00:11:29,440
junk like this is not that good a

321
00:11:25,800 --> 00:11:31,920
fit okay now but importantly this is a

322
00:11:29,440 --> 00:11:33,360
model that only has 350 million

323
00:11:31,920 --> 00:11:35,440
parameters and for those who are not

324
00:11:33,360 --> 00:11:37,920
sort of like conversing about these

325
00:11:35,440 --> 00:11:39,399
model sizes this is pretty small now

326
00:11:37,920 --> 00:11:41,279
it's kind of strange to say that because

327
00:11:39,399 --> 00:11:42,279
if you were someone who grew up in an

328
00:11:41,279 --> 00:11:43,560
era where you were doing like

329
00:11:42,279 --> 00:11:45,720
regressions or you were doing

330
00:11:43,560 --> 00:11:49,320
engineering models right you did like

331
00:11:45,720 --> 00:11:51,040
you know Newton's uh equations in high

332
00:11:49,320 --> 00:11:52,600
school right you were doing things that

333
00:11:51,040 --> 00:11:55,519
might have had like three

334
00:11:52,600 --> 00:11:58,240
parameters right and this has got 350

335
00:11:55,519 --> 00:12:01,880
million of them okay so this is where St

336
00:11:58,240 --> 00:12:03,519
firmly in a deep learning AI world but

337
00:12:01,880 --> 00:12:04,600
for these kind of models this is still

338
00:12:03,519 --> 00:12:07,519
pretty

339
00:12:04,600 --> 00:12:09,959
small all right so what if you give it

340
00:12:07,519 --> 00:12:12,279
the same training regime you give it the

341
00:12:09,959 --> 00:12:15,680
same data but now you train it on a

342
00:12:12,279 --> 00:12:18,639
model with 750 million

343
00:12:15,680 --> 00:12:20,240
parameters okay it's a little better

344
00:12:18,639 --> 00:12:21,839
like that thing standing next to it

345
00:12:20,240 --> 00:12:24,760
looks a little bit more like a

346
00:12:21,839 --> 00:12:26,440
kangaroo right the Sydney Opera house

347
00:12:24,760 --> 00:12:29,279
we're now evoking it a little bit but

348
00:12:26,440 --> 00:12:31,800
we're definitely not there

349
00:12:29,279 --> 00:12:31,800
make it bigger

350
00:12:32,800 --> 00:12:37,880
again okay that that's quite a bit

351
00:12:35,519 --> 00:12:39,480
better right that it's still not quite

352
00:12:37,880 --> 00:12:42,079
the Sydney Opera House in the background

353
00:12:39,480 --> 00:12:43,480
but again it's getting closer that now

354
00:12:42,079 --> 00:12:44,800
the kangaroo starting to look pretty

355
00:12:43,480 --> 00:12:47,240
good right the sunglasses are looking

356
00:12:44,800 --> 00:12:48,320
good the orange hoodies looking good

357
00:12:47,240 --> 00:12:51,440
sign is still

358
00:12:48,320 --> 00:12:53,320
wrong right there two signs and the the

359
00:12:51,440 --> 00:12:55,360
one on the uh sort of cardboard

360
00:12:53,320 --> 00:12:59,680
background there if you squint it might

361
00:12:55,360 --> 00:13:02,240
say welcome friends but it doesn't quite

362
00:12:59,680 --> 00:13:05,880
20 billion

363
00:13:02,240 --> 00:13:10,320
parameters remarkably

364
00:13:05,880 --> 00:13:12,120
good okay so this is pretty interesting

365
00:13:10,320 --> 00:13:13,519
right this is telling us that all of

366
00:13:12,120 --> 00:13:14,920
this when you get have these extra

367
00:13:13,519 --> 00:13:16,360
parameters just like we had back in you

368
00:13:14,920 --> 00:13:17,760
know High School physics when you have

369
00:13:16,360 --> 00:13:20,360
extra parameters you can model something

370
00:13:17,760 --> 00:13:21,839
more flexibly it turns out that that's

371
00:13:20,360 --> 00:13:24,399
really important in an era where you're

372
00:13:21,839 --> 00:13:27,639
giving these models literally trillions

373
00:13:24,399 --> 00:13:30,399
of data points in in terms of the inputs

374
00:13:27,639 --> 00:13:32,480
then actually this really

375
00:13:30,399 --> 00:13:33,880
matters and having this flexibility in

376
00:13:32,480 --> 00:13:36,720
the model really matters and you can do

377
00:13:33,880 --> 00:13:38,279
much better okay so this is at different

378
00:13:36,720 --> 00:13:41,399
sizes you can do

379
00:13:38,279 --> 00:13:45,399
better okay so it turns out this has

380
00:13:41,399 --> 00:13:47,360
been really the dominant way in which

381
00:13:45,399 --> 00:13:48,760
all of these AI companies have been

382
00:13:47,360 --> 00:13:50,639
getting better

383
00:13:48,760 --> 00:13:52,720
performance and so I want to turn this

384
00:13:50,639 --> 00:13:54,440
now from sort of like hey abstract

385
00:13:52,720 --> 00:13:56,360
number parameters which might be a

386
00:13:54,440 --> 00:13:59,720
little think about this actually in

387
00:13:56,360 --> 00:14:01,639
terms of Hardware that people are using

388
00:13:59,720 --> 00:14:03,519
okay so for those of you who' sort of

389
00:14:01,639 --> 00:14:06,040
been following this for a while this

390
00:14:03,519 --> 00:14:08,240
paper in 2012 alexnet this is by Jeff

391
00:14:06,040 --> 00:14:09,839
hinton's Lab at University of Toronto

392
00:14:08,240 --> 00:14:12,160
this was really the paper that kicked

393
00:14:09,839 --> 00:14:14,120
off the deep learning

394
00:14:12,160 --> 00:14:16,959
Revolution that

395
00:14:14,120 --> 00:14:19,120
paper ran on two gpus those are the

396
00:14:16,959 --> 00:14:21,720
specialized processors that people use

397
00:14:19,120 --> 00:14:22,920
for six days okay and I remember when

398
00:14:21,720 --> 00:14:24,720
this came out and it was a little bit

399
00:14:22,920 --> 00:14:29,360
like wow that's a that's a lot of

400
00:14:24,720 --> 00:14:31,480
compute to to to put on this

401
00:14:29,360 --> 00:14:32,880
four years later when Google does the

402
00:14:31,480 --> 00:14:38,759
Google Translation

403
00:14:32,880 --> 00:14:41,079
engine six days 50 times as many

404
00:14:38,759 --> 00:14:45,120
processors a big

405
00:14:41,079 --> 00:14:46,880
number 2023 GPT

406
00:14:45,120 --> 00:14:49,199
4 three

407
00:14:46,880 --> 00:14:51,279
months on

408
00:14:49,199 --> 00:14:55,959
25,000

409
00:14:51,279 --> 00:14:56,959
gpus okay so when I say like this is the

410
00:14:55,959 --> 00:14:59,440
dominant way that people have been

411
00:14:56,959 --> 00:15:02,199
getting progress it it is and you can

412
00:14:59,440 --> 00:15:05,440
see that in all of this and so when you

413
00:15:02,199 --> 00:15:07,800
read on the the news about you know like

414
00:15:05,440 --> 00:15:09,600
the state of Virginia saying oh actually

415
00:15:07,800 --> 00:15:11,320
we're going to substantially increase

416
00:15:09,600 --> 00:15:14,240
our projections for the amount of

417
00:15:11,320 --> 00:15:15,920
electricity being used because of AI

418
00:15:14,240 --> 00:15:17,519
right this is what we're talking about

419
00:15:15,920 --> 00:15:19,920
when people talk about all of these data

420
00:15:17,519 --> 00:15:22,320
centers being built this is why and it's

421
00:15:19,920 --> 00:15:24,600
because that scaling up makes such a

422
00:15:22,320 --> 00:15:26,920
difference in fact you can really see

423
00:15:24,600 --> 00:15:28,519
this so anthropic and open AI obviously

424
00:15:26,920 --> 00:15:30,279
two of the really big companies that

425
00:15:28,519 --> 00:15:32,399
work in this space you can see this

426
00:15:30,279 --> 00:15:34,639
explicitly in their discussions of this

427
00:15:32,399 --> 00:15:36,279
so anthropic says inspired by the

428
00:15:34,639 --> 00:15:38,319
University of scaling and statistical

429
00:15:36,279 --> 00:15:40,040
physics we develop scaling laws to help

430
00:15:38,319 --> 00:15:42,240
us do systematic empirically driven

431
00:15:40,040 --> 00:15:44,600
research which is sort of the you know

432
00:15:42,240 --> 00:15:45,600
technical way of saying if you scale it

433
00:15:44,600 --> 00:15:48,360
up you do

434
00:15:45,600 --> 00:15:50,319
better open AI we believe that scale in

435
00:15:48,360 --> 00:15:53,800
our models our systems ourselves is

436
00:15:50,319 --> 00:15:55,160
magic when in doubt scale it up okay and

437
00:15:53,800 --> 00:15:57,680
in some of the work that my lab has done

438
00:15:55,160 --> 00:15:59,600
we find that something like 70% of all

439
00:15:57,680 --> 00:16:01,040
the Improvement in these models can be

440
00:15:59,600 --> 00:16:03,000
explained by the amount of compute that

441
00:16:01,040 --> 00:16:03,759
they're able to harness okay so it's a

442
00:16:03,000 --> 00:16:07,319
big

443
00:16:03,759 --> 00:16:10,240
number all right so with that in mind

444
00:16:07,319 --> 00:16:12,600
then let's now think about building

445
00:16:10,240 --> 00:16:14,360
models right so you're at a company

446
00:16:12,600 --> 00:16:15,399
you're saying okay we have this data we

447
00:16:14,360 --> 00:16:18,199
have this problem that's really

448
00:16:15,399 --> 00:16:19,759
important to us we really want to adapt

449
00:16:18,199 --> 00:16:22,480
that and think about should we build our

450
00:16:19,759 --> 00:16:25,120
own model in order to help do

451
00:16:22,480 --> 00:16:26,079
this well certainly one thing you should

452
00:16:25,120 --> 00:16:28,279
think about if you're going to build

453
00:16:26,079 --> 00:16:29,680
your own model particularly in something

454
00:16:28,279 --> 00:16:32,399
like large l language model kind of

455
00:16:29,680 --> 00:16:34,360
space but can be more General is that

456
00:16:32,399 --> 00:16:36,279
there's a big cost but also big

457
00:16:34,360 --> 00:16:37,079
potential for these so what do I mean by

458
00:16:36,279 --> 00:16:39,800
that

459
00:16:37,079 --> 00:16:42,319
well to give a sense so back when it was

460
00:16:39,800 --> 00:16:44,279
still a separate entity back in 2018

461
00:16:42,319 --> 00:16:45,959
deep learning uh deep Google Deep Mind

462
00:16:44,279 --> 00:16:47,920
was doing this right and they reported

463
00:16:45,959 --> 00:16:52,319
separately and at that point they were

464
00:16:47,920 --> 00:16:52,319
losing half a billion dollars a

465
00:16:52,440 --> 00:16:57,800
year when they trained Alpha go just

466
00:16:55,199 --> 00:17:00,360
building that system just for go was

467
00:16:57,800 --> 00:17:03,240
estimated to have cost 30 5

468
00:17:00,360 --> 00:17:06,160
million and gbd4 was estimated to cost

469
00:17:03,240 --> 00:17:07,760
$100 million to train and it's now the

470
00:17:06,160 --> 00:17:09,199
case that that people building these

471
00:17:07,760 --> 00:17:12,199
large language models are already

472
00:17:09,199 --> 00:17:14,039
talking about build it costing say 5 to

473
00:17:12,199 --> 00:17:16,760
10 billion for the next generation of

474
00:17:14,039 --> 00:17:18,360
models the train so if you really want

475
00:17:16,760 --> 00:17:19,640
Cutting Edge right it can you you should

476
00:17:18,360 --> 00:17:22,079
definitely be thinking

477
00:17:19,640 --> 00:17:23,360
millions and you know if you're if

478
00:17:22,079 --> 00:17:26,120
you're getting closer to large language

479
00:17:23,360 --> 00:17:29,000
models I mean it's billions and billions

480
00:17:26,120 --> 00:17:30,200
but um this is still very very big

481
00:17:29,000 --> 00:17:31,400
Investments that you should be thinking

482
00:17:30,200 --> 00:17:34,360
about if you're training these new

483
00:17:31,400 --> 00:17:35,600
models but as I say these can be big

484
00:17:34,360 --> 00:17:37,880
right so I already mentioned protein

485
00:17:35,600 --> 00:17:40,160
folding right this is just one the Nobel

486
00:17:37,880 --> 00:17:41,960
Prize it's clearly producing a whole

487
00:17:40,160 --> 00:17:43,400
bunch of Partnerships that Google

488
00:17:41,960 --> 00:17:46,240
deepmind has with pharmaceutical

489
00:17:43,400 --> 00:17:47,559
companies and the like um and then you

490
00:17:46,240 --> 00:17:49,799
know these math problems I already

491
00:17:47,559 --> 00:17:51,400
mentioned again you can imagine say

492
00:17:49,799 --> 00:17:54,120
being able to solve new engineering

493
00:17:51,400 --> 00:17:57,520
problems with this that could be very

494
00:17:54,120 --> 00:17:59,520
valuable okay so with all that said I

495
00:17:57,520 --> 00:18:02,240
want to come to an example

496
00:17:59,520 --> 00:18:05,520
so this is a case study that my lab

497
00:18:02,240 --> 00:18:07,000
did and this was a European supermarket

498
00:18:05,520 --> 00:18:09,080
and they had a problem which I think a

499
00:18:07,000 --> 00:18:10,880
lot of you would empathize with right

500
00:18:09,080 --> 00:18:13,320
they're just want to know how much

501
00:18:10,880 --> 00:18:15,720
should I order each week to keep on the

502
00:18:13,320 --> 00:18:17,159
shelves seems like a very basic question

503
00:18:15,720 --> 00:18:18,360
for a supermarket turns out to be an

504
00:18:17,159 --> 00:18:19,640
incredibly valuable question for

505
00:18:18,360 --> 00:18:22,080
Supermarket as

506
00:18:19,640 --> 00:18:25,159
well the way they did that

507
00:18:22,080 --> 00:18:27,360
historically is they had an sap model

508
00:18:25,159 --> 00:18:29,799
that tracked sales over the past and it

509
00:18:27,360 --> 00:18:32,240
would say oh okay well based last year

510
00:18:29,799 --> 00:18:33,600
this time you sold this many bananas

511
00:18:32,240 --> 00:18:36,240
that would get sent to the store manager

512
00:18:33,600 --> 00:18:37,480
the store manager would say ah you know

513
00:18:36,240 --> 00:18:39,679
things have gotten a little like we've

514
00:18:37,480 --> 00:18:41,039
got some new developments around here uh

515
00:18:39,679 --> 00:18:42,799
the weather's getting good okay I'm

516
00:18:41,039 --> 00:18:44,400
going to up that by a little bit they

517
00:18:42,799 --> 00:18:45,480
would send that order in and they would

518
00:18:44,400 --> 00:18:48,840
get

519
00:18:45,480 --> 00:18:50,000
it that's you know not bad but this is

520
00:18:48,840 --> 00:18:51,440
the kind of problem that you'd expect

521
00:18:50,000 --> 00:18:52,559
deep learning to be able to do a lot

522
00:18:51,440 --> 00:18:54,280
better because deep learning is very

523
00:18:52,559 --> 00:18:57,280
good at integrating different data

524
00:18:54,280 --> 00:18:59,600
sources um it's handling nonlinearity so

525
00:18:57,280 --> 00:19:02,080
maybe like you know I cream demand goes

526
00:18:59,600 --> 00:19:05,320
up but then on the like the first warm

527
00:19:02,080 --> 00:19:07,480
day of the year it goes up a lot right

528
00:19:05,320 --> 00:19:09,080
that kind of like unusual circumstances

529
00:19:07,480 --> 00:19:10,039
again deep learning very very good at

530
00:19:09,080 --> 00:19:11,400
that those kind of things so you might

531
00:19:10,039 --> 00:19:14,880
think this is a great

532
00:19:11,400 --> 00:19:16,679
problem so they worked with the

533
00:19:14,880 --> 00:19:19,600
consultant they put in the system and

534
00:19:16,679 --> 00:19:22,039
what happened Well turns out they were

535
00:19:19,600 --> 00:19:25,600
right it was much better the prediction

536
00:19:22,039 --> 00:19:29,240
error dropped by a third that's

537
00:19:25,600 --> 00:19:30,760
huge but costs were so high that they

538
00:19:29,240 --> 00:19:32,240
delayed implementation initially and

539
00:19:30,760 --> 00:19:34,080
then in the end only rolled it out in

540
00:19:32,240 --> 00:19:36,400
the limited

541
00:19:34,080 --> 00:19:38,159
way so what what's going on here so

542
00:19:36,400 --> 00:19:40,080
first of all part of what was going on

543
00:19:38,159 --> 00:19:41,240
here was just training these systems are

544
00:19:40,080 --> 00:19:43,480
expensive right we just spent a couple

545
00:19:41,240 --> 00:19:45,080
of slides talking about that it is it is

546
00:19:43,480 --> 00:19:47,000
expensive to build these

547
00:19:45,080 --> 00:19:49,400
models but the second thing that was

548
00:19:47,000 --> 00:19:51,559
going on there that was really important

549
00:19:49,400 --> 00:19:53,480
was that they hadn't really thought

550
00:19:51,559 --> 00:19:55,120
about the business side of this when

551
00:19:53,480 --> 00:19:57,880
they went in they went into it with a

552
00:19:55,120 --> 00:20:01,600
mode that was like Hey AI is kind of

553
00:19:57,880 --> 00:20:04,039
great let's see what can do rather than

554
00:20:01,600 --> 00:20:06,840
hey what is the exact business need and

555
00:20:04,039 --> 00:20:08,280
why is it going to matter a lot and the

556
00:20:06,840 --> 00:20:10,159
reason I know that this is true is

557
00:20:08,280 --> 00:20:12,520
because after the fact we went back and

558
00:20:10,159 --> 00:20:14,799
we did a bunch of the economics of this

559
00:20:12,520 --> 00:20:17,080
we said okay well how valuable is it to

560
00:20:14,799 --> 00:20:18,559
you to get a better prediction right if

561
00:20:17,080 --> 00:20:19,679
you got a third better that sounds like

562
00:20:18,559 --> 00:20:23,120
a great

563
00:20:19,679 --> 00:20:25,000
result well for some things it's

564
00:20:23,120 --> 00:20:26,360
incredibly valuable right if you bought

565
00:20:25,000 --> 00:20:28,360
a bunch of lobster and that Lobster is

566
00:20:26,360 --> 00:20:30,320
all going to go bad or a bunch of apples

567
00:20:28,360 --> 00:20:32,360
and those apples are all going to go bad

568
00:20:30,320 --> 00:20:35,799
that saves you a lot of money as a

569
00:20:32,360 --> 00:20:37,480
supermarket but if you have a can of

570
00:20:35,799 --> 00:20:39,720
tuna fish on the

571
00:20:37,480 --> 00:20:42,000
Shelf keeping it an extra week doesn't

572
00:20:39,720 --> 00:20:44,039
matter very much right and these AI

573
00:20:42,000 --> 00:20:45,760
systems are can be expensive enough that

574
00:20:44,039 --> 00:20:47,480
you that actually that extra cost of

575
00:20:45,760 --> 00:20:49,240
doing that Cann not might not be worth

576
00:20:47,480 --> 00:20:50,760
it and in fact in this case wasn't and

577
00:20:49,240 --> 00:20:53,280
so what happened when we redid this

578
00:20:50,760 --> 00:20:55,039
economics is we found that about 85% of

579
00:20:53,280 --> 00:20:56,760
all of the value that could have been

580
00:20:55,039 --> 00:20:58,679
created by this system was all in a very

581
00:20:56,760 --> 00:21:00,280
small number of categories and it was a

582
00:20:58,679 --> 00:21:02,440
stuff that went bad really fast and

583
00:21:00,280 --> 00:21:03,919
stuff like that and that made it a much

584
00:21:02,440 --> 00:21:05,120
more attractive thing if they focused on

585
00:21:03,919 --> 00:21:06,440
that but they just hadn't done that

586
00:21:05,120 --> 00:21:10,360
economics ahead of

587
00:21:06,440 --> 00:21:11,720
time okay so I bring this up just to get

588
00:21:10,360 --> 00:21:13,840
back to this idea of when you're

589
00:21:11,720 --> 00:21:15,440
thinking of applying this in in science

590
00:21:13,840 --> 00:21:17,480
you should not think of this as being

591
00:21:15,440 --> 00:21:19,080
just like any other it system and you

592
00:21:17,480 --> 00:21:20,559
sort of put it in right you should think

593
00:21:19,080 --> 00:21:22,039
that actually these predictions could be

594
00:21:20,559 --> 00:21:23,520
expensive that you're going to be making

595
00:21:22,039 --> 00:21:25,960
with these systems and you might want to

596
00:21:23,520 --> 00:21:28,919
take that into account when you first do

597
00:21:25,960 --> 00:21:30,320
it okay because these system are

598
00:21:28,919 --> 00:21:31,799
expensive there are also some

599
00:21:30,320 --> 00:21:33,000
consequences and so this was a paper

600
00:21:31,799 --> 00:21:35,120
that we wrote in science a couple of

601
00:21:33,000 --> 00:21:37,240
years ago about the growing influence of

602
00:21:35,120 --> 00:21:39,360
industry in AI research and that's

603
00:21:37,240 --> 00:21:40,559
because these price tags are going up so

604
00:21:39,360 --> 00:21:43,320
to give you a little bit of a sense of

605
00:21:40,559 --> 00:21:45,559
what this looks like um so here we're

606
00:21:43,320 --> 00:21:46,760
let's focus first on the inputs AI so in

607
00:21:45,559 --> 00:21:49,720
the top left here we have the

608
00:21:46,760 --> 00:21:52,360
percentages of AI phds coming out of

609
00:21:49,720 --> 00:21:54,559
universities that are hired by industry

610
00:21:52,360 --> 00:21:56,559
this is now uh so I said this was now a

611
00:21:54,559 --> 00:21:59,200
couple of years old but um we you know

612
00:21:56,559 --> 00:22:00,480
we were already on on track to getting

613
00:21:59,200 --> 00:22:02,240
to the very highest levels of

614
00:22:00,480 --> 00:22:04,440
engineering so if you think of sort of

615
00:22:02,240 --> 00:22:06,840
any field like this so there's enormous

616
00:22:04,440 --> 00:22:08,039
demand for um for talent and I not

617
00:22:06,840 --> 00:22:09,080
mention many of you are experiencing

618
00:22:08,039 --> 00:22:11,679
this in your

619
00:22:09,080 --> 00:22:13,840
companies um we saw AI faculty being

620
00:22:11,679 --> 00:22:15,679
hired away right and you know we see

621
00:22:13,840 --> 00:22:17,240
this as well like there are definitely

622
00:22:15,679 --> 00:22:19,200
academics who were saying particular

623
00:22:17,240 --> 00:22:21,480
types of research are now very hard for

624
00:22:19,200 --> 00:22:23,640
academics to do because you need the

625
00:22:21,480 --> 00:22:25,120
giant supercomputer in order to actually

626
00:22:23,640 --> 00:22:27,200
even compete in that

627
00:22:25,120 --> 00:22:30,039
area and then these last ones are

628
00:22:27,200 --> 00:22:31,279
showing uh that a computing power so

629
00:22:30,039 --> 00:22:32,559
this is the number of parameters in a

630
00:22:31,279 --> 00:22:34,600
model remember we talked about that with

631
00:22:32,559 --> 00:22:36,039
the Google example and this is just

632
00:22:34,600 --> 00:22:38,520
showing that the average number of

633
00:22:36,039 --> 00:22:40,679
parameters of academic models were very

634
00:22:38,520 --> 00:22:45,440
stable over this time period the ones in

635
00:22:40,679 --> 00:22:48,480
Industry were going up very rapidly

636
00:22:45,440 --> 00:22:51,880
okay and this gets reflected in

637
00:22:48,480 --> 00:22:53,440
outcomes so here what we're showing is

638
00:22:51,880 --> 00:22:55,480
the percentage of the 10 biggest AI

639
00:22:53,440 --> 00:22:57,240
models that come from industry and you

640
00:22:55,480 --> 00:22:59,200
can see back in the early

641
00:22:57,240 --> 00:23:01,159
2000s all none of them were from

642
00:22:59,200 --> 00:23:03,320
industry they were all from Academia and

643
00:23:01,159 --> 00:23:05,760
now almost all of them are from industry

644
00:23:03,320 --> 00:23:08,080
right there is this kind of scale

645
00:23:05,760 --> 00:23:09,720
up we also see this in terms of the

646
00:23:08,080 --> 00:23:11,440
percentage of papers so these are now

647
00:23:09,720 --> 00:23:13,320
computer science papers that have an

648
00:23:11,440 --> 00:23:14,840
industry affiliate and you can see that

649
00:23:13,320 --> 00:23:16,640
for a long long time that was very

650
00:23:14,840 --> 00:23:18,600
stable that around 20% of papers had an

651
00:23:16,640 --> 00:23:20,600
industry co-author and in the last few

652
00:23:18,600 --> 00:23:22,240
years it had gone up to about 40% it's

653
00:23:20,600 --> 00:23:25,279
about

654
00:23:22,240 --> 00:23:27,279
doubled okay so you know you're you're

655
00:23:25,279 --> 00:23:29,159
you're not in in Academia but I bring

656
00:23:27,279 --> 00:23:31,279
this up just to say like these

657
00:23:29,159 --> 00:23:33,080
constraints of these extra costs they do

658
00:23:31,279 --> 00:23:34,760
bind right if you were a small company

659
00:23:33,080 --> 00:23:36,279
this can be a challenge if you are a

660
00:23:34,760 --> 00:23:37,480
large company but you're competing

661
00:23:36,279 --> 00:23:39,159
against someone who has even deeper

662
00:23:37,480 --> 00:23:41,039
Pockets or bigger things this can also

663
00:23:39,159 --> 00:23:44,000
be an

664
00:23:41,039 --> 00:23:46,320
issue okay

665
00:23:44,000 --> 00:23:49,080
um that is what I've just been talking

666
00:23:46,320 --> 00:23:51,320
about is really the sort of scale of the

667
00:23:49,080 --> 00:23:53,240
cost that you have in this I also just

668
00:23:51,320 --> 00:23:54,679
want to bring in a super super practical

669
00:23:53,240 --> 00:23:56,960
thing that came out of some research we

670
00:23:54,679 --> 00:23:58,559
did which I think is really important

671
00:23:56,960 --> 00:24:01,039
and that is just that the structure of

672
00:23:58,559 --> 00:24:04,120
your R&D budget probably needs to look

673
00:24:01,039 --> 00:24:07,360
different if you are using AI to do the

674
00:24:04,120 --> 00:24:09,360
science okay so what do I mean by

675
00:24:07,360 --> 00:24:11,640
that

676
00:24:09,360 --> 00:24:13,400
so as you can see on the x-axis here

677
00:24:11,640 --> 00:24:15,400
we're looking at the share of an R&D

678
00:24:13,400 --> 00:24:17,559
budget that is being spent on computers

679
00:24:15,400 --> 00:24:19,159
and other equipment and this is academic

680
00:24:17,559 --> 00:24:22,120
science but I imagine it's uh pretty

681
00:24:19,159 --> 00:24:24,559
true in Industry as well so on the left

682
00:24:22,120 --> 00:24:26,760
hand side of this graph you see funding

683
00:24:24,559 --> 00:24:28,840
for the NSF for different projects and

684
00:24:26,760 --> 00:24:32,640
so for example for computer science you

685
00:24:28,840 --> 00:24:34,919
can see that the share of R&D done in

686
00:24:32,640 --> 00:24:36,640
computer science about 11% of that

687
00:24:34,919 --> 00:24:39,039
historically has been you spent on like

688
00:24:36,640 --> 00:24:42,360
computers and other equipment and the

689
00:24:39,039 --> 00:24:43,720
other 80 some percent was people right

690
00:24:42,360 --> 00:24:45,640
and I imagine that's probably true in

691
00:24:43,720 --> 00:24:48,080
many of your R&D

692
00:24:45,640 --> 00:24:50,799
operations and so what we did is we used

693
00:24:48,080 --> 00:24:53,720
some e econom economic and econometric

694
00:24:50,799 --> 00:24:58,000
techniques to extract from papers a good

695
00:24:53,720 --> 00:25:00,480
sense of well for stuff in AI what was

696
00:24:58,000 --> 00:25:03,240
the actual ratio of these things and you

697
00:25:00,480 --> 00:25:04,840
get to it's more like a third capital

698
00:25:03,240 --> 00:25:06,960
and 2/3

699
00:25:04,840 --> 00:25:09,440
people so what does this mean

700
00:25:06,960 --> 00:25:11,600
practically it means that in your

701
00:25:09,440 --> 00:25:14,000
organizations if you say to some one of

702
00:25:11,600 --> 00:25:16,039
your divisions we want you to be AI

703
00:25:14,000 --> 00:25:17,799
first right we're going to AI first

704
00:25:16,039 --> 00:25:19,640
innovation in this

705
00:25:17,799 --> 00:25:21,840
area and then you give them the same

706
00:25:19,640 --> 00:25:22,960
budget you've always given them they're

707
00:25:21,840 --> 00:25:24,320
going to be building with models that

708
00:25:22,960 --> 00:25:25,960
are way too small because they're not

709
00:25:24,320 --> 00:25:27,240
going to have the budget to do the kind

710
00:25:25,960 --> 00:25:28,919
of computing that you need to be at the

711
00:25:27,240 --> 00:25:31,159
frontier of this

712
00:25:28,919 --> 00:25:32,240
or they're going to say now okay well in

713
00:25:31,159 --> 00:25:33,120
that case we're going to need to fire a

714
00:25:32,240 --> 00:25:35,840
bunch of

715
00:25:33,120 --> 00:25:37,120
people so to make room in our budget and

716
00:25:35,840 --> 00:25:39,039
you might not want that either right you

717
00:25:37,120 --> 00:25:41,440
might have cultivated that expertise

718
00:25:39,039 --> 00:25:43,120
over time so I just I bring this up

719
00:25:41,440 --> 00:25:44,120
because it feels like a very practical

720
00:25:43,120 --> 00:25:46,039
like when you get into your next

721
00:25:44,120 --> 00:25:50,200
budgeting cycle you need to be thinking

722
00:25:46,039 --> 00:25:50,200
about this in terms of innovating in

723
00:25:50,559 --> 00:25:54,840
AI okay all right so with all of that as

724
00:25:53,520 --> 00:25:56,640
background I now want to come back to

725
00:25:54,840 --> 00:26:00,240
what I promised at the beginning which

726
00:25:56,640 --> 00:26:00,240
is let's think about

727
00:26:00,480 --> 00:26:04,159
buying you adopt and adopting these

728
00:26:02,840 --> 00:26:05,960
models so we've already just talked

729
00:26:04,159 --> 00:26:08,039
about um build sorry excuse me building

730
00:26:05,960 --> 00:26:09,679
and and um and then adopting so we've

731
00:26:08,039 --> 00:26:12,600
just talked about building super

732
00:26:09,679 --> 00:26:13,919
expensive right in many cases let's talk

733
00:26:12,600 --> 00:26:15,960
about now just using something off the

734
00:26:13,919 --> 00:26:17,320
shelf right maybe you can just use chat

735
00:26:15,960 --> 00:26:19,799
GPT as it

736
00:26:17,320 --> 00:26:21,600
is so you could you can do this I mean

737
00:26:19,799 --> 00:26:23,720
online but much much more practically

738
00:26:21,600 --> 00:26:25,520
you could do it with the apis so this is

739
00:26:23,720 --> 00:26:27,760
geminis you can see you know kind of

740
00:26:25,520 --> 00:26:30,919
amazing only about eight lines of code

741
00:26:27,760 --> 00:26:32,159
to bring that in um there have also been

742
00:26:30,919 --> 00:26:34,279
thoughts of like oh well maybe we'll

743
00:26:32,159 --> 00:26:36,720
have a bunch of rappers so open AI had

744
00:26:34,279 --> 00:26:38,480
this the GPT store my sense is that this

745
00:26:36,720 --> 00:26:40,640
has not been much of a success right I

746
00:26:38,480 --> 00:26:42,919
think people have not really used this

747
00:26:40,640 --> 00:26:44,159
very much but both of these are a way to

748
00:26:42,919 --> 00:26:47,039
say okay we're just going to take us off

749
00:26:44,159 --> 00:26:47,039
the shelf and we're going to use it

750
00:26:47,080 --> 00:26:52,080
ourselves what I think is crucial to

751
00:26:49,440 --> 00:26:54,640
understand about this is that AI is not

752
00:26:52,080 --> 00:26:55,799
like traditional it systems so what do I

753
00:26:54,640 --> 00:26:57,640
mean by that well here we're going to

754
00:26:55,799 --> 00:27:00,200
think about the tasks that need to be

755
00:26:57,640 --> 00:27:03,440
done and the success in performing that

756
00:27:00,200 --> 00:27:05,880
task and traditional it has this kind of

757
00:27:03,440 --> 00:27:08,480
form right it does some things really

758
00:27:05,880 --> 00:27:11,000
well basically at 100% accuracy and then

759
00:27:08,480 --> 00:27:11,880
it falls off entirely so you'd say I

760
00:27:11,000 --> 00:27:14,840
have a

761
00:27:11,880 --> 00:27:15,960
calculator right or I have Excel Excel

762
00:27:14,840 --> 00:27:18,240
certain functions that are built in

763
00:27:15,960 --> 00:27:20,320
Excel Excel does really well and if you

764
00:27:18,240 --> 00:27:25,960
say compose a poem for

765
00:27:20,320 --> 00:27:27,880
me it fails entirely right whereas on a

766
00:27:25,960 --> 00:27:29,320
system that is more like a modern AI

767
00:27:27,880 --> 00:27:31,039
system

768
00:27:29,320 --> 00:27:32,679
right it are some things that it does

769
00:27:31,039 --> 00:27:35,520
really well and then performance

770
00:27:32,679 --> 00:27:37,760
degrades but you absolutely can tell it

771
00:27:35,520 --> 00:27:39,320
to write a poem or do math or do a bunch

772
00:27:37,760 --> 00:27:41,240
of other

773
00:27:39,320 --> 00:27:43,360
things okay so I want you to notice a

774
00:27:41,240 --> 00:27:45,360
couple of things about this graph so one

775
00:27:43,360 --> 00:27:46,960
is that there's some space I've left

776
00:27:45,360 --> 00:27:49,440
some space here between the red line and

777
00:27:46,960 --> 00:27:51,799
the blue line in the sense of it's

778
00:27:49,440 --> 00:27:53,480
incredibly hard and Incredibly expensive

779
00:27:51,799 --> 00:27:56,159
to get that last bit of performance on

780
00:27:53,480 --> 00:27:57,399
an AI system to get it to be perfect

781
00:27:56,159 --> 00:27:58,840
right so there's almost always going to

782
00:27:57,399 --> 00:28:00,720
be some error

783
00:27:58,840 --> 00:28:02,320
and so what this means in practice is if

784
00:28:00,720 --> 00:28:04,559
you build this into your system you have

785
00:28:02,320 --> 00:28:06,760
to treat it differently than it

786
00:28:04,559 --> 00:28:08,519
traditionally okay so what like

787
00:28:06,760 --> 00:28:10,200
concretely imagine you have some process

788
00:28:08,519 --> 00:28:12,799
right you said okay we're going to do

789
00:28:10,200 --> 00:28:15,399
the we originally we have humans doing

790
00:28:12,799 --> 00:28:17,600
each of these steps and you said oh hey

791
00:28:15,399 --> 00:28:19,080
we had this big development in databases

792
00:28:17,600 --> 00:28:20,519
now we're going to put a a database in

793
00:28:19,080 --> 00:28:22,360
the middle of that and that database is

794
00:28:20,519 --> 00:28:23,880
going to sort of tell us maybe all of

795
00:28:22,360 --> 00:28:26,279
the purchases that a customer has made

796
00:28:23,880 --> 00:28:28,519
or something like that well we don't

797
00:28:26,279 --> 00:28:30,440
worry that it's like failing to report

798
00:28:28,519 --> 00:28:32,200
some of the transactions that have been

799
00:28:30,440 --> 00:28:34,399
made right that's we we assume that's

800
00:28:32,200 --> 00:28:35,720
basically perfect and so we build

801
00:28:34,399 --> 00:28:37,120
processes assuming that we're getting

802
00:28:35,720 --> 00:28:39,519
the 100%

803
00:28:37,120 --> 00:28:41,919
accuracy but this red line because it's

804
00:28:39,519 --> 00:28:43,200
lower means that you can't assume that

805
00:28:41,919 --> 00:28:44,120
it's going to be perfect you have to

806
00:28:43,200 --> 00:28:46,440
assume that there're going to be some

807
00:28:44,120 --> 00:28:49,000
errors and you have to build processes

808
00:28:46,440 --> 00:28:52,760
that are going to account for

809
00:28:49,000 --> 00:28:56,679
that okay

810
00:28:52,760 --> 00:28:58,200
so I want to bring up uh some examples

811
00:28:56,679 --> 00:28:59,399
that show that you know this is really

812
00:28:58,200 --> 00:29:02,000
what's happening and the way that people

813
00:28:59,399 --> 00:29:04,440
build it into their things so uh Sundar

814
00:29:02,000 --> 00:29:07,120
pinchai said today more than a quarter

815
00:29:04,440 --> 00:29:08,840
of all new coded Google is generated by

816
00:29:07,120 --> 00:29:10,919
Ai and then reviewed and accepted by

817
00:29:08,840 --> 00:29:12,799
Engineers this helps our Engineers do

818
00:29:10,919 --> 00:29:15,760
more and move

819
00:29:12,799 --> 00:29:17,600
faster so the 25% number is really

820
00:29:15,760 --> 00:29:19,399
interesting right it's a big number it

821
00:29:17,600 --> 00:29:20,679
is interesting Google Engineers talk

822
00:29:19,399 --> 00:29:23,720
about this and they're

823
00:29:20,679 --> 00:29:25,480
like they're uh there like yes it does

824
00:29:23,720 --> 00:29:26,640
fill that in but it's almost always like

825
00:29:25,480 --> 00:29:28,240
I've done the hard part and it's just

826
00:29:26,640 --> 00:29:31,120
filling in like the end of the words

827
00:29:28,240 --> 00:29:33,399
like autocomplete right but I'm more

828
00:29:31,120 --> 00:29:36,080
interested in this quotation because of

829
00:29:33,399 --> 00:29:38,600
what he what he says at the the last the

830
00:29:36,080 --> 00:29:39,960
second Clause there then reviewed and

831
00:29:38,600 --> 00:29:42,840
accepted by our

832
00:29:39,960 --> 00:29:44,480
Engineers right this is not saying we

833
00:29:42,840 --> 00:29:46,440
fully automated that process and now

834
00:29:44,480 --> 00:29:48,960
it's done it's saying we had to build in

835
00:29:46,440 --> 00:29:50,640
a process to check it afterwards right

836
00:29:48,960 --> 00:29:52,919
and that's that that problem of having

837
00:29:50,640 --> 00:29:57,440
the the error not be fully

838
00:29:52,919 --> 00:29:59,200
there so another uh another nice example

839
00:29:57,440 --> 00:30:01,640
of this that shows this distinction

840
00:29:59,200 --> 00:30:03,360
between traditional AI uh and what what

841
00:30:01,640 --> 00:30:04,799
these modern systems can do is something

842
00:30:03,360 --> 00:30:08,720
simple multiplying

843
00:30:04,799 --> 00:30:10,559
numbers okay so you type in numbers into

844
00:30:08,720 --> 00:30:12,799
a calculator right you never think it's

845
00:30:10,559 --> 00:30:14,720
going to get multiplication wrong right

846
00:30:12,799 --> 00:30:18,320
you never sort of sit there and question

847
00:30:14,720 --> 00:30:20,799
that so what about chat

848
00:30:18,320 --> 00:30:24,159
GPT

849
00:30:20,799 --> 00:30:26,799
well so on the two axis of this is the

850
00:30:24,159 --> 00:30:28,600
number of digits in your number right

851
00:30:26,799 --> 00:30:31,880
and so if we take for example Le two

852
00:30:28,600 --> 00:30:33,519
digits in each right of the this this uh

853
00:30:31,880 --> 00:30:36,760
experiment that was run it got 100% of

854
00:30:33,519 --> 00:30:39,159
those problems right

855
00:30:36,760 --> 00:30:41,640
fantastic what if you take a 10 digit

856
00:30:39,159 --> 00:30:44,559
number times a 10 digigit

857
00:30:41,640 --> 00:30:47,840
number basically never gets it

858
00:30:44,559 --> 00:30:50,320
right that's kind of weird right like we

859
00:30:47,840 --> 00:30:52,919
think of if you understand how to do

860
00:30:50,320 --> 00:30:54,919
multiplication with two digit numbers

861
00:30:52,919 --> 00:30:56,639
you understand how to do multiplication

862
00:30:54,919 --> 00:30:58,279
with 10 digit numbers and that is true

863
00:30:56,639 --> 00:31:01,440
of your calculator and it's it's not

864
00:30:58,279 --> 00:31:03,240
true of uh these AI models and that's

865
00:31:01,440 --> 00:31:04,519
just because of the way they're trained

866
00:31:03,240 --> 00:31:06,360
right they're trained with particular

867
00:31:04,519 --> 00:31:08,240
examples it seemed tons and tons of

868
00:31:06,360 --> 00:31:10,440
examples of small digit numbers being

869
00:31:08,240 --> 00:31:12,159
multiplied by each other that helps it

870
00:31:10,440 --> 00:31:14,600
bring it together it's seen very very

871
00:31:12,159 --> 00:31:16,440
few examples of crazy long digit numbers

872
00:31:14,600 --> 00:31:17,600
multiplied together and so it doesn't do

873
00:31:16,440 --> 00:31:20,240
as

874
00:31:17,600 --> 00:31:21,880
well okay now this is not an insoluable

875
00:31:20,240 --> 00:31:24,279
problem like if you cared a ton about

876
00:31:21,880 --> 00:31:26,080
multiplication you could give it tons

877
00:31:24,279 --> 00:31:28,320
and tons of those problems and and solve

878
00:31:26,080 --> 00:31:30,679
this right but it does highlight this

879
00:31:28,320 --> 00:31:33,200
important thing which is that like once

880
00:31:30,679 --> 00:31:34,639
you get away from your training data it

881
00:31:33,200 --> 00:31:37,279
actually is not going to do as well and

882
00:31:34,639 --> 00:31:40,960
that could be a problem so I'm this is a

883
00:31:37,279 --> 00:31:43,159
nice another example of this uh so Air

884
00:31:40,960 --> 00:31:44,760
Canada this is just earlier this year

885
00:31:43,159 --> 00:31:46,880
Air Canada must pay damages after

886
00:31:44,760 --> 00:31:49,080
chatbot lies to a grieving passenger

887
00:31:46,880 --> 00:31:51,399
about a discount so this is someone who

888
00:31:49,080 --> 00:31:52,880
call who who wrote in and said hey I've

889
00:31:51,399 --> 00:31:54,639
just had a death in the family I need to

890
00:31:52,880 --> 00:31:57,200
book some flights I understand you have

891
00:31:54,639 --> 00:31:59,919
this discounted rate and the chatbot

892
00:31:57,200 --> 00:32:01,840
said oh yes we do just go ahead and book

893
00:31:59,919 --> 00:32:03,919
a normal ticket anytime in the next 60

894
00:32:01,840 --> 00:32:06,880
days you can go and reclaim your

895
00:32:03,919 --> 00:32:08,080
money except that's not true that's not

896
00:32:06,880 --> 00:32:10,200
how that works you have to do it ahead

897
00:32:08,080 --> 00:32:14,320
of time or you don't get your money

898
00:32:10,200 --> 00:32:17,000
back and so uh Air Canada is defend so

899
00:32:14,320 --> 00:32:19,279
they this person sued Air Canada and Air

900
00:32:17,000 --> 00:32:21,519
Canada said no no no no uh we shouldn't

901
00:32:19,279 --> 00:32:23,639
be liable for the chatbots faulty

902
00:32:21,519 --> 00:32:25,600
outputs seems a little little strange

903
00:32:23,639 --> 00:32:28,000
and in fact the the uh person who was

904
00:32:25,600 --> 00:32:29,320
helping adjudicate this said in effect

905
00:32:28,000 --> 00:32:31,519
air candidate was suggesting that the

906
00:32:29,320 --> 00:32:33,519
chapot is a separate legal entity that's

907
00:32:31,519 --> 00:32:35,720
responsible for its own

908
00:32:33,519 --> 00:32:37,120
actions you may not be surprised to know

909
00:32:35,720 --> 00:32:40,279
this did not this argument did not fly

910
00:32:37,120 --> 00:32:41,559
at all right this this air candidate was

911
00:32:40,279 --> 00:32:44,120
found they did have to to make

912
00:32:41,559 --> 00:32:46,039
recompense to the person but this is an

913
00:32:44,120 --> 00:32:47,840
interesting example right like I'm sure

914
00:32:46,039 --> 00:32:50,120
that they showed it a bunch of examples

915
00:32:47,840 --> 00:32:51,799
of dealing with customers but there was

916
00:32:50,120 --> 00:32:53,760
still this like 10 digit by 10 digit

917
00:32:51,799 --> 00:32:55,639
number problem right where this

918
00:32:53,760 --> 00:32:57,720
particular thing hadn't come up it gave

919
00:32:55,639 --> 00:32:59,720
an answer that seemed very fluid like oh

920
00:32:57,720 --> 00:33:01,120
yeah yes no problem 60 days go ahead

921
00:32:59,720 --> 00:33:03,240
like this sounds like something somebody

922
00:33:01,120 --> 00:33:04,480
might have as a policy it just wasn't

923
00:33:03,240 --> 00:33:08,840
their

924
00:33:04,480 --> 00:33:11,760
policy okay all right so um I I do want

925
00:33:08,840 --> 00:33:13,240
to highlight here just that um we think

926
00:33:11,760 --> 00:33:15,039
this is important enough in my lab we've

927
00:33:13,240 --> 00:33:16,360
actually built uh something called the

928
00:33:15,039 --> 00:33:18,639
AI risk

929
00:33:16,360 --> 00:33:21,240
repository um so you can find this at

930
00:33:18,639 --> 00:33:23,600
air risk. mit.edu

931
00:33:21,240 --> 00:33:25,240
and because we realized that actually

932
00:33:23,600 --> 00:33:27,880
there just wasn't a good place where you

933
00:33:25,240 --> 00:33:29,639
could get a comprehensive s set of risks

934
00:33:27,880 --> 00:33:32,080
and so what we did is create this living

935
00:33:29,639 --> 00:33:34,880
database of now more than 700 risks that

936
00:33:32,080 --> 00:33:36,320
exist from this um Gathering from all of

937
00:33:34,880 --> 00:33:38,240
the people who kind of talking about

938
00:33:36,320 --> 00:33:39,559
this and so and what we found is that

939
00:33:38,240 --> 00:33:41,200
these previous Frameworks were really

940
00:33:39,559 --> 00:33:44,200
only covering a small subset of the

941
00:33:41,200 --> 00:33:47,200
risks that were coming up from AI in

942
00:33:44,200 --> 00:33:47,200
this

943
00:33:47,360 --> 00:33:52,039
okay

944
00:33:49,440 --> 00:33:54,279
so what I told you right was that

945
00:33:52,039 --> 00:33:56,840
building from scratch can be pretty

946
00:33:54,279 --> 00:33:59,360
expensive can be worth it if the value

947
00:33:56,840 --> 00:34:00,159
that you can unlock is big enough right

948
00:33:59,360 --> 00:34:02,159
for the

949
00:34:00,159 --> 00:34:03,639
supermarket being able to do that on the

950
00:34:02,159 --> 00:34:06,200
things that went bad super super

951
00:34:03,639 --> 00:34:08,720
valuable right protein folding super

952
00:34:06,200 --> 00:34:11,679
super valuable other things maybe not

953
00:34:08,720 --> 00:34:15,639
worth it you could take an off-the-shelf

954
00:34:11,679 --> 00:34:17,440
model much much cheaper fantastic but if

955
00:34:15,639 --> 00:34:19,879
it's not exactly on the thing you care

956
00:34:17,440 --> 00:34:22,040
about it could be pretty uncertain right

957
00:34:19,879 --> 00:34:23,520
you could be like hey I tested on some

958
00:34:22,040 --> 00:34:25,320
two-digit numbers that seemed to do very

959
00:34:23,520 --> 00:34:27,119
well not realizing that your customers

960
00:34:25,320 --> 00:34:28,520
were going to ask at 10 digigit problems

961
00:34:27,119 --> 00:34:30,359
and that was going to be

962
00:34:28,520 --> 00:34:31,720
issue okay so the last thing we can

963
00:34:30,359 --> 00:34:33,480
think about here is customizing these

964
00:34:31,720 --> 00:34:35,040
models so we're going to take one of

965
00:34:33,480 --> 00:34:36,760
these models off the shelf that it's

966
00:34:35,040 --> 00:34:38,079
learned a bunch of stuff ahead of time

967
00:34:36,760 --> 00:34:40,079
and now we say but here are the kind of

968
00:34:38,079 --> 00:34:41,960
things that we deal with more more right

969
00:34:40,079 --> 00:34:43,520
here's some of our own data this will

970
00:34:41,960 --> 00:34:44,919
help guide it to be able to do well on

971
00:34:43,520 --> 00:34:47,520
our particular

972
00:34:44,919 --> 00:34:50,159
problems and this can be name called in

973
00:34:47,520 --> 00:34:51,359
a bunch of different ways so fine-tuning

974
00:34:50,159 --> 00:34:53,200
is the version where you sort of take

975
00:34:51,359 --> 00:34:55,839
the model and you actually train it with

976
00:34:53,200 --> 00:34:59,079
some additional data prompt engineering

977
00:34:55,839 --> 00:35:00,400
just says okay like it might normally

978
00:34:59,079 --> 00:35:02,000
give a certain kind of response but if

979
00:35:00,400 --> 00:35:03,680
you give it a prompt you can push it

980
00:35:02,000 --> 00:35:05,760
down in a particular direction and

981
00:35:03,680 --> 00:35:07,599
prompt tuning is actually sort of doing

982
00:35:05,760 --> 00:35:09,599
this more interactively in a way that

983
00:35:07,599 --> 00:35:11,760
does it so for example like the way that

984
00:35:09,599 --> 00:35:13,599
chat gbt treats you all very nicely when

985
00:35:11,760 --> 00:35:16,200
it interacts right that comes from this

986
00:35:13,599 --> 00:35:18,359
kind of uh promp tuning that's

987
00:35:16,200 --> 00:35:20,200
done so the reason this can be important

988
00:35:18,359 --> 00:35:21,800
is the mismatch and so again this was a

989
00:35:20,200 --> 00:35:23,359
case study that we did so this was

990
00:35:21,800 --> 00:35:25,359
somebody who was doing a computer vision

991
00:35:23,359 --> 00:35:27,920
model and the off-the-shelf model was

992
00:35:25,359 --> 00:35:29,400
trained with imag net this very famous

993
00:35:27,920 --> 00:35:32,920
data set that comes from Stanford

994
00:35:29,400 --> 00:35:36,400
University from F Le's lab and what they

995
00:35:32,920 --> 00:35:37,880
were doing was oil drilling in fact

996
00:35:36,400 --> 00:35:40,079
particularly they what they were doing

997
00:35:37,880 --> 00:35:42,280
was the parts for oil drilling and so

998
00:35:40,079 --> 00:35:44,320
you can sort of see here they like this

999
00:35:42,280 --> 00:35:45,640
is a part of the well and you have these

1000
00:35:44,320 --> 00:35:48,359
these parts these big metal parts that

1001
00:35:45,640 --> 00:35:50,720
are going and when they break what

1002
00:35:48,359 --> 00:35:52,920
happens is somebody at the well takes a

1003
00:35:50,720 --> 00:35:55,520
picture and sends it to the company and

1004
00:35:52,920 --> 00:35:56,640
says send me one of these overnight

1005
00:35:55,520 --> 00:35:58,960
because I need to get the well back up

1006
00:35:56,640 --> 00:36:03,040
and going tomorrow

1007
00:35:58,960 --> 00:36:04,960
now the problem is if you put that like

1008
00:36:03,040 --> 00:36:07,560
you know round piece with a you know

1009
00:36:04,960 --> 00:36:10,040
covered in mud and broken right and you

1010
00:36:07,560 --> 00:36:12,839
put that into the such a model is likely

1011
00:36:10,040 --> 00:36:14,960
to come back and be like puddle right as

1012
00:36:12,839 --> 00:36:16,160
your answer and that's not super helpful

1013
00:36:14,960 --> 00:36:17,800
right because it's actually some

1014
00:36:16,160 --> 00:36:20,240
specific you know part

1015
00:36:17,800 --> 00:36:22,240
r65 that is in in the database somewhere

1016
00:36:20,240 --> 00:36:24,520
but of course image net doesn't have

1017
00:36:22,240 --> 00:36:26,000
your parts in that database so you have

1018
00:36:24,520 --> 00:36:27,680
to do this kind of customization and so

1019
00:36:26,000 --> 00:36:28,960
the problem is that these custom

1020
00:36:27,680 --> 00:36:30,440
sometimes these custom labels just don't

1021
00:36:28,960 --> 00:36:34,359
exist ahead of

1022
00:36:30,440 --> 00:36:37,000
time okay so that process that you need

1023
00:36:34,359 --> 00:36:38,480
to do is very common and it comes with

1024
00:36:37,000 --> 00:36:40,599
some cost and so we wrote an article for

1025
00:36:38,480 --> 00:36:42,839
Brookings recently talking about we call

1026
00:36:40,599 --> 00:36:44,440
this the last mile problem in AI it's

1027
00:36:42,839 --> 00:36:45,800
very evocative of of course you know

1028
00:36:44,440 --> 00:36:47,319
what we have in telecommunications it's

1029
00:36:45,800 --> 00:36:49,200
very cheap to get a phone line to within

1030
00:36:47,319 --> 00:36:51,560
a mile of your house it's very expensive

1031
00:36:49,200 --> 00:36:53,400
to get it that last mile into your house

1032
00:36:51,560 --> 00:36:56,400
it's relatively cheap for you to get an

1033
00:36:53,400 --> 00:36:58,079
API that allows you to use chat GPT to

1034
00:36:56,400 --> 00:36:59,440
things close to what you want to to do

1035
00:36:58,079 --> 00:37:01,680
but it can also be pretty expensive to

1036
00:36:59,440 --> 00:37:03,280
do that last step and to give you some

1037
00:37:01,680 --> 00:37:05,720
sense of the cost of this so we did some

1038
00:37:03,280 --> 00:37:06,800
analysis on computer vision and we said

1039
00:37:05,720 --> 00:37:08,200
okay well people have been throwing

1040
00:37:06,800 --> 00:37:10,319
around some really big numbers about

1041
00:37:08,200 --> 00:37:12,680
automation with computer vision how

1042
00:37:10,319 --> 00:37:13,880
bigger does that uh different how big a

1043
00:37:12,680 --> 00:37:16,040
difference do you get if you take into

1044
00:37:13,880 --> 00:37:20,720
account these last mile

1045
00:37:16,040 --> 00:37:22,440
costs well you go from about 35% of jobs

1046
00:37:20,720 --> 00:37:25,599
having a vision task that you would want

1047
00:37:22,440 --> 00:37:27,960
to automate with it down to about 8% of

1048
00:37:25,599 --> 00:37:30,119
jobs right so these last mile things can

1049
00:37:27,960 --> 00:37:31,880
be expensive so I bring this up just to

1050
00:37:30,119 --> 00:37:33,520
say like we talked about the cost of

1051
00:37:31,880 --> 00:37:35,640
training and we said oh well adopting a

1052
00:37:33,520 --> 00:37:37,599
foundation model helps us do that but it

1053
00:37:35,640 --> 00:37:39,319
doesn't fully account for that it only

1054
00:37:37,599 --> 00:37:41,400
accounts for part of it right and with

1055
00:37:39,319 --> 00:37:43,000
that last bit that you do if you only

1056
00:37:41,400 --> 00:37:44,440
have to train it a little bit that that

1057
00:37:43,000 --> 00:37:46,240
can be a good deal but if you have to

1058
00:37:44,440 --> 00:37:48,040
train it a lot as you have to do in some

1059
00:37:46,240 --> 00:37:52,240
of these cases then it could actually be

1060
00:37:48,040 --> 00:37:53,720
pretty expensive again okay all right um

1061
00:37:52,240 --> 00:37:55,400
I think I'm going to just because I'm

1062
00:37:53,720 --> 00:37:57,160
getting close on time here I'm just

1063
00:37:55,400 --> 00:37:59,280
going to skip over the details of this

1064
00:37:57,160 --> 00:38:01,400
but just to say all of these costs that

1065
00:37:59,280 --> 00:38:05,160
I've been talking about with you today

1066
00:38:01,400 --> 00:38:06,359
are about overall like how the are the

1067
00:38:05,160 --> 00:38:08,400
costs of these things but those costs

1068
00:38:06,359 --> 00:38:10,520
are falling and they're falling because

1069
00:38:08,400 --> 00:38:13,280
of Hardware Improvement so as PE as

1070
00:38:10,520 --> 00:38:14,880
Nvidia for example builds better gpus

1071
00:38:13,280 --> 00:38:16,920
right that means that to do one

1072
00:38:14,880 --> 00:38:19,000
operation might cost you half as much as

1073
00:38:16,920 --> 00:38:20,599
it cost you before that obviously saves

1074
00:38:19,000 --> 00:38:22,119
you a bunch of money there's also

1075
00:38:20,599 --> 00:38:23,920
algorithmic progress we just think of

1076
00:38:22,119 --> 00:38:25,880
clever ways to design these systems and

1077
00:38:23,920 --> 00:38:27,599
so you may have heard of Transformers

1078
00:38:25,880 --> 00:38:29,480
right Transformers are not fundamentally

1079
00:38:27,599 --> 00:38:31,319
doing something that a previous version

1080
00:38:29,480 --> 00:38:32,920
of an AI couldn't do but it does it more

1081
00:38:31,319 --> 00:38:34,880
efficiently and that allows you to get a

1082
00:38:32,920 --> 00:38:36,599
big benefit okay and so both of these

1083
00:38:34,880 --> 00:38:38,640
things uh can can come in and make a big

1084
00:38:36,599 --> 00:38:42,040
difference and I'll skip through that

1085
00:38:38,640 --> 00:38:42,040
okay so let me just conclude

1086
00:38:42,079 --> 00:38:46,720
then so I started us out by giving you

1087
00:38:45,240 --> 00:38:48,599
these these numbers which I think were

1088
00:38:46,720 --> 00:38:50,520
the first ones to produce which says

1089
00:38:48,599 --> 00:38:52,160
actually how much is a AI being used in

1090
00:38:50,520 --> 00:38:54,839
Innovation and you see that in some

1091
00:38:52,160 --> 00:38:56,440
areas like computer science 1% of the

1092
00:38:54,839 --> 00:38:59,839
papers that are being published are

1093
00:38:56,440 --> 00:39:02,920
using AI actually in that paper and

1094
00:38:59,839 --> 00:39:05,520
these in particularly these Foundation

1095
00:39:02,920 --> 00:39:08,560
models that but I showed you that deep

1096
00:39:05,520 --> 00:39:10,040
learning success depends on the scale of

1097
00:39:08,560 --> 00:39:11,440
these models and what you're able to do

1098
00:39:10,040 --> 00:39:15,119
and that means that Cutting Edge models

1099
00:39:11,440 --> 00:39:15,119
are very big and getting very expensive

1100
00:39:15,160 --> 00:39:18,520
quickly but you can take these

1101
00:39:16,880 --> 00:39:20,359
Foundation models instead of building

1102
00:39:18,520 --> 00:39:22,880
your own you can take these Foundation

1103
00:39:20,359 --> 00:39:24,720
models and you can do off the shelf work

1104
00:39:22,880 --> 00:39:26,319
that's great but the problem is it can

1105
00:39:24,720 --> 00:39:28,400
do pretty poorly outside of its training

1106
00:39:26,319 --> 00:39:30,319
regime and our tuitions about when it

1107
00:39:28,400 --> 00:39:31,839
will do poorly may not be good right

1108
00:39:30,319 --> 00:39:33,480
because we might say oh it looks like

1109
00:39:31,839 --> 00:39:35,000
it's doing two-digit multiplication

1110
00:39:33,480 --> 00:39:36,480
right I assume it will get

1111
00:39:35,000 --> 00:39:38,359
multiplication right in general and

1112
00:39:36,480 --> 00:39:41,760
that's not necessarily the

1113
00:39:38,359 --> 00:39:43,520
case and you can adopt these models and

1114
00:39:41,760 --> 00:39:44,760
adapt them to for what you want to do

1115
00:39:43,520 --> 00:39:46,040
the problem is that can carry a bunch of

1116
00:39:44,760 --> 00:39:49,400
Last Mile

1117
00:39:46,040 --> 00:39:51,680
costs and then finally I said that you

1118
00:39:49,400 --> 00:39:54,079
know all of these things it is a moving

1119
00:39:51,680 --> 00:39:55,319
picture these costs are falling both

1120
00:39:54,079 --> 00:39:57,079
because of hardware and algorithmic

1121
00:39:55,319 --> 00:39:58,839
improvements and so a really key thing

1122
00:39:57,079 --> 00:40:00,680
for to think about this is not to say

1123
00:39:58,839 --> 00:40:03,240
when you do your analysis and say do I

1124
00:40:00,680 --> 00:40:06,599
want to use this AI in this particular

1125
00:40:03,240 --> 00:40:08,040
area of science don't say like yes no if

1126
00:40:06,599 --> 00:40:10,040
you say no I don't want to use it

1127
00:40:08,040 --> 00:40:12,520
because maybe the cost is not right then

1128
00:40:10,040 --> 00:40:14,760
do a second calculation that says okay

1129
00:40:12,520 --> 00:40:16,880
at what time would this actually become

1130
00:40:14,760 --> 00:40:18,880
attractive right because in some cases

1131
00:40:16,880 --> 00:40:20,400
that might only be a few years away and

1132
00:40:18,880 --> 00:40:22,000
you could already start taking steps for

1133
00:40:20,400 --> 00:40:23,640
example to gather your data sets or

1134
00:40:22,000 --> 00:40:25,920
stuff like that to get ready for when it

1135
00:40:23,640 --> 00:40:29,079
will be ready and with that I'll stop

1136
00:40:25,920 --> 00:40:29,079
thank you

1137
00:40:34,000 --> 00:40:36,920
thank you Neil it's fascinating to talk

1138
00:40:35,640 --> 00:40:38,160
you got a lot of questions here I'm just

1139
00:40:36,920 --> 00:40:39,839
going to tell the audience we're coming

1140
00:40:38,160 --> 00:40:42,200
up on time I know there's a network

1141
00:40:39,839 --> 00:40:43,119
reception next door so I know we're not

1142
00:40:42,200 --> 00:40:44,599
going to get through all these questions

1143
00:40:43,119 --> 00:40:46,079
we'll take one or two of them but if you

1144
00:40:44,599 --> 00:40:47,839
do have questions for new please stay

1145
00:40:46,079 --> 00:40:51,520
afterwards and feel free to ask him

1146
00:40:47,839 --> 00:40:53,040
those questions too okay so I think um

1147
00:40:51,520 --> 00:40:53,960
one of the questions I've heard a lot

1148
00:40:53,040 --> 00:40:55,560
certainly from a lot of different

1149
00:40:53,960 --> 00:40:57,480
organizations that we deal with is do

1150
00:40:55,560 --> 00:41:00,119
you have any advice for corporations to

1151
00:40:57,480 --> 00:41:05,079
develop AI literacy and to discern the

1152
00:41:00,119 --> 00:41:05,079
organizational Readiness and efficacy of

1153
00:41:07,560 --> 00:41:13,440
models so I mean it's a it's a super

1154
00:41:10,400 --> 00:41:16,560
hard problem right um and it's it's a

1155
00:41:13,440 --> 00:41:18,520
hard problem because it's like at some

1156
00:41:16,560 --> 00:41:20,720
level many of these problems have sort

1157
00:41:18,520 --> 00:41:22,640
of traditional data science problems and

1158
00:41:20,720 --> 00:41:23,839
then you have particular AI problems

1159
00:41:22,640 --> 00:41:25,240
right and so I imagine you you probably

1160
00:41:23,839 --> 00:41:27,839
heard other speakers talk about this but

1161
00:41:25,240 --> 00:41:30,200
it's you know if your data is really

1162
00:41:27,839 --> 00:41:31,880
poor right it will go really badly if

1163
00:41:30,200 --> 00:41:33,440
your processes don't documented very

1164
00:41:31,880 --> 00:41:35,079
well it can go really badly so let me

1165
00:41:33,440 --> 00:41:36,480
let me be sort of concrete about this I

1166
00:41:35,079 --> 00:41:39,920
I mentioned that example of the

1167
00:41:36,480 --> 00:41:41,240
supermarket right turns out that even

1168
00:41:39,920 --> 00:41:42,560
before they got to the level where they

1169
00:41:41,240 --> 00:41:44,400
got the one-third benefit and they were

1170
00:41:42,560 --> 00:41:46,599
like okay fantastic the first

1171
00:41:44,400 --> 00:41:47,960
predictions that they made were terrible

1172
00:41:46,599 --> 00:41:49,920
and in an earlier version of the model

1173
00:41:47,960 --> 00:41:51,839
you say well why was it terrible well it

1174
00:41:49,920 --> 00:41:53,640
was terrible because almost nothing

1175
00:41:51,839 --> 00:41:55,040
about those changes that were made in

1176
00:41:53,640 --> 00:41:57,240
the models was captured so I mentioned

1177
00:41:55,040 --> 00:41:58,640
that the old system right you said it

1178
00:41:57,240 --> 00:42:00,079
predicted the number of bananas and then

1179
00:41:58,640 --> 00:42:02,119
some manager put in there and was like

1180
00:42:00,079 --> 00:42:04,000
oh no it's a little bit higher today

1181
00:42:02,119 --> 00:42:07,160
those extra changes never got put back

1182
00:42:04,000 --> 00:42:09,680
in the system so what that meant was

1183
00:42:07,160 --> 00:42:12,079
that in practice like you might have a s

1184
00:42:09,680 --> 00:42:14,800
the system might say oh we sold this

1185
00:42:12,079 --> 00:42:16,480
many right so so say say take it as an

1186
00:42:14,800 --> 00:42:18,800
example maybe the manager decreased the

1187
00:42:16,480 --> 00:42:21,160
amount that was going be ordered right

1188
00:42:18,800 --> 00:42:22,440
and they sold all of those the system

1189
00:42:21,160 --> 00:42:24,400
would still think you had five more on

1190
00:42:22,440 --> 00:42:25,800
the shelf and they just didn't sell so

1191
00:42:24,400 --> 00:42:26,960
we would think like oh that's that was

1192
00:42:25,800 --> 00:42:28,720
the level of demand but maybe you could

1193
00:42:26,960 --> 00:42:31,520
have sold more that just they ordered

1194
00:42:28,720 --> 00:42:33,559
the wrong amount right or you had stores

1195
00:42:31,520 --> 00:42:35,400
where they just not recorded when the

1196
00:42:33,559 --> 00:42:38,000
store was closed for a holiday or closed

1197
00:42:35,400 --> 00:42:40,680
for repairs and so I was like oh every

1198
00:42:38,000 --> 00:42:42,839
you know two one Tuesday every three

1199
00:42:40,680 --> 00:42:43,920
years there no sales at all and it was

1200
00:42:42,839 --> 00:42:46,160
thinking that that was like the right

1201
00:42:43,920 --> 00:42:48,520
prediction not that there was actually

1202
00:42:46,160 --> 00:42:50,400
thing so this that that sort of like

1203
00:42:48,520 --> 00:42:51,920
messiness of your data can be a problem

1204
00:42:50,400 --> 00:42:54,200
and so that's hugely important for AI

1205
00:42:51,920 --> 00:42:56,319
Readiness if you have bad data on for

1206
00:42:54,200 --> 00:42:57,640
some particular area you you need to be

1207
00:42:56,319 --> 00:42:59,040
thinking about it but then there's a

1208
00:42:57,640 --> 00:43:00,200
second thing about like okay well how do

1209
00:42:59,040 --> 00:43:02,040
you actually know that your people have

1210
00:43:00,200 --> 00:43:03,319
the right level thing and I don't I

1211
00:43:02,040 --> 00:43:05,760
don't think I have a great answer for

1212
00:43:03,319 --> 00:43:07,040
that other than to say you know in the

1213
00:43:05,760 --> 00:43:08,920
same way that you think about expertise

1214
00:43:07,040 --> 00:43:10,680
in other areas you know think about

1215
00:43:08,920 --> 00:43:12,079
trying to have build some teams maybe

1216
00:43:10,680 --> 00:43:13,720
bring in some external people right

1217
00:43:12,079 --> 00:43:15,280
we've definitely seen a bunch of cases

1218
00:43:13,720 --> 00:43:17,359
where if people didn't think they were

1219
00:43:15,280 --> 00:43:19,200
ready they hired a Consulting team that

1220
00:43:17,359 --> 00:43:20,640
Consulting team either did it and then

1221
00:43:19,200 --> 00:43:22,520
hinded it off or sometimes even like

1222
00:43:20,640 --> 00:43:24,280
left some of their people as a way to

1223
00:43:22,520 --> 00:43:27,359
build some of that

1224
00:43:24,280 --> 00:43:29,400
Readiness thanks Neil let's ask one more

1225
00:43:27,359 --> 00:43:31,800
question this last question of the day

1226
00:43:29,400 --> 00:43:35,119
do you uh excuse me what do you think

1227
00:43:31,800 --> 00:43:37,119
the graph on different fields using AI

1228
00:43:35,119 --> 00:43:40,920
for Innovation is going to look like in

1229
00:43:37,119 --> 00:43:42,960
10 years or 10 years from now

1230
00:43:40,920 --> 00:43:46,520
ah

1231
00:43:42,960 --> 00:43:48,559
so so it'll be a lot higher I think it's

1232
00:43:46,520 --> 00:43:50,760
going to converge I don't think we

1233
00:43:48,559 --> 00:43:52,640
should assume that just like the fields

1234
00:43:50,760 --> 00:43:56,480
that started early are the ones that are

1235
00:43:52,640 --> 00:43:58,599
going to end up higher right so um you

1236
00:43:56,480 --> 00:43:59,839
know if for example in medicine I think

1237
00:43:58,599 --> 00:44:02,240
there are a lot of there's a lot of

1238
00:43:59,839 --> 00:44:03,960
potential in medicine for uh these these

1239
00:44:02,240 --> 00:44:05,359
systems to be very very impactful but I

1240
00:44:03,960 --> 00:44:06,880
think it's you know many of the people

1241
00:44:05,359 --> 00:44:08,680
innovating in that space do not come in

1242
00:44:06,880 --> 00:44:10,480
with a programming background or data

1243
00:44:08,680 --> 00:44:11,640
science background and so I think that

1244
00:44:10,480 --> 00:44:13,000
might be an area that will take longer

1245
00:44:11,640 --> 00:44:14,800
to get there but I would expect it to be

1246
00:44:13,000 --> 00:44:16,359
very high as we get there I also think

1247
00:44:14,800 --> 00:44:18,800
we're going to see some new some new

1248
00:44:16,359 --> 00:44:20,359
areas right so for example places in

1249
00:44:18,800 --> 00:44:22,040
like Material Science I think this could

1250
00:44:20,359 --> 00:44:23,599
be very very important for a lot of that

1251
00:44:22,040 --> 00:44:25,559
kind of stuff and so those will go up at

1252
00:44:23,599 --> 00:44:27,200
time over time but I think broadly what

1253
00:44:25,559 --> 00:44:29,359
we're going to see is VE very large

1254
00:44:27,200 --> 00:44:31,599
option it is I will maybe just leave

1255
00:44:29,359 --> 00:44:33,359
this as a bit of a a hint for some

1256
00:44:31,599 --> 00:44:34,400
future meeting but it is interesting

1257
00:44:33,359 --> 00:44:37,079
some of the work that we've been doing

1258
00:44:34,400 --> 00:44:38,640
in my lab does suggest that actually we

1259
00:44:37,079 --> 00:44:40,520
should not expect it to be useful

1260
00:44:38,640 --> 00:44:42,440
everywhere that in some areas of science

1261
00:44:40,520 --> 00:44:44,280
we see it being useful in some areas not

1262
00:44:42,440 --> 00:44:46,400
and we're right now trying to pull apart

1263
00:44:44,280 --> 00:44:47,960
exactly why that is in particular areas

1264
00:44:46,400 --> 00:44:49,160
to be able to give some insights in it

1265
00:44:47,960 --> 00:44:51,599
but I do think it's going to be the case

1266
00:44:49,160 --> 00:44:53,040
and in some places actually as AI is not

1267
00:44:51,599 --> 00:44:55,079
as good as other algorithms that people

1268
00:44:53,040 --> 00:44:57,800
have developed for these

1269
00:44:55,079 --> 00:45:01,359
things thank you so much NE

1270
00:44:57,800 --> 00:45:01,359
really great talk

