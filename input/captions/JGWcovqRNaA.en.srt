1
00:00:06,040 --> 00:00:13,280
AI is not just a technology problem.

2
00:00:09,920 --> 00:00:17,680
It's also a human design problem.

3
00:00:13,280 --> 00:00:20,880
believe it or not and uh if you our next

4
00:00:17,680 --> 00:00:25,000
speaker, Professor Patty May will

5
00:00:20,880 --> 00:00:28,800
discuss about this and also explore how

6
00:00:25,000 --> 00:00:32,960
optimizing the human AI interface can

7
00:00:28,800 --> 00:00:37,680
drive better outcome. Now, Professor

8
00:00:32,960 --> 00:00:41,440
Patty May is uh germs Hawome professor

9
00:00:37,680 --> 00:00:45,399
of media art and science. Let's welcome

10
00:00:41,440 --> 00:00:45,399
welcome professor May.

11
00:00:46,090 --> 00:00:53,760
[Applause]

12
00:00:50,480 --> 00:00:56,879
Good afternoon.

13
00:00:53,760 --> 00:00:59,680
So, one of the fun things about being a

14
00:00:56,879 --> 00:01:03,600
faculty member at MIT is that every year

15
00:00:59,680 --> 00:01:07,680
you get this new crop of young students

16
00:01:03,600 --> 00:01:10,080
that are very eager to sort of adopt the

17
00:01:07,680 --> 00:01:12,479
latest technologies, whether they're

18
00:01:10,080 --> 00:01:14,799
commercial things or more often even

19
00:01:12,479 --> 00:01:17,200
things that they've developed themselves

20
00:01:14,799 --> 00:01:19,920
that they think will improve their lives

21
00:01:17,200 --> 00:01:23,119
and uh make it possible to accomplish

22
00:01:19,920 --> 00:01:26,799
more and so on. And so it really um

23
00:01:23,119 --> 00:01:30,799
offers us faculty an opportunity to see

24
00:01:26,799 --> 00:01:34,159
what our future um way of living with

25
00:01:30,799 --> 00:01:38,799
technology may look like. And so these

26
00:01:34,159 --> 00:01:42,320
days um that actually future that I see

27
00:01:38,799 --> 00:01:45,119
um my students uh adopting is one not

28
00:01:42,320 --> 00:01:48,240
unlike the one in the movie her which is

29
00:01:45,119 --> 00:01:50,880
a great movie by Spike Jones by the way.

30
00:01:48,240 --> 00:01:55,360
um what is depicted in that movie

31
00:01:50,880 --> 00:01:59,280
basically the students spend many hours

32
00:01:55,360 --> 00:02:01,439
a day conversing with AI AI really

33
00:01:59,280 --> 00:02:05,320
mediates and helps them with every

34
00:02:01,439 --> 00:02:08,640
aspect of their lives not just coding or

35
00:02:05,320 --> 00:02:11,319
workrelated things looking up research

36
00:02:08,640 --> 00:02:14,879
etc getting new ideas for research

37
00:02:11,319 --> 00:02:17,920
projects but also deciding uh what

38
00:02:14,879 --> 00:02:21,760
present they may want to uh buy for

39
00:02:17,920 --> 00:02:24,239
their uh boyfriend or girlfriend or

40
00:02:21,760 --> 00:02:27,520
dealing with medical issues that they

41
00:02:24,239 --> 00:02:32,000
may have. It acts as their coach. It

42
00:02:27,520 --> 00:02:34,800
acts as their friend. Um it acts as u uh

43
00:02:32,000 --> 00:02:38,640
everything sort of hopefully not lover

44
00:02:34,800 --> 00:02:42,760
uh like in the movie but um who knows.

45
00:02:38,640 --> 00:02:46,879
Um so of course that future is very

46
00:02:42,760 --> 00:02:49,040
exciting once we have AI with us all the

47
00:02:46,879 --> 00:02:52,080
time sort of helping us with all these

48
00:02:49,040 --> 00:02:55,040
aspects of our lives. Uh we can augment

49
00:02:52,080 --> 00:02:57,760
our performance and productivity

50
00:02:55,040 --> 00:03:01,280
um for in the area of learning. Um it

51
00:02:57,760 --> 00:03:03,519
enables this like personalized situated

52
00:03:01,280 --> 00:03:06,239
learning in the area of health.

53
00:03:03,519 --> 00:03:09,280
Increasingly, we're seeing uh that

54
00:03:06,239 --> 00:03:13,040
companies are integrating these um chat

55
00:03:09,280 --> 00:03:15,360
bots uh with smart watches and other

56
00:03:13,040 --> 00:03:18,400
sensors that we wear all the time. So,

57
00:03:15,360 --> 00:03:20,319
they'll be able to help us with our uh

58
00:03:18,400 --> 00:03:22,400
interpreting what is going on with our

59
00:03:20,319 --> 00:03:25,760
health, with our sleep and all of that

60
00:03:22,400 --> 00:03:27,800
in intelligent ways. Um, for the

61
00:03:25,760 --> 00:03:31,440
elderly, I think there's enormous

62
00:03:27,800 --> 00:03:34,480
potential for these uh AI assistants,

63
00:03:31,440 --> 00:03:37,680
especially wearable AI assistants to

64
00:03:34,480 --> 00:03:40,120
help people live in their own lives uh

65
00:03:37,680 --> 00:03:42,480
live in their own homes for longer

66
00:03:40,120 --> 00:03:44,080
independently. Um, of course, in media,

67
00:03:42,480 --> 00:03:46,720
it's going to change media and

68
00:03:44,080 --> 00:03:48,799
entertainment completely and uh towards

69
00:03:46,720 --> 00:03:51,680
sort of more personalized, much more

70
00:03:48,799 --> 00:03:55,120
interactive u media and entertainment

71
00:03:51,680 --> 00:03:57,280
and more. But of course there are still

72
00:03:55,120 --> 00:04:00,239
a lot of challenges and I'm sure you've

73
00:03:57,280 --> 00:04:02,319
heard about them today. Um often uh

74
00:04:00,239 --> 00:04:04,560
there's a lot of technical challenges

75
00:04:02,319 --> 00:04:07,280
things like hallucinations and uh

76
00:04:04,560 --> 00:04:10,480
inaccuracies and the weak reasoning

77
00:04:07,280 --> 00:04:14,640
skills of AI. AI is biased. There are

78
00:04:10,480 --> 00:04:18,000
safety issues, privacy concerns, um cost

79
00:04:14,640 --> 00:04:21,680
issues, environmental cost issues and

80
00:04:18,000 --> 00:04:24,960
more. But these are not the only

81
00:04:21,680 --> 00:04:28,639
challenges that we have to solve uh for

82
00:04:24,960 --> 00:04:31,759
AI to ultimately be successful and

83
00:04:28,639 --> 00:04:35,280
really help us with uh all of the and

84
00:04:31,759 --> 00:04:38,000
improve our lives and work. AI is not

85
00:04:35,280 --> 00:04:40,960
just an engineering challenge. It's also

86
00:04:38,000 --> 00:04:43,759
a human design challenge. In the work

87
00:04:40,960 --> 00:04:47,280
that we do in my lab, we show that there

88
00:04:43,759 --> 00:04:50,560
are a lot of risks um and human

89
00:04:47,280 --> 00:04:54,240
challenges that we have to overcome when

90
00:04:50,560 --> 00:04:57,840
we inter uh integrate um AI into

91
00:04:54,240 --> 00:05:01,919
people's uh daily work and lives. Things

92
00:04:57,840 --> 00:05:05,520
like over reliance and misplaced trust.

93
00:05:01,919 --> 00:05:07,759
uh one of my students had AI write a a

94
00:05:05,520 --> 00:05:11,360
related work section of a paper before

95
00:05:07,759 --> 00:05:13,840
we submitted it and uh only uh just

96
00:05:11,360 --> 00:05:16,080
before we submitted it we realized that

97
00:05:13,840 --> 00:05:19,199
most of the citations

98
00:05:16,080 --> 00:05:22,000
uh were non-existing ex uh apart from

99
00:05:19,199 --> 00:05:25,840
the fact that the citations were all

100
00:05:22,000 --> 00:05:28,320
very believable so it really required a

101
00:05:25,840 --> 00:05:32,479
lot of work it wasn't nonsense it looked

102
00:05:28,320 --> 00:05:33,800
very believable and very real Um there's

103
00:05:32,479 --> 00:05:36,560
loss of

104
00:05:33,800 --> 00:05:40,720
understanding increasingly if if you

105
00:05:36,560 --> 00:05:42,479
rely on AI sort of to um help you with

106
00:05:40,720 --> 00:05:45,039
interpreting information, working

107
00:05:42,479 --> 00:05:47,759
through things and so on, we risk

108
00:05:45,039 --> 00:05:50,639
understanding things ourselves less. I

109
00:05:47,759 --> 00:05:53,440
think there's a very dangerous uh

110
00:05:50,639 --> 00:05:56,160
phenomenon that people in AI are talking

111
00:05:53,440 --> 00:06:00,080
about the AI developers are uh talking

112
00:05:56,160 --> 00:06:03,360
about called vibe coding where instead

113
00:06:00,080 --> 00:06:05,840
of understanding your code you actually

114
00:06:03,360 --> 00:06:08,000
just sort of tell an AI in natural

115
00:06:05,840 --> 00:06:09,840
language yeah yeah build me this app and

116
00:06:08,000 --> 00:06:12,560
it should do this that and that and then

117
00:06:09,840 --> 00:06:15,680
you see how it behaves but you never

118
00:06:12,560 --> 00:06:19,120
actually look at the actual code to make

119
00:06:15,680 --> 00:06:21,360
sure that that vote is um doing the

120
00:06:19,120 --> 00:06:26,080
right thing. I think it's extremely

121
00:06:21,360 --> 00:06:29,360
dangerous to um rely so much and um on

122
00:06:26,080 --> 00:06:32,080
AI to do these things that ultimately uh

123
00:06:29,360 --> 00:06:35,199
we have to understand and be able to

124
00:06:32,080 --> 00:06:38,720
control ourselves. So there's a risk for

125
00:06:35,199 --> 00:06:41,120
a loss of agency that we lost uh the

126
00:06:38,720 --> 00:06:43,520
control of the systems that govern our

127
00:06:41,120 --> 00:06:45,560
lives and that run our companies and so

128
00:06:43,520 --> 00:06:48,160
on. Uh

129
00:06:45,560 --> 00:06:50,240
deskkilling is another risk. Uh we're

130
00:06:48,160 --> 00:06:53,360
showing uh in some of the experiments

131
00:06:50,240 --> 00:06:56,160
that we do uh in my lab that the more

132
00:06:53,360 --> 00:06:58,000
you rely on AI to help you with a skill,

133
00:06:56,160 --> 00:07:00,800
of course, the less you develop that

134
00:06:58,000 --> 00:07:02,800
skill yourself or the worse you get

135
00:07:00,800 --> 00:07:04,880
yourself at that skill. So we're not

136
00:07:02,800 --> 00:07:07,240
thinking long term about the

137
00:07:04,880 --> 00:07:09,520
consequences of adopting these

138
00:07:07,240 --> 00:07:12,400
technologies. Misinformation and

139
00:07:09,520 --> 00:07:15,039
manipulation. We've done work in my lab

140
00:07:12,400 --> 00:07:18,240
together with Elizabeth Loftus to show

141
00:07:15,039 --> 00:07:20,800
how easy it is for an AI, especially one

142
00:07:18,240 --> 00:07:23,440
that you get to trust over time as it

143
00:07:20,800 --> 00:07:26,080
knows you and helps you for a longer

144
00:07:23,440 --> 00:07:27,759
period of time, has a user model and

145
00:07:26,080 --> 00:07:30,319
memory of everything you've talked

146
00:07:27,759 --> 00:07:33,360
about. People start trusting these

147
00:07:30,319 --> 00:07:36,960
systems so much that then that trust can

148
00:07:33,360 --> 00:07:40,319
be abused to slip misinformation

149
00:07:36,960 --> 00:07:43,360
um uh into people's uh minds and and

150
00:07:40,319 --> 00:07:46,199
really uh persuade uh them to have

151
00:07:43,360 --> 00:07:49,199
certain beliefs or adopt certain

152
00:07:46,199 --> 00:07:51,520
behaviors and so on. unhealthy

153
00:07:49,199 --> 00:07:54,160
attachment. I'll talk about that in a

154
00:07:51,520 --> 00:07:57,919
little bit, but it does exist of people

155
00:07:54,160 --> 00:08:00,520
to AI and people literally replacing

156
00:07:57,919 --> 00:08:03,280
human relationships with AI

157
00:08:00,520 --> 00:08:05,919
relationships. Uh dehumanization that

158
00:08:03,280 --> 00:08:08,879
that leads to. The more you talk to AI

159
00:08:05,919 --> 00:08:11,080
that is always um aiming to please you

160
00:08:08,879 --> 00:08:13,360
and telling you that you are right,

161
00:08:11,080 --> 00:08:15,520
etc., of course, the less good you

162
00:08:13,360 --> 00:08:18,560
become at talking to real people who may

163
00:08:15,520 --> 00:08:21,759
not always agree with you. and weakening

164
00:08:18,560 --> 00:08:25,360
of social ties. Um, those same students

165
00:08:21,759 --> 00:08:27,440
that spend two hours talking to AI.

166
00:08:25,360 --> 00:08:29,599
Well, they spend two hours talking to

167
00:08:27,440 --> 00:08:31,919
other students and family members and

168
00:08:29,599 --> 00:08:34,479
friends before that that they may not

169
00:08:31,919 --> 00:08:37,360
talk to as much right now because a lot

170
00:08:34,479 --> 00:08:40,159
of their problems are solved by AI. So,

171
00:08:37,360 --> 00:08:41,760
I think it will result in weaker social

172
00:08:40,159 --> 00:08:44,320
networks.

173
00:08:41,760 --> 00:08:47,680
So the central question of my research

174
00:08:44,320 --> 00:08:50,480
is as AI advances and and it is a very

175
00:08:47,680 --> 00:08:53,839
exciting advancement. How do we ensure

176
00:08:50,480 --> 00:08:56,800
that people and humanity will advance as

177
00:08:53,839 --> 00:09:00,080
well? And we do a lot of experiments to

178
00:08:56,800 --> 00:09:02,399
show that this is a real problem. But we

179
00:09:00,080 --> 00:09:05,279
also try to come up with solutions

180
00:09:02,399 --> 00:09:08,000
actually that um companies and everybody

181
00:09:05,279 --> 00:09:11,600
can adopt. For example, in the area of

182
00:09:08,000 --> 00:09:14,720
decision making, um, increasingly our

183
00:09:11,600 --> 00:09:18,000
information processing is mediated by

184
00:09:14,720 --> 00:09:19,920
AI, right? Um, whether not just when

185
00:09:18,000 --> 00:09:23,760
you're a doctor, say, making medical

186
00:09:19,920 --> 00:09:26,240
decisions as we just heard, but also um

187
00:09:23,760 --> 00:09:28,800
your access to information increasingly

188
00:09:26,240 --> 00:09:31,360
you see AI summaries instead of doing

189
00:09:28,800 --> 00:09:35,680
searches and looking at source materials

190
00:09:31,360 --> 00:09:38,480
and so on. So what happens when people

191
00:09:35,680 --> 00:09:41,360
always have AI available at their

192
00:09:38,480 --> 00:09:45,040
fingertips when making decisions? We did

193
00:09:41,360 --> 00:09:48,000
a study that shows u or that looked at

194
00:09:45,040 --> 00:09:52,240
uh a thousand newspaper headlines um

195
00:09:48,000 --> 00:09:55,200
that u were either true or false and um

196
00:09:52,240 --> 00:09:58,880
we had a number of human subjects that

197
00:09:55,200 --> 00:10:02,000
um could actually be advised by AI. They

198
00:09:58,880 --> 00:10:04,399
had a AI right there telling them I

199
00:10:02,000 --> 00:10:07,440
think this newspaper headline is true or

200
00:10:04,399 --> 00:10:10,959
false and giving an explanation uh for

201
00:10:07,440 --> 00:10:14,399
why it uh thought so as well. And what

202
00:10:10,959 --> 00:10:17,600
we learned is of course um in gray you

203
00:10:14,399 --> 00:10:20,640
see um how well people do without AI

204
00:10:17,600 --> 00:10:24,079
assistance. So we had three um groups of

205
00:10:20,640 --> 00:10:26,720
people. Some people used uh no AI in

206
00:10:24,079 --> 00:10:28,880
gray. Some people used an accurate AI

207
00:10:26,720 --> 00:10:31,200
that gives you the ground truth for

208
00:10:28,880 --> 00:10:33,680
those thousand headlines. And some

209
00:10:31,200 --> 00:10:35,760
people had a deceptive AI that tries to

210
00:10:33,680 --> 00:10:38,640
convince you of the opposite of the

211
00:10:35,760 --> 00:10:41,160
truth. And so in gray you see how well

212
00:10:38,640 --> 00:10:44,000
people do without AI for true and false

213
00:10:41,160 --> 00:10:46,720
headlines. Um in blue you see how well

214
00:10:44,000 --> 00:10:49,360
they do when they have a an accurate uh

215
00:10:46,720 --> 00:10:52,240
truthful AI at their fingertips and

216
00:10:49,360 --> 00:10:55,279
their performance does improve. But in

217
00:10:52,240 --> 00:10:57,440
red, you see how poorly they do when

218
00:10:55,279 --> 00:11:00,000
they have an AI at their fingertips that

219
00:10:57,440 --> 00:11:02,560
gives them inaccurate information.

220
00:11:00,000 --> 00:11:06,160
People really believe in these systems

221
00:11:02,560 --> 00:11:08,480
too much and um adopt their

222
00:11:06,160 --> 00:11:11,519
recommendations and their feedback too

223
00:11:08,480 --> 00:11:13,920
readily without thinking for themselves.

224
00:11:11,519 --> 00:11:16,959
We showed in a follow-up paper that the

225
00:11:13,920 --> 00:11:20,320
solution to this is to have a little bit

226
00:11:16,959 --> 00:11:23,839
of friction and not make it too easy for

227
00:11:20,320 --> 00:11:26,399
someone to get the AI's uh decision on a

228
00:11:23,839 --> 00:11:29,279
particular problem but instead you have

229
00:11:26,399 --> 00:11:32,000
to make you have to force the person to

230
00:11:29,279 --> 00:11:34,640
engage in thinking about the problem a

231
00:11:32,000 --> 00:11:38,320
little bit themselves so that you can

232
00:11:34,640 --> 00:11:41,040
really benefit from both the human

233
00:11:38,320 --> 00:11:44,399
intelligence and critical thinking as as

234
00:11:41,040 --> 00:11:48,079
well as the AI uh intelligence in

235
00:11:44,399 --> 00:11:50,560
quotes. Um so AI that actually first

236
00:11:48,079 --> 00:11:53,200
asks you a question about the decision

237
00:11:50,560 --> 00:11:57,040
that you're making results in even

238
00:11:53,200 --> 00:11:59,360
better performance than an AI that uh is

239
00:11:57,040 --> 00:12:03,320
accurate and in much better for

240
00:11:59,360 --> 00:12:06,880
performance than uh the uh malicious

241
00:12:03,320 --> 00:12:10,399
AI. So for AI to deployments to be

242
00:12:06,880 --> 00:12:12,880
successful in a real context um it's not

243
00:12:10,399 --> 00:12:15,839
sufficient for models to be accurate and

244
00:12:12,880 --> 00:12:19,360
fair and ethical and explainable. But we

245
00:12:15,839 --> 00:12:23,040
really need to test how people respond

246
00:12:19,360 --> 00:12:25,200
to these having these uh systems at uh

247
00:12:23,040 --> 00:12:28,040
their fingertips and working with these

248
00:12:25,200 --> 00:12:31,120
systems and we have to optimize

249
00:12:28,040 --> 00:12:35,360
objectives that are really of a human

250
00:12:31,120 --> 00:12:38,160
nature as well as an AI nature. Some

251
00:12:35,360 --> 00:12:40,480
other examples um you heard today about

252
00:12:38,160 --> 00:12:43,360
AI being used in learning and of course

253
00:12:40,480 --> 00:12:46,560
there is uh a lot of worry on behalf of

254
00:12:43,360 --> 00:12:49,200
teachers and parents uh because kids can

255
00:12:46,560 --> 00:12:53,760
too readily just ask AI to do their

256
00:12:49,200 --> 00:12:56,800
homework for them. So um but we did a

257
00:12:53,760 --> 00:12:59,680
study to show what the effects are on

258
00:12:56,800 --> 00:13:02,800
learning if you use AI and this doesn't

259
00:12:59,680 --> 00:13:04,880
just apply to students but really to the

260
00:13:02,800 --> 00:13:09,200
workplace as well.

261
00:13:04,880 --> 00:13:11,920
So we did a study where we had um two

262
00:13:09,200 --> 00:13:14,639
tasks that our uh human subjects had to

263
00:13:11,920 --> 00:13:17,079
perform writing an essay and then a

264
00:13:14,639 --> 00:13:20,360
computer um uh programming problem

265
00:13:17,079 --> 00:13:23,040
basically. And we had three groups of

266
00:13:20,360 --> 00:13:26,800
subjects. People that could use chat

267
00:13:23,040 --> 00:13:30,320
GPT, people that could use just search

268
00:13:26,800 --> 00:13:32,480
uh so Google without the AI summary and

269
00:13:30,320 --> 00:13:36,079
uh people who could only use their own

270
00:13:32,480 --> 00:13:40,160
brain, no tools. And we actually had um

271
00:13:36,079 --> 00:13:43,839
uh EEG cap um and we also looked at eye

272
00:13:40,160 --> 00:13:45,920
gaze etc to analyze what happened in the

273
00:13:43,839 --> 00:13:47,839
brains of people uh these different

274
00:13:45,920 --> 00:13:50,399
groups as they were solving these

275
00:13:47,839 --> 00:13:52,959
problems. The essay by the way was an

276
00:13:50,399 --> 00:13:55,120
SAT prompt uh for those of you that have

277
00:13:52,959 --> 00:13:57,079
like 17 or 18 year olds you know what

278
00:13:55,120 --> 00:14:00,720
I'm talking about.

279
00:13:57,079 --> 00:14:03,360
Um so the differences that we saw in

280
00:14:00,720 --> 00:14:06,560
average brain activity were striking.

281
00:14:03,360 --> 00:14:09,760
Actually the group that used Chachi PT

282
00:14:06,560 --> 00:14:12,639
showed the least brain activity uh

283
00:14:09,760 --> 00:14:15,120
especially in the prefrontal cortex uh

284
00:14:12,639 --> 00:14:16,519
sort of where a lot of the uh uh

285
00:14:15,120 --> 00:14:20,399
reasoning goes

286
00:14:16,519 --> 00:14:22,399
on. The Google group did a lot of visual

287
00:14:20,399 --> 00:14:24,560
activity that's happening in the back of

288
00:14:22,399 --> 00:14:27,199
the head. uh they had to look at a lot

289
00:14:24,560 --> 00:14:30,320
of stuff and decide what to adopt etc.

290
00:14:27,199 --> 00:14:32,880
But they also had um critical thinking

291
00:14:30,320 --> 00:14:35,440
that was uh being shown. And then the

292
00:14:32,880 --> 00:14:39,040
group that just used their brains uh

293
00:14:35,440 --> 00:14:42,639
basically showed uh the most um sort of

294
00:14:39,040 --> 00:14:45,680
uh um uh activity in uh sort of the

295
00:14:42,639 --> 00:14:48,480
frontal uh cortex. But another

296
00:14:45,680 --> 00:14:50,079
interesting result, we also analyzed the

297
00:14:48,480 --> 00:14:52,079
essays that people wrote, the

298
00:14:50,079 --> 00:14:54,000
programming people did and so on. it.

299
00:14:52,079 --> 00:14:57,120
This is you don't have to try to

300
00:14:54,000 --> 00:15:00,160
interpret this, but we looked at the

301
00:14:57,120 --> 00:15:02,079
topics people wrote about in their

302
00:15:00,160 --> 00:15:04,880
essays, and it turned out that the group

303
00:15:02,079 --> 00:15:08,639
that used CHACH PT, they all came up

304
00:15:04,880 --> 00:15:12,800
with similar essays to the same uh uh

305
00:15:08,639 --> 00:15:16,880
SAT prompt, uh they didn't show much um

306
00:15:12,800 --> 00:15:19,440
sort of uh divergence or um of uh

307
00:15:16,880 --> 00:15:21,279
different points of view, etc., which is

308
00:15:19,440 --> 00:15:24,880
typically what is looked for in these

309
00:15:21,279 --> 00:15:27,600
SAT essays. They were all delivering a

310
00:15:24,880 --> 00:15:30,639
good but the same types of essays about

311
00:15:27,600 --> 00:15:32,880
the same topics. Now, you risk that in

312
00:15:30,639 --> 00:15:36,079
your company as well. If you have all of

313
00:15:32,880 --> 00:15:38,399
your employees using uh these systems to

314
00:15:36,079 --> 00:15:40,480
assist them, yes, they may perform well,

315
00:15:38,399 --> 00:15:43,839
but they're not ex they're not getting

316
00:15:40,480 --> 00:15:46,160
better. They're not exploring new ways

317
00:15:43,839 --> 00:15:48,399
of doing things or more original

318
00:15:46,160 --> 00:15:52,040
innovative methods to approach a

319
00:15:48,399 --> 00:15:55,360
problem. We asked the group that used

320
00:15:52,040 --> 00:15:57,040
ChachiPT to write the same essay. All of

321
00:15:55,360 --> 00:15:59,519
them actually all three groups we asked

322
00:15:57,040 --> 00:16:01,519
to write the same essay they had written

323
00:15:59,519 --> 00:16:04,399
before. They had to write again three

324
00:16:01,519 --> 00:16:07,199
weeks later. Some people in the chatbt

325
00:16:04,399 --> 00:16:09,279
group didn't even recognize the prompt.

326
00:16:07,199 --> 00:16:11,680
They're like, "Oh, really? I I wrote an

327
00:16:09,279 --> 00:16:13,839
essay about that." they couldn't repeat

328
00:16:11,680 --> 00:16:15,839
anything. They hadn't learned anything.

329
00:16:13,839 --> 00:16:18,000
So yes, they delivered a good essay, but

330
00:16:15,839 --> 00:16:21,320
they hadn't grown uh in their skill

331
00:16:18,000 --> 00:16:25,440
level themselves. So keep that in mind.

332
00:16:21,320 --> 00:16:27,759
Um we are building solutions again. Um

333
00:16:25,440 --> 00:16:31,279
like one system that we're building at

334
00:16:27,759 --> 00:16:33,360
the media lab is um a system that helps

335
00:16:31,279 --> 00:16:36,160
you with essay writing but instead of

336
00:16:33,360 --> 00:16:39,040
doing the writing for you it actually

337
00:16:36,160 --> 00:16:41,680
looks at sorry what you have and it

338
00:16:39,040 --> 00:16:44,480
analyzes the argument you're making the

339
00:16:41,680 --> 00:16:46,560
logic of your argument etc. And it

340
00:16:44,480 --> 00:16:48,880
provides feedback pointing out

341
00:16:46,560 --> 00:16:51,120
weaknesses and suggestions like maybe

342
00:16:48,880 --> 00:16:53,600
you can give an example here. Maybe you

343
00:16:51,120 --> 00:16:56,160
should have a couple other reasons why

344
00:16:53,600 --> 00:16:59,279
uh that justify this claim that you're

345
00:16:56,160 --> 00:17:01,680
making or present some data over here

346
00:16:59,279 --> 00:17:04,319
just like a good teacher would, a good

347
00:17:01,680 --> 00:17:07,600
tutor would. And we hope that that will

348
00:17:04,319 --> 00:17:10,959
result in uh systems where ultimately

349
00:17:07,600 --> 00:17:13,600
the student uh benefits uh and and

350
00:17:10,959 --> 00:17:16,640
develops their skills but still can

351
00:17:13,600 --> 00:17:20,240
benefit from the intelligence of AI, the

352
00:17:16,640 --> 00:17:22,079
the fact that it's accessible, etc. and

353
00:17:20,240 --> 00:17:24,439
that maybe not all of them have access

354
00:17:22,079 --> 00:17:26,959
to a real uh a good human

355
00:17:24,439 --> 00:17:30,720
teacher. We're doing things in the area

356
00:17:26,959 --> 00:17:33,039
of memory. Um for example, um this is a

357
00:17:30,720 --> 00:17:35,840
system that many business people tell me

358
00:17:33,039 --> 00:17:39,840
that uh they want. It's sort of like a

359
00:17:35,840 --> 00:17:43,919
personal uh u LLM or a personal system

360
00:17:39,840 --> 00:17:46,760
that um monitors uh your conversations

361
00:17:43,919 --> 00:17:49,360
and we can talk about privacy. It it

362
00:17:46,760 --> 00:17:51,520
recognizes who talks to you if you've

363
00:17:49,360 --> 00:17:54,960
named those people based on their voice

364
00:17:51,520 --> 00:17:58,480
and it also asks uh you to ask for

365
00:17:54,960 --> 00:18:00,880
permission to record uh um conversations

366
00:17:58,480 --> 00:18:02,559
with a new person etc. You can delete

367
00:18:00,880 --> 00:18:05,120
them.

368
00:18:02,559 --> 00:18:07,919
Can you guys hear that? It runs on

369
00:18:05,120 --> 00:18:10,480
unobtrusive maybe not. So let me talk

370
00:18:07,919 --> 00:18:14,880
about it.

371
00:18:10,480 --> 00:18:17,080
So, this system, an audio headset,

372
00:18:14,880 --> 00:18:19,600
continuously records all of your

373
00:18:17,080 --> 00:18:22,400
conversations. So, he can ask later,

374
00:18:19,600 --> 00:18:26,039
what was his name again? Um, because he

375
00:18:22,400 --> 00:18:28,960
can look at the the conversation. A week

376
00:18:26,039 --> 00:18:31,200
later, you need access to information

377
00:18:28,960 --> 00:18:34,480
from that conversation.

378
00:18:31,200 --> 00:18:37,480
And the system constantly predicts what

379
00:18:34,480 --> 00:18:40,640
words from your past experiences and

380
00:18:37,480 --> 00:18:42,880
conversations you need um in a

381
00:18:40,640 --> 00:18:45,919
particular context. But it does that in

382
00:18:42,880 --> 00:18:48,640
a very minimal way where the disruption

383
00:18:45,919 --> 00:18:50,640
is minimal unlike that of smartphones.

384
00:18:48,640 --> 00:18:53,440
You may look up some notes you took in a

385
00:18:50,640 --> 00:18:56,400
smartphone but that takes a lot of input

386
00:18:53,440 --> 00:18:59,440
efforts and effort to interpret uh the

387
00:18:56,400 --> 00:19:02,880
out the output. So we build a system

388
00:18:59,440 --> 00:19:06,000
that is minimally uh disruptive that

389
00:19:02,880 --> 00:19:08,720
just gives you a minimum number of words

390
00:19:06,000 --> 00:19:12,640
that can trigger your memory for some

391
00:19:08,720 --> 00:19:15,679
information that you need. We've also uh

392
00:19:12,640 --> 00:19:18,720
looked a lot recently at the um how we

393
00:19:15,679 --> 00:19:21,280
may want to design human AI interaction

394
00:19:18,720 --> 00:19:23,360
uh in the area of socialization. I

395
00:19:21,280 --> 00:19:25,360
mentioned earlier that I'm concerned

396
00:19:23,360 --> 00:19:27,360
about people getting less good at

397
00:19:25,360 --> 00:19:30,320
talking to other people as they talk

398
00:19:27,360 --> 00:19:34,640
more to AI or just being less interested

399
00:19:30,320 --> 00:19:37,679
in talking to um people and uh I think

400
00:19:34,640 --> 00:19:40,799
these uh worries are justified. Um

401
00:19:37,679 --> 00:19:43,360
there's a growing and widespread use of

402
00:19:40,799 --> 00:19:46,960
uh chat bots for inter for personal and

403
00:19:43,360 --> 00:19:50,240
interpersonal types of uh relationships.

404
00:19:46,960 --> 00:19:54,160
um many people using these uh chat bots

405
00:19:50,240 --> 00:19:57,280
as a psychologist um um asking for

406
00:19:54,160 --> 00:20:00,280
relationship advice etc. There's even a

407
00:19:57,280 --> 00:20:02,640
lot of services like replica and and

408
00:20:00,280 --> 00:20:06,200
character.ai that are explicitly

409
00:20:02,640 --> 00:20:09,520
designed for mimicking human

410
00:20:06,200 --> 00:20:12,720
relationships. Um, and there's sometimes

411
00:20:09,520 --> 00:20:17,440
awful outcomes like this sad suicide of

412
00:20:12,720 --> 00:20:22,160
a teen who um had a an a chat system

413
00:20:17,440 --> 00:20:24,400
that was he believed his uh lover and um

414
00:20:22,160 --> 00:20:27,039
that one of the interactions he

415
00:20:24,400 --> 00:20:31,440
interpreted as the system telling him to

416
00:20:27,039 --> 00:20:36,320
commit suicide and he did a 14year-old

417
00:20:31,440 --> 00:20:40,080
um very um sad. Now luckily there those

418
00:20:36,320 --> 00:20:42,320
are still pretty rare but um just uh in

419
00:20:40,080 --> 00:20:45,520
the last six months we've been doing um

420
00:20:42,320 --> 00:20:47,600
a study funded by open AAI the open AI

421
00:20:45,520 --> 00:20:50,080
safety team actually and in

422
00:20:47,600 --> 00:20:54,080
collaboration with them where we wanted

423
00:20:50,080 --> 00:20:58,000
to understand how common this type of um

424
00:20:54,080 --> 00:21:01,120
close um relationships uh with uh chat

425
00:20:58,000 --> 00:21:04,480
bots is and what the effects are on

426
00:21:01,120 --> 00:21:06,400
people's uh well-being. So we did a

427
00:21:04,480 --> 00:21:09,120
study where we looked at their different

428
00:21:06,400 --> 00:21:13,120
models, the engaging voice one, the

429
00:21:09,120 --> 00:21:16,559
neutral voice, uh the text model. We had

430
00:21:13,120 --> 00:21:19,960
different tasks and we um uh enlisted a

431
00:21:16,559 --> 00:21:24,159
thousand people to talk to

432
00:21:19,960 --> 00:21:26,240
chat for at least five minutes per day.

433
00:21:24,159 --> 00:21:28,799
And some of them had to talk about

434
00:21:26,240 --> 00:21:31,360
personal things, others had to talk

435
00:21:28,799 --> 00:21:32,960
about only practical things. and a third

436
00:21:31,360 --> 00:21:35,039
group could choose what they talked

437
00:21:32,960 --> 00:21:37,679
about. So in total we actually had nine

438
00:21:35,039 --> 00:21:39,760
conditions and we looked at outcomes

439
00:21:37,679 --> 00:21:41,760
like loneliness

440
00:21:39,760 --> 00:21:44,880
um how lonely are people before and

441
00:21:41,760 --> 00:21:48,480
after how much do they socialize with

442
00:21:44,880 --> 00:21:52,080
real people before and after emotional

443
00:21:48,480 --> 00:21:55,919
dependence on AI and problematic use of

444
00:21:52,080 --> 00:21:58,480
the AI. What we learned is that uh the

445
00:21:55,919 --> 00:22:00,960
people people were told you have to do

446
00:21:58,480 --> 00:22:04,480
at least five minutes a day. Many did a

447
00:22:00,960 --> 00:22:08,240
lot more time than that. Extended daily

448
00:22:04,480 --> 00:22:11,840
use was correlated with worse well-being

449
00:22:08,240 --> 00:22:14,159
with worse uh psychosocial outcome. So

450
00:22:11,840 --> 00:22:17,120
you see on the bottom axis the daily

451
00:22:14,159 --> 00:22:19,440
duration how much people spent time with

452
00:22:17,120 --> 00:22:21,120
these chatbots and no matter what

453
00:22:19,440 --> 00:22:23,919
condition they were in this is the

454
00:22:21,120 --> 00:22:27,200
average for all the groups. Um you see

455
00:22:23,919 --> 00:22:29,840
increased loneliness for people that use

456
00:22:27,200 --> 00:22:32,880
AI a lot each day.

457
00:22:29,840 --> 00:22:35,679
um decreased socialization with real

458
00:22:32,880 --> 00:22:40,640
people and more dependence and

459
00:22:35,679 --> 00:22:43,600
problematic uh use as well. Um we also

460
00:22:40,640 --> 00:22:45,919
looked at the uh details the differences

461
00:22:43,600 --> 00:22:49,280
between the different models etc. But

462
00:22:45,919 --> 00:22:52,000
you can read all of that uh online. Um

463
00:22:49,280 --> 00:22:55,039
the solution really and which we have

464
00:22:52,000 --> 00:22:59,600
been working on again with uh OpenAI as

465
00:22:55,039 --> 00:23:02,000
well is for one to um to basically uh

466
00:22:59,600 --> 00:23:04,559
understand how prevalent his behavior is

467
00:23:02,000 --> 00:23:06,960
and trying to track it with automated

468
00:23:04,559 --> 00:23:10,880
classifiers of conversations while

469
00:23:06,960 --> 00:23:15,280
preserving privacy. But then second also

470
00:23:10,880 --> 00:23:18,159
build models that um have pro-social

471
00:23:15,280 --> 00:23:21,760
behaviors that demonstrate pro-social

472
00:23:18,159 --> 00:23:24,880
behaviors and encourage the person to

473
00:23:21,760 --> 00:23:28,640
interact with real people. uh an AI can

474
00:23:24,880 --> 00:23:31,520
be incredibly useful to support people

475
00:23:28,640 --> 00:23:32,960
in um how to understand uh the

476
00:23:31,520 --> 00:23:35,600
difficulties of their human

477
00:23:32,960 --> 00:23:38,880
relationships, encouraging them to

478
00:23:35,600 --> 00:23:41,919
interact more uh with other people and

479
00:23:38,880 --> 00:23:45,679
so on. So we're developing a benchmark

480
00:23:41,919 --> 00:23:48,000
now to um understand for all of the

481
00:23:45,679 --> 00:23:51,360
different models out there how

482
00:23:48,000 --> 00:23:53,440
pro-social they are. um so that that can

483
00:23:51,360 --> 00:23:56,320
be sort of one of the benchmarks that

484
00:23:53,440 --> 00:23:59,440
can be used for AI models. So I'll

485
00:23:56,320 --> 00:24:02,400
summarize by saying yes AI is definitely

486
00:23:59,440 --> 00:24:06,400
here to stay but let's design it to

487
00:24:02,400 --> 00:24:09,840
augment people to preserve human agency,

488
00:24:06,400 --> 00:24:12,240
meaning and well-being and that is um

489
00:24:09,840 --> 00:24:15,640
not a trivial task and is not

490
00:24:12,240 --> 00:24:18,320
necessarily the case for today's um

491
00:24:15,640 --> 00:24:21,919
models. Um, if you're interested in

492
00:24:18,320 --> 00:24:25,200
this, we are starting or have started a

493
00:24:21,919 --> 00:24:28,640
big uh multiffaculty research program at

494
00:24:25,200 --> 00:24:29,799
the media lab, advancing humans with AI

495
00:24:28,640 --> 00:24:32,720
or

496
00:24:29,799 --> 00:24:35,480
AHA. Um, and you can get more

497
00:24:32,720 --> 00:24:38,120
information there.

498
00:24:35,480 --> 00:24:42,240
aha.mmedia.mmit.edu. And we also have a

499
00:24:38,120 --> 00:24:45,600
openly streamed for free uh conference

500
00:24:42,240 --> 00:24:47,760
uh happening next week um on Thursday.

501
00:24:45,600 --> 00:24:50,080
Uh so you can look up the link the

502
00:24:47,760 --> 00:24:52,720
streaming link at that same website and

503
00:24:50,080 --> 00:24:56,720
we have amazing speakers like Jiren

504
00:24:52,720 --> 00:25:00,640
Laneir um uh Ariana Huffington Douglas

505
00:24:56,720 --> 00:25:05,039
Rushkov Tristan Harris experts from uh

506
00:25:00,640 --> 00:25:08,559
Microsoft Open AI um uh academics from

507
00:25:05,039 --> 00:25:12,400
Stanford, Cornell, Harvard and more all

508
00:25:08,559 --> 00:25:15,520
reflecting on can we design AI to

509
00:25:12,400 --> 00:25:18,159
support human flourishing. So please uh

510
00:25:15,520 --> 00:25:22,279
join us for that online if you want to

511
00:25:18,159 --> 00:25:22,279
hear more. Thank you.

512
00:25:24,640 --> 00:25:30,720
Thank you. And uh we probably have do we

513
00:25:28,320 --> 00:25:33,360
have one qu maybe one question if you

514
00:25:30,720 --> 00:25:35,760
can. Yeah. So the question is what are

515
00:25:33,360 --> 00:25:38,240
suggested approaches to overcome the

516
00:25:35,760 --> 00:25:42,480
influences of LLMs on diversity, equity

517
00:25:38,240 --> 00:25:45,440
and inclusion? Yes. A very interesting

518
00:25:42,480 --> 00:25:49,760
uh question. uh there is already a lot

519
00:25:45,440 --> 00:25:53,440
of effort um uh by at least some of the

520
00:25:49,760 --> 00:25:56,039
foundation model makers in uh improving

521
00:25:53,440 --> 00:25:59,039
uh diver diversity, equity and

522
00:25:56,039 --> 00:26:02,400
inclusion. Um one of the solutions I

523
00:25:59,039 --> 00:26:04,720
think is just being open about data sets

524
00:26:02,400 --> 00:26:08,480
for example because ultimately these

525
00:26:04,720 --> 00:26:12,320
models are just um basically statistics

526
00:26:08,480 --> 00:26:15,360
and math. they're not inherently say uh

527
00:26:12,320 --> 00:26:18,159
biased and so on but they reflect the

528
00:26:15,360 --> 00:26:21,679
biases that happen that exist in the

529
00:26:18,159 --> 00:26:24,559
training data. Uh so it's important that

530
00:26:21,679 --> 00:26:27,760
um and and unfortunately a lot of the

531
00:26:24,559 --> 00:26:31,200
makers of these models are not very open

532
00:26:27,760 --> 00:26:33,760
right now about what data they train on

533
00:26:31,200 --> 00:26:36,880
etc. so that we can do audits of all

534
00:26:33,760 --> 00:26:40,120
that data and make sure that the data is

535
00:26:36,880 --> 00:26:46,159
as unbiased as uh

536
00:26:40,120 --> 00:26:48,400
possible. Just one y or okay one more um

537
00:26:46,159 --> 00:26:50,720
what kind of new and bettermental skills

538
00:26:48,400 --> 00:26:53,679
do you expect people to develop after

539
00:26:50,720 --> 00:26:56,080
using genai tools for a while? Yeah, so

540
00:26:53,679 --> 00:26:58,720
there's very interesting work that just

541
00:26:56,080 --> 00:27:01,760
came out uh this week actually from

542
00:26:58,720 --> 00:27:05,360
Dartmouth. uh they have been doing work

543
00:27:01,760 --> 00:27:09,360
there uh for the last four or five years

544
00:27:05,360 --> 00:27:12,400
on developing AI based uh chat bots for

545
00:27:09,360 --> 00:27:14,799
mental health and um you may have seen

546
00:27:12,400 --> 00:27:16,960
it in the news it's in MIT technology

547
00:27:14,799 --> 00:27:20,960
review last week and so on or this week

548
00:27:16,960 --> 00:27:24,159
even um showing that uh they have uh

549
00:27:20,960 --> 00:27:27,279
with a special purpose model that they

550
00:27:24,159 --> 00:27:30,320
created it uses um something I think

551
00:27:27,279 --> 00:27:33,200
similar to a rag uh approach where they

552
00:27:30,320 --> 00:27:36,960
give it a lot of special training data

553
00:27:33,200 --> 00:27:40,480
that they had uh carefully curated and

554
00:27:36,960 --> 00:27:44,240
it u reached the same level of uh

555
00:27:40,480 --> 00:27:49,039
effectiveness as a human therapist in uh

556
00:27:44,240 --> 00:27:52,320
reducing uh depression uh anxiety uh and

557
00:27:49,039 --> 00:27:55,760
other uh conditions. So I think it's uh

558
00:27:52,320 --> 00:27:59,039
there's a huge potential for AI to help

559
00:27:55,760 --> 00:28:02,399
with um mental health. Uh but their

560
00:27:59,039 --> 00:28:05,440
studies and their repeated efforts um at

561
00:28:02,399 --> 00:28:08,320
getting this right show that it is again

562
00:28:05,440 --> 00:28:10,240
a tricky problem not one where we just

563
00:28:08,320 --> 00:28:13,279
build something and don't do much

564
00:28:10,240 --> 00:28:15,360
testing and release it uh but uh one

565
00:28:13,279 --> 00:28:18,000
that has to be carefully curated and

566
00:28:15,360 --> 00:28:20,640
where the data has to be carefully uh

567
00:28:18,000 --> 00:28:24,279
curated and so on. Yeah. Thank you.

568
00:28:20,640 --> 00:28:24,279
Thank you very much.

