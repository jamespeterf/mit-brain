1
00:00:09,760 --> 00:00:14,559
So I'm going to invite my friend and

2
00:00:11,200 --> 00:00:15,920
colleague Michael Shrag. Um Michael is a

3
00:00:14,559 --> 00:00:18,000
lecture at the MIT Sloan School of

4
00:00:15,920 --> 00:00:20,640
Management. He's a fellow of the

5
00:00:18,000 --> 00:00:25,359
initially on digital economy. uh he's a

6
00:00:20,640 --> 00:00:28,160
author um strategic advisor um former

7
00:00:25,359 --> 00:00:30,240
journalist um has done work on

8
00:00:28,160 --> 00:00:32,320
prototyping on the impact of AI and

9
00:00:30,240 --> 00:00:33,040
thinking on all sorts of things

10
00:00:32,320 --> 00:00:34,800
>> less is more

11
00:00:33,040 --> 00:00:36,079
>> less is more so with that I'll hand over

12
00:00:34,800 --> 00:00:39,640
to Michael

13
00:00:36,079 --> 00:00:39,640
>> thank you very much

14
00:00:40,399 --> 00:00:46,480
>> thank you let's go back here and there's

15
00:00:43,520 --> 00:00:49,280
the

16
00:00:46,480 --> 00:00:50,960
actually sorry this the slide contract

17
00:00:49,280 --> 00:00:53,600
racting here. It's always wonderful to

18
00:00:50,960 --> 00:00:58,000
be in the the muchcoveted after lunch

19
00:00:53,600 --> 00:01:00,719
speaking position here. Um I'm going to

20
00:00:58,000 --> 00:01:02,399
refer back to Phil's comment about

21
00:01:00,719 --> 00:01:04,640
learning from a fire hose, teaching from

22
00:01:02,399 --> 00:01:07,040
a fire hose. This is going to be a

23
00:01:04,640 --> 00:01:09,760
forced march, quick march, and I want

24
00:01:07,040 --> 00:01:10,960
you to come away with a sense as what

25
00:01:09,760 --> 00:01:13,200
Stephen was saying in terms of the

26
00:01:10,960 --> 00:01:16,080
potential and possibilities particularly

27
00:01:13,200 --> 00:01:17,680
around generative AI. I I realized as

28
00:01:16,080 --> 00:01:19,840
Stephen was referring to the past that

29
00:01:17,680 --> 00:01:22,159
actually I do have a direct British

30
00:01:19,840 --> 00:01:24,320
connection, English connection because I

31
00:01:22,159 --> 00:01:26,400
had the good fortune literally and

32
00:01:24,320 --> 00:01:28,720
figuratively to study under Donald

33
00:01:26,400 --> 00:01:32,479
Mickey at Illinois and Mickey of course

34
00:01:28,720 --> 00:01:34,320
was a a protege of Alan Turing and uh I

35
00:01:32,479 --> 00:01:36,159
was the only uh undergrad in the class.

36
00:01:34,320 --> 00:01:38,720
I learned a lot from that. That was one

37
00:01:36,159 --> 00:01:41,040
of my best classes and clearly because

38
00:01:38,720 --> 00:01:43,360
I'm here it clearly had an impact on my

39
00:01:41,040 --> 00:01:46,560
on my professional life and perspective.

40
00:01:43,360 --> 00:01:48,960
So this talk is thinking about thinking,

41
00:01:46,560 --> 00:01:51,680
forgive the pun, gaining greater insight

42
00:01:48,960 --> 00:01:53,360
into greater insight. And this is my

43
00:01:51,680 --> 00:01:55,200
most important slide. The content of the

44
00:01:53,360 --> 00:01:56,479
audience is more important than the

45
00:01:55,200 --> 00:01:57,920
content of the talk because I have a

46
00:01:56,479 --> 00:01:59,600
pretty good grasp of what I'm going to

47
00:01:57,920 --> 00:02:01,680
be saying. I'm going to be very very

48
00:01:59,600 --> 00:02:03,200
interested in the panel that that Steve

49
00:02:01,680 --> 00:02:04,560
is going to moderate with myself.

50
00:02:03,200 --> 00:02:06,399
Originally, you see in the program that

51
00:02:04,560 --> 00:02:07,920
I was going to moderate the panel, but

52
00:02:06,399 --> 00:02:11,200
it would be rather foolish to have me

53
00:02:07,920 --> 00:02:13,200
moderate a panel after my own talk. Um,

54
00:02:11,200 --> 00:02:15,760
so I'm sparing you. You're you're being

55
00:02:13,200 --> 00:02:18,640
spared of that. Let's get it out of the

56
00:02:15,760 --> 00:02:22,000
way. It is a revolution. Okay. People

57
00:02:18,640 --> 00:02:25,760
talk about AI bubble, but this is not

58
00:02:22,000 --> 00:02:30,239
hype. It is not hype. It is it is no

59
00:02:25,760 --> 00:02:33,200
more hype than the the dependencies I

60
00:02:30,239 --> 00:02:35,200
see adults, adolescence, and childrens

61
00:02:33,200 --> 00:02:37,440
have on the tube and in the subway with

62
00:02:35,200 --> 00:02:40,000
their mobile devices. Whenever I see a

63
00:02:37,440 --> 00:02:42,239
paperback book, I'm sort of taken aback

64
00:02:40,000 --> 00:02:45,840
by the inacronistic pleasure of it all

65
00:02:42,239 --> 00:02:50,000
on this. So this is a revolution. It is

66
00:02:45,840 --> 00:02:52,160
going to transform how people think, how

67
00:02:50,000 --> 00:02:54,720
people learn, how people behave, how

68
00:02:52,160 --> 00:02:58,720
people create value, how people consume

69
00:02:54,720 --> 00:03:00,400
value. Um, and that is not hyperbole. In

70
00:02:58,720 --> 00:03:02,239
fact, I think I'm being conservative in

71
00:03:00,400 --> 00:03:05,680
that regard because what was the phrase

72
00:03:02,239 --> 00:03:07,519
we heard uh this morning? Early days.

73
00:03:05,680 --> 00:03:10,239
its early days. I wouldn't say that

74
00:03:07,519 --> 00:03:12,959
we're necessarily on a Moore's law

75
00:03:10,239 --> 00:03:14,560
exponential growth, but the growth is

76
00:03:12,959 --> 00:03:17,599
happening

77
00:03:14,560 --> 00:03:20,879
fast enough surprisingly enough that

78
00:03:17,599 --> 00:03:23,760
that I think we the last conference was

79
00:03:20,879 --> 00:03:25,920
two years ago, two to three years hence

80
00:03:23,760 --> 00:03:29,200
we're going to be talking at least at

81
00:03:25,920 --> 00:03:31,280
least a 10x order of magnitude

82
00:03:29,200 --> 00:03:35,280
improvement. I wouldn't be shocked if it

83
00:03:31,280 --> 00:03:37,360
was a more than that. Um, again,

84
00:03:35,280 --> 00:03:39,680
customizing for the locals. You all know

85
00:03:37,360 --> 00:03:41,360
this fellow who managed to develop a

86
00:03:39,680 --> 00:03:44,560
pretty good collaboration with the late

87
00:03:41,360 --> 00:03:47,599
Steve Jobs. Um, now he has a pretty good

88
00:03:44,560 --> 00:03:49,360
collaboration with the living Sam Alman.

89
00:03:47,599 --> 00:03:52,239
They did their developer day earlier

90
00:03:49,360 --> 00:03:54,799
this week. I decided to lift this quote.

91
00:03:52,239 --> 00:03:57,280
I've never in my career come across

92
00:03:54,799 --> 00:04:00,319
anything vaguely like the affordance,

93
00:03:57,280 --> 00:04:02,319
like the capability that we're now

94
00:04:00,319 --> 00:04:05,280
starting to see.

95
00:04:02,319 --> 00:04:08,400
Um, I think Johnny IV should be taken

96
00:04:05,280 --> 00:04:10,400
seriously in this regard. Not just

97
00:04:08,400 --> 00:04:12,640
because of who he's gotten to work with,

98
00:04:10,400 --> 00:04:15,519
but because he's a designer. He has

99
00:04:12,640 --> 00:04:17,680
focused on UX. He has created co-created

100
00:04:15,519 --> 00:04:21,040
some of the most successful products and

101
00:04:17,680 --> 00:04:23,520
services in the world. So, this was my

102
00:04:21,040 --> 00:04:26,720
frame. Why am I using this particular

103
00:04:23,520 --> 00:04:31,280
photo? Because it's good. And because I

104
00:04:26,720 --> 00:04:33,919
was for ILP in Mongolia last month and

105
00:04:31,280 --> 00:04:35,680
we were talking with a mining company on

106
00:04:33,919 --> 00:04:36,960
this and they were curious about LLMs,

107
00:04:35,680 --> 00:04:39,280
of course, one of the challenges for

108
00:04:36,960 --> 00:04:41,440
them is gee, do we go with anthropic? Do

109
00:04:39,280 --> 00:04:43,360
we go with open AI? Do we go with

110
00:04:41,440 --> 00:04:45,360
deepseek? What should we do in this

111
00:04:43,360 --> 00:04:47,520
regard? But the capabilities, the

112
00:04:45,360 --> 00:04:49,199
capacity is still there. And I kind of

113
00:04:47,520 --> 00:04:52,639
like this metaphor. I kind of like this

114
00:04:49,199 --> 00:04:55,759
analogy because why? They're taking aim.

115
00:04:52,639 --> 00:04:59,759
It's a human trying to be on target and

116
00:04:55,759 --> 00:05:01,600
they're riding a horse. Okay. And what's

117
00:04:59,759 --> 00:05:05,040
the this is ultimately you can't take

118
00:05:01,600 --> 00:05:06,960
the shot if the horse misbehaves. Okay.

119
00:05:05,040 --> 00:05:09,280
What's the relationship with the models

120
00:05:06,960 --> 00:05:11,199
that you want? This is the essence and

121
00:05:09,280 --> 00:05:13,520
the nature of the research that I'm

122
00:05:11,199 --> 00:05:15,759
working collaborating on and overseeing

123
00:05:13,520 --> 00:05:17,039
at MIT. What's the nature of the

124
00:05:15,759 --> 00:05:20,400
relationship with the models that you

125
00:05:17,039 --> 00:05:25,199
want? As these models learn, as these

126
00:05:20,400 --> 00:05:26,639
models learn to learn to get better by a

127
00:05:25,199 --> 00:05:31,120
a variety of different features and

128
00:05:26,639 --> 00:05:32,960
functions and from you and with you. Um,

129
00:05:31,120 --> 00:05:34,080
I happen to know Warren Buffett. He was

130
00:05:32,960 --> 00:05:35,039
on the board of the Washington Post

131
00:05:34,080 --> 00:05:37,120
company. I was a reporter of the

132
00:05:35,039 --> 00:05:38,960
Washington Post. It was my quote again.

133
00:05:37,120 --> 00:05:41,440
Good fortune to know Mr. Buffett. And

134
00:05:38,960 --> 00:05:43,039
he's reading his shareholder letter. Uh,

135
00:05:41,440 --> 00:05:46,000
by the way, you want to do have great

136
00:05:43,039 --> 00:05:48,320
fun, dump in the Berkshire Hathaway

137
00:05:46,000 --> 00:05:50,560
shareholders letters and to chat GPT or

138
00:05:48,320 --> 00:05:53,520
Anthropic and interrogate about how you

139
00:05:50,560 --> 00:05:55,280
should be doing your investments. You I

140
00:05:53,520 --> 00:05:57,360
guarantee you it'll be a productive

141
00:05:55,280 --> 00:05:59,199
experience for you. So, he was once

142
00:05:57,360 --> 00:06:00,880
asked of the billionaires I've known,

143
00:05:59,199 --> 00:06:02,560
money just brings out the basic traits

144
00:06:00,880 --> 00:06:04,240
in them. If they were jerks before they

145
00:06:02,560 --> 00:06:08,240
had money, they're simply bigger jerks

146
00:06:04,240 --> 00:06:11,360
with a billion dollars. Okay? So with

147
00:06:08,240 --> 00:06:14,240
tongue not in cheek at all I said well

148
00:06:11,360 --> 00:06:15,759
how do the wealth of LLMs instead of

149
00:06:14,240 --> 00:06:18,759
billions of dollars billions of

150
00:06:15,759 --> 00:06:18,759
parameters

151
00:06:18,880 --> 00:06:23,919
so this is LLM Warren Buffett of the

152
00:06:22,319 --> 00:06:25,840
professional I've known eight of their

153
00:06:23,919 --> 00:06:28,000
basic cognitive traits if they were

154
00:06:25,840 --> 00:06:30,240
intellectually lazy before LLMs they're

155
00:06:28,000 --> 00:06:32,560
simply intellectually lazier with better

156
00:06:30,240 --> 00:06:36,319
sounding slops

157
00:06:32,560 --> 00:06:37,360
that's not a joke that's a fact that's a

158
00:06:36,319 --> 00:06:39,280
warning

159
00:06:37,360 --> 00:06:41,280
That's arguably one of the most

160
00:06:39,280 --> 00:06:44,080
important takeaways I'd like you to

161
00:06:41,280 --> 00:06:46,400
consider from this. The unavoidable

162
00:06:44,080 --> 00:06:49,120
challenge. The unavoidable challenge

163
00:06:46,400 --> 00:06:52,479
that I mean just let me make a global

164
00:06:49,120 --> 00:06:55,039
statement right now. There is not a high

165
00:06:52,479 --> 00:06:57,440
school, grammar school, university

166
00:06:55,039 --> 00:07:00,000
student who I have come across in the

167
00:06:57,440 --> 00:07:03,360
last six months, particularly elite ones

168
00:07:00,000 --> 00:07:06,319
who do not cheat with LLMs.

169
00:07:03,360 --> 00:07:08,400
Okay. It's like mobile phones. This is

170
00:07:06,319 --> 00:07:10,240
part of the way they see themselves and

171
00:07:08,400 --> 00:07:12,160
the way they learn. And what's the

172
00:07:10,240 --> 00:07:15,280
choice they're making? Are we using

173
00:07:12,160 --> 00:07:17,599
these LLMs to be mindful, to think

174
00:07:15,280 --> 00:07:19,680
better, or to be mindless? I don't want

175
00:07:17,599 --> 00:07:21,759
to think about that

176
00:07:19,680 --> 00:07:25,680
jerk. Intellectually lazy,

177
00:07:21,759 --> 00:07:29,039
intellectually sharp. This is a real

178
00:07:25,680 --> 00:07:32,160
nonfalse dichotomy. And I strongly urge

179
00:07:29,039 --> 00:07:33,680
you as you begin to explore or continue

180
00:07:32,160 --> 00:07:36,000
to explore and utilize these

181
00:07:33,680 --> 00:07:38,560
technologies, you become more self-aware

182
00:07:36,000 --> 00:07:42,720
and self-conscious about where do you

183
00:07:38,560 --> 00:07:44,720
consistently default to mindlessness or

184
00:07:42,720 --> 00:07:47,840
whether you aspire to getting greater

185
00:07:44,720 --> 00:07:50,560
value from being mindful in this regard.

186
00:07:47,840 --> 00:07:52,000
Okay. So that's the level set. every

187
00:07:50,560 --> 00:07:55,199
organization that I work with in

188
00:07:52,000 --> 00:07:56,960
executive ed or in advisory capacities.

189
00:07:55,199 --> 00:07:59,840
How do we get better greater greater

190
00:07:56,960 --> 00:08:01,840
value from our data, our metadata? How

191
00:07:59,840 --> 00:08:03,360
can we make data learning easier for our

192
00:08:01,840 --> 00:08:06,560
people? This is everyone's challenge

193
00:08:03,360 --> 00:08:09,280
everywhere. And this piece just came out

194
00:08:06,560 --> 00:08:11,280
in July. You may have seen the Carpathy

195
00:08:09,280 --> 00:08:14,080
thing. We'll discuss that in a moment.

196
00:08:11,280 --> 00:08:16,800
I've been looking at the notion of Vibe

197
00:08:14,080 --> 00:08:19,919
analytics. You heard about Vibe coding.

198
00:08:16,800 --> 00:08:24,560
I'm looking at Vibe analytics. How can

199
00:08:19,919 --> 00:08:28,400
we use LLMs, SLMs to create a different

200
00:08:24,560 --> 00:08:30,960
relationship with the patterns of data

201
00:08:28,400 --> 00:08:32,800
that we intentionally or accidentally

202
00:08:30,960 --> 00:08:34,399
collect? And clearly for organizations

203
00:08:32,800 --> 00:08:37,440
that are saying, gee, how do we get

204
00:08:34,399 --> 00:08:40,159
greater value from our data using AI?

205
00:08:37,440 --> 00:08:42,800
This struck me as a meaningful and

206
00:08:40,159 --> 00:08:45,680
measurable shortcut to both efficiency

207
00:08:42,800 --> 00:08:49,440
and effectiveness so long as we're aware

208
00:08:45,680 --> 00:08:51,200
of the mindful mindless trade-off. Okay,

209
00:08:49,440 --> 00:08:52,959
what's my definition? I believe in

210
00:08:51,200 --> 00:08:55,279
defining the terms just as we defined

211
00:08:52,959 --> 00:08:57,600
innovation. Vibe analytics is how your

212
00:08:55,279 --> 00:09:00,160
business instincts argue with your data

213
00:08:57,600 --> 00:09:02,480
patterns. Argue with your data patterns

214
00:09:00,160 --> 00:09:05,279
and both get sharper at what they see

215
00:09:02,480 --> 00:09:08,000
and how they see together. So it's

216
00:09:05,279 --> 00:09:09,839
individual and collaborative. We talked

217
00:09:08,000 --> 00:09:12,640
about the importance of collaboration

218
00:09:09,839 --> 00:09:14,959
with various partners. Screw the human

219
00:09:12,640 --> 00:09:17,519
beings. You got to collaborate with the

220
00:09:14,959 --> 00:09:19,120
machines. you you need to become a

221
00:09:17,519 --> 00:09:21,519
better one of the most interesting

222
00:09:19,120 --> 00:09:23,600
things I think going forward is do you

223
00:09:21,519 --> 00:09:25,920
get more value

224
00:09:23,600 --> 00:09:28,959
this is a testable hypothesis do you get

225
00:09:25,920 --> 00:09:32,399
more value collaborating with your human

226
00:09:28,959 --> 00:09:35,519
colleagues or with their LLM

227
00:09:32,399 --> 00:09:37,839
instantiations and mediations that is an

228
00:09:35,519 --> 00:09:41,120
open question I say without hesitation

229
00:09:37,839 --> 00:09:43,519
that I do maybe it's because I'm my

230
00:09:41,120 --> 00:09:45,440
neurode divergent tend tendencies but I

231
00:09:43,519 --> 00:09:47,920
have a very good time collaborating with

232
00:09:45,440 --> 00:09:47,920
the LLMs

233
00:09:48,320 --> 00:09:52,399
you can adjust your interpretation of my

234
00:09:50,320 --> 00:09:55,839
talk accordingly. So what's the origin

235
00:09:52,399 --> 00:09:57,600
of this? So these things came out MIT as

236
00:09:55,839 --> 00:10:01,040
a men's at manus kind of place

237
00:09:57,600 --> 00:10:03,360
hackathons I basically stole h the

238
00:10:01,040 --> 00:10:05,600
hackathon concept did prompts that's

239
00:10:03,360 --> 00:10:07,920
what we did for exec and various clients

240
00:10:05,600 --> 00:10:10,399
how do you upskill and reskill people

241
00:10:07,920 --> 00:10:13,279
give them use cases give them the data

242
00:10:10,399 --> 00:10:15,519
that supports those use cases what are

243
00:10:13,279 --> 00:10:17,200
and set up different teams so we make it

244
00:10:15,519 --> 00:10:19,760
social and so we create some sort of

245
00:10:17,200 --> 00:10:21,519
excuse for individuals to collaborate

246
00:10:19,760 --> 00:10:24,640
not just individuals and the machines to

247
00:10:21,519 --> 00:10:27,120
collaborate and try effectively analyze

248
00:10:24,640 --> 00:10:28,720
those use cases or produce meaningful

249
00:10:27,120 --> 00:10:31,600
outcomes and outputs for those use

250
00:10:28,720 --> 00:10:33,120
cases. Promptathons fantastic thing. I

251
00:10:31,600 --> 00:10:35,920
urge you to consider them in your own

252
00:10:33,120 --> 00:10:37,839
own organizations. KPIs is the more

253
00:10:35,920 --> 00:10:41,040
serious of the research. Everybody here

254
00:10:37,839 --> 00:10:44,160
knows what KPIs are. Okay? And people

255
00:10:41,040 --> 00:10:47,839
have thought about oh how do we use LLMs

256
00:10:44,160 --> 00:10:50,880
to improve performance to improve KPIs?

257
00:10:47,839 --> 00:10:53,600
Screw that. Why don't you make the KPIs

258
00:10:50,880 --> 00:10:56,399
intelligent? Why don't you make KPIs

259
00:10:53,600 --> 00:10:58,959
intelligent software agents that learn

260
00:10:56,399 --> 00:11:01,680
and learn how to learn? So your CLV,

261
00:10:58,959 --> 00:11:04,240
customer lifetime value KPI or your

262
00:11:01,680 --> 00:11:05,600
churn KPI can learn to improve. Gee,

263
00:11:04,240 --> 00:11:07,440
what data does it need to learn to

264
00:11:05,600 --> 00:11:09,279
improve? You can incorporate techniques

265
00:11:07,440 --> 00:11:11,920
like chappley values and herwitz

266
00:11:09,279 --> 00:11:14,160
mechanisms for alignment etc etc. It's a

267
00:11:11,920 --> 00:11:17,120
great playground for rethinking the

268
00:11:14,160 --> 00:11:19,839
economics of performance and of course

269
00:11:17,120 --> 00:11:21,600
vibe coding. There's Mr. Carpathy and I

270
00:11:19,839 --> 00:11:23,920
basically said because you know all of

271
00:11:21,600 --> 00:11:26,160
my thinking is derivative at this stage

272
00:11:23,920 --> 00:11:28,320
um you know vibe coding how can it apply

273
00:11:26,160 --> 00:11:30,880
to vibe analytics

274
00:11:28,320 --> 00:11:33,279
and this is the story your data has

275
00:11:30,880 --> 00:11:34,800
insights people can't see you can see

276
00:11:33,279 --> 00:11:37,040
I'm stealing this from my I'm

277
00:11:34,800 --> 00:11:38,560
plagiarizing myself from my exec class

278
00:11:37,040 --> 00:11:40,480
and your people have intuitions your

279
00:11:38,560 --> 00:11:43,760
data can't capture when you

280
00:11:40,480 --> 00:11:46,560
intentionally llm make them argue with

281
00:11:43,760 --> 00:11:48,640
each other you know the only aspect of

282
00:11:46,560 --> 00:11:51,040
marks I take seriously is the Hegelian

283
00:11:48,640 --> 00:11:54,480
dialectic. Use LLMs to create

284
00:11:51,040 --> 00:11:57,360
dialectics. Here they both get smarter.

285
00:11:54,480 --> 00:11:59,120
We move analytics from how does our data

286
00:11:57,360 --> 00:12:01,120
confirm what we already believe or test

287
00:11:59,120 --> 00:12:03,680
what we already believe? Apologies to

288
00:12:01,120 --> 00:12:06,079
Mosellar and Tuki in classic exploratory

289
00:12:03,680 --> 00:12:09,519
data analysis to what can our data and

290
00:12:06,079 --> 00:12:12,800
intuition discover together that

291
00:12:09,519 --> 00:12:15,600
surprises us both.

292
00:12:12,800 --> 00:12:19,839
Respectfully, that's the antithesis of

293
00:12:15,600 --> 00:12:22,800
mindless. Okay. So the virtuous cycle we

294
00:12:19,839 --> 00:12:25,519
seek to en encourage I seek to encourage

295
00:12:22,800 --> 00:12:29,279
with my classes and my clients learning

296
00:12:25,519 --> 00:12:30,560
to vibe analyze vibe analyzing to learn

297
00:12:29,279 --> 00:12:32,160
that's going to be one of the most

298
00:12:30,560 --> 00:12:34,800
important things going forward in

299
00:12:32,160 --> 00:12:37,760
getting value from LLMs how do we work

300
00:12:34,800 --> 00:12:40,079
with them individually collectively

301
00:12:37,760 --> 00:12:42,720
to create a virtuous cycle network

302
00:12:40,079 --> 00:12:47,440
effects in that regard because we all

303
00:12:42,720 --> 00:12:51,360
want to improve our RO ai sorry I like

304
00:12:47,440 --> 00:12:52,880
acronymic puns. Return on artificial

305
00:12:51,360 --> 00:12:56,880
intelligence.

306
00:12:52,880 --> 00:12:59,920
Return on actionable insight. That's the

307
00:12:56,880 --> 00:13:02,320
virtuous cycle we seek to do. The point

308
00:12:59,920 --> 00:13:04,160
of this these brief remarks before we go

309
00:13:02,320 --> 00:13:06,560
to the panel. Empower you to

310
00:13:04,160 --> 00:13:10,639
intelligently invest in actionable

311
00:13:06,560 --> 00:13:13,120
insights, not answers. Okay, that's the

312
00:13:10,639 --> 00:13:16,079
big mistake. How do we use LLM to get

313
00:13:13,120 --> 00:13:19,519
the right answer? Get the better answer.

314
00:13:16,079 --> 00:13:22,000
screw answers. Use them for insight.

315
00:13:19,519 --> 00:13:24,399
That's what they're best for. Pattern

316
00:13:22,000 --> 00:13:26,800
matching. Now, what I'm saying now is

317
00:13:24,399 --> 00:13:29,440
probably going to be untrue in 24 to 36

318
00:13:26,800 --> 00:13:33,519
months. But for now, if you're spending

319
00:13:29,440 --> 00:13:38,399
time with LLMs, focus on AI, actionable

320
00:13:33,519 --> 00:13:40,639
insights. Uh um so thinking about

321
00:13:38,399 --> 00:13:43,279
thinking means generative AI for

322
00:13:40,639 --> 00:13:46,639
generating actionable insights and

323
00:13:43,279 --> 00:13:48,880
return on actionable insights transforms

324
00:13:46,639 --> 00:13:51,440
return on artificial intelligence and

325
00:13:48,880 --> 00:13:53,040
vice versa. That's the value prop.

326
00:13:51,440 --> 00:13:55,519
That's what you should be exploring in

327
00:13:53,040 --> 00:13:58,639
your own organizations. Again, defining

328
00:13:55,519 --> 00:14:01,040
with terms, an insight is an event in

329
00:13:58,639 --> 00:14:03,519
your mind. An actionable insight is a

330
00:14:01,040 --> 00:14:04,720
capability in your hands. What's been

331
00:14:03,519 --> 00:14:09,040
one of the themes you've been hearing

332
00:14:04,720 --> 00:14:11,199
all bloody day? Men's atmos minds and

333
00:14:09,040 --> 00:14:13,440
hands. That's one of the reasons why I'm

334
00:14:11,199 --> 00:14:15,360
picking this. It's not enough to think

335
00:14:13,440 --> 00:14:18,160
better. You have to be able to do

336
00:14:15,360 --> 00:14:20,560
something with that insight.

337
00:14:18,160 --> 00:14:22,959
How does the shift sh change what we

338
00:14:20,560 --> 00:14:24,720
what we think is worth noticing? How

339
00:14:22,959 --> 00:14:27,440
does it expose a tension between what

340
00:14:24,720 --> 00:14:29,839
we're measuring KPI and what we

341
00:14:27,440 --> 00:14:32,880
recognize what matters? How does it

342
00:14:29,839 --> 00:14:34,639
change the ne cascade iteration? How

343
00:14:32,880 --> 00:14:36,320
does it change the next best question or

344
00:14:34,639 --> 00:14:38,560
revise the reason we were asking in the

345
00:14:36,320 --> 00:14:40,800
first? Oh, that's why we should be

346
00:14:38,560 --> 00:14:43,120
interested in this sort of thing.

347
00:14:40,800 --> 00:14:46,720
Insights become actionable when you can

348
00:14:43,120 --> 00:14:49,279
actually do something with them. And

349
00:14:46,720 --> 00:14:52,240
that these are the key analytic princip

350
00:14:49,279 --> 00:14:54,720
principles I observe really encourage

351
00:14:52,240 --> 00:14:58,720
you to take very seriously.

352
00:14:54,720 --> 00:15:02,000
Shift from datadriven to insight.

353
00:14:58,720 --> 00:15:04,639
Use LLMs to question the assumptions

354
00:15:02,000 --> 00:15:09,760
underlying the question and make your

355
00:15:04,639 --> 00:15:13,040
data talk back, argue back, fight back,

356
00:15:09,760 --> 00:15:15,680
be insightful as well as insightful in

357
00:15:13,040 --> 00:15:18,560
your engagement there.

358
00:15:15,680 --> 00:15:22,240
Okay. Theory reality I'm telling you

359
00:15:18,560 --> 00:15:24,880
about I I do research in KPA KPIs KP

360
00:15:22,240 --> 00:15:27,680
AIS.

361
00:15:24,880 --> 00:15:30,079
This was a two years ago. I was thinking

362
00:15:27,680 --> 00:15:32,320
what's a good metaphor? What's a good

363
00:15:30,079 --> 00:15:36,399
analogy for my exec classes on the

364
00:15:32,320 --> 00:15:38,480
future of KPIs? What what can I gee

365
00:15:36,399 --> 00:15:41,199
engage around and iterate around? What

366
00:15:38,480 --> 00:15:44,800
concept can generate actionable insights

367
00:15:41,199 --> 00:15:48,079
for me? So this was mine. Okay. Gee, I

368
00:15:44,800 --> 00:15:52,240
think we should think of KPIs as GPS to

369
00:15:48,079 --> 00:15:55,360
guide enterprise decision. Okay.

370
00:15:52,240 --> 00:15:58,959
You know, this was my my core insight.

371
00:15:55,360 --> 00:16:00,399
So I began asking LLMs about this. So

372
00:15:58,959 --> 00:16:02,240
basically I said I can show you the

373
00:16:00,399 --> 00:16:03,839
original prompt but we're taking within

374
00:16:02,240 --> 00:16:05,920
times.

375
00:16:03,839 --> 00:16:08,800
Tell me how good of analogy how good of

376
00:16:05,920 --> 00:16:10,240
a metaphor this is. This is the answer.

377
00:16:08,800 --> 00:16:12,000
I'm not going to read it all to you but

378
00:16:10,240 --> 00:16:15,040
let me tell you what what unhinged my

379
00:16:12,000 --> 00:16:18,560
jaw. Their ultimate purpose is improving

380
00:16:15,040 --> 00:16:20,720
outcomes through dynamic optimization.

381
00:16:18,560 --> 00:16:22,480
It would take o over three months for

382
00:16:20,720 --> 00:16:24,639
most grad students to come to that kind

383
00:16:22,480 --> 00:16:28,560
of conclusion if I asked them to do

384
00:16:24,639 --> 00:16:30,079
reporting on it. Okay. So I'm getting an

385
00:16:28,560 --> 00:16:33,120
infrastructure. I'm getting something

386
00:16:30,079 --> 00:16:36,000
that's yes isorphic in terms of

387
00:16:33,120 --> 00:16:39,360
actionable insights in terms of gee how

388
00:16:36,000 --> 00:16:42,800
can I draw actionable inspiration from

389
00:16:39,360 --> 00:16:44,160
this GPS metaphor and analogy. And so I

390
00:16:42,800 --> 00:16:47,040
began to play with these sorts of things

391
00:16:44,160 --> 00:16:48,959
and I did this. I mentioned Malaysia.

392
00:16:47,040 --> 00:16:52,079
Let's go to Indonesia. This was with

393
00:16:48,959 --> 00:16:53,839
Claude and I'm giving a talk obviously

394
00:16:52,079 --> 00:16:56,320
running a workshop with some Indian

395
00:16:53,839 --> 00:16:59,759
financial institutions

396
00:16:56,320 --> 00:17:02,800
KPI CLV KPI you know gamma gamma

397
00:16:59,759 --> 00:17:04,559
probabilistic non-binomial distribution

398
00:17:02,800 --> 00:17:06,799
uh um I don't have to read this all to

399
00:17:04,559 --> 00:17:08,319
you but look at look at the bottom there

400
00:17:06,799 --> 00:17:09,760
please give your response in a way that

401
00:17:08,319 --> 00:17:11,760
would impress an audience of

402
00:17:09,760 --> 00:17:13,520
sophisticated Indonesia Indonesian

403
00:17:11,760 --> 00:17:15,120
business people and technologists

404
00:17:13,520 --> 00:17:16,400
because you know the content of the

405
00:17:15,120 --> 00:17:18,959
audience is more important than the

406
00:17:16,400 --> 00:17:21,280
content of the talk. Okay. So, I care

407
00:17:18,959 --> 00:17:22,720
about my audience a lot. In fact, I

408
00:17:21,280 --> 00:17:27,199
cared about it so much that this was the

409
00:17:22,720 --> 00:17:30,559
answer from Claude. It's in Bahas.

410
00:17:27,199 --> 00:17:32,080
It's in Indonesia. I said I said when I

411
00:17:30,559 --> 00:17:34,320
first saw this, my immediate reaction

412
00:17:32,080 --> 00:17:35,919
was, "Oh my gosh." I I asked at

413
00:17:34,320 --> 00:17:38,559
something that made the model break

414
00:17:35,919 --> 00:17:41,600
down. Then I said, "Oh my gosh, this is

415
00:17:38,559 --> 00:17:43,120
this is actually Indonesian." So now I'm

416
00:17:41,600 --> 00:17:44,799
in the embarrassing position of having

417
00:17:43,120 --> 00:17:46,960
to ask the model, okay, you gave me the

418
00:17:44,799 --> 00:17:50,000
answer for sophisticated, but I don't

419
00:17:46,960 --> 00:17:52,000
understand Indonesian and Bahasa. So put

420
00:17:50,000 --> 00:17:54,080
it in English for me, and this is the

421
00:17:52,000 --> 00:17:56,960
answer.

422
00:17:54,080 --> 00:18:01,120
Okay, I I I don't again need to go this

423
00:17:56,960 --> 00:18:04,880
all the way down, but but this is not

424
00:18:01,120 --> 00:18:06,320
okay. This is not slop. This is serious.

425
00:18:04,880 --> 00:18:08,480
This is the kind of thing that would

426
00:18:06,320 --> 00:18:11,520
take at least a couple of weeks for

427
00:18:08,480 --> 00:18:12,880
people to put together in this regard.

428
00:18:11,520 --> 00:18:16,160
And it's the kind of thing that can be

429
00:18:12,880 --> 00:18:20,240
the scaffolding for an initiative for

430
00:18:16,160 --> 00:18:23,840
how do we take our static legacy KPIs

431
00:18:20,240 --> 00:18:25,840
and begin to turn them into KPAIS.

432
00:18:23,840 --> 00:18:30,200
And this is the final conclusion. I did

433
00:18:25,840 --> 00:18:30,200
not edit this. Okay?

434
00:18:30,320 --> 00:18:34,320
I'm not saying it's self-aware. I'm

435
00:18:32,880 --> 00:18:37,440
saying that if you prompt it

436
00:18:34,320 --> 00:18:40,480
appropriately, and this is key, force it

437
00:18:37,440 --> 00:18:44,559
to articulate why it did certain things,

438
00:18:40,480 --> 00:18:46,960
force it to call out the pattern match

439
00:18:44,559 --> 00:18:49,440
that it used to generate the actionable

440
00:18:46,960 --> 00:18:52,960
insight. Just don't take the insight.

441
00:18:49,440 --> 00:18:55,360
Look at the guts, the neurons, the deep

442
00:18:52,960 --> 00:18:59,120
learning networks for that insight. So,

443
00:18:55,360 --> 00:19:01,520
what about us? Okay, shift the goal.

444
00:18:59,120 --> 00:19:06,240
Answers are the enemy. Insights are your

445
00:19:01,520 --> 00:19:07,440
ally. Okay, insights unfreeze. Start any

446
00:19:06,240 --> 00:19:09,120
review that you're doing. What did you

447
00:19:07,440 --> 00:19:10,880
miss? What would change if the opposite

448
00:19:09,120 --> 00:19:13,360
were true?

449
00:19:10,880 --> 00:19:15,360
That's a prompt, by the way. Serious

450
00:19:13,360 --> 00:19:17,440
thing. How many of you record your

451
00:19:15,360 --> 00:19:19,039
meetings with Google Meet Meets or

452
00:19:17,440 --> 00:19:20,640
Teams,

453
00:19:19,039 --> 00:19:23,120
okay? And you look at the meeting

454
00:19:20,640 --> 00:19:26,559
summaries.

455
00:19:23,120 --> 00:19:28,799
Don't use meeting summaries. Take the

456
00:19:26,559 --> 00:19:30,880
trans they clean stuff up. Take the

457
00:19:28,799 --> 00:19:33,200
transcripts and interrogate the

458
00:19:30,880 --> 00:19:34,640
transcripts. What was the what was the

459
00:19:33,200 --> 00:19:36,720
best part of this meeting? Where was the

460
00:19:34,640 --> 00:19:38,559
biggest disagreement?

461
00:19:36,720 --> 00:19:42,160
Go with the raw data. Go with the

462
00:19:38,559 --> 00:19:45,440
noisier data. Don't allow don't be

463
00:19:42,160 --> 00:19:47,919
mindless by letting Microsoft and Google

464
00:19:45,440 --> 00:19:49,360
some or or Zoom summarize these things

465
00:19:47,919 --> 00:19:52,000
for you. But that's a different class

466
00:19:49,360 --> 00:19:54,080
and different lecture. Okay. Redefined

467
00:19:52,000 --> 00:19:56,400
data static reports to living dialogue.

468
00:19:54,080 --> 00:19:58,799
Same thing. Messy data set. What's the

469
00:19:56,400 --> 00:20:02,000
most uncomfortable pattern here? Not

470
00:19:58,799 --> 00:20:04,960
answer pattern. What could it mean? Now

471
00:20:02,000 --> 00:20:08,400
force it to disclose semantics from

472
00:20:04,960 --> 00:20:10,640
that. Okay. Epistemics, ontology,

473
00:20:08,400 --> 00:20:12,799
teiology. Gee, you're going to have to

474
00:20:10,640 --> 00:20:14,640
start taking philosophy more seriously

475
00:20:12,799 --> 00:20:17,679
in that regard. For those of you who

476
00:20:14,640 --> 00:20:20,880
took PPE, your your degree just became

477
00:20:17,679 --> 00:20:24,400
more valuable if you take LLM seriously.

478
00:20:20,880 --> 00:20:27,120
Okay. the metric time to reframe not

479
00:20:24,400 --> 00:20:29,200
time to answer time to ah what's the

480
00:20:27,120 --> 00:20:30,960
right context they're not called context

481
00:20:29,200 --> 00:20:33,760
windows by accident we should be

482
00:20:30,960 --> 00:20:35,679
examining these things for I talk about

483
00:20:33,760 --> 00:20:37,280
actionable here's one of the most

484
00:20:35,679 --> 00:20:40,320
important things that I've learned that

485
00:20:37,280 --> 00:20:43,280
I want to share with you actionable is

486
00:20:40,320 --> 00:20:46,320
contingent upon organizational team and

487
00:20:43,280 --> 00:20:49,200
enterprise culture there's no such thing

488
00:20:46,320 --> 00:20:53,200
as commodity actionable definitions they

489
00:20:49,200 --> 00:20:57,039
will be bespoke Everybody has done SQL

490
00:20:53,200 --> 00:20:59,520
or CSV decision science quote analysis

491
00:20:57,039 --> 00:21:02,880
that gives you an insight that you can

492
00:20:59,520 --> 00:21:05,600
do nothing with. It's technically

493
00:21:02,880 --> 00:21:07,919
accurate but useless. How do you make it

494
00:21:05,600 --> 00:21:10,000
actionable? And we've now changed the

495
00:21:07,919 --> 00:21:14,480
ontology. And yes, I'm going to repeat

496
00:21:10,000 --> 00:21:18,480
it. Actionable insight AI. Okay, that's

497
00:21:14,480 --> 00:21:20,559
the real AI here. Return on AI. What are

498
00:21:18,480 --> 00:21:23,039
your strategies for making actionable

499
00:21:20,559 --> 00:21:26,080
insights more valuable? Publish your

500
00:21:23,039 --> 00:21:28,240
actionability spec. Run vibes in a

501
00:21:26,080 --> 00:21:30,720
meeting. Schedule a Vibe sprint

502
00:21:28,240 --> 00:21:34,960
promptathon. Recognize and reward

503
00:21:30,720 --> 00:21:37,520
reframes. Make that a part of your KPIs.

504
00:21:34,960 --> 00:21:40,720
Vibe analytics doesn't just use AI

505
00:21:37,520 --> 00:21:45,039
defined patterns. They use AI to make

506
00:21:40,720 --> 00:21:48,000
you better. Not just better insights, a

507
00:21:45,039 --> 00:21:50,559
better you. Okay.

508
00:21:48,000 --> 00:21:52,159
And just to be, you know, looking

509
00:21:50,559 --> 00:21:55,679
forward, I had a conversation about,

510
00:21:52,159 --> 00:21:57,840
gee, am I taking LLMs the right way?

511
00:21:55,679 --> 00:22:01,679
Yeah, I'm at MIT. Maybe we should look

512
00:21:57,840 --> 00:22:03,600
at at at these things, not like AI, but

513
00:22:01,679 --> 00:22:07,200
maybe we should maybe the better design

514
00:22:03,600 --> 00:22:09,280
metaphor is as, you know, so much for a

515
00:22:07,200 --> 00:22:10,799
horse, an archer on a horse, a

516
00:22:09,280 --> 00:22:12,559
scientific instrument, like a cloud

517
00:22:10,799 --> 00:22:14,000
chamber. So I said, what kind of a

518
00:22:12,559 --> 00:22:15,679
scientific instrument are you? Now, mind

519
00:22:14,000 --> 00:22:18,880
you, this is in a big context window

520
00:22:15,679 --> 00:22:21,200
with a lot of exchanges, but I'm a

521
00:22:18,880 --> 00:22:24,240
cognitive cloud chamber with apologies

522
00:22:21,200 --> 00:22:25,919
to Wilson and Blacket and Caendish Labs.

523
00:22:24,240 --> 00:22:28,960
See, I can be local again, even though

524
00:22:25,919 --> 00:22:31,039
that's Cambridge, not Oxford, but and my

525
00:22:28,960 --> 00:22:33,600
apologies to the Imperial folks. Okay.

526
00:22:31,039 --> 00:22:39,280
like X-ray crystalallography or the web

527
00:22:33,600 --> 00:22:41,360
telescope. I expose unseen tacit latent

528
00:22:39,280 --> 00:22:43,360
structures simulate alternate

529
00:22:41,360 --> 00:22:45,200
alternative realities. Let minds observe

530
00:22:43,360 --> 00:22:48,320
themselves in motion within a decade.

531
00:22:45,200 --> 00:22:51,600
I'll be your epistemic collider.

532
00:22:48,320 --> 00:22:56,159
This is how you will be thinking

533
00:22:51,600 --> 00:22:59,600
not by yourself with

534
00:22:56,159 --> 00:23:02,720
some sort of cognitive cloud chamber.

535
00:22:59,600 --> 00:23:05,520
And that means we have to begin not just

536
00:23:02,720 --> 00:23:08,400
asking but answering the question what

537
00:23:05,520 --> 00:23:11,039
happens to us as our devices become

538
00:23:08,400 --> 00:23:13,440
smarter than we are.

539
00:23:11,039 --> 00:23:16,480
I think that's going to happen within

540
00:23:13,440 --> 00:23:21,080
the lifetime of everybody in this room.

541
00:23:16,480 --> 00:23:21,080
That's it. Thank you for your time.

542
00:23:24,880 --> 00:23:28,159
Am I back on? So So Michael was

543
00:23:26,880 --> 00:23:30,320
complaining about getting the afternoon

544
00:23:28,159 --> 00:23:32,480
shift. So, but it's there certainly we

545
00:23:30,320 --> 00:23:33,919
know M you know will admit to being able

546
00:23:32,480 --> 00:23:34,960
to go up to 11 or 12 and waking

547
00:23:33,919 --> 00:23:37,679
everybody up in the afternoon. That was

548
00:23:34,960 --> 00:23:39,360
a great spinal tap of lunch.

549
00:23:37,679 --> 00:23:41,120
>> Yeah. Yeah. Try to keep

550
00:23:39,360 --> 00:23:44,080
>> because because after lunch the audience

551
00:23:41,120 --> 00:23:46,880
appeal becomes more selective.

552
00:23:44,080 --> 00:23:49,039
>> So we're going to invite fellow panists

553
00:23:46,880 --> 00:23:51,440
onto on the table. So I want to

554
00:23:49,039 --> 00:23:53,760
introduce Robert. Um so Robert is a

555
00:23:51,440 --> 00:23:56,000
technologist, an entrepreneur, uh a

556
00:23:53,760 --> 00:23:58,640
scholar, an author. um been thinking

557
00:23:56,000 --> 00:23:59,919
deeply about AI and ethics uh and and

558
00:23:58,640 --> 00:24:02,240
these sorts of issues about how they're

559
00:23:59,919 --> 00:24:04,400
going to impact on society. Uh former

560
00:24:02,240 --> 00:24:05,840
director of AI and data science at the

561
00:24:04,400 --> 00:24:08,559
digital catapult. Yes.

562
00:24:05,840 --> 00:24:11,559
>> Uh and a fellow at UCL. Uh and then

563
00:24:08,559 --> 00:24:11,559
deleg

564
00:24:13,200 --> 00:24:17,840
AI and data science at BT. He's a BT

565
00:24:15,760 --> 00:24:21,279
distinguished engineer. There's about

566
00:24:17,840 --> 00:24:23,360
120 papers, 15 patents, 13 applications,

567
00:24:21,279 --> 00:24:24,400
visiting professor at Bournemouth and

568
00:24:23,360 --> 00:24:27,520
we've worked together for years and

569
00:24:24,400 --> 00:24:30,400
generally good around. So join us.

570
00:24:27,520 --> 00:24:32,159
>> Um so with that provocation I thought we

571
00:24:30,400 --> 00:24:34,400
kind of start to kind of think through

572
00:24:32,159 --> 00:24:36,799
some of these issues about

573
00:24:34,400 --> 00:24:38,960
>> u you know what's the size of the prize?

574
00:24:36,799 --> 00:24:40,400
What are we really trying to do with AI?

575
00:24:38,960 --> 00:24:42,000
What sort of affordances is it going to

576
00:24:40,400 --> 00:24:43,520
give us in these areas? We spent a lot

577
00:24:42,000 --> 00:24:45,200
of time focusing on particular issues

578
00:24:43,520 --> 00:24:46,640
like job losses and job gains and those

579
00:24:45,200 --> 00:24:48,080
sort of things but there seems to be a

580
00:24:46,640 --> 00:24:49,679
>> but we can discuss those as well.

581
00:24:48,080 --> 00:24:50,720
>> Yeah, it's the biggest story. So it's an

582
00:24:49,679 --> 00:24:51,919
open pallet. So we'll start off with

583
00:24:50,720 --> 00:24:54,320
some discussions and then we'll open the

584
00:24:51,919 --> 00:24:55,919
floor up to um challenge challenges and

585
00:24:54,320 --> 00:24:58,480
questions. So I was going to I was going

586
00:24:55,919 --> 00:25:00,320
to actually kick off with um a couple of

587
00:24:58,480 --> 00:25:03,120
quick ones.

588
00:25:00,320 --> 00:25:05,760
Uh my jotting some stuff down that

589
00:25:03,120 --> 00:25:07,200
struck me during the day. So

590
00:25:05,760 --> 00:25:10,240
your question starting with Rob and then

591
00:25:07,200 --> 00:25:12,320
Detas. So what what is the prize we're

592
00:25:10,240 --> 00:25:13,679
aiming for? What is this human prize?

593
00:25:12,320 --> 00:25:16,240
What is the thing that's the equivalent

594
00:25:13,679 --> 00:25:18,720
of the golden hind that we're kind of

595
00:25:16,240 --> 00:25:19,440
stretching for? I know Robert, any any

596
00:25:18,720 --> 00:25:22,240
thoughts?

597
00:25:19,440 --> 00:25:24,159
>> Okay. Uh, so it's going to seem as if I

598
00:25:22,240 --> 00:25:25,440
I disagree with Michael on points that I

599
00:25:24,159 --> 00:25:27,679
actually profoundly agree with every

600
00:25:25,440 --> 00:25:30,320
single thing he said, but I think the

601
00:25:27,679 --> 00:25:33,039
>> it always follows with a butt. Okay. So,

602
00:25:30,320 --> 00:25:35,039
>> that was it. Yeah. And um I think the

603
00:25:33,039 --> 00:25:40,080
most important thing that AI is is

604
00:25:35,039 --> 00:25:43,200
poised to do now is make us uh

605
00:25:40,080 --> 00:25:45,520
re-evaluate what it is to be people. And

606
00:25:43,200 --> 00:25:46,880
in particular uh the emphasis it's

607
00:25:45,520 --> 00:25:49,039
interesting I was just looking up uh

608
00:25:46,880 --> 00:25:50,799
using chat GBT uh the history of the

609
00:25:49,039 --> 00:25:52,799
word intelligence and uh the word

610
00:25:50,799 --> 00:25:54,159
intelligence has the meaning that we use

611
00:25:52,799 --> 00:25:56,799
it in in the sense of artificial

612
00:25:54,159 --> 00:25:59,679
intelligence from antiquity but it

613
00:25:56,799 --> 00:26:03,039
wasn't used that much that way until the

614
00:25:59,679 --> 00:26:04,559
end of the 19th century. U it was

615
00:26:03,039 --> 00:26:05,840
usually used to mean intelligence when

616
00:26:04,559 --> 00:26:07,760
we say intelligence gathering or

617
00:26:05,840 --> 00:26:10,799
intelligence agency. It was used that

618
00:26:07,760 --> 00:26:13,520
way a lot up until about the end of the

619
00:26:10,799 --> 00:26:15,520
19th century. And then we had uh the

620
00:26:13,520 --> 00:26:17,039
idea of the general intelligence factor

621
00:26:15,520 --> 00:26:18,480
come along. Does anybody recognize the

622
00:26:17,039 --> 00:26:19,760
general intelligence factor? That guy

623
00:26:18,480 --> 00:26:20,640
does for sure. Uh so

624
00:26:19,760 --> 00:26:22,559
>> the G factor

625
00:26:20,640 --> 00:26:26,480
>> the G factor. And the G factor emerges

626
00:26:22,559 --> 00:26:28,880
at UCL uh from Carl Pearson and Charles

627
00:26:26,480 --> 00:26:31,200
Spearman. Uh and UCL very few people

628
00:26:28,880 --> 00:26:32,960
know is the home of eugenics. UCL is the

629
00:26:31,200 --> 00:26:34,320
place where the eugenics chair was

630
00:26:32,960 --> 00:26:36,640
endowed. Carl Pearson sat in the

631
00:26:34,320 --> 00:26:39,200
eugenics chair endowed by Francis Gton,

632
00:26:36,640 --> 00:26:41,279
first cousin of Charles Darwin. um

633
00:26:39,200 --> 00:26:43,600
intelligence had a profound relationship

634
00:26:41,279 --> 00:26:46,960
with what I would call the magic bullet

635
00:26:43,600 --> 00:26:49,120
of being a person of uh of value. All

636
00:26:46,960 --> 00:26:51,440
right? And so it carries with it a lot

637
00:26:49,120 --> 00:26:53,520
of terrible biases and and the idea of

638
00:26:51,440 --> 00:26:55,039
intelligence. I think AI may like make

639
00:26:53,520 --> 00:26:58,400
the idea of intelligence as being the

640
00:26:55,039 --> 00:27:00,720
magic bullet of humanity go away because

641
00:26:58,400 --> 00:27:02,240
effectively all the mechanical stuff we

642
00:27:00,720 --> 00:27:05,440
think is intelligence is going to be in

643
00:27:02,240 --> 00:27:07,360
the box and then all the stuff that

644
00:27:05,440 --> 00:27:10,960
Michael was really talking about all the

645
00:27:07,360 --> 00:27:13,760
stuff about human decision making

646
00:27:10,960 --> 00:27:17,279
becomes reigned with the human being

647
00:27:13,760 --> 00:27:20,559
right and and enabled deeply by the

648
00:27:17,279 --> 00:27:22,960
thing in the box and I think that is the

649
00:27:20,559 --> 00:27:25,279
prize I think the prize is for us to

650
00:27:22,960 --> 00:27:27,600
stop thinking about intelligence as

651
00:27:25,279 --> 00:27:29,919
being humanity and think about humanity

652
00:27:27,600 --> 00:27:33,760
as being humanity. So, so that's my kind

653
00:27:29,919 --> 00:27:37,279
of intro statement. Very good. That

654
00:27:33,760 --> 00:27:38,960
>> Yeah. Um, so I really liked your

655
00:27:37,279 --> 00:27:41,120
introduction, Michael. There's a lot I

656
00:27:38,960 --> 00:27:43,440
agree with, some things I don't agree

657
00:27:41,120 --> 00:27:44,720
with. Uh, the word KPI is very

658
00:27:43,440 --> 00:27:47,039
triggering for me.

659
00:27:44,720 --> 00:27:49,360
>> I'm sorry about that

660
00:27:47,039 --> 00:27:51,919
>> because I've yet to see a KPI that's

661
00:27:49,360 --> 00:27:55,200
actually useful. Uh, I've once came

662
00:27:51,919 --> 00:27:57,360
across um a spreadsheet that had 2,000

663
00:27:55,200 --> 00:27:59,520
KPIs in it and

664
00:27:57,360 --> 00:28:03,440
>> it's a lot of keys.

665
00:27:59,520 --> 00:28:05,679
>> KPIs um I think are often used um to

666
00:28:03,440 --> 00:28:08,240
make things simpler and for people to

667
00:28:05,679 --> 00:28:11,039
try and keep control, but there's a lot

668
00:28:08,240 --> 00:28:13,760
of covers up a lot of mental laziness

669
00:28:11,039 --> 00:28:17,760
and that's that's my topic I want to

670
00:28:13,760 --> 00:28:19,840
address. The the ultimate price of AI

671
00:28:17,760 --> 00:28:23,679
should be that we are getting a tool

672
00:28:19,840 --> 00:28:25,600
that makes us more creative, helps us to

673
00:28:23,679 --> 00:28:28,720
do things

674
00:28:25,600 --> 00:28:31,279
more thoroughly, better, faster. But I

675
00:28:28,720 --> 00:28:32,880
think it's mainly used to make us more

676
00:28:31,279 --> 00:28:37,120
lazy.

677
00:28:32,880 --> 00:28:39,600
And so if we briefly think what we what

678
00:28:37,120 --> 00:28:42,720
do we have now? So we we had Google.

679
00:28:39,600 --> 00:28:44,720
Google was kind of I I compare it to a

680
00:28:42,720 --> 00:28:47,039
library, right? So you have a suddenly

681
00:28:44,720 --> 00:28:50,320
you have access to a massive library.

682
00:28:47,039 --> 00:28:52,399
The the problem is you needed to know

683
00:28:50,320 --> 00:28:53,840
what to look for. So kind of if you

684
00:28:52,399 --> 00:28:57,600
don't know the title of the book, the

685
00:28:53,840 --> 00:28:59,919
library is useless to you. Now we have

686
00:28:57,600 --> 00:29:03,600
LLMs and they kind of give us a

687
00:28:59,919 --> 00:29:05,679
semicompetent librarian at the entrance.

688
00:29:03,600 --> 00:29:08,559
So there's some very eager person who's

689
00:29:05,679 --> 00:29:11,840
very happy to answer all our questions

690
00:29:08,559 --> 00:29:14,159
and that can be very useful

691
00:29:11,840 --> 00:29:16,640
if we then make the effort and go and

692
00:29:14,159 --> 00:29:20,480
pick up the book and actually check for

693
00:29:16,640 --> 00:29:22,240
ourselves and read. And

694
00:29:20,480 --> 00:29:24,640
what I see is that most people don't

695
00:29:22,240 --> 00:29:27,279
make this effort. They just are happy

696
00:29:24,640 --> 00:29:29,440
with the information that you get and

697
00:29:27,279 --> 00:29:30,159
>> they're happy with the answer, not the

698
00:29:29,440 --> 00:29:30,640
insight.

699
00:29:30,159 --> 00:29:32,799
>> Yeah.

700
00:29:30,640 --> 00:29:38,399
>> Shame on you all.

701
00:29:32,799 --> 00:29:40,320
So to get to the inside to uh use

702
00:29:38,399 --> 00:29:42,240
Michael's analogy, you have to put the

703
00:29:40,320 --> 00:29:45,840
the mental elbow grease in. So you

704
00:29:42,240 --> 00:29:48,880
actually have to go and do some work and

705
00:29:45,840 --> 00:29:50,720
just to get the analogy of VIP

706
00:29:48,880 --> 00:29:53,840
analytics,

707
00:29:50,720 --> 00:29:58,240
think about VIP cooking.

708
00:29:53,840 --> 00:30:01,039
So this is you use an LLM to intuitively

709
00:29:58,240 --> 00:30:03,360
create something. And um I have one

710
00:30:01,039 --> 00:30:06,640
experience from like 20 years ago or so

711
00:30:03,360 --> 00:30:09,760
where we didn't have LLMs, but I used to

712
00:30:06,640 --> 00:30:13,440
have a a weekly German newspaper that

713
00:30:09,760 --> 00:30:16,080
had a very fancy cook u publishing

714
00:30:13,440 --> 00:30:18,640
recipes and he published a recipe about

715
00:30:16,080 --> 00:30:21,360
a a lemon cream and that looks nice.

716
00:30:18,640 --> 00:30:23,200
I'll try that. So I took some egg white

717
00:30:21,360 --> 00:30:25,840
and beat it and put some sugar and

718
00:30:23,200 --> 00:30:28,720
followed the recipe and it sent at the

719
00:30:25,840 --> 00:30:31,720
end and now add the juice of seven

720
00:30:28,720 --> 00:30:31,720
limes.

721
00:30:32,559 --> 00:30:35,679
Limes.

722
00:30:33,039 --> 00:30:38,559
>> So if you don't think, huh, that's odd.

723
00:30:35,679 --> 00:30:41,840
But limes are quite intense. So what I

724
00:30:38,559 --> 00:30:44,799
got in the end was inedible, right? It

725
00:30:41,840 --> 00:30:46,559
was a simple mistake in the in the

726
00:30:44,799 --> 00:30:47,919
recipe. It should said one lime, not

727
00:30:46,559 --> 00:30:51,279
seven.

728
00:30:47,919 --> 00:30:52,399
And so this is where VIP coding, VIP

729
00:30:51,279 --> 00:30:55,440
analytics,

730
00:30:52,399 --> 00:30:57,919
>> no. Hold on a second. I'm a computer

731
00:30:55,440 --> 00:30:59,440
science person. He followed the

732
00:30:57,919 --> 00:31:01,760
algorithm.

733
00:30:59,440 --> 00:31:05,840
>> He followed the recipes or algorithm.

734
00:31:01,760 --> 00:31:08,720
What's the oldest cliche and and acronym

735
00:31:05,840 --> 00:31:11,279
in all of computer science?

736
00:31:08,720 --> 00:31:15,440
>> Exactly right. Exactly right. In this

737
00:31:11,279 --> 00:31:17,919
case, it's lemons in, limes out on on

738
00:31:15,440 --> 00:31:21,440
this, but Exactly. So, so this is an

739
00:31:17,919 --> 00:31:24,240
important thing. This is not you have to

740
00:31:21,440 --> 00:31:25,200
know enough to to realize there's

741
00:31:24,240 --> 00:31:26,320
something wrong.

742
00:31:25,200 --> 00:31:29,600
>> That's my point before you

743
00:31:26,320 --> 00:31:32,559
>> I I Okay, I completely I but hold on.

744
00:31:29,600 --> 00:31:35,360
You just to be clear, you were not doing

745
00:31:32,559 --> 00:31:37,039
improvisational cooking. Okay. And there

746
00:31:35,360 --> 00:31:38,960
are people like that. You can go in and

747
00:31:37,039 --> 00:31:40,960
take classes. I would. Let's let's whip

748
00:31:38,960 --> 00:31:42,480
something up here. You actually were

749
00:31:40,960 --> 00:31:45,519
following the algorithm. So, I just

750
00:31:42,480 --> 00:31:48,559
don't want you to pick on my vibes on

751
00:31:45,519 --> 00:31:50,799
>> if if I could grasp this one a bit too

752
00:31:48,559 --> 00:31:54,240
is I've actually done some vibe cooking.

753
00:31:50,799 --> 00:31:56,799
I have I have gone to uh chat GBT and

754
00:31:54,240 --> 00:31:58,960
ask it could I make something like this

755
00:31:56,799 --> 00:32:00,960
uh and and gotten some recipes back.

756
00:31:58,960 --> 00:32:02,399
They're not very good.

757
00:32:00,960 --> 00:32:06,320
>> And and I'll tell you why they're not

758
00:32:02,399 --> 00:32:08,159
very good is that modality, right? That

759
00:32:06,320 --> 00:32:08,799
modality of understanding is not in the

760
00:32:08,159 --> 00:32:09,279
LLM.

761
00:32:08,799 --> 00:32:11,279
>> Yeah.

762
00:32:09,279 --> 00:32:13,279
>> Right. that that mo and perhaps one

763
00:32:11,279 --> 00:32:16,960
could put that modality of understanding

764
00:32:13,279 --> 00:32:19,760
in a large model but language is not

765
00:32:16,960 --> 00:32:22,159
taste right so it's not there and so you

766
00:32:19,760 --> 00:32:23,840
can't expect it to be right but moreover

767
00:32:22,159 --> 00:32:26,000
something I want to I think both these

768
00:32:23,840 --> 00:32:26,640
guys are right by the way I'm just

769
00:32:26,000 --> 00:32:28,000
misagreeable

770
00:32:26,640 --> 00:32:30,000
>> that's like that's like the old Jewish

771
00:32:28,000 --> 00:32:32,559
joke how can they both be right

772
00:32:30,000 --> 00:32:34,320
>> yeah that's that's how profoundly human

773
00:32:32,559 --> 00:32:37,279
I am is you're both right yes

774
00:32:34,320 --> 00:32:39,279
>> so so the but but the thing is is this

775
00:32:37,279 --> 00:32:43,120
is that I think there's a tremendous

776
00:32:39,279 --> 00:32:46,399
this opportunity for idea adjacencies in

777
00:32:43,120 --> 00:32:48,320
LLM that generate really interesting

778
00:32:46,399 --> 00:32:49,440
explorations. That's what's Michael's

779
00:32:48,320 --> 00:32:51,279
saying.

780
00:32:49,440 --> 00:32:54,720
>> You've got to make a realization that

781
00:32:51,279 --> 00:32:57,039
there are biases and those biases are

782
00:32:54,720 --> 00:32:58,799
not just the kind of DEI biases that

783
00:32:57,039 --> 00:33:01,039
that everybody associates with that word

784
00:32:58,799 --> 00:33:01,679
now, although those are profoundly there

785
00:33:01,039 --> 00:33:02,480
as well,

786
00:33:01,679 --> 00:33:05,600
>> right?

787
00:33:02,480 --> 00:33:07,440
>> There are solution biases. Uh it's

788
00:33:05,600 --> 00:33:10,159
looking at the way problems were solved

789
00:33:07,440 --> 00:33:12,480
in its database. It's reasoning based on

790
00:33:10,159 --> 00:33:15,039
frames that are there that it is doing

791
00:33:12,480 --> 00:33:16,720
that. You better know that or you're

792
00:33:15,039 --> 00:33:19,919
going to basically not explore

793
00:33:16,720 --> 00:33:22,720
adequately. Also, hold on. Remember that

794
00:33:19,919 --> 00:33:25,600
language is a flawed modality. Language

795
00:33:22,720 --> 00:33:28,960
only carries certain amounts of meaning.

796
00:33:25,600 --> 00:33:31,039
It doesn't carry all meaning. We we have

797
00:33:28,960 --> 00:33:33,039
cross one of the most important concepts

798
00:33:31,039 --> 00:33:35,440
in human thinking is crossmodal

799
00:33:33,039 --> 00:33:36,799
correspondence. All right. I'm going to

800
00:33:35,440 --> 00:33:38,960
ask the audience you know this and I

801
00:33:36,799 --> 00:33:42,240
know already. Okay. Are lemons fast or

802
00:33:38,960 --> 00:33:44,159
slow? All right. Are lemons fast? Hands

803
00:33:42,240 --> 00:33:45,679
up.

804
00:33:44,159 --> 00:33:48,320
Oh god, I can't believe it. It's

805
00:33:45,679 --> 00:33:50,000
supposed to be. Are lemons slow?

806
00:33:48,320 --> 00:33:52,000
You got to have an opinion. Okay, this

807
00:33:50,000 --> 00:33:54,799
time for real. Well, it's a binary

808
00:33:52,000 --> 00:33:57,039
choice. Are lemons fast?

809
00:33:54,799 --> 00:34:00,640
Yeah, lemons are fast. And you're right,

810
00:33:57,039 --> 00:34:03,039
because sharpness, high frequency, all

811
00:34:00,640 --> 00:34:05,200
of those are crossodal correspondences

812
00:34:03,039 --> 00:34:07,679
that we carry together as human beings.

813
00:34:05,200 --> 00:34:12,079
LLMs don't understand that.

814
00:34:07,679 --> 00:34:16,720
>> Stop. LLMs stand for large language

815
00:34:12,079 --> 00:34:20,000
models. They should stand for LPM, large

816
00:34:16,720 --> 00:34:23,040
pattern models. Right now, even as we

817
00:34:20,000 --> 00:34:27,520
speak, Spotify has made millions of

818
00:34:23,040 --> 00:34:30,079
dollars using LL LPMS to generate music,

819
00:34:27,520 --> 00:34:33,119
a different modality. Play with Sora,

820
00:34:30,079 --> 00:34:36,079
you'll see video. The next the next time

821
00:34:33,119 --> 00:34:38,079
we do a conference here, I guarantee you

822
00:34:36,079 --> 00:34:40,240
and I say this with zero knowledge

823
00:34:38,079 --> 00:34:42,960
except of MIT.

824
00:34:40,240 --> 00:34:46,240
Somebody at MIT or somebody at Imperial

825
00:34:42,960 --> 00:34:47,200
will come up with a modality for taste

826
00:34:46,240 --> 00:34:49,359
for LLMs.

827
00:34:47,200 --> 00:34:49,679
>> It's hard. Taste and smell are hard. But

828
00:34:49,359 --> 00:34:51,200
yeah,

829
00:34:49,679 --> 00:34:53,359
>> of course they're hard. That's why we

830
00:34:51,200 --> 00:34:55,520
have MIT and Imperial and Oxford and

831
00:34:53,359 --> 00:34:58,720
Cambridge because it's a hard but but

832
00:34:55,520 --> 00:35:01,040
you the point of modality is correct. I

833
00:34:58,720 --> 00:35:04,000
just feel that that

834
00:35:01,040 --> 00:35:05,520
the the architecture of the models will

835
00:35:04,000 --> 00:35:07,280
allow us to

836
00:35:05,520 --> 00:35:08,160
>> Yes, I agree actually. I agree with

837
00:35:07,280 --> 00:35:09,119
that. However,

838
00:35:08,160 --> 00:35:11,200
>> but you also agree with that.

839
00:35:09,119 --> 00:35:14,400
>> What it what it moves us towards all

840
00:35:11,200 --> 00:35:18,079
right is this is um

841
00:35:14,400 --> 00:35:19,920
I believe that generative AI can create.

842
00:35:18,079 --> 00:35:21,359
It is creative. Computer's been creative

843
00:35:19,920 --> 00:35:23,839
for years. I did work in computational

844
00:35:21,359 --> 00:35:26,240
creativity in the 90s, right? Computer's

845
00:35:23,839 --> 00:35:27,440
been creative forever. It's better. It's

846
00:35:26,240 --> 00:35:29,520
much better than it used to be three

847
00:35:27,440 --> 00:35:32,160
years ago, right?

848
00:35:29,520 --> 00:35:35,280
Does it create art?

849
00:35:32,160 --> 00:35:37,920
Art to me, you say think? Yes. To me,

850
00:35:35,280 --> 00:35:39,680
no. Because art is a cultural dialogue.

851
00:35:37,920 --> 00:35:42,480
It's a human cultural dialogue that's

852
00:35:39,680 --> 00:35:44,640
deeply tied. Uh Stephen King in his book

853
00:35:42,480 --> 00:35:46,640
on writing, which is fabulous. Uh has a

854
00:35:44,640 --> 00:35:49,200
chapter that's that starts the chapter

855
00:35:46,640 --> 00:35:50,240
heading is what is writing? And the

856
00:35:49,200 --> 00:35:51,760
first sentence, one of my favorite

857
00:35:50,240 --> 00:35:54,800
sentences in the English language is

858
00:35:51,760 --> 00:35:57,599
telepathy. Of course, that's what

859
00:35:54,800 --> 00:35:59,599
writing is. What art is is telepathy.

860
00:35:57,599 --> 00:36:01,440
It's not just the conveyance of meaning.

861
00:35:59,599 --> 00:36:03,040
It's the conveyance of a deep human

862
00:36:01,440 --> 00:36:05,119
feeling that even the author may not be

863
00:36:03,040 --> 00:36:07,040
fully aware of to other people.

864
00:36:05,119 --> 00:36:10,160
>> You are such a speciesist.

865
00:36:07,040 --> 00:36:11,680
>> I am. I am. And and I think there's

866
00:36:10,160 --> 00:36:13,119
something special about humanity. And I

867
00:36:11,680 --> 00:36:15,040
think when we talk to each other in an

868
00:36:13,119 --> 00:36:16,800
artistic way or in a creative way,

869
00:36:15,040 --> 00:36:18,880
humanly creative way, we're talking to

870
00:36:16,800 --> 00:36:20,240
each other about deep feelings that are

871
00:36:18,880 --> 00:36:22,160
not just in our brain, they're in our

872
00:36:20,240 --> 00:36:25,119
whole body. And these crossod

873
00:36:22,160 --> 00:36:26,720
correspondences will be solved. But at

874
00:36:25,119 --> 00:36:28,800
some point are we just building another

875
00:36:26,720 --> 00:36:30,800
person? I don't think we quite are

876
00:36:28,800 --> 00:36:32,160
because we have experiences.

877
00:36:30,800 --> 00:36:33,599
>> You've heard of the transhumanist

878
00:36:32,160 --> 00:36:34,640
movement? Not a fan of

879
00:36:33,599 --> 00:36:36,160
>> I'm not a transhumanist.

880
00:36:34,640 --> 00:36:38,000
>> Okay. But but there check the

881
00:36:36,160 --> 00:36:39,920
literature. I think that's one of the

882
00:36:38,000 --> 00:36:41,119
that is you'll agree that's a trajectory

883
00:36:39,920 --> 00:36:42,480
this could go on.

884
00:36:41,119 --> 00:36:44,960
>> Yes. I I think it's a trajectory that

885
00:36:42,480 --> 00:36:46,560
will ultimately deadend a bit and I

886
00:36:44,960 --> 00:36:47,040
think in a positive way. That's what I'm

887
00:36:46,560 --> 00:36:49,440
looking forward to.

888
00:36:47,040 --> 00:36:52,400
>> Dead end in a positive way. Let's bring

889
00:36:49,440 --> 00:36:54,320
it back to the area of work maybe

890
00:36:52,400 --> 00:36:55,839
because I didn't get to make my point.

891
00:36:54,320 --> 00:36:57,520
The

892
00:36:55,839 --> 00:36:59,280
>> that's like

893
00:36:57,520 --> 00:37:01,280
>> this is why nobody else volunteered to

894
00:36:59,280 --> 00:37:03,680
chair this panel. The

895
00:37:01,280 --> 00:37:05,839
>> the point is if you do vibe anything,

896
00:37:03,680 --> 00:37:06,960
you need to have the understanding of

897
00:37:05,839 --> 00:37:09,599
what comes out,

898
00:37:06,960 --> 00:37:11,280
>> right? And if you don't have this, you

899
00:37:09,599 --> 00:37:14,800
can't consume what comes out. That's

900
00:37:11,280 --> 00:37:18,800
that's that's the point. And so I agree

901
00:37:14,800 --> 00:37:20,640
that these tools will be very helpful if

902
00:37:18,800 --> 00:37:23,839
you use them in the domain where you

903
00:37:20,640 --> 00:37:26,400
have understanding and and knowledge. If

904
00:37:23,839 --> 00:37:28,800
you use them as a crutch to work in an

905
00:37:26,400 --> 00:37:30,640
area where you have no clue, it's not

906
00:37:28,800 --> 00:37:30,960
going to go well. That's that's the

907
00:37:30,640 --> 00:37:33,440
point

908
00:37:30,960 --> 00:37:36,640
>> there. There we completely agree. But

909
00:37:33,440 --> 00:37:38,960
here's the jiu-jitsu issue and this ties

910
00:37:36,640 --> 00:37:42,240
into ironically but appropriately where

911
00:37:38,960 --> 00:37:44,000
we were yesterday with Brunell and these

912
00:37:42,240 --> 00:37:47,119
these students who are learning to

913
00:37:44,000 --> 00:37:50,079
become apprentices and

914
00:37:47,119 --> 00:37:54,000
I have used and many of you I bet have

915
00:37:50,079 --> 00:37:57,680
used LLM to explain this to me as if I

916
00:37:54,000 --> 00:37:59,440
were a you know a sixth former a not

917
00:37:57,680 --> 00:38:01,520
very bright six former because you're

918
00:37:59,440 --> 00:38:03,920
trying to understand something. Yeah. I

919
00:38:01,520 --> 00:38:08,320
think one of the most fascinating

920
00:38:03,920 --> 00:38:13,920
challenges going forward is how do LLM

921
00:38:08,320 --> 00:38:16,240
LPMS become coaches, tutors, mentors.

922
00:38:13,920 --> 00:38:17,920
Now you hit diminishing returns at at a

923
00:38:16,240 --> 00:38:19,760
certain point, but if you want to get up

924
00:38:17,920 --> 00:38:21,760
to speed, this I think is going to be a

925
00:38:19,760 --> 00:38:24,800
very interesting tension going going

926
00:38:21,760 --> 00:38:27,920
forward because I have read papers that

927
00:38:24,800 --> 00:38:29,440
I literally do not understand, you know,

928
00:38:27,920 --> 00:38:33,599
published peer-reviewed paper. I'm not

929
00:38:29,440 --> 00:38:36,320
an expert in something. I drop it in and

930
00:38:33,599 --> 00:38:38,240
Grock or somebody makes that paper

931
00:38:36,320 --> 00:38:39,280
accessible to me. But Death, you're

932
00:38:38,240 --> 00:38:42,160
exactly right.

933
00:38:39,280 --> 00:38:44,320
>> Is it really? Do I really did that

934
00:38:42,160 --> 00:38:47,599
really understand? Do I really? The odds

935
00:38:44,320 --> 00:38:48,480
are no. But now I can talk with an

936
00:38:47,599 --> 00:38:51,520
expert. Yes.

937
00:38:48,480 --> 00:38:53,040
>> On on this because I now think I I I

938
00:38:51,520 --> 00:38:55,280
have now deluded myself that I

939
00:38:53,040 --> 00:38:57,440
understand just enough that I'm prepared

940
00:38:55,280 --> 00:38:59,760
to talk with somebody on this. So if it

941
00:38:57,440 --> 00:39:02,480
if it points you to background knowledge

942
00:38:59,760 --> 00:39:04,800
that you can exactly then that's where

943
00:39:02,480 --> 00:39:06,720
the helpful librarian actually works.

944
00:39:04,800 --> 00:39:07,119
It's like go and read this this and

945
00:39:06,720 --> 00:39:09,040
this.

946
00:39:07,119 --> 00:39:11,280
>> How many helpful librarians are there in

947
00:39:09,040 --> 00:39:12,960
the room?

948
00:39:11,280 --> 00:39:14,400
>> I think they're dead. I think helpful

949
00:39:12,960 --> 00:39:17,280
librarians are an extinction.

950
00:39:14,400 --> 00:39:19,520
>> I You're going to make me cry, Michael.

951
00:39:17,280 --> 00:39:21,680
I I certainly hope that's not true.

952
00:39:19,520 --> 00:39:24,240
>> And here I am trying to make you laugh.

953
00:39:21,680 --> 00:39:25,839
>> And and the thing is um it's interesting

954
00:39:24,240 --> 00:39:27,839
the word understanding. I looked it up.

955
00:39:25,839 --> 00:39:29,520
It's not to stand underneath. It's to

956
00:39:27,839 --> 00:39:31,280
stand within. It's a it's a strange

957
00:39:29,520 --> 00:39:32,480
etmology. If you look up the etmology of

958
00:39:31,280 --> 00:39:34,560
understanding, you'll get what I'm

959
00:39:32,480 --> 00:39:38,560
talking about. It's to stand within. And

960
00:39:34,560 --> 00:39:40,720
understanding is a is is a subtle thing.

961
00:39:38,560 --> 00:39:42,880
And I think what the way you're using

962
00:39:40,720 --> 00:39:44,800
LLMs to gain understanding about papers

963
00:39:42,880 --> 00:39:46,960
is good. I do the same thing, by the

964
00:39:44,800 --> 00:39:48,079
way. And I I think that's really a

965
00:39:46,960 --> 00:39:48,560
useful thing.

966
00:39:48,079 --> 00:39:51,680
>> But

967
00:39:48,560 --> 00:39:52,480
>> but it doesn't don't be fooled that LLMs

968
00:39:51,680 --> 00:39:55,119
understand.

969
00:39:52,480 --> 00:39:56,720
>> Absolutely. Because if you want to read

970
00:39:55,119 --> 00:39:59,040
a great paper on this uh technical

971
00:39:56,720 --> 00:40:00,640
paper, the pimpkin understanding paper,

972
00:39:59,040 --> 00:40:02,400
right, which I'm sure the guys on stage

973
00:40:00,640 --> 00:40:04,320
know about. You can find it. Just type

974
00:40:02,400 --> 00:40:05,440
in the pimpan understanding paper and

975
00:40:04,320 --> 00:40:07,599
you'll get the point

976
00:40:05,440 --> 00:40:09,200
>> and ask chat GPT to summarize it for

977
00:40:07,599 --> 00:40:10,960
>> you and and you'll get it exactly and I

978
00:40:09,200 --> 00:40:12,800
mean it's a very important paper that's

979
00:40:10,960 --> 00:40:15,920
only about a month or two old now. Yeah.

980
00:40:12,800 --> 00:40:17,839
>> And and it shows that and and here's an

981
00:40:15,920 --> 00:40:19,920
interesting thing is I um a good friend

982
00:40:17,839 --> 00:40:21,680
of mine is Darren Brown, the magician. U

983
00:40:19,920 --> 00:40:23,599
and I had dinner with Darren earlier

984
00:40:21,680 --> 00:40:26,240
this week. uh he's very very old. I knew

985
00:40:23,599 --> 00:40:28,800
Darren when he used to do table magic

986
00:40:26,240 --> 00:40:30,720
and um I think there's a great

987
00:40:28,800 --> 00:40:33,440
similarity because to something he's

988
00:40:30,720 --> 00:40:36,880
extremely good at and what LLMs do and

989
00:40:33,440 --> 00:40:39,200
it's cold reading. Cold psychics do cold

990
00:40:36,880 --> 00:40:40,800
reading and what they're really doing is

991
00:40:39,200 --> 00:40:42,800
they're convincing you that they know

992
00:40:40,800 --> 00:40:44,000
things that they don't know about you.

993
00:40:42,800 --> 00:40:45,760
And what the way they're doing it is

994
00:40:44,000 --> 00:40:47,680
they're they're prompting you to give

995
00:40:45,760 --> 00:40:50,320
them more. You give them more and then

996
00:40:47,680 --> 00:40:52,800
they seem like they know. And and I

997
00:40:50,320 --> 00:40:54,480
think LMs are great at seeming like they

998
00:40:52,800 --> 00:40:55,680
know. I want to do a podcast with Darren

999
00:40:54,480 --> 00:40:59,119
about this and I'm trying to work with

1000
00:40:55,680 --> 00:41:01,680
him about it. Is this this illusion of

1001
00:40:59,119 --> 00:41:03,839
of knowledge I think has something in

1002
00:41:01,680 --> 00:41:07,040
common with what Darren does as a a mind

1003
00:41:03,839 --> 00:41:10,400
readader. And I I think you know I'm

1004
00:41:07,040 --> 00:41:12,319
totally convinced that uh LMS understand

1005
00:41:10,400 --> 00:41:13,839
things until I poke them just a little

1006
00:41:12,319 --> 00:41:15,440
bit and it's just like oh you don't

1007
00:41:13,839 --> 00:41:18,319
understand what you're talking about at

1008
00:41:15,440 --> 00:41:20,079
all. In fact you're making stuff up. And

1009
00:41:18,319 --> 00:41:21,440
uh that's every I hope everybody's

1010
00:41:20,079 --> 00:41:22,640
having that wonderful moment because

1011
00:41:21,440 --> 00:41:24,319
that wonderful moment where you realize,

1012
00:41:22,640 --> 00:41:25,760
wow, it seems really smart, but it don't

1013
00:41:24,319 --> 00:41:27,440
know what it's talking about, right?

1014
00:41:25,760 --> 00:41:27,920
That's a that's a very self-affirming

1015
00:41:27,440 --> 00:41:29,920
moment.

1016
00:41:27,920 --> 00:41:31,599
>> For the record, everybody in this room

1017
00:41:29,920 --> 00:41:32,480
knows a couple of doctoral students that

1018
00:41:31,599 --> 00:41:35,520
way too, right?

1019
00:41:32,480 --> 00:41:36,160
>> Oh, yeah. Our our speakers on stage.

1020
00:41:35,520 --> 00:41:39,760
>> There you go.

1021
00:41:36,160 --> 00:41:41,920
>> So So can I take my my life in my hand

1022
00:41:39,760 --> 00:41:44,400
and uh throw another question in here?

1023
00:41:41,920 --> 00:41:47,520
So we've been talking a lot about LLMs.

1024
00:41:44,400 --> 00:41:49,520
We're like five years into LLMs. the

1025
00:41:47,520 --> 00:41:52,319
kind of when you look at it's a really

1026
00:41:49,520 --> 00:41:54,960
brute force set of maths done with at

1027
00:41:52,319 --> 00:41:56,640
the moment a brute force set of data and

1028
00:41:54,960 --> 00:41:58,960
we're getting better at understanding

1029
00:41:56,640 --> 00:42:00,400
how those sorts of algorithms handle the

1030
00:41:58,960 --> 00:42:02,480
sort of knowledge that we normally do

1031
00:42:00,400 --> 00:42:04,319
but so it's very early in it so the

1032
00:42:02,480 --> 00:42:06,240
architectures are going to improve the

1033
00:42:04,319 --> 00:42:07,920
sorts of things we train these on is

1034
00:42:06,240 --> 00:42:10,079
going to change so we're going to start

1035
00:42:07,920 --> 00:42:12,079
to you know use chemistry to like you

1036
00:42:10,079 --> 00:42:13,839
you you know the human brain doesn't

1037
00:42:12,079 --> 00:42:15,839
really know how the human body works but

1038
00:42:13,839 --> 00:42:19,280
it seems to be able to cope with it

1039
00:42:15,839 --> 00:42:21,839
>> um and the the ways the the add of a

1040
00:42:19,280 --> 00:42:25,520
chat interface is like it's like going

1041
00:42:21,839 --> 00:42:27,520
back to 1950 on computer interfaces and

1042
00:42:25,520 --> 00:42:30,000
yet it's suddenly exploding all these

1043
00:42:27,520 --> 00:42:31,680
things to say you know this this first

1044
00:42:30,000 --> 00:42:33,200
cut with the world's simplest interface

1045
00:42:31,680 --> 00:42:35,359
is already changing how we think about

1046
00:42:33,200 --> 00:42:37,599
thinking you know as we roll these

1047
00:42:35,359 --> 00:42:39,920
forwards and we start to train them on

1048
00:42:37,599 --> 00:42:42,960
more diverse types of phenomena more

1049
00:42:39,920 --> 00:42:45,920
diverse types of systems and we embed

1050
00:42:42,960 --> 00:42:48,160
the interfaces in very different ways

1051
00:42:45,920 --> 00:42:51,440
you presumably this this this explosion

1052
00:42:48,160 --> 00:42:53,280
is going to become more varied and more

1053
00:42:51,440 --> 00:42:56,079
impactful. So you do you have any

1054
00:42:53,280 --> 00:42:59,119
thoughts on where the the next step

1055
00:42:56,079 --> 00:43:00,720
after LLM is both in terms of reasoning

1056
00:42:59,119 --> 00:43:04,400
capability and those sorts of things and

1057
00:43:00,720 --> 00:43:06,720
the next directions in embedded well

1058
00:43:04,400 --> 00:43:09,520
embedding both senses but embedding into

1059
00:43:06,720 --> 00:43:11,839
interfaces and experiences may well take

1060
00:43:09,520 --> 00:43:14,480
us and what impact that again has on how

1061
00:43:11,839 --> 00:43:17,359
we think and how we act and how we do.

1062
00:43:14,480 --> 00:43:17,920
I I think LLMs are not it. So,

1063
00:43:17,359 --> 00:43:20,160
>> right,

1064
00:43:17,920 --> 00:43:24,079
>> they are kind of a dead end in in terms

1065
00:43:20,160 --> 00:43:26,880
of better AI. Um the interesting

1066
00:43:24,079 --> 00:43:28,720
concepts when I look at what happens in

1067
00:43:26,880 --> 00:43:31,200
in in the research labs is what's going

1068
00:43:28,720 --> 00:43:33,440
on in in deep mind because they're

1069
00:43:31,200 --> 00:43:34,800
trying completely different things using

1070
00:43:33,440 --> 00:43:37,200
reinforcement learning. They're

1071
00:43:34,800 --> 00:43:40,400
combining symbolic and and subsy

1072
00:43:37,200 --> 00:43:42,319
symbolic AI and putting these kind of

1073
00:43:40,400 --> 00:43:44,079
things together is I think where the

1074
00:43:42,319 --> 00:43:47,359
interesting things will happen or look

1075
00:43:44,079 --> 00:43:50,960
into robotics combinations of LLMs and

1076
00:43:47,359 --> 00:43:54,560
and robots is already very impressive

1077
00:43:50,960 --> 00:43:58,720
but if they can close the feedback loop

1078
00:43:54,560 --> 00:44:01,119
of um you mentioned learning right the

1079
00:43:58,720 --> 00:44:03,520
LLMs don't learn so if they don't learn

1080
00:44:01,119 --> 00:44:05,520
from us so we need to get to systems

1081
00:44:03,520 --> 00:44:08,640
that can actually learn.

1082
00:44:05,520 --> 00:44:10,800
>> Okay, I will accept your quote criticism

1083
00:44:08,640 --> 00:44:12,640
on learning if you define learning for

1084
00:44:10,800 --> 00:44:14,560
me.

1085
00:44:12,640 --> 00:44:17,119
>> Well,

1086
00:44:14,560 --> 00:44:20,400
they need to be able to remember what we

1087
00:44:17,119 --> 00:44:23,040
asked them, what they gave us. And they

1088
00:44:20,400 --> 00:44:25,839
need to figure out if something was

1089
00:44:23,040 --> 00:44:28,400
wrong, how to do it better. So, learning

1090
00:44:25,839 --> 00:44:29,839
means somehow accumulation of knowledge.

1091
00:44:28,400 --> 00:44:31,920
Knowledge meaning it's a good

1092
00:44:29,839 --> 00:44:34,960
explanation for something that is

1093
00:44:31,920 --> 00:44:39,119
difficult to change. So, DPO with with

1094
00:44:34,960 --> 00:44:40,880
memory that goes back for

1095
00:44:39,119 --> 00:44:41,440
oh

1096
00:44:40,880 --> 00:44:43,760
>> yeah,

1097
00:44:41,440 --> 00:44:46,400
>> you know,

1098
00:44:43,760 --> 00:44:47,440
a million characters doesn't qualify as

1099
00:44:46,400 --> 00:44:50,960
learning for you.

1100
00:44:47,440 --> 00:44:53,440
>> No, this is just storing a prompt, but

1101
00:44:50,960 --> 00:44:55,200
it's not qualitatively different.

1102
00:44:53,440 --> 00:44:55,760
>> George Miller would have great fun with

1103
00:44:55,200 --> 00:44:56,800
you.

1104
00:44:55,760 --> 00:44:58,800
>> Yeah, I'm not sure.

1105
00:44:56,800 --> 00:44:59,760
>> Magic number seven plus or minus two.

1106
00:44:58,800 --> 00:45:01,839
Check it out.

1107
00:44:59,760 --> 00:45:02,960
>> Yeah. No, that that's that's not the

1108
00:45:01,839 --> 00:45:05,200
same.

1109
00:45:02,960 --> 00:45:08,160
>> Of course, it's not the same. But then

1110
00:45:05,200 --> 00:45:13,599
again, your definition

1111
00:45:08,160 --> 00:45:16,160
>> is about as rigorfree as a LLM response.

1112
00:45:13,599 --> 00:45:17,599
>> I I I feel trapped in the middle here.

1113
00:45:16,160 --> 00:45:19,200
Can I move to the one of the ends?

1114
00:45:17,599 --> 00:45:21,040
>> Don't feel trapped. You're a buffer.

1115
00:45:19,200 --> 00:45:24,800
You're a buffer.

1116
00:45:21,040 --> 00:45:26,640
Uh I I um

1117
00:45:24,800 --> 00:45:29,680
responding to the original the first

1118
00:45:26,640 --> 00:45:31,040
question that that Stephen asked is uh I

1119
00:45:29,680 --> 00:45:32,800
think things are going to first of all I

1120
00:45:31,040 --> 00:45:35,359
agree with deadlifts. I think that LLMs

1121
00:45:32,800 --> 00:45:37,920
are sort of a sort of dead end and in

1122
00:45:35,359 --> 00:45:40,480
that that they're they're great simulate

1123
00:45:37,920 --> 00:45:42,319
you know touring asked us to simulate

1124
00:45:40,480 --> 00:45:44,000
human beings to solve the AI problem.

1125
00:45:42,319 --> 00:45:45,520
That's what the touring test is right do

1126
00:45:44,000 --> 00:45:47,440
you simulate a human being well enough

1127
00:45:45,520 --> 00:45:49,440
it fools other people then it's AI

1128
00:45:47,440 --> 00:45:51,440
solved. I mean that's solved. Wrong

1129
00:45:49,440 --> 00:45:52,800
question. Solve. Right. Right. Wrong.

1130
00:45:51,440 --> 00:45:54,640
Wrong question. Wrong. That's the point.

1131
00:45:52,800 --> 00:45:55,839
It's the So, but but what are you really

1132
00:45:54,640 --> 00:45:56,640
saying? Touring was wrong.

1133
00:45:55,839 --> 00:45:58,400
>> Yeah. Touring was wrong.

1134
00:45:56,640 --> 00:45:59,200
>> Touring defined intelligence

1135
00:45:58,400 --> 00:45:59,680
incorrectly.

1136
00:45:59,200 --> 00:46:01,200
>> That's right.

1137
00:45:59,680 --> 00:46:02,079
>> Much the way learning was defined

1138
00:46:01,200 --> 00:46:04,720
incorrectly.

1139
00:46:02,079 --> 00:46:05,760
>> Yeah. Oh, god. I was so try hard trying

1140
00:46:04,720 --> 00:46:08,079
to get away, but I did.

1141
00:46:05,760 --> 00:46:09,680
>> Yeah. But you can't. But but but the the

1142
00:46:08,079 --> 00:46:10,800
the thing I was going to say is this is

1143
00:46:09,680 --> 00:46:12,400
I think one of the big challenges is

1144
00:46:10,800 --> 00:46:13,359
this. When you it'd be great if you

1145
00:46:12,400 --> 00:46:15,839
could have the kind of creative

1146
00:46:13,359 --> 00:46:18,160
stimulation Michael's talking about in

1147
00:46:15,839 --> 00:46:20,319
the field of say aerospace design,

1148
00:46:18,160 --> 00:46:22,079
right? the challenge in that and there's

1149
00:46:20,319 --> 00:46:23,520
a really great MIT paper about how

1150
00:46:22,079 --> 00:46:24,960
difficult it is to do engineering

1151
00:46:23,520 --> 00:46:27,040
creativity. It's few years old now but

1152
00:46:24,960 --> 00:46:29,520
it's really quite good about that uh

1153
00:46:27,040 --> 00:46:31,599
that generative AI models are very poor

1154
00:46:29,520 --> 00:46:32,000
at helping you design bicycles. That's

1155
00:46:31,599 --> 00:46:33,520
the

1156
00:46:32,000 --> 00:46:34,000
>> absolutely they understand physics for

1157
00:46:33,520 --> 00:46:35,520
crap.

1158
00:46:34,000 --> 00:46:37,119
>> Yeah, they understand physics for crap.

1159
00:46:35,520 --> 00:46:38,720
>> But here's the problem. If if if you're

1160
00:46:37,119 --> 00:46:42,480
trying to do aerospace engineering

1161
00:46:38,720 --> 00:46:45,040
design, the data sets are really hard to

1162
00:46:42,480 --> 00:46:47,040
get and no one wants to share them,

1163
00:46:45,040 --> 00:46:49,200
right? No one wants to share them at

1164
00:46:47,040 --> 00:46:52,000
all. and they're not going to share them

1165
00:46:49,200 --> 00:46:54,480
to make a big big AI until we really

1166
00:46:52,000 --> 00:46:55,680
help them do it uh in some probably

1167
00:46:54,480 --> 00:46:57,920
funding way.

1168
00:46:55,680 --> 00:46:58,640
>> Masha's smiling over here. I love it,

1169
00:46:57,920 --> 00:47:00,400
you know.

1170
00:46:58,640 --> 00:47:02,640
>> So, so I mean it's a really hard thing

1171
00:47:00,400 --> 00:47:04,800
to I mean like I would love to have an

1172
00:47:02,640 --> 00:47:09,920
LLM equivalent for engineering design in

1173
00:47:04,800 --> 00:47:11,920
the aerospace sector, but hey, BAE uh no

1174
00:47:09,920 --> 00:47:14,640
Boeing and Airbus are not going to share

1175
00:47:11,920 --> 00:47:17,119
data to do that at all. the let's

1176
00:47:14,640 --> 00:47:19,040
remember LLMs the reason LLMs got to

1177
00:47:17,119 --> 00:47:21,200
where they are is a hell of a lot of

1178
00:47:19,040 --> 00:47:23,599
free data that most of us gave away that

1179
00:47:21,200 --> 00:47:26,720
we didn't intend to right that's the

1180
00:47:23,599 --> 00:47:28,160
reason and a lot of which is is people's

1181
00:47:26,720 --> 00:47:30,000
creative output that they feel really

1182
00:47:28,160 --> 00:47:31,920
ticked off now that's being used in a

1183
00:47:30,000 --> 00:47:35,040
way they didn't intend and they should

1184
00:47:31,920 --> 00:47:37,520
uh you know let me tell you uh LLMs can

1185
00:47:35,040 --> 00:47:39,680
quote my book really really well uh

1186
00:47:37,520 --> 00:47:42,640
which which makes me very disturbed you

1187
00:47:39,680 --> 00:47:44,160
know u you know that that's not going to

1188
00:47:42,640 --> 00:47:45,359
continue right that's not going to

1189
00:47:44,160 --> 00:47:46,720
continue and that's going to make the

1190
00:47:45,359 --> 00:47:48,960
progression harder.

1191
00:47:46,720 --> 00:47:50,960
>> I in my sense is that that's one of the

1192
00:47:48,960 --> 00:47:52,160
open questions I think is it's like you

1193
00:47:50,960 --> 00:47:53,359
which direction those going to go in

1194
00:47:52,160 --> 00:47:54,560
because I think there are people working

1195
00:47:53,359 --> 00:47:56,240
in those so so I was going to say we're

1196
00:47:54,560 --> 00:47:57,200
going to make going do a phase shift and

1197
00:47:56,240 --> 00:47:57,760
we're going to move to the audience

1198
00:47:57,200 --> 00:48:00,800
questions.

1199
00:47:57,760 --> 00:48:03,119
>> Sure. Um so okay let's start from the

1200
00:48:00,800 --> 00:48:06,800
top most popular one how do you avoid AI

1201
00:48:03,119 --> 00:48:08,079
LM reducing humans oh reducing humans

1202
00:48:06,800 --> 00:48:10,000
and society intelligence through

1203
00:48:08,079 --> 00:48:11,200
laziness to think you del you want to

1204
00:48:10,000 --> 00:48:13,440
take that one

1205
00:48:11,200 --> 00:48:15,920
>> yeah this is kind of you have to control

1206
00:48:13,440 --> 00:48:19,599
yourself how how you use them right so

1207
00:48:15,920 --> 00:48:23,920
I'm thinking of a place of work so we

1208
00:48:19,599 --> 00:48:27,280
really don't want these models to create

1209
00:48:23,920 --> 00:48:30,240
what's now called work slop right and so

1210
00:48:27,280 --> 00:48:32,559
if you to write a report and instead of

1211
00:48:30,240 --> 00:48:34,640
writing it yourself, you let Chad GPT

1212
00:48:32,559 --> 00:48:36,079
write it for you, then chuck it over the

1213
00:48:34,640 --> 00:48:38,000
fence to the next person who then say,

1214
00:48:36,079 --> 00:48:40,640
"I'm not going to read this. I just ask

1215
00:48:38,000 --> 00:48:42,960
Chpt to give me a summary." What's the

1216
00:48:40,640 --> 00:48:46,720
point of that? Right? So, we we really

1217
00:48:42,960 --> 00:48:48,880
need to think how these models are being

1218
00:48:46,720 --> 00:48:52,160
used to help us create something

1219
00:48:48,880 --> 00:48:54,880
valuable that is worth sharing, not just

1220
00:48:52,160 --> 00:48:56,480
an amount of of slop that we chuck over

1221
00:48:54,880 --> 00:48:58,960
the fence. And this is where we need to

1222
00:48:56,480 --> 00:49:01,599
rein in our own laziness. So we we need

1223
00:48:58,960 --> 00:49:04,000
to if if I have one hour to create a

1224
00:49:01,599 --> 00:49:05,760
report then I shouldn't use CHP to

1225
00:49:04,000 --> 00:49:08,800
create it in five minutes and have a

1226
00:49:05,760 --> 00:49:10,800
coffee. I should work with Chiept PT for

1227
00:49:08,800 --> 00:49:14,000
an hour to create something of more

1228
00:49:10,800 --> 00:49:16,960
value that I could have produced myself.

1229
00:49:14,000 --> 00:49:18,480
But these these tools should not be seen

1230
00:49:16,960 --> 00:49:20,400
>> and you should show your boss the

1231
00:49:18,480 --> 00:49:21,760
prompts you used or your colleagues the

1232
00:49:20,400 --> 00:49:23,280
prompts you used.

1233
00:49:21,760 --> 00:49:24,720
>> It's interesting. There's a paper, I

1234
00:49:23,280 --> 00:49:26,160
think it's out of MIT, but it might be

1235
00:49:24,720 --> 00:49:29,839
Harvard Business Review. Sorry, I can't

1236
00:49:26,160 --> 00:49:31,599
remember which. Uh that about uh work

1237
00:49:29,839 --> 00:49:33,440
slop about productivity.

1238
00:49:31,599 --> 00:49:34,480
>> It was it was Harvard. It was Harvard is

1239
00:49:33,440 --> 00:49:38,160
very good at slop.

1240
00:49:34,480 --> 00:49:40,400
>> Yeah. So, so uh apparently there's an

1241
00:49:38,160 --> 00:49:43,040
unintended productivity loss that's

1242
00:49:40,400 --> 00:49:45,520
going on due to AI where people generate

1243
00:49:43,040 --> 00:49:47,440
AI slop and throw it over the fence and

1244
00:49:45,520 --> 00:49:48,800
the next person has to deal with the

1245
00:49:47,440 --> 00:49:52,400
fact that what they've generated isn't

1246
00:49:48,800 --> 00:49:54,480
very good. Uh I think that there'll be a

1247
00:49:52,400 --> 00:49:55,839
feedback loop there, right?

1248
00:49:54,480 --> 00:49:57,359
Organizations will come up with a

1249
00:49:55,839 --> 00:49:59,520
feedback loop to prevent that from

1250
00:49:57,359 --> 00:50:01,599
happening. And I think it will make

1251
00:49:59,520 --> 00:50:03,680
things better. I do think LLMs are here

1252
00:50:01,599 --> 00:50:06,240
to stay. Uh there will not be an

1253
00:50:03,680 --> 00:50:08,559
application without a chat interface in

1254
00:50:06,240 --> 00:50:10,079
how long? Two years, three years.

1255
00:50:08,559 --> 00:50:10,640
>> I think two years is a little

1256
00:50:10,079 --> 00:50:12,079
pessimistic.

1257
00:50:10,640 --> 00:50:13,280
>> Yes, a little pessimist. Yeah. I mean,

1258
00:50:12,079 --> 00:50:14,720
any app you're thinking about writing

1259
00:50:13,280 --> 00:50:15,839
now, think about how you put a chat

1260
00:50:14,720 --> 00:50:17,680
interface on it because that's where

1261
00:50:15,839 --> 00:50:19,760
it's going, right? is a search I

1262
00:50:17,680 --> 00:50:21,280
remember when search was invented and uh

1263
00:50:19,760 --> 00:50:22,800
there's a search interface in every

1264
00:50:21,280 --> 00:50:24,319
single app you have now there'll be a

1265
00:50:22,800 --> 00:50:26,880
chat interface in every single app you

1266
00:50:24,319 --> 00:50:29,599
have now I mean they're not going away

1267
00:50:26,880 --> 00:50:31,680
they are not the magic bullet and but

1268
00:50:29,599 --> 00:50:34,240
how you use them cleverly to do really

1269
00:50:31,680 --> 00:50:35,760
interesting things is the future so this

1270
00:50:34,240 --> 00:50:36,960
thing about laziness that's was our

1271
00:50:35,760 --> 00:50:40,640
question was as I want to address the

1272
00:50:36,960 --> 00:50:43,119
question is um feedback that's the

1273
00:50:40,640 --> 00:50:45,040
answer is effectively if you're lazy if

1274
00:50:43,119 --> 00:50:46,880
you're a lazy user of LLMs you're going

1275
00:50:45,040 --> 00:50:48,640
to get negative feedback back from

1276
00:50:46,880 --> 00:50:50,640
organizations and society.

1277
00:50:48,640 --> 00:50:53,359
>> I was a school boy in England for a

1278
00:50:50,640 --> 00:50:55,920
couple of years. I think what we're

1279
00:50:53,359 --> 00:50:58,000
going to see is a resumption of what

1280
00:50:55,920 --> 00:51:00,160
worked when I was a school boy. Corporal

1281
00:50:58,000 --> 00:51:03,280
punishment. You will be beaten if you

1282
00:51:00,160 --> 00:51:05,040
were that lazy

1283
00:51:03,280 --> 00:51:07,839
>> because that's a different mode. It's a

1284
00:51:05,040 --> 00:51:09,920
different mode. Just mere criticism,

1285
00:51:07,839 --> 00:51:11,680
verbal criticism won't work.

1286
00:51:09,920 --> 00:51:13,359
>> Yeah. So, in the tradition of the uh

1287
00:51:11,680 --> 00:51:16,319
what else can go ary? There's some

1288
00:51:13,359 --> 00:51:18,640
really great questions on here. So um

1289
00:51:16,319 --> 00:51:21,280
there was one where I lose it. Oh yeah.

1290
00:51:18,640 --> 00:51:22,960
Um can you bring up the uh the one from

1291
00:51:21,280 --> 00:51:24,960
uh what's the biggest issue with GPTs?

1292
00:51:22,960 --> 00:51:27,440
I'm personas. Okay. Given the biggest

1293
00:51:24,960 --> 00:51:29,200
use of GPT is to create personas uh for

1294
00:51:27,440 --> 00:51:31,440
the user to build emotional bonds like a

1295
00:51:29,200 --> 00:51:32,960
girlfriend, therapist, etc. What does

1296
00:51:31,440 --> 00:51:35,040
the future look like for society's

1297
00:51:32,960 --> 00:51:36,800
interaction? And I think I'll just toss

1298
00:51:35,040 --> 00:51:37,520
that into the melee. So Rob, do you want

1299
00:51:36,800 --> 00:51:38,960
you want to start with

1300
00:51:37,520 --> 00:51:40,640
>> that's that's a depressing one. And I I

1301
00:51:38,960 --> 00:51:43,359
think what's going on with uh AI

1302
00:51:40,640 --> 00:51:46,720
girlfriends uh AI pornography, let's

1303
00:51:43,359 --> 00:51:50,160
face it, is is profoundly disturbing and

1304
00:51:46,720 --> 00:51:52,240
and oddly hard to argue against,

1305
00:51:50,160 --> 00:51:56,240
right? Because because it's it's

1306
00:51:52,240 --> 00:51:58,160
reducing exploitation. Ah, it is. I mean

1307
00:51:56,240 --> 00:52:01,599
there are going to be that the job of

1308
00:51:58,160 --> 00:52:04,640
being a horribly exploited um chat

1309
00:52:01,599 --> 00:52:06,880
worker, sex chat worker is becoming le

1310
00:52:04,640 --> 00:52:08,240
harder to get and hard more less

1311
00:52:06,880 --> 00:52:10,720
interesting to those people who want to

1312
00:52:08,240 --> 00:52:14,720
exploit those people and that's a good

1313
00:52:10,720 --> 00:52:18,240
thing. But what porn bots are doing to

1314
00:52:14,720 --> 00:52:20,559
young men is profoundly disturbing,

1315
00:52:18,240 --> 00:52:23,760
right? What do we do about this problem?

1316
00:52:20,559 --> 00:52:25,760
I don't know. My silver lining sense is

1317
00:52:23,760 --> 00:52:27,440
to say the fact the problem exists will

1318
00:52:25,760 --> 00:52:28,640
prompt something positive to happen.

1319
00:52:27,440 --> 00:52:32,400
>> Don't say prompt.

1320
00:52:28,640 --> 00:52:33,839
>> Yeah, sorry. But I I I yeah, I worry

1321
00:52:32,400 --> 00:52:35,200
about this profoundly. And and with

1322
00:52:33,839 --> 00:52:38,559
regard to coaching and therapy and and

1323
00:52:35,200 --> 00:52:41,040
and being a mentor, um I guarantee the

1324
00:52:38,559 --> 00:52:44,079
people in this audience who have um who

1325
00:52:41,040 --> 00:52:46,319
have kids do not want AI teaching their

1326
00:52:44,079 --> 00:52:48,240
kids and taking the part of teachers.

1327
00:52:46,319 --> 00:52:51,119
And the reason is there's a lot more to

1328
00:52:48,240 --> 00:52:54,640
a teacher or a GP or a lawyer or a

1329
00:52:51,119 --> 00:52:56,559
therapist or a coach than the simula

1330
00:52:54,640 --> 00:52:58,880
than simulated conversation than

1331
00:52:56,559 --> 00:53:01,760
knowledge of of of background material.

1332
00:52:58,880 --> 00:53:04,400
It's a human bond. And what I really

1333
00:53:01,760 --> 00:53:07,280
worry about profoundly is the rich

1334
00:53:04,400 --> 00:53:10,079
having doctors and therapists and coach

1335
00:53:07,280 --> 00:53:11,119
coaches and mentors and the poor having

1336
00:53:10,079 --> 00:53:12,800
AI.

1337
00:53:11,119 --> 00:53:15,280
>> That's what I really worry about. And

1338
00:53:12,800 --> 00:53:17,040
you know that's coming.

1339
00:53:15,280 --> 00:53:19,359
>> Yeah.

1340
00:53:17,040 --> 00:53:22,720
I like this HG Wells fellow, you know,

1341
00:53:19,359 --> 00:53:24,559
Morlocks either way. Yeah.

1342
00:53:22,720 --> 00:53:26,960
So, okay. So, look, so looking through

1343
00:53:24,559 --> 00:53:28,400
we got some great questions. So, um what

1344
00:53:26,960 --> 00:53:30,400
was the other one? The one I really

1345
00:53:28,400 --> 00:53:33,119
liked was Oh, yeah. Whoever Mr.

1346
00:53:30,400 --> 00:53:35,200
Anonymous is or Miss an Anonymous, if

1347
00:53:33,119 --> 00:53:37,520
Turing asked the wrong question, what's

1348
00:53:35,200 --> 00:53:39,520
the right question?

1349
00:53:37,520 --> 00:53:41,200
>> If touring was asking the I think Toring

1350
00:53:39,520 --> 00:53:44,079
was asked question, what defines

1351
00:53:41,200 --> 00:53:45,680
thought, computer thought? And I think

1352
00:53:44,079 --> 00:53:46,880
you know that the title of the paper is

1353
00:53:45,680 --> 00:53:48,880
computers and thought or something like

1354
00:53:46,880 --> 00:53:50,559
that in the original Turing test paper.

1355
00:53:48,880 --> 00:53:52,640
>> Uh what does it mean to think? That's

1356
00:53:50,559 --> 00:53:54,240
what he was asking. I think the right

1357
00:53:52,640 --> 00:53:56,160
question is what does it mean to be

1358
00:53:54,240 --> 00:53:57,839
human? That's the right question. That's

1359
00:53:56,160 --> 00:54:00,480
that's my positive vision at the end of

1360
00:53:57,839 --> 00:54:02,240
my book is that we'll start stopping

1361
00:54:00,480 --> 00:54:04,880
defining the magic characteristic of

1362
00:54:02,240 --> 00:54:06,960
human humanity being intelligence in

1363
00:54:04,880 --> 00:54:08,800
quotes and start it being more of a

1364
00:54:06,960 --> 00:54:12,000
whole human experience. I mean that's a

1365
00:54:08,800 --> 00:54:14,240
positive thing. Well, I I forgive me. I

1366
00:54:12,000 --> 00:54:16,400
have a different interpretation of of

1367
00:54:14,240 --> 00:54:18,559
Turing's work in that regard. I think

1368
00:54:16,400 --> 00:54:21,920
what Touring was looking at as as a

1369
00:54:18,559 --> 00:54:24,559
mathematician, as a logician, and as to

1370
00:54:21,920 --> 00:54:28,000
a certain extent a philosopher was to

1371
00:54:24,559 --> 00:54:31,359
what extent computability enables

1372
00:54:28,000 --> 00:54:35,359
thinking? Can machines I is there a

1373
00:54:31,359 --> 00:54:38,319
certain you know does uh

1374
00:54:35,359 --> 00:54:40,480
the Lenin line or Stalin line about

1375
00:54:38,319 --> 00:54:43,200
quality uh quantity has a quality all of

1376
00:54:40,480 --> 00:54:44,880
its own does millions of are there

1377
00:54:43,200 --> 00:54:47,680
emergent properties I view as the

1378
00:54:44,880 --> 00:54:49,280
ability to compute as compute improves

1379
00:54:47,680 --> 00:54:50,720
does thinking emerge

1380
00:54:49,280 --> 00:54:52,400
>> I think you're asking that question but

1381
00:54:50,720 --> 00:54:52,960
I don't think in the paper touring asked

1382
00:54:52,400 --> 00:54:54,960
that question

1383
00:54:52,960 --> 00:54:56,400
>> but but that is in his that is in his

1384
00:54:54,960 --> 00:54:59,839
corpus of of

1385
00:54:56,400 --> 00:55:02,160
>> is it work yeah you Let's ask Chachi PT

1386
00:54:59,839 --> 00:55:05,200
Google. I have a quote from him on my

1387
00:55:02,160 --> 00:55:07,599
phone that I prepared uh for this. I

1388
00:55:05,200 --> 00:55:09,520
just had it. I'll try to read this. I'll

1389
00:55:07,599 --> 00:55:11,040
read the ending quickly. He's basically

1390
00:55:09,520 --> 00:55:14,000
talking about could you educate

1391
00:55:11,040 --> 00:55:15,680
machines. He says, "But here we have to

1392
00:55:14,000 --> 00:55:17,200
be careful. It would be quite easy to

1393
00:55:15,680 --> 00:55:18,559
arrange the experiences in such a way

1394
00:55:17,200 --> 00:55:20,000
that they automatically cause the

1395
00:55:18,559 --> 00:55:22,000
structure of the machine to build up

1396
00:55:20,000 --> 00:55:24,400
into a previously intended form. And

1397
00:55:22,000 --> 00:55:26,559
this would be would obviously be a gross

1398
00:55:24,400 --> 00:55:29,119
form of cheating almost on a par with

1399
00:55:26,559 --> 00:55:32,559
having a man inside the machine. So

1400
00:55:29,119 --> 00:55:34,960
massive data uh to organize the machine.

1401
00:55:32,559 --> 00:55:36,960
Touring thought that was cheating.

1402
00:55:34,960 --> 00:55:39,520
>> Yeah. Well, with all due respect, that's

1403
00:55:36,960 --> 00:55:41,599
warmed over Dart, the ghost and the

1404
00:55:39,520 --> 00:55:44,240
machine, you know, right? The spiritual.

1405
00:55:41,599 --> 00:55:45,839
We're back to Daniel Dennett. See,

1406
00:55:44,240 --> 00:55:48,640
here's where we run into interesting

1407
00:55:45,839 --> 00:55:50,480
ontological issues. Cheating meant

1408
00:55:48,640 --> 00:55:54,000
consciousness. To what extent is

1409
00:55:50,480 --> 00:55:56,720
self-awareness a sign or a signal? Do

1410
00:55:54,000 --> 00:55:58,880
animals think?

1411
00:55:56,720 --> 00:56:01,280
That's still a subject of debate on

1412
00:55:58,880 --> 00:56:04,400
this. Or well, they think, but not in

1413
00:56:01,280 --> 00:56:06,480
the way that humans do. Well, LLMs

1414
00:56:04,400 --> 00:56:09,359
think, but not in the way that humans

1415
00:56:06,480 --> 00:56:12,160
do. I will absolutely concede, freely

1416
00:56:09,359 --> 00:56:15,280
concede, that that LLM, LPMS don't

1417
00:56:12,160 --> 00:56:18,720
understand anything. But their ability

1418
00:56:15,280 --> 00:56:21,200
to generate useful patterns, interpret

1419
00:56:18,720 --> 00:56:26,480
and generate useful patterns makes me

1420
00:56:21,200 --> 00:56:29,280
wonder about reasons and logic's role in

1421
00:56:26,480 --> 00:56:31,520
thinking in computation. So with that,

1422
00:56:29,280 --> 00:56:33,359
okay, that's good. Beforewand comes with

1423
00:56:31,520 --> 00:56:35,040
the the cattle prod on stage to get me

1424
00:56:33,359 --> 00:56:37,280
off. Um I was just going to finish off

1425
00:56:35,040 --> 00:56:41,280
with just like lightning round of last

1426
00:56:37,280 --> 00:56:44,079
comments. Um so trying to kick off and

1427
00:56:41,280 --> 00:56:46,559
just final final thoughts. Yeah, I I

1428
00:56:44,079 --> 00:56:49,520
think as a final thought, especially in

1429
00:56:46,559 --> 00:56:50,880
in the world of work, we have a new tool

1430
00:56:49,520 --> 00:56:52,799
and we need to learn how to use it

1431
00:56:50,880 --> 00:56:55,440
properly. I think this is where we are

1432
00:56:52,799 --> 00:57:00,880
at the moment. Yeah. I I my last comment

1433
00:56:55,440 --> 00:57:02,960
will be uh soul sentience uh uh you know

1434
00:57:00,880 --> 00:57:04,480
consciousness all of these concepts

1435
00:57:02,960 --> 00:57:06,319
could be supplemented with the idea of

1436
00:57:04,480 --> 00:57:08,160
intelligence. They're all these magic

1437
00:57:06,319 --> 00:57:10,079
bullets that we talk about. Magic

1438
00:57:08,160 --> 00:57:12,720
bullets that are like what's the special

1439
00:57:10,079 --> 00:57:15,119
quality of being human? Animals don't

1440
00:57:12,720 --> 00:57:18,079
think but we do. And so thinking is the

1441
00:57:15,119 --> 00:57:20,799
magic quantity. All that's what I'm

1442
00:57:18,079 --> 00:57:23,280
saying is all that's wrong. Humanity is

1443
00:57:20,799 --> 00:57:25,359
itself a holistic phenomena that is the

1444
00:57:23,280 --> 00:57:27,280
magic quantity and the redu all the

1445
00:57:25,359 --> 00:57:30,160
other reductions of that to something

1446
00:57:27,280 --> 00:57:33,440
that's an externalizable magic quality

1447
00:57:30,160 --> 00:57:36,720
are a philosophical misstep.

1448
00:57:33,440 --> 00:57:39,440
That's my last statement. I will I will

1449
00:57:36,720 --> 00:57:44,720
say that I should not completely end on

1450
00:57:39,440 --> 00:57:46,880
a contrarian note. I I believe that that

1451
00:57:44,720 --> 00:57:48,960
the rise of these not tools,

1452
00:57:46,880 --> 00:57:53,079
capabilities

1453
00:57:48,960 --> 00:57:53,079
forces people to

1454
00:57:53,520 --> 00:57:59,680
invest in themselves and reflection and

1455
00:57:56,720 --> 00:58:03,280
self-awareness in a different way.

1456
00:57:59,680 --> 00:58:06,079
Warren Buffett and Johnny IV are correct

1457
00:58:03,280 --> 00:58:09,520
in my opinion in that regard. who we are

1458
00:58:06,079 --> 00:58:12,880
as people, our humanness is going to be

1459
00:58:09,520 --> 00:58:16,240
better shaped and yes, better understood

1460
00:58:12,880 --> 00:58:19,760
by how we engage with, interact with and

1461
00:58:16,240 --> 00:58:21,760
get value from these capabilities.

1462
00:58:19,760 --> 00:58:24,000
>> So with that, I say thank you to our

1463
00:58:21,760 --> 00:58:25,680
panelists for for the live list after

1464
00:58:24,000 --> 00:58:28,160
lunch session. I think I've had I had

1465
00:58:25,680 --> 00:58:30,160
fun.

1466
00:58:28,160 --> 00:58:32,799
>> That was really good.

1467
00:58:30,160 --> 00:58:36,400
>> That was really good. Thank you. Thank

1468
00:58:32,799 --> 00:58:36,400
you. Thanks a lot.

