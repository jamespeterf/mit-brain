1
00:00:01,360 --> 00:00:05,279
All

2
00:00:02,120 --> 00:00:07,279
right. Um, so that's great. Um, thank

3
00:00:05,279 --> 00:00:09,920
you for the introduction. Um, we're so

4
00:00:07,279 --> 00:00:12,880
pleased to be here. Um, the CIRC, uh,

5
00:00:09,920 --> 00:00:15,920
funding enabled us to fund a really

6
00:00:12,880 --> 00:00:18,480
expansive and exciting and amazing kind

7
00:00:15,920 --> 00:00:20,160
of rolling public conversation. Um,

8
00:00:18,480 --> 00:00:23,279
we're characterizing it as a rolling

9
00:00:20,160 --> 00:00:26,320
public think tank about all aspects of

10
00:00:23,279 --> 00:00:29,439
AI. Um, so, uh, we're really

11
00:00:26,320 --> 00:00:32,239
appreciative of this funding.

12
00:00:29,439 --> 00:00:35,280
Um, I'm gonna get us started. Um, I want

13
00:00:32,239 --> 00:00:40,000
to ask how many of you raise your hand

14
00:00:35,280 --> 00:00:40,000
if you were consulted about chat

15
00:00:40,600 --> 00:00:46,960
GPT. Yeah. Yeah. We were not consulted.

16
00:00:43,920 --> 00:00:49,680
Nobody was consulted. Um, and this is

17
00:00:46,960 --> 00:00:51,600
actually part of the problem, right? Um

18
00:00:49,680 --> 00:00:54,000
and so this is part of the motivation

19
00:00:51,600 --> 00:00:55,520
for um our original proposal which as

20
00:00:54,000 --> 00:00:58,399
you'll see we actually have diverged

21
00:00:55,520 --> 00:01:01,520
significantly but we remain focused on

22
00:00:58,399 --> 00:01:04,400
these questions of participation and

23
00:01:01,520 --> 00:01:07,119
voice and who gets a say in which

24
00:01:04,400 --> 00:01:09,600
product which AI products are built how

25
00:01:07,119 --> 00:01:12,960
they're built um who they benefit and

26
00:01:09,600 --> 00:01:14,880
and who they harm um and so you know we

27
00:01:12,960 --> 00:01:17,439
came to this work I'm from the data plus

28
00:01:14,880 --> 00:01:19,600
feminism lab so there's a lab in

29
00:01:17,439 --> 00:01:22,799
department of urban studies and planning

30
00:01:19,600 --> 00:01:25,360
Um and what we do is uh we are

31
00:01:22,799 --> 00:01:27,600
interested in the intersection of on the

32
00:01:25,360 --> 00:01:30,159
one hand racial justice and gender

33
00:01:27,600 --> 00:01:33,759
justice but we think about how do we use

34
00:01:30,159 --> 00:01:36,880
data computation and AI and the service

35
00:01:33,759 --> 00:01:38,400
of those goals. Um and in the past we

36
00:01:36,880 --> 00:01:40,920
have actually done a lot of work in

37
00:01:38,400 --> 00:01:44,159
participatory AI. So in fact we have

38
00:01:40,920 --> 00:01:46,479
co-designed AI systems uh with uh

39
00:01:44,159 --> 00:01:47,920
citizen data science citizen data

40
00:01:46,479 --> 00:01:50,079
scientists who are working on

41
00:01:47,920 --> 00:01:52,479
gender-based violence particularly in

42
00:01:50,079 --> 00:01:55,200
Latin America. Um I have a book about

43
00:01:52,479 --> 00:01:57,200
this if you're interested. Um and it's

44
00:01:55,200 --> 00:01:59,920
been an amazing learning process over

45
00:01:57,200 --> 00:02:03,119
the past five years to be co-developing

46
00:01:59,920 --> 00:02:05,360
AI tools um aligned with the visions of

47
00:02:03,119 --> 00:02:08,239
feminist activists working to really

48
00:02:05,360 --> 00:02:10,319
transform their worlds. But these kind

49
00:02:08,239 --> 00:02:12,480
of participatory methods are not

50
00:02:10,319 --> 00:02:14,480
mainstream um and certainly not

51
00:02:12,480 --> 00:02:16,560
mainstream in AI development. So what we

52
00:02:14,480 --> 00:02:18,879
wanted to do is to think about how could

53
00:02:16,560 --> 00:02:20,640
we create a a framework for this this

54
00:02:18,879 --> 00:02:22,400
sort of mainstreamy work. And so that's

55
00:02:20,640 --> 00:02:24,160
why the original title is a framework

56
00:02:22,400 --> 00:02:26,760
for participatory methods and community

57
00:02:24,160 --> 00:02:29,200
engagement across the AI ML

58
00:02:26,760 --> 00:02:30,480
pipeline. Um and so these were original

59
00:02:29,200 --> 00:02:33,040
research questions. This is what we

60
00:02:30,480 --> 00:02:34,560
submitted in our proposal. Um, how can

61
00:02:33,040 --> 00:02:36,160
AI and machine learning projects

62
00:02:34,560 --> 00:02:38,120
integrate methods from participatory

63
00:02:36,160 --> 00:02:41,760
action research, from participatory

64
00:02:38,120 --> 00:02:44,000
design? How can we um use AI development

65
00:02:41,760 --> 00:02:46,879
and this participatory process to uplift

66
00:02:44,000 --> 00:02:49,680
rather than extract community knowledge?

67
00:02:46,879 --> 00:02:51,920
Um, what are the epistemic and ethical

68
00:02:49,680 --> 00:02:54,000
obligations of AI and machine learning

69
00:02:51,920 --> 00:02:55,840
projects that undertake participation

70
00:02:54,000 --> 00:02:58,480
and civic engagement in this development

71
00:02:55,840 --> 00:03:00,239
process? Um but I think one of the

72
00:02:58,480 --> 00:03:03,120
things I want to ask you to note about

73
00:03:00,239 --> 00:03:06,480
these research questions is that their

74
00:03:03,120 --> 00:03:09,120
scale of analysis is already inside a

75
00:03:06,480 --> 00:03:12,480
pipeline of technical development which

76
00:03:09,120 --> 00:03:14,319
is to say outside of these questions. Um

77
00:03:12,480 --> 00:03:15,640
there has already been a decision to

78
00:03:14,319 --> 00:03:18,720
develop a

79
00:03:15,640 --> 00:03:20,560
technology. Um there have already been

80
00:03:18,720 --> 00:03:21,920
questions around you know and the

81
00:03:20,560 --> 00:03:23,599
questions now are more about like how do

82
00:03:21,920 --> 00:03:25,440
we integrate communities into this thing

83
00:03:23,599 --> 00:03:26,480
that like we are already building. we've

84
00:03:25,440 --> 00:03:28,560
already decided this thing will be

85
00:03:26,480 --> 00:03:30,480
useful in the world. Um, and I ask you

86
00:03:28,560 --> 00:03:32,720
to note that because this is exactly

87
00:03:30,480 --> 00:03:34,720
where we ran into um issues and where we

88
00:03:32,720 --> 00:03:37,200
really wanted to pivot because we we

89
00:03:34,720 --> 00:03:38,959
realized if we wanted to actually build

90
00:03:37,200 --> 00:03:41,519
AI and machine learning that modeled

91
00:03:38,959 --> 00:03:43,120
pluralistic methods and supported

92
00:03:41,519 --> 00:03:45,040
integrating community needs and

93
00:03:43,120 --> 00:03:48,400
community knowledge, we actually needed

94
00:03:45,040 --> 00:03:50,000
to expand the scale of our inquiry. Um,

95
00:03:48,400 --> 00:03:51,920
and we needed to do more systems

96
00:03:50,000 --> 00:03:53,440
thinking. We need to think like outside

97
00:03:51,920 --> 00:03:55,840
of just the pipeline of technical

98
00:03:53,440 --> 00:03:57,760
development. Um but first Nico is going

99
00:03:55,840 --> 00:03:59,439
to tell you about our our first foray

100
00:03:57,760 --> 00:04:01,519
into this literature review of

101
00:03:59,439 --> 00:04:03,599
participatory methods in AI development

102
00:04:01,519 --> 00:04:08,080
which are increasingly are being talked

103
00:04:03,599 --> 00:04:09,680
about. So they are um hi everyone. Um so

104
00:04:08,080 --> 00:04:11,040
very quickly you because we're in urban

105
00:04:09,680 --> 00:04:12,879
planning we started with the classic

106
00:04:11,040 --> 00:04:15,040
Arnstein's latter of participation

107
00:04:12,879 --> 00:04:18,799
thinking of how people can get involved

108
00:04:15,040 --> 00:04:20,720
in uh in the a IML pipeline. Um and we

109
00:04:18,799 --> 00:04:22,479
came across what I've come to call sort

110
00:04:20,720 --> 00:04:24,800
of geometries of participation. There's

111
00:04:22,479 --> 00:04:27,280
the ladder, there's the cube, there are

112
00:04:24,800 --> 00:04:30,320
scales, there are all these um ways to

113
00:04:27,280 --> 00:04:32,400
shuttle people from ideiation through

114
00:04:30,320 --> 00:04:35,199
deployment or maintenance um still

115
00:04:32,400 --> 00:04:37,360
within this AI system

116
00:04:35,199 --> 00:04:39,199
um and and ways to measure right their

117
00:04:37,360 --> 00:04:41,280
participation in the system and and even

118
00:04:39,199 --> 00:04:42,960
their own level of satisfaction. Did

119
00:04:41,280 --> 00:04:44,400
they feel agential? Did they feel like

120
00:04:42,960 --> 00:04:45,960
they were respected and consented

121
00:04:44,400 --> 00:04:48,320
through the process?

122
00:04:45,960 --> 00:04:50,880
Um, but I think like Katherine was

123
00:04:48,320 --> 00:04:52,960
saying, uh, we're still working within

124
00:04:50,880 --> 00:04:55,199
with inside of the technical pipeline.

125
00:04:52,960 --> 00:04:56,880
And my own training comes as a software

126
00:04:55,199 --> 00:04:58,880
engineer. I worked in industry for many

127
00:04:56,880 --> 00:05:00,560
years. Um, and my training was to be a

128
00:04:58,880 --> 00:05:02,400
systems thinker and when there are

129
00:05:00,560 --> 00:05:03,919
problems in the pipeline to kind of step

130
00:05:02,400 --> 00:05:06,160
back and step back and step back and

131
00:05:03,919 --> 00:05:08,479
step back until I could see a big enough

132
00:05:06,160 --> 00:05:11,199
picture to understand what the problems

133
00:05:08,479 --> 00:05:13,759
were. And so to me coming from my own

134
00:05:11,199 --> 00:05:16,720
background um even in industry thinking

135
00:05:13,759 --> 00:05:19,280
about well can I um you know add

136
00:05:16,720 --> 00:05:21,919
participants to make more ethical just

137
00:05:19,280 --> 00:05:24,320
felt insufficient. Um and so we started

138
00:05:21,919 --> 00:05:26,320
to ask a different question which is

139
00:05:24,320 --> 00:05:28,080
under what conditions are people and

140
00:05:26,320 --> 00:05:30,320
communities able to meaningfully

141
00:05:28,080 --> 00:05:32,840
participate in the creation evaluation

142
00:05:30,320 --> 00:05:36,560
and governance of

143
00:05:32,840 --> 00:05:38,479
AI. And I think uh like Katherine said

144
00:05:36,560 --> 00:05:40,639
this takes the focus uh no none of us

145
00:05:38,479 --> 00:05:42,320
got asked if chat GPT should be built

146
00:05:40,639 --> 00:05:43,919
and so creation is also a really

147
00:05:42,320 --> 00:05:45,600
important part of this question right

148
00:05:43,919 --> 00:05:48,960
are we building AI that you want should

149
00:05:45,600 --> 00:05:50,560
we build this at all um so we we started

150
00:05:48,960 --> 00:05:52,960
we didn't intend to build a think tank

151
00:05:50,560 --> 00:05:55,199
but we did and we gathered researchers

152
00:05:52,960 --> 00:05:58,240
from um all of these institutions and

153
00:05:55,199 --> 00:06:00,479
more and I think most importantly we

154
00:05:58,240 --> 00:06:02,720
gathered 25 of us from all of these

155
00:06:00,479 --> 00:06:05,039
disciplines so we have early career

156
00:06:02,720 --> 00:06:06,800
graduate students U masters, first to

157
00:06:05,039 --> 00:06:08,800
second year PhD students all the way to

158
00:06:06,800 --> 00:06:10,800
full professors in all of these

159
00:06:08,800 --> 00:06:12,400
disciplines. And we're all having this

160
00:06:10,800 --> 00:06:13,919
conversation together. And some of them

161
00:06:12,400 --> 00:06:15,720
are practitioners. We have lawyers, we

162
00:06:13,919 --> 00:06:18,560
have studio artists.

163
00:06:15,720 --> 00:06:21,360
Um, and so we're thinking of the project

164
00:06:18,560 --> 00:06:24,560
as liberatory AI. And you'll notice in

165
00:06:21,360 --> 00:06:26,160
this animation, AI continues to change

166
00:06:24,560 --> 00:06:28,319
uh across our disciplines. We couldn't

167
00:06:26,160 --> 00:06:30,000
even really agree on a working

168
00:06:28,319 --> 00:06:31,759
definition of AI. And that felt

169
00:06:30,000 --> 00:06:33,919
important and generative. And so we

170
00:06:31,759 --> 00:06:36,319
didn't. Um, we're thinking about all of

171
00:06:33,919 --> 00:06:38,960
the things that AI can stand for and

172
00:06:36,319 --> 00:06:41,680
we're talking um in three three ways

173
00:06:38,960 --> 00:06:44,560
about corporate AI um about the dead

174
00:06:41,680 --> 00:06:46,960
ends uh within a corporate AI ecosystem

175
00:06:44,560 --> 00:06:49,319
and about ways forward both through and

176
00:06:46,960 --> 00:06:53,120
out of corporate

177
00:06:49,319 --> 00:06:54,919
AI. Um so uh the design by one of our

178
00:06:53,120 --> 00:06:57,840
master students Alicia

179
00:06:54,919 --> 00:06:59,919
Delgado and we um collectively and

180
00:06:57,840 --> 00:07:01,520
individually wrote about 20 position

181
00:06:59,919 --> 00:07:03,280
papers. These are short, they're

182
00:07:01,520 --> 00:07:04,960
accessible, but they engage with all of

183
00:07:03,280 --> 00:07:07,199
the most current academic literature.

184
00:07:04,960 --> 00:07:11,199
They're up on a website that is live

185
00:07:07,199 --> 00:07:12,720
now. Um, and so we're thinking um about

186
00:07:11,199 --> 00:07:14,080
ways for both practitioners and

187
00:07:12,720 --> 00:07:16,080
community members and people across

188
00:07:14,080 --> 00:07:18,160
disciplines to really be thinking about

189
00:07:16,080 --> 00:07:20,639
the systems under which we can

190
00:07:18,160 --> 00:07:21,759
meaningfully participate. Um, and also I

191
00:07:20,639 --> 00:07:23,440
want to mention not all the pieces

192
00:07:21,759 --> 00:07:26,160
really even agree with each other and we

193
00:07:23,440 --> 00:07:28,479
find that to be really important.

194
00:07:26,160 --> 00:07:30,160
Um, so really quickly, uh, thinking

195
00:07:28,479 --> 00:07:32,840
about the corporate AI landscape, this

196
00:07:30,160 --> 00:07:35,360
this landscape in which we've got

197
00:07:32,840 --> 00:07:37,599
billion-dollar global tech oligarchs

198
00:07:35,360 --> 00:07:39,840
setting the terms of our engagement. Um,

199
00:07:37,599 --> 00:07:42,160
Issa Krushian at Queen Mary wrote about

200
00:07:39,840 --> 00:07:43,599
how corporate AI can provoke threats to

201
00:07:42,160 --> 00:07:46,000
democracy. I think we're seeing some of

202
00:07:43,599 --> 00:07:49,280
that in current events right now. Um

203
00:07:46,000 --> 00:07:51,599
Amelia and Hungjin wrote about AI being

204
00:07:49,280 --> 00:07:55,840
placeless and they give these examples

205
00:07:51,599 --> 00:07:57,840
of these um you know systems trained to

206
00:07:55,840 --> 00:08:00,160
identify plant life from across the

207
00:07:57,840 --> 00:08:02,960
globe, right? The AI sits somewhere, the

208
00:08:00,160 --> 00:08:05,039
object of study sits somewhere else or

209
00:08:02,960 --> 00:08:07,199
generative AI being asked to generate

210
00:08:05,039 --> 00:08:09,680
images of cities but all the cities kind

211
00:08:07,199 --> 00:08:11,120
of look the same even though most of us

212
00:08:09,680 --> 00:08:13,440
can understand how cities look different

213
00:08:11,120 --> 00:08:15,759
from each other. uh UA who's a CS

214
00:08:13,440 --> 00:08:18,160
student at Brown uh talks about how in

215
00:08:15,759 --> 00:08:21,120
corporate AI a corporation might market

216
00:08:18,160 --> 00:08:22,879
an AI system as being very performant um

217
00:08:21,120 --> 00:08:24,960
increasing the performance but the AI

218
00:08:22,879 --> 00:08:26,800
system sets those benchmarks they're not

219
00:08:24,960 --> 00:08:28,360
set by the communities in which the AI

220
00:08:26,800 --> 00:08:30,879
is used or

221
00:08:28,360 --> 00:08:33,279
tested. Um, we're thinking also about

222
00:08:30,879 --> 00:08:36,159
dead ends, about all of these ways that

223
00:08:33,279 --> 00:08:38,000
corporate AI, um, not to make it sound

224
00:08:36,159 --> 00:08:40,560
too menacing, but sort of tricks us into

225
00:08:38,000 --> 00:08:42,080
thinking that we can engage in ways that

226
00:08:40,560 --> 00:08:44,720
might reduce harm or might be more

227
00:08:42,080 --> 00:08:46,760
ethical. Um, and so we have we're

228
00:08:44,720 --> 00:08:49,200
working on projects to think about

229
00:08:46,760 --> 00:08:50,880
toolkitification. Richmond Wong has this

230
00:08:49,200 --> 00:08:53,360
wonderful idea about toolkitification

231
00:08:50,880 --> 00:08:56,160
being this seductive way to think maybe

232
00:08:53,360 --> 00:08:58,320
we're making a difference. um thinking

233
00:08:56,160 --> 00:09:00,000
about the responsibility gap at AI uh

234
00:08:58,320 --> 00:09:02,000
systems claiming to take responsibility

235
00:09:00,000 --> 00:09:04,320
for the harms that they cause but maybe

236
00:09:02,000 --> 00:09:06,800
not actually doing so in practice. Um

237
00:09:04,320 --> 00:09:08,080
and always thinking about AI audits and

238
00:09:06,800 --> 00:09:11,440
whether those are the most effective

239
00:09:08,080 --> 00:09:13,839
ways to engage with systems. Um I want

240
00:09:11,440 --> 00:09:15,680
to spend the most time on ways forward

241
00:09:13,839 --> 00:09:18,880
uh because I think it feels hopeful and

242
00:09:15,680 --> 00:09:20,880
optimistic uh amidst a lot of critique.

243
00:09:18,880 --> 00:09:22,880
Um and our ways forward are not these

244
00:09:20,880 --> 00:09:24,880
like onetoone solutions. We're not

245
00:09:22,880 --> 00:09:27,200
saying this over here is a problem and

246
00:09:24,880 --> 00:09:29,839
this over here is a solution. But rather

247
00:09:27,200 --> 00:09:31,920
we can reject these terms entirely and

248
00:09:29,839 --> 00:09:34,160
simply think about ways forward. And

249
00:09:31,920 --> 00:09:37,279
those ways forward might include small

250
00:09:34,160 --> 00:09:39,279
and open-source models, laptop scale,

251
00:09:37,279 --> 00:09:42,000
right? Compute that we own, that we can

252
00:09:39,279 --> 00:09:43,760
repair, that we have access to. Um, it

253
00:09:42,000 --> 00:09:46,000
definitely includes community data

254
00:09:43,760 --> 00:09:49,440
governance, community data stewardship,

255
00:09:46,000 --> 00:09:51,200
and community model ownership. um where

256
00:09:49,440 --> 00:09:52,760
communities can set the terms of their

257
00:09:51,200 --> 00:09:54,800
engagement with that

258
00:09:52,760 --> 00:09:56,720
technology. Um and it includes

259
00:09:54,800 --> 00:09:58,800
communitydriven benchmarks where

260
00:09:56,720 --> 00:10:00,240
communities get to decide if AI is

261
00:09:58,800 --> 00:10:02,000
working for them, if a particular model

262
00:10:00,240 --> 00:10:03,680
is working for them and what the uh

263
00:10:02,000 --> 00:10:05,240
repercussions might be if those models

264
00:10:03,680 --> 00:10:08,320
are not a good

265
00:10:05,240 --> 00:10:10,800
fit. Um and then quickly a couple of

266
00:10:08,320 --> 00:10:12,800
instances of really wonderful AI that is

267
00:10:10,800 --> 00:10:14,959
sort of working in this way already that

268
00:10:12,800 --> 00:10:18,160
some of our members have pointed out. So

269
00:10:14,959 --> 00:10:20,880
the Igbo speech um platform that is

270
00:10:18,160 --> 00:10:23,200
starting to make Igbo language available

271
00:10:20,880 --> 00:10:25,160
to programs like Dualingo. So preserving

272
00:10:23,200 --> 00:10:27,480
indigenous language and making

273
00:10:25,160 --> 00:10:30,240
indigenous language more

274
00:10:27,480 --> 00:10:32,480
available. Um ways that people are

275
00:10:30,240 --> 00:10:35,040
thinking about how to make resource

276
00:10:32,480 --> 00:10:36,800
intensive AI work in harmony with those

277
00:10:35,040 --> 00:10:38,240
who are stewarding our environment. So

278
00:10:36,800 --> 00:10:40,000
thinking about frugal computing,

279
00:10:38,240 --> 00:10:41,680
thinking about water cooling systems,

280
00:10:40,000 --> 00:10:43,839
thinking about other ways to make data

281
00:10:41,680 --> 00:10:45,680
centers more performant and easier on

282
00:10:43,839 --> 00:10:47,440
the planet

283
00:10:45,680 --> 00:10:50,480
um in programs um and projects like

284
00:10:47,440 --> 00:10:52,720
WikiBench where people are community

285
00:10:50,480 --> 00:10:54,720
curating the data that AI models are

286
00:10:52,720 --> 00:10:56,800
being trained on using open source

287
00:10:54,720 --> 00:11:00,160
corpora using open source and community

288
00:10:56,800 --> 00:11:03,680
based methods. Um back to Katherine.

289
00:11:00,160 --> 00:11:06,480
Sure. Um, so yeah, we just wanted to

290
00:11:03,680 --> 00:11:09,200
wrap this up um by saying that we would

291
00:11:06,480 --> 00:11:10,880
see this project is we see it as a

292
00:11:09,200 --> 00:11:12,560
rolling public dialogue. That's why

293
00:11:10,880 --> 00:11:14,720
we're calling it a a kind of a think

294
00:11:12,560 --> 00:11:16,320
tank. We're not necessarily giving

295
00:11:14,720 --> 00:11:18,240
answers. We're not even necessarily

296
00:11:16,320 --> 00:11:20,079
agreeing with each other. Um, but we're

297
00:11:18,240 --> 00:11:22,320
coming together to dialogue and debate

298
00:11:20,079 --> 00:11:25,040
about both problems, dead ends, and and

299
00:11:22,320 --> 00:11:28,079
ways forward. Um, and so we think about

300
00:11:25,040 --> 00:11:31,200
this as this as an invented space of

301
00:11:28,079 --> 00:11:33,040
citizenship. So this builds on um very

302
00:11:31,200 --> 00:11:35,839
famous uh planning scholar farneck

303
00:11:33,040 --> 00:11:38,800
Mafab. She has this notion of invited

304
00:11:35,839 --> 00:11:41,040
versus invented spaces of citizenship.

305
00:11:38,800 --> 00:11:44,720
Um and so instead of waiting right now

306
00:11:41,040 --> 00:11:48,079
for open AI or for Google or for Meta to

307
00:11:44,720 --> 00:11:50,480
you know invite us in to participate um

308
00:11:48,079 --> 00:11:54,000
to on their terms um on the products

309
00:11:50,480 --> 00:11:55,920
they deem worthy to build and invest in.

310
00:11:54,000 --> 00:11:57,440
um then this group of us and then maybe

311
00:11:55,920 --> 00:12:00,079
all of you because you're all invited to

312
00:11:57,440 --> 00:12:02,240
be with us as well. Um we we came

313
00:12:00,079 --> 00:12:04,560
together here to sort of contest the

314
00:12:02,240 --> 00:12:06,160
status quo and to be able to think

315
00:12:04,560 --> 00:12:08,839
larger thoughts be able to think

316
00:12:06,160 --> 00:12:11,680
thoughts that help us reorganize

317
00:12:08,839 --> 00:12:13,440
resources in this AI system to recognize

318
00:12:11,680 --> 00:12:15,839
that the organization of resources

319
00:12:13,440 --> 00:12:17,440
structurally it is was having all of

320
00:12:15,839 --> 00:12:18,800
these harmful downstream effects. And so

321
00:12:17,440 --> 00:12:21,120
how do we sort of think about

322
00:12:18,800 --> 00:12:22,639
reorganizing those right now? Um so

323
00:12:21,120 --> 00:12:25,560
think about how do we re rearrange those

324
00:12:22,639 --> 00:12:27,760
resources in hope of a larger societal

325
00:12:25,560 --> 00:12:30,240
transformation and always in a way with

326
00:12:27,760 --> 00:12:33,920
a belief that we can and should build

327
00:12:30,240 --> 00:12:35,839
systems that work for all of us right

328
00:12:33,920 --> 00:12:37,760
um and so here we are for next steps

329
00:12:35,839 --> 00:12:40,320
where this is going this project um

330
00:12:37,760 --> 00:12:42,480
we've kind of started a snowball that

331
00:12:40,320 --> 00:12:44,320
keeps going this project doesn't seem to

332
00:12:42,480 --> 00:12:46,320
be ending anytime soon um it keeps

333
00:12:44,320 --> 00:12:47,680
acrewing uh new members new people keep

334
00:12:46,320 --> 00:12:49,600
joining us which has been really

335
00:12:47,680 --> 00:12:51,360
fascinating um so now it's going to be a

336
00:12:49,600 --> 00:12:53,600
partnership between Smith College where

337
00:12:51,360 --> 00:12:56,320
Nico is headed next year as an assistant

338
00:12:53,600 --> 00:12:58,000
professor and MIT. Uh then we're

339
00:12:56,320 --> 00:13:00,160
thinking of pitching it as an edited

340
00:12:58,000 --> 00:13:01,760
volume um to like an academic trade

341
00:13:00,160 --> 00:13:03,600
crossover book. So the there might be

342
00:13:01,760 --> 00:13:06,000
like a kind of a book that emerges from

343
00:13:03,600 --> 00:13:07,760
this. Um and then continuing our rolling

344
00:13:06,000 --> 00:13:10,000
public think tank. We have bi-weekly

345
00:13:07,760 --> 00:13:11,760
meetings. We do a lot of uh reading and

346
00:13:10,000 --> 00:13:13,279
discussion. We invite people to come and

347
00:13:11,760 --> 00:13:14,959
present their work and we all discuss it

348
00:13:13,279 --> 00:13:16,720
together. Um we're also thinking about

349
00:13:14,959 --> 00:13:18,720
doing public workshops. We really want

350
00:13:16,720 --> 00:13:21,839
to get students involved in particular

351
00:13:18,720 --> 00:13:23,200
students in computer science um students

352
00:13:21,839 --> 00:13:25,519
in these more sort of technical

353
00:13:23,200 --> 00:13:26,800
disciplines um to think about what are

354
00:13:25,519 --> 00:13:28,639
some of these really interesting

355
00:13:26,800 --> 00:13:30,800
problems even as a research space like

356
00:13:28,639 --> 00:13:33,760
how do we build you know small models

357
00:13:30,800 --> 00:13:35,279
how do we do uh distributed uh model

358
00:13:33,760 --> 00:13:37,680
development how do we have uh

359
00:13:35,279 --> 00:13:39,040
sovereignty over our AI models and so on

360
00:13:37,680 --> 00:13:41,519
and so forth I think actually these are

361
00:13:39,040 --> 00:13:42,800
really interesting sociotechnical uh

362
00:13:41,519 --> 00:13:44,160
research questions and be great to

363
00:13:42,800 --> 00:13:46,720
involve students sort of at the

364
00:13:44,160 --> 00:13:49,120
beginning um And then yeah, you're

365
00:13:46,720 --> 00:13:51,040
invited. So it's it's a very open

366
00:13:49,120 --> 00:13:53,360
project and we we invite you all and you

367
00:13:51,040 --> 00:13:55,600
just have to email Nikki. That's that's

368
00:13:53,360 --> 00:13:57,440
the way to get started. Um and so just

369
00:13:55,600 --> 00:13:59,440
to close out, we want to thank everyone

370
00:13:57,440 --> 00:14:01,440
who's joined the project so far. These

371
00:13:59,440 --> 00:14:03,600
are all the folks who were in community

372
00:14:01,440 --> 00:14:05,839
with and this this think tank. Um and

373
00:14:03,600 --> 00:14:07,920
then many many thanks to CIRC for for

374
00:14:05,839 --> 00:14:10,320
making this possible. um we didn't

375
00:14:07,920 --> 00:14:11,519
really know what the we this the output

376
00:14:10,320 --> 00:14:12,959
doesn't look like the output that we

377
00:14:11,519 --> 00:14:14,320
thought we would get but I think that's

378
00:14:12,959 --> 00:14:19,000
actually the best result of a

379
00:14:14,320 --> 00:14:19,000
pluralistic process. So thank you.

380
00:14:22,320 --> 00:14:25,600
Thank you. or Katherine

381
00:14:38,440 --> 00:14:44,800
questions. Um, thank you so much for the

382
00:14:40,959 --> 00:14:47,760
talk. I'm so excited by this idea. Um I

383
00:14:44,800 --> 00:14:50,160
am curious to to know it seems like a

384
00:14:47,760 --> 00:14:52,160
lot of the folks involved in the um

385
00:14:50,160 --> 00:14:55,199
think tank are folks from higher

386
00:14:52,160 --> 00:14:57,199
education. So is there any intention to

387
00:14:55,199 --> 00:14:59,360
involve stakeholders outside of this

388
00:14:57,199 --> 00:15:02,320
realm of higher education which is a bit

389
00:14:59,360 --> 00:15:03,519
of a self- selecting group?

390
00:15:02,320 --> 00:15:05,440
Yeah, I think that's such a good

391
00:15:03,519 --> 00:15:08,480
question. It's 100% a self- selecting

392
00:15:05,440 --> 00:15:10,320
group. I think um the yes, the short

393
00:15:08,480 --> 00:15:13,279
answer is yes, absolutely. I think the

394
00:15:10,320 --> 00:15:15,199
longer answer is how do we engage folks

395
00:15:13,279 --> 00:15:17,279
who maybe don't want to do readings with

396
00:15:15,199 --> 00:15:19,680
us or want to be we you know we we need

397
00:15:17,279 --> 00:15:21,920
to meet them where they are um we've run

398
00:15:19,680 --> 00:15:23,920
out of funding right and so we've spent

399
00:15:21,920 --> 00:15:25,760
the money and what I I don't want to

400
00:15:23,920 --> 00:15:28,079
bring in people who who want to be

401
00:15:25,760 --> 00:15:29,920
compensated right I get we get paid to

402
00:15:28,079 --> 00:15:31,440
be here all of us academics we get paid

403
00:15:29,920 --> 00:15:33,040
to read together and it feels really

404
00:15:31,440 --> 00:15:34,880
extractive to ask a community member to

405
00:15:33,040 --> 00:15:36,959
come and be here on their free time and

406
00:15:34,880 --> 00:15:38,079
so I think if we get money to compensate

407
00:15:36,959 --> 00:15:40,399
people for their part for their

408
00:15:38,079 --> 00:15:43,399
participation right in meaningful ways.

409
00:15:40,399 --> 00:15:43,399
Absolutely.

410
00:15:51,440 --> 00:15:55,040
Thanks for the talk. Um, you mentioned

411
00:15:53,360 --> 00:15:56,240
that like not everybody agreed on

412
00:15:55,040 --> 00:15:58,240
everything when you guys were meeting

413
00:15:56,240 --> 00:16:00,160
together. So, were there any like

414
00:15:58,240 --> 00:16:01,600
disagreements you found especially

415
00:16:00,160 --> 00:16:04,600
productive or like learned something

416
00:16:01,600 --> 00:16:04,600
from?

417
00:16:05,199 --> 00:16:10,560
Um, you know, I think one of the things

418
00:16:07,680 --> 00:16:12,000
that we've been finding funny is we all

419
00:16:10,560 --> 00:16:14,639
have like like I would say a fraught

420
00:16:12,000 --> 00:16:16,399
relationship with AI, but that that

421
00:16:14,639 --> 00:16:18,320
comes from for different reasons and

422
00:16:16,399 --> 00:16:20,000
it's also to different degrees. So there

423
00:16:18,320 --> 00:16:21,680
are people who have participated in the

424
00:16:20,000 --> 00:16:24,560
project who would characterize

425
00:16:21,680 --> 00:16:26,560
themselves as um I think AI refusers. So

426
00:16:24,560 --> 00:16:28,240
they actually like refuse to engage.

427
00:16:26,560 --> 00:16:30,560
They they from a from an ethical

428
00:16:28,240 --> 00:16:32,959
standpoint like they don't use Chachi

429
00:16:30,560 --> 00:16:35,199
PT. um they're not trying to incorporate

430
00:16:32,959 --> 00:16:37,199
anything with AI or generative AI sort

431
00:16:35,199 --> 00:16:38,399
of in their lives or whatever. Um so

432
00:16:37,199 --> 00:16:41,600
they're kind of these really kind of

433
00:16:38,399 --> 00:16:43,199
extreme um sort of refusal standpoints

434
00:16:41,600 --> 00:16:45,279
which I think is interesting and there's

435
00:16:43,199 --> 00:16:48,639
actually a really interesting long

436
00:16:45,279 --> 00:16:50,880
literature on refusal not just as a no

437
00:16:48,639 --> 00:16:53,759
right but actually as a generative

438
00:16:50,880 --> 00:16:55,759
position. So like we've incorporated uh

439
00:16:53,759 --> 00:16:58,560
contributions from folks who are kind of

440
00:16:55,759 --> 00:17:02,079
in that realm of things to say like if

441
00:16:58,560 --> 00:17:04,640
you refuse like you know it also begs

442
00:17:02,079 --> 00:17:06,640
the question of like can we refuse? I

443
00:17:04,640 --> 00:17:09,679
mean literally can we opt out at this

444
00:17:06,640 --> 00:17:11,240
point? Um I'm not sure right. Um and so

445
00:17:09,679 --> 00:17:13,520
that that has been interesting and

446
00:17:11,240 --> 00:17:16,079
provocative. Um I'm trying to think of

447
00:17:13,520 --> 00:17:17,520
what else. Uh I think one of the things

448
00:17:16,079 --> 00:17:19,520
that's been really interesting is to be

449
00:17:17,520 --> 00:17:21,400
in dialogue and one of the things we've

450
00:17:19,520 --> 00:17:24,720
been careful to try to

451
00:17:21,400 --> 00:17:26,600
structure is to is around that

452
00:17:24,720 --> 00:17:29,480
disciplinary um

453
00:17:26,600 --> 00:17:33,120
diversity um because we didn't want to

454
00:17:29,480 --> 00:17:34,760
have people from purely outside of a

455
00:17:33,120 --> 00:17:37,280
development space like

456
00:17:34,760 --> 00:17:39,600
non-developers outside who have never

457
00:17:37,280 --> 00:17:40,880
say built an application or they've

458
00:17:39,600 --> 00:17:42,559
never trained a model or something like

459
00:17:40,880 --> 00:17:44,480
that. kind of being those like stone

460
00:17:42,559 --> 00:17:46,400
throwers, you know, you like, you know,

461
00:17:44,480 --> 00:17:48,640
you armchair critique and you you throw

462
00:17:46,400 --> 00:17:50,160
stones from outside. So we have tried to

463
00:17:48,640 --> 00:17:51,919
structure the dialogue in such a way

464
00:17:50,160 --> 00:17:54,320
that um we're having generative

465
00:17:51,919 --> 00:17:56,400
conversations from inside from you know

466
00:17:54,320 --> 00:17:59,039
literally students that are PhD students

467
00:17:56,400 --> 00:18:00,960
professors of computer science um and

468
00:17:59,039 --> 00:18:03,880
outside so that we're colliding some of

469
00:18:00,960 --> 00:18:06,799
these disciplines um I think in very

470
00:18:03,880 --> 00:18:09,039
productive ways and also um bridging

471
00:18:06,799 --> 00:18:10,960
some of the language gaps that often

472
00:18:09,039 --> 00:18:13,120
prohibit us from from speaking to each

473
00:18:10,960 --> 00:18:15,520
other even like the same language. Um I

474
00:18:13,120 --> 00:18:16,720
don't know if you can think of other No,

475
00:18:15,520 --> 00:18:18,320
I think I think that's right. The one

476
00:18:16,720 --> 00:18:20,480
thing I will add is to the point of like

477
00:18:18,320 --> 00:18:21,840
being in community with developers. All

478
00:18:20,480 --> 00:18:23,520
of my research and scholarship comes out

479
00:18:21,840 --> 00:18:24,880
of my own career as a software engineer.

480
00:18:23,520 --> 00:18:26,480
And it's really important to me to

481
00:18:24,880 --> 00:18:28,559
remember that working AI developers

482
00:18:26,480 --> 00:18:30,400
can't refuse AI, right? They only they

483
00:18:28,559 --> 00:18:31,919
have such a limited range of choices and

484
00:18:30,400 --> 00:18:35,480
that I want that to always be in

485
00:18:31,919 --> 00:18:35,480
conversation with us.

486
00:18:36,039 --> 00:18:39,679
Um, this is maybe the other end of the

487
00:18:38,160 --> 00:18:42,400
spectrum, but it gets at the insider

488
00:18:39,679 --> 00:18:44,080
outsider idea. Um, as I've been doing

489
00:18:42,400 --> 00:18:47,840
this research and been kind of

490
00:18:44,080 --> 00:18:49,760
fascinated by some of the biggest AI

491
00:18:47,840 --> 00:18:52,000
development companies, you know, Open

492
00:18:49,760 --> 00:18:54,480
AI, Google, one of the things you learn

493
00:18:52,000 --> 00:18:58,640
is those are really big, well-funded

494
00:18:54,480 --> 00:19:02,240
companies and with very diverse

495
00:18:58,640 --> 00:19:04,480
um, uh, employees uh, with lots of

496
00:19:02,240 --> 00:19:06,720
different attitudes and I've encountered

497
00:19:04,480 --> 00:19:08,640
in different contexts I think a number

498
00:19:06,720 --> 00:19:09,919
of employees at companies like that who

499
00:19:08,640 --> 00:19:12,080
I think would be interested and

500
00:19:09,919 --> 00:19:14,240
supportive of the work that you're doing

501
00:19:12,080 --> 00:19:17,039
are working on some really interesting

502
00:19:14,240 --> 00:19:19,600
versions of trying to align their

503
00:19:17,039 --> 00:19:22,320
product and their models with the

504
00:19:19,600 --> 00:19:24,320
mission with democratic governance that

505
00:19:22,320 --> 00:19:27,880
sort of thing. I also think anybody

506
00:19:24,320 --> 00:19:30,320
who's looked at the history of business

507
00:19:27,880 --> 00:19:32,240
would question whether those

508
00:19:30,320 --> 00:19:35,520
recommendations or those initiatives

509
00:19:32,240 --> 00:19:37,280
will survive um an era where at some

510
00:19:35,520 --> 00:19:38,720
point the venture capitalists are going

511
00:19:37,280 --> 00:19:40,080
to expect those companies to be

512
00:19:38,720 --> 00:19:42,559
profitable too. So, I'm just curious

513
00:19:40,080 --> 00:19:45,120
what your thoughts are about engaging

514
00:19:42,559 --> 00:19:47,200
with people inside the big tech

515
00:19:45,120 --> 00:19:48,559
companies uh who are doing work that

516
00:19:47,200 --> 00:19:49,600
might be interesting or exciting or

517
00:19:48,559 --> 00:19:52,240
might be aligned with the work that

518
00:19:49,600 --> 00:19:54,400
you're doing. Yeah. No, 100%. And

519
00:19:52,240 --> 00:19:56,799
actually, um none of the current

520
00:19:54,400 --> 00:19:58,880
contributors,

521
00:19:56,799 --> 00:20:00,320
right, are corporate researchers, right?

522
00:19:58,880 --> 00:20:02,880
Yeah. So, none of the current

523
00:20:00,320 --> 00:20:05,280
contributors are, but I will say one of

524
00:20:02,880 --> 00:20:07,280
the my most generative events that I

525
00:20:05,280 --> 00:20:09,480
participated in in the past year was an

526
00:20:07,280 --> 00:20:12,799
event that brought together um industry

527
00:20:09,480 --> 00:20:14,640
researchers and um scholars to think

528
00:20:12,799 --> 00:20:17,919
about cultural inclusion and generative

529
00:20:14,640 --> 00:20:20,160
AI. Um and and it was fascinating to

530
00:20:17,919 --> 00:20:21,520
understand some of the constraints um

531
00:20:20,160 --> 00:20:23,360
but also to understand some of the

532
00:20:21,520 --> 00:20:25,840
opportunities and I I agree with you

533
00:20:23,360 --> 00:20:27,120
that there really there is alignment and

534
00:20:25,840 --> 00:20:29,039
certainly at the researcher to

535
00:20:27,120 --> 00:20:30,919
researcher level I think there is

536
00:20:29,039 --> 00:20:33,520
alignment and I think there's some um

537
00:20:30,919 --> 00:20:34,880
opportunity there what we would I think

538
00:20:33,520 --> 00:20:38,480
call attention to so what we're saying

539
00:20:34,880 --> 00:20:40,000
with like corporate AI is not to that's

540
00:20:38,480 --> 00:20:43,039
not like a judgment on somebody that

541
00:20:40,000 --> 00:20:44,960
works in a corporation MIT is a

542
00:20:43,039 --> 00:20:46,320
corporation

543
00:20:44,960 --> 00:20:48,960
So, you know, like there there's not

544
00:20:46,320 --> 00:20:50,720
like a some kind of like um purity

545
00:20:48,960 --> 00:20:53,200
critique there or something like that.

546
00:20:50,720 --> 00:20:56,080
Um but more to say exactly what you just

547
00:20:53,200 --> 00:20:57,760
said, which is that the um structures

548
00:20:56,080 --> 00:21:00,000
again the allocation of resources, the

549
00:20:57,760 --> 00:21:03,360
way that resources are arranged in a in

550
00:21:00,000 --> 00:21:05,120
a corporate model favors the production

551
00:21:03,360 --> 00:21:07,039
and decisions around certain kinds of

552
00:21:05,120 --> 00:21:09,120
products being made to serve certain

553
00:21:07,039 --> 00:21:10,480
kinds of populations. Meanwhile, there's

554
00:21:09,120 --> 00:21:12,000
a whole space over here, and that's

555
00:21:10,480 --> 00:21:14,159
that's a space that I've often worked in

556
00:21:12,000 --> 00:21:16,120
is sort of uh community technology,

557
00:21:14,159 --> 00:21:17,840
feminist technology, human rights

558
00:21:16,120 --> 00:21:19,679
technology. There all these sort of

559
00:21:17,840 --> 00:21:21,760
application areas that never get built

560
00:21:19,679 --> 00:21:23,679
for because they are not profitable.

561
00:21:21,760 --> 00:21:26,799
There's simply no business model. And so

562
00:21:23,679 --> 00:21:28,960
like the question there is like is that

563
00:21:26,799 --> 00:21:31,679
who we want the tech to serve? That's a

564
00:21:28,960 --> 00:21:33,520
real asymmetry and innovation. And so

565
00:21:31,679 --> 00:21:35,200
how do we do it differently? How do we

566
00:21:33,520 --> 00:21:36,320
structure things differently to produce

567
00:21:35,200 --> 00:21:39,120
different products and different

568
00:21:36,320 --> 00:21:42,000
outcomes? Yeah.

569
00:21:39,120 --> 00:21:44,640
And I guess my question actually relate

570
00:21:42,000 --> 00:21:47,919
uh somewhat to what you just outline is

571
00:21:44,640 --> 00:21:50,480
as a sin tank uh what is your long-term

572
00:21:47,919 --> 00:21:52,400
goal? Yeah. And uh what is the shortterm

573
00:21:50,480 --> 00:21:54,640
goal and how do you sustain yourself

574
00:21:52,400 --> 00:21:56,799
because just now you saying that the

575
00:21:54,640 --> 00:21:59,120
funding is longing out and how do you

576
00:21:56,799 --> 00:22:02,000
sustain yourself? Yeah. Uh to achieve

577
00:21:59,120 --> 00:22:04,000
that kind of goal and love to hear about

578
00:22:02,000 --> 00:22:05,679
it. Yeah. We want to know the answer

579
00:22:04,000 --> 00:22:07,360
too. Yeah. If you have the answer that

580
00:22:05,679 --> 00:22:09,200
would be great. I don't know if you want

581
00:22:07,360 --> 00:22:10,880
I mean I think like the most honest

582
00:22:09,200 --> 00:22:12,880
answer is like I have some startup

583
00:22:10,880 --> 00:22:14,880
funding from my new professorship and

584
00:22:12,880 --> 00:22:16,080
some of that will go to maintaining this

585
00:22:14,880 --> 00:22:18,720
and then we're going to write some

586
00:22:16,080 --> 00:22:21,360
grants and hope for the best. Yeah,

587
00:22:18,720 --> 00:22:23,200
right. Um yeah, I mean a lot of what

588
00:22:21,360 --> 00:22:25,360
these projects end up being is uh

589
00:22:23,200 --> 00:22:27,280
chained funding. You kind of like hope

590
00:22:25,360 --> 00:22:29,280
to get the next grant and you hope to

591
00:22:27,280 --> 00:22:32,080
move to the next grant. But I mean at a

592
00:22:29,280 --> 00:22:34,159
certain point maybe with enough um sort

593
00:22:32,080 --> 00:22:37,200
of structure and participation we could

594
00:22:34,159 --> 00:22:38,960
figure out like a a longer term strategy

595
00:22:37,200 --> 00:22:40,799
um and actually work on a longer term

596
00:22:38,960 --> 00:22:43,520
plan and then seek a like slightly more

597
00:22:40,799 --> 00:22:45,200
stable funding. Um so I like that idea

598
00:22:43,520 --> 00:22:49,799
but it's it's in information. I would

599
00:22:45,200 --> 00:22:49,799
say it's a it's an emerging think tank.

600
00:22:51,280 --> 00:22:56,480
Hi um I'm very appreciate to hear this

601
00:22:54,400 --> 00:22:58,720
kind of like you know information about

602
00:22:56,480 --> 00:23:01,679
like sharing the community model and

603
00:22:58,720 --> 00:23:05,440
families model and also um help with the

604
00:23:01,679 --> 00:23:07,840
data in uh inframe structure. Um the

605
00:23:05,440 --> 00:23:10,400
questions like actually um some of us

606
00:23:07,840 --> 00:23:13,120
are actually not student or like you

607
00:23:10,400 --> 00:23:16,320
know researchers but we are MIT staff

608
00:23:13,120 --> 00:23:20,000
and also using also utilize AI and also

609
00:23:16,320 --> 00:23:21,440
building pipelines. So um my my

610
00:23:20,000 --> 00:23:24,320
colleague and I we we are very

611
00:23:21,440 --> 00:23:26,880
interested to join and also witness and

612
00:23:24,320 --> 00:23:29,440
also I know like MIT is having new ERP

613
00:23:26,880 --> 00:23:31,520
system yeah in implementing in you know

614
00:23:29,440 --> 00:23:34,559
in the following like five to 10 years

615
00:23:31,520 --> 00:23:39,280
and um I'm looking forward to see if um

616
00:23:34,559 --> 00:23:43,200
anything we can um work together if

617
00:23:39,280 --> 00:23:45,280
possible. Yeah, you're welcome. Yeah,

618
00:23:43,200 --> 00:23:47,440
please join us. Yeah. Yeah. Yeah. We we

619
00:23:45,280 --> 00:23:50,000
already have um several folks who are

620
00:23:47,440 --> 00:23:52,000
not uh inside academia as like

621
00:23:50,000 --> 00:23:53,520
professors or students. So there are

622
00:23:52,000 --> 00:23:54,960
already folks who are practitioners who

623
00:23:53,520 --> 00:23:58,720
have joined. Yeah. That would be

624
00:23:54,960 --> 00:24:00,720
wonderful. Thanks. Great.

625
00:23:58,720 --> 00:24:04,280
Okay. Let's thank Nico and Katherine

626
00:24:00,720 --> 00:24:04,280
again. Thank you.

