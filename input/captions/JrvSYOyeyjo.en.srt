1
00:00:01,280 --> 00:00:06,000
Hi everybody. Sorry about the um delay.

2
00:00:04,240 --> 00:00:07,520
I'm Lily Thai. I'm a professor in the

3
00:00:06,000 --> 00:00:09,920
political science department and

4
00:00:07,520 --> 00:00:11,599
director of the MIT governance lab which

5
00:00:09,920 --> 00:00:13,440
is a group of behavioral and social

6
00:00:11,599 --> 00:00:15,360
scientists who work with policy makers

7
00:00:13,440 --> 00:00:17,840
and practitioners to research and

8
00:00:15,360 --> 00:00:19,680
develop innovations um bridging the gap

9
00:00:17,840 --> 00:00:21,439
between citizens and government um

10
00:00:19,680 --> 00:00:23,760
particularly in challenging environments

11
00:00:21,439 --> 00:00:25,840
and by that we mean anything ranging

12
00:00:23,760 --> 00:00:28,240
from Appalachia to you know rural

13
00:00:25,840 --> 00:00:30,000
subsaharan Africa.

14
00:00:28,240 --> 00:00:31,840
So, our research aims to address how

15
00:00:30,000 --> 00:00:33,760
people increasingly want to be able to

16
00:00:31,840 --> 00:00:35,760
have a say in the organizations and

17
00:00:33,760 --> 00:00:37,680
communities they belong to. Even though

18
00:00:35,760 --> 00:00:38,879
they usually don't want to have some

19
00:00:37,680 --> 00:00:40,960
they don't want to say something about

20
00:00:38,879 --> 00:00:42,559
every single issue, they just want to be

21
00:00:40,960 --> 00:00:44,480
able to say something when they have

22
00:00:42,559 --> 00:00:46,640
something. Um, people might want to have

23
00:00:44,480 --> 00:00:48,480
a say in policy decisions in their

24
00:00:46,640 --> 00:00:50,719
communities or what we call big D

25
00:00:48,480 --> 00:00:52,719
democracy such as a town hall in

26
00:00:50,719 --> 00:00:55,120
Wisconsin or participatory budgeting

27
00:00:52,719 --> 00:00:58,960
here in Cambridge. They may also want to

28
00:00:55,120 --> 00:01:01,920
say in everyday decisions um in their um

29
00:00:58,960 --> 00:01:04,159
uh uh communities um such as schools,

30
00:01:01,920 --> 00:01:05,720
workplaces and companies or what we call

31
00:01:04,159 --> 00:01:08,320
small d

32
00:01:05,720 --> 00:01:10,799
democracy. Technology now makes it

33
00:01:08,320 --> 00:01:12,720
possible for everyone to have a say. But

34
00:01:10,799 --> 00:01:14,720
we know this can be very overwhelming in

35
00:01:12,720 --> 00:01:17,439
two ways. Um number one, it can be too

36
00:01:14,720 --> 00:01:18,960
much information. Um so how do we how

37
00:01:17,439 --> 00:01:21,360
can we possibly comprehend everything

38
00:01:18,960 --> 00:01:23,040
and summarize it and digest it? And

39
00:01:21,360 --> 00:01:25,360
second, people are of course learning to

40
00:01:23,040 --> 00:01:27,759
behave very badly on social media.

41
00:01:25,360 --> 00:01:30,000
Discourse has become really univil and

42
00:01:27,759 --> 00:01:34,159
there's an urgent need for online

43
00:01:30,000 --> 00:01:34,159
platforms that are what we call more

44
00:01:34,200 --> 00:01:38,240
pro-democratic. We want to use

45
00:01:35,920 --> 00:01:40,000
technology to help understand all of

46
00:01:38,240 --> 00:01:42,799
this overwhelming information and to

47
00:01:40,000 --> 00:01:44,159
restore civility. And more civility,

48
00:01:42,799 --> 00:01:46,720
thankfully, is something that people

49
00:01:44,159 --> 00:01:49,280
across the board agree about. So we see

50
00:01:46,720 --> 00:01:51,040
former VP Mike Pence in a talk at

51
00:01:49,280 --> 00:01:52,720
Dartmouth highlighting that partisanship

52
00:01:51,040 --> 00:01:55,200
does not need to be vicious and that

53
00:01:52,720 --> 00:01:58,079
quote democracy depends on a heavy dose

54
00:01:55,200 --> 00:01:59,680
of civility. He says, "I honestly think

55
00:01:58,079 --> 00:02:01,960
we would do well as a nation to return

56
00:01:59,680 --> 00:02:04,240
to that principle of freedom and mutual

57
00:02:01,960 --> 00:02:06,240
respect." Similarly, we see former

58
00:02:04,240 --> 00:02:07,759
President Biden asking Americans to cool

59
00:02:06,240 --> 00:02:09,599
down the rhetoric after the

60
00:02:07,759 --> 00:02:12,400
assassination attempt on President

61
00:02:09,599 --> 00:02:15,120
Trump. And ordinary Americans want more

62
00:02:12,400 --> 00:02:16,879
civil discourse, too. Each year, the

63
00:02:15,120 --> 00:02:19,360
American Bar Association polls the

64
00:02:16,879 --> 00:02:21,280
public on civic literacy and current

65
00:02:19,360 --> 00:02:23,280
events. And surveys from previous years

66
00:02:21,280 --> 00:02:25,520
show Americans believe society has

67
00:02:23,280 --> 00:02:28,080
become less civil and therefore want

68
00:02:25,520 --> 00:02:30,400
more compromise from their politicians

69
00:02:28,080 --> 00:02:33,400
and um blame social media and public

70
00:02:30,400 --> 00:02:36,000
officials for the decline in

71
00:02:33,400 --> 00:02:37,440
civility. So one method to use

72
00:02:36,000 --> 00:02:39,040
technology to increase mutual

73
00:02:37,440 --> 00:02:41,760
understanding and civility is through

74
00:02:39,040 --> 00:02:43,760
deliberation. Deliberative democracy is

75
00:02:41,760 --> 00:02:45,599
a form of democracy where people come

76
00:02:43,760 --> 00:02:47,920
together with mutual respect and as

77
00:02:45,599 --> 00:02:50,319
political equals to discuss political

78
00:02:47,920 --> 00:02:52,280
issues, listen to each other and make

79
00:02:50,319 --> 00:02:54,959
decisions about matters of common

80
00:02:52,280 --> 00:02:56,440
interest. Deliberation is thought to

81
00:02:54,959 --> 00:02:58,879
have benefits for individual

82
00:02:56,440 --> 00:03:01,040
participants to improve the quality of

83
00:02:58,879 --> 00:03:03,360
the decision and to have system level

84
00:03:01,040 --> 00:03:06,319
benefits legitimizing the political

85
00:03:03,360 --> 00:03:08,239
system as a whole.

86
00:03:06,319 --> 00:03:10,400
Online deliberative platforms have been

87
00:03:08,239 --> 00:03:12,560
growing in use in both the public sector

88
00:03:10,400 --> 00:03:14,319
and the private sector and there are

89
00:03:12,560 --> 00:03:16,480
applications in the private sector that

90
00:03:14,319 --> 00:03:18,879
could strengthen the small D democracy

91
00:03:16,480 --> 00:03:20,720
and build our skills of engaging um

92
00:03:18,879 --> 00:03:22,800
democratically with one another in our

93
00:03:20,720 --> 00:03:25,200
everyday lives. These include things

94
00:03:22,800 --> 00:03:27,280
like employee town halls or even just

95
00:03:25,200 --> 00:03:29,200
doing quick temperature checks among

96
00:03:27,280 --> 00:03:30,799
working groups to see what people are

97
00:03:29,200 --> 00:03:32,720
thinking without going the through the

98
00:03:30,799 --> 00:03:35,120
you know the sort of tiring rigmarroll

99
00:03:32,720 --> 00:03:37,920
of calendaring and scheduling a meeting.

100
00:03:35,120 --> 00:03:39,360
Um it could help with coping with coping

101
00:03:37,920 --> 00:03:41,680
with the move towards shareholder

102
00:03:39,360 --> 00:03:43,840
democracy and it can even just improve

103
00:03:41,680 --> 00:03:46,560
how consumers deliberate over online

104
00:03:43,840 --> 00:03:49,360
reviews.

105
00:03:46,560 --> 00:03:51,280
Our focus though is on how we can build

106
00:03:49,360 --> 00:03:53,120
on existing technologies and improve

107
00:03:51,280 --> 00:03:55,280
them with rigorous interdisciplinary

108
00:03:53,120 --> 00:03:57,599
research and how we can innovate by

109
00:03:55,280 --> 00:04:01,480
integrating generative AI to enhance the

110
00:03:57,599 --> 00:04:03,920
benefits of online spaces for

111
00:04:01,480 --> 00:04:06,560
deliberation. So first we wanted to

112
00:04:03,920 --> 00:04:08,640
approach this issue by identifying a set

113
00:04:06,560 --> 00:04:10,959
of guiding values and principles for

114
00:04:08,640 --> 00:04:13,120
what we felt were socially and ethically

115
00:04:10,959 --> 00:04:16,239
important. So the first thing we did was

116
00:04:13,120 --> 00:04:18,160
to write an MIT AI impact paper laying

117
00:04:16,239 --> 00:04:20,560
out these principles and values from

118
00:04:18,160 --> 00:04:23,360
political philosophy. We wanted to

119
00:04:20,560 --> 00:04:24,960
preserve um agency. So preserving agency

120
00:04:23,360 --> 00:04:28,160
ensures that participants make the

121
00:04:24,960 --> 00:04:30,400
choices not the AI. Encouraging mutual

122
00:04:28,160 --> 00:04:32,240
respect was another important principle.

123
00:04:30,400 --> 00:04:34,000
And we also wanted to promote equality

124
00:04:32,240 --> 00:04:36,160
and inclusiveness. We want people to

125
00:04:34,000 --> 00:04:37,759
feel protected. So we think that the

126
00:04:36,160 --> 00:04:40,160
identity of participants on platforms

127
00:04:37,759 --> 00:04:41,520
should be anonymous but authenticated

128
00:04:40,160 --> 00:04:44,199
and participants should be

129
00:04:41,520 --> 00:04:46,880
representative of the population as a

130
00:04:44,199 --> 00:04:49,680
whole. Augmenting citizenship ensures

131
00:04:46,880 --> 00:04:51,680
that AI helps participants understand

132
00:04:49,680 --> 00:04:53,919
other perspectives or the issue more

133
00:04:51,680 --> 00:04:56,160
fully but doesn't substitute or take

134
00:04:53,919 --> 00:04:59,840
away from the actions that citizens

135
00:04:56,160 --> 00:05:01,440
themselves should have control over.

136
00:04:59,840 --> 00:05:03,600
With those principles, we've developed

137
00:05:01,440 --> 00:05:06,039
our own AI integrated platform for

138
00:05:03,600 --> 00:05:08,000
deliberative democracy, which we call

139
00:05:06,039 --> 00:05:10,080
deliberation.io. And we've designed this

140
00:05:08,000 --> 00:05:12,160
platform to be modular so we can include

141
00:05:10,080 --> 00:05:13,880
or take out modules as our research

142
00:05:12,160 --> 00:05:15,919
uncovers what is most

143
00:05:13,880 --> 00:05:17,919
effective. Right now, there are four

144
00:05:15,919 --> 00:05:20,080
modules that we are working on. And each

145
00:05:17,919 --> 00:05:22,320
of these circles is a module we have.

146
00:05:20,080 --> 00:05:24,919
Under each of the circles is the goal

147
00:05:22,320 --> 00:05:27,840
that the module is aiming

148
00:05:24,919 --> 00:05:29,360
for. And for each of the modules, we

149
00:05:27,840 --> 00:05:31,520
have particular outcomes that we're

150
00:05:29,360 --> 00:05:33,280
interested in studying. Things like, are

151
00:05:31,520 --> 00:05:35,360
people able to justify their reasoning

152
00:05:33,280 --> 00:05:37,840
for a particular position? Um, do they

153
00:05:35,360 --> 00:05:40,240
have empathy for other positions? What

154
00:05:37,840 --> 00:05:41,919
is the area of agreement? And do they

155
00:05:40,240 --> 00:05:43,520
find the decision legitimate and are

156
00:05:41,919 --> 00:05:46,000
they willing to cooperate with the

157
00:05:43,520 --> 00:05:47,919
decision voluntarily? All of these are

158
00:05:46,000 --> 00:05:50,680
outcomes related to what we want

159
00:05:47,919 --> 00:05:53,919
deliberation to do in order to support

160
00:05:50,680 --> 00:05:55,840
democracy. Importantly, we are testing

161
00:05:53,919 --> 00:05:58,160
each of these modules in what we call

162
00:05:55,840 --> 00:05:59,680
evaluation guided system development.

163
00:05:58,160 --> 00:06:02,000
That is to say, we're using randomized

164
00:05:59,680 --> 00:06:04,479
control trials or AB testing to make

165
00:06:02,000 --> 00:06:06,400
sure that we're not just getting user

166
00:06:04,479 --> 00:06:08,880
engagement or more users using the

167
00:06:06,400 --> 00:06:10,960
platform, but that the positive system

168
00:06:08,880 --> 00:06:12,639
level and decision level democratic

169
00:06:10,960 --> 00:06:16,160
outcomes we care about, the downstream

170
00:06:12,639 --> 00:06:18,240
effects are also um something that um

171
00:06:16,160 --> 00:06:20,800
are we're seeing when people use this

172
00:06:18,240 --> 00:06:22,160
platform. If you take nothing else from

173
00:06:20,800 --> 00:06:23,759
this presentation, I hope that you'll

174
00:06:22,160 --> 00:06:26,000
take away this that we should all be

175
00:06:23,759 --> 00:06:27,919
demanding that the technologies um that

176
00:06:26,000 --> 00:06:29,840
are being developed are tested and

177
00:06:27,919 --> 00:06:32,080
assessed to see if they have these

178
00:06:29,840 --> 00:06:34,800
positive downstream outcomes rather than

179
00:06:32,080 --> 00:06:35,960
just always focusing on maximizing the

180
00:06:34,800 --> 00:06:38,800
number of

181
00:06:35,960 --> 00:06:40,400
users. So, I'll give a brief overview of

182
00:06:38,800 --> 00:06:42,840
each of these modules and say a little

183
00:06:40,400 --> 00:06:45,759
bit about what we're learning in more

184
00:06:42,840 --> 00:06:47,720
detail. So, here's an example of how a

185
00:06:45,759 --> 00:06:51,199
user joins a conversation on

186
00:06:47,720 --> 00:06:52,960
deliberation.io. IO the user starts by

187
00:06:51,199 --> 00:06:55,280
expressing oh let me see if this is play

188
00:06:52,960 --> 00:06:55,280
is this

189
00:06:56,280 --> 00:07:01,120
playing actually this is the screenshot

190
00:06:58,479 --> 00:07:02,720
okay so the user starts by expressing um

191
00:07:01,120 --> 00:07:05,039
how much they agree or disagree with an

192
00:07:02,720 --> 00:07:06,400
issue in this case should the US have

193
00:07:05,039 --> 00:07:08,960
for example an automatic voter

194
00:07:06,400 --> 00:07:10,800
registration policy then the user makes

195
00:07:08,960 --> 00:07:12,199
a comment which will be seen by others

196
00:07:10,800 --> 00:07:14,560
on the

197
00:07:12,199 --> 00:07:16,560
platform after the user enters their

198
00:07:14,560 --> 00:07:18,080
comment they then enter what we call

199
00:07:16,560 --> 00:07:21,039
what we've nicknamed our socratic

200
00:07:18,080 --> 00:07:23,599
dialogue module. So, this video shows a

201
00:07:21,039 --> 00:07:24,599
sample interaction and let me just see

202
00:07:23,599 --> 00:07:27,400
is

203
00:07:24,599 --> 00:07:31,440
this

204
00:07:27,400 --> 00:07:32,880
moving. Okay. Well, um statically, this

205
00:07:31,440 --> 00:07:34,560
is actually supposed to be run as a

206
00:07:32,880 --> 00:07:37,759
little bit of an interactive video, but

207
00:07:34,560 --> 00:07:39,520
um but in any case, um the user here is

208
00:07:37,759 --> 00:07:41,759
concerned about possible biases in such

209
00:07:39,520 --> 00:07:43,840
a policy. And the chatbot then responds

210
00:07:41,759 --> 00:07:45,680
by reasonably probing this concern and

211
00:07:43,840 --> 00:07:48,720
asking the user why would an automatic

212
00:07:45,680 --> 00:07:51,759
policy might have biases. um an AI

213
00:07:48,720 --> 00:07:53,680
chatbot um asks in in other words the

214
00:07:51,759 --> 00:07:56,000
user to reflect and reason through their

215
00:07:53,680 --> 00:07:59,160
comment and then the user has the option

216
00:07:56,000 --> 00:08:02,160
to update their comment if they would

217
00:07:59,160 --> 00:08:04,560
like. So our results from a study of

218
00:08:02,160 --> 00:08:06,720
1500 research participants show that

219
00:08:04,560 --> 00:08:09,680
users who engage in reason giving with

220
00:08:06,720 --> 00:08:11,520
our Socratic dialogue LLM actually

221
00:08:09,680 --> 00:08:13,280
moderated their positions and moved to

222
00:08:11,520 --> 00:08:16,400
more centrist positions after the

223
00:08:13,280 --> 00:08:18,400
exercise. So amazingly, in other words,

224
00:08:16,400 --> 00:08:20,160
an AI chatbot that supports a user

225
00:08:18,400 --> 00:08:22,240
through a little bit of reflection just

226
00:08:20,160 --> 00:08:25,919
between the user and the chatbot

227
00:08:22,240 --> 00:08:25,919
actually helped to depolarize the

228
00:08:25,960 --> 00:08:31,599
discussion. Engaging in reason giving

229
00:08:28,240 --> 00:08:34,240
with an LLM also led users to report um

230
00:08:31,599 --> 00:08:36,959
higher quality of conversation, higher

231
00:08:34,240 --> 00:08:38,800
perceptions of feeling respected, um a

232
00:08:36,959 --> 00:08:40,399
more pleasant experience, and more

233
00:08:38,800 --> 00:08:43,039
willingness to engage in civic

234
00:08:40,399 --> 00:08:45,000
participation after the deliberation. in

235
00:08:43,039 --> 00:08:47,600
this case, signing up to receive a

236
00:08:45,000 --> 00:08:49,279
newsletter. These are all extremely

237
00:08:47,600 --> 00:08:51,360
encouraging results for bringing more

238
00:08:49,279 --> 00:08:54,720
civility back into the discussions and

239
00:08:51,360 --> 00:08:54,720
decisions that we have to make

240
00:08:54,760 --> 00:08:57,920
together. A second module that we're

241
00:08:56,959 --> 00:08:59,839
testing is something that we've

242
00:08:57,920 --> 00:09:01,760
nicknamed face of the crowd. And the

243
00:08:59,839 --> 00:09:03,440
idea behind this module is that people

244
00:09:01,760 --> 00:09:06,000
tend to believe that they are farther

245
00:09:03,440 --> 00:09:08,640
apart on a question than they actually

246
00:09:06,000 --> 00:09:11,519
are.

247
00:09:08,640 --> 00:09:13,279
So visualizations from this module

248
00:09:11,519 --> 00:09:15,920
quickly summarize the conversation for

249
00:09:13,279 --> 00:09:17,839
participants. Um in one of our studies

250
00:09:15,920 --> 00:09:19,839
we asked research participants if they

251
00:09:17,839 --> 00:09:21,880
support the use of the military against

252
00:09:19,839 --> 00:09:24,320
civilians during peaceful

253
00:09:21,880 --> 00:09:26,240
protests. It turns out both Democrats

254
00:09:24,320 --> 00:09:29,360
and Republicans tended to oppose

255
00:09:26,240 --> 00:09:31,600
military use in this case. But people in

256
00:09:29,360 --> 00:09:34,560
both parties had misconceptions about

257
00:09:31,600 --> 00:09:36,640
their fellow citizens. So, Democrats

258
00:09:34,560 --> 00:09:38,399
started off the deliberation believing

259
00:09:36,640 --> 00:09:40,720
that Republicans are more supportive of

260
00:09:38,399 --> 00:09:42,640
military use than they were, than they

261
00:09:40,720 --> 00:09:44,480
actually were. Um, interestingly,

262
00:09:42,640 --> 00:09:46,160
Republicans also thought that other

263
00:09:44,480 --> 00:09:48,920
Republicans were more supportive of

264
00:09:46,160 --> 00:09:51,600
military use than they actually

265
00:09:48,920 --> 00:09:53,120
are. We then showed visualizations of

266
00:09:51,600 --> 00:09:54,640
where people actually stand on the

267
00:09:53,120 --> 00:09:56,399
issue. And those who saw these

268
00:09:54,640 --> 00:09:58,959
visualizations and realized there was a

269
00:09:56,399 --> 00:10:01,360
large common area of agreement between

270
00:09:58,959 --> 00:10:03,360
Democrats and Republicans tended to

271
00:10:01,360 --> 00:10:05,440
increase their support for not using

272
00:10:03,360 --> 00:10:07,839
military force against peaceful protest.

273
00:10:05,440 --> 00:10:11,640
So therefore increasing the number of

274
00:10:07,839 --> 00:10:13,920
people who are in the common area of

275
00:10:11,640 --> 00:10:16,000
overlap. Our third module is something

276
00:10:13,920 --> 00:10:17,839
called dynamic deliberation which is an

277
00:10:16,000 --> 00:10:20,240
AI moderated conversation on our

278
00:10:17,839 --> 00:10:22,000
platform. The AI generates uh an

279
00:10:20,240 --> 00:10:23,760
agreement statement based on the

280
00:10:22,000 --> 00:10:25,920
existing comments that have been made in

281
00:10:23,760 --> 00:10:28,320
the deliberation or conversation and

282
00:10:25,920 --> 00:10:30,480
then poses a subsequent um question to

283
00:10:28,320 --> 00:10:32,680
the group in this case about how to

284
00:10:30,480 --> 00:10:35,200
define peaceful

285
00:10:32,680 --> 00:10:37,600
protest. So what we saw in the

286
00:10:35,200 --> 00:10:39,360
conversation about the use of military

287
00:10:37,600 --> 00:10:41,680
um on peaceful protest was that people

288
00:10:39,360 --> 00:10:44,079
were like they you know people tended to

289
00:10:41,680 --> 00:10:45,600
agree yes we or no we don't think that

290
00:10:44,079 --> 00:10:47,920
the military should be used against

291
00:10:45,600 --> 00:10:50,240
civilians um in peaceful protest. But

292
00:10:47,920 --> 00:10:52,320
then people started to get stuck on what

293
00:10:50,240 --> 00:10:54,880
the definition of peaceful protest was.

294
00:10:52,320 --> 00:10:58,000
And so here the chatbot says okay people

295
00:10:54,880 --> 00:10:59,680
seem to agree on this but um let's now

296
00:10:58,000 --> 00:11:01,440
talk about what we mean by peaceful

297
00:10:59,680 --> 00:11:04,240
protest. So they have a second round of

298
00:11:01,440 --> 00:11:06,320
discussion to try and um come to an

299
00:11:04,240 --> 00:11:09,320
agreement on the definition of peaceful

300
00:11:06,320 --> 00:11:09,320
protest.

301
00:11:10,079 --> 00:11:14,560
This slide shows sort of the process um

302
00:11:12,640 --> 00:11:17,519
through going through the first round of

303
00:11:14,560 --> 00:11:19,680
um discussions, an AI statement of

304
00:11:17,519 --> 00:11:21,600
agreement that summarizes what we agree

305
00:11:19,680 --> 00:11:22,800
on and then an AI question prompt that

306
00:11:21,600 --> 00:11:26,200
then leads to a second round of

307
00:11:22,800 --> 00:11:28,399
deliberation and then a second round of

308
00:11:26,200 --> 00:11:30,640
agreement. And what we found that was

309
00:11:28,399 --> 00:11:32,800
really interesting was that in all cases

310
00:11:30,640 --> 00:11:34,640
the AI moderated conversation led to

311
00:11:32,800 --> 00:11:37,440
more support for rights to peaceful

312
00:11:34,640 --> 00:11:39,440
protest. In other words, um people again

313
00:11:37,440 --> 00:11:42,160
moved towards the area of common

314
00:11:39,440 --> 00:11:44,079
agreement and we saw this change with

315
00:11:42,160 --> 00:11:46,600
all groups with Democrats, with

316
00:11:44,079 --> 00:11:49,040
independents and with

317
00:11:46,600 --> 00:11:51,200
Republicans. Other results that aren't

318
00:11:49,040 --> 00:11:52,800
shown here also include increased

319
00:11:51,200 --> 00:11:56,079
understanding of what was being

320
00:11:52,800 --> 00:11:57,399
discussed and increased perceptions that

321
00:11:56,079 --> 00:11:59,920
the platform was

322
00:11:57,399 --> 00:12:02,480
legitimate. Using NLP, we also saw that

323
00:11:59,920 --> 00:12:04,480
AI moderated conversations encouraged a

324
00:12:02,480 --> 00:12:06,399
greater diversity of comments. um a

325
00:12:04,480 --> 00:12:08,120
greater diversity of words and a greater

326
00:12:06,399 --> 00:12:10,480
diversity of

327
00:12:08,120 --> 00:12:12,399
topics. So it does seem like AI

328
00:12:10,480 --> 00:12:14,160
generated statements can be another way

329
00:12:12,399 --> 00:12:17,040
of summarizing a lot of information

330
00:12:14,160 --> 00:12:18,399
quickly for participants. And we're now

331
00:12:17,040 --> 00:12:21,040
doing research to understand whether

332
00:12:18,399 --> 00:12:23,920
participants feel that in some ways an

333
00:12:21,040 --> 00:12:26,079
AI moderator is seen as more impartial

334
00:12:23,920 --> 00:12:28,639
and that's why they see AI moderated

335
00:12:26,079 --> 00:12:30,160
deliberations as more legitimate.

336
00:12:28,639 --> 00:12:32,079
Although we say all of this of course

337
00:12:30,160 --> 00:12:34,240
while noting we should continue to be

338
00:12:32,079 --> 00:12:36,200
careful about understanding exactly how

339
00:12:34,240 --> 00:12:39,519
the AI

340
00:12:36,200 --> 00:12:43,279
works. The last module I'll discuss is a

341
00:12:39,519 --> 00:12:45,680
comment ordering module and um this

342
00:12:43,279 --> 00:12:47,839
shows um comment voting on our platform

343
00:12:45,680 --> 00:12:49,839
and actually it's supposed to show a a

344
00:12:47,839 --> 00:12:53,120
series of comments. Um after each

345
00:12:49,839 --> 00:12:54,880
comment the user selects a button agree,

346
00:12:53,120 --> 00:12:57,600
disagree or pass and the next comment

347
00:12:54,880 --> 00:12:59,760
shows up. So, comment routing or comment

348
00:12:57,600 --> 00:13:01,440
ordering are important parts of a

349
00:12:59,760 --> 00:13:04,240
participant's experience with online

350
00:13:01,440 --> 00:13:06,800
deliberation platforms. Um, when users

351
00:13:04,240 --> 00:13:09,839
take part in a conversation online on

352
00:13:06,800 --> 00:13:11,680
online deliberative platforms, they're

353
00:13:09,839 --> 00:13:13,760
exposed to a set of comments, usually

354
00:13:11,680 --> 00:13:15,920
displayed sequentially.

355
00:13:13,760 --> 00:13:17,519
So computer scientists generally design

356
00:13:15,920 --> 00:13:19,680
the order in which these comments are

357
00:13:17,519 --> 00:13:22,000
displayed to help the algorithm work

358
00:13:19,680 --> 00:13:24,040
most efficiently and not necessarily by

359
00:13:22,000 --> 00:13:26,320
these kinds of downstream social

360
00:13:24,040 --> 00:13:28,480
considerations such as making the user

361
00:13:26,320 --> 00:13:30,639
feel um more heard or making them feel

362
00:13:28,480 --> 00:13:33,440
like the process is legitimate. So we

363
00:13:30,639 --> 00:13:35,600
were interested in examining this and we

364
00:13:33,440 --> 00:13:37,279
did some research where participants

365
00:13:35,600 --> 00:13:39,120
first indicated their position about

366
00:13:37,279 --> 00:13:41,040
automatic voter registration and then

367
00:13:39,120 --> 00:13:43,600
they were randomized into one of three

368
00:13:41,040 --> 00:13:45,600
conditions. similar comments, comments

369
00:13:43,600 --> 00:13:48,639
that were similar to their own opinion,

370
00:13:45,600 --> 00:13:50,959
moving gradually to different um those

371
00:13:48,639 --> 00:13:52,240
who started off with viewing comments

372
00:13:50,959 --> 00:13:53,920
that were very different from their own

373
00:13:52,240 --> 00:13:55,920
opinions and then slowly moving to ones

374
00:13:53,920 --> 00:13:57,600
that were more similar and then random,

375
00:13:55,920 --> 00:13:59,639
which is actually I think the default

376
00:13:57,600 --> 00:14:03,519
case typically in these

377
00:13:59,639 --> 00:14:05,680
platforms. Um so interestingly we found

378
00:14:03,519 --> 00:14:07,839
that participants in the similar to

379
00:14:05,680 --> 00:14:09,920
different condition were more likely to

380
00:14:07,839 --> 00:14:12,240
find the process more legitimate and to

381
00:14:09,920 --> 00:14:15,040
feel more represented when compared to

382
00:14:12,240 --> 00:14:16,959
the random order condition. People find

383
00:14:15,040 --> 00:14:18,560
it more comfortable to first see

384
00:14:16,959 --> 00:14:20,079
opinions that are closer to themselves

385
00:14:18,560 --> 00:14:22,079
and then move to more different ones.

386
00:14:20,079 --> 00:14:24,240
They're they seem to be more likely to

387
00:14:22,079 --> 00:14:28,199
accept different views when they're

388
00:14:24,240 --> 00:14:28,199
moving from similar to different.

389
00:14:29,680 --> 00:14:33,959
So again we see participants feel that

390
00:14:31,920 --> 00:14:36,800
they feel more respected

391
00:14:33,959 --> 00:14:39,320
um among yeah when they when they are in

392
00:14:36,800 --> 00:14:42,079
um the similar to different comment

393
00:14:39,320 --> 00:14:43,920
ordering. So to summarize through our

394
00:14:42,079 --> 00:14:45,760
Socratic dialogue module we're learning

395
00:14:43,920 --> 00:14:48,000
that AI assisted reasoning can move

396
00:14:45,760 --> 00:14:50,720
opinions to more centrist positions and

397
00:14:48,000 --> 00:14:51,920
depolarize the discussion. Showing

398
00:14:50,720 --> 00:14:54,000
people the face of the crowd

399
00:14:51,920 --> 00:14:55,839
visualizations can help us find clear

400
00:14:54,000 --> 00:14:57,760
areas of agreement, correct

401
00:14:55,839 --> 00:15:00,160
misperceptions about polarization when

402
00:14:57,760 --> 00:15:02,360
it doesn't actually exist, and realize

403
00:15:00,160 --> 00:15:05,279
how much we actually agree with one

404
00:15:02,360 --> 00:15:07,360
another. AI moderated conversations with

405
00:15:05,279 --> 00:15:09,160
AI generated agreement statements and

406
00:15:07,360 --> 00:15:11,839
questions can lead to more nuanced

407
00:15:09,160 --> 00:15:15,120
discussion, more understanding, and more

408
00:15:11,839 --> 00:15:17,440
legitimacy of deliberation and decision.

409
00:15:15,120 --> 00:15:19,440
and similar to different comment routing

410
00:15:17,440 --> 00:15:21,680
helps people feel more respected and

411
00:15:19,440 --> 00:15:21,680
more

412
00:15:21,720 --> 00:15:26,240
represented. All of our studies have

413
00:15:23,920 --> 00:15:27,839
been done in the lab so far with

414
00:15:26,240 --> 00:15:30,560
research participants, but we're now

415
00:15:27,839 --> 00:15:32,240
also working on a set of field studies.

416
00:15:30,560 --> 00:15:34,160
Our first one will be with the

417
00:15:32,240 --> 00:15:36,720
Washington DC government with the office

418
00:15:34,160 --> 00:15:39,360
of the chief technology officer. So

419
00:15:36,720 --> 00:15:42,639
using deliberation.io, we'll discuss two

420
00:15:39,360 --> 00:15:45,839
policy issues with citizens in DC. uh

421
00:15:42,639 --> 00:15:47,959
one the use of AI in government and two

422
00:15:45,839 --> 00:15:50,959
AI in the future of the DC

423
00:15:47,959 --> 00:15:54,160
economy. We've already won one f we've

424
00:15:50,959 --> 00:15:55,759
already run one focus group in DC. Um

425
00:15:54,160 --> 00:15:58,880
here are some of the photos from that.

426
00:15:55,759 --> 00:16:01,120
And um the actual town hall discussions

427
00:15:58,880 --> 00:16:04,759
for the DC city government go live in

428
00:16:01,120 --> 00:16:04,759
July and October.

429
00:16:04,800 --> 00:16:09,120
We also have some plans to design a

430
00:16:06,720 --> 00:16:11,199
preference aggregation module and we're

431
00:16:09,120 --> 00:16:13,040
also running full studies of our pilots

432
00:16:11,199 --> 00:16:14,800
where we're exploring topics with

433
00:16:13,040 --> 00:16:17,680
different levels of polarization to see

434
00:16:14,800 --> 00:16:20,000
if the results um replicate and hold for

435
00:16:17,680 --> 00:16:21,440
these different kinds of topics.

436
00:16:20,000 --> 00:16:23,920
Finally, we're working with potential

437
00:16:21,440 --> 00:16:26,320
partners on other locations for field

438
00:16:23,920 --> 00:16:29,120
studies including Michigan, Texas, and

439
00:16:26,320 --> 00:16:31,199
Utah. And we hope that this work will

440
00:16:29,120 --> 00:16:33,680
help participants and policy makers make

441
00:16:31,199 --> 00:16:36,320
sense of the information they receive

442
00:16:33,680 --> 00:16:40,519
from online town hall meetings and most

443
00:16:36,320 --> 00:16:43,680
of all restore civility to our political

444
00:16:40,519 --> 00:16:45,759
discourse. So a big thanks to CIRC and

445
00:16:43,680 --> 00:16:47,440
Schwarzman for enabling this work. It's

446
00:16:45,759 --> 00:16:49,120
relatively rare for those of us who

447
00:16:47,440 --> 00:16:51,440
study political philosophy and political

448
00:16:49,120 --> 00:16:53,600
theory to be working together with

449
00:16:51,440 --> 00:16:55,360
computer scientists. So we're very

450
00:16:53,600 --> 00:16:57,600
grateful to CIRC for making this

451
00:16:55,360 --> 00:17:01,560
possible. And I should also say that I'm

452
00:16:57,600 --> 00:17:04,240
gonna flip hopefully um to the beginning

453
00:17:01,560 --> 00:17:06,400
because I failed to talk about the

454
00:17:04,240 --> 00:17:08,480
research team. Um this is Copiad with

455
00:17:06,400 --> 00:17:10,400
Sandy Pentland, my colleague in computer

456
00:17:08,480 --> 00:17:13,039
science. And we're joined by a very

457
00:17:10,400 --> 00:17:14,880
talented research team of grad students,

458
00:17:13,039 --> 00:17:18,160
postocs, and research scientists,

459
00:17:14,880 --> 00:17:20,400
including Lula Chen, um GVLab's research

460
00:17:18,160 --> 00:17:22,640
director who's here with us today. So I

461
00:17:20,400 --> 00:17:24,160
look forward to the questions from all

462
00:17:22,640 --> 00:17:28,839
of you. And I'm going to ask Lula

463
00:17:24,160 --> 00:17:28,839
actually to join for the Q&A. Thank you.

464
00:17:33,120 --> 00:17:37,320
Thank you very much. Lily questions.

465
00:17:42,559 --> 00:17:48,080
Uh so you you use the the the phrase

466
00:17:45,440 --> 00:17:51,200
like this is the AI uh supporting that.

467
00:17:48,080 --> 00:17:54,400
Can you say more about which AI is that

468
00:17:51,200 --> 00:17:57,679
your your like customized AI or from a

469
00:17:54,400 --> 00:18:01,280
company and and uh would this AI be able

470
00:17:57,679 --> 00:18:03,520
to get the different uh opinions come to

471
00:18:01,280 --> 00:18:05,760
like more close to the central? Would

472
00:18:03,520 --> 00:18:07,600
that also be able to shift this central

473
00:18:05,760 --> 00:18:11,280
point toward this way or toward that

474
00:18:07,600 --> 00:18:13,440
way? Yeah, we do use I mean we do see

475
00:18:11,280 --> 00:18:16,160
them shifting opinions and moving people

476
00:18:13,440 --> 00:18:18,320
more to the middle. But um if I'm not

477
00:18:16,160 --> 00:18:19,840
mistaken, this is based on GPT40, right

478
00:18:18,320 --> 00:18:22,960
Minnie? Yeah. Um do you want to say

479
00:18:19,840 --> 00:18:25,520
anything more about how it was um the

480
00:18:22,960 --> 00:18:27,440
prompt engineering that we used? Sure.

481
00:18:25,520 --> 00:18:30,960
Actually, oh you might have to just

482
00:18:27,440 --> 00:18:32,320
raise I'll just speak loudly. Um so u

483
00:18:30,960 --> 00:18:36,000
maybe I can address the shifting

484
00:18:32,320 --> 00:18:38,240
opinions part. Um so in our um in the

485
00:18:36,000 --> 00:18:39,840
design of this we really want people to

486
00:18:38,240 --> 00:18:42,000
think through their opinions but we

487
00:18:39,840 --> 00:18:44,480
don't necessarily try to shift opinions.

488
00:18:42,000 --> 00:18:46,799
So, for example, with Socratic dialogue,

489
00:18:44,480 --> 00:18:48,640
uh the the way that the AI prompt works

490
00:18:46,799 --> 00:18:50,720
and what we're asking people to do is to

491
00:18:48,640 --> 00:18:52,080
reason through why they think what it is

492
00:18:50,720 --> 00:18:54,080
that they think, but we don't

493
00:18:52,080 --> 00:18:56,160
necessarily say, well, then you should,

494
00:18:54,080 --> 00:18:58,960
you know, the AI then says you should

495
00:18:56,160 --> 00:19:00,640
think XYZ, something like that. So,

496
00:18:58,960 --> 00:19:02,880
we're not trying to shift opinions in

497
00:19:00,640 --> 00:19:04,720
that way. Um, and then for for the

498
00:19:02,880 --> 00:19:06,160
system prompting, we've been doing uh

499
00:19:04,720 --> 00:19:08,480
it's mainly just prompt engineering that

500
00:19:06,160 --> 00:19:10,799
we're working on here. uh in terms of uh

501
00:19:08,480 --> 00:19:13,120
some of the specific cases like with DC

502
00:19:10,799 --> 00:19:15,440
we are thinking there are some values

503
00:19:13,120 --> 00:19:17,760
and rules that DC wants to have for the

504
00:19:15,440 --> 00:19:19,760
for uh making sure that that interaction

505
00:19:17,760 --> 00:19:20,960
with the AI goes well for citizens so we

506
00:19:19,760 --> 00:19:22,799
do have some training that will happen

507
00:19:20,960 --> 00:19:24,559
with with that but in this first in

508
00:19:22,799 --> 00:19:29,240
these iterations we're just doing system

509
00:19:24,559 --> 00:19:29,240
prompting uh prompt engineering that way

510
00:19:30,720 --> 00:19:34,720
thank you for your talk that felt

511
00:19:32,080 --> 00:19:37,200
actually very uplifting uh we've talked

512
00:19:34,720 --> 00:19:39,480
a little bit about introducing bias when

513
00:19:37,200 --> 00:19:42,400
we use an LM, how do you make sure your

514
00:19:39,480 --> 00:19:45,840
moderator moderated chatbot doesn't

515
00:19:42,400 --> 00:19:46,880
introduce bias? Uh, pretty much Yeah, I

516
00:19:45,840 --> 00:19:50,039
mean, I think this goes back to what

517
00:19:46,880 --> 00:19:52,799
Lulu was saying that the the chatbot is

518
00:19:50,039 --> 00:19:55,200
supporting reflection. So, it's actually

519
00:19:52,799 --> 00:19:57,280
taking what the person themselves says

520
00:19:55,200 --> 00:19:59,840
and just asking what's your reason for

521
00:19:57,280 --> 00:20:03,760
that? Um, or what do you mean by X? Um

522
00:19:59,840 --> 00:20:07,520
so there's no um there's no the chatbot

523
00:20:03,760 --> 00:20:09,600
is not engineered to nudge them in any

524
00:20:07,520 --> 00:20:11,520
particular direction which is why it's

525
00:20:09,600 --> 00:20:13,280
so amazing that once you support a

526
00:20:11,520 --> 00:20:14,960
little bit of reflection like what are

527
00:20:13,280 --> 00:20:16,960
my reasons for that or why do I think

528
00:20:14,960 --> 00:20:18,760
that that you actually do see people

529
00:20:16,960 --> 00:20:23,039
moving towards the

530
00:20:18,760 --> 00:20:25,679
middle. Is that fair to say? Yeah.

531
00:20:23,039 --> 00:20:27,440
Uh hi Lily this is Katherine from urban

532
00:20:25,679 --> 00:20:29,919
planning. Uh thank you so much for a

533
00:20:27,440 --> 00:20:33,200
great presentation. I want to ask you

534
00:20:29,919 --> 00:20:35,120
all about the Overton window, like this

535
00:20:33,200 --> 00:20:37,600
idea that there's like a kind of sphere

536
00:20:35,120 --> 00:20:39,679
of reasonable discourse under which we

537
00:20:37,600 --> 00:20:41,360
can um debate and then certain things

538
00:20:39,679 --> 00:20:44,000
fall outside of that. And I think one of

539
00:20:41,360 --> 00:20:45,919
the things that's so maybe challenging

540
00:20:44,000 --> 00:20:48,720
and polarizing about the current moment

541
00:20:45,919 --> 00:20:51,200
is that both like the statements and the

542
00:20:48,720 --> 00:20:55,600
actions um of both the current

543
00:20:51,200 --> 00:20:58,159
administration and and others as well um

544
00:20:55,600 --> 00:20:59,760
h are are shifting the Overton window.

545
00:20:58,159 --> 00:21:01,520
And so so it makes me think about like

546
00:20:59,760 --> 00:21:04,799
the questions that you're posing here

547
00:21:01,520 --> 00:21:06,960
and like who would be uh engineering the

548
00:21:04,799 --> 00:21:09,360
questions that would be the subject of

549
00:21:06,960 --> 00:21:12,159
deliberation and debate because it seems

550
00:21:09,360 --> 00:21:13,840
like what's um really happening uh at

551
00:21:12,159 --> 00:21:16,559
least in this current administration is

552
00:21:13,840 --> 00:21:18,720
really pushing on that Overton window

553
00:21:16,559 --> 00:21:20,400
such that now we're going to be debating

554
00:21:18,720 --> 00:21:22,080
you know should we be kidnapping

555
00:21:20,400 --> 00:21:24,480
students off the street for writing

556
00:21:22,080 --> 00:21:26,000
opeds? You know what I mean? um which

557
00:21:24,480 --> 00:21:28,559
which feels like an entirely

558
00:21:26,000 --> 00:21:30,080
unreasonable thing to be debating about

559
00:21:28,559 --> 00:21:31,840
and like I don't want to like sort of

560
00:21:30,080 --> 00:21:33,760
move my position on that in any way

561
00:21:31,840 --> 00:21:36,000
whatsoever but but so I guess can you

562
00:21:33,760 --> 00:21:39,159
comment on that and sort of like how do

563
00:21:36,000 --> 00:21:41,760
we how do we think about what is even a

564
00:21:39,159 --> 00:21:44,000
reasonable question to come together

565
00:21:41,760 --> 00:21:46,880
around when we are living this moment of

566
00:21:44,000 --> 00:21:49,600
kind of like extreme actions and

567
00:21:46,880 --> 00:21:52,159
complete norm busting that is going on

568
00:21:49,600 --> 00:21:53,440
right now. Yeah. Yeah. Um, I mean, I'll

569
00:21:52,159 --> 00:21:54,640
turn to you, Lula, because I'd be

570
00:21:53,440 --> 00:21:56,559
interested to know what your question

571
00:21:54,640 --> 00:21:57,799
your answer is, too. But, but Katherine,

572
00:21:56,559 --> 00:22:00,320
I think, you

573
00:21:57,799 --> 00:22:02,559
know, if I could answer if I could, if I

574
00:22:00,320 --> 00:22:04,400
had a good answer to that question, you

575
00:22:02,559 --> 00:22:06,240
know, we'd be all set, I think, in a

576
00:22:04,400 --> 00:22:09,360
way. But I think what I would say is

577
00:22:06,240 --> 00:22:12,960
that my hope would be technologies like

578
00:22:09,360 --> 00:22:14,480
this that are designed to be to uphold

579
00:22:12,960 --> 00:22:16,480
pro-democratic values and principles,

580
00:22:14,480 --> 00:22:18,240
make it easy for anybody to ask a

581
00:22:16,480 --> 00:22:21,679
question. That's number one. And number

582
00:22:18,240 --> 00:22:23,760
two, because they're designed and tested

583
00:22:21,679 --> 00:22:30,080
to be pro-democratic and pro-social,

584
00:22:23,760 --> 00:22:31,679
that um norms that promote respect and

585
00:22:30,080 --> 00:22:34,080
inclusiveness

586
00:22:31,679 --> 00:22:38,080
um and open-ended curiosity and

587
00:22:34,080 --> 00:22:40,559
open-ended um um consideration are

588
00:22:38,080 --> 00:22:43,679
easier to uphold in this kind in this

589
00:22:40,559 --> 00:22:45,280
kind of online environment. um that um

590
00:22:43,679 --> 00:22:47,600
part of the problem of course is that

591
00:22:45,280 --> 00:22:49,440
the social norms that exist in quote

592
00:22:47,600 --> 00:22:51,679
social media which is really antisocial

593
00:22:49,440 --> 00:22:56,159
actually promote don't enable that and

594
00:22:51,679 --> 00:22:58,480
so therefore um perhaps overly narrow

595
00:22:56,159 --> 00:22:59,840
the Overton window beyond what it can be

596
00:22:58,480 --> 00:23:02,240
but I don't know if Lula you want to add

597
00:22:59,840 --> 00:23:04,320
to that. I I agree with Lily there. I

598
00:23:02,240 --> 00:23:06,799
guess the other I hope it's not a cheap

599
00:23:04,320 --> 00:23:08,799
answer, but um at least when we're

600
00:23:06,799 --> 00:23:10,240
testing this in the field, we are really

601
00:23:08,799 --> 00:23:12,640
working with governments and we're

602
00:23:10,240 --> 00:23:15,039
trying to find questions that are very

603
00:23:12,640 --> 00:23:16,080
practical that people are actually care

604
00:23:15,039 --> 00:23:18,400
about, you know, where should we put

605
00:23:16,080 --> 00:23:20,080
bike lanes, those types of things. Um so

606
00:23:18,400 --> 00:23:21,919
we're trying to keep that topic uh

607
00:23:20,080 --> 00:23:23,120
narrow at least in these field tests

608
00:23:21,919 --> 00:23:24,960
that we're doing. So that's where we're

609
00:23:23,120 --> 00:23:27,360
constraining it. But once it's open, I I

610
00:23:24,960 --> 00:23:30,400
agree with Lily. You know, it we'll

611
00:23:27,360 --> 00:23:33,679
we'll we'll hope to see more uh of these

612
00:23:30,400 --> 00:23:36,240
good norms coming about.

613
00:23:33,679 --> 00:23:39,240
Okay, let's thank uh Lily and Lula

614
00:23:36,240 --> 00:23:39,240
again.

