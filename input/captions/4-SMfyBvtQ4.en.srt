1
00:00:00,040 --> 00:00:02,760
it's a pleasure to be here with all of

2
00:00:01,360 --> 00:00:07,120
you

3
00:00:02,760 --> 00:00:10,120
today thank you thank you very very much

4
00:00:07,120 --> 00:00:12,400
I'm just going to reset that timer

5
00:00:10,120 --> 00:00:15,280
there there we

6
00:00:12,400 --> 00:00:17,400
are so I recognize that most of you are

7
00:00:15,280 --> 00:00:19,960
probably not neuroscientists and I want

8
00:00:17,400 --> 00:00:22,760
to try to orient you a little bit about

9
00:00:19,960 --> 00:00:24,400
the general approach that we're taking

10
00:00:22,760 --> 00:00:26,640
in the field of Neuroscience before I

11
00:00:24,400 --> 00:00:27,519
dive into a specific story I'd like to

12
00:00:26,640 --> 00:00:30,840
tell you

13
00:00:27,519 --> 00:00:32,480
about so here's generally how I think

14
00:00:30,840 --> 00:00:33,760
about what we're trying to do or

15
00:00:32,480 --> 00:00:35,480
accomplish as

16
00:00:33,760 --> 00:00:37,239
neuroscientists we're first of all

17
00:00:35,480 --> 00:00:38,760
trying to understand the human condition

18
00:00:37,239 --> 00:00:41,719
what is it that makes us human and why

19
00:00:38,760 --> 00:00:43,160
do we do what we do peripherally or

20
00:00:41,719 --> 00:00:44,800
relatedly we're trying to understand

21
00:00:43,160 --> 00:00:47,199
other species they do amazing things

22
00:00:44,800 --> 00:00:48,840
that we can't do in really fantastic

23
00:00:47,199 --> 00:00:51,399
ways and understanding things about our

24
00:00:48,840 --> 00:00:53,640
place in the world is really informative

25
00:00:51,399 --> 00:00:56,079
obviously we want to kind of Leverage

26
00:00:53,640 --> 00:00:58,320
these pieces of information we glean

27
00:00:56,079 --> 00:01:01,800
from these investigations to treat brain

28
00:00:58,320 --> 00:01:04,439
disease and I ideally improve artificial

29
00:01:01,800 --> 00:01:07,680
neural systems as

30
00:01:04,439 --> 00:01:09,520
well I think about two kinds of ways of

31
00:01:07,680 --> 00:01:11,600
thinking generally in the field for how

32
00:01:09,520 --> 00:01:14,520
to approach these goals most people

33
00:01:11,600 --> 00:01:18,200
subscribe to maybe one or the other the

34
00:01:14,520 --> 00:01:19,840
first is brains biological brains do all

35
00:01:18,200 --> 00:01:22,640
this amazing incredible stuff they make

36
00:01:19,840 --> 00:01:25,320
us human they're what they're why we are

37
00:01:22,640 --> 00:01:28,759
what we are but it is really complicated

38
00:01:25,320 --> 00:01:31,640
and really messy and the past 140 years

39
00:01:28,759 --> 00:01:35,560
or so of Neuroscience has really dived

40
00:01:31,640 --> 00:01:37,960
into this sort of diverse glory of the

41
00:01:35,560 --> 00:01:39,680
brain but it's been hard to get to

42
00:01:37,960 --> 00:01:41,920
general principles and really deep

43
00:01:39,680 --> 00:01:44,360
understanding now an alternative

44
00:01:41,920 --> 00:01:46,840
approach has sort of focused on the

45
00:01:44,360 --> 00:01:49,119
engineering aspects of things can we

46
00:01:46,840 --> 00:01:51,040
distill a few principles out of

47
00:01:49,119 --> 00:01:53,280
neurobiology and build artificial

48
00:01:51,040 --> 00:01:55,360
systems and use those to a help us

49
00:01:53,280 --> 00:01:57,399
understand how brains do things but also

50
00:01:55,360 --> 00:01:59,119
build things that have broad impact in

51
00:01:57,399 --> 00:02:00,880
the world of course that's been

52
00:01:59,119 --> 00:02:03,000
fantastically successful over the past

53
00:02:00,880 --> 00:02:06,240
10 years but it took an enormous amount

54
00:02:03,000 --> 00:02:08,160
of effort over the past 50 before that

55
00:02:06,240 --> 00:02:10,080
to see that come to fruition and it

56
00:02:08,160 --> 00:02:12,440
wasn't clear that it

57
00:02:10,080 --> 00:02:15,080
would so I want to spend a few minutes

58
00:02:12,440 --> 00:02:17,440
and talk about how artificial neural

59
00:02:15,080 --> 00:02:20,400
networks came to work so well and be so

60
00:02:17,440 --> 00:02:22,400
powerful because the interplay between

61
00:02:20,400 --> 00:02:24,200
the study of the biology of the brain

62
00:02:22,400 --> 00:02:25,879
and building artificial systems is where

63
00:02:24,200 --> 00:02:27,840
my lab lives and where I think a lot of

64
00:02:25,879 --> 00:02:29,640
future impact is going to be made in the

65
00:02:27,840 --> 00:02:31,640
field of Neuroscience so this is your

66
00:02:29,640 --> 00:02:33,400
just General cartoon of a neural network

67
00:02:31,640 --> 00:02:35,959
there's inputs there's a hidden layer

68
00:02:33,400 --> 00:02:37,760
and there's some outputs but in order to

69
00:02:35,959 --> 00:02:39,519
make them work and build them this way

70
00:02:37,760 --> 00:02:43,120
and figure out how to make them work so

71
00:02:39,519 --> 00:02:46,000
well we needed to take some important

72
00:02:43,120 --> 00:02:47,400
abstracted ideas away from decades and

73
00:02:46,000 --> 00:02:49,840
decades worth of

74
00:02:47,400 --> 00:02:51,400
biology one of the principles that seems

75
00:02:49,840 --> 00:02:53,080
to be necessary to make these kinds of

76
00:02:51,400 --> 00:02:55,720
systems work well is they have to be

77
00:02:53,080 --> 00:02:57,120
formed of discret units like this one

78
00:02:55,720 --> 00:03:00,560
right here right each of these things

79
00:02:57,120 --> 00:03:02,879
they're discreet units they have some

80
00:03:00,560 --> 00:03:04,480
kind of threshold or nonlinearity

81
00:03:02,879 --> 00:03:06,920
associated with them about how they

82
00:03:04,480 --> 00:03:08,760
integrate their inputs Ru is the really

83
00:03:06,920 --> 00:03:10,640
common one that lots of folks use in

84
00:03:08,760 --> 00:03:13,239
their Anns but there's lots of others

85
00:03:10,640 --> 00:03:17,000
they're connected by lots and lots and

86
00:03:13,239 --> 00:03:20,319
lots of synapses individual synapses and

87
00:03:17,000 --> 00:03:22,360
those synapses have specific weights and

88
00:03:20,319 --> 00:03:25,360
those weights are

89
00:03:22,360 --> 00:03:27,920
modifiable those principles weren't just

90
00:03:25,360 --> 00:03:30,760
invented by Engineers these things came

91
00:03:27,920 --> 00:03:32,519
from Neuroscience so how did we get to

92
00:03:30,760 --> 00:03:34,599
this point I'm going to give you the

93
00:03:32,519 --> 00:03:36,760
history of all of neurophysiology in one

94
00:03:34,599 --> 00:03:38,599
slide are you ready so we started with

95
00:03:36,760 --> 00:03:40,879
kahal at the turn of the previous

96
00:03:38,599 --> 00:03:42,760
Century defining the neuron Doctrine

97
00:03:40,879 --> 00:03:44,599
neurons are in fact independent units

98
00:03:42,760 --> 00:03:46,560
it's not some kind of cium like blood

99
00:03:44,599 --> 00:03:49,080
vessels they're independent cells doing

100
00:03:46,560 --> 00:03:51,200
their own kind of independent thing but

101
00:03:49,080 --> 00:03:53,000
they're connected to one another and

102
00:03:51,200 --> 00:03:54,560
they signal through Action potentials

103
00:03:53,000 --> 00:03:56,799
the mechanisms of the action potentials

104
00:03:54,560 --> 00:04:00,920
were defined Eckles hodkin and Huxley

105
00:03:56,799 --> 00:04:03,000
many others around the 1940s or so

106
00:04:00,920 --> 00:04:05,720
the evidence for synaptic connections

107
00:04:03,000 --> 00:04:08,120
between neurons was a prolonged uh

108
00:04:05,720 --> 00:04:10,319
period span of decades but it really

109
00:04:08,120 --> 00:04:13,079
solidified in the 50s uh with the

110
00:04:10,319 --> 00:04:15,120
quantal hypothesis from Bernard cats

111
00:04:13,079 --> 00:04:17,799
subsequently it was shown about two

112
00:04:15,120 --> 00:04:20,840
decades three decades later that those

113
00:04:17,799 --> 00:04:23,199
synapses could be modified dramatically

114
00:04:20,840 --> 00:04:25,000
by the activity in the network this was

115
00:04:23,199 --> 00:04:27,000
the work of bliss and Lomo in 73 the

116
00:04:25,000 --> 00:04:29,919
work of Eric kandell and many many

117
00:04:27,000 --> 00:04:31,479
others over the subsequent decades so

118
00:04:29,919 --> 00:04:35,639
together that formed the basis for how

119
00:04:31,479 --> 00:04:36,919
to build these neural networks but if we

120
00:04:35,639 --> 00:04:38,919
put these principles into neural

121
00:04:36,919 --> 00:04:41,039
networks as many many people did over

122
00:04:38,919 --> 00:04:43,919
the past you know 50 years or so they

123
00:04:41,039 --> 00:04:45,800
tend not to learn very well it's really

124
00:04:43,919 --> 00:04:49,039
difficult to make them do what you want

125
00:04:45,800 --> 00:04:51,199
them to do in any kind of efficient way

126
00:04:49,039 --> 00:04:52,639
something was missing and something that

127
00:04:51,199 --> 00:04:54,639
something was not something we could get

128
00:04:52,639 --> 00:04:57,240
from the biology instead it took

129
00:04:54,639 --> 00:04:58,880
engineering Insight from Jeff Hinton

130
00:04:57,240 --> 00:05:01,960
hopfield who just won the Nobel Prize

131
00:04:58,880 --> 00:05:04,400
and many many others you need control

132
00:05:01,960 --> 00:05:06,680
systems for learning these networks just

133
00:05:04,400 --> 00:05:09,720
wander off into weird local Minima they

134
00:05:06,680 --> 00:05:11,600
never converge takes forever Etc until

135
00:05:09,720 --> 00:05:14,360
you start to put control systems in

136
00:05:11,600 --> 00:05:15,880
there what do I mean by Control Systems

137
00:05:14,360 --> 00:05:17,280
well one of the things one of the

138
00:05:15,880 --> 00:05:19,600
control systems you've probably heard of

139
00:05:17,280 --> 00:05:22,360
is credit assignment that's the main

140
00:05:19,600 --> 00:05:24,039
thing that seems to run back propagation

141
00:05:22,360 --> 00:05:26,479
which is behind pretty much all of the

142
00:05:24,039 --> 00:05:28,560
advances Inns over the past you know

143
00:05:26,479 --> 00:05:30,919
decade or two so what's the credit

144
00:05:28,560 --> 00:05:32,720
assignment problem if we put some input

145
00:05:30,919 --> 00:05:36,160
into a network like a bunch of images

146
00:05:32,720 --> 00:05:39,000
and we ask the network is that image a

147
00:05:36,160 --> 00:05:41,600
dog it gets it right or it gets it wrong

148
00:05:39,000 --> 00:05:44,120
we want to use that outcome to train the

149
00:05:41,600 --> 00:05:46,160
network and make it better how do we go

150
00:05:44,120 --> 00:05:48,080
about doing that there's all these

151
00:05:46,160 --> 00:05:49,280
synapses all these units are connected

152
00:05:48,080 --> 00:05:51,160
to one another with all these synapses

153
00:05:49,280 --> 00:05:53,000
how do we know which synapses to turn up

154
00:05:51,160 --> 00:05:54,400
and turn down how much do we turn them

155
00:05:53,000 --> 00:05:56,319
up and turn them down in order to get

156
00:05:54,400 --> 00:05:58,800
the network to get

157
00:05:56,319 --> 00:06:00,560
better so one way to do that is to look

158
00:05:58,800 --> 00:06:02,600
at the output

159
00:06:00,560 --> 00:06:04,280
compare that to your target create an

160
00:06:02,600 --> 00:06:06,319
error signal and feed it back into the

161
00:06:04,280 --> 00:06:08,039
network the original ways we thought

162
00:06:06,319 --> 00:06:10,199
about doing this were really simple and

163
00:06:08,039 --> 00:06:12,000
kind of global or scalar it's the way we

164
00:06:10,199 --> 00:06:13,960
think about neuromodulation in the brain

165
00:06:12,000 --> 00:06:15,880
happening A big flood of dopamine says

166
00:06:13,960 --> 00:06:18,880
something good happened but it doesn't

167
00:06:15,880 --> 00:06:21,759
contain specific information about which

168
00:06:18,880 --> 00:06:24,240
synapses or which cells drove the good

169
00:06:21,759 --> 00:06:27,039
outcome so learning from that really

170
00:06:24,240 --> 00:06:28,720
broad signal is pretty difficult the

171
00:06:27,039 --> 00:06:31,960
thing that really changed the game was

172
00:06:28,720 --> 00:06:35,479
figuring out how to vectorize the error

173
00:06:31,960 --> 00:06:37,800
signals and provide specific information

174
00:06:35,479 --> 00:06:40,080
back to individual synapses to say you

175
00:06:37,800 --> 00:06:41,960
were really involved in making that

176
00:06:40,080 --> 00:06:43,080
error I need to turn you down you were

177
00:06:41,960 --> 00:06:45,160
really involved in making the right

178
00:06:43,080 --> 00:06:47,280
thing I need to turn you up so that's

179
00:06:45,160 --> 00:06:49,479
what runs basically the machine learning

180
00:06:47,280 --> 00:06:50,639
Revolution uh that so changed our lives

181
00:06:49,479 --> 00:06:53,639
in our

182
00:06:50,639 --> 00:06:56,520
world but it's not clear that this is

183
00:06:53,639 --> 00:06:58,800
how brains learn it's a super powerful

184
00:06:56,520 --> 00:07:00,759
algorithm neuroscientists like me have

185
00:06:58,800 --> 00:07:03,080
gotten more and more and more interested

186
00:07:00,759 --> 00:07:05,000
in it as a model for potentially how

187
00:07:03,080 --> 00:07:06,160
brains learn it obviously works super

188
00:07:05,000 --> 00:07:08,520
well for machine learning and

189
00:07:06,160 --> 00:07:10,599
Engineering but is it how the biology

190
00:07:08,520 --> 00:07:12,720
implements learning there isn't an

191
00:07:10,599 --> 00:07:14,639
alternative really at this point

192
00:07:12,720 --> 00:07:17,400
learning in biological systems is really

193
00:07:14,639 --> 00:07:19,759
poorly understood so we set out to look

194
00:07:17,400 --> 00:07:22,759
for evidence of this kind of system in

195
00:07:19,759 --> 00:07:25,360
the Mamon brain there isn't any so

196
00:07:22,759 --> 00:07:26,960
far back propagation or vectorized error

197
00:07:25,360 --> 00:07:29,919
signaling has generally been thought to

198
00:07:26,960 --> 00:07:33,000
be implausible or biologically

199
00:07:29,919 --> 00:07:35,720
infeasible for a couple of reasons one

200
00:07:33,000 --> 00:07:38,879
is if you have a feedback kind of input

201
00:07:35,720 --> 00:07:40,960
that contains error and a regular input

202
00:07:38,879 --> 00:07:42,840
the feed forward input they interact

203
00:07:40,960 --> 00:07:45,080
with one another and interfere you can't

204
00:07:42,840 --> 00:07:46,199
have them going on at the same time even

205
00:07:45,080 --> 00:07:48,039
if you do this in regular neural

206
00:07:46,199 --> 00:07:49,639
networks they don't it doesn't work so

207
00:07:48,039 --> 00:07:51,479
the way this is solved in normal neural

208
00:07:49,639 --> 00:07:53,639
networks is temporal separation you have

209
00:07:51,479 --> 00:07:55,800
a feed forward pass then you have a

210
00:07:53,639 --> 00:07:56,639
feedback pass and they don't interfere

211
00:07:55,800 --> 00:07:58,800
with one another because you've

212
00:07:56,639 --> 00:08:00,360
separated them in time the brain does

213
00:07:58,800 --> 00:08:03,240
not work like this the brain is

214
00:08:00,360 --> 00:08:05,039
continuous it's possible that there is a

215
00:08:03,240 --> 00:08:06,720
separation hiding somewhere in one of

216
00:08:05,039 --> 00:08:08,280
the rhythms of the brain but so far at

217
00:08:06,720 --> 00:08:11,080
least to my knowledge it hasn't been

218
00:08:08,280 --> 00:08:13,240
found there's an alternative idea though

219
00:08:11,080 --> 00:08:15,919
that potentially you could

220
00:08:13,240 --> 00:08:17,400
separate feed forward and feedback

221
00:08:15,919 --> 00:08:20,520
passes

222
00:08:17,400 --> 00:08:22,280
spatially in the dendrites of neurons

223
00:08:20,520 --> 00:08:24,000
individual brain cells have these really

224
00:08:22,280 --> 00:08:26,240
complicated processes that look like

225
00:08:24,000 --> 00:08:28,159
branches of trees that are called

226
00:08:26,240 --> 00:08:29,720
dendrites and so this idea has been

227
00:08:28,159 --> 00:08:31,840
around computationally for quite some

228
00:08:29,720 --> 00:08:34,159
time but has never been experimentally

229
00:08:31,840 --> 00:08:35,560
tested so here's an example of one of

230
00:08:34,159 --> 00:08:37,360
these neurons this is a layer five

231
00:08:35,560 --> 00:08:39,440
paramal neuron this is from a mouse but

232
00:08:37,360 --> 00:08:41,399
you have basically the same neurons in

233
00:08:39,440 --> 00:08:42,839
your cortex you have lots and lots and

234
00:08:41,399 --> 00:08:44,360
lots of them a lot more than this mouse

235
00:08:42,839 --> 00:08:46,080
does but this is what they look like

236
00:08:44,360 --> 00:08:48,680
they're the sort of heavy lifters

237
00:08:46,080 --> 00:08:51,040
computationally for the cortex and

238
00:08:48,680 --> 00:08:53,680
generally speaking bottom up kind of

239
00:08:51,040 --> 00:08:56,360
feed forward information comes in down

240
00:08:53,680 --> 00:08:58,360
here around where the cell body is in

241
00:08:56,360 --> 00:09:00,000
the nucleus and this is where the output

242
00:08:58,360 --> 00:09:03,760
of the cell is as well

243
00:09:00,000 --> 00:09:06,600
but more complex higher order kinds of

244
00:09:03,760 --> 00:09:08,640
inputs arrive out here in this really

245
00:09:06,600 --> 00:09:11,480
distal extended process that looks more

246
00:09:08,640 --> 00:09:14,399
like the extended sort of crown uh of a

247
00:09:11,480 --> 00:09:16,200
tree and it's thought that maybe air

248
00:09:14,399 --> 00:09:19,320
signals or Target signals might be

249
00:09:16,200 --> 00:09:22,320
present out there and so we wanted to

250
00:09:19,320 --> 00:09:24,000
test that this idea that dendrites could

251
00:09:22,320 --> 00:09:27,000
maybe mediate some kind of vectorized

252
00:09:24,000 --> 00:09:28,360
error signaling in the brain now there's

253
00:09:27,000 --> 00:09:30,760
a big problem with doing this there's a

254
00:09:28,360 --> 00:09:34,640
reason no one's done this before for is

255
00:09:30,760 --> 00:09:37,160
how do you identify error signals in for

256
00:09:34,640 --> 00:09:39,839
example a mouse doing a task in a neural

257
00:09:37,160 --> 00:09:41,839
network we have access to every single

258
00:09:39,839 --> 00:09:43,200
cell synapse whatever possible

259
00:09:41,839 --> 00:09:44,720
measurement we could want we just get in

260
00:09:43,200 --> 00:09:47,880
there and look at it we built the thing

261
00:09:44,720 --> 00:09:49,640
right the mouse we do not it's very much

262
00:09:47,880 --> 00:09:51,160
through a scanner darkly we can put some

263
00:09:49,640 --> 00:09:52,680
electrodes in and maybe sample a few

264
00:09:51,160 --> 00:09:55,680
cells we can turn on a microscope and

265
00:09:52,680 --> 00:09:57,120
Sample a few cells but we cannot record

266
00:09:55,680 --> 00:09:58,240
from the millions and millions and

267
00:09:57,120 --> 00:10:00,560
millions of cells that are probably

268
00:09:58,240 --> 00:10:03,440
involved in this task

269
00:10:00,560 --> 00:10:05,680
so how could you possibly identify error

270
00:10:03,440 --> 00:10:07,720
signals in the neurons that are actually

271
00:10:05,680 --> 00:10:10,640
involved in the task that has thus far

272
00:10:07,720 --> 00:10:13,320
stymy the field my amazing postto

273
00:10:10,640 --> 00:10:14,839
Valerio frion came to my lab to do

274
00:10:13,320 --> 00:10:17,560
something completely different and when

275
00:10:14,839 --> 00:10:19,320
he arrived here quarantine happened and

276
00:10:17,560 --> 00:10:20,760
he was locked in his apartment for six

277
00:10:19,320 --> 00:10:22,640
months pretty much he couldn't come to

278
00:10:20,760 --> 00:10:25,839
my lab he couldn't do anything while he

279
00:10:22,640 --> 00:10:27,440
was in his room for six months he

280
00:10:25,839 --> 00:10:29,399
thought of this great idea to use a

281
00:10:27,440 --> 00:10:32,720
brain computer interface to Short

282
00:10:29,399 --> 00:10:35,120
circuit the rest of the brain and make

283
00:10:32,720 --> 00:10:38,440
just the neurons he was interested in be

284
00:10:35,120 --> 00:10:41,120
the ones driving the task and so here's

285
00:10:38,440 --> 00:10:43,399
the task design he's got a microscope

286
00:10:41,120 --> 00:10:44,880
that peers into the head of a mouse the

287
00:10:43,399 --> 00:10:47,120
Mouse is awake and behaving he's just

288
00:10:44,880 --> 00:10:49,720
sort of bolted down to the microscope

289
00:10:47,120 --> 00:10:51,200
there's a glass window in his skull and

290
00:10:49,720 --> 00:10:52,160
we're going to image the activity of

291
00:10:51,200 --> 00:10:54,760
some

292
00:10:52,160 --> 00:10:56,160
neurons the activity of one population

293
00:10:54,760 --> 00:10:58,160
of neurons that we've defined we're

294
00:10:56,160 --> 00:11:00,320
going to call it p+ it's literally four

295
00:10:58,160 --> 00:11:03,600
neurons

296
00:11:00,320 --> 00:11:05,120
rotates a visual stimulus and if the

297
00:11:03,600 --> 00:11:06,839
animal rotates it Far Enough by

298
00:11:05,120 --> 00:11:08,000
activating those P positive neurons the

299
00:11:06,839 --> 00:11:10,360
animal will get a

300
00:11:08,000 --> 00:11:11,839
reward that's counterbalanced by the

301
00:11:10,360 --> 00:11:14,519
activity of this other population the

302
00:11:11,839 --> 00:11:16,800
blue ones P minus the more the blue ones

303
00:11:14,519 --> 00:11:18,160
get activated the visual stimulus

304
00:11:16,800 --> 00:11:19,399
rotates counterclockwise and the animal

305
00:11:18,160 --> 00:11:22,480
gets farther and farther from getting a

306
00:11:19,399 --> 00:11:24,360
reward the mice learn to do this really

307
00:11:22,480 --> 00:11:26,680
very effectively they can basically just

308
00:11:24,360 --> 00:11:29,560
sit there and think real hard activate

309
00:11:26,680 --> 00:11:32,279
p+ activate p+ and the visual stim

310
00:11:29,560 --> 00:11:36,120
rotates and they get rewards they get

311
00:11:32,279 --> 00:11:38,320
very very good at doing this and now the

312
00:11:36,120 --> 00:11:40,880
entire task is just dependent on these

313
00:11:38,320 --> 00:11:42,720
four neurons that we are

314
00:11:40,880 --> 00:11:44,160
watching actually it's the two

315
00:11:42,720 --> 00:11:45,519
populations actually eight neurons that

316
00:11:44,160 --> 00:11:47,240
we're watching there's a bunch of other

317
00:11:45,519 --> 00:11:48,800
neurons that we're recording that are

318
00:11:47,240 --> 00:11:52,160
involved in the task but they're not the

319
00:11:48,800 --> 00:11:53,959
ones that actually drive the task so

320
00:11:52,160 --> 00:11:56,279
this is what these neurons look like in

321
00:11:53,959 --> 00:11:57,800
the brain this is a sort of sideon view

322
00:11:56,279 --> 00:11:59,079
that's the surface of the brain all the

323
00:11:57,800 --> 00:12:01,160
way at the top that'd be down deep in

324
00:11:59,079 --> 00:12:02,360
the the brain this is similar to what I

325
00:12:01,160 --> 00:12:03,560
showed you for that layer five paramal

326
00:12:02,360 --> 00:12:05,079
neuron here's the cell bodies and

327
00:12:03,560 --> 00:12:07,079
there's there dendrites going up to the

328
00:12:05,079 --> 00:12:09,440
top where we think that air signal is

329
00:12:07,079 --> 00:12:11,120
present so we don't see it like this

330
00:12:09,440 --> 00:12:12,560
when we're doing the experiment we see

331
00:12:11,120 --> 00:12:13,880
it from the top down because we're

332
00:12:12,560 --> 00:12:15,959
looking down through the window and the

333
00:12:13,880 --> 00:12:18,920
surface of the brain so it looks like

334
00:12:15,959 --> 00:12:21,199
that so all of that blinking is the

335
00:12:18,920 --> 00:12:22,760
mouse thinking that's neural activity

336
00:12:21,199 --> 00:12:25,160
distributed across lots and lots of

337
00:12:22,760 --> 00:12:26,760
neurons flashing so those are the action

338
00:12:25,160 --> 00:12:29,880
potentials and the nerve impulses

339
00:12:26,760 --> 00:12:31,360
recorded microscopically

340
00:12:29,880 --> 00:12:33,240
so what we're going to do is rather than

341
00:12:31,360 --> 00:12:36,160
just look at those were the Somas we're

342
00:12:33,240 --> 00:12:38,279
going to compare Somas with dendrites in

343
00:12:36,160 --> 00:12:41,000
an attempt to see the feed forward and

344
00:12:38,279 --> 00:12:43,760
the feed back kinds of error signals one

345
00:12:41,000 --> 00:12:46,800
of the things we observed right away was

346
00:12:43,760 --> 00:12:49,440
that individual responses just randomly

347
00:12:46,800 --> 00:12:50,959
selected in any given neuron were really

348
00:12:49,440 --> 00:12:53,560
different in terms of their relative

349
00:12:50,959 --> 00:12:55,320
ratio amplitude so sometimes a dendrit

350
00:12:53,560 --> 00:12:56,519
is bigger sometimes the S is bigger and

351
00:12:55,320 --> 00:12:58,880
sometimes it's really dramatically

352
00:12:56,519 --> 00:13:01,199
bigger so here's just a plot of that for

353
00:12:58,880 --> 00:13:02,240
a bunch of samples from this one neuron

354
00:13:01,199 --> 00:13:04,040
and you can see that there's a huge

355
00:13:02,240 --> 00:13:06,160
amount of variance so we called that the

356
00:13:04,040 --> 00:13:08,160
somad dendrite residual it's basically

357
00:13:06,160 --> 00:13:11,480
saying when is the dendrite very active

358
00:13:08,160 --> 00:13:12,560
when is the Soma very active right that

359
00:13:11,480 --> 00:13:14,320
in the interest of time I'm not going to

360
00:13:12,560 --> 00:13:15,760
go into the details is actually

361
00:13:14,320 --> 00:13:17,959
predicted reasonably well by the

362
00:13:15,760 --> 00:13:20,800
activity of the whole neural network

363
00:13:17,959 --> 00:13:22,240
around uh that neuron so that seemed to

364
00:13:20,800 --> 00:13:25,240
us to be like oh maybe this is a

365
00:13:22,240 --> 00:13:27,639
potential you know window into these air

366
00:13:25,240 --> 00:13:30,959
signals is this something we could

367
00:13:27,639 --> 00:13:34,600
perturb so we expressed an optogenetic

368
00:13:30,959 --> 00:13:37,440
actuator in a set of inhibitory neurons

369
00:13:34,600 --> 00:13:39,680
to try to turn these dendrites off while

370
00:13:37,440 --> 00:13:41,480
leaving the Somas intact that's what

371
00:13:39,680 --> 00:13:42,959
this is showing you the inhibitory

372
00:13:41,480 --> 00:13:44,399
neurons are up here in the Sur at the

373
00:13:42,959 --> 00:13:46,440
surface of the brain and there's the

374
00:13:44,399 --> 00:13:47,959
ones we're going to record and so you

375
00:13:46,440 --> 00:13:49,079
can see under control conditions just

376
00:13:47,959 --> 00:13:50,279
like what I showed you before there's

377
00:13:49,079 --> 00:13:51,600
lots of activity there's some

378
00:13:50,279 --> 00:13:54,240
differences in the amplitudes of the

379
00:13:51,600 --> 00:13:56,720
signals but when we turn these neurons

380
00:13:54,240 --> 00:13:58,320
on they inhibit these dendrites and you

381
00:13:56,720 --> 00:13:59,720
can see that the dendritic signals go

382
00:13:58,320 --> 00:14:02,000
down really

383
00:13:59,720 --> 00:14:04,639
dramatically there's just averages from

384
00:14:02,000 --> 00:14:07,120
that cell there's examples from almost

385
00:14:04,639 --> 00:14:09,920
2,000 cells it's a super robust super

386
00:14:07,120 --> 00:14:11,560
powerful effect so we can preferentially

387
00:14:09,920 --> 00:14:13,959
influence the activity of these dorites

388
00:14:11,560 --> 00:14:15,560
and just as a side note anesthesia does

389
00:14:13,959 --> 00:14:17,759
basically the same thing when we lightly

390
00:14:15,560 --> 00:14:19,160
anesthetize the mice which is congruent

391
00:14:17,759 --> 00:14:20,639
with how we think about anesthesia

392
00:14:19,160 --> 00:14:22,920
operating on top down things that

393
00:14:20,639 --> 00:14:27,320
connect different parts of the

394
00:14:22,920 --> 00:14:29,519
brain so these dendritic signals contain

395
00:14:27,320 --> 00:14:31,680
reward related information about the

396
00:14:29,519 --> 00:14:34,759
task we just broke up the task into

397
00:14:31,680 --> 00:14:36,440
different epochs before the outcome that

398
00:14:34,759 --> 00:14:38,880
would be like oh I got the reward okay

399
00:14:36,440 --> 00:14:41,079
great uh oh sorry after the outcome I

400
00:14:38,880 --> 00:14:42,600
got the reward okay great or I think I'm

401
00:14:41,079 --> 00:14:45,399
about to get the reward I'm not really

402
00:14:42,600 --> 00:14:47,399
sure and what you can see between these

403
00:14:45,399 --> 00:14:48,800
two populations this is just the data

404
00:14:47,399 --> 00:14:51,720
and then that's just the shuffle to do

405
00:14:48,800 --> 00:14:54,399
the statistics is there's a really

406
00:14:51,720 --> 00:14:57,720
robust representation in the dendrites

407
00:14:54,399 --> 00:15:00,399
of outcomes so the dendrites are very

408
00:14:57,720 --> 00:15:01,880
effectively predicting I got a reward or

409
00:15:00,399 --> 00:15:04,440
I didn't get a reward but what's

410
00:15:01,880 --> 00:15:07,800
incredible is the dendrites also are

411
00:15:04,440 --> 00:15:09,279
highly correlated with right before the

412
00:15:07,800 --> 00:15:11,040
animal gets the outcome it hasn't

413
00:15:09,279 --> 00:15:13,839
explicitly been told you did it right or

414
00:15:11,040 --> 00:15:15,680
you did it wrong the neural activity is

415
00:15:13,839 --> 00:15:18,240
predictive of

416
00:15:15,680 --> 00:15:20,000
that can we perturb these behaviorally

417
00:15:18,240 --> 00:15:21,800
relevant signals by perturbing the

418
00:15:20,000 --> 00:15:23,560
dendrites like I just showed you we do

419
00:15:21,800 --> 00:15:25,519
exactly the same experiment but while

420
00:15:23,560 --> 00:15:27,079
the mice are running the task right and

421
00:15:25,519 --> 00:15:29,560
you can see that these differences

422
00:15:27,079 --> 00:15:31,040
between here and here go away

423
00:15:29,560 --> 00:15:34,160
right for I don't know how many neurons

424
00:15:31,040 --> 00:15:36,959
it is 50 50ish neurons it's very

425
00:15:34,160 --> 00:15:38,120
robust so now this all looks like it's

426
00:15:36,959 --> 00:15:39,199
going in the right direction we see

427
00:15:38,120 --> 00:15:41,000
these dendritic signals they're

428
00:15:39,199 --> 00:15:44,199
behaviorally relevant they're related to

429
00:15:41,000 --> 00:15:46,399
reward we can perturb them can we see

430
00:15:44,199 --> 00:15:49,279
evidence as far as I know the first

431
00:15:46,399 --> 00:15:51,759
evidence for vectorized signals so we

432
00:15:49,279 --> 00:15:53,920
compare the p+ neurons the ones that

433
00:15:51,759 --> 00:15:55,920
decrease error with the p minus neurons

434
00:15:53,920 --> 00:16:00,639
the ones that increase

435
00:15:55,920 --> 00:16:02,120
air the hypothesis is that if vectorized

436
00:16:00,639 --> 00:16:04,600
signaling is happening we'll see

437
00:16:02,120 --> 00:16:06,040
differential signals depending on the

438
00:16:04,600 --> 00:16:07,880
causal contribution of the neural

439
00:16:06,040 --> 00:16:09,199
populations to the task but it's

440
00:16:07,880 --> 00:16:11,399
possible we won't it's possible we'll

441
00:16:09,199 --> 00:16:12,759
see sort of scaler things where air just

442
00:16:11,399 --> 00:16:15,240
goes up or goes down the same way in all

443
00:16:12,759 --> 00:16:16,759
the neurons or we won't see any real air

444
00:16:15,240 --> 00:16:18,480
representations those are our three

445
00:16:16,759 --> 00:16:20,199
Alternatives I obviously wouldn't be

446
00:16:18,480 --> 00:16:21,160
setting it up this way if I saw these

447
00:16:20,199 --> 00:16:23,199
two things because they're not as

448
00:16:21,160 --> 00:16:25,279
interesting right we do see evidence for

449
00:16:23,199 --> 00:16:27,959
vectorized error signaling and here it

450
00:16:25,279 --> 00:16:29,600
is here's an example neuron that when

451
00:16:27,959 --> 00:16:31,720
it's active when when it's dendrite is

452
00:16:29,600 --> 00:16:34,040
active error goes down you can see it

453
00:16:31,720 --> 00:16:36,480
right here for it decreases in errors

454
00:16:34,040 --> 00:16:38,440
versus increase in errors that neuron is

455
00:16:36,480 --> 00:16:39,880
causally contributing to the animal

456
00:16:38,440 --> 00:16:41,959
being able to do the task more

457
00:16:39,880 --> 00:16:44,360
effectively and you see exactly what you

458
00:16:41,959 --> 00:16:46,319
would expect the inverse in the P minus

459
00:16:44,360 --> 00:16:48,639
neurons and that happens across the

460
00:16:46,319 --> 00:16:52,360
population for lots and lots and lots of

461
00:16:48,639 --> 00:16:55,839
neurons additionally now can we perturb

462
00:16:52,360 --> 00:16:57,680
those signals and actually degrade the

463
00:16:55,839 --> 00:17:00,000
learning are these signals that we're

464
00:16:57,680 --> 00:17:01,639
seeing actually produc ucing the changes

465
00:17:00,000 --> 00:17:03,800
in the animal's behavior that's never

466
00:17:01,639 --> 00:17:05,720
been shown before and so we turn to

467
00:17:03,800 --> 00:17:07,280
exactly that same experiment we activate

468
00:17:05,720 --> 00:17:09,319
these layer one inter neurons to

469
00:17:07,280 --> 00:17:12,000
selectively inhibit the den rites and

470
00:17:09,319 --> 00:17:14,600
now these differences here go away

471
00:17:12,000 --> 00:17:17,559
there's no longer a nice vectorized

472
00:17:14,600 --> 00:17:20,919
representation of different kinds of

473
00:17:17,559 --> 00:17:24,400
air and now the animals don't learn

474
00:17:20,919 --> 00:17:26,439
either so we were really really pleased

475
00:17:24,400 --> 00:17:27,839
when we saw this result this is a I I

476
00:17:26,439 --> 00:17:29,960
know most of you don't don't know this

477
00:17:27,839 --> 00:17:31,840
but this is a hor heroically difficult

478
00:17:29,960 --> 00:17:33,440
experiment that Valerio conducted that

479
00:17:31,840 --> 00:17:35,240
took him many many many months and we

480
00:17:33,440 --> 00:17:38,919
weren't sure it would work and there it

481
00:17:35,240 --> 00:17:43,480
is it's very robust so what I've shown

482
00:17:38,919 --> 00:17:45,720
you today is one attempt that we've made

483
00:17:43,480 --> 00:17:48,559
to kind of work at the interface here

484
00:17:45,720 --> 00:17:51,880
about all of the diversity and messiness

485
00:17:48,559 --> 00:17:54,880
and biology and and beauty of the brain

486
00:17:51,880 --> 00:17:57,440
but thinking about it from a slightly

487
00:17:54,880 --> 00:17:58,880
more a&n perspective we do this a lot

488
00:17:57,440 --> 00:18:00,799
now this has proved to be a really

489
00:17:58,880 --> 00:18:03,000
really fruitful way of doing things we

490
00:18:00,799 --> 00:18:05,240
have a bunch of experiments and papers

491
00:18:03,000 --> 00:18:07,559
and things in progress now looking at

492
00:18:05,240 --> 00:18:09,360
stability versus flexibility looking at

493
00:18:07,559 --> 00:18:11,000
catastrophic forgetting how to build

494
00:18:09,360 --> 00:18:13,200
models about that and how to actually

495
00:18:11,000 --> 00:18:14,799
improve performance in Anns most of you

496
00:18:13,200 --> 00:18:17,000
know that catastrophic forgetting really

497
00:18:14,799 --> 00:18:18,880
plagues most artificial neural networks

498
00:18:17,000 --> 00:18:20,640
we think we have some clever ideas along

499
00:18:18,880 --> 00:18:23,280
these kinds of lines about how to

500
00:18:20,640 --> 00:18:24,720
potentially solve some of those issues

501
00:18:23,280 --> 00:18:25,760
before I go I just want to thank all the

502
00:18:24,720 --> 00:18:27,159
wonderful people that I get the

503
00:18:25,760 --> 00:18:29,400
privilege to work with uh in the

504
00:18:27,159 --> 00:18:30,440
laboratory I highlighted aro's work

505
00:18:29,400 --> 00:18:33,440
today but all these other people are

506
00:18:30,440 --> 00:18:35,400
doing amazing stuff all of the funders

507
00:18:33,440 --> 00:18:37,600
that help keep the lab running we really

508
00:18:35,400 --> 00:18:39,400
appreciate all of their support uh this

509
00:18:37,600 --> 00:18:41,320
is not a drunk member of my lab passed

510
00:18:39,400 --> 00:18:42,600
out it's my son it's past his bedtime

511
00:18:41,320 --> 00:18:43,490
thank you all so much I'd be happy to

512
00:18:42,600 --> 00:18:47,720
take

513
00:18:43,490 --> 00:18:49,919
[Applause]

514
00:18:47,720 --> 00:18:54,039
questions thank you Mark for sharing

515
00:18:49,919 --> 00:18:54,039
these fascinating results um

516
00:18:56,760 --> 00:19:02,720
questions have you guys LP if they

517
00:18:59,919 --> 00:19:06,159
actually modulate the

518
00:19:02,720 --> 00:19:07,320
voriz VOR or in the of not only learning

519
00:19:06,159 --> 00:19:09,320
but also

520
00:19:07,320 --> 00:19:15,400
unlearning sorry you're asking about the

521
00:19:09,320 --> 00:19:18,480
lfp yeah so now for a lot of the neur

522
00:19:15,400 --> 00:19:21,480
mod around looking

523
00:19:18,480 --> 00:19:21,480
at

524
00:19:22,600 --> 00:19:29,280
mod of learning learning so the lfp if

525
00:19:27,559 --> 00:19:31,159
you could briefly repeat question oh

526
00:19:29,280 --> 00:19:34,600
sure so the the first part of the

527
00:19:31,159 --> 00:19:36,200
question was uh is the lfp modulated by

528
00:19:34,600 --> 00:19:39,280
these vectorized eror signals did I

529
00:19:36,200 --> 00:19:41,520
capture that yeah so the lfp uh is a

530
00:19:39,280 --> 00:19:43,679
local field potential it's an electrical

531
00:19:41,520 --> 00:19:46,000
measurement that's very fast much faster

532
00:19:43,679 --> 00:19:48,080
than our calcium signals uh but it's an

533
00:19:46,000 --> 00:19:49,200
average activity of a lot of different

534
00:19:48,080 --> 00:19:51,200
things and actually people don't

535
00:19:49,200 --> 00:19:53,840
entirely understand what the lfp really

536
00:19:51,200 --> 00:19:55,960
reflects because it's a spatial average

537
00:19:53,840 --> 00:19:57,919
of a lot of different stuff I would not

538
00:19:55,960 --> 00:20:00,679
expect us to be able to see these very

539
00:19:57,919 --> 00:20:03,600
specific Iz signals and in fact if we

540
00:20:00,679 --> 00:20:06,000
just average across the neurons blind

541
00:20:03,600 --> 00:20:07,840
that with our data blind to p+ P minus

542
00:20:06,000 --> 00:20:10,200
or not participating you know in the

543
00:20:07,840 --> 00:20:13,080
task it just smears out you just don't

544
00:20:10,200 --> 00:20:14,960
see it there are definitely task related

545
00:20:13,080 --> 00:20:16,440
signals you definitely see task related

546
00:20:14,960 --> 00:20:18,080
signals in the averages and I would

547
00:20:16,440 --> 00:20:20,000
predict you would see them in the lfp

548
00:20:18,080 --> 00:20:21,840
but the vectorized signals are very

549
00:20:20,000 --> 00:20:24,280
specific and just get swamped by

550
00:20:21,840 --> 00:20:25,880
everything that the actual experiment is

551
00:20:24,280 --> 00:20:27,760
really hard to do you can't image and

552
00:20:25,880 --> 00:20:29,799
lfp at the same time the electrodes kind

553
00:20:27,760 --> 00:20:31,360
of in the way but I think the spatial

554
00:20:29,799 --> 00:20:34,039
average of these signals gets you

555
00:20:31,360 --> 00:20:34,039
basically the same

556
00:20:34,559 --> 00:20:41,720
answer hi thanks yeah so uh Sam gersman

557
00:20:38,240 --> 00:20:44,480
gave a a talk at MIT uh maybe in the

558
00:20:41,720 --> 00:20:47,640
last year or so uh and uh and he

559
00:20:44,480 --> 00:20:51,720
mentioned uh this work and that it was

560
00:20:47,640 --> 00:20:54,280
an instance of uh work in technology

561
00:20:51,720 --> 00:20:57,240
that helped us better understand uh how

562
00:20:54,280 --> 00:20:59,200
brains work right that back propagation

563
00:20:57,240 --> 00:21:01,520
uh is uh is something that also happens

564
00:20:59,200 --> 00:21:04,440
in biological systems but I wouldn't go

565
00:21:01,520 --> 00:21:06,320
quite that far the reviewers at nature

566
00:21:04,440 --> 00:21:08,520
really have pushed back when we tried to

567
00:21:06,320 --> 00:21:10,600
say that for good reason so I don't want

568
00:21:08,520 --> 00:21:11,880
to get caught out saying like this isn't

569
00:21:10,600 --> 00:21:13,600
evidence for back propop this is

570
00:21:11,880 --> 00:21:15,840
evidence for vectorized signals it's at

571
00:21:13,600 --> 00:21:17,320
the level of cells not at synapses we're

572
00:21:15,840 --> 00:21:19,760
trying to figure out how to do it at

573
00:21:17,320 --> 00:21:22,240
synapses but just just want to caveat

574
00:21:19,760 --> 00:21:24,640
that a tiny bit no problem yeah in any

575
00:21:22,240 --> 00:21:27,320
case it's a learning from technology

576
00:21:24,640 --> 00:21:30,240
going back to uh you know create a

577
00:21:27,320 --> 00:21:33,200
hypothesis to test in neuroscience and

578
00:21:30,240 --> 00:21:35,480
so uh but his belief seems to be that

579
00:21:33,200 --> 00:21:37,679
it's very difficult uh that there have

580
00:21:35,480 --> 00:21:39,919
not been big breakthroughs really that

581
00:21:37,679 --> 00:21:41,960
go the other way around you seem to be

582
00:21:39,919 --> 00:21:44,559
more optimistic about that can you say a

583
00:21:41,960 --> 00:21:47,080
few words about uh you know looking at

584
00:21:44,559 --> 00:21:49,880
Neuroscience to do biologically inspired

585
00:21:47,080 --> 00:21:51,400
compute or biologically inspired

586
00:21:49,880 --> 00:21:52,880
learning with technology totally totally

587
00:21:51,400 --> 00:21:54,240
I so that's a great question I was going

588
00:21:52,880 --> 00:21:55,520
to try to stick more of that in here but

589
00:21:54,240 --> 00:21:57,720
in the interest of time I kind of took

590
00:21:55,520 --> 00:21:59,919
it out I think we are definitely at the

591
00:21:57,720 --> 00:22:01,880
point where we can start to do that I

592
00:21:59,919 --> 00:22:05,919
think Sam's absolutely right that has

593
00:22:01,880 --> 00:22:08,559
been a problem is trying to take really

594
00:22:05,919 --> 00:22:10,840
you know specific biological discoveries

595
00:22:08,559 --> 00:22:13,240
and Implement them in silico or in

596
00:22:10,840 --> 00:22:16,679
hardware and see some dramatic gains one

597
00:22:13,240 --> 00:22:18,679
of my PhD student projects is to try to

598
00:22:16,679 --> 00:22:21,120
get dendritic nonlinearities a bunch of

599
00:22:18,679 --> 00:22:22,799
other things into networks in a

600
00:22:21,120 --> 00:22:24,200
trainable way which he's accomplished

601
00:22:22,799 --> 00:22:26,480
for the first time which is super cool

602
00:22:24,200 --> 00:22:28,840
using gradient descent but they don't

603
00:22:26,480 --> 00:22:30,679
just somehow magically work better than

604
00:22:28,840 --> 00:22:33,720
just boring Point neurons with raus or

605
00:22:30,679 --> 00:22:35,600
something in them instead you have to

606
00:22:33,720 --> 00:22:37,760
operate them under really severe

607
00:22:35,600 --> 00:22:40,320
constraints then the dendrites start to

608
00:22:37,760 --> 00:22:42,640
get you the magic and so I think

609
00:22:40,320 --> 00:22:44,520
reframing how we think about performance

610
00:22:42,640 --> 00:22:46,679
and efficiency not from just you know

611
00:22:44,520 --> 00:22:48,480
what can I do with a gigantic cluster of

612
00:22:46,679 --> 00:22:50,039
you know infinite neurons right that's

613
00:22:48,480 --> 00:22:52,600
obviously not how brains evolve to solve

614
00:22:50,039 --> 00:22:54,240
problems and that eventually is going to

615
00:22:52,600 --> 00:22:55,600
be how we have to evolve to solve

616
00:22:54,240 --> 00:22:56,919
problems we can't just keep making these

617
00:22:55,600 --> 00:22:59,000
things bigger and more energetically

618
00:22:56,919 --> 00:23:00,799
expensive and I think brains hold the

619
00:22:59,000 --> 00:23:03,240
secret for how to do that in the best

620
00:23:00,799 --> 00:23:05,840
way that's my personal feeling thank you

621
00:23:03,240 --> 00:23:05,840
again that's a great

622
00:23:06,440 --> 00:23:13,400
question okay great thank you very much

623
00:23:09,120 --> 00:23:13,400
Mark let's give him over hand

