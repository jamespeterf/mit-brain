1
00:00:14,400 --> 00:00:20,640
Hi everyone and welcome to the AI and

2
00:00:17,920 --> 00:00:22,320
open education and initiative speaker

3
00:00:20,640 --> 00:00:24,560
series.

4
00:00:22,320 --> 00:00:26,960
Again, you are here at professional

5
00:00:24,560 --> 00:00:28,720
education and the judicious use of AI

6
00:00:26,960 --> 00:00:31,439
which is part of the AI and open

7
00:00:28,720 --> 00:00:34,439
education initiative speaker series.

8
00:00:31,439 --> 00:00:34,439
Welcome.

9
00:00:35,200 --> 00:00:39,120
My name is Sher Seagull and I am the

10
00:00:37,120 --> 00:00:41,840
collaborations and engagement manager at

11
00:00:39,120 --> 00:00:43,600
MIT Open Courseware. I am also one of

12
00:00:41,840 --> 00:00:46,559
the planning committee members of the AI

13
00:00:43,600 --> 00:00:49,039
and open education initiative at MIT

14
00:00:46,559 --> 00:00:50,800
Open Learning. In order to meet the

15
00:00:49,039 --> 00:00:53,199
moment of remarkable growth of

16
00:00:50,800 --> 00:00:55,280
artificial intelligence and to examine

17
00:00:53,199 --> 00:00:58,559
its benefits and challenges to the field

18
00:00:55,280 --> 00:01:00,719
of open education, last summer, MIT Open

19
00:00:58,559 --> 00:01:03,120
Learning solicited rapid response papers

20
00:01:00,719 --> 00:01:05,600
from stakeholders around the world to

21
00:01:03,120 --> 00:01:08,560
articulate how generative AI might

22
00:01:05,600 --> 00:01:11,680
accelerate or hinder the promise of open

23
00:01:08,560 --> 00:01:15,280
education. An international jury of AI

24
00:01:11,680 --> 00:01:17,680
and open education experts reviewed 147

25
00:01:15,280 --> 00:01:20,000
submissions and published nine

26
00:01:17,680 --> 00:01:22,799
thought-provoking papers by 32

27
00:01:20,000 --> 00:01:24,479
researchers on the open platform pub

28
00:01:22,799 --> 00:01:26,159
which you can find via the link in the

29
00:01:24,479 --> 00:01:28,000
chat.

30
00:01:26,159 --> 00:01:30,159
This initiative and today's speaker

31
00:01:28,000 --> 00:01:32,799
series webinar is generously funded by

32
00:01:30,159 --> 00:01:34,240
the William and Flora Hulet Foundation.

33
00:01:32,799 --> 00:01:36,320
We want to thank Huelet for their

34
00:01:34,240 --> 00:01:38,400
ongoing support.

35
00:01:36,320 --> 00:01:40,240
Today's session is the first of two

36
00:01:38,400 --> 00:01:43,040
webinars in the initiative speaker

37
00:01:40,240 --> 00:01:47,040
series. Today's topic is professional

38
00:01:43,040 --> 00:01:49,840
education and the judicious use of AI.

39
00:01:47,040 --> 00:01:52,240
Our next webinar on May 12th is on AI

40
00:01:49,840 --> 00:01:54,240
literacies and evaluation and we'll put

41
00:01:52,240 --> 00:01:56,640
the registration link for that in the

42
00:01:54,240 --> 00:01:59,119
chat.

43
00:01:56,640 --> 00:02:01,200
Please note that today's session is

44
00:01:59,119 --> 00:02:04,079
being recorded and will be openly

45
00:02:01,200 --> 00:02:06,320
licensed as an open educational resource

46
00:02:04,079 --> 00:02:09,360
that will be openly available to people

47
00:02:06,320 --> 00:02:11,680
around the world. We invite you to shape

48
00:02:09,360 --> 00:02:14,400
the narrative we are having about AI and

49
00:02:11,680 --> 00:02:16,720
open education and help us navigate the

50
00:02:14,400 --> 00:02:20,000
terrain for new directions and new

51
00:02:16,720 --> 00:02:21,599
questions. So, please add your voice and

52
00:02:20,000 --> 00:02:24,080
let your critiques, concerns, and

53
00:02:21,599 --> 00:02:26,160
enthusiasm be heard. You can do so

54
00:02:24,080 --> 00:02:28,720
through the Q&A function throughout

55
00:02:26,160 --> 00:02:31,680
today's session. Put your ideas in the

56
00:02:28,720 --> 00:02:33,440
Q&A as you have them. Rest assured that

57
00:02:31,680 --> 00:02:35,920
we will circle back to your comments and

58
00:02:33,440 --> 00:02:37,840
questions later in this session. You're

59
00:02:35,920 --> 00:02:40,400
welcome to let us know your name, role,

60
00:02:37,840 --> 00:02:41,920
or affiliation in your comments. If you

61
00:02:40,400 --> 00:02:43,519
do not want your name referenced in

62
00:02:41,920 --> 00:02:45,920
today's recording, you can post your

63
00:02:43,519 --> 00:02:48,080
questions anonymously.

64
00:02:45,920 --> 00:02:50,400
An overview of today's structure is as

65
00:02:48,080 --> 00:02:53,040
follows. First, I will briefly introduce

66
00:02:50,400 --> 00:02:55,920
today's speakers. Then our respondent

67
00:02:53,040 --> 00:02:57,599
will contextualize this moment for us.

68
00:02:55,920 --> 00:02:59,360
Third, the authors of the two

69
00:02:57,599 --> 00:03:01,519
spotlighted papers will provide a

70
00:02:59,360 --> 00:03:03,840
high-level overview of each of their

71
00:03:01,519 --> 00:03:05,920
papers. This will be followed by a

72
00:03:03,840 --> 00:03:08,319
discussion with the authors led by our

73
00:03:05,920 --> 00:03:10,000
respondent. And finally, we will open

74
00:03:08,319 --> 00:03:12,319
the discussion for your comments and

75
00:03:10,000 --> 00:03:13,920
questions.

76
00:03:12,319 --> 00:03:17,040
I'm thrilled to introduce today's

77
00:03:13,920 --> 00:03:19,120
speakers and papers. Clint Loland is

78
00:03:17,040 --> 00:03:22,800
today's respondent and he is the

79
00:03:19,120 --> 00:03:24,879
director of open education at BC campus.

80
00:03:22,800 --> 00:03:27,200
BC Campus is a government-f funed

81
00:03:24,879 --> 00:03:29,120
nonprofit organization that provides

82
00:03:27,200 --> 00:03:31,760
teaching, learning, educational

83
00:03:29,120 --> 00:03:34,239
technology, and open ed support for the

84
00:03:31,760 --> 00:03:37,280
25 publicly funded postsecondary

85
00:03:34,239 --> 00:03:39,440
institutions of British Columbia. In

86
00:03:37,280 --> 00:03:41,920
2013, Clint was instrumental in the

87
00:03:39,440 --> 00:03:44,000
launch of the BC open textbook project

88
00:03:41,920 --> 00:03:46,480
and currently oversees the operations of

89
00:03:44,000 --> 00:03:49,440
the provincial open textbook repository

90
00:03:46,480 --> 00:03:51,920
and their open publishing platform.

91
00:03:49,440 --> 00:03:56,799
The author of our first featured paper

92
00:03:51,920 --> 00:03:59,439
is Dr. Aarim Aya Sheili Bakova. She is

93
00:03:56,799 --> 00:04:01,200
an associate professor of strategy and

94
00:03:59,439 --> 00:04:03,920
the founding director of the center for

95
00:04:01,200 --> 00:04:06,560
teaching excellence at University Canada

96
00:04:03,920 --> 00:04:08,959
West. She's also a PhD candidate in

97
00:04:06,560 --> 00:04:12,799
edtech and learning design, pursuing her

98
00:04:08,959 --> 00:04:14,640
second PhD at Simon Frasier University.

99
00:04:12,799 --> 00:04:17,600
She's a multilingual educator from

100
00:04:14,640 --> 00:04:20,239
Kazakhstan whose research focuses on AI

101
00:04:17,600 --> 00:04:22,479
powered instructional design, inclusive

102
00:04:20,239 --> 00:04:24,880
faculty development, and knowledge

103
00:04:22,479 --> 00:04:27,280
mobilization in diverse linguistic

104
00:04:24,880 --> 00:04:29,759
contexts. She's the author and editor of

105
00:04:27,280 --> 00:04:32,639
numerous policy papers and four books.

106
00:04:29,759 --> 00:04:35,040
The links will be in the chat. Her paper

107
00:04:32,639 --> 00:04:37,680
is addressing challenges in faculty

108
00:04:35,040 --> 00:04:40,000
professional development UDL training

109
00:04:37,680 --> 00:04:42,800
through AI enhanced OEER in a

110
00:04:40,000 --> 00:04:44,560
non-English context.

111
00:04:42,800 --> 00:04:47,280
One of the co-authors of today's second

112
00:04:44,560 --> 00:04:48,800
featured paper is Dr. Royce Kimmons.

113
00:04:47,280 --> 00:04:50,560
He's an associate professor of

114
00:04:48,800 --> 00:04:53,440
instructional psychology and technology

115
00:04:50,560 --> 00:04:55,520
at Brigham Young University. In his

116
00:04:53,440 --> 00:04:57,919
work, he seeks to end the effects of

117
00:04:55,520 --> 00:05:00,960
socioeconomic divides on educational

118
00:04:57,919 --> 00:05:03,440
opportunities through open education and

119
00:05:00,960 --> 00:05:06,240
transformative technology use. He is

120
00:05:03,440 --> 00:05:08,639
also the founder of edtechbooks.org

121
00:05:06,240 --> 00:05:11,360
and other sites focused on providing

122
00:05:08,639 --> 00:05:14,320
free highquality learning resources to

123
00:05:11,360 --> 00:05:17,280
all. The other co-author of today's

124
00:05:14,320 --> 00:05:19,120
second featured paper is Dr. Tori Trust.

125
00:05:17,280 --> 00:05:22,000
She's a professor of learning technology

126
00:05:19,120 --> 00:05:23,680
at University of Massachusetts Ammerst.

127
00:05:22,000 --> 00:05:25,600
Her work centers on the critical

128
00:05:23,680 --> 00:05:28,160
examination of the relationship between

129
00:05:25,600 --> 00:05:30,160
teaching, learning, and technology, and

130
00:05:28,160 --> 00:05:32,000
how technology can support teachers in

131
00:05:30,160 --> 00:05:34,160
designing contexts that enhance student

132
00:05:32,000 --> 00:05:36,880
learning. She has written and designed

133
00:05:34,160 --> 00:05:39,520
several open educational resources, many

134
00:05:36,880 --> 00:05:40,960
of which have had millions of views. She

135
00:05:39,520 --> 00:05:43,440
is also the author of five books

136
00:05:40,960 --> 00:05:44,880
published on the edtechbooks.org site,

137
00:05:43,440 --> 00:05:47,759
and we'll put those links in the chat

138
00:05:44,880 --> 00:05:50,639
for you. Now I'll pass the baton to our

139
00:05:47,759 --> 00:05:53,039
respondent Clint Loland to contextualize

140
00:05:50,639 --> 00:05:54,800
this moment for us.

141
00:05:53,039 --> 00:05:56,400
>> Thank you very much Sherah and thank you

142
00:05:54,800 --> 00:05:58,720
for inviting me to participate in this

143
00:05:56,400 --> 00:06:00,560
event today. Uh I am joining you from

144
00:05:58,720 --> 00:06:02,960
the traditional territories of the

145
00:06:00,560 --> 00:06:05,199
Lquangan speaking peoples of the Songhes

146
00:06:02,960 --> 00:06:08,319
and Esquyimal nations also known

147
00:06:05,199 --> 00:06:10,720
colonally colonially as Vancouver uh

148
00:06:08,319 --> 00:06:13,680
island Victoria British Columbia to be

149
00:06:10,720 --> 00:06:15,120
exact. Uh thank you also to MIT open

150
00:06:13,680 --> 00:06:16,880
learning uh for sparking these

151
00:06:15,120 --> 00:06:19,120
discussions and launching the rapid

152
00:06:16,880 --> 00:06:21,840
response papers looking at the emerging

153
00:06:19,120 --> 00:06:24,960
relationship between open education and

154
00:06:21,840 --> 00:06:26,800
generative AI. Uh as Sarah Schwetman

155
00:06:24,960 --> 00:06:29,199
says in her preface for the collection

156
00:06:26,800 --> 00:06:31,440
of responses we are at a moment where

157
00:06:29,199 --> 00:06:34,240
quote AI is entering the public

158
00:06:31,440 --> 00:06:36,160
consciousness in a big way and reshaping

159
00:06:34,240 --> 00:06:39,680
how we learn, communicate and

160
00:06:36,160 --> 00:06:42,240
collaborate. Uh end quote. Uh and as

161
00:06:39,680 --> 00:06:44,800
with any new technology, there is both a

162
00:06:42,240 --> 00:06:47,039
promise and a peril. Or as media

163
00:06:44,800 --> 00:06:49,919
theorist Neil Postman famously said,

164
00:06:47,039 --> 00:06:52,720
every technology is both a burden and a

165
00:06:49,919 --> 00:06:55,840
blessing. Not an eitheror but a this and

166
00:06:52,720 --> 00:06:57,840
that. And this means for every advantage

167
00:06:55,840 --> 00:07:00,639
that a new technology offers, there's

168
00:06:57,840 --> 00:07:02,479
always a corresponding disadvantage. Uh

169
00:07:00,639 --> 00:07:04,560
the disadvantage may exceed the

170
00:07:02,479 --> 00:07:07,840
importance of the advantage or the

171
00:07:04,560 --> 00:07:10,160
advantage may well be worth the cost.

172
00:07:07,840 --> 00:07:12,880
Generative AI may be a new technology,

173
00:07:10,160 --> 00:07:15,120
but the reality is it's no different in

174
00:07:12,880 --> 00:07:18,160
this regard. There are both burdens and

175
00:07:15,120 --> 00:07:20,000
blessings, promises and perils. Uh which

176
00:07:18,160 --> 00:07:22,160
is what the rapid response papers

177
00:07:20,000 --> 00:07:24,000
address. Uh in the papers we can see

178
00:07:22,160 --> 00:07:26,560
both the promise and the peril, the the

179
00:07:24,000 --> 00:07:29,919
fouian bargain of technology as Postman

180
00:07:26,560 --> 00:07:31,599
calls it at play once again. uh and it's

181
00:07:29,919 --> 00:07:34,240
this tension that we're hoping to

182
00:07:31,599 --> 00:07:36,240
explore a bit today uh and in this

183
00:07:34,240 --> 00:07:38,880
speaker series in our discussion of the

184
00:07:36,240 --> 00:07:40,960
papers. The collection of papers is also

185
00:07:38,880 --> 00:07:43,360
quite unique in that they specifically

186
00:07:40,960 --> 00:07:45,120
speak to Gen AI within the context of

187
00:07:43,360 --> 00:07:48,560
open education. And now there's been a

188
00:07:45,120 --> 00:07:51,280
lot of discussion about AI and education

189
00:07:48,560 --> 00:07:53,840
in general, but there's been maybe less

190
00:07:51,280 --> 00:07:56,080
so looking at the specifics of AI within

191
00:07:53,840 --> 00:07:58,879
the context of the unique affordances

192
00:07:56,080 --> 00:08:02,240
and consideration of open education and

193
00:07:58,879 --> 00:08:03,759
open educational practices. Um so for

194
00:08:02,240 --> 00:08:05,680
those of us who are deeply committed to

195
00:08:03,759 --> 00:08:07,199
open education and open educational

196
00:08:05,680 --> 00:08:09,599
practices

197
00:08:07,199 --> 00:08:11,919
uh uh and and the values that are

198
00:08:09,599 --> 00:08:14,800
embedded in those practices things like

199
00:08:11,919 --> 00:08:17,520
fairness and equity social justice

200
00:08:14,800 --> 00:08:20,560
access autonomy for both students and

201
00:08:17,520 --> 00:08:22,400
faculty uh critical digital pedagogy

202
00:08:20,560 --> 00:08:24,479
collaborative practices knowledge

203
00:08:22,400 --> 00:08:27,360
construction and co-construction I think

204
00:08:24,479 --> 00:08:29,759
these papers are are very welcome and I

205
00:08:27,360 --> 00:08:31,840
think a much needed addition to our

206
00:08:29,759 --> 00:08:33,680
field. Uh, and if you haven't done so

207
00:08:31,840 --> 00:08:35,599
already, uh, I would urge you to take

208
00:08:33,680 --> 00:08:37,599
some time to explore all the papers in

209
00:08:35,599 --> 00:08:39,279
the collection. Uh, I'm going to hand it

210
00:08:37,599 --> 00:08:40,320
back to Shar. I'm going to be back after

211
00:08:39,279 --> 00:08:42,719
the authors have done their

212
00:08:40,320 --> 00:08:44,320
presentations and facilitate a Q&A with

213
00:08:42,719 --> 00:08:45,279
the authors and I hope you enjoy the

214
00:08:44,320 --> 00:08:47,920
next hour.

215
00:08:45,279 --> 00:08:50,399
>> Fantastic. Thank you so much, Clint. So,

216
00:08:47,920 --> 00:08:53,760
I will now pass the baton to Dr. Sheili

217
00:08:50,399 --> 00:08:55,440
Beova uh to discuss her paper um

218
00:08:53,760 --> 00:08:57,360
addressing challenges in faculty

219
00:08:55,440 --> 00:08:59,600
professional development UDL training

220
00:08:57,360 --> 00:09:02,160
through AI enhanced OEER in a

221
00:08:59,600 --> 00:09:04,399
non-English context.

222
00:09:02,160 --> 00:09:06,560
>> Thank you. Hi everyone. Thank you Shira

223
00:09:04,399 --> 00:09:09,440
for a kind introduction and Clint. It's

224
00:09:06,560 --> 00:09:12,000
great to have you as our respondent. Um

225
00:09:09,440 --> 00:09:14,000
and warm thank you to the MIT open

226
00:09:12,000 --> 00:09:16,399
learning team Sarah Hansen and Sarah

227
00:09:14,000 --> 00:09:19,040
Schwedman and to the HLET Foundation for

228
00:09:16,399 --> 00:09:21,360
supporting this initiative. Um, it's

229
00:09:19,040 --> 00:09:22,959
been a quite a learning process for me.

230
00:09:21,360 --> 00:09:25,040
Before I begin, I want to acknowledge

231
00:09:22,959 --> 00:09:27,279
that I'm joining you today from the

232
00:09:25,040 --> 00:09:30,480
traditional and unseated territories of

233
00:09:27,279 --> 00:09:33,279
Seyamu First Nations here in White Rock,

234
00:09:30,480 --> 00:09:36,000
beautiful British Columbia and grateful

235
00:09:33,279 --> 00:09:38,399
to be here. Uh, today I'm presenting a

236
00:09:36,000 --> 00:09:41,680
paper that actually grew into the core

237
00:09:38,399 --> 00:09:44,800
of my doctoral research. uh a localized

238
00:09:41,680 --> 00:09:48,800
AI enhanced UDL based open educational

239
00:09:44,800 --> 00:09:51,600
resource created for non English Kazak

240
00:09:48,800 --> 00:09:53,519
speaking educa educators. When my

241
00:09:51,600 --> 00:09:56,720
daughter heard this title for the first

242
00:09:53,519 --> 00:09:59,920
time, she was like mom what a coded

243
00:09:56,720 --> 00:10:02,800
language you are talking what is UDL o

244
00:09:59,920 --> 00:10:07,040
so many abbreviations we'll talk about

245
00:10:02,800 --> 00:10:09,760
that today. Uh next slide please. Um

246
00:10:07,040 --> 00:10:12,560
this work began through the reach

247
00:10:09,760 --> 00:10:15,200
project. So I want to contextualize um a

248
00:10:12,560 --> 00:10:17,600
bit. Um it's a multi-country initiative

249
00:10:15,200 --> 00:10:20,240
led by Dr. Robert Williamson at Simon

250
00:10:17,600 --> 00:10:23,120
Fraser University. It was funded by

251
00:10:20,240 --> 00:10:25,680
Canada's Sherk and a reach focused on

252
00:10:23,120 --> 00:10:28,240
inclusive education for displaced and

253
00:10:25,680 --> 00:10:31,120
refugee children with disabilities

254
00:10:28,240 --> 00:10:32,959
particularly in Jordan, Turkey, Canada

255
00:10:31,120 --> 00:10:35,279
and Kazakhstan.

256
00:10:32,959 --> 00:10:38,000
uh Kazakhstan joined from the start this

257
00:10:35,279 --> 00:10:40,000
project but not because of a formal uh

258
00:10:38,000 --> 00:10:43,519
refugee population they had but because

259
00:10:40,000 --> 00:10:46,320
of this unique humanitarian situation.

260
00:10:43,519 --> 00:10:50,000
Um back in 2019, Kazakhstan government

261
00:10:46,320 --> 00:10:53,440
launched a humanitarian Jusan operation

262
00:10:50,000 --> 00:10:57,200
that repatriated around 600 citizens

263
00:10:53,440 --> 00:10:59,120
including 413 children from former ISIS

264
00:10:57,200 --> 00:11:02,959
controlled territories and refugee camps

265
00:10:59,120 --> 00:11:05,440
in Syria. and um 19 of those children

266
00:11:02,959 --> 00:11:08,959
were resettled in the western Kazakhstan

267
00:11:05,440 --> 00:11:11,200
Ataro region. Um although I had moved to

268
00:11:08,959 --> 00:11:13,279
Canada back then, I remained closely

269
00:11:11,200 --> 00:11:15,760
connected to Atrao State University

270
00:11:13,279 --> 00:11:18,000
faculty and colleagues where I had

271
00:11:15,760 --> 00:11:19,760
previously served as vice president for

272
00:11:18,000 --> 00:11:23,920
strategic planning and international

273
00:11:19,760 --> 00:11:27,680
relations. And earlier in that capacity

274
00:11:23,920 --> 00:11:30,320
I initiated this um platform kids

275
00:11:27,680 --> 00:11:33,680
university a community-based open

276
00:11:30,320 --> 00:11:36,720
learning uh for rural uh children like

277
00:11:33,680 --> 00:11:39,120
from rural regions and uh it became a

278
00:11:36,720 --> 00:11:41,680
sort of an early form for me um of

279
00:11:39,120 --> 00:11:43,920
localized open education grounded more

280
00:11:41,680 --> 00:11:46,160
in social connection and institutional

281
00:11:43,920 --> 00:11:47,360
support. I had to study different

282
00:11:46,160 --> 00:11:49,760
experience of different kids

283
00:11:47,360 --> 00:11:52,160
universities around the world and try to

284
00:11:49,760 --> 00:11:54,800
localize those practices. So when this

285
00:11:52,160 --> 00:11:56,800
reintegration of repatriated children,

286
00:11:54,800 --> 00:11:59,839
displaced children from Syria, that

287
00:11:56,800 --> 00:12:02,160
support was needed, the faculty, the

288
00:11:59,839 --> 00:12:04,560
team at at university naturally turned

289
00:12:02,160 --> 00:12:06,639
to this space to kids university because

290
00:12:04,560 --> 00:12:10,079
it was safe, it was working, it was

291
00:12:06,639 --> 00:12:13,680
trusted and we included them into the

292
00:12:10,079 --> 00:12:17,760
reach project to support that inclusion

293
00:12:13,680 --> 00:12:20,399
um to the um children with disabilities.

294
00:12:17,760 --> 00:12:22,800
But as the reach project progressed, it

295
00:12:20,399 --> 00:12:25,920
became clear that the real challenge

296
00:12:22,800 --> 00:12:28,560
wasn't the children in the first place,

297
00:12:25,920 --> 00:12:30,720
but was the readiness of the people

298
00:12:28,560 --> 00:12:33,920
trying to help them. Faculty were

299
00:12:30,720 --> 00:12:36,959
committed um but lacked some training,

300
00:12:33,920 --> 00:12:40,240
digital fluency, um no English language

301
00:12:36,959 --> 00:12:43,440
skills to be using the resources in

302
00:12:40,240 --> 00:12:46,560
English. So there were no resources,

303
00:12:43,440 --> 00:12:48,800
open resources in Kazak. So um I began

304
00:12:46,560 --> 00:12:51,760
offering some fragmented workshops

305
00:12:48,800 --> 00:12:55,040
mostly around UDL framework and um

306
00:12:51,760 --> 00:12:58,639
artsbased strategies but I saw that we

307
00:12:55,040 --> 00:13:00,480
needed more of sustainable um solutions.

308
00:12:58,639 --> 00:13:02,959
Next slide please. So that's when the

309
00:13:00,480 --> 00:13:05,440
idea took shape. We saw that well we've

310
00:13:02,959 --> 00:13:08,160
developed a UDL based open resource

311
00:13:05,440 --> 00:13:11,120
asynchronous course in Kazak that would

312
00:13:08,160 --> 00:13:14,560
educators learn from about inclusion and

313
00:13:11,120 --> 00:13:17,440
how to um adapt it. So um a resource

314
00:13:14,560 --> 00:13:19,920
that they could use flexibly in their

315
00:13:17,440 --> 00:13:23,279
own language and at that stage there was

316
00:13:19,920 --> 00:13:25,120
no AI. I was translating myself using

317
00:13:23,279 --> 00:13:28,240
sometimes Google translate to help but

318
00:13:25,120 --> 00:13:31,920
it was not really helpful. So we had to

319
00:13:28,240 --> 00:13:34,079
adapt um again culturally pedagogically

320
00:13:31,920 --> 00:13:36,560
uh considering Kazak traditions of

321
00:13:34,079 --> 00:13:38,720
pedagogy as well. I reflected in my

322
00:13:36,560 --> 00:13:41,360
paper that much of the global literature

323
00:13:38,720 --> 00:13:43,839
on UTL and open education is in English

324
00:13:41,360 --> 00:13:46,639
and embedded in western assumption about

325
00:13:43,839 --> 00:13:49,040
technology and access and that

326
00:13:46,639 --> 00:13:51,600
disconnect made localization not just

327
00:13:49,040 --> 00:13:54,639
important but essential and critical.

328
00:13:51,600 --> 00:13:57,279
Next slide please. So in early 2024 I

329
00:13:54,639 --> 00:14:02,000
began exploring generative AI tools a

330
00:13:57,279 --> 00:14:04,079
bit cautiously. Um and actually the main

331
00:14:02,000 --> 00:14:06,399
excitement was could they help

332
00:14:04,079 --> 00:14:09,680
accelerate that localization

333
00:14:06,399 --> 00:14:12,480
um save my time with translation uh

334
00:14:09,680 --> 00:14:15,199
generating some visuals um but with

335
00:14:12,480 --> 00:14:19,440
cultural care. So I experimented with

336
00:14:15,199 --> 00:14:22,320
Chad Gupt for drafting D for um visuals.

337
00:14:19,440 --> 00:14:25,040
Yes chat there was a Kazak GPT chatbot

338
00:14:22,320 --> 00:14:28,480
that saved a lot of time as well. I was

339
00:14:25,040 --> 00:14:31,199
inspired partially by the Ludia it's UDL

340
00:14:28,480 --> 00:14:32,880
plus AI chatbot also and I saw the

341
00:14:31,199 --> 00:14:37,279
potential there to make it more

342
00:14:32,880 --> 00:14:39,839
accessible. So um this re overall the

343
00:14:37,279 --> 00:14:44,320
course uh that uh we've I've designed

344
00:14:39,839 --> 00:14:46,959
and um piloted with Ataro University was

345
00:14:44,320 --> 00:14:48,959
not generated by AI but it was designed

346
00:14:46,959 --> 00:14:52,320
with this instructional design model

347
00:14:48,959 --> 00:14:55,600
successive approximation model um and

348
00:14:52,320 --> 00:14:58,320
SAM as its author Michael Allen reminds

349
00:14:55,600 --> 00:15:01,440
us is not just about speed being fast

350
00:14:58,320 --> 00:15:03,600
being reiterative it's about listening

351
00:15:01,440 --> 00:15:06,720
so um that's what this project became

352
00:15:03,600 --> 00:15:08,959
became uh a sort of act of pedagogical

353
00:15:06,720 --> 00:15:11,760
listening through language design and

354
00:15:08,959 --> 00:15:14,959
response. Next slide please. So we chose

355
00:15:11,760 --> 00:15:17,600
UDL and um I just quickly want to share

356
00:15:14,959 --> 00:15:20,560
the anecdotal part about translating

357
00:15:17,600 --> 00:15:23,839
actual concept into English universal

358
00:15:20,560 --> 00:15:25,519
design for learning. Um and it proved a

359
00:15:23,839 --> 00:15:28,079
bit difficult. It's always tricky with

360
00:15:25,519 --> 00:15:32,399
those concepts. Um so machine

361
00:15:28,079 --> 00:15:34,959
translation gave us ambabo

362
00:15:32,399 --> 00:15:37,920
and here ambab which is more of

363
00:15:34,959 --> 00:15:42,079
convenience store type of universality

364
00:15:37,920 --> 00:15:45,760
and uh balao is more about um to pro

365
00:15:42,079 --> 00:15:48,720
projecting um or outlining or drafting.

366
00:15:45,760 --> 00:15:51,519
So after consultation we had with some

367
00:15:48,720 --> 00:15:55,839
of the doctor in philology at that state

368
00:15:51,519 --> 00:15:59,040
university. Um I we made decisions. So I

369
00:15:55,839 --> 00:16:02,560
I kind of was my decision but um I

370
00:15:59,040 --> 00:16:05,839
relied in on the feedback as well to

371
00:16:02,560 --> 00:16:08,959
keep uh universal part univers as it's

372
00:16:05,839 --> 00:16:10,880
an international term and also design

373
00:16:08,959 --> 00:16:13,680
part because we also have that term in

374
00:16:10,880 --> 00:16:17,519
Kazak. So uh universal

375
00:16:13,680 --> 00:16:20,720
design uh but yet as you see uh I kept

376
00:16:17,519 --> 00:16:23,519
UDL in English abbreviation in brackets

377
00:16:20,720 --> 00:16:26,079
in English and that also another was um

378
00:16:23,519 --> 00:16:30,320
another strategic decision here. Hard

379
00:16:26,079 --> 00:16:33,519
one um because um I thought that it

380
00:16:30,320 --> 00:16:36,320
would be a hybrid that preserved um both

381
00:16:33,519 --> 00:16:38,160
local clarity but also global

382
00:16:36,320 --> 00:16:40,320
recognizability.

383
00:16:38,160 --> 00:16:42,720
So faculty could recognize it in English

384
00:16:40,320 --> 00:16:47,199
language, conferences, resources or

385
00:16:42,720 --> 00:16:49,199
books UDL and um use AI tools to

386
00:16:47,199 --> 00:16:52,639
translate those resources because they

387
00:16:49,199 --> 00:16:54,959
now know that it's about UDL. So uh I

388
00:16:52,639 --> 00:16:57,040
would call it conceptual bilingualism,

389
00:16:54,959 --> 00:17:00,240
the way to build both sort of

390
00:16:57,040 --> 00:17:03,199
familiarity and fluency across that

391
00:17:00,240 --> 00:17:05,280
academic scholarly context. Next slide,

392
00:17:03,199 --> 00:17:07,600
please. I will probably skip it. is just

393
00:17:05,280 --> 00:17:10,559
another example of the translation and

394
00:17:07,600 --> 00:17:13,120
localization. U it was quite a

395
00:17:10,559 --> 00:17:15,360
challenging part as well. Another

396
00:17:13,120 --> 00:17:18,160
resource that we used was this again

397
00:17:15,360 --> 00:17:20,480
Kazak GPT that saved so much time

398
00:17:18,160 --> 00:17:22,559
because I started designing my own Kazak

399
00:17:20,480 --> 00:17:24,480
Jet GPT and then thought well let me

400
00:17:22,559 --> 00:17:27,760
check maybe some other smart people

401
00:17:24,480 --> 00:17:30,240
already done it before me. So uh and

402
00:17:27,760 --> 00:17:32,080
voila there was this free resource. Next

403
00:17:30,240 --> 00:17:35,600
slide please.

404
00:17:32,080 --> 00:17:39,200
So uh the course had 95 completion rate

405
00:17:35,600 --> 00:17:40,880
uh among 25 faculty and um it was

406
00:17:39,200 --> 00:17:43,360
unusually high to be honest. I was

407
00:17:40,880 --> 00:17:46,320
surprised um for asynchronous

408
00:17:43,360 --> 00:17:49,200
professional development which is um

409
00:17:46,320 --> 00:17:51,840
like nobody really tracks the progress.

410
00:17:49,200 --> 00:17:54,960
Uh so far we had the precourse survey

411
00:17:51,840 --> 00:17:57,440
and after course survey anonymously and

412
00:17:54,960 --> 00:18:00,080
knowledge scores again we've seen

413
00:17:57,440 --> 00:18:02,000
increased 60% were those faculty who

414
00:18:00,080 --> 00:18:04,000
attended my previous fragmented

415
00:18:02,000 --> 00:18:06,480
workshops during the reach project so

416
00:18:04,000 --> 00:18:10,240
they knew something about UDL that was

417
00:18:06,480 --> 00:18:13,520
really also um like rewarding to see

418
00:18:10,240 --> 00:18:16,799
that uh and the increase was to 85%. And

419
00:18:13,520 --> 00:18:18,960
so there is a lot more to work on and

420
00:18:16,799 --> 00:18:20,720
faculty actually shared that it

421
00:18:18,960 --> 00:18:23,120
reflected their realities. They felt

422
00:18:20,720 --> 00:18:26,720
that it was not translated or decoded

423
00:18:23,120 --> 00:18:28,799
from other language. So here I felt like

424
00:18:26,720 --> 00:18:31,760
localization doesn't just increase

425
00:18:28,799 --> 00:18:34,960
access but it's also about connection or

426
00:18:31,760 --> 00:18:38,559
um to the content. Finally, next slide

427
00:18:34,960 --> 00:18:40,880
please. The lessons I drew actually a

428
00:18:38,559 --> 00:18:43,840
lot more but I I thought the most

429
00:18:40,880 --> 00:18:47,520
important ones I would share today is

430
00:18:43,840 --> 00:18:50,960
that localization is um foundational

431
00:18:47,520 --> 00:18:54,400
um not optional. It's important that we

432
00:18:50,960 --> 00:18:57,200
um start with the design uh on mind and

433
00:18:54,400 --> 00:19:00,320
it shouldn't be an afterthought. Um

434
00:18:57,200 --> 00:19:04,400
second AI integration requires critical

435
00:19:00,320 --> 00:19:06,720
human guidance. Yes. Um AI accelerates,

436
00:19:04,400 --> 00:19:09,679
it supports localization but it cannot

437
00:19:06,720 --> 00:19:12,080
lead it. And finally inclusive design

438
00:19:09,679 --> 00:19:15,760
requires institutional support. Um I

439
00:19:12,080 --> 00:19:18,400
mean we had to get support from the um

440
00:19:15,760 --> 00:19:20,880
Atra State University's um

441
00:19:18,400 --> 00:19:23,679
administrative level also to have access

442
00:19:20,880 --> 00:19:26,160
to their IT resources so that it is

443
00:19:23,679 --> 00:19:28,720
hosted on their platform as they helped

444
00:19:26,160 --> 00:19:30,880
us with the access to articulate rice

445
00:19:28,720 --> 00:19:34,240
platform where we designed the first

446
00:19:30,880 --> 00:19:37,760
interactive uh elements of this course

447
00:19:34,240 --> 00:19:40,320
and um it's important that professional

448
00:19:37,760 --> 00:19:42,720
development isn't only inclusive but

449
00:19:40,320 --> 00:19:46,960
also available and institutional support

450
00:19:42,720 --> 00:19:50,000
is the key. And last slide, please. Um,

451
00:19:46,960 --> 00:19:52,160
so that was a rapid response paper. It's

452
00:19:50,000 --> 00:19:54,640
still going on. Uh, it was my first

453
00:19:52,160 --> 00:19:57,039
pilot project I was excited about. At

454
00:19:54,640 --> 00:19:58,640
the moment we have two more uh

455
00:19:57,039 --> 00:20:01,440
universities in Kazakhstan who are

456
00:19:58,640 --> 00:20:04,640
interested in hosting this uh course and

457
00:20:01,440 --> 00:20:07,360
uh I assume for my doctoral dissertation

458
00:20:04,640 --> 00:20:10,400
I would want to have a larger sample of

459
00:20:07,360 --> 00:20:12,960
respondents and so um look forward to

460
00:20:10,400 --> 00:20:14,640
include more but this time um the

461
00:20:12,960 --> 00:20:16,799
platform sustainability is um

462
00:20:14,640 --> 00:20:19,280
challenging and it's quite expensive

463
00:20:16,799 --> 00:20:22,160
sometimes in those areas so we are

464
00:20:19,280 --> 00:20:24,000
exploring now simpler tools like Moodle

465
00:20:22,160 --> 00:20:26,720
maybe or Google sites

466
00:20:24,000 --> 00:20:28,880
uh to keep resources open and replicable

467
00:20:26,720 --> 00:20:31,039
and in future I hope uh it will grow

468
00:20:28,880 --> 00:20:34,080
into sort of community of practice among

469
00:20:31,039 --> 00:20:37,440
Kazak educators whose this course

470
00:20:34,080 --> 00:20:39,440
actually um brought together and thank

471
00:20:37,440 --> 00:20:42,159
you for your time and listening to me if

472
00:20:39,440 --> 00:20:44,159
there is one takeaway from my talk today

473
00:20:42,159 --> 00:20:46,799
it's this inclusive design does not

474
00:20:44,159 --> 00:20:49,120
begin with AI or frameworks it begins

475
00:20:46,799 --> 00:20:51,679
with the people and the language we

476
00:20:49,120 --> 00:20:55,200
choose to design for and if open

477
00:20:51,679 --> 00:20:58,159
education is truly to open then AI must

478
00:20:55,200 --> 00:21:00,480
serve as a bridge not a gatekeeper uh

479
00:20:58,159 --> 00:21:02,720
enabling educators to build resources

480
00:21:00,480 --> 00:21:04,960
not just in many formats but in many

481
00:21:02,720 --> 00:21:07,600
voices as well. Thank you so much.

482
00:21:04,960 --> 00:21:09,840
>> Beautiful. Thank you so much. Uh we

483
00:21:07,600 --> 00:21:12,320
really appreciate it. As you um are

484
00:21:09,840 --> 00:21:14,320
thinking of your questions and comments

485
00:21:12,320 --> 00:21:16,480
um and ideas, please do put them in the

486
00:21:14,320 --> 00:21:18,640
Q&A and we will come back to them near

487
00:21:16,480 --> 00:21:20,320
the end of this session. Uh so I would

488
00:21:18,640 --> 00:21:22,400
like to pass the baton now to Dr.

489
00:21:20,320 --> 00:21:24,480
Kimmons and Dr. trust who will talk

490
00:21:22,400 --> 00:21:26,640
about judicious AI use to improve

491
00:21:24,480 --> 00:21:29,520
existing OEER.

492
00:21:26,640 --> 00:21:31,280
>> Thank you Shar and I'm delighted to be

493
00:21:29,520 --> 00:21:33,600
here today and what an incredible first

494
00:21:31,280 --> 00:21:37,039
presentation by Ya that I really

495
00:21:33,600 --> 00:21:39,360
enjoyed. I am presenting alongside uh my

496
00:21:37,039 --> 00:21:41,760
co-author Dr. Royce Kimmons and on

497
00:21:39,360 --> 00:21:43,440
behalf of our other co-author Dr. George

498
00:21:41,760 --> 00:21:45,679
Valencianos

499
00:21:43,440 --> 00:21:47,840
today and let's go ahead and jump in.

500
00:21:45,679 --> 00:21:50,000
Next slide please.

501
00:21:47,840 --> 00:21:52,159
So, generative AI technologies are

502
00:21:50,000 --> 00:21:54,799
powerful tools, but they come with

503
00:21:52,159 --> 00:21:58,799
significant costs in terms of financial

504
00:21:54,799 --> 00:22:00,880
costs. Just last week, a CEO of Open AI

505
00:21:58,799 --> 00:22:03,280
said using nicities like please and

506
00:22:00,880 --> 00:22:05,760
thank you, cost the company upwards of

507
00:22:03,280 --> 00:22:07,360
tens of millions of dollars. and

508
00:22:05,760 --> 00:22:09,120
thinking about the environmental costs

509
00:22:07,360 --> 00:22:11,120
of these tools, which are quite

510
00:22:09,120 --> 00:22:13,440
substantial. Unfortunately, many of the

511
00:22:11,120 --> 00:22:15,280
developers of AI technologies aren't

512
00:22:13,440 --> 00:22:17,600
very transparent or clear about the

513
00:22:15,280 --> 00:22:20,320
environmental impact of these tools.

514
00:22:17,600 --> 00:22:22,880
However, when looking at Google's 2024

515
00:22:20,320 --> 00:22:26,559
sustainability reports, it showed that

516
00:22:22,880 --> 00:22:29,039
there was a 13% increase year-over-year

517
00:22:26,559 --> 00:22:33,600
in greenhouse gas emissions since

518
00:22:29,039 --> 00:22:36,799
totaling 48% uh since 2019. and they

519
00:22:33,600 --> 00:22:39,520
attributed that to increase in um data

520
00:22:36,799 --> 00:22:42,320
centers and need for power in those. The

521
00:22:39,520 --> 00:22:46,080
same report also noted there was a 17%

522
00:22:42,320 --> 00:22:48,480
increase in water use uh totaling 6.1

523
00:22:46,080 --> 00:22:51,760
billion gallons of water which is more

524
00:22:48,480 --> 00:22:53,760
than 9,000 Olympic size swimming pools

525
00:22:51,760 --> 00:22:56,320
um and water in that case is often used

526
00:22:53,760 --> 00:22:58,000
to cool down data centers. Some

527
00:22:56,320 --> 00:22:59,600
researchers have also tried to make some

528
00:22:58,000 --> 00:23:01,919
estimates about the environmental cost

529
00:22:59,600 --> 00:23:04,720
of these tools. One group of researchers

530
00:23:01,919 --> 00:23:07,120
found that generating an AI image, one

531
00:23:04,720 --> 00:23:10,080
or two AI images, takes as much

532
00:23:07,120 --> 00:23:12,400
electricity as charging your smartphone.

533
00:23:10,080 --> 00:23:14,880
Another group of researchers found that

534
00:23:12,400 --> 00:23:17,360
getting AI to provide medium length

535
00:23:14,880 --> 00:23:19,760
responses, about 10 to 50 of them, takes

536
00:23:17,360 --> 00:23:21,200
about 16 ounces of water, which may not

537
00:23:19,760 --> 00:23:22,720
seem like a lot, but then when you

538
00:23:21,200 --> 00:23:24,720
multiply that by the hundreds of

539
00:23:22,720 --> 00:23:27,520
millions of users of these tools data

540
00:23:24,720 --> 00:23:29,679
daily, it's a quite a significant cost.

541
00:23:27,520 --> 00:23:32,080
and also the fact that many of these

542
00:23:29,679 --> 00:23:35,120
data centers are housed in towns that

543
00:23:32,080 --> 00:23:37,039
they are now taking electricity and

544
00:23:35,120 --> 00:23:40,159
water and other resources from the

545
00:23:37,039 --> 00:23:42,480
residential supply. Uh these tools also

546
00:23:40,159 --> 00:23:45,120
have several other ethical issues in

547
00:23:42,480 --> 00:23:47,520
terms of their accuracy um you know

548
00:23:45,120 --> 00:23:49,840
hallucinating information and have

549
00:23:47,520 --> 00:23:51,520
exacerbated the digital divide um

550
00:23:49,840 --> 00:23:53,360
between those who have access to these

551
00:23:51,520 --> 00:23:54,960
tools and know how to use them and know

552
00:23:53,360 --> 00:23:57,280
how to think critically about them and

553
00:23:54,960 --> 00:23:59,919
those who do not. So, I'm gonna hand it

554
00:23:57,280 --> 00:24:02,320
over to my co-author here, uh, Dr. Royce

555
00:23:59,919 --> 00:24:04,960
Kimmans to talk more about judicious

556
00:24:02,320 --> 00:24:09,120
uses of AI.

557
00:24:04,960 --> 00:24:11,360
>> Thank you. So, um, as has already been

558
00:24:09,120 --> 00:24:13,600
mentioned, AI came into kind of this

559
00:24:11,360 --> 00:24:15,520
critical consciousness of everyone or

560
00:24:13,600 --> 00:24:18,559
um, for the vast majority of people with

561
00:24:15,520 --> 00:24:20,880
the advent of specifically chat GPT um,

562
00:24:18,559 --> 00:24:22,960
but chat bots specifically as an as a

563
00:24:20,880 --> 00:24:24,640
type of AI. And for that reason,

564
00:24:22,960 --> 00:24:26,400
whenever we talk about AI, whenever you

565
00:24:24,640 --> 00:24:28,000
hear a new presentation about AI, most

566
00:24:26,400 --> 00:24:30,400
of the time, uh, it's going to be about

567
00:24:28,000 --> 00:24:33,440
chat bots. And that's true in education,

568
00:24:30,400 --> 00:24:34,880
it's true in every field. Um, and so the

569
00:24:33,440 --> 00:24:37,279
kind of knee-jerk reaction that we've

570
00:24:34,880 --> 00:24:39,919
had to AI and across lots of disciplines

571
00:24:37,279 --> 00:24:41,440
is, oh, we can use AI now to do some

572
00:24:39,919 --> 00:24:43,520
exciting things. Let's throw chat bots

573
00:24:41,440 --> 00:24:45,360
at our problems and see what happens.

574
00:24:43,520 --> 00:24:47,120
Um, the problem though, well, one of the

575
00:24:45,360 --> 00:24:48,640
problems is that chat bots require a

576
00:24:47,120 --> 00:24:50,400
high level of literacy as Tori

577
00:24:48,640 --> 00:24:52,880
mentioned. They also require a lot of

578
00:24:50,400 --> 00:24:54,960
energy. um more than a lot of the other

579
00:24:52,880 --> 00:24:58,240
kinds of AI or uses of AI that we often

580
00:24:54,960 --> 00:24:59,279
deal with. And so what we what I would

581
00:24:58,240 --> 00:25:01,919
what we would refer to as a

582
00:24:59,279 --> 00:25:05,840
non-judicious use of AI um specifically

583
00:25:01,919 --> 00:25:07,760
when it comes to OAR is to um embed a

584
00:25:05,840 --> 00:25:09,440
chatbot or to throw a chatbot at a

585
00:25:07,760 --> 00:25:10,559
learner and just expect the chatbot or

586
00:25:09,440 --> 00:25:12,159
the learner to be able to interact with

587
00:25:10,559 --> 00:25:14,320
the chatbot in a way that is beneficial

588
00:25:12,159 --> 00:25:16,960
to the learner. Like going back to Ya's

589
00:25:14,320 --> 00:25:18,960
example, um giving a learner a chatbot

590
00:25:16,960 --> 00:25:22,799
and asking them to localize OAR for

591
00:25:18,960 --> 00:25:24,480
themselves, right? Um, now there are

592
00:25:22,799 --> 00:25:26,400
other options or other ways of thinking

593
00:25:24,480 --> 00:25:28,240
about this. And so what we're framing

594
00:25:26,400 --> 00:25:30,799
here is what we refer to as a judicious

595
00:25:28,240 --> 00:25:33,279
use of AI. And what this means is that

596
00:25:30,799 --> 00:25:34,799
instead of putting the a having the user

597
00:25:33,279 --> 00:25:37,520
or the learner interact directly with

598
00:25:34,799 --> 00:25:40,320
the AI, are there high value things that

599
00:25:37,520 --> 00:25:42,400
we can have AI do that we then store, we

600
00:25:40,320 --> 00:25:44,320
save the results of what the AI has done

601
00:25:42,400 --> 00:25:46,559
and then we distribute those to learners

602
00:25:44,320 --> 00:25:48,159
at scale. So this requires three steps.

603
00:25:46,559 --> 00:25:50,799
First, we identify impactful

604
00:25:48,159 --> 00:25:52,799
improvements to existing OEER that can

605
00:25:50,799 --> 00:25:55,039
be accomplished with AI. And I'll sh

606
00:25:52,799 --> 00:25:56,720
share some examples in a second. Then we

607
00:25:55,039 --> 00:25:58,480
use AI to enact and store those

608
00:25:56,720 --> 00:26:00,799
improvements in a finite typically

609
00:25:58,480 --> 00:26:02,880
one-time manner. So if we use AI to

610
00:26:00,799 --> 00:26:05,520
localize some content, we store that.

611
00:26:02,880 --> 00:26:08,080
And then the third step, we release what

612
00:26:05,520 --> 00:26:09,760
we have done under a new as OAR

613
00:26:08,080 --> 00:26:11,679
themselves. So we efficiently serve

614
00:26:09,760 --> 00:26:16,080
those improved materials to learners at

615
00:26:11,679 --> 00:26:18,960
scale as OEAR. Next, next slide, please.

616
00:26:16,080 --> 00:26:20,880
So our context for doing this is um

617
00:26:18,960 --> 00:26:24,080
edtechbooks which is an open textbook

618
00:26:20,880 --> 00:26:25,440
publishing platform. Um just a couple of

619
00:26:24,080 --> 00:26:27,760
a little bit of information about the

620
00:26:25,440 --> 00:26:29,760
background for this. So this system um

621
00:26:27,760 --> 00:26:32,400
is very intentionally open. It's very

622
00:26:29,760 --> 00:26:34,159
intentionally available to everybody. Um

623
00:26:32,400 --> 00:26:36,400
there are no log in or access walls.

624
00:26:34,159 --> 00:26:38,240
Currently we serve our content to over

625
00:26:36,400 --> 00:26:41,440
one and a half million users across the

626
00:26:38,240 --> 00:26:43,440
world every year. Um we have over 200

627
00:26:41,440 --> 00:26:46,159
published books about 8,000 published

628
00:26:43,440 --> 00:26:48,080
chapters. All content is automatically

629
00:26:46,159 --> 00:26:49,440
converted into PDFs and Microsoft Word

630
00:26:48,080 --> 00:26:52,320
documents as well. So they can be

631
00:26:49,440 --> 00:26:54,320
accessed offline and all content is also

632
00:26:52,320 --> 00:26:57,200
converted to high-quality audio books

633
00:26:54,320 --> 00:26:58,960
using AI. Um and that audio is stored so

634
00:26:57,200 --> 00:27:01,360
that people throughout the world can

635
00:26:58,960 --> 00:27:03,520
access um audio versions of these this

636
00:27:01,360 --> 00:27:05,679
content as well. And so when thinking

637
00:27:03,520 --> 00:27:07,919
about how can we use AI to improve this

638
00:27:05,679 --> 00:27:09,200
kind of an OAR resource, well, if we

639
00:27:07,919 --> 00:27:11,120
have one and a half million users

640
00:27:09,200 --> 00:27:12,960
accessing our content, we don't want one

641
00:27:11,120 --> 00:27:15,279
and a half million users using chat bots

642
00:27:12,960 --> 00:27:17,520
to to get content that they need. Why

643
00:27:15,279 --> 00:27:20,720
can't we just figure out what our users

644
00:27:17,520 --> 00:27:23,679
want, use AI to to create that and then

645
00:27:20,720 --> 00:27:24,880
re-release that to them as OEER in a way

646
00:27:23,679 --> 00:27:27,039
that is more efficient and more

647
00:27:24,880 --> 00:27:29,679
sustainable. Next slide, please. So

648
00:27:27,039 --> 00:27:31,279
we'll give uh four examples of of how

649
00:27:29,679 --> 00:27:33,200
this plays out. So one is simple

650
00:27:31,279 --> 00:27:35,279
glossery. So if we want learners to be

651
00:27:33,200 --> 00:27:37,360
able to define terms that they come

652
00:27:35,279 --> 00:27:39,919
across, we can use AI so if someone

653
00:27:37,360 --> 00:27:41,679
clicks on a term, AI will define it. But

654
00:27:39,919 --> 00:27:43,840
what we don't want is AI to have to

655
00:27:41,679 --> 00:27:45,120
define this term a million times. So if

656
00:27:43,840 --> 00:27:46,640
a million people don't know what an

657
00:27:45,120 --> 00:27:48,720
abduction is, for instance, we don't

658
00:27:46,640 --> 00:27:50,559
want AI to be called upon a million

659
00:27:48,720 --> 00:27:53,440
times to define this. Instead, what we

660
00:27:50,559 --> 00:27:54,880
can do is we have the author decide what

661
00:27:53,440 --> 00:27:56,799
they want defined. So they create a

662
00:27:54,880 --> 00:27:59,760
glossery. They use AI to create an

663
00:27:56,799 --> 00:28:01,760
initial um an initial definition for

664
00:27:59,760 --> 00:28:04,320
this. And then we serve that definition

665
00:28:01,760 --> 00:28:07,039
up to all users. So that makes it more

666
00:28:04,320 --> 00:28:09,520
sustainable, makes it lower cost and

667
00:28:07,039 --> 00:28:12,640
also allows the human so in this case

668
00:28:09,520 --> 00:28:14,559
the author to have control over uh have

669
00:28:12,640 --> 00:28:16,720
a quality assurance control over that

670
00:28:14,559 --> 00:28:18,720
definition. So that uh for instance

671
00:28:16,720 --> 00:28:20,559
we've come into we've had examples in

672
00:28:18,720 --> 00:28:22,559
the past and you've seen many papers

673
00:28:20,559 --> 00:28:25,279
written about this about hey how AI is

674
00:28:22,559 --> 00:28:28,080
biased, how AI replicates even things

675
00:28:25,279 --> 00:28:30,159
like racism and sexism in and what it in

676
00:28:28,080 --> 00:28:32,159
what it provides to people. So by

677
00:28:30,159 --> 00:28:34,480
allowing a human in this case the author

678
00:28:32,159 --> 00:28:36,480
to come between the what the AI

679
00:28:34,480 --> 00:28:38,240
generates in terms of the definition and

680
00:28:36,480 --> 00:28:40,640
what is provided to the learner it

681
00:28:38,240 --> 00:28:42,799
allows the the author to take control of

682
00:28:40,640 --> 00:28:45,360
that and to ensure that content that is

683
00:28:42,799 --> 00:28:47,760
being provided is unbiased and and so

684
00:28:45,360 --> 00:28:49,679
forth. Next slide please. Another

685
00:28:47,760 --> 00:28:52,640
example

686
00:28:49,679 --> 00:28:54,880
is in the form of learning checks. So um

687
00:28:52,640 --> 00:28:56,559
we can have an AI go through content and

688
00:28:54,880 --> 00:28:58,960
maybe after every couple of paragraphs

689
00:28:56,559 --> 00:29:00,559
it it discerns what is the core content

690
00:28:58,960 --> 00:29:02,480
of this paragraph and it creates a

691
00:29:00,559 --> 00:29:05,200
multiple choice learning check that then

692
00:29:02,480 --> 00:29:07,120
is stored as static HTML and with some

693
00:29:05,200 --> 00:29:08,799
JavaScript layered on top that is then

694
00:29:07,120 --> 00:29:11,039
efficiently served to all learners. So

695
00:29:08,799 --> 00:29:12,559
you don't have to have an AI to ask so

696
00:29:11,039 --> 00:29:14,960
that every learner doesn't have to ask

697
00:29:12,559 --> 00:29:17,279
an AI to help me find what are the key

698
00:29:14,960 --> 00:29:19,039
points of this of this content. We do it

699
00:29:17,279 --> 00:29:20,720
once and then we provide that to all

700
00:29:19,039 --> 00:29:22,080
learners so that all learners can can

701
00:29:20,720 --> 00:29:24,320
benefit from that learning check whether

702
00:29:22,080 --> 00:29:26,559
or not they have access to an AI. Next

703
00:29:24,320 --> 00:29:28,320
slide please.

704
00:29:26,559 --> 00:29:30,399
Um the audiobook example that I already

705
00:29:28,320 --> 00:29:33,039
mentioned audiobooks doing high quality

706
00:29:30,399 --> 00:29:36,000
audio with an AI actually takes a ton of

707
00:29:33,039 --> 00:29:38,399
energy um and it takes a a lot of time

708
00:29:36,000 --> 00:29:40,559
even with a high-powered machine. So if

709
00:29:38,399 --> 00:29:42,480
you were to have uh a learner for

710
00:29:40,559 --> 00:29:45,600
instance in the Philippines click on an

711
00:29:42,480 --> 00:29:47,520
click on a chapter and have AI translate

712
00:29:45,600 --> 00:29:49,919
it into audio for them that would be

713
00:29:47,520 --> 00:29:52,640
hugely expensive, use a ton of bandwidth

714
00:29:49,919 --> 00:29:55,120
and um just not be sustainable. Um so

715
00:29:52,640 --> 00:29:57,919
what we do with audio is whenever a book

716
00:29:55,120 --> 00:30:00,799
uh a book a chapter is edited or updated

717
00:29:57,919 --> 00:30:02,240
we have AI go through and create a an

718
00:30:00,799 --> 00:30:05,840
audio version of that. And some of these

719
00:30:02,240 --> 00:30:07,360
chapters the audio ends up being um an

720
00:30:05,840 --> 00:30:10,000
hour and a half long. Like these are

721
00:30:07,360 --> 00:30:12,000
huge audio files. Uh it takes a ton of

722
00:30:10,000 --> 00:30:14,320
energy to create. But we generate this

723
00:30:12,000 --> 00:30:17,440
once we store it in efficient storage

724
00:30:14,320 --> 00:30:19,200
with like Amazon S3 that then can be um

725
00:30:17,440 --> 00:30:20,559
mirrored across multiple sites across

726
00:30:19,200 --> 00:30:23,120
the world so that it can be easily

727
00:30:20,559 --> 00:30:25,919
accessed um at multiple points across

728
00:30:23,120 --> 00:30:27,440
the world. Um and so we're generating it

729
00:30:25,919 --> 00:30:29,520
once but we're able to deliver it to

730
00:30:27,440 --> 00:30:33,360
thousands or millions of learners at

731
00:30:29,520 --> 00:30:35,919
scale. And next slide please.

732
00:30:33,360 --> 00:30:38,799
And one final example is translation.

733
00:30:35,919 --> 00:30:41,120
Um, so instead of having learners have

734
00:30:38,799 --> 00:30:43,120
to have like a an account with chatgpt

735
00:30:41,120 --> 00:30:44,640
or something like that to translate

736
00:30:43,120 --> 00:30:47,039
content for them that they come across

737
00:30:44,640 --> 00:30:49,679
in an OEER, why can't we bake that into

738
00:30:47,039 --> 00:30:51,840
the o the the system itself so that in

739
00:30:49,679 --> 00:30:54,320
our case we're translating all of our

740
00:30:51,840 --> 00:30:57,279
content into English, Spanish,

741
00:30:54,320 --> 00:30:58,799
Portuguese, Chinese, and then those are

742
00:30:57,279 --> 00:31:00,480
made available to all learners who come

743
00:30:58,799 --> 00:31:03,120
to the content. So it's translated once

744
00:31:00,480 --> 00:31:05,120
by AI and that initial translation is

745
00:31:03,120 --> 00:31:06,880
costly but once you scale it across

746
00:31:05,120 --> 00:31:10,320
thousands of us owners who are accessing

747
00:31:06,880 --> 00:31:13,200
it the cost per user is is negligible it

748
00:31:10,320 --> 00:31:15,679
the and so the return on investment is

749
00:31:13,200 --> 00:31:18,240
is it approaches infinity because at

750
00:31:15,679 --> 00:31:19,760
some point you create this resource and

751
00:31:18,240 --> 00:31:22,799
then it's available to everybody and

752
00:31:19,760 --> 00:31:25,200
then last slide. So um to compare these

753
00:31:22,799 --> 00:31:27,200
two, non-judicious use tends to be very

754
00:31:25,200 --> 00:31:30,320
expensive to scale versus judicious use

755
00:31:27,200 --> 00:31:33,279
is inexpensive. Um judicious use uses

756
00:31:30,320 --> 00:31:35,440
less energy um and has resulting in much

757
00:31:33,279 --> 00:31:38,399
lower economic or ecological and

758
00:31:35,440 --> 00:31:41,519
economic impact because learner AI

759
00:31:38,399 --> 00:31:43,840
interaction is visible. Uh so like in

760
00:31:41,519 --> 00:31:46,000
like the case of the glossery, the

761
00:31:43,840 --> 00:31:48,240
author can see what the AI is providing

762
00:31:46,000 --> 00:31:50,240
before it's provided to the the learner.

763
00:31:48,240 --> 00:31:52,559
So the author can step in and make do

764
00:31:50,240 --> 00:31:54,320
quality assurance on it. The so AI

765
00:31:52,559 --> 00:31:56,640
responses can be audited, verified and

766
00:31:54,320 --> 00:31:58,960
checked for quality. Those AI responses

767
00:31:56,640 --> 00:32:01,600
are available to everyone. So it's there

768
00:31:58,960 --> 00:32:03,600
isn't an there's no equity concern where

769
00:32:01,600 --> 00:32:05,919
only some people have access to enhanced

770
00:32:03,600 --> 00:32:08,640
resources because they have licenses

771
00:32:05,919 --> 00:32:10,799
with specific AI tools. No, because the

772
00:32:08,640 --> 00:32:12,000
AI uh results are generated and stored

773
00:32:10,799 --> 00:32:14,159
and then shared with everyone. They're

774
00:32:12,000 --> 00:32:15,679
equitably available to everyone and it

775
00:32:14,159 --> 00:32:17,200
requires low bandwidth and can be made

776
00:32:15,679 --> 00:32:18,799
available offline. And so if we create

777
00:32:17,200 --> 00:32:21,679
like for instance an audiobook version

778
00:32:18,799 --> 00:32:23,679
of a chapter or um generate other like

779
00:32:21,679 --> 00:32:25,200
translations of chapters, learners can

780
00:32:23,679 --> 00:32:26,880
download those and access them offline

781
00:32:25,200 --> 00:32:30,159
without having to have persistent access

782
00:32:26,880 --> 00:32:33,440
to like chat GPT for instance.

783
00:32:30,159 --> 00:32:35,919
>> Fantastic. Thank you so much. Um so I'd

784
00:32:33,440 --> 00:32:38,799
now like to invite Clint to have a

785
00:32:35,919 --> 00:32:42,240
larger conversation with ya Royce and

786
00:32:38,799 --> 00:32:45,679
Tori. Um and that will last probably 10

787
00:32:42,240 --> 00:32:48,000
15 minutes or or so 15 minutes and then

788
00:32:45,679 --> 00:32:51,200
we will um come back to your questions.

789
00:32:48,000 --> 00:32:52,559
So do make use of that Q&A function uh

790
00:32:51,200 --> 00:32:54,159
and let us know what questions or

791
00:32:52,559 --> 00:32:55,760
comments you have as you are thinking

792
00:32:54,159 --> 00:32:57,919
them.

793
00:32:55,760 --> 00:32:59,519
>> Great. Thank you Shar. And I I uh I want

794
00:32:57,919 --> 00:33:02,159
to start with you and your project

795
00:32:59,519 --> 00:33:03,760
because uh the localization work that

796
00:33:02,159 --> 00:33:06,240
you did reminds me a lot of the

797
00:33:03,760 --> 00:33:08,080
localization work that we did in the

798
00:33:06,240 --> 00:33:11,120
early days of the BC open textbook

799
00:33:08,080 --> 00:33:13,279
project way back in 2013. Uh one of the

800
00:33:11,120 --> 00:33:14,880
one of the challenges we had when we

801
00:33:13,279 --> 00:33:17,440
started our open textbook project here

802
00:33:14,880 --> 00:33:19,840
in British Columbia was taking US-based

803
00:33:17,440 --> 00:33:22,080
textbooks and contextualizing them and

804
00:33:19,840 --> 00:33:24,240
localizing them for a Canadian context.

805
00:33:22,080 --> 00:33:26,320
And I can tell you it was difficult work

806
00:33:24,240 --> 00:33:28,399
and I really wish we had access to some

807
00:33:26,320 --> 00:33:31,600
of the tools that you did when doing

808
00:33:28,399 --> 00:33:34,559
this kind of uh local localization work.

809
00:33:31,600 --> 00:33:37,039
But I you know as you note um using AI

810
00:33:34,559 --> 00:33:39,039
was not without its challenges to do

811
00:33:37,039 --> 00:33:41,600
localization. And the one I want to hone

812
00:33:39,039 --> 00:33:43,279
in on is the linguistic barriers with AI

813
00:33:41,600 --> 00:33:45,679
translation as I think this might be a

814
00:33:43,279 --> 00:33:48,799
good example of a type of uh cultural

815
00:33:45,679 --> 00:33:50,480
bias that we might find in AI tools. uh

816
00:33:48,799 --> 00:33:52,480
in your paper you noted that um

817
00:33:50,480 --> 00:33:55,039
translating resources while preserving

818
00:33:52,480 --> 00:33:57,519
pedagogical integrity requires more than

819
00:33:55,039 --> 00:33:59,440
direct translation. I wonder if you can

820
00:33:57,519 --> 00:34:02,480
expand on that a little bit. Why was the

821
00:33:59,440 --> 00:34:04,480
direct translation inadequate and how

822
00:34:02,480 --> 00:34:06,240
did you work around that?

823
00:34:04,480 --> 00:34:08,079
>> Yeah, thank you so much Clint. I really

824
00:34:06,240 --> 00:34:10,480
appreciate you naming that experience

825
00:34:08,079 --> 00:34:12,240
with textbooks localization. It

826
00:34:10,480 --> 00:34:15,679
resonates deeply with what we

827
00:34:12,240 --> 00:34:18,480
encountered. But we had AI and all those

828
00:34:15,679 --> 00:34:21,040
tools that supported this process that

829
00:34:18,480 --> 00:34:24,399
made it way easier even fun. Sometimes

830
00:34:21,040 --> 00:34:28,800
we were um juggling with different

831
00:34:24,399 --> 00:34:31,679
dialects and terms um and um in sort of

832
00:34:28,800 --> 00:34:34,159
educating and training AI as well. So

833
00:34:31,679 --> 00:34:37,280
one of the most consistent challenges we

834
00:34:34,159 --> 00:34:41,440
faced was um not just linguistic really.

835
00:34:37,280 --> 00:34:43,919
It was more cultural um and um AI and

836
00:34:41,440 --> 00:34:45,520
machine translation tools. Um Google

837
00:34:43,919 --> 00:34:49,040
translate that was the first thing they

838
00:34:45,520 --> 00:34:51,679
used. It was uh more familiar to faculty

839
00:34:49,040 --> 00:34:54,079
uh involved. Um they often carried more

840
00:34:51,679 --> 00:34:56,079
of that implicit cultural assumptions

841
00:34:54,079 --> 00:34:58,160
already embedded in the source material

842
00:34:56,079 --> 00:35:00,960
whatever they were using. So as I

843
00:34:58,160 --> 00:35:03,359
mentioned in my um today's talk um

844
00:35:00,960 --> 00:35:05,599
universal design itself like seemingly

845
00:35:03,359 --> 00:35:07,920
neutral brought up some unintended

846
00:35:05,599 --> 00:35:11,119
associations with translated directly

847
00:35:07,920 --> 00:35:13,680
into Kazak like ambib which more used

848
00:35:11,119 --> 00:35:15,920
like in my childhood we would say ambib

849
00:35:13,680 --> 00:35:18,480
store which is universal store uh

850
00:35:15,920 --> 00:35:20,640
convenience store. So technically it

851
00:35:18,480 --> 00:35:23,920
worked but culturally it undermined the

852
00:35:20,640 --> 00:35:26,320
integrity of the concept. Um that wasn't

853
00:35:23,920 --> 00:35:29,280
also the translation issue exactly as

854
00:35:26,320 --> 00:35:32,880
you said um around the cultural bias the

855
00:35:29,280 --> 00:35:35,119
idea that uh what is neutral in English

856
00:35:32,880 --> 00:35:37,040
may carry different weight in other or

857
00:35:35,119 --> 00:35:40,000
tone or authority in another language

858
00:35:37,040 --> 00:35:43,200
when it translated um directly. So all

859
00:35:40,000 --> 00:35:45,839
these biases um um magnified by this

860
00:35:43,200 --> 00:35:48,320
machine translation at the same time uh

861
00:35:45,839 --> 00:35:52,000
if used without reflection or review

862
00:35:48,320 --> 00:35:54,560
from um human judgment um may lead to

863
00:35:52,000 --> 00:35:57,680
sort of um not even confusion but

864
00:35:54,560 --> 00:35:59,280
misrepresentation or worse unintentional

865
00:35:57,680 --> 00:36:01,359
um reinforcement of cultural

866
00:35:59,280 --> 00:36:04,640
hierarchies, right? what we call through

867
00:36:01,359 --> 00:36:06,800
this geese of open access but yet we

868
00:36:04,640 --> 00:36:09,760
wanted really to that it sounded

869
00:36:06,800 --> 00:36:12,480
natural, respectful and pedagogically

870
00:36:09,760 --> 00:36:15,280
coherent. So educators do not learn

871
00:36:12,480 --> 00:36:17,839
something new but rather review what

872
00:36:15,280 --> 00:36:21,520
they have been doing with these new

873
00:36:17,839 --> 00:36:23,920
tools or new approaches. So we brought

874
00:36:21,520 --> 00:36:26,240
really I've involved um a lot of

875
00:36:23,920 --> 00:36:28,000
feedback from uh faculty at at

876
00:36:26,240 --> 00:36:31,359
university. I'm very grateful to them

877
00:36:28,000 --> 00:36:33,119
here. Um uh we even asked some of the

878
00:36:31,359 --> 00:36:34,960
educators from different regions of

879
00:36:33,119 --> 00:36:37,440
Kazakhstan where we would have some

880
00:36:34,960 --> 00:36:39,200
dialect. So we wanted the term would

881
00:36:37,440 --> 00:36:40,560
resonate with someone in western

882
00:36:39,200 --> 00:36:43,040
Kazakhstan as well as in south

883
00:36:40,560 --> 00:36:45,839
Kazakhstan where I'm from. So we uh

884
00:36:43,040 --> 00:36:48,800
wanted to we had to actually to think

885
00:36:45,839 --> 00:36:50,960
about those uh pecularities there and it

886
00:36:48,800 --> 00:36:54,240
was not only about clarity. I think for

887
00:36:50,960 --> 00:36:57,680
me it was more about credibility.

888
00:36:54,240 --> 00:36:59,280
Thanks. So to get around some of what

889
00:36:57,680 --> 00:37:01,200
you were finding, it sounds like you

890
00:36:59,280 --> 00:37:02,960
definitely had humans in the loop kind

891
00:37:01,200 --> 00:37:05,359
of reviewing the work that was coming

892
00:37:02,960 --> 00:37:08,079
out and the importance of having people

893
00:37:05,359 --> 00:37:10,800
still involved checking the work I think

894
00:37:08,079 --> 00:37:12,960
is something that I also picked up on in

895
00:37:10,800 --> 00:37:14,480
uh Royce and Tori's paper too. The

896
00:37:12,960 --> 00:37:16,880
importance of humans in the loop and

897
00:37:14,480 --> 00:37:19,760
having oversight. I think you mentioned

898
00:37:16,880 --> 00:37:22,320
uh human quality assurance is essential

899
00:37:19,760 --> 00:37:24,400
to correct AI hallucinations, errors and

900
00:37:22,320 --> 00:37:25,440
misinformation. And so Tori and Roy, I

901
00:37:24,400 --> 00:37:27,440
want to bring you into the conversation

902
00:37:25,440 --> 00:37:29,359
here and just maybe tell us some of the

903
00:37:27,440 --> 00:37:31,440
ways that you're thinking about how to

904
00:37:29,359 --> 00:37:34,240
address this in your work to ensure that

905
00:37:31,440 --> 00:37:35,839
there are humans in the loop. I can

906
00:37:34,240 --> 00:37:38,720
start briefly just from an author

907
00:37:35,839 --> 00:37:41,359
perspective. So I have written several

908
00:37:38,720 --> 00:37:44,800
books on the edtech books platform which

909
00:37:41,359 --> 00:37:47,119
are open educational resources OEARS and

910
00:37:44,800 --> 00:37:48,960
along the way Royce has been adding in a

911
00:37:47,119 --> 00:37:51,760
lot of different features which I always

912
00:37:48,960 --> 00:37:53,599
love to try out in my books. Um, and at

913
00:37:51,760 --> 00:37:56,400
one point there was like an AI generated

914
00:37:53,599 --> 00:37:58,320
abstract and I'm talking one of my books

915
00:37:56,400 --> 00:38:00,160
has like 700 pages cuz I have a

916
00:37:58,320 --> 00:38:01,760
co-author who just loves to write

917
00:38:00,160 --> 00:38:04,960
nonstop.

918
00:38:01,760 --> 00:38:06,240
Um, and so,

919
00:38:04,960 --> 00:38:08,160
you know, I don't have time to go

920
00:38:06,240 --> 00:38:10,560
through and write an abstract for every

921
00:38:08,160 --> 00:38:12,960
single one of those chapters.

922
00:38:10,560 --> 00:38:16,079
But when I had the AI generated version,

923
00:38:12,960 --> 00:38:18,720
I was like, ooh, this is not like fully

924
00:38:16,079 --> 00:38:21,520
capturing what happened in this chapter.

925
00:38:18,720 --> 00:38:23,920
Um, but it's a starting point. And so

926
00:38:21,520 --> 00:38:26,960
for me, I found it helpful that it like

927
00:38:23,920 --> 00:38:28,880
it put text on paper for me then to work

928
00:38:26,960 --> 00:38:30,480
around and revise and figure out what is

929
00:38:28,880 --> 00:38:32,240
the most important part of this chapter

930
00:38:30,480 --> 00:38:34,320
I want to include in this abstract

931
00:38:32,240 --> 00:38:36,160
rather than starting from a blank page.

932
00:38:34,320 --> 00:38:39,359
Uh, and I do think that helped speed up

933
00:38:36,160 --> 00:38:40,960
the ability to include abstracts. And um

934
00:38:39,359 --> 00:38:43,200
but I do think there had to be a human

935
00:38:40,960 --> 00:38:45,280
in the loop in that process because the

936
00:38:43,200 --> 00:38:47,119
AI I don't think ever created an

937
00:38:45,280 --> 00:38:50,240
abstract that was fully accurate and

938
00:38:47,119 --> 00:38:52,400
correct for any of the chapters um its

939
00:38:50,240 --> 00:38:53,520
very first try. And that's we know how

940
00:38:52,400 --> 00:38:55,280
these tools work. They're they're

941
00:38:53,520 --> 00:38:57,680
predictability machines guessing which

942
00:38:55,280 --> 00:38:59,440
words go together. Um so you have to

943
00:38:57,680 --> 00:39:01,359
have that human aspect and that's what

944
00:38:59,440 --> 00:39:04,880
Royce has really built into the

945
00:39:01,359 --> 00:39:06,480
edtechbooks platform.

946
00:39:04,880 --> 00:39:08,800
>> Yeah, just to build on that. So it's

947
00:39:06,480 --> 00:39:10,880
it's it's accuracy, it's quality, it's

948
00:39:08,800 --> 00:39:13,040
speed, but it's also contextualization

949
00:39:10,880 --> 00:39:15,119
too. Um that's one of the things that AI

950
00:39:13,040 --> 00:39:16,320
struggles with a bit. Um I mean if you

951
00:39:15,119 --> 00:39:17,839
especially if you're not providing it

952
00:39:16,320 --> 00:39:19,119
with deep understanding of the context.

953
00:39:17,839 --> 00:39:20,800
So if I'm writing to elementary school

954
00:39:19,119 --> 00:39:22,480
teachers for instance, that context is

955
00:39:20,800 --> 00:39:25,040
going to be very different from writing

956
00:39:22,480 --> 00:39:27,520
to a group of professional engineers and

957
00:39:25,040 --> 00:39:30,480
and what I convey, how I convey it. And

958
00:39:27,520 --> 00:39:32,400
so, um, allowing the a using the AI to

959
00:39:30,480 --> 00:39:34,400
create kind of a boilerplate or a

960
00:39:32,400 --> 00:39:35,440
beginning, uh, kind of outline of what

961
00:39:34,400 --> 00:39:36,960
this looks like. Just getting something

962
00:39:35,440 --> 00:39:38,960
on the page is really, I don't know,

963
00:39:36,960 --> 00:39:40,560
encouraging as an author because then

964
00:39:38,960 --> 00:39:42,400
it's easier to edit. It's easier to go

965
00:39:40,560 --> 00:39:44,720
and adjust than it is to just stare at a

966
00:39:42,400 --> 00:39:46,640
blank page. And so, that that applies to

967
00:39:44,720 --> 00:39:50,480
abstracts. It applies to like glossery

968
00:39:46,640 --> 00:39:51,680
terms. Um, and and and even beyond that

969
00:39:50,480 --> 00:39:53,920
too, that to things that are more

970
00:39:51,680 --> 00:39:56,000
strictly created entirely by the AI like

971
00:39:53,920 --> 00:39:57,599
audio books. um you know just being able

972
00:39:56,000 --> 00:39:59,119
to go and listen to it and make sure

973
00:39:57,599 --> 00:40:01,760
that you know you're you're happy with

974
00:39:59,119 --> 00:40:03,280
the way that it sounds um is is an

975
00:40:01,760 --> 00:40:04,880
important step. And I guess translations

976
00:40:03,280 --> 00:40:07,520
as well translations are a bit tricky

977
00:40:04,880 --> 00:40:09,520
because um if when we like for instance

978
00:40:07,520 --> 00:40:11,119
if I translate my content into 10

979
00:40:09,520 --> 00:40:13,920
different languages I I don't know all

980
00:40:11,119 --> 00:40:15,440
10 of those languages. So um we have

981
00:40:13,920 --> 00:40:17,680
built in the ability for the author to

982
00:40:15,440 --> 00:40:19,599
go in and edit what the the AI has

983
00:40:17,680 --> 00:40:22,079
generated and is now providing in those

984
00:40:19,599 --> 00:40:23,599
different languages. But unless I also

985
00:40:22,079 --> 00:40:26,800
collaborate with people who speak those

986
00:40:23,599 --> 00:40:28,240
languages or that can help me um uh

987
00:40:26,800 --> 00:40:31,440
engage in quality assurance with those

988
00:40:28,240 --> 00:40:32,960
languages um then even though I I'm

989
00:40:31,440 --> 00:40:35,359
there as the author I'm not able to do

990
00:40:32,960 --> 00:40:38,240
much or I don't even know what to do. So

991
00:40:35,359 --> 00:40:39,920
so there is so part of it is how we

992
00:40:38,240 --> 00:40:41,839
structure our relationships with the AI

993
00:40:39,920 --> 00:40:44,000
by by putting the author between the AI

994
00:40:41,839 --> 00:40:45,839
and the learner but part of it too is

995
00:40:44,000 --> 00:40:47,839
thinking about what resources and

996
00:40:45,839 --> 00:40:49,520
literacies are needed to the author so

997
00:40:47,839 --> 00:40:54,560
they can can actually engage in quality

998
00:40:49,520 --> 00:40:56,240
assurance. the AI has generated results.

999
00:40:54,560 --> 00:40:58,000
>> I want to circle back to that literacy

1000
00:40:56,240 --> 00:40:59,200
piece in just a moment here, but first I

1001
00:40:58,000 --> 00:41:01,599
want to I want to talk a little bit

1002
00:40:59,200 --> 00:41:04,000
about the environmental aspects uh in

1003
00:41:01,599 --> 00:41:05,680
your paper uh and the environmental cost

1004
00:41:04,000 --> 00:41:07,520
of using generative AI because your

1005
00:41:05,680 --> 00:41:09,680
paper raises the environmental cost as

1006
00:41:07,520 --> 00:41:12,240
really being a serious social justice

1007
00:41:09,680 --> 00:41:14,880
issue. Uh and I think there's a real

1008
00:41:12,240 --> 00:41:16,480
tension especially for us in education.

1009
00:41:14,880 --> 00:41:18,720
I mean we're feeling this pressure to

1010
00:41:16,480 --> 00:41:20,240
perhaps use these tools. we're seeing

1011
00:41:18,720 --> 00:41:22,480
some of the benefits of using these

1012
00:41:20,240 --> 00:41:25,200
tools, but we know that there there are

1013
00:41:22,480 --> 00:41:27,280
steep costs to actually using them. And

1014
00:41:25,200 --> 00:41:29,680
and this is kind of a big question, but

1015
00:41:27,280 --> 00:41:32,079
I mean, how do you think we go about

1016
00:41:29,680 --> 00:41:34,480
reconciling that tension between the

1017
00:41:32,079 --> 00:41:37,119
benefits of generative AI and the

1018
00:41:34,480 --> 00:41:38,720
environmental costs that you talk about?

1019
00:41:37,119 --> 00:41:40,880
>> Well, I think Tori brought brought to my

1020
00:41:38,720 --> 00:41:42,720
attention just uh recent research on

1021
00:41:40,880 --> 00:41:44,160
even how youth are using AI, right? So,

1022
00:41:42,720 --> 00:41:46,800
they use it for information seeking

1023
00:41:44,160 --> 00:41:48,319
primarily. So, for instance, if they

1024
00:41:46,800 --> 00:41:51,119
don't know what the word abduction

1025
00:41:48,319 --> 00:41:53,040
means, they'll go to chat GPT to ask it

1026
00:41:51,119 --> 00:41:55,280
what abduction is. Well, doing that

1027
00:41:53,040 --> 00:41:57,599
using a a chatbot, specifically like the

1028
00:41:55,280 --> 00:41:59,359
the newest versions of the chatbot, like

1029
00:41:57,599 --> 00:42:01,280
if you're using a 4.1 or something like

1030
00:41:59,359 --> 00:42:02,960
that, you're going to use more 10 times

1031
00:42:01,280 --> 00:42:04,560
more energy than if you did a Google

1032
00:42:02,960 --> 00:42:06,319
search, which is going to use 10 or 100

1033
00:42:04,560 --> 00:42:08,160
times more energy than if you just went

1034
00:42:06,319 --> 00:42:11,520
to Mariam Webster and did it and

1035
00:42:08,160 --> 00:42:13,520
actually typed it in there. So, um, or

1036
00:42:11,520 --> 00:42:15,680
had had a dictionary in your operating

1037
00:42:13,520 --> 00:42:17,520
system that you're using. Um, so I mean,

1038
00:42:15,680 --> 00:42:19,839
super easy. I'm I'm on a Mac. Just hit

1039
00:42:17,520 --> 00:42:21,440
command space and type in a word and it

1040
00:42:19,839 --> 00:42:23,280
uses the built-in dictionary to define

1041
00:42:21,440 --> 00:42:24,560
it for you. But youth do not know how to

1042
00:42:23,280 --> 00:42:26,240
do that. So, that's a literacy question

1043
00:42:24,560 --> 00:42:27,760
which we can talk more about later. But

1044
00:42:26,240 --> 00:42:29,440
I think part of what it speaks to is

1045
00:42:27,760 --> 00:42:30,800
just that we don't understand what our

1046
00:42:29,440 --> 00:42:33,119
tools are good for and what they're

1047
00:42:30,800 --> 00:42:34,800
efficient at and what is like because

1048
00:42:33,119 --> 00:42:36,400
there's cost associated with everything

1049
00:42:34,800 --> 00:42:38,960
that we do with them. We don't know what

1050
00:42:36,400 --> 00:42:42,720
is what is a legitimate cost. Is it it's

1051
00:42:38,960 --> 00:42:44,480
is it really um desirable to use this

1052
00:42:42,720 --> 00:42:47,440
much energy to define a word that I

1053
00:42:44,480 --> 00:42:49,200
could just easily find a definition to a

1054
00:42:47,440 --> 00:42:50,720
different way? And so I think so much of

1055
00:42:49,200 --> 00:42:53,040
it is, you know, there's the literacy

1056
00:42:50,720 --> 00:42:54,960
aspect, but there's also just like for

1057
00:42:53,040 --> 00:42:57,920
those of us who are creating tools or in

1058
00:42:54,960 --> 00:42:59,760
positions to um provide resources to

1059
00:42:57,920 --> 00:43:01,520
people. Hopefully, we're not falling

1060
00:42:59,760 --> 00:43:03,440
into that same trap where we're not just

1061
00:43:01,520 --> 00:43:04,960
throwing a you know, oh, I need to

1062
00:43:03,440 --> 00:43:06,880
define terms. Let me throw the newest

1063
00:43:04,960 --> 00:43:08,880
version of chat GPT at it which I know

1064
00:43:06,880 --> 00:43:10,960
is going to be super resource intensive.

1065
00:43:08,880 --> 00:43:12,720
Um there are lots of different large

1066
00:43:10,960 --> 00:43:14,400
language models out there. Lots of them

1067
00:43:12,720 --> 00:43:16,880
are more efficient than others. So for

1068
00:43:14,400 --> 00:43:18,880
instance in our current onet books in

1069
00:43:16,880 --> 00:43:20,800
our current use of the glossery and the

1070
00:43:18,880 --> 00:43:22,960
abstract feature we're using llama 3

1071
00:43:20,800 --> 00:43:24,960
point I think it's 3.2 but we're using

1072
00:43:22,960 --> 00:43:27,200
the the three billion or the three

1073
00:43:24,960 --> 00:43:30,240
billion parameter model which is much

1074
00:43:27,200 --> 00:43:32,079
more efficient than than the the the

1075
00:43:30,240 --> 00:43:33,440
kind of flagship llama is 70 billion.

1076
00:43:32,079 --> 00:43:36,160
And I and I don't know know what the

1077
00:43:33,440 --> 00:43:37,680
chat GPT model current GPT models are

1078
00:43:36,160 --> 00:43:39,040
right now. If you're not an AI person,

1079
00:43:37,680 --> 00:43:40,960
what that just basically means is a

1080
00:43:39,040 --> 00:43:43,119
three billion parameter model, you can

1081
00:43:40,960 --> 00:43:45,599
run it even like on a pretty, you know,

1082
00:43:43,119 --> 00:43:48,400
mid-range like laptop. Um, if you wanted

1083
00:43:45,599 --> 00:43:50,240
to, it's not a super intensive model to

1084
00:43:48,400 --> 00:43:53,440
run versus if you're running cutting

1085
00:43:50,240 --> 00:43:55,119
edge models, uh, you need a super high

1086
00:43:53,440 --> 00:43:57,440
performing machine that it's going to

1087
00:43:55,119 --> 00:43:59,920
use a ton of energy. Um, I actually have

1088
00:43:57,440 --> 00:44:02,160
a super high performing Mac Studio uh

1089
00:43:59,920 --> 00:44:03,440
under my desk here right now that is

1090
00:44:02,160 --> 00:44:04,800
constantly running stuff in the

1091
00:44:03,440 --> 00:44:06,480
background because I'm analyzing data

1092
00:44:04,800 --> 00:44:09,520
and I can feel the heat coming off of

1093
00:44:06,480 --> 00:44:11,280
it. Um, uh, and so I have to have the

1094
00:44:09,520 --> 00:44:12,960
air up higher in my office. So, I'm

1095
00:44:11,280 --> 00:44:14,640
creating an ecological concern here

1096
00:44:12,960 --> 00:44:17,839
within my office just using how I'm

1097
00:44:14,640 --> 00:44:19,599
using how I'm using AI. But, um, but

1098
00:44:17,839 --> 00:44:22,160
using more efficient models and I think

1099
00:44:19,599 --> 00:44:23,680
using models that are that are kind of

1100
00:44:22,160 --> 00:44:25,280
that are designed for the kinds of

1101
00:44:23,680 --> 00:44:28,079
things that we're after. I don't need to

1102
00:44:25,280 --> 00:44:31,119
throw a 70 billion parameter model at a

1103
00:44:28,079 --> 00:44:33,680
simple definition task. Um, and by not

1104
00:44:31,119 --> 00:44:35,359
doing that, I'm I'm drastically saving

1105
00:44:33,680 --> 00:44:37,520
cost. I'm drastically saving um

1106
00:44:35,359 --> 00:44:38,480
ecological impacts.

1107
00:44:37,520 --> 00:44:40,400
>> It's interesting that you mentioned

1108
00:44:38,480 --> 00:44:42,000
that. I mean, one of the things I I use

1109
00:44:40,400 --> 00:44:44,400
a large language model, but I've been

1110
00:44:42,000 --> 00:44:45,839
working on a locally hosted large

1111
00:44:44,400 --> 00:44:49,680
language model. So, I've been using a

1112
00:44:45,839 --> 00:44:51,520
tool called LM Studio and running models

1113
00:44:49,680 --> 00:44:54,160
just on my desktop because that's one of

1114
00:44:51,520 --> 00:44:57,280
the ways that I kind of feel okay about

1115
00:44:54,160 --> 00:44:59,040
the environmental impact of of using

1116
00:44:57,280 --> 00:45:01,920
large language models is if I can at

1117
00:44:59,040 --> 00:45:04,079
least run it locally on my laptop, then

1118
00:45:01,920 --> 00:45:06,960
I'm not, you know, relying on huge data

1119
00:45:04,079 --> 00:45:09,119
centers uh to power the the work that

1120
00:45:06,960 --> 00:45:10,560
I'm doing. And it has been it has been

1121
00:45:09,119 --> 00:45:12,800
one of the ways that I've been kind of

1122
00:45:10,560 --> 00:45:15,280
managing that environmental concern and

1123
00:45:12,800 --> 00:45:18,800
I think maybe holds out a little bit of

1124
00:45:15,280 --> 00:45:21,599
hope for uh uh for you know the the sort

1125
00:45:18,800 --> 00:45:23,760
of more basic queries that we tend to do

1126
00:45:21,599 --> 00:45:26,160
or that that as you mentioned youth tend

1127
00:45:23,760 --> 00:45:27,920
to do on uh on the large language

1128
00:45:26,160 --> 00:45:30,000
models.

1129
00:45:27,920 --> 00:45:32,160
Um, I want to stick on the technology

1130
00:45:30,000 --> 00:45:34,079
piece a bit and just talk a little bit

1131
00:45:32,160 --> 00:45:36,960
and and I know uh Rory, you talked

1132
00:45:34,079 --> 00:45:38,960
Royce, you talked a little bit about um,

1133
00:45:36,960 --> 00:45:40,880
Lama and I I do want to come back to

1134
00:45:38,960 --> 00:45:42,319
that, but I mean the I want to talk

1135
00:45:40,880 --> 00:45:44,240
about the technology choices you made

1136
00:45:42,319 --> 00:45:46,079
for both of your projects because I

1137
00:45:44,240 --> 00:45:47,839
think the technology choices kind of do

1138
00:45:46,079 --> 00:45:50,640
paint a complex and really a very

1139
00:45:47,839 --> 00:45:53,119
realistic picture of the kind of mix of

1140
00:45:50,640 --> 00:45:55,359
open and proprietary tools that we often

1141
00:45:53,119 --> 00:45:57,520
have to use in open education projects.

1142
00:45:55,359 --> 00:45:59,040
So a let me start with you here. Can you

1143
00:45:57,520 --> 00:46:01,359
talk through some of the rationale for

1144
00:45:59,040 --> 00:46:03,200
your technology choices for your project

1145
00:46:01,359 --> 00:46:04,800
and and is there anything that you think

1146
00:46:03,200 --> 00:46:07,040
you might have done differently with

1147
00:46:04,800 --> 00:46:10,079
your technology choices?

1148
00:46:07,040 --> 00:46:13,200
>> Um yeah that was the tricky part. Of

1149
00:46:10,079 --> 00:46:15,839
course the easiest one was chachi as uh

1150
00:46:13,200 --> 00:46:20,240
our colleagues at other university also

1151
00:46:15,839 --> 00:46:22,240
had started experimenting that um but

1152
00:46:20,240 --> 00:46:25,920
let's say for translation like Google

1153
00:46:22,240 --> 00:46:29,119
translate was the first um intuitive way

1154
00:46:25,920 --> 00:46:32,960
like to go exactly for English the ma

1155
00:46:29,119 --> 00:46:35,599
matching the um terms. So when um I

1156
00:46:32,960 --> 00:46:38,240
started to explore AI tools, I think um

1157
00:46:35,599 --> 00:46:42,160
I looked for those who had free access

1158
00:46:38,240 --> 00:46:44,560
in the first place. So uh faculty back

1159
00:46:42,160 --> 00:46:46,560
in uh Kazakhstan context wouldn't or

1160
00:46:44,560 --> 00:46:49,599
university wouldn't have burden like to

1161
00:46:46,560 --> 00:46:52,400
pay for those things. Um I didn't have

1162
00:46:49,599 --> 00:46:54,800
special funding for that. So it was all

1163
00:46:52,400 --> 00:46:58,000
voluntary pro uh like a volunteering

1164
00:46:54,800 --> 00:47:00,960
project for us. So um it it should be

1165
00:46:58,000 --> 00:47:03,440
free uh at least for a trial period for

1166
00:47:00,960 --> 00:47:06,240
a couple of weeks or months that even

1167
00:47:03,440 --> 00:47:08,960
would be helpful and uh because it was a

1168
00:47:06,240 --> 00:47:13,200
very um speedy process we wanted to be

1169
00:47:08,960 --> 00:47:14,880
fast and um prototype fast based on this

1170
00:47:13,200 --> 00:47:17,200
SAM model that I followed for

1171
00:47:14,880 --> 00:47:20,480
instructional design. So and a

1172
00:47:17,200 --> 00:47:23,599
technology that um at that moment was

1173
00:47:20,480 --> 00:47:26,560
mainly accessible uh was through the

1174
00:47:23,599 --> 00:47:29,839
open AI platform definitely uh we've

1175
00:47:26,560 --> 00:47:33,599
tested with PO uh another platform where

1176
00:47:29,839 --> 00:47:36,880
we looked specifically for um their like

1177
00:47:33,599 --> 00:47:41,040
approaches what kind of data um they use

1178
00:47:36,880 --> 00:47:44,319
or how they process it um

1179
00:47:41,040 --> 00:47:46,640
in that context what's interesting um

1180
00:47:44,319 --> 00:47:49,440
not the um like ethical or privacy

1181
00:47:46,640 --> 00:47:51,760
issues they pop up later on with the

1182
00:47:49,440 --> 00:47:55,119
faculty as they started to learn more

1183
00:47:51,760 --> 00:47:57,359
about the technology. So um and we

1184
00:47:55,119 --> 00:48:00,319
didn't have a clear criteria for which

1185
00:47:57,359 --> 00:48:04,640
would be the best or how even to access

1186
00:48:00,319 --> 00:48:07,520
or assess that uh technology um uh I

1187
00:48:04,640 --> 00:48:10,960
think uh the start was the very simple

1188
00:48:07,520 --> 00:48:14,319
like free accessible easy to use in any

1189
00:48:10,960 --> 00:48:17,839
language um and helpful. So

1190
00:48:14,319 --> 00:48:19,920
>> wonderful. Um, if I may interject,

1191
00:48:17,839 --> 00:48:22,079
there's a handful of really juicy

1192
00:48:19,920 --> 00:48:24,559
questions in the Q&A. Um, so we're going

1193
00:48:22,079 --> 00:48:28,240
to go ahead and start with Vikram Singh

1194
00:48:24,559 --> 00:48:31,280
who asks how to use judicious AI in the

1195
00:48:28,240 --> 00:48:33,440
case of dynamic content.

1196
00:48:31,280 --> 00:48:35,040
>> Yeah. So it's a great question. Um, the

1197
00:48:33,440 --> 00:48:37,839
short answer is we have to build systems

1198
00:48:35,040 --> 00:48:41,119
that allow us to do it. And it's similar

1199
00:48:37,839 --> 00:48:45,280
to uh just modern web approaches. So,

1200
00:48:41,119 --> 00:48:47,119
um, if, uh, so, so going back in time to

1201
00:48:45,280 --> 00:48:49,599
when we first did when I first built the

1202
00:48:47,119 --> 00:48:51,760
the alpha version of edtech books, I had

1203
00:48:49,599 --> 00:48:54,000
nonAI tools that I built that would

1204
00:48:51,760 --> 00:48:56,160
generate PDFs of all the content on the

1205
00:48:54,000 --> 00:48:57,839
platform. And so, the issue was, yeah,

1206
00:48:56,160 --> 00:48:59,599
dynamic content like authors are

1207
00:48:57,839 --> 00:49:01,119
adjusting content continually, but we

1208
00:48:59,599 --> 00:49:03,440
want to provide a static version of it

1209
00:49:01,119 --> 00:49:05,440
as a PDF. And so, my brilliant thought

1210
00:49:03,440 --> 00:49:07,119
was, let me just have the the web server

1211
00:49:05,440 --> 00:49:09,119
generate the PDF every time someone

1212
00:49:07,119 --> 00:49:11,040
wants to access it. But the problem is

1213
00:49:09,119 --> 00:49:13,520
the amount of energy to generate a PDF

1214
00:49:11,040 --> 00:49:14,800
out of that dynamic content is it's

1215
00:49:13,520 --> 00:49:16,240
really heavy. And so when you have a

1216
00:49:14,800 --> 00:49:18,640
hundred people try to do it at the exact

1217
00:49:16,240 --> 00:49:21,520
same time, your server fails very

1218
00:49:18,640 --> 00:49:23,119
quickly. Um and so but it's a similar

1219
00:49:21,520 --> 00:49:24,640
kind of thing with AI in the sense that

1220
00:49:23,119 --> 00:49:27,200
yeah, you have dynamic content, you want

1221
00:49:24,640 --> 00:49:29,520
to create like static snapshots of it at

1222
00:49:27,200 --> 00:49:31,280
different times um that then you provide

1223
00:49:29,520 --> 00:49:32,559
efficiently to people. But so you just

1224
00:49:31,280 --> 00:49:33,760
have to figure out when is it

1225
00:49:32,559 --> 00:49:35,280
appropriate to take one of those

1226
00:49:33,760 --> 00:49:36,960
snapshots. You don't want to create a

1227
00:49:35,280 --> 00:49:38,880
snapshot every single time. Like say

1228
00:49:36,960 --> 00:49:40,400
someone wants a translation of your

1229
00:49:38,880 --> 00:49:42,240
chapter. You don't want to create a

1230
00:49:40,400 --> 00:49:44,480
snapshot every single time someone asks

1231
00:49:42,240 --> 00:49:45,920
for a translation. Maybe when you when

1232
00:49:44,480 --> 00:49:47,680
you edit the content, you make a

1233
00:49:45,920 --> 00:49:50,160
significant edit, then you flag it, and

1234
00:49:47,680 --> 00:49:52,400
then it creates um then it creates a

1235
00:49:50,160 --> 00:49:54,079
snapshot of it that is that the AI uses

1236
00:49:52,400 --> 00:49:56,079
as to create a translation that it

1237
00:49:54,079 --> 00:49:57,760
efficiently serves up. Um but you have

1238
00:49:56,079 --> 00:49:59,599
to create kind of what those milestone

1239
00:49:57,760 --> 00:50:03,119
stones are. Decide how much of a change

1240
00:49:59,599 --> 00:50:06,319
to the dynamic content uh merits

1241
00:50:03,119 --> 00:50:08,880
regeneration with the AI or or or re

1242
00:50:06,319 --> 00:50:10,640
reinvolving the AI. Um, so that's always

1243
00:50:08,880 --> 00:50:12,160
challenging. Yeah. So like in terms of

1244
00:50:10,640 --> 00:50:13,760
like the audio like very concrete

1245
00:50:12,160 --> 00:50:16,160
example in terms of like the audiobooks

1246
00:50:13,760 --> 00:50:17,680
that we generate, if someone goes on and

1247
00:50:16,160 --> 00:50:19,599
corrects a typo, I'm probably not going

1248
00:50:17,680 --> 00:50:21,359
to regenerate the entire audio book for

1249
00:50:19,599 --> 00:50:23,760
that for that chapter just because a

1250
00:50:21,359 --> 00:50:25,280
typo is changed in the text. But if

1251
00:50:23,760 --> 00:50:27,520
paragraphs are added or something like

1252
00:50:25,280 --> 00:50:29,599
that, then yeah, we'll we'll initiate a

1253
00:50:27,520 --> 00:50:32,160
rebuild.

1254
00:50:29,599 --> 00:50:34,079
>> Wonderful. Thank you.

1255
00:50:32,160 --> 00:50:37,119
Our next question comes from Muhammad

1256
00:50:34,079 --> 00:50:39,760
Sed and the question is in the context

1257
00:50:37,119 --> 00:50:43,359
of professional education, how can

1258
00:50:39,760 --> 00:50:46,160
institutions ensure that AI tools used

1259
00:50:43,359 --> 00:50:49,280
for assessment and feedback are fair,

1260
00:50:46,160 --> 00:50:51,599
transparent and free from bias,

1261
00:50:49,280 --> 00:50:54,000
especially when serving diverse learner

1262
00:50:51,599 --> 00:50:57,839
populations?

1263
00:50:54,000 --> 00:51:00,319
>> Yeah, thank you Muhammad. Um that's um

1264
00:50:57,839 --> 00:51:02,480
really critical question um especially

1265
00:51:00,319 --> 00:51:05,280
in the context of professional education

1266
00:51:02,480 --> 00:51:08,960
definitely where assessment shapes often

1267
00:51:05,280 --> 00:51:12,000
shapes access to opportunity and if

1268
00:51:08,960 --> 00:51:14,800
institutions want to ensure that uh AI

1269
00:51:12,000 --> 00:51:17,520
tools used for um assessment and

1270
00:51:14,800 --> 00:51:19,760
feedback are fair and transparent

1271
00:51:17,520 --> 00:51:22,880
especially for diverse learning uh

1272
00:51:19,760 --> 00:51:25,040
population the first step at least what

1273
00:51:22,880 --> 00:51:27,119
we've learned and observed from our

1274
00:51:25,040 --> 00:51:30,960
experience igrants is to recognize that

1275
00:51:27,119 --> 00:51:34,079
bias is not a technical error there and

1276
00:51:30,960 --> 00:51:37,839
it's often a reflection of uh whose

1277
00:51:34,079 --> 00:51:40,559
language norms um learning behaviors uh

1278
00:51:37,839 --> 00:51:45,440
the system was trained on. And one of

1279
00:51:40,559 --> 00:51:49,599
the lessons um that specifically um

1280
00:51:45,440 --> 00:51:52,480
stood out uh for my colleagues um there

1281
00:51:49,599 --> 00:51:55,040
is that fairness requires human context.

1282
00:51:52,480 --> 00:51:58,079
We discussed that um specifically at the

1283
00:51:55,040 --> 00:52:00,240
level of design um and it's not enough

1284
00:51:58,079 --> 00:52:01,760
to translate just rubrics or adapt

1285
00:52:00,240 --> 00:52:05,200
instructions

1286
00:52:01,760 --> 00:52:06,720
um institutions here we really wanted to

1287
00:52:05,200 --> 00:52:09,119
um

1288
00:52:06,720 --> 00:52:12,640
prompt them to think were these models

1289
00:52:09,119 --> 00:52:15,680
trained with us in mind right um no

1290
00:52:12,640 --> 00:52:18,720
obviously in the beginning so um we

1291
00:52:15,680 --> 00:52:20,319
recognize that culturally relevant let's

1292
00:52:18,720 --> 00:52:23,280
say expression of knowledge that's

1293
00:52:20,319 --> 00:52:28,000
something that um even The feedback when

1294
00:52:23,280 --> 00:52:31,680
the feedback was given um the um we we

1295
00:52:28,000 --> 00:52:35,680
we should be aware that the feedback was

1296
00:52:31,680 --> 00:52:37,440
maybe um based on western assump

1297
00:52:35,680 --> 00:52:40,720
academic writing conventions or

1298
00:52:37,440 --> 00:52:44,480
unfamiliar terminology or a different

1299
00:52:40,720 --> 00:52:48,480
logic of argumentation that is um alien

1300
00:52:44,480 --> 00:52:50,319
to that cultural or context. Right? So

1301
00:52:48,480 --> 00:52:54,000
um and that feedback will not be

1302
00:52:50,319 --> 00:52:56,400
actionable. So therefore um for us um

1303
00:52:54,000 --> 00:53:00,000
was to get the feedback and again

1304
00:52:56,400 --> 00:53:02,480
translate it quote unquote to the um

1305
00:53:00,000 --> 00:53:05,760
management of the university who were

1306
00:53:02,480 --> 00:53:09,440
observing the project in a way that um

1307
00:53:05,760 --> 00:53:14,000
it also um supports that learning and

1308
00:53:09,440 --> 00:53:16,319
helps them to improve. So um here that

1309
00:53:14,000 --> 00:53:19,200
building human uh in the loop validation

1310
00:53:16,319 --> 00:53:22,960
I think it's very important um being

1311
00:53:19,200 --> 00:53:26,079
able to interpret AI uh behavior or make

1312
00:53:22,960 --> 00:53:29,040
AI behavior interpretable. So faculty

1313
00:53:26,079 --> 00:53:31,440
and um staff should be able to see that

1314
00:53:29,040 --> 00:53:34,880
what's the model is evaluating or and

1315
00:53:31,440 --> 00:53:38,079
why. So the clear explanation reasoning

1316
00:53:34,880 --> 00:53:40,960
um is important and um have a feedback

1317
00:53:38,079 --> 00:53:43,280
again not as a design opportunity not

1318
00:53:40,960 --> 00:53:46,960
just a measurement output if it's a good

1319
00:53:43,280 --> 00:53:50,800
project or a good course or a good um so

1320
00:53:46,960 --> 00:53:54,960
uh we need to u not to guide learning

1321
00:53:50,800 --> 00:53:57,839
across uh context um like not only

1322
00:53:54,960 --> 00:54:00,079
through that direct feedback again so

1323
00:53:57,839 --> 00:54:03,119
for institutions to be aware of that

1324
00:54:00,079 --> 00:54:04,319
it's important Beautiful.

1325
00:54:03,119 --> 00:54:06,720
Thank you.

1326
00:54:04,319 --> 00:54:08,480
>> Um, our next I just want to get as many

1327
00:54:06,720 --> 00:54:10,400
questions in as possible, but please

1328
00:54:08,480 --> 00:54:11,839
Rice, go ahead.

1329
00:54:10,400 --> 00:54:13,520
>> Um, I just want to mention just

1330
00:54:11,839 --> 00:54:14,880
auditability is super important for

1331
00:54:13,520 --> 00:54:16,559
this. So, one of the big problems with

1332
00:54:14,880 --> 00:54:17,760
AI is that you put in your input and

1333
00:54:16,559 --> 00:54:19,359
then you get output and there's just

1334
00:54:17,760 --> 00:54:22,000
this huge black box. You have no idea

1335
00:54:19,359 --> 00:54:23,920
what happened, right? Um, and so one of

1336
00:54:22,000 --> 00:54:26,000
the ways to help alleviate that, so like

1337
00:54:23,920 --> 00:54:28,160
I'm in a former life, I was a a high

1338
00:54:26,000 --> 00:54:30,160
school English teacher. So, if I wanted

1339
00:54:28,160 --> 00:54:32,079
a AI to give me feedback according to my

1340
00:54:30,160 --> 00:54:34,000
rubric that I've created for student

1341
00:54:32,079 --> 00:54:35,119
essays, if I say, "Here's the rubric.

1342
00:54:34,000 --> 00:54:37,599
Here's a student essay. Give me

1343
00:54:35,119 --> 00:54:38,960
feedback." Well, the essay has lots of

1344
00:54:37,599 --> 00:54:40,480
different components to it, right? And

1345
00:54:38,960 --> 00:54:41,920
so, the feedback I'm going to get is,

1346
00:54:40,480 --> 00:54:43,599
you know, it's just this really messy

1347
00:54:41,920 --> 00:54:44,640
black box. I don't know why it's saying

1348
00:54:43,599 --> 00:54:46,079
what it's saying in all these different

1349
00:54:44,640 --> 00:54:48,160
ways. It may not even remember all the

1350
00:54:46,079 --> 00:54:51,040
criteria that I gave it. Um, and it just

1351
00:54:48,160 --> 00:54:52,160
gets lost very quickly. Um, and so one

1352
00:54:51,040 --> 00:54:55,680
of the kind of the general best

1353
00:54:52,160 --> 00:54:57,200
practices that that I've uh uh come to

1354
00:54:55,680 --> 00:54:59,839
realize is that when you're working with

1355
00:54:57,200 --> 00:55:01,200
AI, it's better to be it's best to be as

1356
00:54:59,839 --> 00:55:03,359
discreet as possible and what you're

1357
00:55:01,200 --> 00:55:05,599
asking it to do. Um, so instead of

1358
00:55:03,359 --> 00:55:08,240
asking it like one omnibus prompt like

1359
00:55:05,599 --> 00:55:09,839
do this thing and it does this huge kind

1360
00:55:08,240 --> 00:55:11,040
of blackbox thing in the background and

1361
00:55:09,839 --> 00:55:12,800
you get the results and you have no idea

1362
00:55:11,040 --> 00:55:14,480
where this came from. Ask it more

1363
00:55:12,800 --> 00:55:16,240
discreet things. Ask okay instead of

1364
00:55:14,480 --> 00:55:17,839
like feeding in my entire rubric, feed

1365
00:55:16,240 --> 00:55:19,599
it one element of my rubric. how did it

1366
00:55:17,839 --> 00:55:21,359
rubric how did it do on this specific

1367
00:55:19,599 --> 00:55:22,960
thing and then you get that feedback

1368
00:55:21,359 --> 00:55:25,200
because it's easier to audit on very

1369
00:55:22,960 --> 00:55:27,359
specific small black boxes than on a

1370
00:55:25,200 --> 00:55:28,960
huge black box right and so by

1371
00:55:27,359 --> 00:55:30,880
separating it out like that it allows

1372
00:55:28,960 --> 00:55:33,760
you to audit and to do quality assurance

1373
00:55:30,880 --> 00:55:36,160
more effectively

1374
00:55:33,760 --> 00:55:39,040
>> ties into some of the other questions I

1375
00:55:36,160 --> 00:55:41,920
think the next question is from Jessica

1376
00:55:39,040 --> 00:55:44,400
Glla and her question is I appreciate

1377
00:55:41,920 --> 00:55:47,200
the examples of judicious use could you

1378
00:55:44,400 --> 00:55:50,160
recommend where to find data on learner

1379
00:55:47,200 --> 00:55:52,559
interactions with AI to help anticipate

1380
00:55:50,160 --> 00:55:55,119
learner needs and preferences and

1381
00:55:52,559 --> 00:55:57,920
support the judicious use of AI with

1382
00:55:55,119 --> 00:56:00,000
open educational resources.

1383
00:55:57,920 --> 00:56:03,040
Um, I'll just say most of what we do

1384
00:56:00,000 --> 00:56:04,160
comes out of um kind of op priori

1385
00:56:03,040 --> 00:56:06,079
assumptions that we're making about

1386
00:56:04,160 --> 00:56:08,319
learners based upon instructional

1387
00:56:06,079 --> 00:56:10,000
design. Um, so like what do we already

1388
00:56:08,319 --> 00:56:12,559
know students going to want? So that's

1389
00:56:10,000 --> 00:56:16,960
one way. Another way is using learning

1390
00:56:12,559 --> 00:56:18,960
analytics um and and just UX um like

1391
00:56:16,960 --> 00:56:21,680
gorilla research with with our learners.

1392
00:56:18,960 --> 00:56:22,960
So um as learners uh read through a

1393
00:56:21,680 --> 00:56:24,559
chapter we have a quality assurance

1394
00:56:22,960 --> 00:56:26,640
rating at the at the bottom where they

1395
00:56:24,559 --> 00:56:28,640
can rate this in five stars or one to

1396
00:56:26,640 --> 00:56:30,480
five stars and then we get feedback from

1397
00:56:28,640 --> 00:56:32,160
them like what what would they want or

1398
00:56:30,480 --> 00:56:35,920
as authors request things what would

1399
00:56:32,160 --> 00:56:38,079
they want um so there's no like single

1400
00:56:35,920 --> 00:56:40,400
place to go but I think that yeah

1401
00:56:38,079 --> 00:56:41,839
looking to UX research on like what what

1402
00:56:40,400 --> 00:56:43,200
are high impact things we can do for

1403
00:56:41,839 --> 00:56:44,720
learners to help them looking at

1404
00:56:43,200 --> 00:56:46,880
instructional design research to do that

1405
00:56:44,720 --> 00:56:49,359
as well but then also having some kind

1406
00:56:46,880 --> 00:56:51,040
of a mechanism for feedback back to help

1407
00:56:49,359 --> 00:56:53,520
us to know what is it that our learners

1408
00:56:51,040 --> 00:56:55,359
wish that they had or maybe our learners

1409
00:56:53,520 --> 00:56:57,520
are going through all of our OEER and

1410
00:56:55,359 --> 00:56:59,119
copy pasting all of it into chat GBT to

1411
00:56:57,520 --> 00:57:00,720
get a cliffnotes version of it. Even

1412
00:56:59,119 --> 00:57:02,559
just knowing that they're doing that is

1413
00:57:00,720 --> 00:57:04,559
super valuable because then we know okay

1414
00:57:02,559 --> 00:57:06,079
that's a use case that would be use that

1415
00:57:04,559 --> 00:57:08,079
that we should be able to provide to

1416
00:57:06,079 --> 00:57:10,319
them that would um eliminate a lot of

1417
00:57:08,079 --> 00:57:13,119
this waste and redundancy of if if we

1418
00:57:10,319 --> 00:57:15,359
have multiple users doing this.

1419
00:57:13,119 --> 00:57:17,040
>> Fantastic. Thank you. Um, I think we

1420
00:57:15,359 --> 00:57:19,040
have time for one more question and it's

1421
00:57:17,040 --> 00:57:21,040
actually from another one of the authors

1422
00:57:19,040 --> 00:57:22,799
from the rapid response papers and I'm

1423
00:57:21,040 --> 00:57:24,960
not going to pronounce their name

1424
00:57:22,799 --> 00:57:29,599
correctly but I will do my best. Um,

1425
00:57:24,960 --> 00:57:32,160
Joel Alberto Arantes do Amaral asks,

1426
00:57:29,599 --> 00:57:34,720
"The environmental impact is one of the

1427
00:57:32,160 --> 00:57:37,200
unintended consequences of AI use that

1428
00:57:34,720 --> 00:57:39,920
is largely overlooked today. How can we

1429
00:57:37,200 --> 00:57:42,160
make these impacts more visible?"

1430
00:57:39,920 --> 00:57:43,920
So, this is

1431
00:57:42,160 --> 00:57:45,440
this always continues to surprise me, I

1432
00:57:43,920 --> 00:57:47,200
guess, because I'm in the the AI and

1433
00:57:45,440 --> 00:57:48,799
education world and early on learned

1434
00:57:47,200 --> 00:57:50,640
about the environmental impacts. And so,

1435
00:57:48,799 --> 00:57:52,000
when I talked to someone about it who

1436
00:57:50,640 --> 00:57:53,520
said, "I had no idea." I was just

1437
00:57:52,000 --> 00:57:54,880
talking to a reporter last week. It was

1438
00:57:53,520 --> 00:57:57,200
like, "I had no idea there was

1439
00:57:54,880 --> 00:57:58,240
environmental impact." Um, and then on

1440
00:57:57,200 --> 00:58:00,079
the other side, there's people who

1441
00:57:58,240 --> 00:58:01,520
literally won't use AI because of the

1442
00:58:00,079 --> 00:58:04,000
environmental impact, and there's

1443
00:58:01,520 --> 00:58:06,000
everyone in between trying to figure

1444
00:58:04,000 --> 00:58:07,760
this all out. I want to contextualize

1445
00:58:06,000 --> 00:58:10,799
this really quickly in the last minute

1446
00:58:07,760 --> 00:58:14,240
or so. um right now like the you know

1447
00:58:10,799 --> 00:58:16,559
global impact of of data energies um

1448
00:58:14,240 --> 00:58:19,599
data center energy uses is like 1.5% of

1449
00:58:16,559 --> 00:58:21,760
the global energy. So it is a it is an

1450
00:58:19,599 --> 00:58:25,119
impact and it's expected to double but

1451
00:58:21,760 --> 00:58:26,720
it's not like um

1452
00:58:25,119 --> 00:58:30,240
massively huge in destructing our

1453
00:58:26,720 --> 00:58:31,680
environment just yet. Um, I do think

1454
00:58:30,240 --> 00:58:34,400
though it is something we need to pay

1455
00:58:31,680 --> 00:58:36,640
attention to and I would love for chat

1456
00:58:34,400 --> 00:58:39,599
GPT to say before you prompt this might

1457
00:58:36,640 --> 00:58:41,359
take this much water, this much energy.

1458
00:58:39,599 --> 00:58:42,960
I don't think any of these companies

1459
00:58:41,359 --> 00:58:45,280
will do that because that will ruin

1460
00:58:42,960 --> 00:58:47,680
their profit margins. Um, but maybe an

1461
00:58:45,280 --> 00:58:50,000
an AI tool will come out. Um, just like

1462
00:58:47,680 --> 00:58:51,520
you know Eosia for internet searches can

1463
00:58:50,000 --> 00:58:54,480
show you how much like trees it's

1464
00:58:51,520 --> 00:58:55,920
planting or carbon dioxide it's reducing

1465
00:58:54,480 --> 00:58:57,280
from the environment. So, I would love

1466
00:58:55,920 --> 00:58:59,040
to see that more, but right now I think

1467
00:58:57,280 --> 00:59:00,880
it comes down to AI literacy. A lot of

1468
00:58:59,040 --> 00:59:03,520
this comes down to AI literacy and

1469
00:59:00,880 --> 00:59:05,920
preparing people who use AI to think

1470
00:59:03,520 --> 00:59:08,960
critically about what are the unintended

1471
00:59:05,920 --> 00:59:10,480
consequences of these tools. Um, because

1472
00:59:08,960 --> 00:59:11,920
that's really important impact on

1473
00:59:10,480 --> 00:59:14,000
society.

1474
00:59:11,920 --> 00:59:16,240
>> Well, that's the perfect transition for

1475
00:59:14,000 --> 00:59:18,319
me to thank you all because our next

1476
00:59:16,240 --> 00:59:21,280
webinar is actually about AI literacy

1477
00:59:18,319 --> 00:59:23,119
and AI evaluation. So, um, deep

1478
00:59:21,280 --> 00:59:25,520
appreciation and thanks to all of our

1479
00:59:23,119 --> 00:59:26,880
speakers today, uh, for joining us, for

1480
00:59:25,520 --> 00:59:29,119
writing your papers, for doing your

1481
00:59:26,880 --> 00:59:31,839
work, and for having this public

1482
00:59:29,119 --> 00:59:34,240
conversation. Um, we're really eager to

1483
00:59:31,839 --> 00:59:36,319
continue the conversation. Um, and thank

1484
00:59:34,240 --> 00:59:37,920
everyone for I just want to thank

1485
00:59:36,319 --> 00:59:40,319
everyone for attending today. This is

1486
00:59:37,920 --> 00:59:44,000
really exciting. Um, and you can sign up

1487
00:59:40,319 --> 00:59:46,640
for um, for updates of the initiative.

1488
00:59:44,000 --> 00:59:49,520
And, uh, and we'll sign off uh, here at

1489
00:59:46,640 --> 00:59:51,280
3:00. So, thanks so much everyone and

1490
00:59:49,520 --> 00:59:52,240
we'll see you next time.

1491
00:59:51,280 --> 00:59:56,280
>> Thank you.

1492
00:59:52,240 --> 00:59:56,280
>> Thank you, Shar. Thank you everyone.

