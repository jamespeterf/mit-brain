1
00:00:01,120 --> 00:00:05,799
thank you um for for coming back uh or

2
00:00:03,879 --> 00:00:08,679
for joining us if you have not been in

3
00:00:05,799 --> 00:00:10,519
here before um so just again to welcome

4
00:00:08,679 --> 00:00:14,080
you to a a series of sessions this

5
00:00:10,519 --> 00:00:16,279
afternoon on generative Ai and copyright

6
00:00:14,080 --> 00:00:18,800
um that sponsored by the MIT schwarzman

7
00:00:16,279 --> 00:00:20,560
College of computing um and just I'll

8
00:00:18,800 --> 00:00:22,560
start by introducing myself and then uh

9
00:00:20,560 --> 00:00:25,000
I'll introduce Clare Schneider uh

10
00:00:22,560 --> 00:00:28,160
momentarily um but I'm a professor in

11
00:00:25,000 --> 00:00:29,720
the history Department um uh here at MIT

12
00:00:28,160 --> 00:00:32,280
um and that usually I follow that by

13
00:00:29,720 --> 00:00:35,079
saying yes you can major in history at

14
00:00:32,280 --> 00:00:37,920
MIT um and I teach and publish uh in the

15
00:00:35,079 --> 00:00:39,840
field of 20th century US legal history

16
00:00:37,920 --> 00:00:42,280
um and and teach students about

17
00:00:39,840 --> 00:00:45,399
questions uh sort of like this um full

18
00:00:42,280 --> 00:00:47,760
disclosure um I am uh I am not a lawyer

19
00:00:45,399 --> 00:00:49,520
um uh although for years I have taught

20
00:00:47,760 --> 00:00:52,760
uh many MIT students who have gone on to

21
00:00:49,520 --> 00:00:54,399
become lawyers um and uh can advise you

22
00:00:52,760 --> 00:00:55,800
I I always say I can talk you into law

23
00:00:54,399 --> 00:01:00,000
school or out of law school but you have

24
00:00:55,800 --> 00:01:01,079
to tell me which one um you want and and

25
00:01:00,000 --> 00:01:02,840
one thing I also wanted to just share

26
00:01:01,079 --> 00:01:04,839
with everyone in the community uh here

27
00:01:02,840 --> 00:01:07,240
if you are looking for a lawyer um I

28
00:01:04,839 --> 00:01:09,400
also like to uh alert people and

29
00:01:07,240 --> 00:01:12,200
particularly students um as or really

30
00:01:09,400 --> 00:01:14,520
just students um to the MIT Buu student

31
00:01:12,200 --> 00:01:17,280
Innovation law clinic um this is a free

32
00:01:14,520 --> 00:01:20,040
and confidential legal service for MIT

33
00:01:17,280 --> 00:01:22,200
students undergrad or grad who uh are

34
00:01:20,040 --> 00:01:24,119
seeking um or may not even know that

35
00:01:22,200 --> 00:01:25,880
they should be seeking uh legal

36
00:01:24,119 --> 00:01:28,960
assistance related to their research

37
00:01:25,880 --> 00:01:31,520
advocacy or creative projects um and so

38
00:01:28,960 --> 00:01:33,079
if you are a student um or know a

39
00:01:31,520 --> 00:01:34,880
student who would benefit from this I'm

40
00:01:33,079 --> 00:01:35,720
happy to uh give you information about

41
00:01:34,880 --> 00:01:38,759
that

42
00:01:35,720 --> 00:01:41,119
group um what I'm going to do before uh

43
00:01:38,759 --> 00:01:44,759
before CLA starts is just um give a a

44
00:01:41,119 --> 00:01:46,680
brief sketch of uh some of the range of

45
00:01:44,759 --> 00:01:48,399
scholarly views around some of the

46
00:01:46,680 --> 00:01:52,439
questions that we were discussing with

47
00:01:48,399 --> 00:01:54,240
um um just uh in the previous hour um

48
00:01:52,439 --> 00:01:56,079
and one of the remarkable things about

49
00:01:54,240 --> 00:01:58,439
the scholarly approaches to generative

50
00:01:56,079 --> 00:02:00,280
Ai and copyright is there's actually one

51
00:01:58,439 --> 00:02:03,159
thing that pretty much everyone agrees

52
00:02:00,280 --> 00:02:05,399
on uh in the legal scholarship World

53
00:02:03,159 --> 00:02:09,640
which is that the uh the existing

54
00:02:05,399 --> 00:02:13,200
copyright regime um is not really a good

55
00:02:09,640 --> 00:02:16,040
fit um for this new emerging

56
00:02:13,200 --> 00:02:18,640
technology um but that's about where the

57
00:02:16,040 --> 00:02:21,440
uh consensus breaks down um and beyond

58
00:02:18,640 --> 00:02:23,200
that um Scholars um who may themselves

59
00:02:21,440 --> 00:02:26,760
not be sort of practicing attorneys

60
00:02:23,200 --> 00:02:28,560
defending either authors or AI companies

61
00:02:26,760 --> 00:02:31,160
um but Scholars thinking kind of more

62
00:02:28,560 --> 00:02:33,000
conceptually about this um differ in

63
00:02:31,160 --> 00:02:35,519
terms of whether that means the

64
00:02:33,000 --> 00:02:38,120
technology needs to be more rigorously

65
00:02:35,519 --> 00:02:40,560
regulated to bring it into line with the

66
00:02:38,120 --> 00:02:43,519
existing copyright regime or that the

67
00:02:40,560 --> 00:02:45,599
copyright regime needs to be reformed um

68
00:02:43,519 --> 00:02:47,640
in order to accommodate the new

69
00:02:45,599 --> 00:02:49,640
technology um and this is some version

70
00:02:47,640 --> 00:02:51,720
of uh the chicken and egg or if you

71
00:02:49,640 --> 00:02:53,560
remember the old Reese's Peanut Butter

72
00:02:51,720 --> 00:02:55,800
Cup uh commercial you know you got your

73
00:02:53,560 --> 00:02:57,360
peanut butter in my chocolate um it's

74
00:02:55,800 --> 00:02:59,120
it's obviously there will be a series of

75
00:02:57,360 --> 00:03:02,120
both of these unfolding over the next

76
00:02:59,120 --> 00:03:04,519
few years um as the Law changes um and

77
00:03:02,120 --> 00:03:07,959
as the technology continues to evolve

78
00:03:04,519 --> 00:03:09,480
and also to be subject to uh regulation

79
00:03:07,959 --> 00:03:12,720
both in the United States and in other

80
00:03:09,480 --> 00:03:13,920
jurisdictions like the UK or the EU um

81
00:03:12,720 --> 00:03:15,480
but just to give you a sense of it you

82
00:03:13,920 --> 00:03:18,000
know one of the first um there are those

83
00:03:15,480 --> 00:03:20,560
who really feel um that that that

84
00:03:18,000 --> 00:03:24,959
copyright is in fact actually um the

85
00:03:20,560 --> 00:03:27,879
thing to defend and renew um by um and

86
00:03:24,959 --> 00:03:30,640
using the law to bring the technology um

87
00:03:27,879 --> 00:03:33,799
into alignment with it um and who do

88
00:03:30,640 --> 00:03:36,239
really feel um that uh that the task is

89
00:03:33,799 --> 00:03:39,040
basically to uh sort of accelerate and

90
00:03:36,239 --> 00:03:41,080
expand regulation um to meet the new

91
00:03:39,040 --> 00:03:43,599
technology u a good place to look for

92
00:03:41,080 --> 00:03:45,280
this is in a piece in an article called

93
00:03:43,599 --> 00:03:47,200
uh in a journal called issues in science

94
00:03:45,280 --> 00:03:49,519
of Technology this is published by the

95
00:03:47,200 --> 00:03:51,519
National Academy of Sciences um by Kate

96
00:03:49,519 --> 00:03:53,840
Crawford a professor at USC and the

97
00:03:51,519 --> 00:03:56,360
author of the atlas of of AI an

98
00:03:53,840 --> 00:03:59,079
excellent book and her co-author NYU Law

99
00:03:56,360 --> 00:04:01,040
Professor Jason Schultz um who uh

100
00:03:59,079 --> 00:04:02,920
together sort of reflect on the ways

101
00:04:01,040 --> 00:04:06,239
that generative AI has as they put it

102
00:04:02,920 --> 00:04:08,439
been driving copyright into a crisis and

103
00:04:06,239 --> 00:04:10,200
just to briefly quote from their work it

104
00:04:08,439 --> 00:04:12,239
may be time to develop concepts of

105
00:04:10,200 --> 00:04:15,000
intellectual property with a stronger

106
00:04:12,239 --> 00:04:17,639
focus on equity and creativity as

107
00:04:15,000 --> 00:04:20,000
opposed to economic incentives for media

108
00:04:17,639 --> 00:04:22,280
corporations we are seeing we meaning

109
00:04:20,000 --> 00:04:23,840
the authors are seeing early prototypes

110
00:04:22,280 --> 00:04:26,199
emerge from the recent collective

111
00:04:23,840 --> 00:04:28,199
bargaining agreements for writers actors

112
00:04:26,199 --> 00:04:30,120
and directors many of whom lack

113
00:04:28,199 --> 00:04:32,400
copyrights but are nonetheless less at

114
00:04:30,120 --> 00:04:34,479
the creative core of film making the

115
00:04:32,400 --> 00:04:36,600
lessons we learned from them could set a

116
00:04:34,479 --> 00:04:38,960
powerful precedent for how to pluralize

117
00:04:36,600 --> 00:04:40,639
intellectual property making a better

118
00:04:38,960 --> 00:04:42,759
world will require a deeper

119
00:04:40,639 --> 00:04:45,199
philosophical engagement with what it is

120
00:04:42,759 --> 00:04:48,759
to create who has a say and how

121
00:04:45,199 --> 00:04:51,639
Creations can be used and who should

122
00:04:48,759 --> 00:04:54,199
profit now a second almost uh sort of

123
00:04:51,639 --> 00:04:56,280
very different sort of approach um is uh

124
00:04:54,199 --> 00:04:58,120
coming from people who have been argued

125
00:04:56,280 --> 00:05:00,600
in some ways that the copyright regime

126
00:04:58,120 --> 00:05:02,720
um is not only a bad fit for this new

127
00:05:00,600 --> 00:05:05,039
technology but maybe has been broken for

128
00:05:02,720 --> 00:05:07,759
a very long time um and that have been

129
00:05:05,039 --> 00:05:10,120
seeking to dismantle it jettison it or

130
00:05:07,759 --> 00:05:12,560
develop alternatives to it um whether

131
00:05:10,120 --> 00:05:15,720
they're coming from Worlds of of Open

132
00:05:12,560 --> 00:05:18,680
Source software open publishing um open

133
00:05:15,720 --> 00:05:20,080
uh access worlds um more generally um

134
00:05:18,680 --> 00:05:22,520
one really great place to look for this

135
00:05:20,080 --> 00:05:24,840
is an an an essay published in the

136
00:05:22,520 --> 00:05:27,199
journal science um by the law professor

137
00:05:24,840 --> 00:05:29,440
Pam Pamela Samuelson from uh the

138
00:05:27,199 --> 00:05:30,759
University of California at Berkeley um

139
00:05:29,440 --> 00:05:32,560
who has really sort of been at the

140
00:05:30,759 --> 00:05:34,840
Forefront of sort of arguing against

141
00:05:32,560 --> 00:05:36,680
copyright uh for a very long time and so

142
00:05:34,840 --> 00:05:38,800
in some ways is sort of adapting her

143
00:05:36,680 --> 00:05:41,440
argument against copyright to the new

144
00:05:38,800 --> 00:05:43,800
technology of generative AI itself um

145
00:05:41,440 --> 00:05:45,960
and argues that in the end uh this this

146
00:05:43,800 --> 00:05:48,000
task uh requires balancing the

147
00:05:45,960 --> 00:05:50,039
legitimate interest of copyright owners

148
00:05:48,000 --> 00:05:52,319
to prevent misappropriations of their

149
00:05:50,039 --> 00:05:54,720
works that undermine incentives to

150
00:05:52,319 --> 00:05:56,680
create with the legitimate interests of

151
00:05:54,720 --> 00:05:59,039
developers of Innovative Technologies

152
00:05:56,680 --> 00:06:01,280
and followon creators who need some

153
00:05:59,039 --> 00:06:03,400
breathing space in which they too can

154
00:06:01,280 --> 00:06:05,280
innovate um and so as we talked about in

155
00:06:03,400 --> 00:06:07,599
the in the the previous session you know

156
00:06:05,280 --> 00:06:10,360
trying to find that win-win um for both

157
00:06:07,599 --> 00:06:13,360
creators um and uh and sort of AI

158
00:06:10,360 --> 00:06:16,800
innovators at the same time is really uh

159
00:06:13,360 --> 00:06:19,199
as a high stakes uh task these days um

160
00:06:16,800 --> 00:06:21,080
one place that that I personally and and

161
00:06:19,199 --> 00:06:23,919
many in the MIT Community have looked to

162
00:06:21,080 --> 00:06:26,319
this um is from our as past engagement

163
00:06:23,919 --> 00:06:28,360
with an organization called Creative

164
00:06:26,319 --> 00:06:30,479
Commons uh Creative Commons is an

165
00:06:28,360 --> 00:06:32,520
international nonprofit uh that exists

166
00:06:30,479 --> 00:06:34,599
to build a network for the open sharing

167
00:06:32,520 --> 00:06:37,000
of knowledge and culture um in many ways

168
00:06:34,599 --> 00:06:39,560
emerges from from the MIT community and

169
00:06:37,000 --> 00:06:41,360
and key players here and in recent years

170
00:06:39,560 --> 00:06:44,360
has been advocating the creation and

171
00:06:41,360 --> 00:06:47,240
Adoption of preference signals um ways

172
00:06:44,360 --> 00:06:49,680
that creators can signal uh to large

173
00:06:47,240 --> 00:06:51,479
language model developers uh the

174
00:06:49,680 --> 00:06:54,720
Creator's preference for how their

175
00:06:51,479 --> 00:06:56,639
material is or isn't used um so just as

176
00:06:54,720 --> 00:06:59,720
when you would publish a work and maybe

177
00:06:56,639 --> 00:07:03,039
have a Creative Commons license on it CC

178
00:06:59,720 --> 00:07:05,199
CC by Etc and there may emerge sort of

179
00:07:03,039 --> 00:07:07,720
from the community um sort of preference

180
00:07:05,199 --> 00:07:10,280
signals that you could use as an author

181
00:07:07,720 --> 00:07:12,520
uh to communicate whether you wish your

182
00:07:10,280 --> 00:07:15,000
model you know wish your your materials

183
00:07:12,520 --> 00:07:18,120
to be trained with AI wish them not to

184
00:07:15,000 --> 00:07:22,080
be or train not trained trained you know

185
00:07:18,120 --> 00:07:23,560
trained by AI or ingested by AI Etc um

186
00:07:22,080 --> 00:07:26,080
and they have also been arguing for a

187
00:07:23,560 --> 00:07:28,599
broader Suite of communication uh tools

188
00:07:26,080 --> 00:07:31,800
um and these may or may not necessarily

189
00:07:28,599 --> 00:07:33,960
be uh legally enforcable mechanisms

190
00:07:31,800 --> 00:07:36,280
right what they aim instead is to as

191
00:07:33,960 --> 00:07:38,520
they as Creative Commons puts it is to

192
00:07:36,280 --> 00:07:41,199
define a new vocabulary and establish

193
00:07:38,520 --> 00:07:43,639
new norms for sharing and reuse in the

194
00:07:41,199 --> 00:07:45,479
world of generative AI right so as we

195
00:07:43,639 --> 00:07:47,639
look forward from this afternoon I think

196
00:07:45,479 --> 00:07:49,960
that kind of work um sort of thoughtful

197
00:07:47,639 --> 00:07:51,840
creation um is is what is going to be

198
00:07:49,960 --> 00:07:55,560
necessary to get us to the kind of next

199
00:07:51,840 --> 00:07:57,759
phase um past this moment where Scholars

200
00:07:55,560 --> 00:07:59,759
uh creatives and in fact many uh large

201
00:07:57,759 --> 00:08:02,440
language model developers themselves

202
00:07:59,759 --> 00:08:04,680
agree that the existing copyright regime

203
00:08:02,440 --> 00:08:05,560
is not a good fit for the technology as

204
00:08:04,680 --> 00:08:08,319
it

205
00:08:05,560 --> 00:08:11,120
exists now uh what does all this mean

206
00:08:08,319 --> 00:08:13,520
you know uh at MIT right um and so we're

207
00:08:11,120 --> 00:08:15,159
very lucky to have with us today uh a

208
00:08:13,520 --> 00:08:17,440
member of mit's office of General

209
00:08:15,159 --> 00:08:20,240
Council um and we're particularly likely

210
00:08:17,440 --> 00:08:22,840
to have her today as you can imagine um

211
00:08:20,240 --> 00:08:24,479
uh uh lawyers um are very busy um these

212
00:08:22,840 --> 00:08:26,759
days um reading the news and and

213
00:08:24,479 --> 00:08:29,639
responding to it um so we appreciate CLA

214
00:08:26,759 --> 00:08:31,440
taking some time uh to join us here CLA

215
00:08:29,639 --> 00:08:33,240
fine Schneider is a member uh she's

216
00:08:31,440 --> 00:08:34,760
Council uh to the Massachusetts

217
00:08:33,240 --> 00:08:38,120
Institute of Technology she's been with

218
00:08:34,760 --> 00:08:40,039
the ogc since September of 2018 and her

219
00:08:38,120 --> 00:08:42,760
areas of focus at MIT include

220
00:08:40,039 --> 00:08:44,959
intellectual property sponsored research

221
00:08:42,760 --> 00:08:47,480
conflict of interest entrepreneurial

222
00:08:44,959 --> 00:08:49,800
activities and academic and business

223
00:08:47,480 --> 00:08:51,920
transactions and for uh some time now

224
00:08:49,800 --> 00:08:54,720
maybe uh since around

225
00:08:51,920 --> 00:08:57,160
2023 um she has been sort of part of U

226
00:08:54,720 --> 00:08:58,920
of people of groups of people at MIT who

227
00:08:57,160 --> 00:09:01,279
have been helping the community

228
00:08:58,920 --> 00:09:03,680
understand how generative AI is changing

229
00:09:01,279 --> 00:09:06,519
the way that we teach learn and research

230
00:09:03,680 --> 00:09:08,320
uh at MIT she is a trained attorney uh

231
00:09:06,519 --> 00:09:10,600
with a degree from the Boston University

232
00:09:08,320 --> 00:09:12,800
School of Law um and with previous years

233
00:09:10,600 --> 00:09:15,560
of experience before MIT uh in private

234
00:09:12,800 --> 00:09:19,480
practice at mince Levan and Wilmer har

235
00:09:15,560 --> 00:09:19,480
and so with that I'll turn it over to

236
00:09:21,600 --> 00:09:26,320
Claire thank you so much Chris and um I

237
00:09:24,440 --> 00:09:27,800
wish that I had had you as a history

238
00:09:26,320 --> 00:09:29,880
Professor so you could have talked me

239
00:09:27,800 --> 00:09:33,079
out of law school but

240
00:09:29,880 --> 00:09:34,959
that that ship has sailed yes um and I

241
00:09:33,079 --> 00:09:37,760
know this is a small group so tell me if

242
00:09:34,959 --> 00:09:40,959
the microphone is too loud and I will

243
00:09:37,760 --> 00:09:43,079
adjust it so I'm going to do a bit of

244
00:09:40,959 --> 00:09:45,600
internal marketing since it's such a

245
00:09:43,079 --> 00:09:49,600
small group and just ask who in this

246
00:09:45,600 --> 00:09:51,640
audience has had occasion um some might

247
00:09:49,600 --> 00:09:54,600
say for bad but I would say for good to

248
00:09:51,640 --> 00:09:57,880
interact with the general council's

249
00:09:54,600 --> 00:10:00,279
office Chris knows us well we have one

250
00:09:57,880 --> 00:10:04,320
other taker Okay so I just want to give

251
00:10:00,279 --> 00:10:06,680
a a very very brief overview um of who

252
00:10:04,320 --> 00:10:10,800
we are and what we do at The Institute

253
00:10:06,680 --> 00:10:15,480
so of course um as Chris said I am an

254
00:10:10,800 --> 00:10:17,640
attorney um myself and 10 other Council

255
00:10:15,480 --> 00:10:19,519
uh are in our general council's office

256
00:10:17,640 --> 00:10:22,040
and then we also have a deputy general

257
00:10:19,519 --> 00:10:27,240
counsel and the vice president general

258
00:10:22,040 --> 00:10:29,000
counsel uh Mark Devan chenzo um we all

259
00:10:27,240 --> 00:10:31,120
represent the Institute first and

260
00:10:29,000 --> 00:10:33,279
foremost for most but it's obviously

261
00:10:31,120 --> 00:10:36,560
more complicated than that because there

262
00:10:33,279 --> 00:10:40,600
are many stakeholders within the

263
00:10:36,560 --> 00:10:43,639
Institute right so we often are working

264
00:10:40,600 --> 00:10:47,160
for and with groups of stakeholders and

265
00:10:43,639 --> 00:10:49,720
part of our job is to figure out how

266
00:10:47,160 --> 00:10:53,279
those interests are aligned with the

267
00:10:49,720 --> 00:10:56,920
interests of MIT so that we can uh be

268
00:10:53,279 --> 00:10:59,600
sure that we are always representing MIT

269
00:10:56,920 --> 00:11:02,160
um we do a variety of things we give

270
00:10:59,600 --> 00:11:04,839
counsel we give advice we defend the

271
00:11:02,160 --> 00:11:06,880
institute in litigation we represent the

272
00:11:04,839 --> 00:11:08,639
The Institute in the rare instances when

273
00:11:06,880 --> 00:11:12,120
we're on the other side of the V and

274
00:11:08,639 --> 00:11:15,639
litigation um we advise on transactions

275
00:11:12,120 --> 00:11:18,519
we advise on policy issues um and we

276
00:11:15,639 --> 00:11:20,760
generally act as thought Partners to

277
00:11:18,519 --> 00:11:24,000
other senior leaders on campus to

278
00:11:20,760 --> 00:11:26,200
administrators to faculty and also um to

279
00:11:24,000 --> 00:11:27,720
graduate students and other students as

280
00:11:26,200 --> 00:11:29,800
Chris noted and I'm so glad you called

281
00:11:27,720 --> 00:11:32,760
out the Buu law

282
00:11:29,800 --> 00:11:35,000
um MIT law clinic because it's a great

283
00:11:32,760 --> 00:11:37,399
resource for our students because we in

284
00:11:35,000 --> 00:11:39,120
the ogc can't represent students

285
00:11:37,399 --> 00:11:42,360
individually whether they're undergrad

286
00:11:39,120 --> 00:11:44,519
or or grad students um but I find in my

287
00:11:42,360 --> 00:11:47,040
practice personally that I spend a lot

288
00:11:44,519 --> 00:11:49,279
of time talking to students especially

289
00:11:47,040 --> 00:11:51,920
about intellectual property and

290
00:11:49,279 --> 00:11:54,040
entrepreneurial activities um and as

291
00:11:51,920 --> 00:11:57,279
Chris said my focus really is primarily

292
00:11:54,040 --> 00:12:00,839
intellectual property I do not uh take

293
00:11:57,279 --> 00:12:02,440
part in actually protect inventions on

294
00:12:00,839 --> 00:12:04,839
campus so you know our technology

295
00:12:02,440 --> 00:12:06,600
licensing office does that and I I work

296
00:12:04,839 --> 00:12:09,639
closely with them at times and advise

297
00:12:06,600 --> 00:12:12,600
them but mostly I'm thinking about um

298
00:12:09,639 --> 00:12:16,120
intellectual property in terms of big

299
00:12:12,600 --> 00:12:18,160
transactions technology licensing IP

300
00:12:16,120 --> 00:12:20,480
terms that we are granting to our

301
00:12:18,160 --> 00:12:23,839
industry sponsors so many of you may

302
00:12:20,480 --> 00:12:26,519
work on or spis may have industry

303
00:12:23,839 --> 00:12:29,800
sponsored research agreements um and in

304
00:12:26,519 --> 00:12:32,760
exchange for funding research we permit

305
00:12:29,800 --> 00:12:34,320
the company to elect um to exercise

306
00:12:32,760 --> 00:12:36,399
rights to intellectual property

307
00:12:34,320 --> 00:12:39,000
developed under those sponsored research

308
00:12:36,399 --> 00:12:41,240
agreements um so there's lots of ways in

309
00:12:39,000 --> 00:12:44,199
which IP sort of touches the day in and

310
00:12:41,240 --> 00:12:46,560
day out functions of MIT but as Chris

311
00:12:44,199 --> 00:12:50,240
alluded to there's also these sort of

312
00:12:46,560 --> 00:12:54,000
sea shifts that happen that also touch

313
00:12:50,240 --> 00:12:57,320
on IP um in this case mostly copyright

314
00:12:54,000 --> 00:13:00,000
but there are um patent questions that

315
00:12:57,320 --> 00:13:03,399
surround generative AI too and so I have

316
00:13:00,000 --> 00:13:07,000
really had the Good Fortune of being at

317
00:13:03,399 --> 00:13:08,519
MIT during this sea shift cuz obviously

318
00:13:07,000 --> 00:13:13,800
it is

319
00:13:08,519 --> 00:13:16,279
fascinating um I did want to sort of ask

320
00:13:13,800 --> 00:13:18,839
you know the audience cuz for for me I

321
00:13:16,279 --> 00:13:22,079
mean generative AI for many of you who

322
00:13:18,839 --> 00:13:25,480
are engineers and scientists this is not

323
00:13:22,079 --> 00:13:27,959
all that new um of course there's been

324
00:13:25,480 --> 00:13:29,920
significant leaps in the past few years

325
00:13:27,959 --> 00:13:31,800
I actually went to Dartmouth as an

326
00:13:29,920 --> 00:13:33,760
undergrad and Dartmouth is always

327
00:13:31,800 --> 00:13:36,639
touting the fact that like they had the

328
00:13:33,760 --> 00:13:38,760
first artificial intelligence conference

329
00:13:36,639 --> 00:13:40,600
back in the 50s or that's what that's

330
00:13:38,760 --> 00:13:43,519
what they say they lay claim to that

331
00:13:40,600 --> 00:13:47,399
whether or not it's true um so of of

332
00:13:43,519 --> 00:13:50,000
course I am aware and have been for some

333
00:13:47,399 --> 00:13:51,800
time that researchers and scientists

334
00:13:50,000 --> 00:13:56,120
have been working on this but I would

335
00:13:51,800 --> 00:14:00,480
say from from a legal perspective and

336
00:13:56,120 --> 00:14:04,360
the tentacles that generative AI has had

337
00:14:00,480 --> 00:14:06,519
um in my world and at MIT as a whole has

338
00:14:04,360 --> 00:14:08,079
really as Chris said happened in the in

339
00:14:06,519 --> 00:14:10,680
the past few years I think you know

340
00:14:08,079 --> 00:14:14,440
since early winter J you know in

341
00:14:10,680 --> 00:14:17,800
2023 um and it's now pervasive you know

342
00:14:14,440 --> 00:14:23,240
it's it's a real Zeitgeist and it's

343
00:14:17,800 --> 00:14:25,440
changing kind of all aspects of um of my

344
00:14:23,240 --> 00:14:29,720
practice and I think others others like

345
00:14:25,440 --> 00:14:31,920
me so I I wanted to start by before we

346
00:14:29,720 --> 00:14:35,759
get a little bit more deeply into the

347
00:14:31,920 --> 00:14:38,720
precise topic today of data and content

348
00:14:35,759 --> 00:14:41,040
generators and creators at MIT I wanted

349
00:14:38,720 --> 00:14:45,720
to kind of give a

350
00:14:41,040 --> 00:14:49,199
broad swath or almost a list of all the

351
00:14:45,720 --> 00:14:53,120
different places and community members

352
00:14:49,199 --> 00:14:56,839
on campus um that we're trying to track

353
00:14:53,120 --> 00:14:59,199
when we think about um you know ways in

354
00:14:56,839 --> 00:15:02,320
which generative AI issues are affect

355
00:14:59,199 --> 00:15:04,519
ing stakeholders at MIT so we've been

356
00:15:02,320 --> 00:15:06,320
Loosely and you know as we go on I don't

357
00:15:04,519 --> 00:15:07,639
know if we'll wait till Q&A or feel free

358
00:15:06,320 --> 00:15:09,880
to raise your hand now but you know

359
00:15:07,639 --> 00:15:12,560
we're we're starting to collect this

360
00:15:09,880 --> 00:15:15,800
information um you wouldn't necessarily

361
00:15:12,560 --> 00:15:17,839
be able to go to MIT you know the MIT

362
00:15:15,800 --> 00:15:19,519
website and find a lot of this written

363
00:15:17,839 --> 00:15:22,000
down but we are starting to think about

364
00:15:19,519 --> 00:15:24,639
it and we've been breaking these

365
00:15:22,000 --> 00:15:26,240
categories into three groups so we're

366
00:15:24,639 --> 00:15:29,040
thinking about community members

367
00:15:26,240 --> 00:15:31,199
obviously who use generative AI tools

368
00:15:29,040 --> 00:15:35,360
we're thinking about MIT community

369
00:15:31,199 --> 00:15:38,759
members who create generate models so AI

370
00:15:35,360 --> 00:15:41,800
systems and then of course um most of

371
00:15:38,759 --> 00:15:45,839
the MIT Community who is generating

372
00:15:41,800 --> 00:15:48,720
content that can be used by both MIT

373
00:15:45,839 --> 00:15:53,720
community members to train models in you

374
00:15:48,720 --> 00:15:55,839
know during research efforts or um by

375
00:15:53,720 --> 00:15:58,720
Third parties that can use that content

376
00:15:55,839 --> 00:16:02,120
to train third party models by virtue of

377
00:15:58,720 --> 00:16:04,880
the fact that we have an open campus and

378
00:16:02,120 --> 00:16:08,440
the goal of fundamental research is to

379
00:16:04,880 --> 00:16:10,040
publish right so um in fact I don't know

380
00:16:08,440 --> 00:16:12,720
if any of you know this maybe some of

381
00:16:10,040 --> 00:16:15,759
you do that actually in I think it's

382
00:16:12,720 --> 00:16:19,399
section 14.1 or something like that of

383
00:16:15,759 --> 00:16:24,079
P&P um is our open research campus

384
00:16:19,399 --> 00:16:27,240
policy and indeed all research on campus

385
00:16:24,079 --> 00:16:29,480
must be open and must be published and

386
00:16:27,240 --> 00:16:30,720
if it's not intended to be there is a

387
00:16:29,480 --> 00:16:33,959
special

388
00:16:30,720 --> 00:16:35,399
escalation um process that goes all the

389
00:16:33,959 --> 00:16:37,519
way up to the vice president for

390
00:16:35,399 --> 00:16:39,600
research of course that excludes Lincoln

391
00:16:37,519 --> 00:16:42,440
lab which has its own rules and much of

392
00:16:39,600 --> 00:16:44,839
what Lincoln does um is of course not

393
00:16:42,440 --> 00:16:47,600
published so this is really

394
00:16:44,839 --> 00:16:48,920
fundamental you know to who MIT is and I

395
00:16:47,600 --> 00:16:51,480
think you know Chris and I will both

396
00:16:48,920 --> 00:16:53,519
touch on this a little bit more later

397
00:16:51,480 --> 00:16:55,279
but that's when we think of those three

398
00:16:53,519 --> 00:16:57,079
categories that's kind of what we're

399
00:16:55,279 --> 00:17:00,600
we're trying to track and when I say

400
00:16:57,079 --> 00:17:03,480
track I mean if issues

401
00:17:00,600 --> 00:17:06,360
concerns legal rules or

402
00:17:03,480 --> 00:17:08,880
regulations um and then Downstream

403
00:17:06,360 --> 00:17:10,520
consequences or exposure from taking one

404
00:17:08,880 --> 00:17:11,959
position or taking another position

405
00:17:10,520 --> 00:17:13,720
that's you know and then the type of

406
00:17:11,959 --> 00:17:15,160
guidance we need to provide to MIT

407
00:17:13,720 --> 00:17:17,480
community members that's what I say when

408
00:17:15,160 --> 00:17:22,000
I mean like sort of keeping track of all

409
00:17:17,480 --> 00:17:23,439
of these different categories so um with

410
00:17:22,000 --> 00:17:25,919
with respect to the first one which is

411
00:17:23,439 --> 00:17:27,520
kind of the easiest and the the category

412
00:17:25,919 --> 00:17:29,520
in which I think you'll find most of our

413
00:17:27,520 --> 00:17:32,360
peers including MIT do have some

414
00:17:29,520 --> 00:17:35,280
guidance out already it's using tools

415
00:17:32,360 --> 00:17:37,400
right so how to use them responsibly um

416
00:17:35,280 --> 00:17:39,880
should you be downloading the open-

417
00:17:37,400 --> 00:17:42,360
source version of chat GPT or should you

418
00:17:39,880 --> 00:17:45,160
be using the one that we have a license

419
00:17:42,360 --> 00:17:48,480
to under an Enterprise um license

420
00:17:45,160 --> 00:17:51,200
agreement um do you have the right

421
00:17:48,480 --> 00:17:53,280
permissions in place to be using tools

422
00:17:51,200 --> 00:17:55,600
for certain projects so of course you

423
00:17:53,280 --> 00:17:58,000
all and I assume mostly researchers in

424
00:17:55,600 --> 00:17:59,559
this room though correct me are using

425
00:17:58,000 --> 00:18:02,080
tools for research purposes but remember

426
00:17:59,559 --> 00:18:04,679
we have a lot of other folks you know in

427
00:18:02,080 --> 00:18:06,640
HR or admissions who might want to use

428
00:18:04,679 --> 00:18:08,159
these tools and that's that's a very

429
00:18:06,640 --> 00:18:10,200
different story right we have to think

430
00:18:08,159 --> 00:18:12,679
about what data is being used and

431
00:18:10,200 --> 00:18:15,320
inputed into these tools in terms of

432
00:18:12,679 --> 00:18:17,159
sensitivity Etc but back to the

433
00:18:15,320 --> 00:18:18,960
researcher side we also have to be

434
00:18:17,159 --> 00:18:21,440
giving guidance to researchers who are

435
00:18:18,960 --> 00:18:24,320
using thirdparty Tools in terms of using

436
00:18:21,440 --> 00:18:26,640
and inputting unpublished research

437
00:18:24,320 --> 00:18:29,520
results right will you is does that

438
00:18:26,640 --> 00:18:32,679
constitute a public disclosure under the

439
00:18:29,520 --> 00:18:34,840
patent and trademark office's rules such

440
00:18:32,679 --> 00:18:36,320
that you might be limited in the patent

441
00:18:34,840 --> 00:18:39,120
protection that you could get from

442
00:18:36,320 --> 00:18:42,799
subsequent outputs or inventions after

443
00:18:39,120 --> 00:18:46,280
inputting that data so these are all um

444
00:18:42,799 --> 00:18:48,159
items that we are trying to think

445
00:18:46,280 --> 00:18:50,080
through and and provide guidance about

446
00:18:48,159 --> 00:18:52,240
but those are those are a little bit

447
00:18:50,080 --> 00:18:54,200
simpler and I think that we all kind of

448
00:18:52,240 --> 00:18:55,600
generally understand the issues that we

449
00:18:54,200 --> 00:18:59,400
need to think

450
00:18:55,600 --> 00:19:01,760
about the second category

451
00:18:59,400 --> 00:19:05,520
is you know for the developers of models

452
00:19:01,760 --> 00:19:10,200
and I think you know traditionally we

453
00:19:05,520 --> 00:19:13,480
don't give a lot of uh structure and

454
00:19:10,200 --> 00:19:15,400
Scaffolding nor I do I think we should

455
00:19:13,480 --> 00:19:17,200
you know necessarily to piis and

456
00:19:15,400 --> 00:19:20,320
researchers when they are putting

457
00:19:17,200 --> 00:19:22,200
together their Grant proposals um scopes

458
00:19:20,320 --> 00:19:24,000
of work under industry sponsored

459
00:19:22,200 --> 00:19:25,799
research agreements and thinking through

460
00:19:24,000 --> 00:19:30,320
the research that they want to pursue

461
00:19:25,799 --> 00:19:35,240
that is has never been um part of the

462
00:19:30,320 --> 00:19:39,000
MIT you know core but as we think about

463
00:19:35,240 --> 00:19:42,159
models that may have risk associated

464
00:19:39,000 --> 00:19:44,520
with them and may be regulated and when

465
00:19:42,159 --> 00:19:48,600
we think about the data going into those

466
00:19:44,520 --> 00:19:52,039
models that also May um be associated

467
00:19:48,600 --> 00:19:54,840
with risk and have potential you know

468
00:19:52,039 --> 00:19:58,280
legal ramifications of using data in one

469
00:19:54,840 --> 00:20:00,440
way or another we're starting to try to

470
00:19:58,280 --> 00:20:03,600
get our arms around that and thinking

471
00:20:00,440 --> 00:20:06,159
about what kind of guidance can we give

472
00:20:03,600 --> 00:20:09,520
researchers as they are putting together

473
00:20:06,159 --> 00:20:11,559
research projects um and obviously this

474
00:20:09,520 --> 00:20:13,360
is early stages there's a lot of

475
00:20:11,559 --> 00:20:16,240
conversations happening among and

476
00:20:13,360 --> 00:20:20,640
between research administrators dlcis

477
00:20:16,240 --> 00:20:25,400
and faculty to think through this um but

478
00:20:20,640 --> 00:20:28,200
it it really needs to stem from how we

479
00:20:25,400 --> 00:20:31,000
think about when and where this model

480
00:20:28,200 --> 00:20:33,840
will be released and accessible outside

481
00:20:31,000 --> 00:20:36,960
of MIT um and again as I said before

482
00:20:33,840 --> 00:20:38,919
traditionally you know models um are

483
00:20:36,960 --> 00:20:41,600
publicly released it is my understanding

484
00:20:38,919 --> 00:20:44,840
that for publication purposes and you

485
00:20:41,600 --> 00:20:46,760
know for certain journals um there are

486
00:20:44,840 --> 00:20:48,840
requirements about

487
00:20:46,760 --> 00:20:51,120
releasing uh Parts at least of

488
00:20:48,840 --> 00:20:53,480
algorithms and data sets etc for

489
00:20:51,120 --> 00:20:56,440
replicability and transparency Etc

490
00:20:53,480 --> 00:20:59,880
there's also rules under certain federal

491
00:20:56,440 --> 00:21:02,640
awards that require data to be be

492
00:20:59,880 --> 00:21:05,000
promptly released and easily

493
00:21:02,640 --> 00:21:09,400
accessible and we are trying to think

494
00:21:05,000 --> 00:21:11,919
through sort of are there um are there

495
00:21:09,400 --> 00:21:13,720
guide posts that need to be considered

496
00:21:11,919 --> 00:21:16,279
when doing this for

497
00:21:13,720 --> 00:21:19,960
instance there are regulations that have

498
00:21:16,279 --> 00:21:22,360
started to be developed there is a EU AI

499
00:21:19,960 --> 00:21:25,600
act which many of you may be aware of

500
00:21:22,360 --> 00:21:28,159
that has officially come into Force um

501
00:21:25,600 --> 00:21:31,279
the EU is very frequently ahead of the

502
00:21:28,159 --> 00:21:33,159
US in terms of Broad and sweeping and

503
00:21:31,279 --> 00:21:35,679
detailed regulation that happened with

504
00:21:33,159 --> 00:21:39,120
gdpr and data privacy some years ago as

505
00:21:35,679 --> 00:21:41,039
well um in America of course the Biden

506
00:21:39,120 --> 00:21:43,400
Administration put out their executive

507
00:21:41,039 --> 00:21:46,000
order an AI that addressed some of these

508
00:21:43,400 --> 00:21:49,000
topics in October

509
00:21:46,000 --> 00:21:50,840
2023 um that order will probably be

510
00:21:49,000 --> 00:21:53,120
revoked under the Trump

511
00:21:50,840 --> 00:21:55,320
Administration but regardless we can

512
00:21:53,120 --> 00:21:58,840
assume that there will be regulations

513
00:21:55,320 --> 00:22:01,360
that will apply to AI systems and keep

514
00:21:58,840 --> 00:22:03,360
using that word AI systems it might not

515
00:22:01,360 --> 00:22:05,360
be something that rolls off the tongue

516
00:22:03,360 --> 00:22:08,720
for all of you but that is because it

517
00:22:05,360 --> 00:22:10,919
was defined as such in the Biden

518
00:22:08,720 --> 00:22:14,120
executive order so trying to keep some

519
00:22:10,919 --> 00:22:15,600
of the wording the same um we can assume

520
00:22:14,120 --> 00:22:18,440
that these models will be regulated in

521
00:22:15,600 --> 00:22:20,760
some way shape or form even if they are

522
00:22:18,440 --> 00:22:22,960
used just for research purposes or

523
00:22:20,760 --> 00:22:25,559
developed in the context of research and

524
00:22:22,960 --> 00:22:26,960
that is really you know that's that's

525
00:22:25,559 --> 00:22:29,960
different and when Chris alluded to

526
00:22:26,960 --> 00:22:32,600
before sort of the traditional IP rules

527
00:22:29,960 --> 00:22:35,799
not necessarily fitting with generative

528
00:22:32,600 --> 00:22:38,679
AI um I think this is a little bit

529
00:22:35,799 --> 00:22:41,080
similar in the sort of regulatory space

530
00:22:38,679 --> 00:22:44,400
because previously this is a good

531
00:22:41,080 --> 00:22:46,039
analogy um and you may know this export

532
00:22:44,400 --> 00:22:48,600
control

533
00:22:46,039 --> 00:22:52,279
rules I won't say they don't apply but

534
00:22:48,600 --> 00:22:55,400
they're they're less of a thing for MIT

535
00:22:52,279 --> 00:22:58,760
because everything we do falls under the

536
00:22:55,400 --> 00:23:01,200
fundamental research exception and under

537
00:22:58,760 --> 00:23:04,440
that fundamental research exemption

538
00:23:01,200 --> 00:23:06,640
under the export control rules we don't

539
00:23:04,440 --> 00:23:09,640
have to worry about getting certain

540
00:23:06,640 --> 00:23:12,360
licenses from the government to share

541
00:23:09,640 --> 00:23:15,679
certain otherwise sensitive technology

542
00:23:12,360 --> 00:23:20,480
with non- us persons um on our campus

543
00:23:15,679 --> 00:23:22,120
for instance that's not true for for um

544
00:23:20,480 --> 00:23:24,960
companies that are not operating under

545
00:23:22,120 --> 00:23:26,640
the phom fundamental research exemption

546
00:23:24,960 --> 00:23:28,039
caveat to that is that those rules under

547
00:23:26,640 --> 00:23:29,760
the Trump Administration we could see

548
00:23:28,039 --> 00:23:30,640
some of that change too which would be

549
00:23:29,760 --> 00:23:33,760
really

550
00:23:30,640 --> 00:23:36,200
unfortunate but the way this relates um

551
00:23:33,760 --> 00:23:38,960
to the models is that I think

552
00:23:36,200 --> 00:23:41,600
historically the thought was if you

553
00:23:38,960 --> 00:23:44,240
wrote software code and you put it on

554
00:23:41,600 --> 00:23:47,640
GitHub and you you know slapped a you

555
00:23:44,240 --> 00:23:50,279
know MIT open- Source Institute approved

556
00:23:47,640 --> 00:23:53,400
license on it um you didn't have to

557
00:23:50,279 --> 00:23:56,679
really do much more than that now the

558
00:23:53,400 --> 00:23:59,880
thinking is if there is a model that

559
00:23:56,679 --> 00:24:03,200
perhaps was trained um with very very

560
00:23:59,880 --> 00:24:06,400
large data sets or with high compute

561
00:24:03,200 --> 00:24:10,240
power or otherwise has dual use

562
00:24:06,400 --> 00:24:12,880
capabilities or Falls in you know the

563
00:24:10,240 --> 00:24:15,960
number of other buckets that the EU AI

564
00:24:12,880 --> 00:24:19,760
act references as being potentially high

565
00:24:15,960 --> 00:24:22,400
risk um that we have to be and I mean we

566
00:24:19,760 --> 00:24:26,240
MIT as an Institute have to be more

567
00:24:22,400 --> 00:24:28,559
thoughtful about who um might pick that

568
00:24:26,240 --> 00:24:32,360
model up and run with it and use it for

569
00:24:28,559 --> 00:24:36,640
purposes that perhaps were unintended or

570
00:24:32,360 --> 00:24:38,760
for which we now somehow um are going to

571
00:24:36,640 --> 00:24:39,720
be I don't want to I don't want to use

572
00:24:38,760 --> 00:24:41,919
the word

573
00:24:39,720 --> 00:24:43,679
responsible lightly but you know

574
00:24:41,919 --> 00:24:45,919
responsible for in some way shape or

575
00:24:43,679 --> 00:24:47,840
form whether that's legal responsibility

576
00:24:45,919 --> 00:24:50,799
or reputational or

577
00:24:47,840 --> 00:24:54,279
otherwise so we're we're trying to get

578
00:24:50,799 --> 00:24:56,200
our arms around that I can give you I

579
00:24:54,279 --> 00:25:00,640
wrote it down here it might be

580
00:24:56,200 --> 00:25:03,159
interesting the EU AI act um thinks

581
00:25:00,640 --> 00:25:05,039
about sort of various levels of models

582
00:25:03,159 --> 00:25:07,640
in terms of risk categories you may have

583
00:25:05,039 --> 00:25:09,720
seen this there's high risk medium risk

584
00:25:07,640 --> 00:25:12,279
low risk and then kind of like no risk

585
00:25:09,720 --> 00:25:14,760
you can do whatever you want um and

586
00:25:12,279 --> 00:25:17,720
there's a lot of reporting obligations

587
00:25:14,760 --> 00:25:19,720
and restrictions for high-risk models

588
00:25:17,720 --> 00:25:23,640
and those are models that at least the

589
00:25:19,720 --> 00:25:26,080
EU would Define as obviously I noted

590
00:25:23,640 --> 00:25:29,520
before correlated to compute power dual

591
00:25:26,080 --> 00:25:31,840
use capabilities um also the capability

592
00:25:29,520 --> 00:25:34,399
to be used for purposes that are judged

593
00:25:31,840 --> 00:25:37,039
to be functions that could result in

594
00:25:34,399 --> 00:25:40,720
high-risk outcomes if performed

595
00:25:37,039 --> 00:25:43,559
improperly well that's quite broad right

596
00:25:40,720 --> 00:25:45,799
um or whose output serves as a basis for

597
00:25:43,559 --> 00:25:48,159
decisions or actions that have a legal

598
00:25:45,799 --> 00:25:51,240
or significant effect on an individual

599
00:25:48,159 --> 00:25:53,960
or community so you could imagine um law

600
00:25:51,240 --> 00:25:57,520
enforcement you could imagine hiring

601
00:25:53,960 --> 00:25:59,399
decisions admissions decisions Etc um

602
00:25:57,520 --> 00:26:01,679
but you could also imagine a model you

603
00:25:59,399 --> 00:26:04,120
know being used for a purpose that was

604
00:26:01,679 --> 00:26:07,000
otherwise not intended by researchers at

605
00:26:04,120 --> 00:26:10,039
MIT but shared with collaborators and

606
00:26:07,000 --> 00:26:11,960
research sponsors so these are all you

607
00:26:10,039 --> 00:26:14,080
know areas it's a little bit off topic

608
00:26:11,960 --> 00:26:16,799
from the what we're talking about today

609
00:26:14,080 --> 00:26:19,840
but they're it's all connected and one

610
00:26:16,799 --> 00:26:23,039
of the uh most precise ways it's

611
00:26:19,840 --> 00:26:25,320
connected is that the EU AI act requires

612
00:26:23,039 --> 00:26:27,640
that for all high-risk and medium risk

613
00:26:25,320 --> 00:26:29,480
models and this includes models being

614
00:26:27,640 --> 00:26:32,600
developed at research

615
00:26:29,480 --> 00:26:35,120
institutions that there is um TR

616
00:26:32,600 --> 00:26:37,880
traceability of the data that is

617
00:26:35,120 --> 00:26:40,120
ingested and used to create the models

618
00:26:37,880 --> 00:26:43,919
and that all the appropriate consents

619
00:26:40,120 --> 00:26:46,240
and permissions have been you know uh

620
00:26:43,919 --> 00:26:49,960
have been gotten for for the data that's

621
00:26:46,240 --> 00:26:54,520
being used um that is not something

622
00:26:49,960 --> 00:26:58,000
necessarily that we currently bake in to

623
00:26:54,520 --> 00:27:00,559
our research projects although we do

624
00:26:58,000 --> 00:27:04,679
have many researchers on campus who are

625
00:27:00,559 --> 00:27:07,200
dutifully and diligently working with um

626
00:27:04,679 --> 00:27:09,679
the libraries and other stor of content

627
00:27:07,200 --> 00:27:14,240
that we'll talk about in a moment to try

628
00:27:09,679 --> 00:27:16,880
to access data for training purposes in

629
00:27:14,240 --> 00:27:16,880
permissible

630
00:27:17,520 --> 00:27:24,480
ways so I think that now when we talk

631
00:27:21,480 --> 00:27:26,320
about you know the the third topic the

632
00:27:24,480 --> 00:27:28,440
third category that I was mentioning

633
00:27:26,320 --> 00:27:31,600
before that's really the the focus of

634
00:27:28,440 --> 00:27:33,640
today's um Talk which are data and

635
00:27:31,600 --> 00:27:37,520
content creators and

636
00:27:33,640 --> 00:27:39,559
generators um we really think about sort

637
00:27:37,520 --> 00:27:44,159
of a few different buckets we think

638
00:27:39,559 --> 00:27:48,000
about researchers who are accessing

639
00:27:44,159 --> 00:27:50,519
other MIT content for their own research

640
00:27:48,000 --> 00:27:53,880
purposes and we think about researchers

641
00:27:50,519 --> 00:27:56,360
who are accessing thirdparty contact

642
00:27:53,880 --> 00:27:59,240
content for research purposes and then

643
00:27:56,360 --> 00:28:01,960
we also think about all the researchers

644
00:27:59,240 --> 00:28:03,880
who make their own content available

645
00:28:01,960 --> 00:28:07,519
perhaps as part of a journal or other

646
00:28:03,880 --> 00:28:10,399
scholarly publication um and now want to

647
00:28:07,519 --> 00:28:12,960
have a voice or a say which is very

648
00:28:10,399 --> 00:28:15,039
appropriate into how that content is

649
00:28:12,960 --> 00:28:18,200
going to be used moving

650
00:28:15,039 --> 00:28:20,559
forward and the libraries and open

651
00:28:18,200 --> 00:28:22,880
learning um you know Chris can talk to

652
00:28:20,559 --> 00:28:26,720
us more about Open learning are two of

653
00:28:22,880 --> 00:28:28,760
our biggest stewards of content that MIT

654
00:28:26,720 --> 00:28:33,000
either owns

655
00:28:28,760 --> 00:28:37,120
licenses or otherwise um houses for lack

656
00:28:33,000 --> 00:28:39,080
of a better word at MIT and those

657
00:28:37,120 --> 00:28:42,080
offices on campus really are trying to

658
00:28:39,080 --> 00:28:44,880
think very thoughtfully about how to

659
00:28:42,080 --> 00:28:47,200
continue to give researchers the access

660
00:28:44,880 --> 00:28:50,960
they need under the appropriate

661
00:28:47,200 --> 00:28:54,360
permissions preserve the openness of

662
00:28:50,960 --> 00:28:56,960
content and encourage Scholars to

663
00:28:54,360 --> 00:28:59,480
continue to embrace the value of that

664
00:28:56,960 --> 00:29:01,120
openness while at the same time

665
00:28:59,480 --> 00:29:04,320
acknowledging that sometimes this

666
00:29:01,120 --> 00:29:07,279
content is being used um either for

667
00:29:04,320 --> 00:29:10,200
purposes that are not aligned with that

668
00:29:07,279 --> 00:29:12,360
author or content generator's principles

669
00:29:10,200 --> 00:29:15,799
or values or in a way that they're just

670
00:29:12,360 --> 00:29:18,799
not aware of which of course takes away

671
00:29:15,799 --> 00:29:22,559
um you know some autonomy and and makes

672
00:29:18,799 --> 00:29:25,440
it a little uh less likely perhaps

673
00:29:22,559 --> 00:29:28,760
subconsciously to want to make things

674
00:29:25,440 --> 00:29:32,240
available and open in the future

675
00:29:28,760 --> 00:29:34,360
um I think there are a a very and and

676
00:29:32,240 --> 00:29:37,360
and those categories that I just named

677
00:29:34,360 --> 00:29:40,000
of course there's overlap right so you

678
00:29:37,360 --> 00:29:43,600
could be a researcher training a model

679
00:29:40,000 --> 00:29:46,000
who is desperate to access content from

680
00:29:43,600 --> 00:29:47,960
a thirdparty publisher through an

681
00:29:46,000 --> 00:29:50,519
agreement with the libraries let's just

682
00:29:47,960 --> 00:29:52,559
say and that thirdparty publisher could

683
00:29:50,519 --> 00:29:55,919
be pushing back on the librar about

684
00:29:52,559 --> 00:29:59,279
giving access to that data for training

685
00:29:55,919 --> 00:30:01,320
purposes and the research could be very

686
00:29:59,279 --> 00:30:03,919
frustrated on the other hand that

687
00:30:01,320 --> 00:30:06,399
researcher could have published a book

688
00:30:03,919 --> 00:30:09,960
let's just say with the MIT press and

689
00:30:06,399 --> 00:30:14,279
could now be being asked to opt in to

690
00:30:09,960 --> 00:30:16,679
licensing that book to open AI or Google

691
00:30:14,279 --> 00:30:18,200
to train their commercial product and

692
00:30:16,679 --> 00:30:19,360
that researcher could have a very

693
00:30:18,200 --> 00:30:22,480
different

694
00:30:19,360 --> 00:30:26,480
perspective um on whether and to what

695
00:30:22,480 --> 00:30:28,880
extent to allow that access um I also

696
00:30:26,480 --> 00:30:32,600
think there's a really big difference in

697
00:30:28,880 --> 00:30:37,159
perspectives you know at MIT in terms of

698
00:30:32,600 --> 00:30:39,919
if we make if we don't make our content

699
00:30:37,159 --> 00:30:42,919
available and open to anybody to train

700
00:30:39,919 --> 00:30:46,080
models um those models and the public

701
00:30:42,919 --> 00:30:49,320
will suffer right because we at MIT you

702
00:30:46,080 --> 00:30:51,799
at MIT have the best science right so we

703
00:30:49,320 --> 00:30:56,200
want the best science to go into

704
00:30:51,799 --> 00:30:59,399
training um others feel that if we have

705
00:30:56,200 --> 00:31:02,760
no control over how content is used for

706
00:30:59,399 --> 00:31:07,320
training especially outside of MIT we

707
00:31:02,760 --> 00:31:11,159
will be disrupting the way that um

708
00:31:07,320 --> 00:31:13,840
attribution has worked for you know

709
00:31:11,159 --> 00:31:18,880
science and scholarly

710
00:31:13,840 --> 00:31:18,880
advancement um for decades if not

711
00:31:19,880 --> 00:31:27,159
centuries so I think and I want to be

712
00:31:22,880 --> 00:31:29,679
mindful of time here um I think one of

713
00:31:27,159 --> 00:31:31,760
the the big items that we're thinking

714
00:31:29,679 --> 00:31:35,559
about in terms of providing guidance is

715
00:31:31,760 --> 00:31:38,600
also education I think there are a lot

716
00:31:35,559 --> 00:31:42,000
of particularly grad students and other

717
00:31:38,600 --> 00:31:45,200
researchers who are very used to

718
00:31:42,000 --> 00:31:47,039
culturally um simply making things

719
00:31:45,200 --> 00:31:50,760
available whether it's software on

720
00:31:47,039 --> 00:31:53,760
GitHub or other content um on you know

721
00:31:50,760 --> 00:31:56,720
whatever Lab website they have or Etc

722
00:31:53,760 --> 00:31:58,720
sharing it very freely um because it's

723
00:31:56,720 --> 00:32:00,240
part of our culture so I I think part of

724
00:31:58,720 --> 00:32:02,880
the guidance is going to be around

725
00:32:00,240 --> 00:32:05,120
educating and and talking about sort of

726
00:32:02,880 --> 00:32:08,360
um you know what choices are available

727
00:32:05,120 --> 00:32:10,720
if there are choices and what do certain

728
00:32:08,360 --> 00:32:14,919
things mean when you you know take path

729
00:32:10,720 --> 00:32:16,639
a versus path B um and then I I really

730
00:32:14,919 --> 00:32:19,200
quickly just wanted

731
00:32:16,639 --> 00:32:21,000
to touch on another group of

732
00:32:19,200 --> 00:32:24,240
stakeholders that I briefly mentioned

733
00:32:21,000 --> 00:32:26,840
before and those are our inhouse

734
00:32:24,240 --> 00:32:30,159
publishing entities so to speak and that

735
00:32:26,840 --> 00:32:32,440
includes the MIT press the MIT tech

736
00:32:30,159 --> 00:32:35,519
review and the slow management

737
00:32:32,440 --> 00:32:38,080
review and those entities and for those

738
00:32:35,519 --> 00:32:39,919
of you who don't know the MIT tech

739
00:32:38,080 --> 00:32:41,880
review is separate from MIT it's an

740
00:32:39,919 --> 00:32:45,000
affiliate it's a wholly owned subsidiary

741
00:32:41,880 --> 00:32:47,840
but they are not you know part of the

742
00:32:45,000 --> 00:32:51,960
the educational research institution

743
00:32:47,840 --> 00:32:55,159
bubble but MIT press is um they're just

744
00:32:51,960 --> 00:32:58,399
like any other dlci and that is really

745
00:32:55,159 --> 00:33:01,159
fascinating because their interests

746
00:32:58,399 --> 00:33:04,639
are not always perfectly aligned right

747
00:33:01,159 --> 00:33:07,000
with um complete openness for research

748
00:33:04,639 --> 00:33:09,440
purposes and they along with the tech

749
00:33:07,000 --> 00:33:13,440
review and Sal management review to a

750
00:33:09,440 --> 00:33:16,320
certain extent see an existential crisis

751
00:33:13,440 --> 00:33:20,200
in terms of science

752
00:33:16,320 --> 00:33:22,799
journalism and scholarly book publishing

753
00:33:20,200 --> 00:33:26,279
um that I think is is you know those

754
00:33:22,799 --> 00:33:29,240
concerns are very real and very valid so

755
00:33:26,279 --> 00:33:32,000
we are also trying to be mindful of

756
00:33:29,240 --> 00:33:33,679
those interests um while thinking

757
00:33:32,000 --> 00:33:36,600
holistically which kind of gets me back

758
00:33:33,679 --> 00:33:39,880
to my first point about how working at

759
00:33:36,600 --> 00:33:43,159
MIT as a lawyer is challenging and

760
00:33:39,880 --> 00:33:47,320
fascinating um because MIT as a client

761
00:33:43,159 --> 00:33:49,440
is not as simple as one might think so I

762
00:33:47,320 --> 00:33:54,080
think I'll Chris should I stop there and

763
00:33:49,440 --> 00:33:54,080
go to questions thank you all right than

764
00:33:55,120 --> 00:33:59,760
you all right so I might just kick us

765
00:33:57,880 --> 00:34:01,440
off with maybe maybe one or two um but

766
00:33:59,760 --> 00:34:05,399
then I'll move very quickly to the the

767
00:34:01,440 --> 00:34:08,399
Q&A um for the audience thank you um so

768
00:34:05,399 --> 00:34:11,919
um you know I mean I think

769
00:34:08,399 --> 00:34:14,200
that uh we touched on on this this sort

770
00:34:11,919 --> 00:34:17,280
of question of of open and maybe I'll

771
00:34:14,200 --> 00:34:18,960
come back to that in a minute but let me

772
00:34:17,280 --> 00:34:22,720
um let me talk actually a little bit

773
00:34:18,960 --> 00:34:25,879
from where uh you know where Circ um

774
00:34:22,720 --> 00:34:27,399
kind of fits with with this so you know

775
00:34:25,879 --> 00:34:29,919
uh social and ethical responsibilities

776
00:34:27,399 --> 00:34:33,040
of Computing as as I mentioned in the

777
00:34:29,919 --> 00:34:35,119
first session exists to Foster uh what

778
00:34:33,040 --> 00:34:37,919
they call quote responsible habits of

779
00:34:35,119 --> 00:34:39,119
mind and action for those who deploy

780
00:34:37,919 --> 00:34:41,119
Computing

781
00:34:39,119 --> 00:34:42,720
Technologies and and I think that you've

782
00:34:41,119 --> 00:34:47,359
kind of hit on some very concrete

783
00:34:42,720 --> 00:34:51,040
moments where um you know there there

784
00:34:47,359 --> 00:34:54,879
may never be brightline guidance that's

785
00:34:51,040 --> 00:34:56,919
going to come from your office um but

786
00:34:54,879 --> 00:34:59,040
what in your view you know what what

787
00:34:56,919 --> 00:35:01,480
might respond ible habits of mind and

788
00:34:59,040 --> 00:35:03,800
action look like both you know what

789
00:35:01,480 --> 00:35:06,480
advice you know might you give to MIT

790
00:35:03,800 --> 00:35:09,400
students or those of us who are teaching

791
00:35:06,480 --> 00:35:11,839
MIT students to kind of foster foster

792
00:35:09,400 --> 00:35:15,359
those responsible

793
00:35:11,839 --> 00:35:18,599
habits so I think I is this on yep I

794
00:35:15,359 --> 00:35:20,599
think that's a great question um so I

795
00:35:18,599 --> 00:35:23,800
have a a few different thoughts about

796
00:35:20,599 --> 00:35:26,640
this and I have you know spent a lot of

797
00:35:23,800 --> 00:35:30,560
time thinking about this because I do

798
00:35:26,640 --> 00:35:33,079
think when so there's I think about MIT

799
00:35:30,560 --> 00:35:34,960
as being a very open Kingdom but then

800
00:35:33,079 --> 00:35:38,480
there's all these Gates and there's sort

801
00:35:34,960 --> 00:35:41,560
of checks at all of these Gates um where

802
00:35:38,480 --> 00:35:43,560
technology leaves and once it leaves you

803
00:35:41,560 --> 00:35:47,280
know you you can never take it back

804
00:35:43,560 --> 00:35:51,599
right um so I

805
00:35:47,280 --> 00:35:54,359
think there is something to be said in

806
00:35:51,599 --> 00:35:57,760
in the context of wanting to make

807
00:35:54,359 --> 00:36:00,560
research open so that researchers can

808
00:35:57,760 --> 00:36:02,880
build upon each other's work um Advanced

809
00:36:00,560 --> 00:36:05,280
knowledge and also for you know science

810
00:36:02,880 --> 00:36:08,079
sake and for curiosity there is

811
00:36:05,280 --> 00:36:12,200
something to be said for balancing that

812
00:36:08,079 --> 00:36:15,880
and taking a breath right and and being

813
00:36:12,200 --> 00:36:19,960
thoughtful about potential

814
00:36:15,880 --> 00:36:23,960
Downstream uses of technology and

815
00:36:19,960 --> 00:36:25,880
whether there are ways to um you know

816
00:36:23,960 --> 00:36:27,839
you use the term before signaling with

817
00:36:25,880 --> 00:36:29,720
the Creative Commons I think you know

818
00:36:27,839 --> 00:36:33,520
there's you can't have a lot of control

819
00:36:29,720 --> 00:36:35,839
but can you signal and can you put up um

820
00:36:33,520 --> 00:36:37,920
some gates that you know where you have

821
00:36:35,839 --> 00:36:40,839
to walk through them before you leave

822
00:36:37,920 --> 00:36:42,760
the MIT Kingdom and one of those this is

823
00:36:40,839 --> 00:36:45,359
a great example this happened with the

824
00:36:42,760 --> 00:36:49,040
crisper technology um that you know was

825
00:36:45,359 --> 00:36:50,359
developed by MIT and bro and Harvard and

826
00:36:49,040 --> 00:36:52,160
I don't know how many of you know this

827
00:36:50,359 --> 00:36:54,599
but you know we have an agreement with

828
00:36:52,160 --> 00:36:56,040
broad so broad actually does our our

829
00:36:54,599 --> 00:36:58,079
license agreements even though we

830
00:36:56,040 --> 00:37:00,280
partially own that technology

831
00:36:58,079 --> 00:37:03,119
and the way broad very thoughtfully

832
00:37:00,280 --> 00:37:07,480
licensed the crisper technology is with

833
00:37:03,119 --> 00:37:09,839
with a with a set of very um intricate

834
00:37:07,480 --> 00:37:11,200
thoughtful Provisions in license

835
00:37:09,839 --> 00:37:13,200
agreements and of course I'm going

836
00:37:11,200 --> 00:37:16,400
straight to the legal the legal thinking

837
00:37:13,200 --> 00:37:18,839
but that restricted what you could do

838
00:37:16,400 --> 00:37:21,720
with crisper technology right so very

839
00:37:18,839 --> 00:37:24,200
early on you couldn't use it to make

840
00:37:21,720 --> 00:37:27,520
designer babies you know very early on

841
00:37:24,200 --> 00:37:31,240
you couldn't use it to basically dis

842
00:37:27,520 --> 00:37:35,040
disrupt certain um agricultural

843
00:37:31,240 --> 00:37:36,920
agriculture activities with seed um

844
00:37:35,040 --> 00:37:38,480
genetics especially in developing

845
00:37:36,920 --> 00:37:41,359
countries I mean I'm just naming a few

846
00:37:38,480 --> 00:37:45,440
these were very lengthy thoughtful

847
00:37:41,359 --> 00:37:49,400
provisions and the licensing um and the

848
00:37:45,440 --> 00:37:51,720
choice of commercial Partners was made

849
00:37:49,400 --> 00:37:53,280
very strategically and very thoughtfully

850
00:37:51,720 --> 00:37:56,200
in terms of who is going to take this

851
00:37:53,280 --> 00:37:58,720
technology and commercialize it now that

852
00:37:56,200 --> 00:38:01,599
was a really easy thing to do because

853
00:37:58,720 --> 00:38:03,400
crisper technology was covered um by

854
00:38:01,599 --> 00:38:06,480
Patent protection right and patent

855
00:38:03,400 --> 00:38:08,280
protection is easier to control than all

856
00:38:06,480 --> 00:38:10,440
of the different pieces that you know go

857
00:38:08,280 --> 00:38:12,599
into an AI system whether it's the model

858
00:38:10,440 --> 00:38:14,280
which is usually not patentable and the

859
00:38:12,599 --> 00:38:18,440
data sets and you know the parameters

860
00:38:14,280 --> 00:38:20,000
and weights Etc um but I think there are

861
00:38:18,440 --> 00:38:21,880
you know bodies and Chris you may know

862
00:38:20,000 --> 00:38:24,400
about this who are thinking about ways

863
00:38:21,880 --> 00:38:26,720
to thoughtfully license generative AI

864
00:38:24,400 --> 00:38:28,440
tools I'm forgetting the names off the

865
00:38:26,720 --> 00:38:30,960
top of my head but there a few nonprofit

866
00:38:28,440 --> 00:38:33,040
organizations that are coming out with

867
00:38:30,960 --> 00:38:35,480
models for how to thoughtfully license

868
00:38:33,040 --> 00:38:38,560
this technology so something really easy

869
00:38:35,480 --> 00:38:41,160
for a lab to think about would be do we

870
00:38:38,560 --> 00:38:44,520
have a lab policy like an internal lab

871
00:38:41,160 --> 00:38:46,599
policy where we we are thoughtful about

872
00:38:44,520 --> 00:38:49,720
under what license we release something

873
00:38:46,599 --> 00:38:51,319
on GitHub you know will it control

874
00:38:49,720 --> 00:38:54,720
Downstream use maybe not but it's

875
00:38:51,319 --> 00:38:57,760
certainly signals and I think MIT has

876
00:38:54,720 --> 00:39:01,280
such a loud voice and and a power full

877
00:38:57,760 --> 00:39:03,359
voice in The open- Source community that

878
00:39:01,280 --> 00:39:05,240
I think you know I mean the MIT license

879
00:39:03,359 --> 00:39:07,400
itself that that everybody uses the open

880
00:39:05,240 --> 00:39:09,760
is called the MIT license you know we as

881
00:39:07,400 --> 00:39:11,640
lawyers often get law squeamish because

882
00:39:09,760 --> 00:39:14,000
it it's not like we licensed to them

883
00:39:11,640 --> 00:39:17,440
it's just in fact I'm sure that some of

884
00:39:14,000 --> 00:39:20,720
you even saw that um deep seek said they

885
00:39:17,440 --> 00:39:23,240
had an MIT license well they don't they

886
00:39:20,720 --> 00:39:27,160
they're you know they licensed things

887
00:39:23,240 --> 00:39:29,960
under the open- source MIT license right

888
00:39:27,160 --> 00:39:31,640
so the point being is that we have a

889
00:39:29,960 --> 00:39:35,560
powerful voice and I think we can be

890
00:39:31,640 --> 00:39:36,839
really thoughtful about how we um

891
00:39:35,560 --> 00:39:39,680
proliferate

892
00:39:36,839 --> 00:39:41,040
technology yeah I think um you've just

893
00:39:39,680 --> 00:39:44,160
answered the other question I was going

894
00:39:41,040 --> 00:39:45,839
to ask which is what's different at MIT

895
00:39:44,160 --> 00:39:48,119
than at other universities right in some

896
00:39:45,839 --> 00:39:49,480
ways if you were in your job at at any

897
00:39:48,119 --> 00:39:51,599
university in the United States a

898
00:39:49,480 --> 00:39:53,680
research University in particular you

899
00:39:51,599 --> 00:39:56,640
would be facing these things but at an

900
00:39:53,680 --> 00:39:59,720
MIT with this long-standing commitment

901
00:39:56,640 --> 00:40:02,839
to the world of open and also This

902
00:39:59,720 --> 00:40:04,400
Global reputation for it whether it is

903
00:40:02,839 --> 00:40:07,160
through the publication of materials on

904
00:40:04,400 --> 00:40:08,680
open courseware the open uh uh

905
00:40:07,160 --> 00:40:10,880
publishing license first you know

906
00:40:08,680 --> 00:40:12,839
initiated uh through the MIT libraries

907
00:40:10,880 --> 00:40:15,440
and adopted at you know thousands of

908
00:40:12,839 --> 00:40:18,240
universities around the world and the

909
00:40:15,440 --> 00:40:20,680
the the MIT license or so-called MIT

910
00:40:18,240 --> 00:40:23,319
license that is a real gold standard for

911
00:40:20,680 --> 00:40:25,319
for open source software um does that

912
00:40:23,319 --> 00:40:27,760
make it does that make your job easier

913
00:40:25,319 --> 00:40:29,040
or harder

914
00:40:27,760 --> 00:40:30,520
another really good question I think

915
00:40:29,040 --> 00:40:32,760
it's a little bit of both I mean I think

916
00:40:30,520 --> 00:40:34,760
it's easier in the sense that we get to

917
00:40:32,760 --> 00:40:37,960
think the big thoughts and when we talk

918
00:40:34,760 --> 00:40:39,960
people listen to us um and I think we

919
00:40:37,960 --> 00:40:44,200
have a real opportunity to be

920
00:40:39,960 --> 00:40:45,560
forward-looking and sort of put out some

921
00:40:44,200 --> 00:40:46,960
thoughts about how we think things

922
00:40:45,560 --> 00:40:48,839
should be done and I don't think that

923
00:40:46,960 --> 00:40:53,079
all universities have that but on the

924
00:40:48,839 --> 00:40:57,359
flip side we are in the news um we will

925
00:40:53,079 --> 00:41:00,200
be judged uh by historians in terms of

926
00:40:57,359 --> 00:41:02,400
of what actions we took or didn't take

927
00:41:00,200 --> 00:41:05,000
um so I think it's I think it's really a

928
00:41:02,400 --> 00:41:07,960
mix great all right well let's open

929
00:41:05,000 --> 00:41:10,040
things up to uh Q&A and then again as

930
00:41:07,960 --> 00:41:11,920
last time um you know we'll I'll try to

931
00:41:10,040 --> 00:41:13,560
pass a mic to you and if you're too far

932
00:41:11,920 --> 00:41:16,040
I'll just repeat the question into the

933
00:41:13,560 --> 00:41:19,359
mic so people watching the recording can

934
00:41:16,040 --> 00:41:19,359
can hear so the floor is

935
00:41:20,520 --> 00:41:26,720
open yeah go for it come on up

936
00:41:29,760 --> 00:41:34,280
I I just want to say what they just

937
00:41:31,560 --> 00:41:35,079
shared is very very useful helpful I

938
00:41:34,280 --> 00:41:38,359
just

939
00:41:35,079 --> 00:41:41,040
wonder if you are preparing your office

940
00:41:38,359 --> 00:41:45,040
preparing or plan to prepare some

941
00:41:41,040 --> 00:41:47,640
document about like a AI guidance or

942
00:41:45,040 --> 00:41:49,200
something uh for the entire campus that

943
00:41:47,640 --> 00:41:52,319
could be really helpful I know it's not

944
00:41:49,200 --> 00:41:54,680
easy to be complete especially still

945
00:41:52,319 --> 00:41:56,480
developing right but it still be very

946
00:41:54,680 --> 00:41:58,760
even some key points will be very very

947
00:41:56,480 --> 00:42:03,040
helpful you can show some point if you

948
00:41:58,760 --> 00:42:06,160
can surely um so and and can I ask to

949
00:42:03,040 --> 00:42:10,040
are are you what's your um

950
00:42:06,160 --> 00:42:12,839
M I'm part of MIT office of relations oh

951
00:42:10,040 --> 00:42:15,200
great Interac excellent yes I know

952
00:42:12,839 --> 00:42:18,760
you're office as well okay thank you um

953
00:42:15,200 --> 00:42:20,720
so absolutely the goal is to come out

954
00:42:18,760 --> 00:42:22,520
with guidance sort of for the three

955
00:42:20,720 --> 00:42:25,680
categories that I spoke about at the

956
00:42:22,520 --> 00:42:29,480
beginning um it won't come from my

957
00:42:25,680 --> 00:42:32,359
office per se you know we we try at MIT

958
00:42:29,480 --> 00:42:34,680
to keep the lawyer guidance to a minimum

959
00:42:32,359 --> 00:42:37,359
um but we will certainly be

960
00:42:34,680 --> 00:42:40,720
participating in preparing that Guidance

961
00:42:37,359 --> 00:42:42,599
with a number of stakeholders on campus

962
00:42:40,720 --> 00:42:45,760
um who really need to be involved and I

963
00:42:42,599 --> 00:42:47,480
think that's what's making things slow

964
00:42:45,760 --> 00:42:49,559
believe it or not I think MIT is ahead

965
00:42:47,480 --> 00:42:52,400
of the game in terms of a lot of this

966
00:42:49,559 --> 00:42:54,000
but it's still hard to ensure that we've

967
00:42:52,400 --> 00:42:55,960
properly considered which is really

968
00:42:54,000 --> 00:42:57,599
important different perspectives from

969
00:42:55,960 --> 00:43:00,079
all over campus and we have the right

970
00:42:57,599 --> 00:43:02,599
people in the room um you know Chris can

971
00:43:00,079 --> 00:43:04,599
attest to this sometimes when we don't

972
00:43:02,599 --> 00:43:07,000
have the right people in the room we

973
00:43:04,599 --> 00:43:08,079
miss things and and we need to backtrack

974
00:43:07,000 --> 00:43:10,559
a little bit so we're trying to be

975
00:43:08,079 --> 00:43:14,040
careful there is guidance that first

976
00:43:10,559 --> 00:43:15,920
category about users of AI tools um

977
00:43:14,040 --> 00:43:18,880
there is some guidance actually oddly

978
00:43:15,920 --> 00:43:22,559
enough on the isnt website about

979
00:43:18,880 --> 00:43:25,559
responsible use of AI tools just in

980
00:43:22,559 --> 00:43:27,640
research or in other offices on campus

981
00:43:25,559 --> 00:43:29,800
you know as researchers you may

982
00:43:27,640 --> 00:43:33,520
find it interesting because it gives you

983
00:43:29,800 --> 00:43:36,040
the breakdown of what types of data MIT

984
00:43:33,520 --> 00:43:38,599
treats with different various levels of

985
00:43:36,040 --> 00:43:40,440
of security and so when you're using

986
00:43:38,599 --> 00:43:43,160
certain types of data you know whether

987
00:43:40,440 --> 00:43:46,760
certain permissions are needed but

988
00:43:43,160 --> 00:43:49,720
probably not so useful to all of you I

989
00:43:46,760 --> 00:43:52,400
think it's a it's a little bit more

990
00:43:49,720 --> 00:43:55,760
basic the next two categories are a

991
00:43:52,400 --> 00:43:57,880
little bit harder um the second category

992
00:43:55,760 --> 00:44:00,240
I I hope there will be up dates sort of

993
00:43:57,880 --> 00:44:02,559
about that research scaffolding soon but

994
00:44:00,240 --> 00:44:06,079
I I don't really know what the timeline

995
00:44:02,559 --> 00:44:08,359
is for that the last category um there

996
00:44:06,079 --> 00:44:11,200
has been a new working group that has

997
00:44:08,359 --> 00:44:13,640
been tasked by the vice president for

998
00:44:11,200 --> 00:44:15,359
research um Chris and I will be

999
00:44:13,640 --> 00:44:17,400
participating on that working group I

1000
00:44:15,359 --> 00:44:20,319
hope I can say that oh you've caught me

1001
00:44:17,400 --> 00:44:22,520
publicly I know he can't say no now um

1002
00:44:20,319 --> 00:44:24,760
but so I I think that there will

1003
00:44:22,520 --> 00:44:26,280
hopefully be some guidance coming out of

1004
00:44:24,760 --> 00:44:29,280
that which will be written and that will

1005
00:44:26,280 --> 00:44:33,920
be you know very specific you know for

1006
00:44:29,280 --> 00:44:36,319
instance researchers giving them offices

1007
00:44:33,920 --> 00:44:38,400
and actual administrators that they can

1008
00:44:36,319 --> 00:44:41,280
go to for questions about how to

1009
00:44:38,400 --> 00:44:44,319
permission and access certain data sets

1010
00:44:41,280 --> 00:44:46,680
properly and in a timely manner stuff

1011
00:44:44,319 --> 00:44:50,680
like that um some of these bigger

1012
00:44:46,680 --> 00:44:54,400
questions around how do you ethically

1013
00:44:50,680 --> 00:44:56,240
and responsibly think about sharing your

1014
00:44:54,400 --> 00:44:57,800
technology and your research results I

1015
00:44:56,240 --> 00:45:00,599
think that will be

1016
00:44:57,800 --> 00:45:04,000
a little bit trickier to put together

1017
00:45:00,599 --> 00:45:04,000
holistic comprehensive

1018
00:45:06,640 --> 00:45:13,480
guidance all

1019
00:45:09,280 --> 00:45:13,480
right other thoughts

1020
00:45:18,000 --> 00:45:24,280
questions a specific challenge that you

1021
00:45:21,319 --> 00:45:27,960
think we could come up with a solution

1022
00:45:24,280 --> 00:45:29,160
for that we don't yet have like I feel

1023
00:45:27,960 --> 00:45:31,559
like we're

1024
00:45:29,160 --> 00:45:35,880
anticipating so much and there's also

1025
00:45:31,559 --> 00:45:38,520
already so much to respond to you're put

1026
00:45:35,880 --> 00:45:42,240
in an impossible

1027
00:45:38,520 --> 00:45:45,599
position just repeat yeah so um I was

1028
00:45:42,240 --> 00:45:47,800
asked can I think can I pinpoint sort of

1029
00:45:45,599 --> 00:45:50,480
a a specific instance where there's

1030
00:45:47,800 --> 00:45:53,720
something I guess we can do

1031
00:45:50,480 --> 00:45:56,640
preventatively is that right to try to

1032
00:45:53,720 --> 00:46:01,200
head off maybe some big risk if I could

1033
00:45:56,640 --> 00:46:04,200
reward that does that okay um so I think

1034
00:46:01,200 --> 00:46:07,119
I touched on this before with various

1035
00:46:04,200 --> 00:46:09,839
licensing models

1036
00:46:07,119 --> 00:46:11,319
um the same way that Chris was saying

1037
00:46:09,839 --> 00:46:15,400
and I totally agree with this that we

1038
00:46:11,319 --> 00:46:18,520
need to have new IP rules to cover

1039
00:46:15,400 --> 00:46:23,240
generative Ai and inputs and outputs and

1040
00:46:18,520 --> 00:46:27,520
um you know copyright Etc I think we're

1041
00:46:23,240 --> 00:46:28,760
going to need to think through how we as

1042
00:46:27,520 --> 00:46:31,839
research

1043
00:46:28,760 --> 00:46:36,280
institutions um think about sharing

1044
00:46:31,839 --> 00:46:38,559
right I I think that we want sharing and

1045
00:46:36,280 --> 00:46:41,319
collaboration and openness always has to

1046
00:46:38,559 --> 00:46:44,839
be a priority but I think we're going to

1047
00:46:41,319 --> 00:46:51,119
have to think about new ways to do

1048
00:46:44,839 --> 00:46:54,240
it in a manner that bakes in this ethic

1049
00:46:51,119 --> 00:46:57,240
com ethics component and responsibility

1050
00:46:54,240 --> 00:46:59,000
component um and I would argue also

1051
00:46:57,240 --> 00:47:02,280
maybe a little bit of sort

1052
00:46:59,000 --> 00:47:06,520
of foresight and predictability

1053
00:47:02,280 --> 00:47:06,520
component instead of

1054
00:47:06,599 --> 00:47:14,599
simply not thinking about about those

1055
00:47:09,880 --> 00:47:18,680
things but I think that requires some

1056
00:47:14,599 --> 00:47:22,599
centralization of how we do things and I

1057
00:47:18,680 --> 00:47:24,240
don't know that we are close to that for

1058
00:47:22,599 --> 00:47:26,960
many good reasons

1059
00:47:24,240 --> 00:47:28,640
too yeah I think you know another

1060
00:47:26,960 --> 00:47:29,760
concept that's useful for thinking about

1061
00:47:28,640 --> 00:47:32,960
this and this came up in our

1062
00:47:29,760 --> 00:47:34,640
conversation with um earlier uh is what

1063
00:47:32,960 --> 00:47:38,079
the author's Guild calls you know their

1064
00:47:34,640 --> 00:47:41,000
three C's right consent credit and and

1065
00:47:38,079 --> 00:47:43,319
compensation and what credit means in in

1066
00:47:41,000 --> 00:47:45,839
a kind of scholarly context is is very

1067
00:47:43,319 --> 00:47:47,520
different right because of as you

1068
00:47:45,839 --> 00:47:51,079
mentioned it these sort of questions of

1069
00:47:47,520 --> 00:47:53,839
attribution citation replicability those

1070
00:47:51,079 --> 00:47:56,200
kind of core values of scholarly

1071
00:47:53,839 --> 00:47:59,559
research in not only in The Sciences but

1072
00:47:56,200 --> 00:48:01,960
across you know disciplines um are are

1073
00:47:59,559 --> 00:48:05,800
very much disrupted by generative AI

1074
00:48:01,960 --> 00:48:08,520
models as they exist now um and you know

1075
00:48:05,800 --> 00:48:11,359
that's I think an area where um you know

1076
00:48:08,520 --> 00:48:14,440
as a research Community we'll we'll be

1077
00:48:11,359 --> 00:48:16,200
looking for for guidance from MIT on how

1078
00:48:14,440 --> 00:48:19,280
to do our work but it would be great

1079
00:48:16,200 --> 00:48:22,119
then also to share share that and maybe

1080
00:48:19,280 --> 00:48:24,359
model for other research communities you

1081
00:48:22,119 --> 00:48:27,680
know best ways to kind of capture the

1082
00:48:24,359 --> 00:48:30,160
the spirit and values of academic sit a

1083
00:48:27,680 --> 00:48:31,880
and adapt them to the new technology I

1084
00:48:30,160 --> 00:48:33,359
think that's absolutely right and the

1085
00:48:31,880 --> 00:48:36,359
you know maybe this is just the lawyer

1086
00:48:33,359 --> 00:48:38,160
and me but the immediate response I have

1087
00:48:36,359 --> 00:48:41,000
that and we we have to solve this we

1088
00:48:38,160 --> 00:48:44,240
have to figure out a way to manage this

1089
00:48:41,000 --> 00:48:47,240
is the simple misalignment

1090
00:48:44,240 --> 00:48:49,559
between you know academic attribution

1091
00:48:47,240 --> 00:48:51,760
the the three C's the you know consent

1092
00:48:49,559 --> 00:48:54,200
credit and compensation although maybe

1093
00:48:51,760 --> 00:49:00,359
compensation is less important for us

1094
00:48:54,200 --> 00:49:03,280
and the um ability to access data and

1095
00:49:00,359 --> 00:49:08,160
other research freely to advance your

1096
00:49:03,280 --> 00:49:10,200
own research so um I think that's the

1097
00:49:08,160 --> 00:49:12,760
and and as I was ref referencing before

1098
00:49:10,200 --> 00:49:14,839
I mean you could be the same researcher

1099
00:49:12,760 --> 00:49:17,839
and have a be of two

1100
00:49:14,839 --> 00:49:21,960
minds B

1101
00:49:17,839 --> 00:49:21,960
absolutely all right uh yeah

1102
00:49:23,440 --> 00:49:29,240
question uh so the question actually

1103
00:49:26,079 --> 00:49:31,160
that you already touch also is uh I

1104
00:49:29,240 --> 00:49:33,319
wonder when you're thinking about all

1105
00:49:31,160 --> 00:49:36,599
this issue and come up with some kind of

1106
00:49:33,319 --> 00:49:40,040
guideline for the community what kind of

1107
00:49:36,599 --> 00:49:42,720
like a uh guiding principle that you

1108
00:49:40,040 --> 00:49:46,119
follow I I I think you mention a lot

1109
00:49:42,720 --> 00:49:49,599
about openness and I wonder what other

1110
00:49:46,119 --> 00:49:52,319
the factor and in terms of priority wise

1111
00:49:49,599 --> 00:49:55,920
and how does it uh incorporate like MIT

1112
00:49:52,319 --> 00:49:57,520
is a historical like a value uh into

1113
00:49:55,920 --> 00:50:00,160
this kind of things thinking so that

1114
00:49:57,520 --> 00:50:03,799
will be a question that i' like to ask

1115
00:50:00,160 --> 00:50:05,040
you Clarity on that so first of all I

1116
00:50:03,799 --> 00:50:07,480
appreciate that question because I'd

1117
00:50:05,040 --> 00:50:09,799
like to take a note to remember this for

1118
00:50:07,480 --> 00:50:11,559
the working group because I think that

1119
00:50:09,799 --> 00:50:14,040
should be part of our first order of

1120
00:50:11,559 --> 00:50:16,760
business is to think about what are our

1121
00:50:14,040 --> 00:50:18,640
priorities and list them you know in

1122
00:50:16,760 --> 00:50:21,480
terms of the values we want to be

1123
00:50:18,640 --> 00:50:23,839
thinking about um my short answer is I

1124
00:50:21,480 --> 00:50:26,200
don't know what

1125
00:50:23,839 --> 00:50:29,880
mit's position will be I can tell you

1126
00:50:26,200 --> 00:50:33,720
what personally I think Mak sense which

1127
00:50:29,880 --> 00:50:36,599
is that openness because first and

1128
00:50:33,720 --> 00:50:40,000
foremost we are a research and

1129
00:50:36,599 --> 00:50:43,559
educational institution and we want to

1130
00:50:40,000 --> 00:50:46,960
um pursue research for research sake

1131
00:50:43,559 --> 00:50:51,119
right we science for for science sake so

1132
00:50:46,960 --> 00:50:53,880
openness for sure I think um autonomy

1133
00:50:51,119 --> 00:50:57,319
and choice I think we want to

1134
00:50:53,880 --> 00:51:00,119
preserve uh our you know

1135
00:50:57,319 --> 00:51:02,559
the people who undertake that research

1136
00:51:00,119 --> 00:51:06,559
and develop that scholarly work I think

1137
00:51:02,559 --> 00:51:10,400
we want to listen to you know what they

1138
00:51:06,559 --> 00:51:13,799
want to have done with their research um

1139
00:51:10,400 --> 00:51:16,359
and then third I think we want to be

1140
00:51:13,799 --> 00:51:17,720
good Community we want to be good

1141
00:51:16,359 --> 00:51:20,720
worldwide

1142
00:51:17,720 --> 00:51:23,400
citizens right we need to think about

1143
00:51:20,720 --> 00:51:26,880
what how what we do affects the rest of

1144
00:51:23,400 --> 00:51:29,240
the world and I think it's not too

1145
00:51:26,880 --> 00:51:32,799
far-fetched to say that in the world of

1146
00:51:29,240 --> 00:51:37,240
generative AI especially at a place like

1147
00:51:32,799 --> 00:51:39,960
MIT we decisions we make or don't make

1148
00:51:37,240 --> 00:51:41,960
can have

1149
00:51:39,960 --> 00:51:43,960
worldwide

1150
00:51:41,960 --> 00:51:45,599
consequences so that would be those

1151
00:51:43,960 --> 00:51:47,359
would be the first priorities I would

1152
00:51:45,599 --> 00:51:49,760
think about I don't know Chris if you

1153
00:51:47,359 --> 00:51:52,079
have something to add to that I think I

1154
00:51:49,760 --> 00:51:55,240
would start with M's mission statement

1155
00:51:52,079 --> 00:51:57,280
which I don't know by heart but I I know

1156
00:51:55,240 --> 00:52:00,000
how to find it on the website

1157
00:51:57,280 --> 00:52:02,160
and assume that you know what whatever

1158
00:52:00,000 --> 00:52:04,400
whatever there is whether it's formal or

1159
00:52:02,160 --> 00:52:05,760
informal guidance or education you know

1160
00:52:04,400 --> 00:52:08,240
should be

1161
00:52:05,760 --> 00:52:10,240
about um advancing you know advancing

1162
00:52:08,240 --> 00:52:11,880
the mission that's the kind of that's

1163
00:52:10,240 --> 00:52:13,559
that's certainly square one where I

1164
00:52:11,880 --> 00:52:16,359
think we would want to

1165
00:52:13,559 --> 00:52:18,720
start I think uh do we have maybe one

1166
00:52:16,359 --> 00:52:18,720
more

1167
00:52:20,720 --> 00:52:25,480
question I'm cognizant that many of you

1168
00:52:23,000 --> 00:52:27,799
are are staring directly into the sunset

1169
00:52:25,480 --> 00:52:29,280
and are having um uh depending on the

1170
00:52:27,799 --> 00:52:32,160
afternoon harder and harder time

1171
00:52:29,280 --> 00:52:34,000
watching the this the session um I think

1172
00:52:32,160 --> 00:52:36,319
we've had a a really kind of amazing

1173
00:52:34,000 --> 00:52:39,280
conversation we've all actually learned

1174
00:52:36,319 --> 00:52:41,160
a great deal of information um uh got

1175
00:52:39,280 --> 00:52:43,480
answers to some questions got questions

1176
00:52:41,160 --> 00:52:44,960
in response to other questions um and

1177
00:52:43,480 --> 00:52:47,280
know that in this rapidly advancing

1178
00:52:44,960 --> 00:52:49,200
field um that that we will continue to

1179
00:52:47,280 --> 00:52:51,040
explore in this area but I'd like to

1180
00:52:49,200 --> 00:52:52,440
thank um from our first session and

1181
00:52:51,040 --> 00:52:54,480
Claire from the second session for

1182
00:52:52,440 --> 00:52:56,640
making time for us today and thank you

1183
00:52:54,480 --> 00:52:58,310
all for coming and Carry On from here

1184
00:52:56,640 --> 00:53:03,300
thank you

1185
00:52:58,310 --> 00:53:03,300
[Applause]

