1
00:00:06,000 --> 00:00:11,040
Uh so hello my name is Maria. I'm a

2
00:00:08,880 --> 00:00:14,080
second year postto fellow uh with the

3
00:00:11,040 --> 00:00:18,080
Smith Center and let me introduce you

4
00:00:14,080 --> 00:00:19,760
Dr. Richard Bono. Richard Ced focuses on

5
00:00:18,080 --> 00:00:21,279
developing who is sorry the vice

6
00:00:19,760 --> 00:00:23,519
president of machine learning for drug

7
00:00:21,279 --> 00:00:26,480
discovery at gintech computational

8
00:00:23,519 --> 00:00:28,240
science. Richard's research focuses on

9
00:00:26,480 --> 00:00:30,720
developing machine learning methods to

10
00:00:28,240 --> 00:00:33,280
power drug discovery. In particular, he

11
00:00:30,720 --> 00:00:35,200
is focused on biomolecular structure and

12
00:00:33,280 --> 00:00:37,840
structure function relationships that

13
00:00:35,200 --> 00:00:40,600
underin drug rank discovery. His work

14
00:00:37,840 --> 00:00:43,440
integrates structure biology approaches

15
00:00:40,600 --> 00:00:46,079
biioinformatics, chemiratics and natural

16
00:00:43,440 --> 00:00:49,600
language processing to derive new hybrid

17
00:00:46,079 --> 00:00:52,160
methods for design. Richard received his

18
00:00:49,600 --> 00:00:54,559
a PhD from University of Washington

19
00:00:52,160 --> 00:00:57,039
prior to his role at Gintech. He was the

20
00:00:54,559 --> 00:00:59,920
director of New York University Center

21
00:00:57,039 --> 00:01:03,840
for Data Science. Richard, we're all

22
00:00:59,920 --> 00:01:03,840
excited for your talk and the floor is

23
00:01:07,960 --> 00:01:12,720
yours. Thanks very much for the invite

24
00:01:10,240 --> 00:01:14,960
and the introduction. Uh today I'm going

25
00:01:12,720 --> 00:01:16,880
to keep this a little bit high level but

26
00:01:14,960 --> 00:01:18,640
we'll dive into just a few things around

27
00:01:16,880 --> 00:01:21,040
active learning and a few things around

28
00:01:18,640 --> 00:01:22,240
generation and sampling. But um the

29
00:01:21,040 --> 00:01:26,080
first thing I want to say is that the

30
00:01:22,240 --> 00:01:28,240
the title here beyond the funnel um you

31
00:01:26,080 --> 00:01:30,560
know the the spirit of this talk is is

32
00:01:28,240 --> 00:01:33,119
to sort of show you some of the efforts

33
00:01:30,560 --> 00:01:35,280
that we carry out at GenEnt to integrate

34
00:01:33,119 --> 00:01:37,759
machine learning into drug discovery.

35
00:01:35,280 --> 00:01:40,079
And some of those things that we have to

36
00:01:37,759 --> 00:01:41,920
do are bookkeeping, some of them are

37
00:01:40,079 --> 00:01:43,200
end-to-end integration in the machine

38
00:01:41,920 --> 00:01:45,759
learning sense of the word and others

39
00:01:43,200 --> 00:01:48,000
are process optimization. But the goal

40
00:01:45,759 --> 00:01:50,640
is to try to find the places where we

41
00:01:48,000 --> 00:01:53,040
take a huge numbers of years and and and

42
00:01:50,640 --> 00:01:55,040
person hours of work and we condense

43
00:01:53,040 --> 00:01:57,960
them to a single lead candidate or we

44
00:01:55,040 --> 00:02:00,399
condense them to a single uh functional

45
00:01:57,960 --> 00:02:02,560
hypothesis and therefore we make a place

46
00:02:00,399 --> 00:02:04,000
where if we fail we don't really know

47
00:02:02,560 --> 00:02:05,840
how to go back. And so we're really

48
00:02:04,000 --> 00:02:07,360
going to talk today about end-to-end

49
00:02:05,840 --> 00:02:09,119
integration with the hope that at any

50
00:02:07,360 --> 00:02:12,879
given moment we always have a large

51
00:02:09,119 --> 00:02:15,200
ensemble of leads, targets, hypotheses

52
00:02:12,879 --> 00:02:17,120
so that we can uh be very fast and very

53
00:02:15,200 --> 00:02:19,840
parallel instead of sequential when we

54
00:02:17,120 --> 00:02:21,520
approach the drug discovery process. And

55
00:02:19,840 --> 00:02:25,760
you know I'm somewhat new to this. Um

56
00:02:21,520 --> 00:02:28,879
I'm part of uh the Genentech um uh

57
00:02:25,760 --> 00:02:30,160
computational sciences uh unit. This was

58
00:02:28,879 --> 00:02:32,480
one of the first times where

59
00:02:30,160 --> 00:02:35,840
computational folks were were elevated

60
00:02:32,480 --> 00:02:38,160
to VPs and SVPs at Genentech. And so I

61
00:02:35,840 --> 00:02:39,840
think this is um part of Aviv Reg's

62
00:02:38,160 --> 00:02:42,160
vision to make computation a real part

63
00:02:39,840 --> 00:02:44,879
of drug discovery. Um I work with John

64
00:02:42,160 --> 00:02:47,959
Marion and this new unit uh Genentech

65
00:02:44,879 --> 00:02:50,959
computation sciences. So if you like uh

66
00:02:47,959 --> 00:02:52,959
genes or cells or mechanisms, you know,

67
00:02:50,959 --> 00:02:54,640
still reach out and talk to us if if

68
00:02:52,959 --> 00:02:56,080
you're looking for things to do in this

69
00:02:54,640 --> 00:02:58,879
area.

70
00:02:56,080 --> 00:03:01,120
This is my unit and again you can I'll

71
00:02:58,879 --> 00:03:04,480
I'll kind of go into where this fits in.

72
00:03:01,120 --> 00:03:06,400
Um but at we were a preient design uh we

73
00:03:04,480 --> 00:03:08,879
were a startup company for about seven

74
00:03:06,400 --> 00:03:12,000
minutes and then we were I sent a note

75
00:03:08,879 --> 00:03:13,800
to Aviv and the rest is history. Um

76
00:03:12,000 --> 00:03:16,319
we're now a unit or a department at

77
00:03:13,800 --> 00:03:19,519
Genentech that that's divided in these

78
00:03:16,319 --> 00:03:22,200
ways. Um, so the the one of the some of

79
00:03:19,519 --> 00:03:25,120
the divisions here are are somewhat

80
00:03:22,200 --> 00:03:27,120
canonical or old-fashioned. And that

81
00:03:25,120 --> 00:03:30,080
would be saying that we need someone to

82
00:03:27,120 --> 00:03:32,720
cover each of the two major modalities.

83
00:03:30,080 --> 00:03:35,120
And so Adams are atoms. All of the

84
00:03:32,720 --> 00:03:37,599
target genes and mechanisms are all made

85
00:03:35,120 --> 00:03:39,680
out of similar stuff in the human. But

86
00:03:37,599 --> 00:03:41,599
the way that we make drugs is still very

87
00:03:39,680 --> 00:03:43,200
ti very strongly tied to two of the

88
00:03:41,599 --> 00:03:46,799
different major ways that we screen for

89
00:03:43,200 --> 00:03:49,200
hits and synthesize the material. And so

90
00:03:46,799 --> 00:03:51,920
even though we're very revolutionary um

91
00:03:49,200 --> 00:03:54,000
we still um we still divide in this very

92
00:03:51,920 --> 00:03:57,439
classical way. The rest of how we divide

93
00:03:54,000 --> 00:03:59,760
things is to have a handful of uh folks

94
00:03:57,439 --> 00:04:02,159
who have no mandate. They just wander

95
00:03:59,760 --> 00:04:03,920
the halls in their in their bathroes

96
00:04:02,159 --> 00:04:06,159
thinking about machine learning. This is

97
00:04:03,920 --> 00:04:08,000
led by Kyongyang Cho who was involved

98
00:04:06,159 --> 00:04:09,360
with the attention mechanism and

99
00:04:08,000 --> 00:04:12,159
involved with a lot of really cool work

100
00:04:09,360 --> 00:04:14,959
with Yosua in the beginning of this deep

101
00:04:12,159 --> 00:04:17,280
learning revolution. Uh we also have

102
00:04:14,959 --> 00:04:20,000
folks that are focused on infrastructure

103
00:04:17,280 --> 00:04:22,560
and we divide them into engineering and

104
00:04:20,000 --> 00:04:25,280
the large language model folks. Due to

105
00:04:22,560 --> 00:04:26,639
the size of these models, they end up uh

106
00:04:25,280 --> 00:04:29,199
thinking about engineering as much as

107
00:04:26,639 --> 00:04:32,080
they think about science. And so we kind

108
00:04:29,199 --> 00:04:35,040
of have the uh uh the theory and the

109
00:04:32,080 --> 00:04:37,120
practice, the application and the theory

110
00:04:35,040 --> 00:04:39,919
divided differently on different days.

111
00:04:37,120 --> 00:04:41,520
Um and in the end we identify as

112
00:04:39,919 --> 00:04:43,600
scientists and so we are always thinking

113
00:04:41,520 --> 00:04:45,120
about technology and platforms but when

114
00:04:43,600 --> 00:04:46,960
we actually dive into the individual

115
00:04:45,120 --> 00:04:49,199
drug discovery projects it's always

116
00:04:46,960 --> 00:04:51,919
quite a bit of fun and if I use the word

117
00:04:49,199 --> 00:04:54,120
portfolio it it means um individual

118
00:04:51,919 --> 00:04:57,120
targets that we're trying to

119
00:04:54,120 --> 00:05:00,720
drug um so this is a picture uh that was

120
00:04:57,120 --> 00:05:02,479
made u mostly by Sarah Mustafabi who is

121
00:05:00,720 --> 00:05:04,639
who's our new vice president of

122
00:05:02,479 --> 00:05:06,720
computational biology and translation.

123
00:05:04,639 --> 00:05:08,880
Um, and I guess she's not that new.

124
00:05:06,720 --> 00:05:10,320
She's about a year old now, which means

125
00:05:08,880 --> 00:05:12,960
she's 5 minutes older than I am at

126
00:05:10,320 --> 00:05:16,000
Genitech. But the idea here is that we

127
00:05:12,960 --> 00:05:18,720
have this interloop or this molecule

128
00:05:16,000 --> 00:05:21,039
making part of the machinery. Uh, we

129
00:05:18,720 --> 00:05:22,880
have the target discovery and

130
00:05:21,039 --> 00:05:24,960
prioritization part of the machinery,

131
00:05:22,880 --> 00:05:27,440
but they use common infrastructure,

132
00:05:24,960 --> 00:05:29,440
common larger uh, language models which

133
00:05:27,440 --> 00:05:31,199
are the interface to this whole system.

134
00:05:29,440 --> 00:05:33,280
So that as we're figuring out what

135
00:05:31,199 --> 00:05:35,360
hypotheses, what targets, what pathways

136
00:05:33,280 --> 00:05:38,800
we think are important to modulate for a

137
00:05:35,360 --> 00:05:40,800
given disease, we are also uh in real

138
00:05:38,800 --> 00:05:42,560
time seeing what what we could do to

139
00:05:40,800 --> 00:05:45,680
modulate those genes at the sort of

140
00:05:42,560 --> 00:05:47,759
composition of matter or molecule level.

141
00:05:45,680 --> 00:05:50,080
And again, we've got some older

142
00:05:47,759 --> 00:05:52,560
divisions and some newer, more

143
00:05:50,080 --> 00:05:54,960
technologically uh motivated divisions

144
00:05:52,560 --> 00:05:56,639
mixed in here. Um, we're really excited

145
00:05:54,960 --> 00:05:58,720
about how we orchestrate this whole

146
00:05:56,639 --> 00:06:01,120
system with larger machine learning

147
00:05:58,720 --> 00:06:03,680
models. Today, I'll mostly give you a

148
00:06:01,120 --> 00:06:07,199
large molecule drug discovery focused

149
00:06:03,680 --> 00:06:10,080
introduction to what we do here. Um, and

150
00:06:07,199 --> 00:06:12,639
you know, the key message really is is

151
00:06:10,080 --> 00:06:14,479
the the utility of active learning, the

152
00:06:12,639 --> 00:06:17,280
utility of using models to collect the

153
00:06:14,479 --> 00:06:18,720
next sample. Um there's lots of other

154
00:06:17,280 --> 00:06:20,960
moving parts but that's the vignette

155
00:06:18,720 --> 00:06:23,800
that I'll drop into in in detail today

156
00:06:20,960 --> 00:06:25,479
or you know in detail I guess talk

157
00:06:23,800 --> 00:06:28,080
detail.

158
00:06:25,479 --> 00:06:30,639
Um before I you know I start I'll just

159
00:06:28,080 --> 00:06:33,360
kind of show this this picture which

160
00:06:30,639 --> 00:06:34,960
shows you the the drugs or the the

161
00:06:33,360 --> 00:06:37,680
medicines that have been approved and

162
00:06:34,960 --> 00:06:40,639
that are on the market um and that

163
00:06:37,680 --> 00:06:42,560
topped out in terms of sales. Um, now

164
00:06:40,639 --> 00:06:45,520
again, we've got all sorts of things

165
00:06:42,560 --> 00:06:47,600
that we know about these drugs from from

166
00:06:45,520 --> 00:06:49,600
our our work as scholars, but also just

167
00:06:47,600 --> 00:06:50,800
reading the newspaper. Um, and so

168
00:06:49,600 --> 00:06:51,840
there's a lot of different mechanisms

169
00:06:50,800 --> 00:06:54,000
here. There's a lot of really

170
00:06:51,840 --> 00:06:56,319
interesting biology behind these drugs,

171
00:06:54,000 --> 00:06:57,840
but what I want you to look at here is

172
00:06:56,319 --> 00:06:59,199
just how wildly different the

173
00:06:57,840 --> 00:07:00,960
composition of matter of all these

174
00:06:59,199 --> 00:07:02,400
things are. So, we have peptides, but

175
00:07:00,960 --> 00:07:04,800
then they have some really far out

176
00:07:02,400 --> 00:07:06,800
chemistry to make them work, you know,

177
00:07:04,800 --> 00:07:10,160
as far as clearance and the mechanism of

178
00:07:06,800 --> 00:07:12,639
the drug over time. Um, we have

179
00:07:10,160 --> 00:07:15,360
antibodies, bicepecific antibodies. We

180
00:07:12,639 --> 00:07:18,720
have pieces of of viral capsids and

181
00:07:15,360 --> 00:07:20,319
coats. We've got RNAs that then express

182
00:07:18,720 --> 00:07:22,240
proteins and and different things that

183
00:07:20,319 --> 00:07:24,400
are also very very very carefully

184
00:07:22,240 --> 00:07:26,720
engineered. And so there's a huge amount

185
00:07:24,400 --> 00:07:28,639
of bioengineering and chemical

186
00:07:26,720 --> 00:07:30,400
engineering, you know, for the actual

187
00:07:28,639 --> 00:07:32,639
manufacturer and process optimization of

188
00:07:30,400 --> 00:07:34,240
these. And there's a lot of molecular

189
00:07:32,639 --> 00:07:35,440
engineering that goes into all of these

190
00:07:34,240 --> 00:07:37,840
things.

191
00:07:35,440 --> 00:07:40,400
um a lot of them are antibodies but at

192
00:07:37,840 --> 00:07:42,880
Genitech we're devoted to research along

193
00:07:40,400 --> 00:07:44,880
many lines across many modalities and so

194
00:07:42,880 --> 00:07:47,680
for every modality you see on this slide

195
00:07:44,880 --> 00:07:50,160
we we have uh efforts at Genitech to to

196
00:07:47,680 --> 00:07:53,560
optimize how machine learning can

197
00:07:50,160 --> 00:07:55,840
improve uh the design of those

198
00:07:53,560 --> 00:07:58,319
molecules and again we're going to drop

199
00:07:55,840 --> 00:08:00,280
into lab in the loop for antibbody drug

200
00:07:58,319 --> 00:08:02,800
discovery or antibbody based drug

201
00:08:00,280 --> 00:08:04,879
discovery when we say antibbody we still

202
00:08:02,800 --> 00:08:07,120
are doing lots of interesting things.

203
00:08:04,879 --> 00:08:08,960
We're making them pH and ATP sensitive.

204
00:08:07,120 --> 00:08:11,120
We're adding them into formats that have

205
00:08:08,960 --> 00:08:13,520
multiple veilances and you know cross

206
00:08:11,120 --> 00:08:15,280
linkers that have strange functions,

207
00:08:13,520 --> 00:08:18,639
transport mechanisms like the brain

208
00:08:15,280 --> 00:08:21,199
shuttle. But at some point we do have to

209
00:08:18,639 --> 00:08:23,360
design uh the affinity part of that and

210
00:08:21,199 --> 00:08:24,560
we have to do that in a context where

211
00:08:23,360 --> 00:08:26,680
we're paying attention to all those

212
00:08:24,560 --> 00:08:29,520
other properties.

213
00:08:26,680 --> 00:08:34,800
Um, so these are the key ingredients I

214
00:08:29,520 --> 00:08:36,719
think for any uh any recipe that uh that

215
00:08:34,800 --> 00:08:38,640
that designs antibodies well and that

216
00:08:36,719 --> 00:08:40,880
does so in a drug discovery context. And

217
00:08:38,640 --> 00:08:42,880
so the first thing to notice is that the

218
00:08:40,880 --> 00:08:47,920
this the key theme on this slide is this

219
00:08:42,880 --> 00:08:49,600
loop. And you know we can say without um

220
00:08:47,920 --> 00:08:51,279
inventing any new math or without

221
00:08:49,600 --> 00:08:54,160
reading any nurips papers or anything

222
00:08:51,279 --> 00:08:57,360
like that that it makes sense to collect

223
00:08:54,160 --> 00:08:59,200
some data and anytime you collect data

224
00:08:57,360 --> 00:09:00,720
if you can you should update your models

225
00:08:59,200 --> 00:09:02,279
and some of that's engineering some of

226
00:09:00,720 --> 00:09:06,560
that's moving data

227
00:09:02,279 --> 00:09:08,720
around but once you have a model what

228
00:09:06,560 --> 00:09:10,720
outputs from the model should you test

229
00:09:08,720 --> 00:09:13,040
and so I encourage everyone to to really

230
00:09:10,720 --> 00:09:14,720
think about active learning and model

231
00:09:13,040 --> 00:09:16,800
driven experimental design is something

232
00:09:14,720 --> 00:09:18,880
that works substantially better today

233
00:09:16,800 --> 00:09:20,320
than it worked two years ago and and

234
00:09:18,880 --> 00:09:22,399
really really like quite a bit better

235
00:09:20,320 --> 00:09:24,360
than it worked 10 years ago. And there's

236
00:09:22,399 --> 00:09:26,959
a couple of different high throughput

237
00:09:24,360 --> 00:09:29,120
biology synthesis

238
00:09:26,959 --> 00:09:30,800
um the way that sort of CRO's for making

239
00:09:29,120 --> 00:09:32,000
DNA and proteins are structured. There's

240
00:09:30,800 --> 00:09:33,600
a lot of things that are process

241
00:09:32,000 --> 00:09:35,360
optimized or just high throughput

242
00:09:33,600 --> 00:09:37,360
biology. There's a lot of other things

243
00:09:35,360 --> 00:09:39,040
that are also uh technologies that

244
00:09:37,360 --> 00:09:41,360
enable this that that are on the machine

245
00:09:39,040 --> 00:09:44,000
learning side, but active learning to

246
00:09:41,360 --> 00:09:45,600
power this is key. So when we're doing

247
00:09:44,000 --> 00:09:47,120
this, when we're turning this cycle, we

248
00:09:45,600 --> 00:09:49,360
might be making libraries. We might be

249
00:09:47,120 --> 00:09:51,279
making plates. We might be doing things

250
00:09:49,360 --> 00:09:53,200
that actually have expensive tests that

251
00:09:51,279 --> 00:09:54,920
require animals. And there's very small

252
00:09:53,200 --> 00:09:57,040
number of tests like that that are

253
00:09:54,920 --> 00:09:59,680
allowable. But you should know how many

254
00:09:57,040 --> 00:10:01,680
tests you can make and you should know

255
00:09:59,680 --> 00:10:03,680
the, you know, the various elements of

256
00:10:01,680 --> 00:10:04,800
error in the model. Tracking those

257
00:10:03,680 --> 00:10:06,880
things, you could figure, you should

258
00:10:04,800 --> 00:10:08,640
figure out not just what to test, but

259
00:10:06,880 --> 00:10:10,000
how to diversify what you test. And then

260
00:10:08,640 --> 00:10:11,760
the key thing that people don't really

261
00:10:10,000 --> 00:10:13,279
think about that often is the bet size.

262
00:10:11,760 --> 00:10:16,160
how many things you should you test

263
00:10:13,279 --> 00:10:19,200
given the time delays, the cost of a

264
00:10:16,160 --> 00:10:21,600
time delay and the overall cost of each

265
00:10:19,200 --> 00:10:23,800
step here. And so that's something that

266
00:10:21,600 --> 00:10:26,320
that can be done uh

267
00:10:23,800 --> 00:10:28,079
today. Um obviously we need deep

268
00:10:26,320 --> 00:10:29,839
predictive power for how we actually

269
00:10:28,079 --> 00:10:31,680
generate these these antibodies and

270
00:10:29,839 --> 00:10:34,320
we'll talk about that today. But I think

271
00:10:31,680 --> 00:10:37,200
the actual generative AI that produces

272
00:10:34,320 --> 00:10:39,040
denovo hits for proteins, we've all seen

273
00:10:37,200 --> 00:10:40,640
that whether it's through media

274
00:10:39,040 --> 00:10:42,079
narrative, whether it's through, you

275
00:10:40,640 --> 00:10:45,120
know, reading the minutes of the Nobel

276
00:10:42,079 --> 00:10:47,279
Prize or whether it's uh through um

277
00:10:45,120 --> 00:10:49,120
actually following that field closely. I

278
00:10:47,279 --> 00:10:51,600
think this is something that uh we have

279
00:10:49,120 --> 00:10:53,600
a lot to contribute at Preient, but um

280
00:10:51,600 --> 00:10:55,200
this is part of part of the story that I

281
00:10:53,600 --> 00:10:56,800
think is told quite often. And so I'm

282
00:10:55,200 --> 00:11:00,000
going to talk less about that today and

283
00:10:56,800 --> 00:11:01,200
more about about this. Um and then the

284
00:11:00,000 --> 00:11:03,040
last thing these are sort of more

285
00:11:01,200 --> 00:11:06,079
controversial things things that where I

286
00:11:03,040 --> 00:11:08,720
think the the story is still developing

287
00:11:06,079 --> 00:11:11,200
but um at Genitech we really believe

288
00:11:08,720 --> 00:11:13,440
that cross modality thinking is key and

289
00:11:11,200 --> 00:11:16,959
so whenever we can actually have a unit

290
00:11:13,440 --> 00:11:18,959
or team work on a simulation for small

291
00:11:16,959 --> 00:11:21,120
large molecule and peptides cell

292
00:11:18,959 --> 00:11:23,360
therapies we should do that as opposed

293
00:11:21,120 --> 00:11:26,000
to have them really drill down into the

294
00:11:23,360 --> 00:11:28,399
specifics of the screens or the modality

295
00:11:26,000 --> 00:11:30,000
that they're they're thinking about. Um,

296
00:11:28,399 --> 00:11:32,160
this is something that that I'll show a

297
00:11:30,000 --> 00:11:34,160
few examples of. And then the last thing

298
00:11:32,160 --> 00:11:36,399
I'll end the talk with just a little bit

299
00:11:34,160 --> 00:11:39,440
of thoughts about how agents can be used

300
00:11:36,399 --> 00:11:42,279
to orchestrate workflows and perhaps

301
00:11:39,440 --> 00:11:44,480
allow some of this to run

302
00:11:42,279 --> 00:11:46,720
autonomously. Um, but before we dive

303
00:11:44,480 --> 00:11:49,920
into that, um, I just want to highlight

304
00:11:46,720 --> 00:11:52,959
this key problem or this key formulation

305
00:11:49,920 --> 00:11:54,480
of the active learning problem. Um,

306
00:11:52,959 --> 00:11:55,760
there's lots of different ways to start

307
00:11:54,480 --> 00:11:57,680
thinking about active learning, but one

308
00:11:55,760 --> 00:12:00,240
of the ways to think of it is that with

309
00:11:57,680 --> 00:12:02,399
generative AI that's efficient at

310
00:12:00,240 --> 00:12:04,399
inference time. So, you amarize your

311
00:12:02,399 --> 00:12:06,079
cost of of using and building this

312
00:12:04,399 --> 00:12:08,320
model. You spend a huge amount of time

313
00:12:06,079 --> 00:12:09,760
to train the model. And then after that,

314
00:12:08,320 --> 00:12:12,000
there are lots of different ways you can

315
00:12:09,760 --> 00:12:14,480
sample diverse sets of things from that

316
00:12:12,000 --> 00:12:17,040
generative AI. And so, you have a nearly

317
00:12:14,480 --> 00:12:20,000
limitless number of hypotheses about

318
00:12:17,040 --> 00:12:23,519
things that would work. um proteins,

319
00:12:20,000 --> 00:12:26,240
small molecules, um synthetic biology,

320
00:12:23,519 --> 00:12:28,880
cell biology, uh setups. You know, we

321
00:12:26,240 --> 00:12:30,880
are usually in cominatorial spaces where

322
00:12:28,880 --> 00:12:33,200
the things that actually work, function,

323
00:12:30,880 --> 00:12:35,760
that hit the TCP for a given project,

324
00:12:33,200 --> 00:12:37,480
they're probably in in the very large

325
00:12:35,760 --> 00:12:40,240
numbers of zeros

326
00:12:37,480 --> 00:12:42,560
regime. Once we have that that core

327
00:12:40,240 --> 00:12:44,320
generation, we then have to apply lots

328
00:12:42,560 --> 00:12:46,240
and lots of filters. Can it be

329
00:12:44,320 --> 00:12:47,600
manufactured? Does it will it stay in

330
00:12:46,240 --> 00:12:49,600
the blood for a long time? Will it

331
00:12:47,600 --> 00:12:51,600
aggravate the immune system? That

332
00:12:49,600 --> 00:12:54,160
usually narrows things down, but we

333
00:12:51,600 --> 00:12:56,160
still have error in the inference of

334
00:12:54,160 --> 00:12:58,000
these multiple properties and we're

335
00:12:56,160 --> 00:12:59,920
still starting on a, you know,

336
00:12:58,000 --> 00:13:02,959
practically from a budget perspective,

337
00:12:59,920 --> 00:13:05,240
infinite field. How do we then select

338
00:13:02,959 --> 00:13:08,480
the subset that we want to submit for

339
00:13:05,240 --> 00:13:10,600
testing? And you know, this is the core

340
00:13:08,480 --> 00:13:13,200
concept that we often use when we think

341
00:13:10,600 --> 00:13:15,600
about what are we going to do next in an

342
00:13:13,200 --> 00:13:18,880
experimental design setting.

343
00:13:15,600 --> 00:13:22,639
um is this idea of Pareto frontiers and

344
00:13:18,880 --> 00:13:25,200
here I'm showing a a picture where we

345
00:13:22,639 --> 00:13:27,519
have uh for example two properties one

346
00:13:25,200 --> 00:13:30,000
would be yield and one would be affinity

347
00:13:27,519 --> 00:13:31,680
and in this case these are real designs

348
00:13:30,000 --> 00:13:33,839
for an antibody that we care about this

349
00:13:31,680 --> 00:13:36,000
is I think you know interlucan 6 which

350
00:13:33,839 --> 00:13:37,600
is something that we're uh it's a proof

351
00:13:36,000 --> 00:13:39,680
for release we can talk about it I'll

352
00:13:37,600 --> 00:13:41,600
show you some data for that one and so

353
00:13:39,680 --> 00:13:44,480
the things that we should test should do

354
00:13:41,600 --> 00:13:45,440
something novel or you know further from

355
00:13:44,480 --> 00:13:46,720
the origin

356
00:13:45,440 --> 00:13:48,160
with respect to both of these

357
00:13:46,720 --> 00:13:49,440
properties. And ideally, we should test

358
00:13:48,160 --> 00:13:52,959
the things that are at the frontier

359
00:13:49,440 --> 00:13:54,560
here. And this makes sense and this can

360
00:13:52,959 --> 00:13:56,560
be difficult to compute when you've got

361
00:13:54,560 --> 00:13:58,720
lots of properties. But it's it's an

362
00:13:56,560 --> 00:14:00,000
under control problem. And this is

363
00:13:58,720 --> 00:14:01,920
slightly different from some of the

364
00:14:00,000 --> 00:14:04,560
other key um principles behind

365
00:14:01,920 --> 00:14:07,040
experimental design like deoptimality or

366
00:14:04,560 --> 00:14:08,880
block random. If you read a lot of early

367
00:14:07,040 --> 00:14:10,560
work in regression that might even be,

368
00:14:08,880 --> 00:14:12,560
you know, might be so old it's difficult

369
00:14:10,560 --> 00:14:14,880
to find. There's a lot of discussion

370
00:14:12,560 --> 00:14:18,399
about experimental design from models

371
00:14:14,880 --> 00:14:20,399
but a lot of it assumes uh you know iid

372
00:14:18,399 --> 00:14:22,639
sort of independence of variables and

373
00:14:20,399 --> 00:14:24,399
then a lot of it is also um you know

374
00:14:22,639 --> 00:14:26,120
furiously difficult to compute when you

375
00:14:24,399 --> 00:14:28,800
have a lot of data and a lot of

376
00:14:26,120 --> 00:14:30,639
dimensions. But another you know really

377
00:14:28,800 --> 00:14:32,800
really really key thing that I think

378
00:14:30,639 --> 00:14:35,440
keeps us from using that that ancient

379
00:14:32,800 --> 00:14:38,240
wisdom is just that everything that we

380
00:14:35,440 --> 00:14:40,480
do in in living systems is really really

381
00:14:38,240 --> 00:14:42,639
strongly dependent on each other. not

382
00:14:40,480 --> 00:14:45,680
just in terms of the variables having

383
00:14:42,639 --> 00:14:47,279
some correlation or some uh block block

384
00:14:45,680 --> 00:14:49,519
diagonal structure or something that

385
00:14:47,279 --> 00:14:51,680
breaks the math. The bigger problem is

386
00:14:49,519 --> 00:14:53,360
just that they're dependent in a sort of

387
00:14:51,680 --> 00:14:55,680
a life or death fashion. And so for

388
00:14:53,360 --> 00:14:57,519
example, if we have an antibbody and I

389
00:14:55,680 --> 00:14:59,120
am very excited about all sorts of

390
00:14:57,519 --> 00:15:02,320
properties on its surface, but it

391
00:14:59,120 --> 00:15:03,920
doesn't fold or express. And I don't

392
00:15:02,320 --> 00:15:05,519
even know why it doesn't express. It

393
00:15:03,920 --> 00:15:07,440
just doesn't express. I don't get any

394
00:15:05,519 --> 00:15:09,560
yield. without being able to make the

395
00:15:07,440 --> 00:15:12,480
thing, you certainly can't test anything

396
00:15:09,560 --> 00:15:13,880
else. And then if it doesn't bind the

397
00:15:12,480 --> 00:15:17,040
thing it's supposed

398
00:15:13,880 --> 00:15:20,000
to, Yan Woo, who's a VP in charge of

399
00:15:17,040 --> 00:15:22,079
antibbody engineering at Genitech, is

400
00:15:20,000 --> 00:15:23,920
she's very devoted to thinking about lab

401
00:15:22,079 --> 00:15:25,040
and loop and computation, but she is not

402
00:15:23,920 --> 00:15:26,399
going to be able to convince her

403
00:15:25,040 --> 00:15:28,320
scientists to do something with an

404
00:15:26,399 --> 00:15:30,560
antibbody that doesn't bind because from

405
00:15:28,320 --> 00:15:33,040
her perspective, it's dead, even if

406
00:15:30,560 --> 00:15:34,959
there might be imunological or surface

407
00:15:33,040 --> 00:15:37,600
properties that we want to test next.

408
00:15:34,959 --> 00:15:40,639
And so this is this little tree, but

409
00:15:37,600 --> 00:15:42,800
this tree is like a bush and the bush is

410
00:15:40,639 --> 00:15:44,160
from a mathematical perspective burning.

411
00:15:42,800 --> 00:15:45,600
It's a it's just a nightmare because

412
00:15:44,160 --> 00:15:48,160
basically we've got, you know, millions

413
00:15:45,600 --> 00:15:50,720
of things here, hundreds of things here,

414
00:15:48,160 --> 00:15:53,360
you know, off we go. And so if we can

415
00:15:50,720 --> 00:15:55,519
build Gen AI so that more bigger

416
00:15:53,360 --> 00:15:57,600
fraction of things are binders, well,

417
00:15:55,519 --> 00:16:00,240
that's great, but we never avoid this um

418
00:15:57,600 --> 00:16:02,000
key problem. And the the paper that I

419
00:16:00,240 --> 00:16:03,519
put this a little bit out of date here.

420
00:16:02,000 --> 00:16:04,959
I think the paper that I would recommend

421
00:16:03,519 --> 00:16:06,720
taking a peek at and then you know

422
00:16:04,959 --> 00:16:08,000
reverse citation search will give you a

423
00:16:06,720 --> 00:16:09,759
lot of other things that Natasha has

424
00:16:08,000 --> 00:16:12,880
done but this is Natasha Tavasov

425
00:16:09,759 --> 00:16:15,440
Sovska's uh entry into this space with

426
00:16:12,880 --> 00:16:17,199
her one of her good friends Giwan Park.

427
00:16:15,440 --> 00:16:18,959
uh the two of them figured out ways to

428
00:16:17,199 --> 00:16:20,959
use vine copulas and some other

429
00:16:18,959 --> 00:16:22,800
mathematical trips to take the

430
00:16:20,959 --> 00:16:26,240
dependence tree of properties they might

431
00:16:22,800 --> 00:16:30,480
measure then use that tree to figure out

432
00:16:26,240 --> 00:16:32,480
ways uh to act like this even though the

433
00:16:30,480 --> 00:16:34,240
reality is is interdependent like that.

434
00:16:32,480 --> 00:16:36,480
And so that property dag work and the

435
00:16:34,240 --> 00:16:37,639
paper bow tide are two things that

436
00:16:36,480 --> 00:16:41,519
essentially let

437
00:16:37,639 --> 00:16:43,120
us then get together with our 60 closest

438
00:16:41,519 --> 00:16:45,759
friends in antibbody engineering and

439
00:16:43,120 --> 00:16:47,639
actually try this um active learning uh

440
00:16:45,759 --> 00:16:50,320
cycle to design

441
00:16:47,639 --> 00:16:52,000
antibodies. And uh this is this is the

442
00:16:50,320 --> 00:16:54,720
the the recent paper. The first author

443
00:16:52,000 --> 00:16:56,720
is Nathan Frey. The last author uh is

444
00:16:54,720 --> 00:16:58,160
Vladimir Gregorovich who's one of our

445
00:16:56,720 --> 00:17:01,199
co-founders of Prussian along with

446
00:16:58,160 --> 00:17:03,519
myself and Kyongyang Cho. Um this this

447
00:17:01,199 --> 00:17:05,760
paper represents a whole lot of work. Um

448
00:17:03,519 --> 00:17:08,480
but basically for many different

449
00:17:05,760 --> 00:17:10,640
properties and for uh the six antibodies

450
00:17:08,480 --> 00:17:13,280
that we were allowed uh to talk about in

451
00:17:10,640 --> 00:17:15,520
this paper um because on the biology

452
00:17:13,280 --> 00:17:17,199
side or for some other reason

453
00:17:15,520 --> 00:17:19,280
partnership they were no longer uh

454
00:17:17,199 --> 00:17:23,280
frontline projects. We were allowed to

455
00:17:19,280 --> 00:17:26,480
talk about six uh uh antibbody design

456
00:17:23,280 --> 00:17:28,319
efforts here. Um, we are allowed to talk

457
00:17:26,480 --> 00:17:30,640
about most of the properties that were

458
00:17:28,319 --> 00:17:32,240
involved in this. Um, but this is

459
00:17:30,640 --> 00:17:34,880
essentially the the pipeline that we've

460
00:17:32,240 --> 00:17:37,679
applied to all large molecule drug

461
00:17:34,880 --> 00:17:39,600
discovery projects across the RO family.

462
00:17:37,679 --> 00:17:42,799
Um, but here we're showing the results

463
00:17:39,600 --> 00:17:44,799
for a handful of of projects where we

464
00:17:42,799 --> 00:17:47,120
had design rounds and I think this is

465
00:17:44,799 --> 00:17:50,640
again interlucan six. We had design

466
00:17:47,120 --> 00:17:52,559
round one, two, three, four. And if we

467
00:17:50,640 --> 00:17:54,080
believe this Pareto front or this active

468
00:17:52,559 --> 00:17:56,400
learning is working, we should see that

469
00:17:54,080 --> 00:18:00,160
our binding goes up and our expression

470
00:17:56,400 --> 00:18:03,280
goes up. And our binding here is in uh

471
00:18:00,160 --> 00:18:05,440
log of KD. And so uh as we go up here,

472
00:18:03,280 --> 00:18:08,120
we're getting to 10 the minus 10 KD.

473
00:18:05,440 --> 00:18:10,720
We're getting to um

474
00:18:08,120 --> 00:18:13,039
100,000 etc. fold improvement in these

475
00:18:10,720 --> 00:18:15,600
seeds. These plots at the top are the

476
00:18:13,039 --> 00:18:18,480
distributions of therapeutic antibodies

477
00:18:15,600 --> 00:18:21,200
for a handful of properties computed in

478
00:18:18,480 --> 00:18:24,480
Charlotte Dean's therapeutic antibbody

479
00:18:21,200 --> 00:18:25,840
profile or the tap program which is at

480
00:18:24,480 --> 00:18:27,559
this point pretty much an industry

481
00:18:25,840 --> 00:18:30,480
standard for filtering things before

482
00:18:27,559 --> 00:18:33,799
testing. Um and then we also have BVLISA

483
00:18:30,480 --> 00:18:36,559
here. This is back basil

484
00:18:33,799 --> 00:18:39,200
virus. This is yeah basillus virus

485
00:18:36,559 --> 00:18:40,960
eliza. So you basically rupture bacteria

486
00:18:39,200 --> 00:18:42,880
and see whether or not that mess

487
00:18:40,960 --> 00:18:46,000
interferes with a with the binding of

488
00:18:42,880 --> 00:18:47,799
your antibbody. It's a really um it's a

489
00:18:46,000 --> 00:18:49,919
proxy essentially for clearance in the

490
00:18:47,799 --> 00:18:51,679
bloodstream. And so we showed that we

491
00:18:49,919 --> 00:18:53,360
stabilize these properties, stabilize

492
00:18:51,679 --> 00:18:54,640
these properties and improve the two

493
00:18:53,360 --> 00:18:58,960
things that we tried to optimize

494
00:18:54,640 --> 00:19:01,280
against. And again um the key thing here

495
00:18:58,960 --> 00:19:04,240
is that over time over a small number of

496
00:19:01,280 --> 00:19:07,120
designs we improve binding by a factor

497
00:19:04,240 --> 00:19:08,679
of 100 for multiple antibodies and we

498
00:19:07,120 --> 00:19:10,799
improve and diversify the set of

499
00:19:08,679 --> 00:19:13,280
antibodies substantially for all of

500
00:19:10,799 --> 00:19:16,080
these different targets. Um and these

501
00:19:13,280 --> 00:19:18,720
are a handful of different uh families

502
00:19:16,080 --> 00:19:23,360
that include well behaved folds membrane

503
00:19:18,720 --> 00:19:25,360
proteins and and other things like that.

504
00:19:23,360 --> 00:19:27,520
And again, you know, I'm just I'm

505
00:19:25,360 --> 00:19:29,440
showing this one last time because uh

506
00:19:27,520 --> 00:19:30,720
active learning and and model driven

507
00:19:29,440 --> 00:19:32,720
experimental design is something I've

508
00:19:30,720 --> 00:19:35,360
cared about for my whole career, but

509
00:19:32,720 --> 00:19:37,360
it's not easy. It's really difficult.

510
00:19:35,360 --> 00:19:39,919
People graduate, people move to their

511
00:19:37,360 --> 00:19:42,160
next career phase. Grants start and

512
00:19:39,919 --> 00:19:44,240
stop. And when you do things in biology,

513
00:19:42,160 --> 00:19:45,919
it doesn't take minutes or weeks, it

514
00:19:44,240 --> 00:19:47,440
takes it takes years. And so, it's

515
00:19:45,919 --> 00:19:48,960
difficult to have multiple rounds and

516
00:19:47,440 --> 00:19:52,000
really see whether or not you're model

517
00:19:48,960 --> 00:19:53,679
driven or automatically model driven. uh

518
00:19:52,000 --> 00:19:55,520
uh design works and I don't think it

519
00:19:53,679 --> 00:19:57,440
works in every context but for molecular

520
00:19:55,520 --> 00:19:59,039
design it seems to be an ideal

521
00:19:57,440 --> 00:20:02,039
playground for experimenting with those

522
00:19:59,039 --> 00:20:02,039
ideas.

523
00:20:02,080 --> 00:20:08,080
Now I said that I wouldn't talk a lot

524
00:20:03,919 --> 00:20:09,679
about um uh automated uh denovo design

525
00:20:08,080 --> 00:20:12,640
of antibodies but it is something that

526
00:20:09,679 --> 00:20:15,360
we have um we have up and running and we

527
00:20:12,640 --> 00:20:18,000
have integrated with library design. And

528
00:20:15,360 --> 00:20:20,559
the key thing here is to think through

529
00:20:18,000 --> 00:20:22,400
what it means to have the ability to

530
00:20:20,559 --> 00:20:24,080
paint an epitope and design antibodies.

531
00:20:22,400 --> 00:20:27,280
And so the first thing that this helps

532
00:20:24,080 --> 00:20:30,039
us do is to actually optimize and

533
00:20:27,280 --> 00:20:33,600
diversify antibodies. So a lot of

534
00:20:30,039 --> 00:20:36,240
times when we start projects we almost

535
00:20:33,600 --> 00:20:39,039
always have a nanomol or binder to start

536
00:20:36,240 --> 00:20:41,200
with and of course we could we could

537
00:20:39,039 --> 00:20:43,840
compute these things computationally and

538
00:20:41,200 --> 00:20:46,240
then iterate but genitech is really good

539
00:20:43,840 --> 00:20:47,760
at making antibodies and has been for I

540
00:20:46,240 --> 00:20:49,919
don't know however many years two too

541
00:20:47,760 --> 00:20:52,400
many years to count greater than five

542
00:20:49,919 --> 00:20:55,440
right and one year is is infinity in the

543
00:20:52,400 --> 00:20:59,120
machine learning field right so um you

544
00:20:55,440 --> 00:21:01,760
could of course identify binders that

545
00:20:59,120 --> 00:21:04,000
bind different places. If you can use

546
00:21:01,760 --> 00:21:06,080
denovo, you can actually diversify not

547
00:21:04,000 --> 00:21:08,159
just the sequence, but you can actually

548
00:21:06,080 --> 00:21:10,640
rationally choose different regions

549
00:21:08,159 --> 00:21:12,720
around an epitope that you care about to

550
00:21:10,640 --> 00:21:14,880
try to come up with new functions. Um,

551
00:21:12,720 --> 00:21:18,080
the way that antibbody uh drugs and

552
00:21:14,880 --> 00:21:19,679
medicines work is often that you bind

553
00:21:18,080 --> 00:21:21,200
and you bring the complement pathway

554
00:21:19,679 --> 00:21:23,360
that's hanging out in the constant

555
00:21:21,200 --> 00:21:25,520
region and you induce cell killing. But

556
00:21:23,360 --> 00:21:27,200
that is one of 15 different ways that

557
00:21:25,520 --> 00:21:29,200
antibbody drugs work. They work through

558
00:21:27,200 --> 00:21:32,480
aidity bringing receptors together. They

559
00:21:29,200 --> 00:21:34,720
work uh targeting things. They work in

560
00:21:32,480 --> 00:21:37,280
in just you know really numerous number

561
00:21:34,720 --> 00:21:39,679
of of of mechanisms. Once you get to

562
00:21:37,280 --> 00:21:41,600
multipecific formats playing around with

563
00:21:39,679 --> 00:21:43,360
the epitope and figuring out what's the

564
00:21:41,600 --> 00:21:44,640
exact geometry that optimizes the

565
00:21:43,360 --> 00:21:46,080
structure function relationship

566
00:21:44,640 --> 00:21:48,559
important to the disease is super

567
00:21:46,080 --> 00:21:51,760
important.

568
00:21:48,559 --> 00:21:53,679
Um, I think I just said complex formats,

569
00:21:51,760 --> 00:21:56,240
but one of the things that we care quite

570
00:21:53,679 --> 00:21:57,840
a bit about are the ability to engineer

571
00:21:56,240 --> 00:21:59,440
or rationally engineer things where you

572
00:21:57,840 --> 00:22:01,760
might have a targeting domain, a killing

573
00:21:59,440 --> 00:22:05,440
domain, a T- cell recruiting domain, or

574
00:22:01,760 --> 00:22:08,000
a receptor engaging domain. And so, um,

575
00:22:05,440 --> 00:22:10,559
try and quattrapecific formats are

576
00:22:08,000 --> 00:22:12,159
becoming actually somewhat routine. When

577
00:22:10,559 --> 00:22:14,080
we hand those off to development

578
00:22:12,159 --> 00:22:16,559
sciences and product development, they

579
00:22:14,080 --> 00:22:19,440
almost always have liabilities. And so

580
00:22:16,559 --> 00:22:21,440
the need to be substantially more um

581
00:22:19,440 --> 00:22:22,760
active in designing complex formats is

582
00:22:21,440 --> 00:22:25,360
something that's

583
00:22:22,760 --> 00:22:29,440
important. The fact that this is fast I

584
00:22:25,360 --> 00:22:30,799
guess is a the least uh exciting thing

585
00:22:29,440 --> 00:22:32,000
intellectually but maybe the most

586
00:22:30,799 --> 00:22:34,720
exciting thing from the process

587
00:22:32,000 --> 00:22:36,720
optimization part. And then structure

588
00:22:34,720 --> 00:22:39,200
activity hypothesis. Here this is the

589
00:22:36,720 --> 00:22:42,000
idea again with binder identification.

590
00:22:39,200 --> 00:22:44,080
If we're rational about the initial

591
00:22:42,000 --> 00:22:47,720
design, then we also probably have a

592
00:22:44,080 --> 00:22:50,000
good model of of what what's

593
00:22:47,720 --> 00:22:52,080
happening. All right. So, that's another

594
00:22:50,000 --> 00:22:54,320
key thing. We need denovo hit finding

595
00:22:52,080 --> 00:22:56,480
not just to avoid screening and

596
00:22:54,320 --> 00:22:58,640
immunizing animals, but to be rational

597
00:22:56,480 --> 00:23:01,039
and to be uh model driven when we

598
00:22:58,640 --> 00:23:03,679
design. Another key thing is we have to

599
00:23:01,039 --> 00:23:07,520
figure out how to scale to lots and lots

600
00:23:03,679 --> 00:23:09,440
of different um projects. And so here

601
00:23:07,520 --> 00:23:11,679
this is um I'm switching gears to show

602
00:23:09,440 --> 00:23:14,000
you a slide from my small molecule

603
00:23:11,679 --> 00:23:16,559
group. And here we have this idea that

604
00:23:14,000 --> 00:23:19,600
we have a large large number of things

605
00:23:16,559 --> 00:23:21,520
um including um some things like potency

606
00:23:19,600 --> 00:23:23,360
modeling that we build inhouse active

607
00:23:21,520 --> 00:23:25,520
learning which is this sort of overall

608
00:23:23,360 --> 00:23:28,000
sample selection. We also have things

609
00:23:25,520 --> 00:23:30,159
that we uh use open access code or we

610
00:23:28,000 --> 00:23:31,600
license code to do. But we have a large

611
00:23:30,159 --> 00:23:34,880
number of technologies that we have to

612
00:23:31,600 --> 00:23:36,720
apply to a large number of projects.

613
00:23:34,880 --> 00:23:38,320
And the right way to put that into an

614
00:23:36,720 --> 00:23:39,919
augmented design framework is something

615
00:23:38,320 --> 00:23:41,360
that we think about a lot. And so I

616
00:23:39,919 --> 00:23:44,240
don't I won't go into that in detail

617
00:23:41,360 --> 00:23:47,600
today. But here we have issues of

618
00:23:44,240 --> 00:23:49,679
interface, issues of the active learning

619
00:23:47,600 --> 00:23:52,039
driving but still allowing the chemist

620
00:23:49,679 --> 00:23:54,320
to override uh and other things like

621
00:23:52,039 --> 00:23:57,120
that. And and this is something that's

622
00:23:54,320 --> 00:23:58,960
been really successful at Genitech and

623
00:23:57,120 --> 00:24:01,600
has led us to think about how far we can

624
00:23:58,960 --> 00:24:03,280
push it. So beyond interface, can we

625
00:24:01,600 --> 00:24:06,000
actually think about autonomous agents

626
00:24:03,280 --> 00:24:07,919
and can we think about models uh that

627
00:24:06,000 --> 00:24:10,400
have been trained in such a way that

628
00:24:07,919 --> 00:24:12,760
they know the location uh and they have

629
00:24:10,400 --> 00:24:17,360
good vector stores for all the data at

630
00:24:12,760 --> 00:24:19,520
RO. RO is 125 years old and and and that

631
00:24:17,360 --> 00:24:22,400
goes back to times where in Switzerland

632
00:24:19,520 --> 00:24:23,919
there were union laws, you know, that

633
00:24:22,400 --> 00:24:25,679
whatever your mother tongue was, you

634
00:24:23,919 --> 00:24:27,679
were allowed to put that in your lab

635
00:24:25,679 --> 00:24:30,159
notebook. And so the the lab notebooks

636
00:24:27,679 --> 00:24:31,919
in RO are in a great number of

637
00:24:30,159 --> 00:24:34,480
languages. They're handwritten. They're

638
00:24:31,919 --> 00:24:36,720
hundred years old. So I just just from

639
00:24:34,480 --> 00:24:38,320
like a kind of a liking machine learning

640
00:24:36,720 --> 00:24:40,559
perspective, it's challenging and weird

641
00:24:38,320 --> 00:24:42,000
enough that it could be fun. Um but we

642
00:24:40,559 --> 00:24:43,520
need to build our own models for a

643
00:24:42,000 --> 00:24:45,840
number of reasons. One is that we have

644
00:24:43,520 --> 00:24:49,120
our own special data. It's not just the

645
00:24:45,840 --> 00:24:53,320
internet and and a bunch of books. Um,

646
00:24:49,120 --> 00:24:56,240
we we also need to know that whatever we

647
00:24:53,320 --> 00:24:58,400
do, it can be compliant with regulatory

648
00:24:56,240 --> 00:24:59,919
processes. The EU just passed a whole

649
00:24:58,400 --> 00:25:02,080
bunch of laws about what can and can't

650
00:24:59,919 --> 00:25:04,240
be done with machine learning, and a lot

651
00:25:02,080 --> 00:25:06,640
of those are actually have specific

652
00:25:04,240 --> 00:25:08,799
clauses that target machine learning.

653
00:25:06,640 --> 00:25:10,720
Um, and then a lot of the things that we

654
00:25:08,799 --> 00:25:12,400
do involve handing things off to

655
00:25:10,720 --> 00:25:14,400
clinical trials. And so there has to be

656
00:25:12,400 --> 00:25:16,480
some ability to put error estimates and

657
00:25:14,400 --> 00:25:17,919
safety estimates on these things which I

658
00:25:16,480 --> 00:25:20,559
think you need to be able to at least

659
00:25:17,919 --> 00:25:22,799
see the parameters to do that. And so

660
00:25:20,559 --> 00:25:25,200
Kyouongyong Cho and Steven Raw who's

661
00:25:22,799 --> 00:25:27,919
formerly at Fizer put together a team to

662
00:25:25,200 --> 00:25:30,559
build our own large language models. Um

663
00:25:27,919 --> 00:25:32,559
this is something that's cross crossro

664
00:25:30,559 --> 00:25:36,240
crosspillar and is also being sponsored

665
00:25:32,559 --> 00:25:39,520
by Aviv and John Marion. Um but in the

666
00:25:36,240 --> 00:25:42,159
beginning we started off just saying for

667
00:25:39,520 --> 00:25:43,679
whatever reason we need uh to have

668
00:25:42,159 --> 00:25:46,400
models that are distilled to the

669
00:25:43,679 --> 00:25:48,000
smallest size possible that function in

670
00:25:46,400 --> 00:25:50,240
ways that commodity large language

671
00:25:48,000 --> 00:25:51,919
models work. And so we built large

672
00:25:50,240 --> 00:25:54,000
language models. We worked with large

673
00:25:51,919 --> 00:25:55,919
open access language models like the

674
00:25:54,000 --> 00:25:57,760
llama herd. And we found the sort of

675
00:25:55,919 --> 00:25:59,840
sweet spot for some of the tasks that

676
00:25:57,760 --> 00:26:01,840
that were required from commercial down

677
00:25:59,840 --> 00:26:03,600
to research.

678
00:26:01,840 --> 00:26:05,039
Um the thing that we're excited about

679
00:26:03,600 --> 00:26:07,760
now and the thing that I'll maybe take a

680
00:26:05,039 --> 00:26:10,799
beat to discuss a little bit is this uh

681
00:26:07,760 --> 00:26:12,320
this uh trend that I think is being

682
00:26:10,799 --> 00:26:14,159
adopted widely across different

683
00:26:12,320 --> 00:26:16,480
companies and different uh efforts to

684
00:26:14,159 --> 00:26:18,559
build these models. We want things that

685
00:26:16,480 --> 00:26:20,400
have deep expertise in vision and

686
00:26:18,559 --> 00:26:22,279
molecules. This is important for

687
00:26:20,400 --> 00:26:24,960
biology. This is important for molecular

688
00:26:22,279 --> 00:26:26,960
design. We want agents that are trained

689
00:26:24,960 --> 00:26:29,520
not just on the data but also have deep

690
00:26:26,960 --> 00:26:31,919
tie-ins here, programmatic tie-ins. And

691
00:26:29,520 --> 00:26:33,840
then lastly, we want uh models that were

692
00:26:31,919 --> 00:26:36,080
trained and enriched either on the

693
00:26:33,840 --> 00:26:38,559
architecture or the training subset side

694
00:26:36,080 --> 00:26:39,720
to be good at reasoning. And uh we put

695
00:26:38,559 --> 00:26:42,159
these things

696
00:26:39,720 --> 00:26:45,039
together. Um we're really focused on

697
00:26:42,159 --> 00:26:47,880
tool calling in the design space. And

698
00:26:45,039 --> 00:26:51,520
our example or deep dive uh for

699
00:26:47,880 --> 00:26:53,600
reasoning is uh safety. So can you look

700
00:26:51,520 --> 00:26:56,880
at the literature and our internal data

701
00:26:53,600 --> 00:26:59,679
and make hypotheses about compound and

702
00:26:56,880 --> 00:27:01,440
um uh safety? Um, and here there's two

703
00:26:59,679 --> 00:27:04,000
things. Some of it's about toxicology,

704
00:27:01,440 --> 00:27:05,919
which has deep and ancient literature.

705
00:27:04,000 --> 00:27:07,919
Um, some of it's about ontarget

706
00:27:05,919 --> 00:27:09,679
toxicity. So, if you do the thing you do

707
00:27:07,919 --> 00:27:11,200
and it seems to be safe in a mouse, will

708
00:27:09,679 --> 00:27:12,960
it also be safe in a human? And the

709
00:27:11,200 --> 00:27:15,600
answer there is about it's, you know,

710
00:27:12,960 --> 00:27:17,200
it's no a third of the time and it can

711
00:27:15,600 --> 00:27:20,559
be very surprising. And so, there's a

712
00:27:17,200 --> 00:27:22,480
lot of a lot of work there in safety. Um

713
00:27:20,559 --> 00:27:24,480
for multimodality

714
00:27:22,480 --> 00:27:27,200
uh we're we're really thinking a lot

715
00:27:24,480 --> 00:27:30,000
about the points where the green part

716
00:27:27,200 --> 00:27:31,679
molecule design touch the uh the the

717
00:27:30,000 --> 00:27:33,480
target discovery stuff that Sarah's

718
00:27:31,679 --> 00:27:36,320
groups are working

719
00:27:33,480 --> 00:27:38,240
on. Um we do have permission from the

720
00:27:36,320 --> 00:27:40,320
lawyers to release most of these models

721
00:27:38,240 --> 00:27:42,480
as open access models and so this won't

722
00:27:40,320 --> 00:27:45,919
be this won't be top secret for much

723
00:27:42,480 --> 00:27:47,760
longer. Um but uh we do have some models

724
00:27:45,919 --> 00:27:49,120
that are trained on everything at RO and

725
00:27:47,760 --> 00:27:51,760
other models that are trained with the

726
00:27:49,120 --> 00:27:53,919
intent to release and you can imagine

727
00:27:51,760 --> 00:27:55,360
that makes uh everyone nervous. So we're

728
00:27:53,919 --> 00:27:56,960
working through that and uh you know

729
00:27:55,360 --> 00:28:00,320
stay tuned if you're interested in in

730
00:27:56,960 --> 00:28:01,640
playing with these models. Um overall

731
00:28:00,320 --> 00:28:04,240
you know one of our

732
00:28:01,640 --> 00:28:06,240
key takeaways here and I'm showing you

733
00:28:04,240 --> 00:28:08,080
this this somewhat older model here.

734
00:28:06,240 --> 00:28:09,679
This is updated a little bit but this

735
00:28:08,080 --> 00:28:11,440
isn't our our frontline model at this

736
00:28:09,679 --> 00:28:14,080
point. But if we look at models where

737
00:28:11,440 --> 00:28:16,159
we've pushed really hard to distill them

738
00:28:14,080 --> 00:28:17,919
down, they're very very small. They take

739
00:28:16,159 --> 00:28:20,559
up, you know, hundth of the memory on

740
00:28:17,919 --> 00:28:23,039
the GPU that the larger public models

741
00:28:20,559 --> 00:28:26,799
take or the larger enterprise models

742
00:28:23,039 --> 00:28:29,360
take. We're still able to um in a safety

743
00:28:26,799 --> 00:28:32,080
context um build competitive models

744
00:28:29,360 --> 00:28:35,600
without a huge amount of fine-tuning. um

745
00:28:32,080 --> 00:28:38,080
in the context of um uh tool calling I

746
00:28:35,600 --> 00:28:41,120
think this is a case where um just the

747
00:28:38,080 --> 00:28:43,279
last year of work has shown huge amounts

748
00:28:41,120 --> 00:28:45,360
of progress. These numbers even a year

749
00:28:43,279 --> 00:28:49,279
ago for even the biggest enterprise

750
00:28:45,360 --> 00:28:51,279
models were you know below 50%. Um we we

751
00:28:49,279 --> 00:28:53,760
see that we're able to get up to, you

752
00:28:51,279 --> 00:28:56,559
know, various um various uh correct

753
00:28:53,760 --> 00:28:58,320
response for the next tool. Correct tool

754
00:28:56,559 --> 00:29:00,320
request. And again, the correct tool

755
00:28:58,320 --> 00:29:02,480
request is more about are the inputs and

756
00:29:00,320 --> 00:29:04,080
outputs compatible. This is about

757
00:29:02,480 --> 00:29:05,279
whether or not the user believes this.

758
00:29:04,080 --> 00:29:07,679
This is actually something where there's

759
00:29:05,279 --> 00:29:10,320
a like scale that the user uses to say

760
00:29:07,679 --> 00:29:11,919
does did this make sense? And then tool

761
00:29:10,320 --> 00:29:13,840
choice is again about not just

762
00:29:11,919 --> 00:29:15,520
compatibility, but more compatibility

763
00:29:13,840 --> 00:29:17,520
with the the objective for the next

764
00:29:15,520 --> 00:29:18,880
phase in the workflow. And so we we

765
00:29:17,520 --> 00:29:22,399
split these things up, but you can

766
00:29:18,880 --> 00:29:26,640
imagine uh the work to build these two

767
00:29:22,399 --> 00:29:29,039
plots was 70% metrics, 20% machine

768
00:29:26,640 --> 00:29:30,720
learning, and whatever's left, you know,

769
00:29:29,039 --> 00:29:32,559
messing around with our high performance

770
00:29:30,720 --> 00:29:35,039
computing just to get the the darn model

771
00:29:32,559 --> 00:29:37,120
to train in the first place. Um, when we

772
00:29:35,039 --> 00:29:40,760
put this all together, we do have this

773
00:29:37,120 --> 00:29:42,799
sort of user workflow tool interface uh

774
00:29:40,760 --> 00:29:44,480
combinations and we've got something

775
00:29:42,799 --> 00:29:46,679
that's actually getting uh fairly

776
00:29:44,480 --> 00:29:48,720
fullfeatured for small and large

777
00:29:46,679 --> 00:29:50,559
molecules and it's something that we're

778
00:29:48,720 --> 00:29:52,720
we're very excited about. And so this is

779
00:29:50,559 --> 00:29:55,440
a a slide that's far from comprehensive,

780
00:29:52,720 --> 00:29:57,799
but it's showing a query and some ideas

781
00:29:55,440 --> 00:30:00,720
for what the model has sort of pre

782
00:29:57,799 --> 00:30:03,440
prepackaged. Moving through that, we get

783
00:30:00,720 --> 00:30:07,120
questions about antibodies. We light up

784
00:30:03,440 --> 00:30:08,799
a few tools. We can use rag to see what

785
00:30:07,120 --> 00:30:10,720
was used for that inference and what

786
00:30:08,799 --> 00:30:13,120
what we might also look at. We can rank

787
00:30:10,720 --> 00:30:16,720
models and rank outputs, model outputs

788
00:30:13,120 --> 00:30:19,679
to test things. And we can queue up uh

789
00:30:16,720 --> 00:30:21,279
uh automation or experiments. Um and

790
00:30:19,679 --> 00:30:23,200
then augmented design is something that

791
00:30:21,279 --> 00:30:25,039
we love. It looks cool. It's fun to

792
00:30:23,200 --> 00:30:26,399
make. You get these cool interfaces and

793
00:30:25,039 --> 00:30:28,320
then the chemists and the biologists

794
00:30:26,399 --> 00:30:30,159
don't really want it, but we're making

795
00:30:28,320 --> 00:30:31,760
it anyways because, you know, we're

796
00:30:30,159 --> 00:30:32,840
people, too. We want to have some fun

797
00:30:31,760 --> 00:30:36,720
there.

798
00:30:32,840 --> 00:30:39,200
Um, I think when it comes to these these

799
00:30:36,720 --> 00:30:41,360
ideas of how we put all this together, I

800
00:30:39,200 --> 00:30:43,120
feel really I wouldn't say comfortable,

801
00:30:41,360 --> 00:30:45,120
but I feel like I know what to try next

802
00:30:43,120 --> 00:30:47,279
on the active learning side. I feel like

803
00:30:45,120 --> 00:30:49,840
when it comes to agents, there's as many

804
00:30:47,279 --> 00:30:52,080
open questions as there are um next

805
00:30:49,840 --> 00:30:54,640
steps. Uh, I think there's a lot of need

806
00:30:52,080 --> 00:30:56,720
to continue sharing our data layers and

807
00:30:54,640 --> 00:30:58,399
how we register models and encapsulate

808
00:30:56,720 --> 00:30:59,960
tools. There's a lot of work that still

809
00:30:58,399 --> 00:31:02,159
needs to be done there or a lot of

810
00:30:59,960 --> 00:31:05,200
discussion. And then you know the key

811
00:31:02,159 --> 00:31:08,320
issue for us is you know if you're

812
00:31:05,200 --> 00:31:10,000
making a medicine people have workflows

813
00:31:08,320 --> 00:31:11,919
but those workflows aren't people who

814
00:31:10,000 --> 00:31:14,080
can make a medicine in a year. It takes

815
00:31:11,919 --> 00:31:16,159
too many years and too many people. And

816
00:31:14,080 --> 00:31:18,399
so do we embrace the long tale of all

817
00:31:16,159 --> 00:31:20,080
the different methods people use or do

818
00:31:18,399 --> 00:31:22,159
we start to use these efforts to

819
00:31:20,080 --> 00:31:24,799
orchestrate things to also kind of clean

820
00:31:22,159 --> 00:31:27,600
up things and ask people to use fewer or

821
00:31:24,799 --> 00:31:29,520
more standardized subset of tools.

822
00:31:27,600 --> 00:31:31,279
The answer to this might be that there's

823
00:31:29,520 --> 00:31:32,799
a popular answer which is embrace the

824
00:31:31,279 --> 00:31:34,960
long tail and there's a correct answer

825
00:31:32,799 --> 00:31:37,760
which is ask people to change what they

826
00:31:34,960 --> 00:31:40,960
do and we'll see where we get with that.

827
00:31:37,760 --> 00:31:45,320
And that's it. Um I I'll stay on this

828
00:31:40,960 --> 00:31:45,320
slide for for now and for questions.

829
00:31:49,600 --> 00:31:55,519
Thanks. Uh I think we have time for a

830
00:31:52,080 --> 00:31:55,519
couple of questions but not more.

831
00:31:57,399 --> 00:32:03,279
Hi. Hello.

832
00:32:00,320 --> 00:32:06,640
Okay. Here. Yes. Hello. Thank you. It's

833
00:32:03,279 --> 00:32:08,320
very nice talk. So, my question is uh

834
00:32:06,640 --> 00:32:11,440
you know you have this uh lab in the

835
00:32:08,320 --> 00:32:15,200
loop uh generate data and uh predict

836
00:32:11,440 --> 00:32:17,279
antibody and so on. So, uh what's a

837
00:32:15,200 --> 00:32:19,840
relationship between the scale of the

838
00:32:17,279 --> 00:32:22,640
data set and the performance of antibody

839
00:32:19,840 --> 00:32:24,840
design? So how much data you need

840
00:32:22,640 --> 00:32:27,600
basically? Yeah.

841
00:32:24,840 --> 00:32:29,960
So there there's there are a lot of

842
00:32:27,600 --> 00:32:32,600
extant antibodies and a lot of protein

843
00:32:29,960 --> 00:32:36,080
sequences. Um and so

844
00:32:32,600 --> 00:32:37,840
for the generative core like the

845
00:32:36,080 --> 00:32:40,000
language model that suggests mutations

846
00:32:37,840 --> 00:32:41,840
or is let's say the proposal

847
00:32:40,000 --> 00:32:44,559
distribution generator the underlying

848
00:32:41,840 --> 00:32:46,399
thing. I think right now we've seen that

849
00:32:44,559 --> 00:32:49,200
increasing the scale of those models,

850
00:32:46,399 --> 00:32:51,760
feeding them new protein sequences is

851
00:32:49,200 --> 00:32:53,919
less important than how you balance, you

852
00:32:51,760 --> 00:32:55,760
know, using ideas about evolution or the

853
00:32:53,919 --> 00:32:58,399
tree of life, how you balance what goes

854
00:32:55,760 --> 00:33:00,080
in there. Um, when it comes to affinity

855
00:32:58,399 --> 00:33:03,120
and some of these properties, there are

856
00:33:00,080 --> 00:33:05,840
still things like clearance in the

857
00:33:03,120 --> 00:33:08,080
animal where we need proxy assays and we

858
00:33:05,840 --> 00:33:09,640
need more data. and then everything in

859
00:33:08,080 --> 00:33:11,919
between

860
00:33:09,640 --> 00:33:14,159
affinity expression we're in a state

861
00:33:11,919 --> 00:33:15,840
where our models are very predictive we

862
00:33:14,159 --> 00:33:18,399
think things work but I think for the

863
00:33:15,840 --> 00:33:20,000
core unsupervised generative models were

864
00:33:18,399 --> 00:33:22,320
there and for a lot of these other

865
00:33:20,000 --> 00:33:24,720
properties were actually working hard to

866
00:33:22,320 --> 00:33:27,360
collect new data and so that's just the

867
00:33:24,720 --> 00:33:29,360
the quantity and we we see that we could

868
00:33:27,360 --> 00:33:31,519
use a lot of additional data for many of

869
00:33:29,360 --> 00:33:33,679
the properties we care about um not the

870
00:33:31,519 --> 00:33:37,120
core ones not binding not stability not

871
00:33:33,679 --> 00:33:39,440
affinity um now the the the Subp part of

872
00:33:37,120 --> 00:33:41,919
this question is how do you go about for

873
00:33:39,440 --> 00:33:44,159
things like immunogenicity or clearance

874
00:33:41,919 --> 00:33:45,600
or high concentration behavior. How do

875
00:33:44,159 --> 00:33:47,640
you go about collecting new data and

876
00:33:45,600 --> 00:33:50,480
there we have seen that the active

877
00:33:47,640 --> 00:33:52,640
learning motivated uh collection data

878
00:33:50,480 --> 00:33:55,760
collection is vastly more valuable than

879
00:33:52,640 --> 00:33:57,360
random or historical data and and so the

880
00:33:55,760 --> 00:33:59,519
the data we've collected over the last

881
00:33:57,360 --> 00:34:02,240
two years is as valuable as the data

882
00:33:59,519 --> 00:34:04,720
from the last 10 years for example. Also

883
00:34:02,240 --> 00:34:07,279
a short question for I noticed there's

884
00:34:04,720 --> 00:34:09,440
an anti-correlation between the yield

885
00:34:07,279 --> 00:34:13,639
and the affinity for the antibody

886
00:34:09,440 --> 00:34:13,639
design, right? There's anti-correlation.

887
00:34:14,079 --> 00:34:20,560
I didn't notice that. But no, the higher

888
00:34:16,480 --> 00:34:22,560
affinity has a lower yield.

889
00:34:20,560 --> 00:34:24,879
Oh, you're talking about that PTO front

890
00:34:22,560 --> 00:34:27,119
picture that I show. Yeah, that I think

891
00:34:24,879 --> 00:34:28,720
that's one of those pictures where, you

892
00:34:27,119 --> 00:34:30,879
know, my thesis advisor should have told

893
00:34:28,720 --> 00:34:32,399
me that there were too many P, you know,

894
00:34:30,879 --> 00:34:34,320
points on that plot and that they were

895
00:34:32,399 --> 00:34:37,280
like landing on each other. Generally

896
00:34:34,320 --> 00:34:39,200
speaking, um,

897
00:34:37,280 --> 00:34:41,280
we could make things a lot more stable

898
00:34:39,200 --> 00:34:43,399
and we would So, you're you're right in

899
00:34:41,280 --> 00:34:45,839
one regard. There is that that

900
00:34:43,399 --> 00:34:47,679
correlation. The most stable and the

901
00:34:45,839 --> 00:34:49,520
best expressing proteins are very

902
00:34:47,679 --> 00:34:52,240
unlikely to be the best binders. in the

903
00:34:49,520 --> 00:34:54,240
best binders could involve all sorts of

904
00:34:52,240 --> 00:34:56,000
hydrophobic and large loops that would

905
00:34:54,240 --> 00:34:57,800
then be problems for those other things.

906
00:34:56,000 --> 00:34:59,839
And so I think I think that

907
00:34:57,800 --> 00:35:02,240
anti-correlation for the extremes of

908
00:34:59,839 --> 00:35:04,640
those properties is probably, you know,

909
00:35:02,240 --> 00:35:06,240
probably something we'd observe. And now

910
00:35:04,640 --> 00:35:08,560
I don't know also if that's also a

911
00:35:06,240 --> 00:35:10,400
trivial geometric result of how you draw

912
00:35:08,560 --> 00:35:12,000
Pareto fronts. So we we should think

913
00:35:10,400 --> 00:35:14,640
about that a little bit too. Yeah,

914
00:35:12,000 --> 00:35:17,200
that's a good question.

915
00:35:14,640 --> 00:35:20,560
Okay, let's thank the speaker again for

916
00:35:17,200 --> 00:35:20,560
the amazing talk.

