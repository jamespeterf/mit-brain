1
00:00:00,320 --> 00:00:05,520
The AI and rationality course that we're

2
00:00:01,964 --> 00:00:07,919
[music] teaching uh examines really the

3
00:00:05,520 --> 00:00:10,559
the the value of thinking about

4
00:00:07,919 --> 00:00:12,559
machines, complicated computer systems

5
00:00:10,559 --> 00:00:14,000
as if they were rational. That is the

6
00:00:12,559 --> 00:00:16,375
way we think about humans. And so we

7
00:00:14,000 --> 00:00:18,000
think about humans, each other, as if we

8
00:00:16,375 --> 00:00:20,000
[music] have beliefs about the world,

9
00:00:18,000 --> 00:00:22,160
about as if we have desires what we

10
00:00:20,000 --> 00:00:25,199
would like to have be true. [music]

11
00:00:22,160 --> 00:00:26,855
And we think of a rational human as

12
00:00:25,199 --> 00:00:29,599
taking actions that will achieve their

13
00:00:26,855 --> 00:00:32,079
[music] goals. And so uh in the class we

14
00:00:29,599 --> 00:00:35,520
think about how can that be a helpful

15
00:00:32,079 --> 00:00:36,680
view for building AI systems. Um and we

16
00:00:35,520 --> 00:00:39,040
can think of it as a helpful view

17
00:00:36,680 --> 00:00:40,719
[music] for talking about AI systems. We

18
00:00:39,040 --> 00:00:42,320
can also think about it potentially as

19
00:00:40,719 --> 00:00:43,600
inspiring an engineering approach to

20
00:00:42,320 --> 00:00:45,120
building such systems.

21
00:00:43,600 --> 00:00:46,320
>> I think there's two two places where

22
00:00:45,120 --> 00:00:48,640
philosophy really interacts with

23
00:00:46,320 --> 00:00:51,200
computing. One is about the nature of

24
00:00:48,640 --> 00:00:53,280
rational agency and what it is to have

25
00:00:51,200 --> 00:00:55,520
beliefs and desires and to be an

26
00:00:53,280 --> 00:00:58,719
autonomous intelligent agent and have a

27
00:00:55,520 --> 00:01:00,879
mind. Uh the other is ethics where we're

28
00:00:58,719 --> 00:01:02,640
concerned about the social impacts of

29
00:01:00,879 --> 00:01:04,400
these technologies that are changing our

30
00:01:02,640 --> 00:01:06,159
world really quickly and we want to

31
00:01:04,400 --> 00:01:08,720
think through how should we design

32
00:01:06,159 --> 00:01:10,640
systems to achieve good outcomes and

33
00:01:08,720 --> 00:01:13,040
avoid harms and risks.

34
00:01:10,640 --> 00:01:14,640
>> And I think it's important when I work

35
00:01:13,040 --> 00:01:18,640
with students who study machine learning

36
00:01:14,640 --> 00:01:20,320
or robotics that they step back a bit

37
00:01:18,640 --> 00:01:22,400
and examine the assumptions that they're

38
00:01:20,320 --> 00:01:24,640
making. So often a technical field will

39
00:01:22,400 --> 00:01:26,159
just make a bunch of assumptions and not

40
00:01:24,640 --> 00:01:28,479
even state them and everyone operates

41
00:01:26,159 --> 00:01:30,880
under assuming that they're true. But in

42
00:01:28,479 --> 00:01:33,040
many cases they're not so warranted. And

43
00:01:30,880 --> 00:01:34,960
so thinking about things from a

44
00:01:33,040 --> 00:01:36,960
philosophical perspective helps people

45
00:01:34,960 --> 00:01:38,560
back up and understand better how to

46
00:01:36,960 --> 00:01:39,119
situate their work in the actual

47
00:01:38,560 --> 00:01:40,799
context.

48
00:01:39,119 --> 00:01:42,960
>> I think I'm probably an unusual student

49
00:01:40,799 --> 00:01:45,360
in this course because I actually don't

50
00:01:42,960 --> 00:01:47,680
do AI or philosophy. [music]

51
00:01:45,360 --> 00:01:49,200
Um actually I do cognitive science. So

52
00:01:47,680 --> 00:01:51,680
my main interests are in human

53
00:01:49,200 --> 00:01:54,159
intelligence. [music] Um but a dominant

54
00:01:51,680 --> 00:01:56,000
paradigm in my field is to what extent

55
00:01:54,159 --> 00:01:57,840
are humans rational [music] and I think

56
00:01:56,000 --> 00:01:59,119
to really understand how to engage in

57
00:01:57,840 --> 00:02:00,880
that question you sort of need to

58
00:01:59,119 --> 00:02:03,200
understand what it means to be rational

59
00:02:00,880 --> 00:02:04,880
and I think philosophy [music] and AI um

60
00:02:03,200 --> 00:02:06,560
have ideas about the fundamentals of

61
00:02:04,880 --> 00:02:08,479
rationality and machine learning

62
00:02:06,560 --> 00:02:10,319
algorithms are attempting to build

63
00:02:08,479 --> 00:02:12,160
intelligent agents to [music]

64
00:02:10,319 --> 00:02:14,276
um act rationally in the world. So I

65
00:02:12,160 --> 00:02:15,840
wanted to have that interdicciplinary

66
00:02:14,276 --> 00:02:17,840
[music] perspective to understand my own

67
00:02:15,840 --> 00:02:20,480
field. I think what surprised me the

68
00:02:17,840 --> 00:02:23,760
most about this course is [music] that

69
00:02:20,480 --> 00:02:25,411
uh the math and the logic off of which

70
00:02:23,760 --> 00:02:28,640
um all of this machine learning theory

71
00:02:25,411 --> 00:02:30,959
[music] is based off of can lead to

72
00:02:28,640 --> 00:02:33,760
machines taking actions that [music]

73
00:02:30,959 --> 00:02:35,891
seem irrational, right? We're kind of

74
00:02:33,760 --> 00:02:37,360
taught that math and logic are like this

75
00:02:35,891 --> 00:02:40,640
[music] kind of golden standard or

76
00:02:37,360 --> 00:02:42,400
truth, right? It must be um that if

77
00:02:40,640 --> 00:02:44,000
something is mathematically or logically

78
00:02:42,400 --> 00:02:46,239
consistent or correct, it must be

79
00:02:44,000 --> 00:02:48,640
rational. But this class showed us a

80
00:02:46,239 --> 00:02:51,120
variety of examples where [music] um at

81
00:02:48,640 --> 00:02:53,360
least humans don't behave uh in

82
00:02:51,120 --> 00:02:56,160
consistency with these mathematical and

83
00:02:53,360 --> 00:02:58,400
logical frameworks. So other uh choices

84
00:02:56,160 --> 00:03:00,319
of action seem more rational to us. And

85
00:02:58,400 --> 00:03:02,560
then we open this whole can of worms as

86
00:03:00,319 --> 00:03:04,800
to whether is it humans that are irr

87
00:03:02,560 --> 00:03:06,959
irrational? Is it the machine learning

88
00:03:04,800 --> 00:03:09,920
uh system that we designed irrational?

89
00:03:06,959 --> 00:03:11,440
Is it math and logic itself? um and the

90
00:03:09,920 --> 00:03:13,120
different implications of different

91
00:03:11,440 --> 00:03:15,200
definitions of rationality.

92
00:03:13,120 --> 00:03:17,200
>> When we're thinking about ethics and AI

93
00:03:15,200 --> 00:03:19,599
technologies, I think there it's hard to

94
00:03:17,200 --> 00:03:21,360
give a a sort of quick answer. And the

95
00:03:19,599 --> 00:03:24,000
part of the reason is that AI is

96
00:03:21,360 --> 00:03:26,879
extremely broad. It goes well beyond the

97
00:03:24,000 --> 00:03:28,800
sort of you know chat GPT paradigm of

98
00:03:26,879 --> 00:03:31,440
large language models with chat bots.

99
00:03:28,800 --> 00:03:33,760
It's it's in our cars. You use it when

100
00:03:31,440 --> 00:03:35,280
you use Google maps whenever we've got

101
00:03:33,760 --> 00:03:36,560
uh you know facial recognition

102
00:03:35,280 --> 00:03:38,400
technologies. And each of these

103
00:03:36,560 --> 00:03:40,959
technologies is going to raise very

104
00:03:38,400 --> 00:03:43,200
different issues. So if you're thinking

105
00:03:40,959 --> 00:03:44,799
about automated hiring tools to decide

106
00:03:43,200 --> 00:03:46,720
who to who to hire for a certain job,

107
00:03:44,799 --> 00:03:48,640
you might be concerned about bias

108
00:03:46,720 --> 00:03:50,080
against certain demographic groups. If

109
00:03:48,640 --> 00:03:52,000
you're thinking about facial recognition

110
00:03:50,080 --> 00:03:54,080
technology, you might be concerned about

111
00:03:52,000 --> 00:03:56,080
privacy and surveillance. If you're

112
00:03:54,080 --> 00:03:57,920
thinking about a chat GPT sort of

113
00:03:56,080 --> 00:04:01,280
technology, you might be concerned that

114
00:03:57,920 --> 00:04:03,439
it doesn't spew out racist content or

115
00:04:01,280 --> 00:04:05,200
advise people in how to commit crimes.

116
00:04:03,439 --> 00:04:07,840
So each of these different technologies

117
00:04:05,200 --> 00:04:09,599
is going to raise a a very different set

118
00:04:07,840 --> 00:04:12,799
of ethical considerations.

119
00:04:09,599 --> 00:04:14,879
>> Given that the rate of progress in AI

120
00:04:12,799 --> 00:04:16,720
and related technologies, it's

121
00:04:14,879 --> 00:04:18,239
impossible to teach students what

122
00:04:16,720 --> 00:04:20,880
they're going to need to know 5 years

123
00:04:18,239 --> 00:04:22,560
from now at the basic level. So what we

124
00:04:20,880 --> 00:04:23,967
need to do is give them the tools at a

125
00:04:22,560 --> 00:04:25,680
higher level, the habits of mind,

126
00:04:23,967 --> 00:04:27,600
[music] the ways of thinking that will

127
00:04:25,680 --> 00:04:31,828
help them approach the stuff that we

128
00:04:27,600 --> 00:04:33,848
can't really anticipate right now.

129
00:04:31,828 --> 00:04:33,848
>> [music]

