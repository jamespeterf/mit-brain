1
00:00:11,679 --> 00:00:18,880
All right. Well, let's continue with our

2
00:00:15,599 --> 00:00:21,279
discussion of linear regression

3
00:00:18,880 --> 00:00:25,840
modeling. And as we introduced last

4
00:00:21,279 --> 00:00:29,279
time, there are ways one can formalize

5
00:00:25,840 --> 00:00:32,000
fitting a model, which is to propose a

6
00:00:29,279 --> 00:00:34,800
model, specify criteria for judging

7
00:00:32,000 --> 00:00:36,320
different fits or estimators of model

8
00:00:34,800 --> 00:00:40,640
parameters,

9
00:00:36,320 --> 00:00:44,000
then find the best estimators, and then

10
00:00:40,640 --> 00:00:47,039
check our assumptions underlying the

11
00:00:44,000 --> 00:00:51,280
specification of the criterion. and

12
00:00:47,039 --> 00:00:54,320
possibly uh if necessary modify the

13
00:00:51,280 --> 00:00:56,879
model because the assumptions that we

14
00:00:54,320 --> 00:01:01,039
made aren't satisfied. So we either want

15
00:00:56,879 --> 00:01:04,000
to u add additional assumptions or

16
00:01:01,039 --> 00:01:05,519
consider transformations of the model

17
00:01:04,000 --> 00:01:11,840
perhaps.

18
00:01:05,519 --> 00:01:13,439
Um so um with ordinary le squares

19
00:01:11,840 --> 00:01:18,880
regression

20
00:01:13,439 --> 00:01:22,240
um we have this criterion for specifying

21
00:01:18,880 --> 00:01:27,320
regression parameters and we specify

22
00:01:22,240 --> 00:01:27,320
these in terms of a y vector

23
00:01:27,439 --> 00:01:30,439
and

24
00:01:30,560 --> 00:01:35,280
this has values of a dependent variable

25
00:01:33,280 --> 00:01:38,320
across cases in the data.

26
00:01:35,280 --> 00:01:43,920
an X matrix

27
00:01:38,320 --> 00:01:46,560
which will have n rows and p columns

28
00:01:43,920 --> 00:01:52,479
and we'll have sort of general elements

29
00:01:46,560 --> 00:01:57,439
x i j and then u our model will be that

30
00:01:52,479 --> 00:02:00,439
y is equal to x the matrix times a beta

31
00:01:57,439 --> 00:02:00,439
vector.

32
00:02:00,560 --> 00:02:06,479
So we can have a beta vector

33
00:02:03,600 --> 00:02:08,399
that has p components

34
00:02:06,479 --> 00:02:11,200
and

35
00:02:08,399 --> 00:02:13,120
an error vector

36
00:02:11,200 --> 00:02:15,599
which

37
00:02:13,120 --> 00:02:19,280
has n

38
00:02:15,599 --> 00:02:20,800
terms corresponding to the n cases.

39
00:02:19,280 --> 00:02:24,560
So

40
00:02:20,800 --> 00:02:28,560
in specifying the regression model we

41
00:02:24,560 --> 00:02:33,680
have that our n vector y is going to be

42
00:02:28,560 --> 00:02:38,879
a linear combination of the columns of x

43
00:02:33,680 --> 00:02:40,879
and there will be a an error vector uh

44
00:02:38,879 --> 00:02:45,920
characterizing the discrepancy from

45
00:02:40,879 --> 00:02:50,160
that. So um

46
00:02:45,920 --> 00:02:53,040
so how should we specify this beta

47
00:02:50,160 --> 00:02:56,040
vector if we have a least squares

48
00:02:53,040 --> 00:02:56,040
criterion?

49
00:02:57,040 --> 00:03:05,200
We can look at sum one to n of y i minus

50
00:03:02,800 --> 00:03:06,959
y i

51
00:03:05,200 --> 00:03:13,519
where

52
00:03:06,959 --> 00:03:18,720
say y hat is equal to x beta hat

53
00:03:13,519 --> 00:03:22,800
and this criterion uh

54
00:03:18,720 --> 00:03:26,400
is basically a sum of squares criterion.

55
00:03:22,800 --> 00:03:31,040
So it's like a sum of squared errors.

56
00:03:26,400 --> 00:03:34,640
Now minimizing this can be done

57
00:03:31,040 --> 00:03:36,400
just with simple calculus. We can plug

58
00:03:34,640 --> 00:03:41,519
in for

59
00:03:36,400 --> 00:03:44,879
uh the in the formula and

60
00:03:41,519 --> 00:03:46,640
have this sum of squares criterion which

61
00:03:44,879 --> 00:03:50,000
is simply

62
00:03:46,640 --> 00:03:55,360
y - x beta transpose

63
00:03:50,000 --> 00:03:57,120
y - x beta. So we have Q of beta is

64
00:03:55,360 --> 00:04:00,400
basically

65
00:03:57,120 --> 00:04:05,680
Y - X beta

66
00:04:00,400 --> 00:04:07,840
squared. So it's the distance of our Y

67
00:04:05,680 --> 00:04:12,000
our n vector

68
00:04:07,840 --> 00:04:14,959
from the

69
00:04:12,000 --> 00:04:17,840
prediction or fitted value. I guess

70
00:04:14,959 --> 00:04:20,239
maybe I'll put the beta hat here to make

71
00:04:17,840 --> 00:04:23,919
it uh explicit.

72
00:04:20,239 --> 00:04:27,680
And we basically want our fitted values

73
00:04:23,919 --> 00:04:32,240
to be as close as possible to the actual

74
00:04:27,680 --> 00:04:34,720
values. Now, because this criterion is a

75
00:04:32,240 --> 00:04:38,400
quadratic,

76
00:04:34,720 --> 00:04:41,680
okay, it's a quadratic in the regression

77
00:04:38,400 --> 00:04:46,960
coefficients. We can simply take

78
00:04:41,680 --> 00:04:49,759
derivatives of Q and solve for those

79
00:04:46,960 --> 00:04:54,080
derivatives equal to zero. So that's the

80
00:04:49,759 --> 00:04:57,759
first order conditions and um let's see

81
00:04:54,080 --> 00:04:59,759
how do we know whether the solution to

82
00:04:57,759 --> 00:05:02,960
this

83
00:04:59,759 --> 00:05:07,680
equation actually minimizes

84
00:05:02,960 --> 00:05:11,120
the sum of squared errors criterion.

85
00:05:07,680 --> 00:05:14,960
A first order condition is one where the

86
00:05:11,120 --> 00:05:18,320
derivative of the function is equal to

87
00:05:14,960 --> 00:05:22,280
zero. That could be at a minimum, but it

88
00:05:18,320 --> 00:05:22,280
also could be at a maximum.

89
00:05:25,199 --> 00:05:27,280
>> Yes.

90
00:05:25,840 --> 00:05:28,800
>> You also want to check for the second

91
00:05:27,280 --> 00:05:31,919
order

92
00:05:28,800 --> 00:05:34,639
and make sure that it's positive.

93
00:05:31,919 --> 00:05:38,720
>> Yes. Right. We want to take the second

94
00:05:34,639 --> 00:05:42,720
order derivative. And

95
00:05:38,720 --> 00:05:45,360
if it's positive then we have a I guess

96
00:05:42,720 --> 00:05:48,320
it's a concave

97
00:05:45,360 --> 00:05:51,280
or sorry a convex function and so the

98
00:05:48,320 --> 00:05:56,080
first derivative will solve that. Now

99
00:05:51,280 --> 00:05:57,600
this is the first derivative equation.

100
00:05:56,080 --> 00:05:59,759
Um

101
00:05:57,600 --> 00:06:05,840
if we were to

102
00:05:59,759 --> 00:06:09,800
uh look at dq of beta

103
00:06:05,840 --> 00:06:09,800
by d beta j

104
00:06:11,039 --> 00:06:21,520
let's see is that beta j right um if we

105
00:06:15,840 --> 00:06:24,080
were to look at the uh second order

106
00:06:21,520 --> 00:06:26,000
derivative

107
00:06:24,080 --> 00:06:29,600
by d beta

108
00:06:26,000 --> 00:06:32,560
bet theta transpose then

109
00:06:29,600 --> 00:06:34,479
analytically we'll basically get

110
00:06:32,560 --> 00:06:38,880
this

111
00:06:34,479 --> 00:06:41,919
term times minus x and so that will end

112
00:06:38,880 --> 00:06:45,039
up being a positive term in terms of

113
00:06:41,919 --> 00:06:46,720
products of columns of x and the full x

114
00:06:45,039 --> 00:06:51,520
matrix

115
00:06:46,720 --> 00:06:56,080
and uh so so with that we have this set

116
00:06:51,520 --> 00:06:59,759
of equations um now if you read up on

117
00:06:56,080 --> 00:07:03,440
regression theory, linear model theory.

118
00:06:59,759 --> 00:07:06,479
Um, this equation

119
00:07:03,440 --> 00:07:10,080
of the derivative of Q with respect to

120
00:07:06,479 --> 00:07:12,560
beta equaling zero is equivalent to this

121
00:07:10,080 --> 00:07:16,160
equation here,

122
00:07:12,560 --> 00:07:22,000
which is just obtained by eliminating

123
00:07:16,160 --> 00:07:26,160
the minus2 factor and solving for beta.

124
00:07:22,000 --> 00:07:27,759
And this is called the normal equations.

125
00:07:26,160 --> 00:07:30,800
Um

126
00:07:27,759 --> 00:07:35,599
what is familiar to some people is that

127
00:07:30,800 --> 00:07:39,440
in statistics theory the names of

128
00:07:35,599 --> 00:07:42,000
important terms is often not very

129
00:07:39,440 --> 00:07:44,479
sophisticated in terms of here we have

130
00:07:42,000 --> 00:07:46,160
what we call the normal equations.

131
00:07:44,479 --> 00:07:49,199
There's nothing really normal about it,

132
00:07:46,160 --> 00:07:52,639
but it's a way of specifying a set of

133
00:07:49,199 --> 00:07:54,160
equations that we need to solve. And if

134
00:07:52,639 --> 00:07:57,440
the

135
00:07:54,160 --> 00:08:02,160
xrpose x matrix

136
00:07:57,440 --> 00:08:04,240
is invertible, then we have a solution

137
00:08:02,160 --> 00:08:07,039
by multiplying both sides of this

138
00:08:04,240 --> 00:08:12,319
equation by the inverse.

139
00:08:07,039 --> 00:08:14,319
And in order for this process to work,

140
00:08:12,319 --> 00:08:16,560
we need to have that X must have full

141
00:08:14,319 --> 00:08:19,840
column rank.

142
00:08:16,560 --> 00:08:23,479
And um

143
00:08:19,840 --> 00:08:23,479
all right. So

144
00:08:24,000 --> 00:08:32,399
so here if we plug in that le squares

145
00:08:28,479 --> 00:08:36,800
estimate. So we have beta hat equal to

146
00:08:32,399 --> 00:08:41,519
xrpose x inverse xrpose y

147
00:08:36,800 --> 00:08:45,680
If we plug that into our

148
00:08:41,519 --> 00:08:47,519
formula for the fitted values,

149
00:08:45,680 --> 00:08:51,399
then

150
00:08:47,519 --> 00:08:51,399
this becomes

151
00:08:52,399 --> 00:08:57,600
this formula time y

152
00:08:55,760 --> 00:08:59,120
and I'll put square brackets around

153
00:08:57,600 --> 00:09:00,959
that.

154
00:08:59,120 --> 00:09:04,760
um

155
00:09:00,959 --> 00:09:04,760
and this is called

156
00:09:04,880 --> 00:09:11,440
the hat matrix by uh some statisticians.

157
00:09:09,040 --> 00:09:15,519
Now

158
00:09:11,440 --> 00:09:20,399
this matrix H is actually a very special

159
00:09:15,519 --> 00:09:22,480
matrix. Um it has special properties and

160
00:09:20,399 --> 00:09:24,480
does anyone

161
00:09:22,480 --> 00:09:26,880
sort of know what those special

162
00:09:24,480 --> 00:09:29,880
properties might be?

163
00:09:26,880 --> 00:09:29,880
Anybody

164
00:09:31,440 --> 00:09:36,320
in the back? No. Kai,

165
00:09:35,279 --> 00:09:38,720
>> uh,

166
00:09:36,320 --> 00:09:40,720
>> from memory, it's a it's a projection on

167
00:09:38,720 --> 00:09:45,360
this column space.

168
00:09:40,720 --> 00:09:48,160
>> Yes, it is exactly. So, H is a

169
00:09:45,360 --> 00:09:50,320
projection

170
00:09:48,160 --> 00:09:53,519
matrix.

171
00:09:50,320 --> 00:09:56,720
um and it's onto

172
00:09:53,519 --> 00:09:59,519
sort of the column space of the X

173
00:09:56,720 --> 00:10:03,839
matrix.

174
00:09:59,519 --> 00:10:08,320
Now what are properties of

175
00:10:03,839 --> 00:10:12,080
a projection matrix? If we multiply

176
00:10:08,320 --> 00:10:16,160
the projection matrix by itself,

177
00:10:12,080 --> 00:10:20,959
we actually get the projection matrix.

178
00:10:16,160 --> 00:10:25,440
And what's um relevant is that if we

179
00:10:20,959 --> 00:10:32,079
were to take sort of h yhat

180
00:10:25,440 --> 00:10:34,160
um which is then equal to h * h y.

181
00:10:32,079 --> 00:10:40,959
If this

182
00:10:34,160 --> 00:10:45,120
first term is in the column space of X

183
00:10:40,959 --> 00:10:47,040
as a projection, then projecting it onto

184
00:10:45,120 --> 00:10:52,959
that same column space will have no

185
00:10:47,040 --> 00:10:55,360
difference. So this is sort of logically

186
00:10:52,959 --> 00:10:58,079
uh what it should be. And if we multiply

187
00:10:55,360 --> 00:11:04,279
out the projection matrix times itself,

188
00:10:58,079 --> 00:11:04,279
we get this um this property.

189
00:11:05,040 --> 00:11:11,760
Okay. Well,

190
00:11:08,560 --> 00:11:13,680
importantly in modeling is to look at

191
00:11:11,760 --> 00:11:17,360
residuals

192
00:11:13,680 --> 00:11:22,880
um which are sort of model errors. And

193
00:11:17,360 --> 00:11:25,200
so if we have epsilon hat is equal to y

194
00:11:22,880 --> 00:11:29,519
- yhat.

195
00:11:25,200 --> 00:11:34,959
Okay, this is y -

196
00:11:29,519 --> 00:11:36,880
or y * the identity matrix minus h

197
00:11:34,959 --> 00:11:38,640
y

198
00:11:36,880 --> 00:11:42,640
and

199
00:11:38,640 --> 00:11:45,839
this um

200
00:11:42,640 --> 00:11:49,320
this matrix here multiplying y to give

201
00:11:45,839 --> 00:11:49,320
us our residuals.

202
00:11:52,240 --> 00:12:01,480
It is also a projection matrix.

203
00:11:55,920 --> 00:12:01,480
So this is a projection as well.

204
00:12:02,160 --> 00:12:12,880
And if we multiply I - H * I - H and

205
00:12:08,240 --> 00:12:18,560
expand it out, we get I

206
00:12:12,880 --> 00:12:21,279
2 - 2 H + H 2

207
00:12:18,560 --> 00:12:23,839
and that equ= I - H. So you know we can

208
00:12:21,279 --> 00:12:26,000
mathematically verify that it's a

209
00:12:23,839 --> 00:12:28,480
projection

210
00:12:26,000 --> 00:12:31,200
and

211
00:12:28,480 --> 00:12:36,160
a very important property of this

212
00:12:31,200 --> 00:12:37,839
residual vector is that it is

213
00:12:36,160 --> 00:12:42,320
orthogonal

214
00:12:37,839 --> 00:12:47,839
um to the fitted value. So in the normal

215
00:12:42,320 --> 00:12:50,399
equations we have this equation

216
00:12:47,839 --> 00:12:54,800
must equal zero.

217
00:12:50,399 --> 00:12:58,639
This term here is the residual vector.

218
00:12:54,800 --> 00:13:01,360
And if we multiply that n vector by the

219
00:12:58,639 --> 00:13:05,279
transpose of x and it's equal to zero

220
00:13:01,360 --> 00:13:10,000
then we must have that um the error

221
00:13:05,279 --> 00:13:14,440
vector epsilon hat is orthogonal to x.

222
00:13:10,000 --> 00:13:14,440
Um, now

223
00:13:16,000 --> 00:13:19,600
let's see. I'm not sure if it's on the

224
00:13:17,680 --> 00:13:21,839
next slide

225
00:13:19,600 --> 00:13:23,680
or not.

226
00:13:21,839 --> 00:13:27,440
Um,

227
00:13:23,680 --> 00:13:30,800
and something to keep in mind

228
00:13:27,440 --> 00:13:34,720
that's really quite useful when we use

229
00:13:30,800 --> 00:13:37,200
linear algebra with leaf squares is that

230
00:13:34,720 --> 00:13:40,560
we have y

231
00:13:37,200 --> 00:13:42,639
being in n-dimensional space.

232
00:13:40,560 --> 00:13:49,839
Um

233
00:13:42,639 --> 00:13:52,720
the uh we have an x matrix n by p um

234
00:13:49,839 --> 00:13:58,800
which corresponds to sort of a column

235
00:13:52,720 --> 00:14:01,279
space of x which is a subspace of r to

236
00:13:58,800 --> 00:14:03,279
the n

237
00:14:01,279 --> 00:14:07,440
and

238
00:14:03,279 --> 00:14:09,279
um let's see if we have

239
00:14:07,440 --> 00:14:13,160
yhat

240
00:14:09,279 --> 00:14:13,160
and epsilon hat

241
00:14:19,360 --> 00:14:27,519
these are orthogonal to each other. So

242
00:14:24,160 --> 00:14:30,240
if we think in sort of n dimensions of

243
00:14:27,519 --> 00:14:33,240
having a y vector

244
00:14:30,240 --> 00:14:33,240
and

245
00:14:33,279 --> 00:14:36,959
have

246
00:14:35,040 --> 00:14:40,720
a

247
00:14:36,959 --> 00:14:44,320
yhat n vector

248
00:14:40,720 --> 00:14:51,440
um then

249
00:14:44,320 --> 00:14:51,440
y minus y sorry y not yhat y minus yhat

250
00:14:54,800 --> 00:14:59,920
So if we take the vector from here to

251
00:14:58,000 --> 00:15:04,720
here

252
00:14:59,920 --> 00:15:08,639
that's y minus y hat.

253
00:15:04,720 --> 00:15:12,560
Um okay this is going to be orthogonal

254
00:15:08,639 --> 00:15:16,000
to yhat. So we actually have

255
00:15:12,560 --> 00:15:20,880
right angles here between the yhat

256
00:15:16,000 --> 00:15:23,920
vector and the y vector. Sorry, we have

257
00:15:20,880 --> 00:15:25,839
the yhat vector and the error vector.

258
00:15:23,920 --> 00:15:30,000
And so

259
00:15:25,839 --> 00:15:33,000
from this we have that

260
00:15:30,000 --> 00:15:33,000
um

261
00:15:34,880 --> 00:15:42,079
the squared length of y is equal to the

262
00:15:39,279 --> 00:15:46,399
squared length of y hat plus the squared

263
00:15:42,079 --> 00:15:49,519
length of the residual vector. And then

264
00:15:46,399 --> 00:15:51,440
if we expand this

265
00:15:49,519 --> 00:15:53,839
squared length out, we actually get zero

266
00:15:51,440 --> 00:15:58,880
because of the orthogonality.

267
00:15:53,839 --> 00:16:02,720
So with le squares, we basically have

268
00:15:58,880 --> 00:16:04,959
a sort of Pythagorean theorem

269
00:16:02,720 --> 00:16:06,800
that generalizes from two dimensions to

270
00:16:04,959 --> 00:16:11,680
n dimensions.

271
00:16:06,800 --> 00:16:15,600
And that generalization is quite

272
00:16:11,680 --> 00:16:17,440
uh convenient and useful when we think

273
00:16:15,600 --> 00:16:20,240
about

274
00:16:17,440 --> 00:16:22,240
not the mathematical exercise of le

275
00:16:20,240 --> 00:16:25,360
squares but we think about having

276
00:16:22,240 --> 00:16:28,000
probability distributions on say the

277
00:16:25,360 --> 00:16:30,160
errors. Um

278
00:16:28,000 --> 00:16:35,519
so

279
00:16:30,160 --> 00:16:39,440
so that leads us to trying to extend the

280
00:16:35,519 --> 00:16:44,639
mathematical regression model to a

281
00:16:39,440 --> 00:16:47,519
probability model. And so with this

282
00:16:44,639 --> 00:16:49,279
we have constants

283
00:16:47,519 --> 00:16:52,160
assumptions for the independent

284
00:16:49,279 --> 00:16:54,560
variables and the X matrix constants for

285
00:16:52,160 --> 00:16:57,120
the regression parameters.

286
00:16:54,560 --> 00:16:59,600
But the error terms

287
00:16:57,120 --> 00:17:02,720
uh those will be assumed to follow some

288
00:16:59,600 --> 00:17:05,520
probability model.

289
00:17:02,720 --> 00:17:07,439
What's useful to think about is well

290
00:17:05,520 --> 00:17:10,959
what would be the simplest possible

291
00:17:07,439 --> 00:17:13,919
probability model to to apply to a

292
00:17:10,959 --> 00:17:18,079
regression? Well, we could assume that

293
00:17:13,919 --> 00:17:20,400
the errors are IID

294
00:17:18,079 --> 00:17:22,880
which means independent and identically

295
00:17:20,400 --> 00:17:26,799
distributed.

296
00:17:22,880 --> 00:17:30,160
And we could also for convenience assume

297
00:17:26,799 --> 00:17:35,760
that they are normally uh distributed.

298
00:17:30,160 --> 00:17:40,559
Um and so this is a normal linear

299
00:17:35,760 --> 00:17:44,160
regression model. And when we write out

300
00:17:40,559 --> 00:17:47,200
the model equation here,

301
00:17:44,160 --> 00:17:50,720
um the model equation basically is

302
00:17:47,200 --> 00:17:52,799
constants plus the error vector. and the

303
00:17:50,720 --> 00:17:56,480
error vector

304
00:17:52,799 --> 00:17:59,280
epsilon 1 to epsilon n being iid normal.

305
00:17:56,480 --> 00:18:03,039
This in fact is consistent or is

306
00:17:59,280 --> 00:18:06,240
equivalent to a multivariate normal

307
00:18:03,039 --> 00:18:09,600
distribution of dimension n with a zero

308
00:18:06,240 --> 00:18:12,799
vector for the mean and a covariance

309
00:18:09,600 --> 00:18:16,000
matrix that's diagonal

310
00:18:12,799 --> 00:18:18,480
with constant variance.

311
00:18:16,000 --> 00:18:20,720
So this is going to be a convenient

312
00:18:18,480 --> 00:18:24,160
model for us to work with. And what

313
00:18:20,720 --> 00:18:27,200
we'll see is that different inferences

314
00:18:24,160 --> 00:18:28,799
about how important different

315
00:18:27,200 --> 00:18:32,320
independent variables are in a

316
00:18:28,799 --> 00:18:35,760
regression model can be judged using

317
00:18:32,320 --> 00:18:38,240
this model as sort of a baseline model

318
00:18:35,760 --> 00:18:40,000
for that evaluation.

319
00:18:38,240 --> 00:18:42,000
So

320
00:18:40,000 --> 00:18:47,440
importantly

321
00:18:42,000 --> 00:18:50,960
um the distribution of the residuals

322
00:18:47,440 --> 00:18:53,440
leads us to a distribution

323
00:18:50,960 --> 00:18:57,520
of

324
00:18:53,440 --> 00:19:03,360
the dependent variable vector y.

325
00:18:57,520 --> 00:19:06,720
And so in this slide we use the notation

326
00:19:03,360 --> 00:19:10,559
of a mu vector for the conditional

327
00:19:06,720 --> 00:19:14,919
expectation of the y vector

328
00:19:10,559 --> 00:19:14,919
given x and beta.

329
00:19:16,080 --> 00:19:20,960
And

330
00:19:18,160 --> 00:19:24,320
we'll define this just to be a mu vector

331
00:19:20,960 --> 00:19:27,360
that's an n vector.

332
00:19:24,320 --> 00:19:31,240
And we'll define the co-variance matrix

333
00:19:27,360 --> 00:19:31,240
of the y vector

334
00:19:36,400 --> 00:19:41,200
for the conditional covariance.

335
00:19:38,880 --> 00:19:47,960
Okay, this is going to be equal to the

336
00:19:41,200 --> 00:19:47,960
product of y minus its mean

337
00:19:51,840 --> 00:19:58,360
times the transpose of y minus its mean.

338
00:19:59,840 --> 00:20:05,640
And so this is an n by one 1 by n

339
00:20:06,640 --> 00:20:12,080
term. So this is an n byn

340
00:20:09,840 --> 00:20:16,880
matrix

341
00:20:12,080 --> 00:20:20,000
and we'll call this

342
00:20:16,880 --> 00:20:23,760
capital sigma

343
00:20:20,000 --> 00:20:28,000
as a coariance matrix. Now for the

344
00:20:23,760 --> 00:20:32,320
special case of iid errors um this

345
00:20:28,000 --> 00:20:34,960
coariance matrix will be diagonal with

346
00:20:32,320 --> 00:20:41,440
constant sigma squared

347
00:20:34,960 --> 00:20:45,760
and it will turn out that um this

348
00:20:41,440 --> 00:20:49,000
conditional distribution of y given x

349
00:20:45,760 --> 00:20:49,000
and beta

350
00:20:49,120 --> 00:20:55,679
we'll have a multormal distribution with

351
00:20:52,240 --> 00:20:58,320
a mean vector mu and a covariance matrix

352
00:20:55,679 --> 00:21:01,520
sigma. So an n-dimensional

353
00:20:58,320 --> 00:21:03,760
multivariant normal distribution.

354
00:21:01,520 --> 00:21:09,840
Okay. Well,

355
00:21:03,760 --> 00:21:13,200
what we're going to do is show

356
00:21:09,840 --> 00:21:17,440
results of what distribution the Y

357
00:21:13,200 --> 00:21:20,320
vector is as well as show

358
00:21:17,440 --> 00:21:22,880
what the distribution is of the le

359
00:21:20,320 --> 00:21:28,159
squares estimate is using moment

360
00:21:22,880 --> 00:21:30,880
generating functions. So this um example

361
00:21:28,159 --> 00:21:33,039
u of the application of moment

362
00:21:30,880 --> 00:21:37,520
generating functions is really quite

363
00:21:33,039 --> 00:21:42,480
neat because the arguments are

364
00:21:37,520 --> 00:21:46,960
very simple and accessible. So if we

365
00:21:42,480 --> 00:21:50,480
have a an nvariat

366
00:21:46,960 --> 00:21:56,480
random vector y

367
00:21:50,480 --> 00:21:59,600
and a constant vector t, then the moment

368
00:21:56,480 --> 00:22:03,919
generating function of

369
00:21:59,600 --> 00:22:08,000
the y vector is equal to the expected

370
00:22:03,919 --> 00:22:11,280
value of the exponential of t transpose

371
00:22:08,000 --> 00:22:12,880
y. So this moment generating function

372
00:22:11,280 --> 00:22:18,400
has

373
00:22:12,880 --> 00:22:22,799
uh is a function of the t vector and we

374
00:22:18,400 --> 00:22:26,159
just plug into this formula. Now because

375
00:22:22,799 --> 00:22:28,960
the y's in fact are independent of each

376
00:22:26,159 --> 00:22:32,960
other then

377
00:22:28,960 --> 00:22:35,919
we have this expected product of terms

378
00:22:32,960 --> 00:22:39,120
is the product of the expectations.

379
00:22:35,919 --> 00:22:42,000
So we're going to use that independence

380
00:22:39,120 --> 00:22:44,320
property uh where expectations of

381
00:22:42,000 --> 00:22:45,919
products are products of the

382
00:22:44,320 --> 00:22:49,840
expectations.

383
00:22:45,919 --> 00:22:51,840
And then for each of the y's

384
00:22:49,840 --> 00:22:55,440
we can plug in the moment generating

385
00:22:51,840 --> 00:23:00,400
function for each y

386
00:22:55,440 --> 00:23:03,200
I and that's e to the ti mui plus a half

387
00:23:00,400 --> 00:23:05,760
ti 2 sigma squ. We went through this

388
00:23:03,200 --> 00:23:09,440
moment generating function you know in

389
00:23:05,760 --> 00:23:11,280
our section on on probability theory.

390
00:23:09,440 --> 00:23:16,240
Now

391
00:23:11,280 --> 00:23:19,120
what's important from this example is if

392
00:23:16,240 --> 00:23:21,120
we take this product of all the moment

393
00:23:19,120 --> 00:23:25,600
generating functions and then try to

394
00:23:21,120 --> 00:23:29,440
express it in terms of the T vector,

395
00:23:25,600 --> 00:23:32,080
the mu vector and the sigma matrix.

396
00:23:29,440 --> 00:23:36,080
we get this formula

397
00:23:32,080 --> 00:23:38,480
and this formula is in fact the moment

398
00:23:36,080 --> 00:23:41,840
generating function for a multivariant

399
00:23:38,480 --> 00:23:44,799
normal with a given mean vector mu and a

400
00:23:41,840 --> 00:23:49,200
given coariance matrix sigma.

401
00:23:44,799 --> 00:23:50,880
So that result is sort of trivial, not

402
00:23:49,200 --> 00:23:54,240
surprising.

403
00:23:50,880 --> 00:23:58,400
What's less surprising is that we can

404
00:23:54,240 --> 00:24:01,039
use moment generating functions to solve

405
00:23:58,400 --> 00:24:02,559
for the distribution of the le squares

406
00:24:01,039 --> 00:24:05,760
estimate.

407
00:24:02,559 --> 00:24:09,039
So this first

408
00:24:05,760 --> 00:24:12,400
formula here is the moment generating

409
00:24:09,039 --> 00:24:14,240
function for beta hat.

410
00:24:12,400 --> 00:24:18,120
And

411
00:24:14,240 --> 00:24:18,120
in writing that out

412
00:24:20,240 --> 00:24:26,559
m some sub beta hat I'm going to use a

413
00:24:23,120 --> 00:24:31,640
different argument toao

414
00:24:26,559 --> 00:24:31,640
which will be a p vector

415
00:24:32,880 --> 00:24:39,039
and this is going to be the expected

416
00:24:35,360 --> 00:24:42,799
value of e to the tow transpose beta

417
00:24:39,039 --> 00:24:45,919
hat. So this is the definition of the

418
00:24:42,799 --> 00:24:47,520
moment generating function.

419
00:24:45,919 --> 00:24:49,679
And

420
00:24:47,520 --> 00:24:51,360
if we

421
00:24:49,679 --> 00:24:56,240
define

422
00:24:51,360 --> 00:25:00,000
the matrix A to be the basically the

423
00:24:56,240 --> 00:25:05,039
premultiple factor

424
00:25:00,000 --> 00:25:08,159
of uh A that gives us beta hat.

425
00:25:05,039 --> 00:25:12,880
Then our moment generating function for

426
00:25:08,159 --> 00:25:17,360
beta hat can be written as

427
00:25:12,880 --> 00:25:23,679
this expression just plugging in a y for

428
00:25:17,360 --> 00:25:26,880
beta hat. But then we can express

429
00:25:23,679 --> 00:25:30,159
to transpose a

430
00:25:26,880 --> 00:25:32,799
equal to tanspose.

431
00:25:30,159 --> 00:25:35,039
So this

432
00:25:32,799 --> 00:25:39,520
moment generating function of the le

433
00:25:35,039 --> 00:25:42,080
squares estimates beta hat is actually

434
00:25:39,520 --> 00:25:45,039
the value of the moment generating

435
00:25:42,080 --> 00:25:49,520
function of the multinnormal vector

436
00:25:45,039 --> 00:25:51,679
evaluated at t equal to arpose

437
00:25:49,520 --> 00:25:59,840
toao.

438
00:25:51,679 --> 00:26:02,480
And so we can simply plug in this value

439
00:25:59,840 --> 00:26:05,520
of uh t

440
00:26:02,480 --> 00:26:09,000
and get

441
00:26:05,520 --> 00:26:09,000
our result.

442
00:26:09,840 --> 00:26:18,559
And so tanspose mu is ends up being to

443
00:26:13,360 --> 00:26:21,520
transpose beta. and tRpose sigma t

444
00:26:18,559 --> 00:26:26,400
is equal to this product.

445
00:26:21,520 --> 00:26:30,000
And if we simplify it, we basically have

446
00:26:26,400 --> 00:26:33,600
a sort of mean vector beta for the le

447
00:26:30,000 --> 00:26:36,640
squares estimate and a covariance matrix

448
00:26:33,600 --> 00:26:38,799
for the le squares estimate that's given

449
00:26:36,640 --> 00:26:42,480
by

450
00:26:38,799 --> 00:26:45,679
sigma^ 2 xrpose x inverse.

451
00:26:42,480 --> 00:26:48,640
So this is our moment generating

452
00:26:45,679 --> 00:26:50,320
function of beta hat and by the

453
00:26:48,640 --> 00:26:53,600
uniqueness of moment generating

454
00:26:50,320 --> 00:26:57,360
functions we can recognize this as a

455
00:26:53,600 --> 00:27:00,720
multi-normal with the given mean vector

456
00:26:57,360 --> 00:27:04,320
and the given coariance matrix.

457
00:27:00,720 --> 00:27:07,520
So uh so we have multivariate normality

458
00:27:04,320 --> 00:27:11,520
of the le squares estimates which is

459
00:27:07,520 --> 00:27:16,159
very convenient and useful.

460
00:27:11,520 --> 00:27:17,840
Now from this we end up getting marginal

461
00:27:16,159 --> 00:27:20,720
distributions

462
00:27:17,840 --> 00:27:22,640
that are normal for each of the le

463
00:27:20,720 --> 00:27:27,200
squares estimates.

464
00:27:22,640 --> 00:27:30,400
So we're just thinking in this slide of

465
00:27:27,200 --> 00:27:36,480
the j component of beta hat. Well, it's

466
00:27:30,400 --> 00:27:40,000
univariate normal with mean beta j and

467
00:27:36,480 --> 00:27:44,960
variance given by sigma squar times the

468
00:27:40,000 --> 00:27:50,640
j diagonal element of xrpose x inverse

469
00:27:44,960 --> 00:27:54,720
and uh so so we get this nice

470
00:27:50,640 --> 00:27:56,720
distribution result. Um

471
00:27:54,720 --> 00:28:02,159
now there's more distribution theory

472
00:27:56,720 --> 00:28:05,200
here to go through but um

473
00:28:02,159 --> 00:28:09,840
let's see something just to think about

474
00:28:05,200 --> 00:28:13,279
that's I think very powerful is um in

475
00:28:09,840 --> 00:28:15,919
some examples

476
00:28:13,279 --> 00:28:19,360
um

477
00:28:15,919 --> 00:28:21,600
where we have this model

478
00:28:19,360 --> 00:28:24,159
um

479
00:28:21,600 --> 00:28:28,720
we may have control over what our

480
00:28:24,159 --> 00:28:31,679
independent variable matrix is. And in a

481
00:28:28,720 --> 00:28:35,840
scientific experiment, we might vary

482
00:28:31,679 --> 00:28:37,840
different conditions of the experiment

483
00:28:35,840 --> 00:28:39,919
yielding Y for different conditions of

484
00:28:37,840 --> 00:28:41,679
X.

485
00:28:39,919 --> 00:28:46,399
And

486
00:28:41,679 --> 00:28:48,480
with these uh models, we might be

487
00:28:46,399 --> 00:28:53,039
interested in

488
00:28:48,480 --> 00:28:55,520
a particular sort of column of the X

489
00:28:53,039 --> 00:28:58,799
matrix

490
00:28:55,520 --> 00:29:01,200
and want to estimate

491
00:28:58,799 --> 00:29:03,760
the beta J the regression parameter

492
00:29:01,200 --> 00:29:05,840
relationship for that.

493
00:29:03,760 --> 00:29:10,320
And so

494
00:29:05,840 --> 00:29:16,559
if we had control over

495
00:29:10,320 --> 00:29:18,240
the X matrix, we might try to make the J

496
00:29:16,559 --> 00:29:21,600
diagonal

497
00:29:18,240 --> 00:29:24,640
the smallest we can

498
00:29:21,600 --> 00:29:26,480
so that the precision of that regression

499
00:29:24,640 --> 00:29:28,720
parameter estimate is as small as

500
00:29:26,480 --> 00:29:32,120
possible.

501
00:29:28,720 --> 00:29:32,120
And so

502
00:29:32,640 --> 00:29:39,279
suppose

503
00:29:34,240 --> 00:29:42,240
I guess for simplicity that the X matrix

504
00:29:39,279 --> 00:29:46,480
um

505
00:29:42,240 --> 00:29:49,480
suppose that XRpose X is a diagonal

506
00:29:46,480 --> 00:29:49,480
matrix

507
00:29:50,799 --> 00:30:00,919
and here's CJ

508
00:29:54,399 --> 00:30:00,919
the diagonal values of XRpose X

509
00:30:02,000 --> 00:30:05,520
inverse

510
00:30:03,520 --> 00:30:07,200
will simply

511
00:30:05,520 --> 00:30:09,360
be

512
00:30:07,200 --> 00:30:13,679
so if we do a minus one there we can do

513
00:30:09,360 --> 00:30:16,640
a minus one here. Our uh diagonal

514
00:30:13,679 --> 00:30:22,000
entries will be smallest

515
00:30:16,640 --> 00:30:24,880
smaller the more large the J column is.

516
00:30:22,000 --> 00:30:31,520
And so when you think of

517
00:30:24,880 --> 00:30:34,240
um regression uh models and and uh uh

518
00:30:31,520 --> 00:30:36,960
experiments, we basically would perhaps

519
00:30:34,240 --> 00:30:39,440
want to have the spread in an

520
00:30:36,960 --> 00:30:43,120
independent variable be as big as

521
00:30:39,440 --> 00:30:44,720
possible in order to specify the line.

522
00:30:43,120 --> 00:30:46,880
So

523
00:30:44,720 --> 00:30:50,640
if we

524
00:30:46,880 --> 00:30:55,360
if we think of uh a regression model of

525
00:30:50,640 --> 00:30:58,080
y on x and we have um

526
00:30:55,360 --> 00:31:00,880
if we have points

527
00:30:58,080 --> 00:31:03,120
of x and y

528
00:31:00,880 --> 00:31:06,240
um

529
00:31:03,120 --> 00:31:10,640
basically the uh

530
00:31:06,240 --> 00:31:14,080
if we look at the sample mean of the x's

531
00:31:10,640 --> 00:31:17,760
and we consider consider

532
00:31:14,080 --> 00:31:21,200
sort of concentrating our x values

533
00:31:17,760 --> 00:31:23,679
as far away as possible. That presumably

534
00:31:21,200 --> 00:31:26,960
will yield more accurate estimates of

535
00:31:23,679 --> 00:31:31,679
the slope parameter in the relationship.

536
00:31:26,960 --> 00:31:34,559
And so you can think of basically if we

537
00:31:31,679 --> 00:31:37,519
observe points that are far away from

538
00:31:34,559 --> 00:31:41,360
the mean

539
00:31:37,519 --> 00:31:45,600
then those points will sort of pin down

540
00:31:41,360 --> 00:31:50,559
the regression line the greatest and

541
00:31:45,600 --> 00:31:52,240
this u issue arises in uh experimental

542
00:31:50,559 --> 00:31:55,200
design.

543
00:31:52,240 --> 00:31:58,240
Um and in experimental design, one can

544
00:31:55,200 --> 00:32:00,640
think of how one can construct X

545
00:31:58,240 --> 00:32:05,519
matrices that have these these

546
00:32:00,640 --> 00:32:08,320
properties. And as you might expect,

547
00:32:05,519 --> 00:32:12,880
the properties relate to

548
00:32:08,320 --> 00:32:16,559
the igen values of xrpose x

549
00:32:12,880 --> 00:32:21,200
and uh looking at

550
00:32:16,559 --> 00:32:23,279
uh methods for maximizing those values.

551
00:32:21,200 --> 00:32:25,760
All right. Well,

552
00:32:23,279 --> 00:32:30,640
we have we can do some more distribution

553
00:32:25,760 --> 00:32:35,120
theory which is to look at what the

554
00:32:30,640 --> 00:32:38,880
distribution is of our error vector

555
00:32:35,120 --> 00:32:42,000
y vector but we can also look at any uh

556
00:32:38,880 --> 00:32:44,320
matrix A* Y

557
00:32:42,000 --> 00:32:46,320
and look at the distribution of that

558
00:32:44,320 --> 00:32:49,679
transformation.

559
00:32:46,320 --> 00:32:53,279
It'll be a

560
00:32:49,679 --> 00:32:56,880
linear combination of the Y's for each

561
00:32:53,279 --> 00:33:00,000
component of Z. And

562
00:32:56,880 --> 00:33:02,960
it will actually have an M dimensional

563
00:33:00,000 --> 00:33:05,760
multinnormal distribution with mean

564
00:33:02,960 --> 00:33:10,880
value given by A times the expected

565
00:33:05,760 --> 00:33:14,559
value of Y and coariance matrix equal to

566
00:33:10,880 --> 00:33:18,320
A times the coariance of Y a transpose.

567
00:33:14,559 --> 00:33:21,120
And so this matrix A will give us the

568
00:33:18,320 --> 00:33:23,279
distribution of beta hat as we just did.

569
00:33:21,120 --> 00:33:27,039
But we can also use different

570
00:33:23,279 --> 00:33:29,279
definitions of A and Z.

571
00:33:27,039 --> 00:33:33,760
And

572
00:33:29,279 --> 00:33:37,760
the normal distribution theory leads to

573
00:33:33,760 --> 00:33:42,640
these sort of key properties of uh

574
00:33:37,760 --> 00:33:44,960
normal linear model terms. We have

575
00:33:42,640 --> 00:33:46,960
the beta hat le squar's estimate is

576
00:33:44,960 --> 00:33:51,679
multinnormal.

577
00:33:46,960 --> 00:33:55,919
We have that the error vector epsilon

578
00:33:51,679 --> 00:33:59,039
hat is also multinnormal.

579
00:33:55,919 --> 00:34:02,559
Um and but it's multormal in n

580
00:33:59,039 --> 00:34:05,039
dimensions with

581
00:34:02,559 --> 00:34:06,720
mean vector

582
00:34:05,039 --> 00:34:10,399
zero.

583
00:34:06,720 --> 00:34:13,520
So epsilon hat which is equal to i minus

584
00:34:10,399 --> 00:34:16,879
h y

585
00:34:13,520 --> 00:34:21,679
is distributed as a multinnormal

586
00:34:16,879 --> 00:34:26,720
in n dimensions with a covariance matrix

587
00:34:21,679 --> 00:34:28,639
given by sigma squ times the projection.

588
00:34:26,720 --> 00:34:30,480
Now

589
00:34:28,639 --> 00:34:34,639
what's

590
00:34:30,480 --> 00:34:39,320
significant is that this co-variance

591
00:34:34,639 --> 00:34:39,320
matrix here is not a full rank.

592
00:34:39,919 --> 00:34:45,320
So um this is non-s singular

593
00:34:49,679 --> 00:34:53,280
actually. Is that the right word? No,

594
00:34:51,280 --> 00:34:56,800
it's singular

595
00:34:53,280 --> 00:34:59,800
I guess. Um but it's singular. It's not

596
00:34:56,800 --> 00:34:59,800
invertible.

597
00:35:02,880 --> 00:35:08,880
And what does that mean?

598
00:35:06,240 --> 00:35:11,280
Well, that means that

599
00:35:08,880 --> 00:35:16,079
there are

600
00:35:11,280 --> 00:35:20,160
linear combinations of the error vector

601
00:35:16,079 --> 00:35:23,680
that have uh zero variance. There's a

602
00:35:20,160 --> 00:35:25,839
linear dependence in the error vectors.

603
00:35:23,680 --> 00:35:30,960
And so

604
00:35:25,839 --> 00:35:33,040
if we consider xhat and xrpose this and

605
00:35:30,960 --> 00:35:37,200
this is equal to the zero vector this is

606
00:35:33,040 --> 00:35:40,560
the normal equations. Um

607
00:35:37,200 --> 00:35:43,520
we basically have that fixed linear

608
00:35:40,560 --> 00:35:45,440
combinations of these residuals

609
00:35:43,520 --> 00:35:50,240
the residual vector are identically

610
00:35:45,440 --> 00:35:53,599
equal to zero. So we don't have um n

611
00:35:50,240 --> 00:35:59,520
sort of independent error terms.

612
00:35:53,599 --> 00:36:01,920
And so um because of that uh when we

613
00:35:59,520 --> 00:36:03,839
look at

614
00:36:01,920 --> 00:36:06,800
for example estimating the error

615
00:36:03,839 --> 00:36:10,240
variance sigma squared well if we take

616
00:36:06,800 --> 00:36:13,280
the sum of the squared residuals that's

617
00:36:10,240 --> 00:36:16,240
the trace of the coariance matrix

618
00:36:13,280 --> 00:36:20,960
of the residual vector and that's equal

619
00:36:16,240 --> 00:36:25,359
to the trace of this

620
00:36:20,960 --> 00:36:27,760
projection uh matrix I n minus H. And so

621
00:36:25,359 --> 00:36:30,720
the trace of a sum is the sum of the

622
00:36:27,760 --> 00:36:37,119
traces.

623
00:36:30,720 --> 00:36:39,839
And the trace of a matrix is product is

624
00:36:37,119 --> 00:36:44,000
also the trace of the product with terms

625
00:36:39,839 --> 00:36:47,520
reversed. So this gives us sigma^ 2 n

626
00:36:44,000 --> 00:36:49,839
minus trace of h and we get n minus p

627
00:36:47,520 --> 00:36:53,440
sigma squar for the expected value of

628
00:36:49,839 --> 00:36:56,800
sigma squar. And in regression models

629
00:36:53,440 --> 00:37:01,359
with normal linear models, we use this

630
00:36:56,800 --> 00:37:04,960
relationship to estimate sigma squar

631
00:37:01,359 --> 00:37:07,440
dividing both sides by n minus p, we get

632
00:37:04,960 --> 00:37:10,800
an unbiased estimate

633
00:37:07,440 --> 00:37:14,480
um of the error variance.

634
00:37:10,800 --> 00:37:18,560
Now an additional property that's from

635
00:37:14,480 --> 00:37:20,160
part C of this theorem is that the error

636
00:37:18,560 --> 00:37:23,359
vector

637
00:37:20,160 --> 00:37:27,359
epsilon hat and the regression parameter

638
00:37:23,359 --> 00:37:30,560
beta hat those happen to be independent

639
00:37:27,359 --> 00:37:33,680
of each other

640
00:37:30,560 --> 00:37:37,359
and the independence

641
00:37:33,680 --> 00:37:39,440
follows by looking at the joint

642
00:37:37,359 --> 00:37:41,440
distribution

643
00:37:39,440 --> 00:37:44,560
the moment joint moment generating

644
00:37:41,440 --> 00:37:46,960
function of

645
00:37:44,560 --> 00:37:50,800
a y

646
00:37:46,960 --> 00:37:53,119
and showing that that is equal to the

647
00:37:50,800 --> 00:37:55,680
moment generating function of beta hat

648
00:37:53,119 --> 00:37:58,880
times the moment generating function of

649
00:37:55,680 --> 00:38:01,440
epsilon hat. So the product of moment

650
00:37:58,880 --> 00:38:06,400
generating functions sort of allows us

651
00:38:01,440 --> 00:38:08,800
to determine the independence there.

652
00:38:06,400 --> 00:38:11,440
And when we have independence of the

653
00:38:08,800 --> 00:38:15,680
error vector and the regression

654
00:38:11,440 --> 00:38:17,839
parameter vector, then we can compute

655
00:38:15,680 --> 00:38:19,599
tstistics

656
00:38:17,839 --> 00:38:24,079
for

657
00:38:19,599 --> 00:38:28,560
regression parameters. And we end up as

658
00:38:24,079 --> 00:38:32,400
stated in the notes there that TJ

659
00:38:28,560 --> 00:38:38,359
equal to beta hat J minus the true beta

660
00:38:32,400 --> 00:38:38,359
J divided by sigma hat

661
00:38:39,440 --> 00:38:44,440
times C JJ J.

662
00:38:45,760 --> 00:38:51,119
Um this distribution will have a T

663
00:38:49,359 --> 00:38:54,560
distribution.

664
00:38:51,119 --> 00:38:57,200
Now, when you studied statistics, if you

665
00:38:54,560 --> 00:39:00,800
took a statistics course, you I'm sure

666
00:38:57,200 --> 00:39:06,880
came across the t distribution.

667
00:39:00,800 --> 00:39:09,920
And the t distribution is uh rather uh

668
00:39:06,880 --> 00:39:11,680
remarkable. Um

669
00:39:09,920 --> 00:39:13,440
this

670
00:39:11,680 --> 00:39:20,400
this turns out to be equal in

671
00:39:13,440 --> 00:39:23,040
distribution to a normal zero.

672
00:39:20,400 --> 00:39:28,640
uh distribution

673
00:39:23,040 --> 00:39:32,800
mean zero and some variance divided by

674
00:39:28,640 --> 00:39:35,119
the square root of a ki squared

675
00:39:32,800 --> 00:39:38,400
distribution

676
00:39:35,119 --> 00:39:40,320
divided by its degrees of freedom

677
00:39:38,400 --> 00:39:42,960
where these two things are independent

678
00:39:40,320 --> 00:39:48,480
of each other.

679
00:39:42,960 --> 00:39:52,960
And so um the t distribution

680
00:39:48,480 --> 00:39:55,280
well who can comment on what properties

681
00:39:52,960 --> 00:39:58,280
a t distribution has compared to a

682
00:39:55,280 --> 00:39:58,280
normal.

683
00:40:05,200 --> 00:40:09,480
The t distribution is symmetrical.

684
00:40:09,839 --> 00:40:13,480
It's somewhat bell-shaped.

685
00:40:21,920 --> 00:40:25,720
Well, if this numerator

686
00:40:26,320 --> 00:40:33,400
is a normal and this denominator is

687
00:40:30,400 --> 00:40:33,400
random,

688
00:40:33,839 --> 00:40:41,839
it's random near one. Um, then we'll

689
00:40:38,640 --> 00:40:45,119
have a heavier tail distribution.

690
00:40:41,839 --> 00:40:49,920
with the t. And so

691
00:40:45,119 --> 00:40:54,320
we need to uh be able to quantify sort

692
00:40:49,920 --> 00:40:56,240
of how significantly different from uh

693
00:40:54,320 --> 00:41:00,480
zero

694
00:40:56,240 --> 00:41:05,680
the tstistic is. So if we have an H knot

695
00:41:00,480 --> 00:41:07,200
that beta J is equal to zero then this

696
00:41:05,680 --> 00:41:11,599
distribution

697
00:41:07,200 --> 00:41:14,319
turns out to be um

698
00:41:11,599 --> 00:41:16,960
basically have a t distribution

699
00:41:14,319 --> 00:41:21,839
a t distribution

700
00:41:16,960 --> 00:41:25,119
with uh n minus p degrees of freedom

701
00:41:21,839 --> 00:41:28,240
and so we can uh

702
00:41:25,119 --> 00:41:31,440
judge whether

703
00:41:28,240 --> 00:41:34,640
our data provides evidence against this

704
00:41:31,440 --> 00:41:36,560
null hypothesis or not. And we can test

705
00:41:34,640 --> 00:41:38,880
other hypotheses

706
00:41:36,560 --> 00:41:41,760
as well, not only that the beta j is

707
00:41:38,880 --> 00:41:45,280
equal to zero. We'll be covering this in

708
00:41:41,760 --> 00:41:46,960
a little more detail um with

709
00:41:45,280 --> 00:41:50,880
uh one of the lecture notes that was

710
00:41:46,960 --> 00:41:53,200
passed out for today. But um

711
00:41:50,880 --> 00:41:55,200
the uh

712
00:41:53,200 --> 00:41:57,839
let's see does anyone know the history

713
00:41:55,200 --> 00:42:00,839
of the t distribution? how it was

714
00:41:57,839 --> 00:42:00,839
discovered

715
00:42:02,400 --> 00:42:07,520
>> through Guinness.

716
00:42:04,720 --> 00:42:09,520
>> Yes. Uh there was a a statistician who

717
00:42:07,520 --> 00:42:16,079
was working at Guinness with quality

718
00:42:09,520 --> 00:42:19,119
control and uh he he would look at um

719
00:42:16,079 --> 00:42:25,520
he would look at uh samples very small

720
00:42:19,119 --> 00:42:28,880
samples of size four from a uh

721
00:42:25,520 --> 00:42:31,680
so we'll just write here Guinness

722
00:42:28,880 --> 00:42:35,440
um he

723
00:42:31,680 --> 00:42:37,359
would look at samples say X1 X2 X2, X3,

724
00:42:35,440 --> 00:42:39,599
X4

725
00:42:37,359 --> 00:42:44,240
of a measurement. I'm not sure quite

726
00:42:39,599 --> 00:42:48,240
what the qual quality measure was, but

727
00:42:44,240 --> 00:42:54,040
you would calculate Xbar

728
00:42:48,240 --> 00:42:54,040
and the standard deviation of X

729
00:42:54,240 --> 00:43:05,119
um which is the square root of

730
00:42:58,319 --> 00:43:07,119
uh the sum of Xi - Xar^ 2 over over um

731
00:43:05,119 --> 00:43:10,880
over three. Well, actually I think he

732
00:43:07,119 --> 00:43:12,960
did it. Well, I I'll write in three. And

733
00:43:10,880 --> 00:43:16,079
um

734
00:43:12,960 --> 00:43:19,760
what he discovered was that

735
00:43:16,079 --> 00:43:25,520
uh if we look at Z

736
00:43:19,760 --> 00:43:27,040
equal to sort of Xbar over SX

737
00:43:25,520 --> 00:43:31,599
um

738
00:43:27,040 --> 00:43:35,599
these outcomes of the sample means

739
00:43:31,599 --> 00:43:37,440
which uh suitably rescaled should follow

740
00:43:35,599 --> 00:43:39,520
a normal distribution because we're

741
00:43:37,440 --> 00:43:42,400
looking at a zcore.

742
00:43:39,520 --> 00:43:46,000
um this was not

743
00:43:42,400 --> 00:43:49,920
uh distributed

744
00:43:46,000 --> 00:43:52,880
as a normal 01.

745
00:43:49,920 --> 00:43:55,599
there was much greater variability and

746
00:43:52,880 --> 00:43:58,000
so um

747
00:43:55,599 --> 00:44:00,160
the person who did this work they

748
00:43:58,000 --> 00:44:02,800
wouldn't allow him to publish under his

749
00:44:00,160 --> 00:44:05,440
uh real name and so he published a paper

750
00:44:02,800 --> 00:44:07,599
under the pseudonym student and so we

751
00:44:05,440 --> 00:44:10,319
call this the students t distribution

752
00:44:07,599 --> 00:44:13,680
now but what was really quite remarkable

753
00:44:10,319 --> 00:44:18,880
was to see that in small samples the

754
00:44:13,680 --> 00:44:20,560
variability of these kinds of statistics

755
00:44:18,880 --> 00:44:22,640
um

756
00:44:20,560 --> 00:44:26,800
in theory should be close to normal but

757
00:44:22,640 --> 00:44:28,480
in fact are systematically different. Um

758
00:44:26,800 --> 00:44:32,640
and

759
00:44:28,480 --> 00:44:37,359
with let's see with uh other questions

760
00:44:32,640 --> 00:44:40,240
that come up in regression um one can

761
00:44:37,359 --> 00:44:43,839
construct an F test

762
00:44:40,240 --> 00:44:46,960
of whether all the regression parameters

763
00:44:43,839 --> 00:44:49,040
beyond the first P are equal to zero or

764
00:44:46,960 --> 00:44:53,280
not.

765
00:44:49,040 --> 00:44:57,280
And one can look at the residual sum of

766
00:44:53,280 --> 00:44:59,119
squares from fitting the full model

767
00:44:57,280 --> 00:45:02,319
and then the residual sum of squares

768
00:44:59,119 --> 00:45:07,599
from fitting a submodel using just the

769
00:45:02,319 --> 00:45:12,079
first K. Sorry, the first K. I misspoke.

770
00:45:07,599 --> 00:45:14,240
Um and then this F statistic

771
00:45:12,079 --> 00:45:17,119
is a ratio

772
00:45:14,240 --> 00:45:20,400
of normalized sort of sums of squares

773
00:45:17,119 --> 00:45:23,200
and differences thereof. and and this F

774
00:45:20,400 --> 00:45:29,440
test statistic comes up

775
00:45:23,200 --> 00:45:32,960
as uh an analysis of variance statistic

776
00:45:29,440 --> 00:45:36,319
where one can estimate variability

777
00:45:32,960 --> 00:45:40,319
using the residual sum of squares from

778
00:45:36,319 --> 00:45:43,599
the full model and one can then also

779
00:45:40,319 --> 00:45:49,440
look at the residual sum of squares from

780
00:45:43,599 --> 00:45:51,599
the submodel and if both models are true

781
00:45:49,440 --> 00:45:56,079
meaning

782
00:45:51,599 --> 00:45:59,119
beta k + 1 to p are equal to zero then

783
00:45:56,079 --> 00:46:01,040
these formulas here are estimating the

784
00:45:59,119 --> 00:46:03,680
same variance

785
00:46:01,040 --> 00:46:05,920
and they're independent of each other

786
00:46:03,680 --> 00:46:11,680
and so we get the f distribution as the

787
00:46:05,920 --> 00:46:13,680
ratio of two uh kaiquare distributions

788
00:46:11,680 --> 00:46:15,359
that have uh different degrees of

789
00:46:13,680 --> 00:46:18,000
freedom.

790
00:46:15,359 --> 00:46:23,280
All right. Well, let's take a look at

791
00:46:18,000 --> 00:46:25,760
some real data. Um, the uh the the

792
00:46:23,280 --> 00:46:29,040
example data set I want to introduce is

793
00:46:25,760 --> 00:46:35,520
actually not a financial data set. This

794
00:46:29,040 --> 00:46:38,000
is a medical study. But um this uh

795
00:46:35,520 --> 00:46:45,040
this example

796
00:46:38,000 --> 00:46:46,560
uh comes up in some work by Efron and

797
00:46:45,040 --> 00:46:52,240
Hasty,

798
00:46:46,560 --> 00:46:55,200
Brad Efine and uh Trevor Hasty and they

799
00:46:52,240 --> 00:46:58,079
have some books. There's the elements of

800
00:46:55,200 --> 00:47:02,319
statistical learning.

801
00:46:58,079 --> 00:47:04,800
Actually this is uh hasty and then

802
00:47:02,319 --> 00:47:07,839
there's um

803
00:47:04,800 --> 00:47:11,200
with Ephrine there's u an advanced book

804
00:47:07,839 --> 00:47:15,040
on sort of computational

805
00:47:11,200 --> 00:47:19,920
uh uh statistics

806
00:47:15,040 --> 00:47:25,040
um a book that I text that I use in 1865

807
00:47:19,920 --> 00:47:28,160
um and uh in this data set

808
00:47:25,040 --> 00:47:30,079
um one basically has a response variable

809
00:47:28,160 --> 00:47:33,760
and independent variables. And what's

810
00:47:30,079 --> 00:47:35,760
really key to know is

811
00:47:33,760 --> 00:47:37,760
experience we have with one regression

812
00:47:35,760 --> 00:47:40,240
problem extends to other regression

813
00:47:37,760 --> 00:47:43,920
problems. Just sort of the names of

814
00:47:40,240 --> 00:47:49,280
variables change and uh but the same

815
00:47:43,920 --> 00:47:54,000
issues arise. And so with this data set

816
00:47:49,280 --> 00:47:56,079
um we're trying to predict the uh LPSA

817
00:47:54,000 --> 00:47:59,200
variable

818
00:47:56,079 --> 00:48:00,960
um as it varies with other independent

819
00:47:59,200 --> 00:48:05,359
variables

820
00:48:00,960 --> 00:48:08,560
uh for subjects in a uh data set. This

821
00:48:05,359 --> 00:48:11,359
data set happened to be a data set on uh

822
00:48:08,560 --> 00:48:15,839
patients or subjects with prostate

823
00:48:11,359 --> 00:48:18,880
cancer. And so with any

824
00:48:15,839 --> 00:48:20,400
data set one can compute summary

825
00:48:18,880 --> 00:48:23,200
statistics

826
00:48:20,400 --> 00:48:25,200
which help us sort of detect whether

827
00:48:23,200 --> 00:48:29,440
there are sort of issues with the data

828
00:48:25,200 --> 00:48:32,559
or not. Um there's this pairs function

829
00:48:29,440 --> 00:48:36,240
in R which is quite convenient for just

830
00:48:32,559 --> 00:48:40,640
displaying all of the data. And if you

831
00:48:36,240 --> 00:48:43,599
look at this pairs plot, every pair of

832
00:48:40,640 --> 00:48:45,119
variables in the data set are

833
00:48:43,599 --> 00:48:47,280
plotted

834
00:48:45,119 --> 00:48:50,240
in a scatter plot with the other

835
00:48:47,280 --> 00:48:52,079
variables. And so along the diagonal we

836
00:48:50,240 --> 00:48:56,559
have the names of the variables and then

837
00:48:52,079 --> 00:49:01,680
we have um different relationships of

838
00:48:56,559 --> 00:49:05,760
those. Now if uh LPSA is the

839
00:49:01,680 --> 00:49:09,280
uh dependent variable of interest, if we

840
00:49:05,760 --> 00:49:11,200
look sort of across this row, um we're

841
00:49:09,280 --> 00:49:14,480
looking at the scatter plot of different

842
00:49:11,200 --> 00:49:17,440
independent variables with that. And it

843
00:49:14,480 --> 00:49:19,520
looks like the LCA V is probably a very

844
00:49:17,440 --> 00:49:22,319
strong relationship. Some of the

845
00:49:19,520 --> 00:49:25,119
variables in fact are discrete

846
00:49:22,319 --> 00:49:27,680
and we're able to see that as well.

847
00:49:25,119 --> 00:49:29,920
Um and if we just fit a simple

848
00:49:27,680 --> 00:49:32,559
regression,

849
00:49:29,920 --> 00:49:35,599
we end up getting

850
00:49:32,559 --> 00:49:38,960
output from the regression which

851
00:49:35,599 --> 00:49:42,319
consists of an estimate.

852
00:49:38,960 --> 00:49:44,079
We have a coefficient table

853
00:49:42,319 --> 00:49:46,720
with

854
00:49:44,079 --> 00:49:50,559
an estimate column,

855
00:49:46,720 --> 00:49:55,000
a standard error column,

856
00:49:50,559 --> 00:49:55,000
and then a t value.

857
00:49:56,800 --> 00:50:01,680
And then finally

858
00:50:00,160 --> 00:50:04,480
what's

859
00:50:01,680 --> 00:50:07,680
called a p value column. This is the

860
00:50:04,480 --> 00:50:11,200
probability that

861
00:50:07,680 --> 00:50:13,200
we have a larger value of the tstistic

862
00:50:11,200 --> 00:50:18,079
being observed.

863
00:50:13,200 --> 00:50:20,559
And what's being tabled here is results

864
00:50:18,079 --> 00:50:22,960
of hypothesis testing for each of the

865
00:50:20,559 --> 00:50:26,079
regression coefficients equaling zero or

866
00:50:22,960 --> 00:50:28,640
not in the true model. So we have

867
00:50:26,079 --> 00:50:30,240
estimates of these different regression

868
00:50:28,640 --> 00:50:33,839
parameters.

869
00:50:30,240 --> 00:50:36,000
We can get standard errors of those. The

870
00:50:33,839 --> 00:50:42,200
standard errors

871
00:50:36,000 --> 00:50:42,200
correspond to sigma hat cj

872
00:50:45,040 --> 00:50:52,720
actually it's I think it's the

873
00:50:48,160 --> 00:50:57,920
it's the square root of sigma squar cj

874
00:50:52,720 --> 00:51:02,000
um and then uh the t value is just the

875
00:50:57,920 --> 00:51:03,839
signal estimate to noise ratio

876
00:51:02,000 --> 00:51:09,440
which

877
00:51:03,839 --> 00:51:14,480
scales u in magnitude how important

878
00:51:09,440 --> 00:51:17,920
those factors are and we can calculate

879
00:51:14,480 --> 00:51:24,800
the uh p value

880
00:51:17,920 --> 00:51:28,240
let's see so if we have say a beta

881
00:51:24,800 --> 00:51:32,800
uh hat j value

882
00:51:28,240 --> 00:51:36,960
and it has this t distribution ution

883
00:51:32,800 --> 00:51:42,319
which is centered at the true beta J and

884
00:51:36,960 --> 00:51:44,160
um the spread is given by C hat CJ J

885
00:51:42,319 --> 00:51:46,400
square root

886
00:51:44,160 --> 00:51:49,680
um

887
00:51:46,400 --> 00:51:52,680
we can

888
00:51:49,680 --> 00:51:52,680
calculate

889
00:51:52,960 --> 00:51:59,599
um

890
00:51:55,359 --> 00:52:02,079
we can basically test whether beta J is

891
00:51:59,599 --> 00:52:06,400
equal to zero

892
00:52:02,079 --> 00:52:10,880
And so if we sort of set this equal to

893
00:52:06,400 --> 00:52:12,800
zero, if we observe a value of beta hat

894
00:52:10,880 --> 00:52:16,319
j,

895
00:52:12,800 --> 00:52:20,720
um we can

896
00:52:16,319 --> 00:52:25,839
calculate how likely is it that we get a

897
00:52:20,720 --> 00:52:29,440
sort of beta hat j minus beta minus 0

898
00:52:25,839 --> 00:52:31,920
over sigma hat cj.

899
00:52:29,440 --> 00:52:36,559
This is our t statistic.

900
00:52:31,920 --> 00:52:38,319
We can look at uh calculating this

901
00:52:36,559 --> 00:52:42,720
probability

902
00:52:38,319 --> 00:52:44,880
that the uh t statistic or of getting a

903
00:52:42,720 --> 00:52:47,440
greater than

904
00:52:44,880 --> 00:52:48,960
a large a larger value of that t

905
00:52:47,440 --> 00:52:53,920
statistic.

906
00:52:48,960 --> 00:52:59,920
And um with this

907
00:52:53,920 --> 00:53:03,280
uh let's see if we sort of consider

908
00:52:59,920 --> 00:53:09,680
Changing the scale here to just be beta

909
00:53:03,280 --> 00:53:14,000
hat j / sigma hat cj square root. This

910
00:53:09,680 --> 00:53:16,960
is our t statistic. Then we basically

911
00:53:14,000 --> 00:53:21,119
have a

912
00:53:16,960 --> 00:53:23,839
t distribution for the outcome of the le

913
00:53:21,119 --> 00:53:26,800
squares estimate. and we're calculating

914
00:53:23,839 --> 00:53:29,760
how likely is it we

915
00:53:26,800 --> 00:53:34,240
would get as large or larger a t

916
00:53:29,760 --> 00:53:37,200
statistic if uh the true

917
00:53:34,240 --> 00:53:42,160
regression parameters were zero or not.

918
00:53:37,200 --> 00:53:45,440
Now in looking at this these estimates,

919
00:53:42,160 --> 00:53:48,160
one of the uh challenges in part is that

920
00:53:45,440 --> 00:53:49,920
the scale of the parameter estimates

921
00:53:48,160 --> 00:53:51,599
varies

922
00:53:49,920 --> 00:53:54,480
depending on the units of the

923
00:53:51,599 --> 00:53:56,880
independent variables. And so in some

924
00:53:54,480 --> 00:54:00,400
problems we have very diverse

925
00:53:56,880 --> 00:54:02,480
independent variables in the data set.

926
00:54:00,400 --> 00:54:05,680
And the units

927
00:54:02,480 --> 00:54:08,800
are a property of the data set, but

928
00:54:05,680 --> 00:54:14,400
they're not really an important part of

929
00:54:08,800 --> 00:54:19,720
the problem. Um, and so what we can do

930
00:54:14,400 --> 00:54:19,720
is standardize the co-variants

931
00:54:20,000 --> 00:54:23,000
by

932
00:54:23,119 --> 00:54:30,240
normalizing them to have mean zero and

933
00:54:25,760 --> 00:54:35,040
standard deviation one. So if we have

934
00:54:30,240 --> 00:54:40,440
our X matrix equaling

935
00:54:35,040 --> 00:54:40,440
let's see how do I want to do this. Um

936
00:54:45,760 --> 00:54:54,319
so if we have P columns in our X matrix

937
00:54:50,319 --> 00:54:58,160
um we can basically take X

938
00:54:54,319 --> 00:54:59,760
one and transform it to

939
00:54:58,160 --> 00:55:02,480
X1

940
00:54:59,760 --> 00:55:06,000
- Xar1

941
00:55:02,480 --> 00:55:08,160
* the vector of ones

942
00:55:06,000 --> 00:55:10,000
and divide true by the standard

943
00:55:08,160 --> 00:55:13,240
deviation

944
00:55:10,000 --> 00:55:13,240
of X1.

945
00:55:13,599 --> 00:55:19,520
And so this

946
00:55:15,839 --> 00:55:23,480
we'll call this a random vector Z1 of

947
00:55:19,520 --> 00:55:23,480
standardized values.

948
00:55:23,839 --> 00:55:29,760
And

949
00:55:26,000 --> 00:55:34,079
if we do this, we're basically shifting

950
00:55:29,760 --> 00:55:37,359
the X's and rescaling the shifted

951
00:55:34,079 --> 00:55:39,359
values. It turns out that our regression

952
00:55:37,359 --> 00:55:42,000
model

953
00:55:39,359 --> 00:55:45,040
on the zcores or standardized scores are

954
00:55:42,000 --> 00:55:47,119
the same as the regression coefficients

955
00:55:45,040 --> 00:55:49,200
in the original

956
00:55:47,119 --> 00:55:51,920
units.

957
00:55:49,200 --> 00:55:56,400
So

958
00:55:51,920 --> 00:55:59,040
it's sort of obvious I guess but um if

959
00:55:56,400 --> 00:56:01,520
we have uh

960
00:55:59,040 --> 00:56:03,760
y

961
00:56:01,520 --> 00:56:05,920
uh

962
00:56:03,760 --> 00:56:08,720
is equal to

963
00:56:05,920 --> 00:56:10,640
beta 1 x1

964
00:56:08,720 --> 00:56:15,359
plus

965
00:56:10,640 --> 00:56:18,319
beta 2 x2 up to beta pxp

966
00:56:15,359 --> 00:56:22,760
plus an error vector.

967
00:56:18,319 --> 00:56:22,760
And we consider Zj

968
00:56:22,960 --> 00:56:28,079
is just simply equal to XJ

969
00:56:26,240 --> 00:56:33,319
minus

970
00:56:28,079 --> 00:56:33,319
Xar J * the vector of ones

971
00:56:33,599 --> 00:56:41,200
and 1 over say SJ where SJ is equal to

972
00:56:38,880 --> 00:56:44,240
the sample variance

973
00:56:41,200 --> 00:56:46,480
of the X

974
00:56:44,240 --> 00:56:46,480
J's.

975
00:56:50,240 --> 00:56:54,240
Um

976
00:56:52,319 --> 00:56:59,920
we basically

977
00:56:54,240 --> 00:57:08,400
can sort of substitute in for each of

978
00:56:59,920 --> 00:57:15,960
the x's the formula here. So xbar j is

979
00:57:08,400 --> 00:57:15,960
equal to s j z j

980
00:57:16,000 --> 00:57:21,680
plus xar j

981
00:57:19,599 --> 00:57:24,400
uh

982
00:57:21,680 --> 00:57:27,839
plus xar j.

983
00:57:24,400 --> 00:57:31,200
So this times this is xar minus little

984
00:57:27,839 --> 00:57:33,119
xar. So and this is times the vector of

985
00:57:31,200 --> 00:57:37,280
ones.

986
00:57:33,119 --> 00:57:42,480
So if we plug in to

987
00:57:37,280 --> 00:57:45,440
our XJ values here, this then we end up

988
00:57:42,480 --> 00:57:50,119
getting this

989
00:57:45,440 --> 00:57:50,119
equation with um

990
00:57:51,200 --> 00:57:58,240
the same uh regression parameters come

991
00:57:54,319 --> 00:58:03,680
coming into play. All right. Um

992
00:57:58,240 --> 00:58:07,920
well when we fit the scaled regression

993
00:58:03,680 --> 00:58:10,799
uh independent variables uh model. Then

994
00:58:07,920 --> 00:58:13,520
notice that the t values and the p

995
00:58:10,799 --> 00:58:15,040
values for these two regressions are

996
00:58:13,520 --> 00:58:19,839
identical.

997
00:58:15,040 --> 00:58:22,880
So if we pick say the largest t value

998
00:58:19,839 --> 00:58:27,920
SVI or that's not quite the largest LCA

999
00:58:22,880 --> 00:58:32,359
of all but let's do SVI 2.949.

1000
00:58:27,920 --> 00:58:32,359
If we scroll back

1001
00:58:39,440 --> 00:58:46,799
to the original units, the t statistics

1002
00:58:44,000 --> 00:58:50,799
are identical across all the variables

1003
00:58:46,799 --> 00:58:53,040
and so are the p values. So in terms of

1004
00:58:50,799 --> 00:58:56,640
interpreting how important different

1005
00:58:53,040 --> 00:58:59,680
variables are, we get the same judge

1006
00:58:56,640 --> 00:59:02,960
judgments of what's important in terms

1007
00:58:59,680 --> 00:59:05,359
of t values and p values. But what's

1008
00:59:02,960 --> 00:59:08,000
particularly useful though when we use

1009
00:59:05,359 --> 00:59:11,520
the standardized covariants

1010
00:59:08,000 --> 00:59:13,599
is that the magnitude

1011
00:59:11,520 --> 00:59:15,520
of the

1012
00:59:13,599 --> 00:59:18,480
coefficients when we have the

1013
00:59:15,520 --> 00:59:21,839
standardized scale that corresponds to

1014
00:59:18,480 --> 00:59:26,240
the impact of a one standard deviation

1015
00:59:21,839 --> 00:59:32,160
move of the independent variable.

1016
00:59:26,240 --> 00:59:36,160
So if we have a an age value that's one

1017
00:59:32,160 --> 00:59:38,960
standard deviation above the mean, it

1018
00:59:36,160 --> 00:59:43,119
doesn't have much of an impact

1019
00:59:38,960 --> 00:59:46,799
on the dependent variable. If we have

1020
00:59:43,119 --> 00:59:52,559
the SVI a one standard deviation move

1021
00:59:46,799 --> 00:59:55,040
above a zcore of one then um it has a

1022
00:59:52,559 --> 00:59:58,240
larger impact on on the dependent

1023
00:59:55,040 --> 01:00:00,880
variable. So this

1024
00:59:58,240 --> 01:00:06,119
standard deviation units can be a very

1025
01:00:00,880 --> 01:00:06,119
convenient way to rescale our data.

1026
01:00:07,760 --> 01:00:16,319
All right. Um now in in in judging the

1027
01:00:11,359 --> 01:00:20,559
quality of regression models um we can

1028
01:00:16,319 --> 01:00:23,760
calculate fitted values and pair those

1029
01:00:20,559 --> 01:00:25,839
with the observed values and look at the

1030
01:00:23,760 --> 01:00:31,520
scatter plot of observed versus fitted

1031
01:00:25,839 --> 01:00:35,520
values. And this simple scatter plot

1032
01:00:31,520 --> 01:00:37,760
will have a correlation statistic.

1033
01:00:35,520 --> 01:00:39,280
And the square of that correlation

1034
01:00:37,760 --> 01:00:42,240
statistic is actually called the

1035
01:00:39,280 --> 01:00:44,640
multiple R squared or a coefficient of

1036
01:00:42,240 --> 01:00:46,720
determination.

1037
01:00:44,640 --> 01:00:48,720
If you're familiar with simple linear

1038
01:00:46,720 --> 01:00:51,119
regression where you calculate

1039
01:00:48,720 --> 01:00:53,359
correlation statistics,

1040
01:00:51,119 --> 01:00:57,280
um you can think of the squared

1041
01:00:53,359 --> 01:00:59,839
correlation as being useful. In multiple

1042
01:00:57,280 --> 01:01:01,599
regression, we generalize a single

1043
01:00:59,839 --> 01:01:05,920
correlation

1044
01:01:01,599 --> 01:01:08,640
um statistic with the multiple R 2

1045
01:01:05,920 --> 01:01:12,160
coefficient. And so it basically tells

1046
01:01:08,640 --> 01:01:14,480
us how um predictable

1047
01:01:12,160 --> 01:01:17,440
our dependent variable is given the

1048
01:01:14,480 --> 01:01:23,359
independent variables.

1049
01:01:17,440 --> 01:01:26,319
Um now uh in terms of evaluating

1050
01:01:23,359 --> 01:01:29,680
the assumptions of our regression model,

1051
01:01:26,319 --> 01:01:34,720
there are regression diagnostics that we

1052
01:01:29,680 --> 01:01:38,160
can apply to uh fitted models and there

1053
01:01:34,720 --> 01:01:45,680
are a number of different important

1054
01:01:38,160 --> 01:01:48,079
uh measures that we can uh apply and uh

1055
01:01:45,680 --> 01:01:50,319
in the stats package There's this

1056
01:01:48,079 --> 01:01:53,040
influence measures

1057
01:01:50,319 --> 01:01:55,200
uh general function which gives a table

1058
01:01:53,040 --> 01:02:00,960
of different statistics.

1059
01:01:55,200 --> 01:02:05,119
Um there's this r student for computing

1060
01:02:00,960 --> 01:02:07,599
studentized residuals. Um if we have

1061
01:02:05,119 --> 01:02:11,599
epsilon hat is distributed as

1062
01:02:07,599 --> 01:02:16,880
multivariant normal with mean zero and

1063
01:02:11,599 --> 01:02:18,559
covariance matrix i - h * sigma squar

1064
01:02:16,880 --> 01:02:20,720
um

1065
01:02:18,559 --> 01:02:24,079
our residuals

1066
01:02:20,720 --> 01:02:27,520
may have very different variances

1067
01:02:24,079 --> 01:02:31,839
because of this

1068
01:02:27,520 --> 01:02:35,200
i minus h factor in the co-variance and

1069
01:02:31,839 --> 01:02:40,240
so Studentized residuals

1070
01:02:35,200 --> 01:02:43,920
um basically will divide the residuals

1071
01:02:40,240 --> 01:02:47,040
by the square root of that variance and

1072
01:02:43,920 --> 01:02:50,160
an estimate of that. And that estimate

1073
01:02:47,040 --> 01:02:53,920
of the variance that's in the divisor

1074
01:02:50,160 --> 01:02:56,880
leads to non-normal

1075
01:02:53,920 --> 01:03:00,640
but t distributions for the epsilon

1076
01:02:56,880 --> 01:03:03,119
hats. So we'll see that play out in a

1077
01:03:00,640 --> 01:03:07,200
moment. Um and then there are other

1078
01:03:03,119 --> 01:03:09,200
statistics like how much does the

1079
01:03:07,200 --> 01:03:11,599
regression parameter change if we

1080
01:03:09,200 --> 01:03:16,960
include or exclude different data

1081
01:03:11,599 --> 01:03:18,559
points, different cases in the data. Um

1082
01:03:16,960 --> 01:03:23,200
so let's just see some of the results.

1083
01:03:18,559 --> 01:03:28,799
So here's the student residuals

1084
01:03:23,200 --> 01:03:32,319
for this regression model. And we h have

1085
01:03:28,799 --> 01:03:36,000
a histogram which is unimodal

1086
01:03:32,319 --> 01:03:40,960
symmetrical. Here is the uh what's

1087
01:03:36,000 --> 01:03:46,240
called a uh quantile plot of the

1088
01:03:40,960 --> 01:03:48,240
residuals that um where the

1089
01:03:46,240 --> 01:03:51,039
quantile plot will follow a straight

1090
01:03:48,240 --> 01:03:56,559
line if the data are consistent with the

1091
01:03:51,039 --> 01:03:59,839
normal distribution model. and uh

1092
01:03:56,559 --> 01:04:02,559
the t distribution for normalizing the

1093
01:03:59,839 --> 01:04:07,119
residuals by estimates of the variance.

1094
01:04:02,559 --> 01:04:10,640
Um what's uh what's rather

1095
01:04:07,119 --> 01:04:13,200
useful when you use these uh methods for

1096
01:04:10,640 --> 01:04:16,559
different regression problems is that

1097
01:04:13,200 --> 01:04:18,160
you'll see that um there's sampling

1098
01:04:16,559 --> 01:04:21,200
variability

1099
01:04:18,160 --> 01:04:22,960
due to just the data set we we're

1100
01:04:21,200 --> 01:04:25,839
working with. If you were to collect a

1101
01:04:22,960 --> 01:04:29,520
new data set under identical conditions,

1102
01:04:25,839 --> 01:04:33,200
you would get different results. These

1103
01:04:29,520 --> 01:04:36,000
red bands correspond to bands you might

1104
01:04:33,200 --> 01:04:40,799
expect to see with variation from one uh

1105
01:04:36,000 --> 01:04:43,760
example to another. Now um this uh here

1106
01:04:40,799 --> 01:04:46,319
is a plot called the influence plot in

1107
01:04:43,760 --> 01:04:50,720
the car package

1108
01:04:46,319 --> 01:04:57,559
and uh it plots hat values versus

1109
01:04:50,720 --> 01:04:57,559
studentized residuals. Um the hat values

1110
01:04:57,839 --> 01:05:05,160
are

1111
01:05:00,319 --> 01:05:05,160
the diagonals of the H matrix.

1112
01:05:08,000 --> 01:05:13,920
And um

1113
01:05:10,480 --> 01:05:17,119
what's important about the hat values is

1114
01:05:13,920 --> 01:05:20,240
that um

1115
01:05:17,119 --> 01:05:23,920
let's see if the hat values were equal

1116
01:05:20,240 --> 01:05:28,920
to one. So if

1117
01:05:23,920 --> 01:05:28,920
say h subi i equals 1

1118
01:05:29,839 --> 01:05:36,160
then

1119
01:05:31,520 --> 01:05:38,319
we would have y i is actually equal to y

1120
01:05:36,160 --> 01:05:41,520
i.

1121
01:05:38,319 --> 01:05:43,200
And so the i case it would be the sort

1122
01:05:41,520 --> 01:05:45,119
of

1123
01:05:43,200 --> 01:05:49,440
only case in the data set that allows

1124
01:05:45,119 --> 01:05:52,960
you to estimate that um

1125
01:05:49,440 --> 01:05:56,640
that value. So, so that case would have

1126
01:05:52,960 --> 01:06:00,799
very high influence. So, hat values

1127
01:05:56,640 --> 01:06:07,000
generally um are some well are close to

1128
01:06:00,799 --> 01:06:07,000
P over N um and uh

1129
01:06:07,039 --> 01:06:14,000
lower values correspond to lower

1130
01:06:09,119 --> 01:06:18,960
influence and studentized residuals um

1131
01:06:14,000 --> 01:06:24,160
are uh given by these magnitudes.

1132
01:06:18,960 --> 01:06:27,440
Um and I believe what is drawn in this

1133
01:06:24,160 --> 01:06:31,200
slide is yeah is the circle size is

1134
01:06:27,440 --> 01:06:36,200
proportional to cook's distance and so

1135
01:06:31,200 --> 01:06:36,200
um let's see with cook's distance

1136
01:06:38,319 --> 01:06:48,400
if we have beta hat equal to

1137
01:06:42,079 --> 01:06:50,640
uh xrpose x inverse xrpose y

1138
01:06:48,400 --> 01:06:53,520
Um

1139
01:06:50,640 --> 01:06:56,880
we can think of beta hat sub minus i

1140
01:06:53,520 --> 01:07:00,400
which is the le squares estimate if we

1141
01:06:56,880 --> 01:07:04,559
were to exclude the i case.

1142
01:07:00,400 --> 01:07:08,280
Um so this corresponds to excluding

1143
01:07:04,559 --> 01:07:08,280
the i case.

1144
01:07:08,559 --> 01:07:15,359
Then well we have that this beta hat is

1145
01:07:13,200 --> 01:07:19,119
distributed multinnormal

1146
01:07:15,359 --> 01:07:21,440
with a mean vector the true beta and

1147
01:07:19,119 --> 01:07:23,920
covariance matrix

1148
01:07:21,440 --> 01:07:25,760
given by this.

1149
01:07:23,920 --> 01:07:30,480
Um,

1150
01:07:25,760 --> 01:07:36,680
we can actually look at beta hat minus

1151
01:07:30,480 --> 01:07:36,680
or beta hat I minus beta hat

1152
01:07:37,359 --> 01:07:40,599
and then

1153
01:07:43,359 --> 01:07:48,920
consider the distance of this

1154
01:07:49,359 --> 01:07:59,280
or the magnitude of

1155
01:07:53,520 --> 01:08:03,599
the change in the beta and this cook's

1156
01:07:59,280 --> 01:08:06,079
distance um will actually correspond

1157
01:08:03,599 --> 01:08:10,319
closely to sort of a kai squared

1158
01:08:06,079 --> 01:08:13,200
distribution because of um the

1159
01:08:10,319 --> 01:08:15,760
relationship. Um so we're basically

1160
01:08:13,200 --> 01:08:19,920
looking at the distance of beta hat I

1161
01:08:15,760 --> 01:08:24,319
from the true beta um and normalizing

1162
01:08:19,920 --> 01:08:27,440
that by the coariance matrix.

1163
01:08:24,319 --> 01:08:29,839
Now um let's see and there there's

1164
01:08:27,440 --> 01:08:33,759
basically a whole slew of different

1165
01:08:29,839 --> 01:08:36,080
diagnostics that we can graph and I

1166
01:08:33,759 --> 01:08:39,040
think it's very useful to have graphical

1167
01:08:36,080 --> 01:08:44,239
methods to highlight what's perhaps

1168
01:08:39,040 --> 01:08:49,839
important. Um in looking at this uh

1169
01:08:44,239 --> 01:08:54,000
plotting from the car package or plot.lm

1170
01:08:49,839 --> 01:08:56,560
um we can see how the residuals compare

1171
01:08:54,000 --> 01:09:00,239
with the fitted values

1172
01:08:56,560 --> 01:09:03,199
and we don't like to see any systematic

1173
01:09:00,239 --> 01:09:05,839
pattern in that plot. We'd like that

1174
01:09:03,199 --> 01:09:08,319
just to be flat. How that will change

1175
01:09:05,839 --> 01:09:10,960
sometimes is that as the fitted values

1176
01:09:08,319 --> 01:09:14,239
increase the residuals might increase in

1177
01:09:10,960 --> 01:09:16,239
magnitude as well. So that would suggest

1178
01:09:14,239 --> 01:09:19,520
that the residuals

1179
01:09:16,239 --> 01:09:22,640
uh are dependent on the magnitude of the

1180
01:09:19,520 --> 01:09:25,600
fitted values. Okay. We can also look at

1181
01:09:22,640 --> 01:09:28,960
the normal QQ plot to see whether the

1182
01:09:25,600 --> 01:09:35,440
data consistent with normal or not. And

1183
01:09:28,960 --> 01:09:40,000
then one can look at the um the scale

1184
01:09:35,440 --> 01:09:43,279
uh of the uh the scale of measures of

1185
01:09:40,000 --> 01:09:45,600
the scale of the residual. So we can

1186
01:09:43,279 --> 01:09:49,120
take the square root of the standardized

1187
01:09:45,600 --> 01:09:53,600
residuals or the magnitude

1188
01:09:49,120 --> 01:09:55,600
and see whether those have variation

1189
01:09:53,600 --> 01:09:58,239
depending upon whether the fitted values

1190
01:09:55,600 --> 01:10:00,080
are small or large. Here it looks like

1191
01:09:58,239 --> 01:10:03,280
there may be a nonlinear relationship

1192
01:10:00,080 --> 01:10:08,159
there. When we discover relationships

1193
01:10:03,280 --> 01:10:10,480
then that u leads us to sort of refine

1194
01:10:08,159 --> 01:10:12,080
our model assumptions.

1195
01:10:10,480 --> 01:10:14,239
Um

1196
01:10:12,080 --> 01:10:17,600
let's see this

1197
01:10:14,239 --> 01:10:21,920
uh residual plots function

1198
01:10:17,600 --> 01:10:24,560
is uh actually a function that looks at

1199
01:10:21,920 --> 01:10:28,159
the sensitivity of our linear regression

1200
01:10:24,560 --> 01:10:31,520
model to potential nonlinearity

1201
01:10:28,159 --> 01:10:34,480
in the model dependence on on

1202
01:10:31,520 --> 01:10:38,640
independent factors. And so it actually

1203
01:10:34,480 --> 01:10:41,760
tries to fit curvature

1204
01:10:38,640 --> 01:10:44,480
terms in

1205
01:10:41,760 --> 01:10:46,719
to the residuals in the model. And if

1206
01:10:44,480 --> 01:10:50,239
there's residual pre if there's

1207
01:10:46,719 --> 01:10:53,199
curvature present in the residuals, then

1208
01:10:50,239 --> 01:10:58,239
we might have a nonlinear model and need

1209
01:10:53,199 --> 01:11:00,640
to expand with nonlinear terms.

1210
01:10:58,239 --> 01:11:03,360
Okay. Um

1211
01:11:00,640 --> 01:11:06,480
let's see. Well,

1212
01:11:03,360 --> 01:11:08,480
with this normal linear model,

1213
01:11:06,480 --> 01:11:12,719
um,

1214
01:11:08,480 --> 01:11:16,159
we can consider the assumptions

1215
01:11:12,719 --> 01:11:18,159
underlying that minus normality and

1216
01:11:16,159 --> 01:11:22,480
those are called the Gaus Markov

1217
01:11:18,159 --> 01:11:26,320
assumptions. So uh if we have our

1218
01:11:22,480 --> 01:11:32,800
regression model in our for our y vector

1219
01:11:26,320 --> 01:11:36,320
and x matrix um we set that up as the

1220
01:11:32,800 --> 01:11:39,679
conditional expectation is x beta and

1221
01:11:36,320 --> 01:11:42,560
the covariance matrix is equal to sigma^

1222
01:11:39,679 --> 01:11:47,520
2 times the identity

1223
01:11:42,560 --> 01:11:49,360
and with this gaus marov assumptions

1224
01:11:47,520 --> 01:11:52,880
there's a really important

1225
01:11:49,360 --> 01:11:54,640
theorem in uh linear models which is

1226
01:11:52,880 --> 01:11:57,760
that

1227
01:11:54,640 --> 01:12:01,199
the least squares estimates

1228
01:11:57,760 --> 01:12:04,800
of the regression parameters

1229
01:12:01,199 --> 01:12:06,960
give us the best estimates of any linear

1230
01:12:04,800 --> 01:12:10,719
combination of the true regression

1231
01:12:06,960 --> 01:12:13,520
parameters. So if we want to estimate a

1232
01:12:10,719 --> 01:12:16,239
parameter theta which is a linear

1233
01:12:13,520 --> 01:12:19,600
combination of the regression parameters

1234
01:12:16,239 --> 01:12:22,480
then if the gaus markoff assumptions are

1235
01:12:19,600 --> 01:12:26,239
satisfied then plugging in the le

1236
01:12:22,480 --> 01:12:28,239
squares estimates provides an unbiased

1237
01:12:26,239 --> 01:12:31,679
estimator

1238
01:12:28,239 --> 01:12:34,239
that is also

1239
01:12:31,679 --> 01:12:36,000
the unbiased estimator with the smallest

1240
01:12:34,239 --> 01:12:39,600
variability.

1241
01:12:36,000 --> 01:12:43,120
And these are called uh best linear

1242
01:12:39,600 --> 01:12:46,080
unbiased estimates. Um and what's

1243
01:12:43,120 --> 01:12:48,800
important well I mean as as it's framed

1244
01:12:46,080 --> 01:12:52,239
here it's quite general but think about

1245
01:12:48,800 --> 01:12:56,560
this if if we have the constants C1

1246
01:12:52,239 --> 01:13:00,560
through CP correspond to specific

1247
01:12:56,560 --> 01:13:02,239
values of the explanatory variables

1248
01:13:00,560 --> 01:13:05,840
corresponding to a different case. then

1249
01:13:02,239 --> 01:13:08,719
we can be estimating the sort of true

1250
01:13:05,840 --> 01:13:11,280
mean value for a given case of the X

1251
01:13:08,719 --> 01:13:14,960
variables.

1252
01:13:11,280 --> 01:13:17,920
If we have two X

1253
01:13:14,960 --> 01:13:20,239
values, X vectors for different cases

1254
01:13:17,920 --> 01:13:24,800
and we're interested in what's the

1255
01:13:20,239 --> 01:13:28,320
difference in it in val in mean values

1256
01:13:24,800 --> 01:13:30,400
for those two cases, then we can have

1257
01:13:28,320 --> 01:13:32,560
these C's represent the difference in

1258
01:13:30,400 --> 01:13:35,360
the X's. and we're estimating the

1259
01:13:32,560 --> 01:13:38,080
difference in y values, the difference

1260
01:13:35,360 --> 01:13:42,480
in mean values for different cases. So

1261
01:13:38,080 --> 01:13:44,480
it really is a very very general theorem

1262
01:13:42,480 --> 01:13:47,360
and

1263
01:13:44,480 --> 01:13:50,880
while it's a very wonderful theorem, it

1264
01:13:47,360 --> 01:13:55,360
depends on the Gaus Markov assumptions.

1265
01:13:50,880 --> 01:13:59,199
So what we can do is consider

1266
01:13:55,360 --> 01:14:01,040
generalizing the Gaus Markov assumptions

1267
01:13:59,199 --> 01:14:06,159
to

1268
01:14:01,040 --> 01:14:09,280
again have zero mean errors but have the

1269
01:14:06,159 --> 01:14:13,199
co-variance matrix of the errors be

1270
01:14:09,280 --> 01:14:15,760
given by sigma squared times a capital

1271
01:14:13,199 --> 01:14:18,159
sigma matrix.

1272
01:14:15,760 --> 01:14:23,040
And

1273
01:14:18,159 --> 01:14:24,800
this kind of covariant structure in

1274
01:14:23,040 --> 01:14:27,280
error terms

1275
01:14:24,800 --> 01:14:29,280
actually arises frequently in time

1276
01:14:27,280 --> 01:14:34,320
series modeling

1277
01:14:29,280 --> 01:14:36,719
where we're ordering our data by time

1278
01:14:34,320 --> 01:14:40,080
and maybe near

1279
01:14:36,719 --> 01:14:43,280
uh values of the error that are observed

1280
01:14:40,080 --> 01:14:46,960
at uh nearby times are more correlated

1281
01:14:43,280 --> 01:14:48,880
than those at far away times and the

1282
01:14:46,960 --> 01:14:52,080
covariant structure of those errors

1283
01:14:48,880 --> 01:14:54,320
might be systematically

1284
01:14:52,080 --> 01:14:58,159
represented by

1285
01:14:54,320 --> 01:15:00,800
a multiple of a known sigma matrix.

1286
01:14:58,159 --> 01:15:04,960
Well, if this is the case,

1287
01:15:00,800 --> 01:15:06,560
we can transform our original data

1288
01:15:04,960 --> 01:15:09,040
by

1289
01:15:06,560 --> 01:15:10,719
premultiplying by

1290
01:15:09,040 --> 01:15:14,960
the

1291
01:15:10,719 --> 01:15:18,320
inverse square root of sigma and do the

1292
01:15:14,960 --> 01:15:21,440
same for our x matrix.

1293
01:15:18,320 --> 01:15:26,080
And if we do that, we're basically

1294
01:15:21,440 --> 01:15:29,120
transforming our model to the starred

1295
01:15:26,080 --> 01:15:33,040
case where

1296
01:15:29,120 --> 01:15:36,640
the Y star and XAR as we've

1297
01:15:33,040 --> 01:15:40,800
represented here. The epsilon star will

1298
01:15:36,640 --> 01:15:46,320
have mean zero and the covariance matrix

1299
01:15:40,800 --> 01:15:49,600
of the epsilon stars will actually be

1300
01:15:46,320 --> 01:15:53,440
diagonal with constant variance. So in

1301
01:15:49,600 --> 01:15:57,280
this transformed case we have the same

1302
01:15:53,440 --> 01:16:01,600
regression parameters. we have

1303
01:15:57,280 --> 01:16:03,760
the epsilon stars with the uh

1304
01:16:01,600 --> 01:16:07,840
with the gaus marov assumptions being

1305
01:16:03,760 --> 01:16:10,480
satisfied. So by the Gaus Marov theorem

1306
01:16:07,840 --> 01:16:13,679
we just can write directly our le

1307
01:16:10,480 --> 01:16:17,520
squares estimate in terms of the x stars

1308
01:16:13,679 --> 01:16:21,600
and y stars. And when we write that out

1309
01:16:17,520 --> 01:16:23,520
it ends up giving us this formula for

1310
01:16:21,600 --> 01:16:27,760
the generalized

1311
01:16:23,520 --> 01:16:32,080
uh le squares estimate. And so if we

1312
01:16:27,760 --> 01:16:34,800
have variable errors where we know the

1313
01:16:32,080 --> 01:16:37,120
relative variation of those errors, we

1314
01:16:34,800 --> 01:16:40,000
can accommodate that and get best

1315
01:16:37,120 --> 01:16:45,920
estimates with this generalized le

1316
01:16:40,000 --> 01:16:47,679
squares uh formula. And importantly,

1317
01:16:45,920 --> 01:16:50,159
the

1318
01:16:47,679 --> 01:16:52,000
generalized le squares estimate

1319
01:16:50,159 --> 01:16:54,080
basically does sort of a weighted

1320
01:16:52,000 --> 01:16:58,560
regression

1321
01:16:54,080 --> 01:17:00,640
where we weight cases

1322
01:16:58,560 --> 01:17:05,840
proportional to the inverse of the

1323
01:17:00,640 --> 01:17:09,920
variance of the random variable. So if

1324
01:17:05,840 --> 01:17:13,760
this sigma matrix in fact were diagonal

1325
01:17:09,920 --> 01:17:19,679
with different variances, we would be

1326
01:17:13,760 --> 01:17:22,080
downweing those cases with high variance

1327
01:17:19,679 --> 01:17:23,440
and uh

1328
01:17:22,080 --> 01:17:26,239
performing the least squares

1329
01:17:23,440 --> 01:17:28,239
computations you know in a weighted way.

1330
01:17:26,239 --> 01:17:30,640
So that's the generalized le squares

1331
01:17:28,239 --> 01:17:35,679
estimate. All right. Well, we'll finish

1332
01:17:30,640 --> 01:17:40,600
there uh for today and uh

1333
01:17:35,679 --> 01:17:40,600
cover more regression next time.

