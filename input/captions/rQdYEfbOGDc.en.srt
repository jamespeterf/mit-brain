1
00:00:05,279 --> 00:00:08,880
We're going to move to our first

2
00:00:06,319 --> 00:00:11,759
speaker. Um, so Dan, if you want to join

3
00:00:08,880 --> 00:00:14,080
us. So I'm a somewhat lapsed computer

4
00:00:11,759 --> 00:00:16,160
scientist. Uh, so it's with absolute

5
00:00:14,080 --> 00:00:19,760
pleasure that I'd like to introduce uh

6
00:00:16,160 --> 00:00:22,880
Dan Hulocker who's the um inaugural dean

7
00:00:19,760 --> 00:00:25,439
of the sportsman's MIT sportsman college

8
00:00:22,880 --> 00:00:28,480
of computing. um also found helpful and

9
00:00:25,439 --> 00:00:31,279
found Cornelltech uh former C fintech

10
00:00:28,480 --> 00:00:32,880
CTR I believe fellow of the ACM uh

11
00:00:31,279 --> 00:00:36,800
distinguished publishing career produced

12
00:00:32,880 --> 00:00:39,600
a book with Eric Schmidt and uh the late

13
00:00:36,800 --> 00:00:42,320
Henry K. Late Henry Kissinger. Yeah. Um

14
00:00:39,600 --> 00:00:46,040
and I'll pass over to D. Thanks so much.

15
00:00:42,320 --> 00:00:46,040
Delightful to be here.

16
00:00:47,530 --> 00:00:50,399
[Applause]

17
00:00:48,800 --> 00:00:52,399
Terrific to have the opportunity to be

18
00:00:50,399 --> 00:00:55,520
with you all this morning. And actually

19
00:00:52,399 --> 00:00:57,520
one of the things I was noting uh as I

20
00:00:55,520 --> 00:01:00,960
was listening to the introduction there

21
00:00:57,520 --> 00:01:04,080
is uh how much the Schwarzman College of

22
00:01:00,960 --> 00:01:06,159
Computing is changing MIT. Uh many of

23
00:01:04,080 --> 00:01:08,640
the actually the vast majority of the

24
00:01:06,159 --> 00:01:10,240
people you'll hear from today uh are

25
00:01:08,640 --> 00:01:13,360
part of the college and affiliated with

26
00:01:10,240 --> 00:01:16,320
the college. So um so I always start

27
00:01:13,360 --> 00:01:17,640
with this slide because I think uh it

28
00:01:16,320 --> 00:01:20,479
just

29
00:01:17,640 --> 00:01:22,799
um we are constantly confused about what

30
00:01:20,479 --> 00:01:25,200
artificial intelligence actually is.

31
00:01:22,799 --> 00:01:27,600
It's a technology fundamentally it's a

32
00:01:25,200 --> 00:01:29,920
technology and it does things that

33
00:01:27,600 --> 00:01:31,920
technologies have done for a while plus

34
00:01:29,920 --> 00:01:33,439
things that technologies have only

35
00:01:31,920 --> 00:01:35,960
recently started to do with the advent

36
00:01:33,439 --> 00:01:38,640
of AI. But what's different about modern

37
00:01:35,960 --> 00:01:42,079
AI is that rather than being, you know,

38
00:01:38,640 --> 00:01:44,560
carefully handcoded by uh humans, it's

39
00:01:42,079 --> 00:01:46,079
trained to perform a task. Now, the way

40
00:01:44,560 --> 00:01:48,720
it's trained is very carefully

41
00:01:46,079 --> 00:01:51,680
handdeveloped by humans, but the the AI

42
00:01:48,720 --> 00:01:53,840
itself is not. And this yields results

43
00:01:51,680 --> 00:01:55,759
that are very humanlike. They are

44
00:01:53,840 --> 00:01:57,280
results that are imprecise, that are

45
00:01:55,759 --> 00:01:58,880
adaptive, that can have emergent

46
00:01:57,280 --> 00:02:01,200
behaviors. And if you think of

47
00:01:58,880 --> 00:02:02,640
conventional software, it's almost the

48
00:02:01,200 --> 00:02:04,280
opposite of that, right? it's sort of

49
00:02:02,640 --> 00:02:06,799
rigid and and and

50
00:02:04,280 --> 00:02:09,479
inflexible. And machine learning is the

51
00:02:06,799 --> 00:02:12,400
way that we train uh uh

52
00:02:09,479 --> 00:02:14,720
AI. And almost every model that we have

53
00:02:12,400 --> 00:02:16,520
today is some form of a deep neural

54
00:02:14,720 --> 00:02:19,760
network generally with billions of

55
00:02:16,520 --> 00:02:21,040
parameters. Um I think until recently we

56
00:02:19,760 --> 00:02:23,760
were in kind of a world where more

57
00:02:21,040 --> 00:02:25,280
parameters was better. Uh I'm not sure

58
00:02:23,760 --> 00:02:27,520
we're completely in net world anymore.

59
00:02:25,280 --> 00:02:29,440
We're getting smaller parameter models

60
00:02:27,520 --> 00:02:31,840
often distilled from ones that had many

61
00:02:29,440 --> 00:02:34,640
many more parameters uh that are very

62
00:02:31,840 --> 00:02:36,640
very powerful in many settings and these

63
00:02:34,640 --> 00:02:39,480
are learned just by adjusting a bunch of

64
00:02:36,640 --> 00:02:42,000
weights to improve some measure of

65
00:02:39,480 --> 00:02:43,840
performance. So at a high level there's

66
00:02:42,000 --> 00:02:46,200
not much magic to that but there's a lot

67
00:02:43,840 --> 00:02:48,200
of magic in how that's actually

68
00:02:46,200 --> 00:02:51,040
done.

69
00:02:48,200 --> 00:02:52,480
Um though there are three types of uh

70
00:02:51,040 --> 00:02:54,160
machine learning that are just worth

71
00:02:52,480 --> 00:02:56,319
kind of having in mind. So one is

72
00:02:54,160 --> 00:02:58,560
supervised learning and that's where

73
00:02:56,319 --> 00:03:00,160
some human generally somebody went

74
00:02:58,560 --> 00:03:01,760
through and labeled a bunch of the data

75
00:03:00,160 --> 00:03:03,280
like this is a picture of a car, this is

76
00:03:01,760 --> 00:03:05,440
a picture of a dog, this is a picture of

77
00:03:03,280 --> 00:03:07,200
a cat. I'll often make computer vision

78
00:03:05,440 --> 00:03:10,280
type references because that's the area

79
00:03:07,200 --> 00:03:12,800
in which I've worked the most as a as a

80
00:03:10,280 --> 00:03:14,319
scholar. And supervised learning has

81
00:03:12,800 --> 00:03:17,680
been around since the very beginning of

82
00:03:14,319 --> 00:03:20,159
of of uh of AI techniques. And then

83
00:03:17,680 --> 00:03:21,599
unsupervised learning you can think of

84
00:03:20,159 --> 00:03:23,040
as sort of clustering. you've got a

85
00:03:21,599 --> 00:03:24,800
bunch of data and you try to look at

86
00:03:23,040 --> 00:03:27,319
sort of inherent properties of that data

87
00:03:24,800 --> 00:03:30,720
to group things

88
00:03:27,319 --> 00:03:32,319
together. What's much more recent is

89
00:03:30,720 --> 00:03:34,120
self-supervised learning and this is

90
00:03:32,319 --> 00:03:37,120
what's been driving most of the

91
00:03:34,120 --> 00:03:38,799
advances. Uh so things like masked

92
00:03:37,120 --> 00:03:42,239
autoenccoders

93
00:03:38,799 --> 00:03:43,959
uh um uh of various forms uh

94
00:03:42,239 --> 00:03:46,400
transformers are a type of masked

95
00:03:43,959 --> 00:03:48,239
autoenccoder uh are self-supervised and

96
00:03:46,400 --> 00:03:51,280
the idea of self-supervised learning is

97
00:03:48,239 --> 00:03:53,440
that if you the data has any structure

98
00:03:51,280 --> 00:03:56,080
to it which most data does you can

99
00:03:53,440 --> 00:03:58,319
actually use properties of the data to

100
00:03:56,080 --> 00:04:00,400
perform the supervision. So the simple

101
00:03:58,319 --> 00:04:02,560
example that probably most of you have

102
00:04:00,400 --> 00:04:04,080
seen is with transformers. If you're

103
00:04:02,560 --> 00:04:06,159
trying to do something like predict the

104
00:04:04,080 --> 00:04:08,560
next word, you can use the words

105
00:04:06,159 --> 00:04:11,439
beforehand as the training for that next

106
00:04:08,560 --> 00:04:13,040
word that's unknown. Uh and this is

107
00:04:11,439 --> 00:04:14,720
really what's driven uh because it

108
00:04:13,040 --> 00:04:16,280
allows you to train at a huge scale

109
00:04:14,720 --> 00:04:18,239
compared to any other form of

110
00:04:16,280 --> 00:04:20,320
supervision. And then once you have

111
00:04:18,239 --> 00:04:22,000
these models, conventionally they were

112
00:04:20,320 --> 00:04:24,479
used for some sort of classification

113
00:04:22,000 --> 00:04:26,000
prediction type of task. So, you know,

114
00:04:24,479 --> 00:04:27,199
you could look at an image like this,

115
00:04:26,000 --> 00:04:28,880
run it through one of these networks

116
00:04:27,199 --> 00:04:33,280
after it been trained and, you know, out

117
00:04:28,880 --> 00:04:35,440
pops car. Uh, and the sort of magic of

118
00:04:33,280 --> 00:04:37,040
the last, say, seven, eight years has

119
00:04:35,440 --> 00:04:39,520
been you can just sort of essentially

120
00:04:37,040 --> 00:04:41,520
turn this process around and you can

121
00:04:39,520 --> 00:04:44,880
start with the label and this train

122
00:04:41,520 --> 00:04:46,400
network and generate an output and then

123
00:04:44,880 --> 00:04:49,199
the pace at which that has been

124
00:04:46,400 --> 00:04:51,240
advancing over the last five years uh is

125
00:04:49,199 --> 00:04:53,360
really uh nothing short of completely

126
00:04:51,240 --> 00:04:55,680
stunning. Uh, and you know, there's

127
00:04:53,360 --> 00:04:57,360
weird artifacts in these images, uh,

128
00:04:55,680 --> 00:04:59,680
but, uh, but they're they're quite

129
00:04:57,360 --> 00:05:02,560
interesting. I also wanted to touch on

130
00:04:59,680 --> 00:05:05,600
AI agents here for a minute. Um, because

131
00:05:02,560 --> 00:05:09,440
there's a lot of hype out there right

132
00:05:05,600 --> 00:05:11,039
now about AI and, uh, you know, I think

133
00:05:09,440 --> 00:05:13,680
maybe I'll comment on the hype cycle

134
00:05:11,039 --> 00:05:15,960
more generally in a moment. Uh, but you

135
00:05:13,680 --> 00:05:20,320
know, today's generative AI models at

136
00:05:15,960 --> 00:05:22,240
least are not so great at being agents.

137
00:05:20,320 --> 00:05:24,160
They're trained on lots of text and

138
00:05:22,240 --> 00:05:27,160
images which makes them good at

139
00:05:24,160 --> 00:05:30,560
producing surprise text and

140
00:05:27,160 --> 00:05:32,479
images and producing is very producing

141
00:05:30,560 --> 00:05:36,080
sort of output is very different from

142
00:05:32,479 --> 00:05:37,520
doing things. So in fact I asked uh a

143
00:05:36,080 --> 00:05:39,120
generative AI to just give me a

144
00:05:37,520 --> 00:05:41,360
cartoon-like image of someone who can

145
00:05:39,120 --> 00:05:43,600
talk big but can't get anything done.

146
00:05:41,360 --> 00:05:46,400
And among human beings we're very aware

147
00:05:43,600 --> 00:05:48,880
of those kinds of people. uh and that's

148
00:05:46,400 --> 00:05:50,320
sort of what these current generative

149
00:05:48,880 --> 00:05:52,960
eyes have been trained to do. They're

150
00:05:50,320 --> 00:05:54,639
trained to talk big uh with text and

151
00:05:52,960 --> 00:05:56,639
with images to produce those kinds of

152
00:05:54,639 --> 00:05:58,400
outputs, but it's much harder to get

153
00:05:56,639 --> 00:06:00,800
them to get things done. So when you

154
00:05:58,400 --> 00:06:02,639
think about AI agents, it's actually

155
00:06:00,800 --> 00:06:04,319
relatively challenging with today's

156
00:06:02,639 --> 00:06:06,400
training techniques to get them to work

157
00:06:04,319 --> 00:06:07,600
very reliably. Now, of course, this is a

158
00:06:06,400 --> 00:06:10,160
place where there's a lot of research

159
00:06:07,600 --> 00:06:13,039
and a lot of uh engineering innovation

160
00:06:10,160 --> 00:06:15,039
going on. uh so that state of the world

161
00:06:13,039 --> 00:06:16,560
may change quite a bit uh in the next

162
00:06:15,039 --> 00:06:19,199
couple of years and one area that I'm

163
00:06:16,560 --> 00:06:20,560
fairly familiar with is robotics uh

164
00:06:19,199 --> 00:06:22,960
where there are a lot of really

165
00:06:20,560 --> 00:06:24,960
interesting advances going on where

166
00:06:22,960 --> 00:06:27,199
these systems are being trained with

167
00:06:24,960 --> 00:06:29,440
actions not just observations right if

168
00:06:27,199 --> 00:06:30,880
you look at texts that humans have

169
00:06:29,440 --> 00:06:33,080
produced if you look at images that

170
00:06:30,880 --> 00:06:37,360
people have taken uh that's just

171
00:06:33,080 --> 00:06:39,120
observing things and actions uh would

172
00:06:37,360 --> 00:06:41,199
look at causes and effects and looking

173
00:06:39,120 --> 00:06:43,759
at the whole chain of action uh and in

174
00:06:41,199 --> 00:06:45,840
robotics that kind of training is now

175
00:06:43,759 --> 00:06:48,960
started to produce very very interesting

176
00:06:45,840 --> 00:06:52,080
kinds of results. Um but it it's at

177
00:06:48,960 --> 00:06:54,880
least currently can often be relatively

178
00:06:52,080 --> 00:06:57,919
heavily supervised. So it's not scaling

179
00:06:54,880 --> 00:07:00,639
at the same in the same way as uh as

180
00:06:57,919 --> 00:07:05,680
these self-supervised techniques. Um,

181
00:07:00,639 --> 00:07:08,080
but I I think that will also probably uh

182
00:07:05,680 --> 00:07:11,199
not become a bottleneck soon because

183
00:07:08,080 --> 00:07:12,960
simulated uh actions in robotics are

184
00:07:11,199 --> 00:07:15,039
proving to actually be quite useful in

185
00:07:12,960 --> 00:07:16,800
training and you can scale simulations a

186
00:07:15,039 --> 00:07:18,520
lot more easily than you can scale

187
00:07:16,800 --> 00:07:20,199
humans training

188
00:07:18,520 --> 00:07:22,560
something.

189
00:07:20,199 --> 00:07:24,240
So again, this is something I like to

190
00:07:22,560 --> 00:07:26,479
make sure to always say when I'm talking

191
00:07:24,240 --> 00:07:28,639
about AI. It's a distorted view of the

192
00:07:26,479 --> 00:07:30,319
world. It's not inherently good or bad.

193
00:07:28,639 --> 00:07:32,319
People like to put lots of value

194
00:07:30,319 --> 00:07:34,880
judgments on AI's destroying the world,

195
00:07:32,319 --> 00:07:36,639
AI saving the world. Uh we should be

196
00:07:34,880 --> 00:07:39,520
really timid about using it. We should

197
00:07:36,639 --> 00:07:41,199
be incredibly audacious about using it.

198
00:07:39,520 --> 00:07:43,120
I think what we have to be is attentive

199
00:07:41,199 --> 00:07:45,440
about using it because it is a different

200
00:07:43,120 --> 00:07:47,039
view. And it's one that sometimes we can

201
00:07:45,440 --> 00:07:48,560
confuse ourselves into thinking looks

202
00:07:47,039 --> 00:07:50,880
like our view. And I'll sort of say a

203
00:07:48,560 --> 00:07:53,440
few more things about that. So depending

204
00:07:50,880 --> 00:07:55,840
how it's trained, it can reinforce

205
00:07:53,440 --> 00:07:57,680
errors, it can reinforce human biases,

206
00:07:55,840 --> 00:07:59,680
or it can make decisions better or

207
00:07:57,680 --> 00:08:02,080
fairer. And you know, I think much of

208
00:07:59,680 --> 00:08:04,639
what you see out there just takes one of

209
00:08:02,080 --> 00:08:06,960
those two positions as if it's gospel

210
00:08:04,639 --> 00:08:09,199
and doesn't even recognize the other.

211
00:08:06,960 --> 00:08:11,360
And I pretty much dismiss anything that

212
00:08:09,199 --> 00:08:13,120
just talks about how unfair AI is or how

213
00:08:11,360 --> 00:08:14,639
much it just makes decisions better

214
00:08:13,120 --> 00:08:15,720
because it's the balance between those

215
00:08:14,639 --> 00:08:18,240
that's

216
00:08:15,720 --> 00:08:19,599
important. It also depends on how it's

217
00:08:18,240 --> 00:08:21,599
used. So there have been some very

218
00:08:19,599 --> 00:08:24,639
interesting studies in the last few

219
00:08:21,599 --> 00:08:26,319
years looking at using uh automatic

220
00:08:24,639 --> 00:08:28,000
decision-making techniques in the

221
00:08:26,319 --> 00:08:30,479
judicial system and in the health care

222
00:08:28,000 --> 00:08:32,800
system and depending on how you use

223
00:08:30,479 --> 00:08:35,200
those if the AI presents itself as an

224
00:08:32,800 --> 00:08:38,320
expert it can actually make decisions

225
00:08:35,200 --> 00:08:40,000
worse. If it presents itself as you know

226
00:08:38,320 --> 00:08:42,560
sort of like you know in the medical

227
00:08:40,000 --> 00:08:43,839
case sort of a consult then so it's

228
00:08:42,560 --> 00:08:45,440
supposed to be thinking together with

229
00:08:43,839 --> 00:08:48,440
the doctor it can actually make better

230
00:08:45,440 --> 00:08:50,279
outcomes. So the usage is really

231
00:08:48,440 --> 00:08:55,040
important.

232
00:08:50,279 --> 00:08:58,560
Um so for many centuries now we've

233
00:08:55,040 --> 00:09:02,160
understood the world in two ways and

234
00:08:58,560 --> 00:09:05,120
that is changing. So we've we've had

235
00:09:02,160 --> 00:09:07,040
reason which has been human and we've

236
00:09:05,120 --> 00:09:09,120
had faith in the divine. And what's

237
00:09:07,040 --> 00:09:13,040
happening with AI is it's starting to

238
00:09:09,120 --> 00:09:15,519
exhibit some form of reason. Um and it's

239
00:09:13,040 --> 00:09:18,839
really critical that we always remember

240
00:09:15,519 --> 00:09:21,839
this is not human reason underscored

241
00:09:18,839 --> 00:09:24,320
stop think because it's very easy we're

242
00:09:21,839 --> 00:09:27,360
only you know we have you know millennia

243
00:09:24,320 --> 00:09:30,160
of only interacting with human reason uh

244
00:09:27,360 --> 00:09:31,920
and this is a different form of reason

245
00:09:30,160 --> 00:09:35,120
and really it's adding a new way to

246
00:09:31,920 --> 00:09:37,040
understanding the world and this raises

247
00:09:35,120 --> 00:09:39,839
very interesting philosophical kinds of

248
00:09:37,040 --> 00:09:41,120
questions and in fact um one of the

249
00:09:39,839 --> 00:09:42,640
things that we've been doing in the

250
00:09:41,120 --> 00:09:44,720
Schwarzman College of Computing and I'll

251
00:09:42,640 --> 00:09:47,440
come to that uh in a couple of minutes

252
00:09:44,720 --> 00:09:49,120
is really trying to bring together uh

253
00:09:47,440 --> 00:09:51,120
you know philosophers and other

254
00:09:49,120 --> 00:09:53,279
humanists and social scientists who are

255
00:09:51,120 --> 00:09:58,560
interested in these sorts of questions

256
00:09:53,279 --> 00:09:58,560
with computer scientists uh and and AI

257
00:09:58,680 --> 00:10:05,080
researchers AI also one of the things

258
00:10:01,600 --> 00:10:08,080
where you know it can go either way

259
00:10:05,080 --> 00:10:12,320
is the degree of agency that we have as

260
00:10:08,080 --> 00:10:14,279
humans. So um this is not unique to AI.

261
00:10:12,320 --> 00:10:16,480
If you think about automation and

262
00:10:14,279 --> 00:10:19,279
mechanization going on for a century and

263
00:10:16,480 --> 00:10:22,000
a half at least same kind of thing if

264
00:10:19,279 --> 00:10:23,839
you work on an assembly line there's not

265
00:10:22,000 --> 00:10:26,480
a whole lot of autonomy there. You are

266
00:10:23,839 --> 00:10:29,040
literally a cog in the system but you

267
00:10:26,480 --> 00:10:30,959
know power tools give people incredible

268
00:10:29,040 --> 00:10:33,440
amount of ability to do things compared

269
00:10:30,959 --> 00:10:36,240
to having to do things by hand. This may

270
00:10:33,440 --> 00:10:40,600
be more fundamental for AI uh than it is

271
00:10:36,240 --> 00:10:42,720
for uh other um uh other kinds of of of

272
00:10:40,600 --> 00:10:46,079
technologies because it's sort of about

273
00:10:42,720 --> 00:10:48,160
how we think. So an example to make this

274
00:10:46,079 --> 00:10:49,760
very clear is a driver of navigation

275
00:10:48,160 --> 00:10:51,120
systems which all have some kind of

276
00:10:49,760 --> 00:10:52,320
machine learning and prediction in them.

277
00:10:51,120 --> 00:10:54,399
That's how they're telling you where to

278
00:10:52,320 --> 00:10:56,120
go. They can be very empowering or

279
00:10:54,399 --> 00:10:58,440
disempowering. So as an

280
00:10:56,120 --> 00:11:01,040
individual, you're still making the

281
00:10:58,440 --> 00:11:02,880
decisions. as a ride share driver all

282
00:11:01,040 --> 00:11:04,800
this is starting to change now it's

283
00:11:02,880 --> 00:11:06,240
really interesting but you know at the

284
00:11:04,800 --> 00:11:07,600
beginning the ride share drivers had

285
00:11:06,240 --> 00:11:10,560
freedom to go anywhere and that didn't

286
00:11:07,600 --> 00:11:12,320
work very well um so then the ride

287
00:11:10,560 --> 00:11:14,959
sharing software they had to follow the

288
00:11:12,320 --> 00:11:17,839
route prescribed by the AI now they have

289
00:11:14,959 --> 00:11:20,320
a little bit of flexibility um and but

290
00:11:17,839 --> 00:11:22,160
they have to sort of supposedly get the

291
00:11:20,320 --> 00:11:23,920
consent of the rider and sort of inform

292
00:11:22,160 --> 00:11:26,800
the system that they're doing that and

293
00:11:23,920 --> 00:11:28,160
this is this tradeoff between human a

294
00:11:26,800 --> 00:11:30,440
you know presence and lack of human

295
00:11:28,160 --> 00:11:32,880
agency when we're working with AI

296
00:11:30,440 --> 00:11:34,480
systems. So I think that a really

297
00:11:32,880 --> 00:11:38,240
important thing and this dates back to

298
00:11:34,480 --> 00:11:40,959
this early history of of of of AI at MIT

299
00:11:38,240 --> 00:11:42,959
and elsewhere. uh if you date if you

300
00:11:40,959 --> 00:11:45,440
look at work by JCR Lick Lighter and

301
00:11:42,959 --> 00:11:48,720
Lick was a faculty member at MIT when I

302
00:11:45,440 --> 00:11:52,399
was a graduate student uh in the 1980s

303
00:11:48,720 --> 00:11:55,560
uh still um but he had a a paper in 1960

304
00:11:52,399 --> 00:11:58,399
called man computer symbiosis and in

305
00:11:55,560 --> 00:12:00,320
that what the framing he had is what

306
00:11:58,399 --> 00:12:03,440
we're really looking for is how can

307
00:12:00,320 --> 00:12:05,600
machines and humans together produce

308
00:12:03,440 --> 00:12:08,399
results that are better than either

309
00:12:05,600 --> 00:12:10,079
machines alone or humans alone and that

310
00:12:08,399 --> 00:12:12,320
is very different than the sort of

311
00:12:10,079 --> 00:12:14,720
received view about AI and automation

312
00:12:12,320 --> 00:12:16,480
today, which is more rooted in Turing's

313
00:12:14,720 --> 00:12:18,800
view of the world, which is AI

314
00:12:16,480 --> 00:12:20,480
replicating human behavior, his sort of

315
00:12:18,800 --> 00:12:22,959
imitation game or what's sometimes known

316
00:12:20,480 --> 00:12:25,120
as the Turing test. And now that we have

317
00:12:22,959 --> 00:12:28,560
AI that does replicate human behavior

318
00:12:25,120 --> 00:12:30,240
relatively well, it's kind of, you know,

319
00:12:28,560 --> 00:12:32,000
the the proof point is really going to

320
00:12:30,240 --> 00:12:35,279
be the next level of these things, which

321
00:12:32,000 --> 00:12:38,240
is uh which is how machines and humans

322
00:12:35,279 --> 00:12:40,000
together are better than either alone.

323
00:12:38,240 --> 00:12:41,600
This is particularly important for a lot

324
00:12:40,000 --> 00:12:43,760
of the reasons I mentioned before in

325
00:12:41,600 --> 00:12:46,880
areas like healthcare, law, banking,

326
00:12:43,760 --> 00:12:50,440
education, employment, uh, and so forth,

327
00:12:46,880 --> 00:12:53,360
uh, where human judgment remains really

328
00:12:50,440 --> 00:12:55,440
important. So, collaborative AI is a big

329
00:12:53,360 --> 00:12:58,839
focus of things that, um, we're thinking

330
00:12:55,440 --> 00:12:58,839
about at MIT.

331
00:12:59,839 --> 00:13:06,000
So AI in society when you think about it

332
00:13:02,959 --> 00:13:08,639
it's very hard to get away from sort of

333
00:13:06,000 --> 00:13:12,200
science fiction you know things that are

334
00:13:08,639 --> 00:13:16,399
trying to look out many many decades

335
00:13:12,200 --> 00:13:18,480
ahead and in those the utopian or the

336
00:13:16,399 --> 00:13:20,639
dystopian narrative are basically the

337
00:13:18,480 --> 00:13:23,200
most interesting. They make the best

338
00:13:20,639 --> 00:13:25,200
stories. It doesn't necessarily mean

339
00:13:23,200 --> 00:13:28,160
that they're going to be the reality of

340
00:13:25,200 --> 00:13:29,519
the future. And in fact, you know, two

341
00:13:28,160 --> 00:13:31,519
things that I think are really important

342
00:13:29,519 --> 00:13:33,040
to remember on this is first of all, new

343
00:13:31,519 --> 00:13:35,920
technologies just seem like magic

344
00:13:33,040 --> 00:13:38,079
initially that just like very hard to

345
00:13:35,920 --> 00:13:39,680
really understand what they're doing in

346
00:13:38,079 --> 00:13:41,760
any way. Whereas technologies that we

347
00:13:39,680 --> 00:13:43,279
have experience with, everybody

348
00:13:41,760 --> 00:13:44,800
understands them in some sense, right?

349
00:13:43,279 --> 00:13:47,920
Like you know not to shove a fork in a

350
00:13:44,800 --> 00:13:49,440
toaster. Um that you know if you do

351
00:13:47,920 --> 00:13:51,360
you're probably going to have a problem

352
00:13:49,440 --> 00:13:53,920
at least if it's plugged in and turned

353
00:13:51,360 --> 00:13:56,240
on. uh and so it's not that they have to

354
00:13:53,920 --> 00:13:58,000
understand you know deeply the you know

355
00:13:56,240 --> 00:13:59,839
electricity and you know how it's

356
00:13:58,000 --> 00:14:02,000
converted into heat in the toaster and

357
00:13:59,839 --> 00:14:03,839
everything else but you have a broad

358
00:14:02,000 --> 00:14:05,959
understanding of these technologies we

359
00:14:03,839 --> 00:14:10,399
don't have that at all with

360
00:14:05,959 --> 00:14:13,440
AI and you know the outcomes result from

361
00:14:10,399 --> 00:14:15,839
really complicated sets of societal

362
00:14:13,440 --> 00:14:18,120
decisions for which we do have to

363
00:14:15,839 --> 00:14:20,000
develop these kinds of shared norms and

364
00:14:18,120 --> 00:14:21,680
understandings. So this is something

365
00:14:20,000 --> 00:14:24,560
that it's important for all of us to be

366
00:14:21,680 --> 00:14:25,920
as educated about as we can. So now I

367
00:14:24,560 --> 00:14:28,959
want to just for the last few minutes

368
00:14:25,920 --> 00:14:31,040
here switch to at MIT and then uh get to

369
00:14:28,959 --> 00:14:33,519
some questions. So the Schwarzman

370
00:14:31,040 --> 00:14:36,880
College of Computing is focused not just

371
00:14:33,519 --> 00:14:39,600
on AI but on computing more broadly. Uh

372
00:14:36,880 --> 00:14:43,760
but AI is a big piece of it because uh

373
00:14:39,600 --> 00:14:46,800
and and and actually really you know MIT

374
00:14:43,760 --> 00:14:49,920
uh and uh Steve Schwarzman uh in in

375
00:14:46,800 --> 00:14:52,079
making this gift to MIT really um

376
00:14:49,920 --> 00:14:54,240
there's a lot of foresight of what we've

377
00:14:52,079 --> 00:14:56,800
seen uh you know the gift was announced

378
00:14:54,240 --> 00:14:58,639
in 2018. So the last seven years there's

379
00:14:56,800 --> 00:15:01,120
been a huge advance in AI and it

380
00:14:58,639 --> 00:15:03,040
positioned us very well for that era.

381
00:15:01,120 --> 00:15:06,240
But the college is about the fact that

382
00:15:03,040 --> 00:15:08,000
computing now plays a not only more

383
00:15:06,240 --> 00:15:10,560
important but a different kind of role

384
00:15:08,000 --> 00:15:14,079
than it used to. Computing used to be a

385
00:15:10,560 --> 00:15:17,279
tool that could be broadly used but more

386
00:15:14,079 --> 00:15:19,600
and more now it's how progress is

387
00:15:17,279 --> 00:15:22,160
envisioned and realized. It's changing

388
00:15:19,600 --> 00:15:24,240
how things are conceptualized in almost

389
00:15:22,160 --> 00:15:26,959
every sector and almost every academic

390
00:15:24,240 --> 00:15:28,959
discipline. So there's huge student and

391
00:15:26,959 --> 00:15:31,360
employer demand for people trained in

392
00:15:28,959 --> 00:15:32,720
computing, but it's not so much comput.

393
00:15:31,360 --> 00:15:33,920
It's not so much programming and

394
00:15:32,720 --> 00:15:35,920
software engineering and system

395
00:15:33,920 --> 00:15:38,079
building. It's more and more formulating

396
00:15:35,920 --> 00:15:40,079
and solving problems. It's this fact

397
00:15:38,079 --> 00:15:42,000
that computing can really change

398
00:15:40,079 --> 00:15:44,560
knowledge of the forefront of computing

399
00:15:42,000 --> 00:15:47,120
can really change uh how you look at

400
00:15:44,560 --> 00:15:49,600
problems. There are the critical broader

401
00:15:47,120 --> 00:15:51,839
and and and and ethical and social

402
00:15:49,600 --> 00:15:54,079
aspects which if you didn't believe it

403
00:15:51,839 --> 00:15:57,120
was important before this AI explosion

404
00:15:54,079 --> 00:16:00,000
clearly is research being transformed

405
00:15:57,120 --> 00:16:02,000
and rapid changes in the core fields. So

406
00:16:00,000 --> 00:16:04,959
the college has enabled MIT to expand

407
00:16:02,000 --> 00:16:07,199
its faculty by about by by about 5% 50

408
00:16:04,959 --> 00:16:08,720
new faculty positions. Half of those are

409
00:16:07,199 --> 00:16:11,519
in electrical engineering and computer

410
00:16:08,720 --> 00:16:14,320
science in sort of core CS and AI and

411
00:16:11,519 --> 00:16:16,880
and part of EE uh and half are in

412
00:16:14,320 --> 00:16:18,720
departments across MIT to do what we

413
00:16:16,880 --> 00:16:20,560
call infusing computing with other

414
00:16:18,720 --> 00:16:22,480
disciplines. This is the fact that we're

415
00:16:20,560 --> 00:16:26,320
not just going to drive things outwards

416
00:16:22,480 --> 00:16:28,680
from the core advances in computing but

417
00:16:26,320 --> 00:16:31,199
in a in a bilingual birectional kind of

418
00:16:28,680 --> 00:16:33,199
way. We've actually hired about 50

419
00:16:31,199 --> 00:16:35,600
faculty since the college started in the

420
00:16:33,199 --> 00:16:37,279
last five years. twothirds of those are

421
00:16:35,600 --> 00:16:38,959
using those new faculty positions. The

422
00:16:37,279 --> 00:16:42,480
other iss hiring who you would have done

423
00:16:38,959 --> 00:16:45,120
anyway in that kind of time period. Uh

424
00:16:42,480 --> 00:16:46,480
but it's not just these new kinds of

425
00:16:45,120 --> 00:16:49,199
positions that are shared with

426
00:16:46,480 --> 00:16:50,800
departments across MIT that are infusing

427
00:16:49,199 --> 00:16:52,720
computing in other disciplines. So if

428
00:16:50,800 --> 00:16:54,720
you look at these areas like algorithms

429
00:16:52,720 --> 00:16:57,120
and society, computing and human

430
00:16:54,720 --> 00:16:58,560
experience, AI and scientific discovery

431
00:16:57,120 --> 00:17:01,880
etc.

432
00:16:58,560 --> 00:17:04,480
about um half of the faculty that we've

433
00:17:01,880 --> 00:17:06,559
hired both in these core positions in

434
00:17:04,480 --> 00:17:10,240
EECS and in these shared positions in

435
00:17:06,559 --> 00:17:11,919
departments across MIT uh are working in

436
00:17:10,240 --> 00:17:13,360
ways that infuse computing in other

437
00:17:11,919 --> 00:17:16,079
disciplines and these are sort of the

438
00:17:13,360 --> 00:17:18,039
key focal areas where that's happening

439
00:17:16,079 --> 00:17:20,959
and about twothirds of those are doing

440
00:17:18,039 --> 00:17:23,039
AI. So you know the AI revolution is

441
00:17:20,959 --> 00:17:25,839
definitely uh disproportionately

442
00:17:23,039 --> 00:17:27,839
represented here. The college has a

443
00:17:25,839 --> 00:17:30,480
broad educational program called the

444
00:17:27,839 --> 00:17:34,080
common ground. The idea is to bring the

445
00:17:30,480 --> 00:17:36,400
forefront of computing uh to students at

446
00:17:34,080 --> 00:17:38,080
uh at all levels in ways that involve

447
00:17:36,400 --> 00:17:39,880
multiple departments in the development

448
00:17:38,080 --> 00:17:42,559
and offering of those classes. So it's

449
00:17:39,880 --> 00:17:44,720
complimentary to core CS and AI that

450
00:17:42,559 --> 00:17:46,400
might be offered in the ECS. It's an

451
00:17:44,720 --> 00:17:47,840
enabler for new degrees and about a

452
00:17:46,400 --> 00:17:49,520
thousand students a year are now

453
00:17:47,840 --> 00:17:52,240
enrolling in these common ground

454
00:17:49,520 --> 00:17:53,760
subjects. uh and there about two dozen

455
00:17:52,240 --> 00:17:55,919
uh departments involved in teaching

456
00:17:53,760 --> 00:17:57,600
them. This is an eye chart. I'm not

457
00:17:55,919 --> 00:17:59,840
going to do much other than you know

458
00:17:57,600 --> 00:18:02,240
it's here for the historical record. Uh

459
00:17:59,840 --> 00:18:04,160
but these are some of the larger classes

460
00:18:02,240 --> 00:18:05,840
uh in the common ground now. And for

461
00:18:04,160 --> 00:18:07,840
those of you who you know know all the

462
00:18:05,840 --> 00:18:09,880
MIT numbering schemes, you can see which

463
00:18:07,840 --> 00:18:11,480
courses are involved in each of

464
00:18:09,880 --> 00:18:14,880
these.

465
00:18:11,480 --> 00:18:16,240
Um the college also has a very important

466
00:18:14,880 --> 00:18:17,840
focus on social and ethical

467
00:18:16,240 --> 00:18:20,720
responsibilities of computing through

468
00:18:17,840 --> 00:18:23,360
something we call CIRC. uh that looks at

469
00:18:20,720 --> 00:18:25,120
education, research, and engagement.

470
00:18:23,360 --> 00:18:26,640
There's a student prize called the

471
00:18:25,120 --> 00:18:28,559
envisioning the future of computing

472
00:18:26,640 --> 00:18:32,080
prize. I encourage you to use your

473
00:18:28,559 --> 00:18:34,160
favorite search engine uh you know to

474
00:18:32,080 --> 00:18:36,240
find this that the student essays from

475
00:18:34,160 --> 00:18:38,240
the last two years are nothing short of

476
00:18:36,240 --> 00:18:39,919
breathtaking that won these prizes in

477
00:18:38,240 --> 00:18:41,520
the last two years and we're right in

478
00:18:39,919 --> 00:18:44,240
the middle of the current competition

479
00:18:41,520 --> 00:18:46,400
that'll be announced next month. Um,

480
00:18:44,240 --> 00:18:50,080
CIRC scholars are students from across

481
00:18:46,400 --> 00:18:52,720
MIT uh, who work collaboratively on on

482
00:18:50,080 --> 00:18:55,360
on projects. There's about 75 of them

483
00:18:52,720 --> 00:18:57,440
per year in groups of sort of three to

484
00:18:55,360 --> 00:18:59,919
six working on projects that look at

485
00:18:57,440 --> 00:19:02,720
both the social and ethical issues and

486
00:18:59,919 --> 00:19:04,960
the technological ones. Um we've been

487
00:19:02,720 --> 00:19:08,880
producing a set of briefs around policy

488
00:19:04,960 --> 00:19:10,960
issues uh on AI and uh the social and

489
00:19:08,880 --> 00:19:12,799
ethical issues in computing are now also

490
00:19:10,960 --> 00:19:14,799
an important part of what's going on in

491
00:19:12,799 --> 00:19:16,480
the classroom. Uh there's a new common

492
00:19:14,799 --> 00:19:19,840
ground subject that just got taught this

493
00:19:16,480 --> 00:19:22,559
year. Uh um uh jointly between

494
00:19:19,840 --> 00:19:25,520
philosophy uh and computer science. So I

495
00:19:22,559 --> 00:19:27,280
want to just close with uh AI for

496
00:19:25,520 --> 00:19:29,120
scientific discovery is a really

497
00:19:27,280 --> 00:19:32,160
important focal area in what we're doing

498
00:19:29,120 --> 00:19:35,200
in the college and at MIT. It's a huge

499
00:19:32,160 --> 00:19:37,600
opportunity for transforming science. So

500
00:19:35,200 --> 00:19:39,200
at MIT now in material science and

501
00:19:37,600 --> 00:19:43,039
chemistry and chemical engineering and

502
00:19:39,200 --> 00:19:44,480
climate and the environment there are uh

503
00:19:43,039 --> 00:19:47,679
people doing work that's at the

504
00:19:44,480 --> 00:19:49,600
forefront of AI. It's often published in

505
00:19:47,679 --> 00:19:52,720
machine learning types of conferences

506
00:19:49,600 --> 00:19:55,799
and venues but it's also published in

507
00:19:52,720 --> 00:19:58,160
the discipline. So it has depth in both

508
00:19:55,799 --> 00:20:00,320
arenas and that's very important for

509
00:19:58,160 --> 00:20:02,080
driving things forward at the forefront

510
00:20:00,320 --> 00:20:03,840
but it's also having another kind of

511
00:20:02,080 --> 00:20:06,320
effect of bringing different areas of

512
00:20:03,840 --> 00:20:08,160
science closer together as they start to

513
00:20:06,320 --> 00:20:09,720
really think about developing these

514
00:20:08,160 --> 00:20:12,640
common machine learning

515
00:20:09,720 --> 00:20:14,799
tools. So biology is just an area I

516
00:20:12,640 --> 00:20:16,400
wanted to highlight quickly. Uh you know

517
00:20:14,799 --> 00:20:18,160
machine learning has transformed the

518
00:20:16,400 --> 00:20:21,840
understanding of protein structure. It

519
00:20:18,160 --> 00:20:24,080
was a Nobel Prize last year uh for uh

520
00:20:21,840 --> 00:20:29,000
some of these um protein structure

521
00:20:24,080 --> 00:20:32,720
algorithms uh um alpha fold um and and

522
00:20:29,000 --> 00:20:35,200
Rosetta. But generative AI is already

523
00:20:32,720 --> 00:20:38,000
starting to really revolutionize

524
00:20:35,200 --> 00:20:40,320
understanding of proteins in a in a more

525
00:20:38,000 --> 00:20:42,640
fundamental way with the development of

526
00:20:40,320 --> 00:20:47,039
genomic language models. sort of the

527
00:20:42,640 --> 00:20:48,799
language of the genome and uh this is

528
00:20:47,039 --> 00:20:50,360
not just structure it's starting to

529
00:20:48,799 --> 00:20:54,000
reveal things about functional and

530
00:20:50,360 --> 00:20:55,919
regulatory kinds of issues um uh and

531
00:20:54,000 --> 00:20:58,480
there's actually Alex Reeves who we just

532
00:20:55,919 --> 00:21:01,520
hired uh at MIT between the Broad

533
00:20:58,480 --> 00:21:04,240
Institute uh and EECS uh has a startup

534
00:21:01,520 --> 00:21:07,120
that's that's uh developing uh these

535
00:21:04,240 --> 00:21:08,880
kinds of models uh and they're really uh

536
00:21:07,120 --> 00:21:10,240
extremely interesting at uncovering

537
00:21:08,880 --> 00:21:13,520
things that we really haven't seen

538
00:21:10,240 --> 00:21:16,080
before in ways that just structure of

539
00:21:13,520 --> 00:21:18,960
things like Alphafold doesn't. So just

540
00:21:16,080 --> 00:21:21,200
to finish I think you know building a

541
00:21:18,960 --> 00:21:22,960
better AI world I'm neither the utopian

542
00:21:21,200 --> 00:21:24,960
nor the dystopian I'm going to put the

543
00:21:22,960 --> 00:21:28,080
responsibility on everyone in this room

544
00:21:24,960 --> 00:21:29,919
and on society more broadly. I think

545
00:21:28,080 --> 00:21:32,520
focusing on collaboration and

546
00:21:29,919 --> 00:21:35,520
partnership is extremely important on AI

547
00:21:32,520 --> 00:21:38,240
augmentation. Uh and I think we will

548
00:21:35,520 --> 00:21:40,640
have new insights and discoveries uh

549
00:21:38,240 --> 00:21:43,240
that um that will really advance

550
00:21:40,640 --> 00:21:47,039
humankind. So, with that,

551
00:21:43,240 --> 00:21:48,320
um, happy to go to questions. So, is

552
00:21:47,039 --> 00:21:50,840
someone gonna ask the questions or

553
00:21:48,320 --> 00:21:54,600
should try to read things off the

554
00:21:50,840 --> 00:21:57,440
prompter? I should just read. Okay. So,

555
00:21:54,600 --> 00:21:59,120
um, uh, if I was on the board of one of

556
00:21:57,440 --> 00:22:02,480
the companies here, what would be my big

557
00:21:59,120 --> 00:22:04,480
takeaway and advice for industry?

558
00:22:02,480 --> 00:22:07,640
So my big takeaway and advice for

559
00:22:04,480 --> 00:22:11,360
industry is

560
00:22:07,640 --> 00:22:12,760
um you have to be doing

561
00:22:11,360 --> 00:22:16,039
uh

562
00:22:12,760 --> 00:22:18,280
experiments using AI to really

563
00:22:16,039 --> 00:22:21,360
reinvision your

564
00:22:18,280 --> 00:22:22,480
business. And I say experiments because

565
00:22:21,360 --> 00:22:23,840
it's going to be very different

566
00:22:22,480 --> 00:22:25,120
depending on the company, but I'm going

567
00:22:23,840 --> 00:22:27,520
to assume for a moment it's a company

568
00:22:25,120 --> 00:22:30,400
that doesn't, you know, is not sort of

569
00:22:27,520 --> 00:22:33,080
at the forefront of a AI and and machine

570
00:22:30,400 --> 00:22:35,440
learning. uh

571
00:22:33,080 --> 00:22:37,520
and you know they basically need to be

572
00:22:35,440 --> 00:22:42,480
skunk works kinds of activities. You

573
00:22:37,520 --> 00:22:45,440
need a few technical people who know how

574
00:22:42,480 --> 00:22:48,320
technology is used successfully in your

575
00:22:45,440 --> 00:22:49,840
businesses today and you need a few

576
00:22:48,320 --> 00:22:51,440
machine learning people who are really

577
00:22:49,840 --> 00:22:52,799
sort of at the forefront of machine

578
00:22:51,440 --> 00:22:56,679
learning and they're hard to find and

579
00:22:52,799 --> 00:22:59,280
hire and you need to run some

580
00:22:56,679 --> 00:23:01,440
experiments. You can also of course be

581
00:22:59,280 --> 00:23:03,679
just sort of you know using today's AI

582
00:23:01,440 --> 00:23:05,280
tools to improve your business the way

583
00:23:03,679 --> 00:23:07,600
you would use any new technology to

584
00:23:05,280 --> 00:23:11,480
improve your business. But the real

585
00:23:07,600 --> 00:23:15,679
issue is machine learning is going to

586
00:23:11,480 --> 00:23:17,360
reinvent many many areas of work and

587
00:23:15,679 --> 00:23:19,840
you'll see that I think through many of

588
00:23:17,360 --> 00:23:21,679
the talks and presentations today and if

589
00:23:19,840 --> 00:23:22,720
you don't have some skunk works projects

590
00:23:21,679 --> 00:23:25,520
doing that you're going to be left

591
00:23:22,720 --> 00:23:26,960
behind. It'll be too late. the learning

592
00:23:25,520 --> 00:23:29,679
out of those things will be very

593
00:23:26,960 --> 00:23:31,760
important. Um and you know sometimes

594
00:23:29,679 --> 00:23:35,120
when I say that people say oh well you

595
00:23:31,760 --> 00:23:37,919
know it's expensive it's so then I'll

596
00:23:35,120 --> 00:23:40,159
just I'll just use something that I find

597
00:23:37,919 --> 00:23:44,320
one of the most useful things uh out

598
00:23:40,159 --> 00:23:45,840
there um which is uh uh you know

599
00:23:44,320 --> 00:23:47,840
something that I've learned through uh

600
00:23:45,840 --> 00:23:50,720
engagement uh with Amazon where I'm on

601
00:23:47,840 --> 00:23:52,600
the board. One of their sort of mantras

602
00:23:50,720 --> 00:23:54,880
is double down on positive

603
00:23:52,600 --> 00:23:56,720
surprise. Extremely useful. You can

604
00:23:54,880 --> 00:23:59,200
start something very small. Doesn't have

605
00:23:56,720 --> 00:24:02,159
to be expensive. But if you get a

606
00:23:59,200 --> 00:24:03,840
positive surprise, double down. And the

607
00:24:02,159 --> 00:24:05,840
thing about doubling down repeatedly is

608
00:24:03,840 --> 00:24:07,440
then you end up with something big. But

609
00:24:05,840 --> 00:24:08,799
at the beginning, it's small. And it's

610
00:24:07,440 --> 00:24:10,279
also important to remember that at the

611
00:24:08,799 --> 00:24:12,640
beginning, exponentials grow very

612
00:24:10,279 --> 00:24:14,120
slowly. So it doesn't get big until

613
00:24:12,640 --> 00:24:16,320
you've really had a bunch of positive

614
00:24:14,120 --> 00:24:18,480
surprises. So you can start things

615
00:24:16,320 --> 00:24:20,320
small. You need you need sort of, you

616
00:24:18,480 --> 00:24:24,159
know, good KPIs for that to understand

617
00:24:20,320 --> 00:24:25,840
what positive surprise would be.

618
00:24:24,159 --> 00:24:27,520
All right, the next question is, what do

619
00:24:25,840 --> 00:24:30,520
you think is the timing for AI to be

620
00:24:27,520 --> 00:24:32,360
able to demonstrate quality levels of

621
00:24:30,520 --> 00:24:36,080
reasoning?

622
00:24:32,360 --> 00:24:38,880
Um, I think AI is already delivering

623
00:24:36,080 --> 00:24:41,679
quality levels of reasoning, but it's

624
00:24:38,880 --> 00:24:43,279
not delivering human reasoning, and I

625
00:24:41,679 --> 00:24:46,000
don't think it ever will deliver human

626
00:24:43,279 --> 00:24:48,720
reasoning. So one of the things that we

627
00:24:46,000 --> 00:24:51,039
need to be prepared for at all levels of

628
00:24:48,720 --> 00:24:52,679
expertise about AI is that these may do

629
00:24:51,039 --> 00:24:55,600
very high quality

630
00:24:52,679 --> 00:24:57,520
things that don't align with the way

631
00:24:55,600 --> 00:24:59,600
humans would reason about something and

632
00:24:57,520 --> 00:25:04,480
how are we going to use them? This is

633
00:24:59,600 --> 00:25:06,640
why I'm very skeptical of uh AI uh sort

634
00:25:04,480 --> 00:25:10,159
of standalone AI systems rather than

635
00:25:06,640 --> 00:25:14,000
collaborative AI systems in any sort of

636
00:25:10,159 --> 00:25:16,159
uh you know high impact domain like you

637
00:25:14,000 --> 00:25:18,880
could pick health care as a as a classic

638
00:25:16,159 --> 00:25:20,480
example but I'm a huge believer in those

639
00:25:18,880 --> 00:25:22,320
systems in a way that they're

640
00:25:20,480 --> 00:25:25,520
collaborative with doctors already

641
00:25:22,320 --> 00:25:28,600
today. Um and I think in science which

642
00:25:25,520 --> 00:25:32,279
of course is what I understand the best

643
00:25:28,600 --> 00:25:35,200
uh the opportunity is unbelievable uh

644
00:25:32,279 --> 00:25:38,000
because we've gotten to a stage now

645
00:25:35,200 --> 00:25:40,400
where many scientific problems the

646
00:25:38,000 --> 00:25:42,080
complexity of them and the you know at

647
00:25:40,400 --> 00:25:44,159
the forefront of science the complexity

648
00:25:42,080 --> 00:25:46,200
of them and the degree to which you need

649
00:25:44,159 --> 00:25:49,760
to know things from different scientific

650
00:25:46,200 --> 00:25:52,880
disciplines is really you know beyond or

651
00:25:49,760 --> 00:25:55,279
stretching the ability of humans.

652
00:25:52,880 --> 00:25:58,480
to conceptualize what's going on. And so

653
00:25:55,279 --> 00:26:00,480
machine learning can gather that kind of

654
00:25:58,480 --> 00:26:02,679
broader information and in collaboration

655
00:26:00,480 --> 00:26:04,880
can do things that are uh really really

656
00:26:02,679 --> 00:26:07,840
powerful. Um all right, I think I

657
00:26:04,880 --> 00:26:09,679
probably have time for one more. Um if

658
00:26:07,840 --> 00:26:11,679
you look at how AI is quickly advancing,

659
00:26:09,679 --> 00:26:13,440
how do you see the way students learn

660
00:26:11,679 --> 00:26:16,440
changing particularly at places like

661
00:26:13,440 --> 00:26:16,440
MIT?

662
00:26:18,400 --> 00:26:22,080
So I'm a I'm a firm believer that like

663
00:26:20,559 --> 00:26:24,880
students should be using all of these

664
00:26:22,080 --> 00:26:26,320
tools right now. And I think in MIT, you

665
00:26:24,880 --> 00:26:29,520
know, that's the approach that most

666
00:26:26,320 --> 00:26:32,080
faculty are taking. Uh universities are

667
00:26:29,520 --> 00:26:34,159
collectives when it comes to academic

668
00:26:32,080 --> 00:26:37,360
programs. There's no sort of top- down

669
00:26:34,159 --> 00:26:40,640
dictation. Uh but I think most MIT

670
00:26:37,360 --> 00:26:42,480
faculty are not doing what happened say

671
00:26:40,640 --> 00:26:44,400
you know like I'm old so I remember when

672
00:26:42,480 --> 00:26:46,240
programmable calculators first came out

673
00:26:44,400 --> 00:26:47,600
and a bunch of faculty banned them

674
00:26:46,240 --> 00:26:51,039
because they were cheating you should

675
00:26:47,600 --> 00:26:52,559
use your slide rule. Um so uh I think

676
00:26:51,039 --> 00:26:56,320
that's not the approach that's being

677
00:26:52,559 --> 00:26:59,480
taken. But the big opportunity is again

678
00:26:56,320 --> 00:27:03,159
uh not yet realized which

679
00:26:59,480 --> 00:27:06,159
is how do we start to change the ways

680
00:27:03,159 --> 00:27:08,960
that students think and solve problems

681
00:27:06,159 --> 00:27:11,400
by collaborating with these tools? How

682
00:27:08,960 --> 00:27:14,000
do we make sure that we test student

683
00:27:11,400 --> 00:27:16,080
learning? How do we reframe our learning

684
00:27:14,000 --> 00:27:17,279
outcomes given that tools like this, you

685
00:27:16,080 --> 00:27:18,640
know, if you're just going to assign an

686
00:27:17,279 --> 00:27:20,880
essay, you have no idea who really wrote

687
00:27:18,640 --> 00:27:24,320
the essay. Although I can tell you the

688
00:27:20,880 --> 00:27:26,159
AIS essay is still not so great. Uh but

689
00:27:24,320 --> 00:27:27,600
but you don't really know but you never

690
00:27:26,159 --> 00:27:29,120
knew, right? Students could have been

691
00:27:27,600 --> 00:27:30,919
hiring somebody to write their papers

692
00:27:29,120 --> 00:27:33,200
for them. So it's not really that

693
00:27:30,919 --> 00:27:35,360
different. But this is going to amplify

694
00:27:33,200 --> 00:27:36,799
this issue of how are we framing learn

695
00:27:35,360 --> 00:27:38,240
learning outcomes? How are we framing

696
00:27:36,799 --> 00:27:39,840
learning outcomes for a world where

697
00:27:38,240 --> 00:27:41,840
students need to collaborate effectively

698
00:27:39,840 --> 00:27:43,760
with AI? And then how are we judging

699
00:27:41,840 --> 00:27:45,120
those learning outcomes. So thank you

700
00:27:43,760 --> 00:27:47,039
all for the opportunity to speak with

701
00:27:45,120 --> 00:27:49,279
you this morning. I'm going to do my

702
00:27:47,039 --> 00:27:53,559
best to stay on schedule here. uh and

703
00:27:49,279 --> 00:27:53,559
hand things over. So, thank you

704
00:27:56,010 --> 00:27:59,630
[Applause]

