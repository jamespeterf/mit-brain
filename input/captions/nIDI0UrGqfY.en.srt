1
00:00:00,160 --> 00:00:04,560
Hi everyone, welcome back to another

2
00:00:02,720 --> 00:00:07,359
interview with an amazing Sloan

3
00:00:04,560 --> 00:00:10,320
professor. Today I am joined by

4
00:00:07,359 --> 00:00:13,040
professor VC Barayas and he's going to

5
00:00:10,320 --> 00:00:14,480
tell us about his journey here to MIT.

6
00:00:13,040 --> 00:00:17,279
Would you like to introduce yourself to

7
00:00:14,480 --> 00:00:18,880
the audience and tell us about your path

8
00:00:17,279 --> 00:00:20,960
as a professor here?

9
00:00:18,880 --> 00:00:22,960
>> Yeah, so it's great great to be uh you

10
00:00:20,960 --> 00:00:25,760
know being a part of this. I've been at

11
00:00:22,960 --> 00:00:28,080
MIT for 18 coming up actually I'm in my

12
00:00:25,760 --> 00:00:30,800
18th year over here. I started here in

13
00:00:28,080 --> 00:00:34,160
2007. Uh I'm a professor in the

14
00:00:30,800 --> 00:00:36,880
operations group. Um and uh I'm also

15
00:00:34,160 --> 00:00:39,040
associated with the O center. Uh you

16
00:00:36,880 --> 00:00:42,559
know more recently MIT started something

17
00:00:39,040 --> 00:00:44,640
called uh our Genai impact consortium.

18
00:00:42,559 --> 00:00:46,879
>> It's a you know center of MIT's geni

19
00:00:44,640 --> 00:00:48,719
work around campus. Uh and I'm one of

20
00:00:46,879 --> 00:00:51,200
the faculty co-directors for that. The

21
00:00:48,719 --> 00:00:52,800
other faculty co-directors in CELL

22
00:00:51,200 --> 00:00:55,760
>> uh which is the computer science and AI

23
00:00:52,800 --> 00:00:57,440
lab. I think I've always enjoyed uh

24
00:00:55,760 --> 00:00:59,920
learning new things and I've always

25
00:00:57,440 --> 00:01:00,879
enjoyed like tinkering and like building

26
00:00:59,920 --> 00:01:02,239
things

27
00:01:00,879 --> 00:01:04,159
>> uh you know and stuff like that. I

28
00:01:02,239 --> 00:01:06,960
worked actually a bit while I was in in

29
00:01:04,159 --> 00:01:08,400
graduate school. Uh and so you know

30
00:01:06,960 --> 00:01:09,920
through the lens of that work that's

31
00:01:08,400 --> 00:01:12,159
kind of what I thought I' I'd continue

32
00:01:09,920 --> 00:01:14,880
doing. Uh but weirdly enough the the

33
00:01:12,159 --> 00:01:16,880
content of that work it got me exposed

34
00:01:14,880 --> 00:01:19,600
to

35
00:01:16,880 --> 00:01:20,960
>> you know the academic career because it

36
00:01:19,600 --> 00:01:23,759
turns out there's lots of academics

37
00:01:20,960 --> 00:01:25,840
involved in in that world. Did you study

38
00:01:23,759 --> 00:01:26,479
um data science in undergrad? What was

39
00:01:25,840 --> 00:01:28,159
your

40
00:01:26,479 --> 00:01:31,520
>> Not really. So

41
00:01:28,159 --> 00:01:33,119
>> yeah. Yeah. Yeah. So um again like I I

42
00:01:31,520 --> 00:01:34,799
never really knew what I wanted to do.

43
00:01:33,119 --> 00:01:36,960
So So here's the funny thing. So when I

44
00:01:34,799 --> 00:01:39,759
applied to graduate school,

45
00:01:36,960 --> 00:01:42,000
>> uh just to give you a sense of my sort

46
00:01:39,759 --> 00:01:44,560
of indecision. Uh so when I applied to

47
00:01:42,000 --> 00:01:48,640
graduate school, I applied to programs

48
00:01:44,560 --> 00:01:50,640
in applied physics, in applied math, in

49
00:01:48,640 --> 00:01:53,520
electrical engineering, in operations

50
00:01:50,640 --> 00:01:56,000
research, like all over the place,

51
00:01:53,520 --> 00:01:57,200
>> right? And I ended up I ended up doing

52
00:01:56,000 --> 00:01:58,880
electrical engineering, you know, I

53
00:01:57,200 --> 00:02:00,640
picked up all these research projects to

54
00:01:58,880 --> 00:02:03,520
work on, like things that like caught my

55
00:02:00,640 --> 00:02:06,880
fancy. Um and in kind of working on

56
00:02:03,520 --> 00:02:08,640
these things I you know I found my path

57
00:02:06,880 --> 00:02:10,239
in in fact actually like you know what I

58
00:02:08,640 --> 00:02:14,400
ended up doing in graduate school in

59
00:02:10,239 --> 00:02:16,720
graduate school I ended up working on um

60
00:02:14,400 --> 00:02:18,400
uh reinforcement learning that so all my

61
00:02:16,720 --> 00:02:20,400
research was and it continues to be in

62
00:02:18,400 --> 00:02:21,760
that on that on that topic right and how

63
00:02:20,400 --> 00:02:23,120
did I get involved in reinforcement

64
00:02:21,760 --> 00:02:25,520
learning and how did I get involved in

65
00:02:23,120 --> 00:02:26,879
doing this so I'm not like necessarily a

66
00:02:25,520 --> 00:02:28,640
mathematical person or anything like

67
00:02:26,879 --> 00:02:30,640
this so and I'm going to date myself

68
00:02:28,640 --> 00:02:33,040
here you know there used to be this

69
00:02:30,640 --> 00:02:35,200
thing called uh RoboCop,

70
00:02:33,040 --> 00:02:36,640
>> not COP, C cup, right? And it was like

71
00:02:35,200 --> 00:02:39,680
robotic soccer.

72
00:02:36,640 --> 00:02:41,280
>> And um you know, so so I wanted to get

73
00:02:39,680 --> 00:02:43,200
involved in Robo Cup.

74
00:02:41,280 --> 00:02:45,280
>> And uh as I sort of started getting

75
00:02:43,200 --> 00:02:46,959
involved in Robo Cup, I realized that a

76
00:02:45,280 --> 00:02:48,080
lot of the the stuff that one needed to

77
00:02:46,959 --> 00:02:50,879
do over there

78
00:02:48,080 --> 00:02:52,959
>> pertained to planning, right? Like like

79
00:02:50,879 --> 00:02:54,560
you're you're like if you play soccer,

80
00:02:52,959 --> 00:02:56,080
you're you're you're not thinking about

81
00:02:54,560 --> 00:02:57,599
the first domino that falls. You're

82
00:02:56,080 --> 00:02:59,120
thinking about the the hundth domino

83
00:02:57,599 --> 00:03:01,760
that falls. Ideally, if you're good,

84
00:02:59,120 --> 00:03:03,200
right? And uh you know that that got me

85
00:03:01,760 --> 00:03:04,400
like you know down this path and I very

86
00:03:03,200 --> 00:03:05,840
quickly realized there were so many

87
00:03:04,400 --> 00:03:07,360
things I didn't know. It's a very

88
00:03:05,840 --> 00:03:09,519
tenuous

89
00:03:07,360 --> 00:03:11,040
process. Uh but like ultimately I think

90
00:03:09,519 --> 00:03:13,200
it boiled down to just doing what I like

91
00:03:11,040 --> 00:03:14,640
to that was kind of it.

92
00:03:13,200 --> 00:03:17,040
>> That's so cool. I I never knew that.

93
00:03:14,640 --> 00:03:17,360
From electrical engineering to Robocup

94
00:03:17,040 --> 00:03:18,879
to

95
00:03:17,360 --> 00:03:20,319
>> the Robocup was before electrical

96
00:03:18,879 --> 00:03:21,040
engineering actually. Yeah. Yeah. Yeah.

97
00:03:20,319 --> 00:03:24,000
Yeah.

98
00:03:21,040 --> 00:03:25,360
>> To uh genai at MIT. Tell us about some

99
00:03:24,000 --> 00:03:28,080
of the research that you're doing with

100
00:03:25,360 --> 00:03:29,760
your PhD students. So a few of the areas

101
00:03:28,080 --> 00:03:31,840
I'm particularly interested in so I

102
00:03:29,760 --> 00:03:33,280
spoke about reinforcement learning right

103
00:03:31,840 --> 00:03:34,720
and one of the things I'm very

104
00:03:33,280 --> 00:03:37,440
interested in with respect to

105
00:03:34,720 --> 00:03:40,799
reinforcement learning uh today is kind

106
00:03:37,440 --> 00:03:42,400
of thinking about um how and so there's

107
00:03:40,799 --> 00:03:44,640
a broad there's this broad problem

108
00:03:42,400 --> 00:03:47,120
called alignment okay which is how do

109
00:03:44,640 --> 00:03:49,120
you take large language models which

110
00:03:47,120 --> 00:03:50,959
chat GPT is a large language model right

111
00:03:49,120 --> 00:03:54,799
how do I take large language models and

112
00:03:50,959 --> 00:03:56,560
align those models with uh you know our

113
00:03:54,799 --> 00:03:58,720
preferences as human beings so this

114
00:03:56,560 --> 00:04:02,720
problem is called the alignment problem.

115
00:03:58,720 --> 00:04:04,480
>> And uh you know as LLMs and agentic AI

116
00:04:02,720 --> 00:04:05,680
and things like that get like more

117
00:04:04,480 --> 00:04:07,840
common

118
00:04:05,680 --> 00:04:09,920
>> this alignment is going to get more and

119
00:04:07,840 --> 00:04:11,840
more and more and more important.

120
00:04:09,920 --> 00:04:13,840
>> Okay. Like why? Like imagine right like

121
00:04:11,840 --> 00:04:16,479
imagine you have your own agent that's

122
00:04:13,840 --> 00:04:18,000
making decisions on your behalf. Well

123
00:04:16,479 --> 00:04:18,400
>> you you wanted to behave like you.

124
00:04:18,000 --> 00:04:20,959
>> Yes.

125
00:04:18,400 --> 00:04:23,600
>> Right. And uh so so these tasks are

126
00:04:20,959 --> 00:04:25,120
non-trivial and you know when faced with

127
00:04:23,600 --> 00:04:27,600
training data from lots of different

128
00:04:25,120 --> 00:04:29,759
individuals uh from a diversity of

129
00:04:27,600 --> 00:04:31,040
individuals how do you kind of weight

130
00:04:29,759 --> 00:04:32,479
this how do you think about this these

131
00:04:31,040 --> 00:04:33,919
are non-trivial questions so this is an

132
00:04:32,479 --> 00:04:35,440
important research area for me there

133
00:04:33,919 --> 00:04:37,280
also more foundational things that I'm

134
00:04:35,440 --> 00:04:39,919
I'm very interested in and these range

135
00:04:37,280 --> 00:04:41,759
from uh problems of continual learning

136
00:04:39,919 --> 00:04:45,280
this is the problem of how do you kind

137
00:04:41,759 --> 00:04:46,639
of just train a neural network forever

138
00:04:45,280 --> 00:04:48,080
>> right like and you can imagine that

139
00:04:46,639 --> 00:04:49,440
that's not a crazy thing to ask right

140
00:04:48,080 --> 00:04:51,199
like I build a model, I want to

141
00:04:49,440 --> 00:04:52,479
continuously feed it data over time.

142
00:04:51,199 --> 00:04:54,639
Well, what happens when you do this?

143
00:04:52,479 --> 00:04:55,759
Turns out lots of crazy things. Another

144
00:04:54,639 --> 00:04:57,040
problem I've been thinking quite a bit

145
00:04:55,759 --> 00:04:59,199
about again relating to sort of

146
00:04:57,040 --> 00:05:01,199
generative AI. If you think about like

147
00:04:59,199 --> 00:05:02,639
running generative AI, right? You're

148
00:05:01,199 --> 00:05:04,400
running a large language model

149
00:05:02,639 --> 00:05:06,479
inferencing as they call it a large

150
00:05:04,400 --> 00:05:08,160
language model. Uh this is a process

151
00:05:06,479 --> 00:05:10,880
that's fundamentally sequential.

152
00:05:08,160 --> 00:05:12,800
>> So I generate like you can colloquially

153
00:05:10,880 --> 00:05:14,240
think of it as I generate a word then

154
00:05:12,800 --> 00:05:15,680
having generated that word I generate

155
00:05:14,240 --> 00:05:17,120
the next word and then having generated

156
00:05:15,680 --> 00:05:17,759
I generate the third word and so on and

157
00:05:17,120 --> 00:05:19,360
so forth. Yeah.

158
00:05:17,759 --> 00:05:21,600
>> So this is fundamentally a sequential

159
00:05:19,360 --> 00:05:24,400
process. It takes a lot of time,

160
00:05:21,600 --> 00:05:26,320
>> takes a lot of energy, blah blah blah.

161
00:05:24,400 --> 00:05:27,840
Can we make this faster?

162
00:05:26,320 --> 00:05:29,039
>> Are there fundamental limits to making

163
00:05:27,840 --> 00:05:31,919
this faster?

164
00:05:29,039 --> 00:05:33,759
>> Right? Um and this is like there so many

165
00:05:31,919 --> 00:05:35,919
attacks at this problem, right? Like I

166
00:05:33,759 --> 00:05:38,240
can try think about making this faster

167
00:05:35,919 --> 00:05:40,080
by building clever hardware,

168
00:05:38,240 --> 00:05:44,320
>> right? I can think about making this

169
00:05:40,080 --> 00:05:45,840
faster by building um clever models. And

170
00:05:44,320 --> 00:05:47,919
in fact, people are thinking about this

171
00:05:45,840 --> 00:05:49,520
like do we need really need LLMs the way

172
00:05:47,919 --> 00:05:50,960
they are or can you know diffusion

173
00:05:49,520 --> 00:05:52,560
models be used? There's so many

174
00:05:50,960 --> 00:05:55,120
different approaches out there. I'm kind

175
00:05:52,560 --> 00:05:57,840
of focused more on with that problem I

176
00:05:55,120 --> 00:05:59,440
view it as being so fundamental. I'm

177
00:05:57,840 --> 00:06:01,199
curious about the limits

178
00:05:59,440 --> 00:06:03,520
>> of what we can do on

179
00:06:01,199 --> 00:06:05,199
>> right. So so yes, we'll eventually build

180
00:06:03,520 --> 00:06:06,880
algorithms and approaches and so on and

181
00:06:05,199 --> 00:06:08,560
so forth, but I think what what what's

182
00:06:06,880 --> 00:06:10,080
merited over here is an understanding of

183
00:06:08,560 --> 00:06:11,600
like the fundamental information

184
00:06:10,080 --> 00:06:14,560
theoretic limits of doing this.

185
00:06:11,600 --> 00:06:16,720
>> Wow. And if a student out there would

186
00:06:14,560 --> 00:06:20,160
like to do research with you, how would

187
00:06:16,720 --> 00:06:21,360
they go to go about um becoming part of

188
00:06:20,160 --> 00:06:23,840
your research group?

189
00:06:21,360 --> 00:06:25,840
>> Uh one is to talk with me or write me an

190
00:06:23,840 --> 00:06:28,080
email or something like that, right? Uh

191
00:06:25,840 --> 00:06:30,240
but no, so so for instance, you know,

192
00:06:28,080 --> 00:06:31,280
for for the masters in analytic, the MAN

193
00:06:30,240 --> 00:06:34,080
program,

194
00:06:31,280 --> 00:06:36,479
>> um I've been very proud uh to be

195
00:06:34,080 --> 00:06:38,960
associated with over the years

196
00:06:36,479 --> 00:06:41,120
>> on average I think three or four

197
00:06:38,960 --> 00:06:42,800
students every year. U so I didn't

198
00:06:41,120 --> 00:06:44,800
really talk about this but there's a

199
00:06:42,800 --> 00:06:47,840
large sort of applied element to kind of

200
00:06:44,800 --> 00:06:49,680
everything uh you know we work on right

201
00:06:47,840 --> 00:06:53,360
um and the applied element actually

202
00:06:49,680 --> 00:06:54,639
allows for an easier ingress point

203
00:06:53,360 --> 00:06:55,680
>> uh you know for students there's

204
00:06:54,639 --> 00:06:57,039
something for them to do they get

205
00:06:55,680 --> 00:06:58,960
momentum they're not stuck on something

206
00:06:57,039 --> 00:07:00,240
super hard and they can make progress

207
00:06:58,960 --> 00:07:02,080
>> and so all of these things are

208
00:07:00,240 --> 00:07:04,560
associated with like very often I have

209
00:07:02,080 --> 00:07:05,599
like a ton of like very real very

210
00:07:04,560 --> 00:07:07,520
applied projects I'll give you an

211
00:07:05,599 --> 00:07:09,120
example of a real project right like so

212
00:07:07,520 --> 00:07:11,039
one one thing we're working on right now

213
00:07:09,120 --> 00:07:12,400
is with this large consumer goods

214
00:07:11,039 --> 00:07:14,960
company

215
00:07:12,400 --> 00:07:17,759
>> uh they're trying to kind of understand

216
00:07:14,960 --> 00:07:19,680
whether they can do a better job of

217
00:07:17,759 --> 00:07:20,800
understanding how their consumers

218
00:07:19,680 --> 00:07:22,880
respond

219
00:07:20,800 --> 00:07:25,120
>> to various things they may do

220
00:07:22,880 --> 00:07:27,360
>> but doing this using generative AI okay

221
00:07:25,120 --> 00:07:28,880
so using generative AI maybe to make a

222
00:07:27,360 --> 00:07:30,319
guess at this

223
00:07:28,880 --> 00:07:32,319
>> a fascinating problem

224
00:07:30,319 --> 00:07:34,400
>> okay it's very applied it's something

225
00:07:32,319 --> 00:07:36,479
that's like you know very natural for a

226
00:07:34,400 --> 00:07:38,479
stu for you know a master student that

227
00:07:36,479 --> 00:07:40,000
that you know to to get involved in the

228
00:07:38,479 --> 00:07:42,800
only sort Honestly, the only

229
00:07:40,000 --> 00:07:44,960
prerequisites for doing this are one

230
00:07:42,800 --> 00:07:45,919
just being intellectually curious,

231
00:07:44,960 --> 00:07:48,080
>> right? You have to be super

232
00:07:45,919 --> 00:07:49,759
intellectually curious. Uh and then the

233
00:07:48,080 --> 00:07:51,280
other is like, you know, a willing like

234
00:07:49,759 --> 00:07:53,039
like how bring something to the table,

235
00:07:51,280 --> 00:07:55,440
right? Like so for instance, maybe like

236
00:07:53,039 --> 00:07:58,000
some of the algorithm design scares you

237
00:07:55,440 --> 00:07:59,520
>> uh but uh you know you're a great hacker

238
00:07:58,000 --> 00:08:00,879
>> or conversely maybe you're not a great

239
00:07:59,520 --> 00:08:02,800
hacker but like you know you're a great

240
00:08:00,879 --> 00:08:04,319
al so right like so bring something to

241
00:08:02,800 --> 00:08:06,240
the table and be intellectually curious.

242
00:08:04,319 --> 00:08:08,560
That's really typically what's needed.

243
00:08:06,240 --> 00:08:10,160
And could you talk about this um

244
00:08:08,560 --> 00:08:11,759
certificate that you launched as well,

245
00:08:10,160 --> 00:08:12,960
the product management certificate?

246
00:08:11,759 --> 00:08:13,440
>> So, I enjoy teaching.

247
00:08:12,960 --> 00:08:15,039
>> Yeah.

248
00:08:13,440 --> 00:08:17,599
>> I've been involved in teaching a lot of

249
00:08:15,039 --> 00:08:19,280
things, you know, um starting this

250
00:08:17,599 --> 00:08:20,319
coming year, I'll teach a you know, I'

251
00:08:19,280 --> 00:08:22,080
I've been teaching this class called

252
00:08:20,319 --> 00:08:23,919
hands-on deep learning. The idea is to

253
00:08:22,080 --> 00:08:28,160
kind of get you up and running and and

254
00:08:23,919 --> 00:08:31,919
and and proficient uh with uh you know,

255
00:08:28,160 --> 00:08:33,680
building complex generative models from

256
00:08:31,919 --> 00:08:35,839
the ground up. And that's a class that I

257
00:08:33,680 --> 00:08:38,000
co-developed with a colleague of mine,

258
00:08:35,839 --> 00:08:39,680
uh, you know, Rama Rama Krishnan. Um,

259
00:08:38,000 --> 00:08:41,120
and we we started on this class in like

260
00:08:39,680 --> 00:08:42,800
like like a long time ago. Okay, we

261
00:08:41,120 --> 00:08:44,720
started in 2020

262
00:08:42,800 --> 00:08:46,399
>> and uh, right like we're like we're

263
00:08:44,720 --> 00:08:49,120
nerds. We're kind of very excited about

264
00:08:46,399 --> 00:08:51,440
the space. Uh, but yeah, that's grown

265
00:08:49,120 --> 00:08:54,080
tremendously in popularity. I've enjoyed

266
00:08:51,440 --> 00:08:55,760
doing this. I continue to do it. Uh,

267
00:08:54,080 --> 00:08:57,200
right now outside of these classes,

268
00:08:55,760 --> 00:09:00,640
there are other classes I taught and

269
00:08:57,200 --> 00:09:02,800
teach at MIT. So uh you know about uh

270
00:09:00,640 --> 00:09:05,040
it's about eight years ago now I sort of

271
00:09:02,800 --> 00:09:07,440
saw a sense a trend where you know

272
00:09:05,040 --> 00:09:09,279
students from the business school as

273
00:09:07,440 --> 00:09:11,760
opposed to thinking about careers in

274
00:09:09,279 --> 00:09:12,880
finance or in consulting they were

275
00:09:11,760 --> 00:09:14,399
increasingly thinking about building

276
00:09:12,880 --> 00:09:15,040
their own companies going into careers

277
00:09:14,399 --> 00:09:15,680
in tech.

278
00:09:15,040 --> 00:09:18,880
>> Yeah.

279
00:09:15,680 --> 00:09:21,040
>> And um you know it sort of became

280
00:09:18,880 --> 00:09:22,640
important to understand okay well

281
00:09:21,040 --> 00:09:23,519
>> what is the core education for a career

282
00:09:22,640 --> 00:09:25,360
in tech.

283
00:09:23,519 --> 00:09:28,399
>> So we created this like product

284
00:09:25,360 --> 00:09:31,360
management uh curriculum. Okay, starting

285
00:09:28,399 --> 00:09:33,360
with a class and actually that that was

286
00:09:31,360 --> 00:09:35,120
you know the first class right that has

287
00:09:33,360 --> 00:09:36,800
grown into a full-on curriculum and an

288
00:09:35,120 --> 00:09:38,560
associated certificate. It's called the

289
00:09:36,800 --> 00:09:40,720
product management certificate

290
00:09:38,560 --> 00:09:43,680
>> uh where you take u you know a class

291
00:09:40,720 --> 00:09:46,320
there's u you know over you work closely

292
00:09:43,680 --> 00:09:48,560
as sort of being shadowing a product

293
00:09:46,320 --> 00:09:50,640
manager at a company and this and that

294
00:09:48,560 --> 00:09:53,120
uh so it's been very exciting uh you

295
00:09:50,640 --> 00:09:55,600
know um just given the the limits on my

296
00:09:53,120 --> 00:09:57,680
physical abilities in terms of teaching

297
00:09:55,600 --> 00:09:59,120
I handed that class so I could focus on

298
00:09:57,680 --> 00:10:01,279
hands-on deep learning I handed that

299
00:09:59,120 --> 00:10:03,120
class over to another colleague of mine

300
00:10:01,279 --> 00:10:05,360
uh you know a couple years ago uh

301
00:10:03,120 --> 00:10:06,800
Charlie Fine and so now Charlie Fine uh

302
00:10:05,360 --> 00:10:08,880
you know manages is that it's very

303
00:10:06,800 --> 00:10:10,320
successful. It's very exciting. And of

304
00:10:08,880 --> 00:10:11,600
course, you know, I'm in the operations

305
00:10:10,320 --> 00:10:12,959
group over here. I teach the core

306
00:10:11,600 --> 00:10:15,120
operations manager. I used to teach the

307
00:10:12,959 --> 00:10:17,839
core operations management class, but

308
00:10:15,120 --> 00:10:20,000
still teach it uh over the summer to our

309
00:10:17,839 --> 00:10:21,519
Sloan fellows. These are folks that have

310
00:10:20,000 --> 00:10:22,800
>> kind of been in industry for a while.

311
00:10:21,519 --> 00:10:25,120
They're coming back.

312
00:10:22,800 --> 00:10:28,160
>> Well, great. Well, I hope you stay at

313
00:10:25,120 --> 00:10:29,519
MIT for another 18 minimum years. And

314
00:10:28,160 --> 00:10:30,240
>> Oh, yeah. Minimum.

315
00:10:29,519 --> 00:10:32,160
>> Yeah. Yeah. Yeah.

316
00:10:30,240 --> 00:10:33,920
>> And uh thank you so much for your time

317
00:10:32,160 --> 00:10:36,000
today. We learned so much from you.

318
00:10:33,920 --> 00:10:38,160
>> Thanks, Michelle. And thank you all.

319
00:10:36,000 --> 00:10:40,079
>> Hi everyone. Thank you for watching. If

320
00:10:38,160 --> 00:10:42,720
you like this video, check out our other

321
00:10:40,079 --> 00:10:45,839
videos in the series where I interview

322
00:10:42,720 --> 00:10:46,959
influential professors at MIT Sloan. If

323
00:10:45,839 --> 00:10:48,880
you would like to learn more about the

324
00:10:46,959 --> 00:10:51,120
Master Business Analytics program, we

325
00:10:48,880 --> 00:10:52,640
also have a playlist where you can see

326
00:10:51,120 --> 00:10:55,200
more videos about the admissions

327
00:10:52,640 --> 00:10:57,920
process, student life, and the program

328
00:10:55,200 --> 00:10:57,920
overall.

