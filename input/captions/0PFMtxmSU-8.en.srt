1
00:00:00,880 --> 00:00:04,640
Yeah, this is going to be co-odderated

2
00:00:02,560 --> 00:00:07,359
by Anna and myself. Anna gave me the

3
00:00:04,640 --> 00:00:09,120
questions. Um, but before we start here,

4
00:00:07,359 --> 00:00:11,200
just uh congratulations to everybody.

5
00:00:09,120 --> 00:00:14,679
Thank you for staying f you know, Friday

6
00:00:11,200 --> 00:00:17,359
afternoon 4:30. That's an impressive uh

7
00:00:14,679 --> 00:00:19,359
turnout. So, we're not going to uh this

8
00:00:17,359 --> 00:00:20,720
panel needs no truly no introduction, I

9
00:00:19,359 --> 00:00:22,480
think. But what we're going to do to

10
00:00:20,720 --> 00:00:24,640
start is we'll start with one question.

11
00:00:22,480 --> 00:00:26,400
We also did not rehearse this, so none

12
00:00:24,640 --> 00:00:27,920
of the panelists have had time to think

13
00:00:26,400 --> 00:00:30,599
about their answers. So you're going to

14
00:00:27,920 --> 00:00:32,800
get it fresh which I think can always be

15
00:00:30,599 --> 00:00:34,480
interesting but I thought uh I'll give

16
00:00:32,800 --> 00:00:36,880
the first question ask all of you to

17
00:00:34,480 --> 00:00:38,079
respond but from your own context right

18
00:00:36,880 --> 00:00:39,840
because you all have different context

19
00:00:38,079 --> 00:00:41,840
and what you have represented here are

20
00:00:39,840 --> 00:00:44,879
you have two from academia with Wendy

21
00:00:41,840 --> 00:00:47,440
and Anna you've got small with you've

22
00:00:44,879 --> 00:00:49,680
got small to large biotech with John or

23
00:00:47,440 --> 00:00:51,920
John's sort of done everything and large

24
00:00:49,680 --> 00:00:53,520
pharma of course with um Fiona so I

25
00:00:51,920 --> 00:00:54,960
think we have a really nice um

26
00:00:53,520 --> 00:00:57,920
cross-section of people who've lived in

27
00:00:54,960 --> 00:01:01,440
this ecosystem so the question is uh

28
00:00:57,920 --> 00:01:03,440
historically um everything fails. 99 out

29
00:01:01,440 --> 00:01:06,280
of a hundred things that are conceived

30
00:01:03,440 --> 00:01:09,200
of in the lab fail before they get to

31
00:01:06,280 --> 00:01:12,080
approval. Are we better? The science is

32
00:01:09,200 --> 00:01:14,799
better today than it's ever been. Are we

33
00:01:12,080 --> 00:01:17,759
better in terms of those odds or those

34
00:01:14,799 --> 00:01:20,960
statistics? So, uh maybe I'll start with

35
00:01:17,759 --> 00:01:23,520
you Wendy and uh

36
00:01:20,960 --> 00:01:26,080
uh anyway,

37
00:01:23,520 --> 00:01:28,080
uh I'm Wendy Chung. I guess the academic

38
00:01:26,080 --> 00:01:29,759
uh one of the academic representatives

39
00:01:28,080 --> 00:01:32,880
um representing children especially with

40
00:01:29,759 --> 00:01:34,560
rare diseases. Um so I I don't know at

41
00:01:32,880 --> 00:01:36,079
large in terms of all of pharma. You

42
00:01:34,560 --> 00:01:38,479
guys probably know the statistics better

43
00:01:36,079 --> 00:01:40,240
than I do. Uh I do think because we're

44
00:01:38,479 --> 00:01:42,000
getting to things that are getting to

45
00:01:40,240 --> 00:01:43,600
the root cause if you will. So not just

46
00:01:42,000 --> 00:01:45,840
symptom relief but getting to the root

47
00:01:43,600 --> 00:01:47,680
cause from a genetic point of view. uh

48
00:01:45,840 --> 00:01:49,680
there are a lot of parameters but if we

49
00:01:47,680 --> 00:01:51,759
can get in at the right time uh which

50
00:01:49,680 --> 00:01:54,560
was said earlier uh I do think we're

51
00:01:51,759 --> 00:01:57,200
showing much better success in terms of

52
00:01:54,560 --> 00:01:58,719
uh efficacy safety and I think we're

53
00:01:57,200 --> 00:02:00,880
going to be going forward I think the

54
00:01:58,719 --> 00:02:03,280
big key to me is getting in at the right

55
00:02:00,880 --> 00:02:04,960
time right time and that getting in at

56
00:02:03,280 --> 00:02:06,320
the right time means it will take less

57
00:02:04,960 --> 00:02:08,640
time or just will have a higher

58
00:02:06,320 --> 00:02:11,039
probability of success uh I think it'll

59
00:02:08,640 --> 00:02:12,480
have a higher probability of success and

60
00:02:11,039 --> 00:02:14,160
uh the right time I'll just be

61
00:02:12,480 --> 00:02:17,120
provocative and say is sometimes even

62
00:02:14,160 --> 00:02:19,200
before the onset of symptoms.

63
00:02:17,120 --> 00:02:21,200
Yeah.

64
00:02:19,200 --> 00:02:23,200
So, yeah, I think we are getting better,

65
00:02:21,200 --> 00:02:25,840
but we still need to get much much

66
00:02:23,200 --> 00:02:27,360
better. Uh, to Wendy's point, addressing

67
00:02:25,840 --> 00:02:28,959
the cause of the disease helps. To

68
00:02:27,360 --> 00:02:31,360
Fiona's point, understanding the

69
00:02:28,959 --> 00:02:32,959
genetics better also helps to design

70
00:02:31,360 --> 00:02:34,879
clinical trials in which you enroll the

71
00:02:32,959 --> 00:02:36,959
right patients and you may get the

72
00:02:34,879 --> 00:02:39,040
outcomes that you're looking for. But

73
00:02:36,959 --> 00:02:41,040
still, even within the genetic diseases,

74
00:02:39,040 --> 00:02:43,680
as Wendy knows, there is a huge

75
00:02:41,040 --> 00:02:46,720
diversity even within one gene that is

76
00:02:43,680 --> 00:02:48,879
mutated. So to understand that

77
00:02:46,720 --> 00:02:51,599
diversity, to be able to address the

78
00:02:48,879 --> 00:02:54,000
symptoms that are specific to that gene

79
00:02:51,599 --> 00:02:55,599
within that diversity is still very

80
00:02:54,000 --> 00:02:57,360
complicated and we still need to

81
00:02:55,599 --> 00:02:59,280
understand how to handle it within a

82
00:02:57,360 --> 00:03:01,920
clinical trial and obviously in the

83
00:02:59,280 --> 00:03:04,000
commercial landscape.

84
00:03:01,920 --> 00:03:06,879
So Anna, actually want to go to Anna

85
00:03:04,000 --> 00:03:08,560
next. Oh, I'm next. From where do you

86
00:03:06,879 --> 00:03:10,640
sit, does it feel like we're getting

87
00:03:08,560 --> 00:03:12,400
better? You're you're driving this whole

88
00:03:10,640 --> 00:03:15,519
thing to try to make it go better. Well,

89
00:03:12,400 --> 00:03:17,200
you know, the the inspiration for, you

90
00:03:15,519 --> 00:03:19,840
know, starting the ladder secures

91
00:03:17,200 --> 00:03:22,560
accelerator was really uh to try to

92
00:03:19,840 --> 00:03:24,319
bring all the uh members of the

93
00:03:22,560 --> 00:03:25,680
ecosystem together. So, we just like

94
00:03:24,319 --> 00:03:27,200
what we're doing today to sort of

95
00:03:25,680 --> 00:03:29,680
exchange ideas in ways that might

96
00:03:27,200 --> 00:03:32,000
actually uh accelerate our path to

97
00:03:29,680 --> 00:03:33,920
therapies. But speaking from our sort of

98
00:03:32,000 --> 00:03:37,840
broad perspective, if you will, um I

99
00:03:33,920 --> 00:03:39,680
will say that uh every major discovery,

100
00:03:37,840 --> 00:03:42,239
you know, that anyone here has been a

101
00:03:39,680 --> 00:03:44,319
part of is founded on fundamental

102
00:03:42,239 --> 00:03:46,400
discoveries that happened in the lab,

103
00:03:44,319 --> 00:03:48,400
right? And so we can't possibly skip

104
00:03:46,400 --> 00:03:50,000
that step in order to accelerate our

105
00:03:48,400 --> 00:03:52,000
path to therapies. I think that's true.

106
00:03:50,000 --> 00:03:54,159
And I think, you know, I'll put a plug

107
00:03:52,000 --> 00:03:55,840
in for sustained funding for fundamental

108
00:03:54,159 --> 00:03:58,080
research is really important if we're

109
00:03:55,840 --> 00:03:59,840
going to continue, especially now. I

110
00:03:58,080 --> 00:04:01,680
feel like we're in a golden era in biio

111
00:03:59,840 --> 00:04:04,159
medicine. We've never had more tools as

112
00:04:01,680 --> 00:04:06,720
been represented by all the talks today.

113
00:04:04,159 --> 00:04:08,239
And so now to not have adequate funding

114
00:04:06,720 --> 00:04:09,920
for those fundamental sort of

115
00:04:08,239 --> 00:04:12,000
mechanistic studies that will enable

116
00:04:09,920 --> 00:04:14,239
therapies would be of course a grave

117
00:04:12,000 --> 00:04:16,239
mistake I think and a lot of it happens

118
00:04:14,239 --> 00:04:18,880
in the not only but it a lot of it

119
00:04:16,239 --> 00:04:21,199
happens in the academic setting. Um but

120
00:04:18,880 --> 00:04:22,800
I think that what from my perspective uh

121
00:04:21,199 --> 00:04:25,600
is something that I would like to see us

122
00:04:22,800 --> 00:04:27,040
do better is especially all of us here.

123
00:04:25,600 --> 00:04:28,720
We're sitting in Kendall Square. we have

124
00:04:27,040 --> 00:04:30,880
all of us around and yet of course we're

125
00:04:28,720 --> 00:04:32,560
all busy in our own sort of domains but

126
00:04:30,880 --> 00:04:35,120
if we could find ways to break some of

127
00:04:32,560 --> 00:04:36,560
these silos and have more communication

128
00:04:35,120 --> 00:04:39,120
uh I think that there are many many more

129
00:04:36,560 --> 00:04:41,040
opportunities to um you know meet each

130
00:04:39,120 --> 00:04:42,560
other where we are and learn from each

131
00:04:41,040 --> 00:04:45,040
other so that maybe we can accelerate

132
00:04:42,560 --> 00:04:47,360
our path to discovery so uh it's my hope

133
00:04:45,040 --> 00:04:49,759
that initiatives like this can help us

134
00:04:47,360 --> 00:04:52,080
get there so we'll come back to that so

135
00:04:49,759 --> 00:04:54,639
John you've seen everything here well I

136
00:04:52,080 --> 00:04:58,639
mean I in terms of probabilities and and

137
00:04:54,639 --> 00:05:01,240
and and so forth I mean I think that um

138
00:04:58,639 --> 00:05:05,680
you know if you integrate human

139
00:05:01,240 --> 00:05:09,919
genetics into an R&D effort and also

140
00:05:05,680 --> 00:05:12,800
have a modality which has true platform

141
00:05:09,919 --> 00:05:15,759
characteristics and I'm partial to what

142
00:05:12,800 --> 00:05:19,039
you know we were able to do with RAI um

143
00:05:15,759 --> 00:05:22,639
you can you can generate outsized

144
00:05:19,039 --> 00:05:25,600
um uh success rates um and you know from

145
00:05:22,639 --> 00:05:26,759
IND to positive phase 3 results at Elnum

146
00:05:25,600 --> 00:05:30,160
we were at

147
00:05:26,759 --> 00:05:31,840
62%. Uh which is a remarkable number

148
00:05:30,160 --> 00:05:36,360
compared to the industry averages which

149
00:05:31,840 --> 00:05:41,600
are under 10. Um and and that that is a

150
00:05:36,360 --> 00:05:44,400
playbook that um can be can be used um

151
00:05:41,600 --> 00:05:47,280
in many other examples. So can you go

152
00:05:44,400 --> 00:05:50,080
back to the early days of SINA because

153
00:05:47,280 --> 00:05:52,639
Al Nylm was many years in development

154
00:05:50,080 --> 00:05:54,320
and those early days and the failures in

155
00:05:52,639 --> 00:05:55,840
the Can can you just talk a little bit

156
00:05:54,320 --> 00:05:57,440
about the failures in the early days and

157
00:05:55,840 --> 00:06:00,240
I asked because to your point once you

158
00:05:57,440 --> 00:06:02,639
got the platform working? Exactly. Yeah.

159
00:06:00,240 --> 00:06:05,919
No, we had to fail a lot for about a

160
00:06:02,639 --> 00:06:08,639
decade um and spend a lot of money to

161
00:06:05,919 --> 00:06:11,440
figure out how to um ultimately get that

162
00:06:08,639 --> 00:06:13,199
modality to be operative. But once you

163
00:06:11,440 --> 00:06:15,440
get there, and it speaks a little bit

164
00:06:13,199 --> 00:06:17,919
to, you know, Anna's comments about the

165
00:06:15,440 --> 00:06:20,479
academic side of it. Um, in our case, we

166
00:06:17,919 --> 00:06:22,000
took took this on uh maybe earlier than

167
00:06:20,479 --> 00:06:24,400
we should have as a translational

168
00:06:22,000 --> 00:06:26,400
effort, but we took it on as a company

169
00:06:24,400 --> 00:06:28,199
um and had to really figure out how to

170
00:06:26,400 --> 00:06:31,840
make drugs out of

171
00:06:28,199 --> 00:06:34,479
sas. Um, you know, probably would it be

172
00:06:31,840 --> 00:06:36,319
best done in a company in hindsight? I

173
00:06:34,479 --> 00:06:38,800
mean there are things that companies do

174
00:06:36,319 --> 00:06:41,120
around um understanding pharmaceutical

175
00:06:38,800 --> 00:06:43,919
properties that I think may be best done

176
00:06:41,120 --> 00:06:46,319
in in a company but uh it took a long

177
00:06:43,919 --> 00:06:49,919
time before it was ready for prime time.

178
00:06:46,319 --> 00:06:51,919
And in that context the translation were

179
00:06:49,919 --> 00:06:53,440
you too quick or too slow to go to the

180
00:06:51,919 --> 00:06:55,360
clinic?

181
00:06:53,440 --> 00:06:58,160
Well, I think we could have we could

182
00:06:55,360 --> 00:07:01,440
have pivoted a little bit faster but not

183
00:06:58,160 --> 00:07:04,080
much. Not much. um the the key

184
00:07:01,440 --> 00:07:07,199
discoveries that that would have enabled

185
00:07:04,080 --> 00:07:10,160
us to make that pivot could have enabled

186
00:07:07,199 --> 00:07:12,960
us to do it um a little bit earlier, but

187
00:07:10,160 --> 00:07:15,680
we first needed to be hit by a 2x4 when

188
00:07:12,960 --> 00:07:18,400
the pharma industry left the field. Um

189
00:07:15,680 --> 00:07:21,360
uh and so that was helpful. Maybe we'll

190
00:07:18,400 --> 00:07:23,039
come back to that serendipity. So Kiana,

191
00:07:21,360 --> 00:07:25,280
you manage more programs than anybody

192
00:07:23,039 --> 00:07:26,720
here. Um I mean if you look at the total

193
00:07:25,280 --> 00:07:29,039
number of drugs getting approved every

194
00:07:26,720 --> 00:07:30,720
year it's not going up and a lot more

195
00:07:29,039 --> 00:07:32,800
money is being spent. So there's

196
00:07:30,720 --> 00:07:34,720
something sort of not there's something

197
00:07:32,800 --> 00:07:36,400
missing really from that point of view.

198
00:07:34,720 --> 00:07:39,199
Uh I think there's several things here.

199
00:07:36,400 --> 00:07:40,560
One is we do have incredible opportunity

200
00:07:39,199 --> 00:07:43,520
because we've got all these incredible

201
00:07:40,560 --> 00:07:45,039
tools thanks to companies like Alm of

202
00:07:43,520 --> 00:07:48,319
sticking with them and getting them the

203
00:07:45,039 --> 00:07:50,000
tools to work. So I think most targets

204
00:07:48,319 --> 00:07:51,759
now are druggable by one of the

205
00:07:50,000 --> 00:07:52,800
modalities that we have. I think Wendy

206
00:07:51,759 --> 00:07:55,039
you're absolutely right. We're much

207
00:07:52,800 --> 00:07:56,720
better at safety. Uh much fewer there's

208
00:07:55,039 --> 00:07:58,319
less attrition through safety because

209
00:07:56,720 --> 00:08:01,280
we're much better at understanding that

210
00:07:58,319 --> 00:08:02,560
and and predicting it. Um but I think we

211
00:08:01,280 --> 00:08:04,400
are trying to do much more difficult

212
00:08:02,560 --> 00:08:06,160
things than when I first started doing

213
00:08:04,400 --> 00:08:08,000
symptomatic treatment. We really are

214
00:08:06,160 --> 00:08:10,639
trying to get to the the heart and the

215
00:08:08,000 --> 00:08:13,680
cause of disease and even really trying

216
00:08:10,639 --> 00:08:15,840
to get you know a a cure if possible but

217
00:08:13,680 --> 00:08:17,520
certainly long-term remission or drugs

218
00:08:15,840 --> 00:08:19,199
that you know can really change your

219
00:08:17,520 --> 00:08:21,919
life so that you can forget about your

220
00:08:19,199 --> 00:08:23,680
disease. this is a high bar and so we

221
00:08:21,919 --> 00:08:25,440
we're it's good we're we're all pushing

222
00:08:23,680 --> 00:08:27,440
ourselves but I think what we need more

223
00:08:25,440 --> 00:08:28,960
of what worries me a bit is that a lot

224
00:08:27,440 --> 00:08:31,680
of these new tools are just going back

225
00:08:28,960 --> 00:08:34,399
to the same old targets that we've sort

226
00:08:31,680 --> 00:08:37,039
of drugged already and sometimes there's

227
00:08:34,399 --> 00:08:39,440
a benefit of coming with a new modality

228
00:08:37,039 --> 00:08:43,120
but I think where there's a real

229
00:08:39,440 --> 00:08:44,800
opportunity for academic uh research and

230
00:08:43,120 --> 00:08:47,360
and institutes like the broad and the

231
00:08:44,800 --> 00:08:49,279
work you do is coming up with those new

232
00:08:47,360 --> 00:08:52,320
completely new targets new approaches

233
00:08:49,279 --> 00:08:54,720
that we haven't uh drugged before.

234
00:08:52,320 --> 00:08:56,800
So, are we technology deficient or

235
00:08:54,720 --> 00:08:59,800
target deficient? I would say target

236
00:08:56,800 --> 00:08:59,800
deficient

237
00:09:00,360 --> 00:09:06,640
others. Yeah, go ahead. No, I I I'm just

238
00:09:04,640 --> 00:09:08,160
nodding voraciously because uh the

239
00:09:06,640 --> 00:09:10,399
frustration that I have is what I call

240
00:09:08,160 --> 00:09:12,640
too rare to care. Um so there are just

241
00:09:10,399 --> 00:09:14,320
so many literally thousands of

242
00:09:12,640 --> 00:09:16,399
conditions that are really not

243
00:09:14,320 --> 00:09:18,399
commercially viable. Uh they're too rare

244
00:09:16,399 --> 00:09:20,240
and they're too hard right now. And so

245
00:09:18,399 --> 00:09:22,160
instead what I see is companies go after

246
00:09:20,240 --> 00:09:23,519
the things that'll be incrementally

247
00:09:22,160 --> 00:09:25,040
better because there's already the

248
00:09:23,519 --> 00:09:26,399
market patients have already been

249
00:09:25,040 --> 00:09:28,480
identified. You know there's something

250
00:09:26,399 --> 00:09:29,920
that is softening in terms of that

251
00:09:28,480 --> 00:09:32,160
whereas we're not really making the

252
00:09:29,920 --> 00:09:34,480
inroads into the tough problems. But

253
00:09:32,160 --> 00:09:36,160
once we make the inroads I am optimistic

254
00:09:34,480 --> 00:09:38,000
that we'll be able to like a step

255
00:09:36,160 --> 00:09:40,160
function step up and bring a bunch of

256
00:09:38,000 --> 00:09:42,000
conditions with us. But but I am worried

257
00:09:40,160 --> 00:09:45,440
that as you said we're going after the

258
00:09:42,000 --> 00:09:47,360
same things over and over again. Yeah.

259
00:09:45,440 --> 00:09:49,200
But it's interesting as you translate

260
00:09:47,360 --> 00:09:51,120
things and understand the genetics

261
00:09:49,200 --> 00:09:53,120
better, it becomes smaller, right?

262
00:09:51,120 --> 00:09:55,040
Because the So if you treat diabetes,

263
00:09:53,120 --> 00:09:57,440
you still don't really understand why

264
00:09:55,040 --> 00:09:59,120
people develop di you have hypothesis.

265
00:09:57,440 --> 00:10:00,800
You may know some genes, but you're not

266
00:09:59,120 --> 00:10:03,600
treating the genetic cause of it. You're

267
00:10:00,800 --> 00:10:06,080
treating the symptoms. As you go smaller

268
00:10:03,600 --> 00:10:08,240
and smaller to the too rare to care, you

269
00:10:06,080 --> 00:10:10,560
understand the biology better, but it

270
00:10:08,240 --> 00:10:12,720
becomes too small. And how do you tackle

271
00:10:10,560 --> 00:10:14,720
that? I'd like to challenge you Fiona

272
00:10:12,720 --> 00:10:17,040
and next year when you publish your

273
00:10:14,720 --> 00:10:19,839
report and you're starting it with the

274
00:10:17,040 --> 00:10:21,839
renal diseases publish how many new

275
00:10:19,839 --> 00:10:23,920
patients did you treat with disorders

276
00:10:21,839 --> 00:10:26,640
that have never had a treatment before

277
00:10:23,920 --> 00:10:32,000
and if that incentivizes you then maybe

278
00:10:26,640 --> 00:10:33,839
you'll be care more caring about rare

279
00:10:32,000 --> 00:10:35,279
um well I was going to take the

280
00:10:33,839 --> 00:10:36,560
discussion in a slightly different

281
00:10:35,279 --> 00:10:39,440
direction in the sense of yes I think

282
00:10:36,560 --> 00:10:43,200
it's targets that we're missing and um

283
00:10:39,440 --> 00:10:45,279
maybe you know As academics, we tend to

284
00:10:43,200 --> 00:10:47,519
research targets in our labs, right? And

285
00:10:45,279 --> 00:10:50,079
we do work to try to understand those

286
00:10:47,519 --> 00:10:52,079
targets. But oftentimes we don't do that

287
00:10:50,079 --> 00:10:54,079
with a view of what is what Fiona needs

288
00:10:52,079 --> 00:10:56,399
or what John or you know, uh, needs in

289
00:10:54,079 --> 00:10:58,240
order to develop a program. Maybe

290
00:10:56,399 --> 00:11:00,800
because we have never been part of that

291
00:10:58,240 --> 00:11:03,120
process, you know, justifiably so. But

292
00:11:00,800 --> 00:11:04,640
I'm curious and this is very

293
00:11:03,120 --> 00:11:06,399
future-looking and I don't know what the

294
00:11:04,640 --> 00:11:08,959
answer will be but I'm interested in you

295
00:11:06,399 --> 00:11:10,399
know many many of our colleagues here um

296
00:11:08,959 --> 00:11:12,240
you know we're thinking about ways in

297
00:11:10,399 --> 00:11:16,240
which we can use uh machine learning and

298
00:11:12,240 --> 00:11:19,200
AI to try to understand um you know uh

299
00:11:16,240 --> 00:11:21,440
paths to targets better and one of the

300
00:11:19,200 --> 00:11:23,440
thoughts that I have is can we imagine

301
00:11:21,440 --> 00:11:25,680
like this is my sort of you know dream

302
00:11:23,440 --> 00:11:28,240
about sort of the R&D engine of the

303
00:11:25,680 --> 00:11:30,079
future which actually combines someone

304
00:11:28,240 --> 00:11:32,399
like me who works in the lab or any one

305
00:11:30,079 --> 00:11:35,440
of or uh colleagues who work in the lab

306
00:11:32,399 --> 00:11:37,360
with a co-pilot who never forgets

307
00:11:35,440 --> 00:11:39,519
anything about they've ever read read in

308
00:11:37,360 --> 00:11:42,800
the literature, right? And can help you

309
00:11:39,519 --> 00:11:45,440
uh do hypothesis uh driven work like we

310
00:11:42,800 --> 00:11:47,760
do, but without really forgetting any of

311
00:11:45,440 --> 00:11:48,959
the facts that you knew from before and

312
00:11:47,760 --> 00:11:50,880
you know papers that you have read

313
00:11:48,959 --> 00:11:52,800
before and that you have seen. Can we

314
00:11:50,880 --> 00:11:56,240
imagine combining those tools and having

315
00:11:52,800 --> 00:11:59,360
a co-pilot that uh can lead us to more

316
00:11:56,240 --> 00:12:01,760
um better testable hypotheses which of

317
00:11:59,360 --> 00:12:03,920
course would have to be adjudicated in

318
00:12:01,760 --> 00:12:05,760
the lab. Um I'm not saying this in a

319
00:12:03,920 --> 00:12:07,200
vacuum. I actually have discussions with

320
00:12:05,760 --> 00:12:09,360
my colleagues at Deep Mind about this

321
00:12:07,200 --> 00:12:11,600
all the time and there is a world in

322
00:12:09,360 --> 00:12:12,959
which this is not really just a a vision

323
00:12:11,600 --> 00:12:14,800
that it's actually something that can

324
00:12:12,959 --> 00:12:16,720
become a reality. But now you can

325
00:12:14,800 --> 00:12:18,880
imagine combining that with what I call

326
00:12:16,720 --> 00:12:20,800
biology at scale which a lot of it we do

327
00:12:18,880 --> 00:12:23,120
here which is testing multiple

328
00:12:20,800 --> 00:12:25,200
hypotheses at scale rather than one by

329
00:12:23,120 --> 00:12:27,200
one like when I was a grad student. So

330
00:12:25,200 --> 00:12:29,200
if you combine biology at scale with a

331
00:12:27,200 --> 00:12:31,600
scientific co-pilot who never forgets

332
00:12:29,200 --> 00:12:34,040
anything you know maybe there's a way in

333
00:12:31,600 --> 00:12:36,320
which we can get to better target ID

334
00:12:34,040 --> 00:12:37,519
faster. That's what I'm dreaming about.

335
00:12:36,320 --> 00:12:38,959
You know I'm not sure if it will be

336
00:12:37,519 --> 00:12:40,560
possible but I would love to be a part

337
00:12:38,959 --> 00:12:42,560
of that journey. So let's dig on that

338
00:12:40,560 --> 00:12:45,120
just a little bit. Um but in the context

339
00:12:42,560 --> 00:12:46,880
of rare okay I mean AI and of course

340
00:12:45,120 --> 00:12:49,839
drug discovery at large is maybe a

341
00:12:46,880 --> 00:12:52,399
bigger topic but the role of AI machine

342
00:12:49,839 --> 00:12:57,760
learning in rare is that going to open

343
00:12:52,399 --> 00:13:00,560
things up in a way that a step function.

344
00:12:57,760 --> 00:13:02,560
Well, I I I think it's I mean I I I I

345
00:13:00,560 --> 00:13:04,800
think we have to sort of make sure we're

346
00:13:02,560 --> 00:13:07,600
out of the hype cycle with with AI and

347
00:13:04,800 --> 00:13:11,519
you know I think it is mostly step

348
00:13:07,600 --> 00:13:15,680
function. Um important super important.

349
00:13:11,519 --> 00:13:18,160
Um I I think we're far away from a day I

350
00:13:15,680 --> 00:13:20,320
I've you know uh Demi had some

351
00:13:18,160 --> 00:13:22,480
interesting comments on 60 minutes and

352
00:13:20,320 --> 00:13:24,320
talked about curing all diseases within

353
00:13:22,480 --> 00:13:26,480
a short period of time. I I love that,

354
00:13:24,320 --> 00:13:28,880
but I I think it'll be a little bit

355
00:13:26,480 --> 00:13:30,560
longer than a short time. Um, and we're

356
00:13:28,880 --> 00:13:33,519
still far away from being able to, you

357
00:13:30,560 --> 00:13:35,760
know, type in a disease of interest into

358
00:13:33,519 --> 00:13:37,760
a computer and have it spit out a small

359
00:13:35,760 --> 00:13:39,680
molecule that you ought to synthesize to

360
00:13:37,760 --> 00:13:42,000
treat that disease. I mean, that's we're

361
00:13:39,680 --> 00:13:43,440
still a little bit away from that um

362
00:13:42,000 --> 00:13:46,320
realistically. And part of the reason is

363
00:13:43,440 --> 00:13:50,800
biology is really complicated. Um and

364
00:13:46,320 --> 00:13:53,120
the model systems that um are used um do

365
00:13:50,800 --> 00:13:56,160
not necessarily do all the experiments

366
00:13:53,120 --> 00:13:57,680
the right way. We have to feed biology

367
00:13:56,160 --> 00:14:00,639
with the right experiments to ultimately

368
00:13:57,680 --> 00:14:02,959
inform models at the end. And to that

369
00:14:00,639 --> 00:14:05,680
point, does that make rare one of sort

370
00:14:02,959 --> 00:14:07,600
of at the lower part of the list as

371
00:14:05,680 --> 00:14:09,279
opposed to larger diseases with more

372
00:14:07,600 --> 00:14:13,120
data that would drive perhaps more

373
00:14:09,279 --> 00:14:16,160
informative AI use?

374
00:14:13,120 --> 00:14:18,000
I yeah I I I think you you just have the

375
00:14:16,160 --> 00:14:20,480
power of genetics that's so powerful

376
00:14:18,000 --> 00:14:24,560
with rare that I think you know informs

377
00:14:20,480 --> 00:14:27,800
a better um set of hypotheses than what

378
00:14:24,560 --> 00:14:30,240
you have in larger complex

379
00:14:27,800 --> 00:14:32,160
diseases. That's my view. I don't know.

380
00:14:30,240 --> 00:14:34,079
I can just add when we're developing

381
00:14:32,160 --> 00:14:36,240
therapies genetic therapies for

382
00:14:34,079 --> 00:14:39,040
neurodedevelopmental disorders. I'm

383
00:14:36,240 --> 00:14:40,720
trying to think if there's a time in the

384
00:14:39,040 --> 00:14:42,560
development process that we went through

385
00:14:40,720 --> 00:14:44,880
that I felt that I needed more

386
00:14:42,560 --> 00:14:47,279
information or AI help and at the moment

387
00:14:44,880 --> 00:14:50,079
I will say no but I don't know what I

388
00:14:47,279 --> 00:14:52,000
don't know so maybe I'm wrong I I'm a

389
00:14:50,079 --> 00:14:54,399
bit more of a fan of AI and I we are

390
00:14:52,000 --> 00:14:56,639
using it really across all of the pieces

391
00:14:54,399 --> 00:14:58,399
of work that we do so I think one thing

392
00:14:56,639 --> 00:15:02,160
from in the context of rare is much

393
00:14:58,399 --> 00:15:04,000
better diagnosis of diseases earlier on

394
00:15:02,160 --> 00:15:05,839
um so people who don't have access to

395
00:15:04,000 --> 00:15:08,320
have you know whole genome

396
00:15:05,839 --> 00:15:11,120
sequencing. I think that there where you

397
00:15:08,320 --> 00:15:14,399
can already start to see AI very good is

398
00:15:11,120 --> 00:15:16,240
di diagnostic of rare diseases. Uh and

399
00:15:14,399 --> 00:15:17,680
so that's one area I I think and then

400
00:15:16,240 --> 00:15:20,160
that goes to also then some of the

401
00:15:17,680 --> 00:15:22,399
biomarkers where AI can help with

402
00:15:20,160 --> 00:15:24,880
imaging for example and and reading

403
00:15:22,399 --> 00:15:27,279
scans and subdividing what we thought

404
00:15:24,880 --> 00:15:28,560
was one disease into five diseases. Uh

405
00:15:27,279 --> 00:15:29,920
so it turns out what we thought was a

406
00:15:28,560 --> 00:15:32,160
common disease actually is made up of

407
00:15:29,920 --> 00:15:33,600
lots of rare diseases. uh AI is very

408
00:15:32,160 --> 00:15:35,360
good at doing that and we've already

409
00:15:33,600 --> 00:15:38,880
seen a number of publications there for

410
00:15:35,360 --> 00:15:42,000
example uh designing molecules uh we're

411
00:15:38,880 --> 00:15:43,519
working with isomorphic uh so that's

412
00:15:42,000 --> 00:15:45,680
another area where we're showing and

413
00:15:43,519 --> 00:15:48,000
also predictive safety and then in trial

414
00:15:45,680 --> 00:15:50,480
design we're using a lot of AI to design

415
00:15:48,000 --> 00:15:51,839
the trials what you can do is look at

416
00:15:50,480 --> 00:15:53,920
all the different parameters of a

417
00:15:51,839 --> 00:15:55,839
protocol of how you recruit people into

418
00:15:53,920 --> 00:15:58,160
a trial and how changing those

419
00:15:55,839 --> 00:16:00,160
parameters will alter the rate of

420
00:15:58,160 --> 00:16:02,240
recruitment and so that hopefully is one

421
00:16:00,160 --> 00:16:04,560
of the ways you and really speed up uh

422
00:16:02,240 --> 00:16:05,759
clinical development. Okay. So let's u

423
00:16:04,560 --> 00:16:08,959
I'm going to go back to the community

424
00:16:05,759 --> 00:16:13,440
here but I want to start with um so

425
00:16:08,959 --> 00:16:16,079
should children's the broad and novartis

426
00:16:13,440 --> 00:16:17,920
all be doing the same thing and the

427
00:16:16,079 --> 00:16:19,279
scientists right I mean noartis at large

428
00:16:17,920 --> 00:16:21,680
is doing things different but at the

429
00:16:19,279 --> 00:16:23,839
research level is the research lab at

430
00:16:21,680 --> 00:16:26,839
children's novartis and the broad are

431
00:16:23,839 --> 00:16:26,839
those

432
00:16:26,959 --> 00:16:31,040
no question we should not be doing the

433
00:16:28,720 --> 00:16:32,560
same things we should we're each good at

434
00:16:31,040 --> 00:16:34,000
different things right but on the other

435
00:16:32,560 --> 00:16:36,240
hand to the points that were made

436
00:16:34,000 --> 00:16:38,399
earlier. It's kind of ridiculous. We're

437
00:16:36,240 --> 00:16:40,000
in not that I mean 20 minutes from each

438
00:16:38,399 --> 00:16:41,279
other, right? But how often do we get

439
00:16:40,000 --> 00:16:42,800
together in terms of actually

440
00:16:41,279 --> 00:16:44,079
brainstorming and figuring out our

441
00:16:42,800 --> 00:16:45,839
synergies and what we could do

442
00:16:44,079 --> 00:16:48,079
collectively together? Unfortunately,

443
00:16:45,839 --> 00:16:49,360
not very often. Um, but we clearly have

444
00:16:48,079 --> 00:16:51,040
different strengths and weaknesses.

445
00:16:49,360 --> 00:16:52,720
Maybe we'll go into them, but they're

446
00:16:51,040 --> 00:16:54,160
people that are close to the patients.

447
00:16:52,720 --> 00:16:56,160
And the people that are close to the

448
00:16:54,160 --> 00:16:57,839
patients ultimately know what are the

449
00:16:56,160 --> 00:16:59,600
critical factors in terms of what are

450
00:16:57,839 --> 00:17:00,959
the unmet needs, what are the pressure

451
00:16:59,600 --> 00:17:02,800
points, how do you find patients,

452
00:17:00,959 --> 00:17:05,039
recruit patients, how do you design a

453
00:17:02,800 --> 00:17:06,559
trial that they'll actually do and be

454
00:17:05,039 --> 00:17:08,240
able to comply with, you know, what's

455
00:17:06,559 --> 00:17:09,760
reasonable in terms of this and at the

456
00:17:08,240 --> 00:17:11,360
end of the day, what matters most to

457
00:17:09,760 --> 00:17:13,199
them in terms of an outcome measure for

458
00:17:11,360 --> 00:17:14,799
the FDA. So, I mean, you need the

459
00:17:13,199 --> 00:17:16,880
clinicians who really know these

460
00:17:14,799 --> 00:17:19,120
conditions extremely well. On the other

461
00:17:16,880 --> 00:17:20,640
hand, I can say and you know this is

462
00:17:19,120 --> 00:17:23,360
somewhat self- serving and maybe an

463
00:17:20,640 --> 00:17:25,760
unabashed advertisement. I mean, excuse

464
00:17:23,360 --> 00:17:27,839
me, Children's and the Broad Clearly

465
00:17:25,760 --> 00:17:30,320
realize that we are complimentary and so

466
00:17:27,839 --> 00:17:32,160
we're strategically partnering uh to be

467
00:17:30,320 --> 00:17:34,080
able to put these pieces together and to

468
00:17:32,160 --> 00:17:36,000
know that some of the technologies, some

469
00:17:34,080 --> 00:17:38,480
of the scalability that's here is going

470
00:17:36,000 --> 00:17:40,160
to get us there faster. And so, uh,

471
00:17:38,480 --> 00:17:41,679
we're, it's clear that we should be

472
00:17:40,160 --> 00:17:43,280
partnering. And then, I mean, we would

473
00:17:41,679 --> 00:17:45,200
love to be able to partner with whether

474
00:17:43,280 --> 00:17:46,480
it's Noardis or quite honestly anyone

475
00:17:45,200 --> 00:17:48,480
else who wants to come to the party

476
00:17:46,480 --> 00:17:50,080
because we just have too much work to be

477
00:17:48,480 --> 00:17:52,320
done. I mean, I'm sitting in the

478
00:17:50,080 --> 00:17:54,160
hospital and I've got 200 kids with rare

479
00:17:52,320 --> 00:17:56,160
genetic diseases that desperately need

480
00:17:54,160 --> 00:17:59,760
treatments. And so, anyone who wants to

481
00:17:56,160 --> 00:18:02,400
come to the party, come on over.

482
00:17:59,760 --> 00:18:04,400
So Diana, yeah, I mean I I think that

483
00:18:02,400 --> 00:18:06,240
all working on the same problems is a

484
00:18:04,400 --> 00:18:07,919
good thing because you actually want

485
00:18:06,240 --> 00:18:09,840
different perspectives and different

486
00:18:07,919 --> 00:18:11,919
approaches to tackle the same problems.

487
00:18:09,840 --> 00:18:14,400
I think we're more likely to get success

488
00:18:11,919 --> 00:18:16,160
when we are thinking about a problem

489
00:18:14,400 --> 00:18:17,520
together and then coming at it from

490
00:18:16,160 --> 00:18:20,160
different directions, bringing our

491
00:18:17,520 --> 00:18:21,760
different types of expertise. Uh and

492
00:18:20,160 --> 00:18:23,120
even doing the same experiment in

493
00:18:21,760 --> 00:18:24,480
different places is actually pretty

494
00:18:23,120 --> 00:18:26,960
useful. I mean it just builds that

495
00:18:24,480 --> 00:18:28,640
credibility and and certainly you know

496
00:18:26,960 --> 00:18:30,559
if you if you've done an experiment and

497
00:18:28,640 --> 00:18:32,160
you see somebody else doing it you

498
00:18:30,559 --> 00:18:34,080
actually feel more confident about the

499
00:18:32,160 --> 00:18:37,200
results. So I I don't think duplicating

500
00:18:34,080 --> 00:18:38,559
is is necessarily a bad thing but uh of

501
00:18:37,200 --> 00:18:42,000
course there are different strengths in

502
00:18:38,559 --> 00:18:44,000
different labs and then how how we can

503
00:18:42,000 --> 00:18:46,640
carry carry on working together. I think

504
00:18:44,000 --> 00:18:48,480
this is one and sharing the expertise uh

505
00:18:46,640 --> 00:18:50,080
where we need it. Especially I think

506
00:18:48,480 --> 00:18:51,679
what we're not good at is sharing the

507
00:18:50,080 --> 00:18:53,600
expertise of when things haven't worked

508
00:18:51,679 --> 00:18:55,840
very well. We tend to share the

509
00:18:53,600 --> 00:18:57,600
successes and celebrate them which is

510
00:18:55,840 --> 00:19:00,799
great but we don't always share the

511
00:18:57,600 --> 00:19:03,039
learnings of the failures very well. So

512
00:19:00,799 --> 00:19:04,480
the same mistakes get made over and over

513
00:19:03,039 --> 00:19:06,400
again and that's what I think is very

514
00:19:04,480 --> 00:19:08,240
wasteful in the work that we're doing.

515
00:19:06,400 --> 00:19:11,600
We don't have time to all be making the

516
00:19:08,240 --> 00:19:14,400
same mistakes this over and over again.

517
00:19:11,600 --> 00:19:15,520
So uh this is next question skirts a

518
00:19:14,400 --> 00:19:17,360
little bit on that and maybe we come

519
00:19:15,520 --> 00:19:19,120
back try to put a bow around the need to

520
00:19:17,360 --> 00:19:21,440
work together probably doesn't require

521
00:19:19,120 --> 00:19:23,760
much of a bow. Um, John, how important

522
00:19:21,440 --> 00:19:26,480
is the business model here for the rare

523
00:19:23,760 --> 00:19:29,840
disease? Uh, I think it's hugely

524
00:19:26,480 --> 00:19:33,520
important. Um, and I I wish it weren't.

525
00:19:29,840 --> 00:19:36,160
Um, but it it really is. And it it um I

526
00:19:33,520 --> 00:19:38,520
mean, for example, I I think, you know,

527
00:19:36,160 --> 00:19:41,880
until we have more commercial

528
00:19:38,520 --> 00:19:45,360
successes with some of these therapeutic

529
00:19:41,880 --> 00:19:48,400
approaches, we could see, you know, many

530
00:19:45,360 --> 00:19:52,559
many um very very promising treatments

531
00:19:48,400 --> 00:19:55,960
and cures. not ever get funded. Um, and

532
00:19:52,559 --> 00:19:58,480
I I I think um

533
00:19:55,960 --> 00:20:02,640
unfortunately um you know that reward

534
00:19:58,480 --> 00:20:05,120
system is super important to um support

535
00:20:02,640 --> 00:20:07,600
the investment that's needed even for

536
00:20:05,120 --> 00:20:08,960
small rare diseases, it's still a

537
00:20:07,600 --> 00:20:11,320
significant investment that's needed.

538
00:20:08,960 --> 00:20:15,039
And if there's no meaningful reward

539
00:20:11,320 --> 00:20:16,559
system, then people will just stop, you

540
00:20:15,039 --> 00:20:22,400
know, investors will just stop putting

541
00:20:16,559 --> 00:20:25,679
money behind it. Um so it it is a um big

542
00:20:22,400 --> 00:20:27,919
concern of mine um because it would be a

543
00:20:25,679 --> 00:20:29,679
tragedy. I mean, you know, many of the

544
00:20:27,919 --> 00:20:32,600
of the modalities, you know, gene

545
00:20:29,679 --> 00:20:36,240
therapy, gene editing, and so forth are

546
00:20:32,600 --> 00:20:38,880
cures and and and you know, will we get

547
00:20:36,240 --> 00:20:41,919
to a point where society says we we

548
00:20:38,880 --> 00:20:44,559
don't can't fund cures anymore um

549
00:20:41,919 --> 00:20:46,080
because um they're too expensive or

550
00:20:44,559 --> 00:20:47,679
whatever the case might be or we we

551
00:20:46,080 --> 00:20:49,600
don't we want to create obstacles around

552
00:20:47,679 --> 00:20:51,440
their adoption and so forth. That would

553
00:20:49,600 --> 00:20:53,120
be a tragedy. That would really be a

554
00:20:51,440 --> 00:20:55,520
tragedy.

555
00:20:53,120 --> 00:20:58,720
I'd like to put a plug in for parents as

556
00:20:55,520 --> 00:21:01,080
well. Lisa, are you still here? Lisa,

557
00:20:58,720 --> 00:21:03,280
Lisa Manister from the Kakna 1A

558
00:21:01,080 --> 00:21:05,280
Foundation. Patients are doing it.

559
00:21:03,280 --> 00:21:07,600
They're not waiting for us anymore. And

560
00:21:05,280 --> 00:21:09,679
we cannot ignore it. And what's

561
00:21:07,600 --> 00:21:13,760
happening now is that once there was no

562
00:21:09,679 --> 00:21:15,760
cure, no no therapy, no option. Now

563
00:21:13,760 --> 00:21:17,520
foundations are fun, patient advocacy

564
00:21:15,760 --> 00:21:20,000
groups are funding research, moving

565
00:21:17,520 --> 00:21:22,799
things closer to the clinic, moving the

566
00:21:20,000 --> 00:21:24,320
bottleneck to the clinical studies, but

567
00:21:22,799 --> 00:21:26,720
still there's no one that will pick them

568
00:21:24,320 --> 00:21:28,640
up and help them commercialize it. So to

569
00:21:26,720 --> 00:21:31,520
John's point, as a society, as a

570
00:21:28,640 --> 00:21:33,200
community, it's our responsibility to

571
00:21:31,520 --> 00:21:35,679
help these therapies get to market

572
00:21:33,200 --> 00:21:37,360
because they work. They will make a

573
00:21:35,679 --> 00:21:40,480
difference in famil family's lives. And

574
00:21:37,360 --> 00:21:43,280
we need a way to to fund these and to

575
00:21:40,480 --> 00:21:45,200
get these to patients.

576
00:21:43,280 --> 00:21:48,000
What did we learn from yesterday's case

577
00:21:45,200 --> 00:21:50,320
and how do we extrapolate from there?

578
00:21:48,000 --> 00:21:52,760
This the KJ case everybody was talked

579
00:21:50,320 --> 00:21:56,240
about earlier and of

580
00:21:52,760 --> 00:21:58,880
one thought on that. Yeah, I mean I'll

581
00:21:56,240 --> 00:22:00,799
just say cautious uh but optimism,

582
00:21:58,880 --> 00:22:02,960
right? I mean it's very early days in

583
00:22:00,799 --> 00:22:05,200
terms of this. Um, you know, it's sort

584
00:22:02,960 --> 00:22:07,679
of I can't tell you my email box has

585
00:22:05,200 --> 00:22:09,840
exploded over the last 24 hours in terms

586
00:22:07,679 --> 00:22:11,440
of every it feels like thousands of

587
00:22:09,840 --> 00:22:13,440
patients I've ever treated all saying is

588
00:22:11,440 --> 00:22:15,760
this going to work for me and when. Uh,

589
00:22:13,440 --> 00:22:18,080
so there's hope uh which is I think

590
00:22:15,760 --> 00:22:19,840
incredibly important for the community.

591
00:22:18,080 --> 00:22:21,760
On the other hands, there is this

592
00:22:19,840 --> 00:22:23,760
measure of realism we need to have so

593
00:22:21,760 --> 00:22:26,880
that people uh don't have unrealistic

594
00:22:23,760 --> 00:22:28,480
expectations. Um it is and and I would

595
00:22:26,880 --> 00:22:30,159
love to have anyone in this room who can

596
00:22:28,480 --> 00:22:32,080
help us think about this because I do

597
00:22:30,159 --> 00:22:34,159
think that just as a if you think about

598
00:22:32,080 --> 00:22:36,480
an editing technology there are enough

599
00:22:34,159 --> 00:22:38,760
editors now to take care of the majority

600
00:22:36,480 --> 00:22:42,000
of mutations at least DNA sequence

601
00:22:38,760 --> 00:22:44,240
mutations. Um the problem in part is

602
00:22:42,000 --> 00:22:46,159
delivery and safety and long-term

603
00:22:44,240 --> 00:22:48,080
knowing about the durability and if we

604
00:22:46,159 --> 00:22:50,480
can get to saturation in terms of the

605
00:22:48,080 --> 00:22:52,480
cells that we're trying to target uh in

606
00:22:50,480 --> 00:22:54,080
in the brain in particular uh that's

607
00:22:52,480 --> 00:22:55,840
hard in terms of being able to know that

608
00:22:54,080 --> 00:22:57,840
we're going to get to enough of them. So

609
00:22:55,840 --> 00:22:59,600
I can think of about I don't know a

610
00:22:57,840 --> 00:23:01,760
couple dozen sort of combinatorial

611
00:22:59,600 --> 00:23:03,360
problems you need to solve that if you

612
00:23:01,760 --> 00:23:05,120
could solve all of those and I know

613
00:23:03,360 --> 00:23:07,280
that's a lot but if you could solve all

614
00:23:05,120 --> 00:23:09,600
of those you can actually think about

615
00:23:07,280 --> 00:23:11,840
you know 10,000 rare genetic diseases

616
00:23:09,600 --> 00:23:14,159
that within the next whatever period of

617
00:23:11,840 --> 00:23:16,000
time are potentially treatable if you

618
00:23:14,159 --> 00:23:18,240
can diagnose them and treat them early

619
00:23:16,000 --> 00:23:20,480
enough. It gets to the point as I think

620
00:23:18,240 --> 00:23:22,400
about that with scalability. How much

621
00:23:20,480 --> 00:23:24,880
does it cost for each of those? Will the

622
00:23:22,400 --> 00:23:26,640
FDA and other regul regulators will get

623
00:23:24,880 --> 00:23:28,799
to a point where you can literally dial

624
00:23:26,640 --> 00:23:29,679
in the sequence for the guides? Uh, and

625
00:23:28,799 --> 00:23:31,840
you're going to have the same

626
00:23:29,679 --> 00:23:33,520
manufacturing technology for delivery

627
00:23:31,840 --> 00:23:35,520
systems and we'll know enough about that

628
00:23:33,520 --> 00:23:38,320
safety to be able to bring the cost of

629
00:23:35,520 --> 00:23:40,240
goods down significantly. I don't know.

630
00:23:38,320 --> 00:23:42,559
But I would love to be able to before I

631
00:23:40,240 --> 00:23:44,320
retire. I've given myself that. I give

632
00:23:42,559 --> 00:23:45,840
that 20 years for any of you who care.

633
00:23:44,320 --> 00:23:49,000
We've got 20 years to be able to get

634
00:23:45,840 --> 00:23:49,000
this done.

635
00:23:49,200 --> 00:23:53,440
Are they learning? So the only thing I

636
00:23:51,679 --> 00:23:57,200
would comment about that it was heroic.

637
00:23:53,440 --> 00:23:59,280
I mean anybody reading that um one two

638
00:23:57,200 --> 00:24:01,840
it was incredible the willingness of the

639
00:23:59,280 --> 00:24:03,039
different parties. So multiple five I

640
00:24:01,840 --> 00:24:05,039
don't know how many different parties

641
00:24:03,039 --> 00:24:06,960
had to step in to make that happen and

642
00:24:05,039 --> 00:24:09,919
happen in a short period of time. There

643
00:24:06,960 --> 00:24:12,080
is no business model for that. Um that's

644
00:24:09,919 --> 00:24:13,840
not scalable what happened uh yesterday

645
00:24:12,080 --> 00:24:16,640
which is why most of these emails will

646
00:24:13,840 --> 00:24:20,880
go um without the hope being fulfilled.

647
00:24:16,640 --> 00:24:23,440
But um it clearly falls in that realm of

648
00:24:20,880 --> 00:24:27,159
if any group what wherever the line is

649
00:24:23,440 --> 00:24:30,000
that it is too small needs a collective

650
00:24:27,159 --> 00:24:32,159
effort. And my impression of government

651
00:24:30,000 --> 00:24:33,000
putting aside our current state but in

652
00:24:32,159 --> 00:24:35,679
general

653
00:24:33,000 --> 00:24:38,159
is I think we need to present ourselves

654
00:24:35,679 --> 00:24:39,840
to the government in a way that says we

655
00:24:38,159 --> 00:24:42,720
have the solution. We're just missing

656
00:24:39,840 --> 00:24:44,159
the funds. They pay one way or another.

657
00:24:42,720 --> 00:24:46,320
It's you know depending on where you are

658
00:24:44,159 --> 00:24:48,320
in the system. But if you could organize

659
00:24:46,320 --> 00:24:50,240
the rare disease world in a way that

660
00:24:48,320 --> 00:24:51,600
said you know there's a super fund for

661
00:24:50,240 --> 00:24:53,760
this kind of thing. I mean those kind of

662
00:24:51,600 --> 00:24:55,679
things can be done but they get created

663
00:24:53,760 --> 00:24:57,840
when pe when they become real. You know

664
00:24:55,679 --> 00:24:59,520
they don't nobody creates a super fun

665
00:24:57,840 --> 00:25:01,760
around an idea. It needs to have a

666
00:24:59,520 --> 00:25:03,520
certain level of tangibility. So I mean

667
00:25:01,760 --> 00:25:04,880
for me the hospitals are actually the

668
00:25:03,520 --> 00:25:07,279
key here. the leading children's

669
00:25:04,880 --> 00:25:10,080
hospitals, I mean in the UK, Great Orman

670
00:25:07,279 --> 00:25:13,279
Street and Boston Children's Hospital,

671
00:25:10,080 --> 00:25:16,240
uh who see these newborn patients and if

672
00:25:13,279 --> 00:25:19,600
you can get the diagnosis quite quickly,

673
00:25:16,240 --> 00:25:21,360
um and have the funds to provide the

674
00:25:19,600 --> 00:25:24,000
technology which if it's like all

675
00:25:21,360 --> 00:25:25,600
optimized so it's an efficient system

676
00:25:24,000 --> 00:25:27,760
that maybe you've actually got in the

677
00:25:25,600 --> 00:25:29,919
hospital. I I don't see this as a farmer

678
00:25:27,760 --> 00:25:31,360
type of commercial opportunity at all,

679
00:25:29,919 --> 00:25:33,520
but I think there could be a lot of

680
00:25:31,360 --> 00:25:34,720
other creative ways centered around

681
00:25:33,520 --> 00:25:37,520
hospitals. I mean, you talked about

682
00:25:34,720 --> 00:25:39,919
surgery as a an equivalent, but it is it

683
00:25:37,520 --> 00:25:42,799
should be a service that a hospital

684
00:25:39,919 --> 00:25:45,600
specialized hospital can provide that is

685
00:25:42,799 --> 00:25:48,400
part of the you comes with whether it's

686
00:25:45,600 --> 00:25:50,200
the insurance or the hospital, but you

687
00:25:48,400 --> 00:25:52,159
know, we pay for other health care

688
00:25:50,200 --> 00:25:55,520
interventions. Uh and this should just

689
00:25:52,159 --> 00:25:56,960
be one of those in my view. Okay. Uh it

690
00:25:55,520 --> 00:25:58,480
saves money in the long run. This is the

691
00:25:56,960 --> 00:26:00,400
thing. I mean, so if you actually do the

692
00:25:58,480 --> 00:26:03,279
economics of how much it would take to

693
00:26:00,400 --> 00:26:05,200
care for many of these children, um it's

694
00:26:03,279 --> 00:26:07,120
worth it. It's a good point. Yeah. The

695
00:26:05,200 --> 00:26:09,919
only thing I want to add to this is, and

696
00:26:07,120 --> 00:26:12,320
again, building on what Wendy also said,

697
00:26:09,919 --> 00:26:14,159
um I mean, this is the place where David

698
00:26:12,320 --> 00:26:15,919
Lu conceived of a lot of these editors,

699
00:26:14,159 --> 00:26:17,520
Christopher, you know, Fun Zang, of

700
00:26:15,919 --> 00:26:19,600
course, here at the Broad. So we feel

701
00:26:17,520 --> 00:26:21,440
like we have a unique role to play in

702
00:26:19,600 --> 00:26:23,200
partnership with with children's for

703
00:26:21,440 --> 00:26:25,279
example and we've actually formalized

704
00:26:23,200 --> 00:26:27,200
this um it's always about the people

705
00:26:25,279 --> 00:26:28,640
Wendy and I got together and um the

706
00:26:27,200 --> 00:26:30,320
teams got together and we found that

707
00:26:28,640 --> 00:26:31,760
there's a lot of opportunity so it's

708
00:26:30,320 --> 00:26:33,120
just the beginning but it speaks a

709
00:26:31,760 --> 00:26:34,880
little bit to what I said earlier in the

710
00:26:33,120 --> 00:26:36,960
day about you know like maybe we can

711
00:26:34,880 --> 00:26:39,200
think about this as a nodal technology

712
00:26:36,960 --> 00:26:41,039
or a platform programmable therapeutics

713
00:26:39,200 --> 00:26:42,799
that are you know you don't reinvent the

714
00:26:41,039 --> 00:26:44,320
wheel every which time for every

715
00:26:42,799 --> 00:26:46,799
mutation but rather you find the

716
00:26:44,320 --> 00:26:48,640
commonalities the nodes and you harness

717
00:26:46,799 --> 00:26:50,320
them to develop these therapies. But of

718
00:26:48,640 --> 00:26:52,320
course, the business model or how one

719
00:26:50,320 --> 00:26:54,080
funds something like this and whether

720
00:26:52,320 --> 00:26:56,880
there's a super fund or philanthropy

721
00:26:54,080 --> 00:26:59,200
that plays a role in this remains to be

722
00:26:56,880 --> 00:27:01,360
explored. But maybe this to bring it

723
00:26:59,200 --> 00:27:03,120
back is in some ways why we have to all

724
00:27:01,360 --> 00:27:04,320
get together and brainstorm about these

725
00:27:03,120 --> 00:27:06,960
things because it's not going to come

726
00:27:04,320 --> 00:27:08,559
from you know not sort of being as part

727
00:27:06,960 --> 00:27:10,720
of a dialogue and bringing all the

728
00:27:08,559 --> 00:27:12,080
stakeholders together for how we're

729
00:27:10,720 --> 00:27:14,240
going to you know eventually achieve

730
00:27:12,080 --> 00:27:16,320
these goals whether it's for more common

731
00:27:14,240 --> 00:27:17,679
rare diseases like the ones that Fiona

732
00:27:16,320 --> 00:27:20,080
is working on or whether it's more of

733
00:27:17,679 --> 00:27:23,039
these sort of like end of a few if you

734
00:27:20,080 --> 00:27:26,240
will to help a few children um who are

735
00:27:23,039 --> 00:27:28,240
waiting. So, so maybe it's 425. Maybe

736
00:27:26,240 --> 00:27:30,720
that's a good place to end on. Um, the

737
00:27:28,240 --> 00:27:33,200
biggest challenge for rare is rare and

738
00:27:30,720 --> 00:27:35,200
you got to deal with the numbers and uh

739
00:27:33,200 --> 00:27:37,200
collectively is often the best way to do

740
00:27:35,200 --> 00:27:40,200
that. So, thank you. Thank you. Thank

741
00:27:37,200 --> 00:27:40,200
you.

742
00:27:43,440 --> 00:27:47,039
Okay. Well, this is going to be very

743
00:27:44,960 --> 00:27:48,480
short in terms of closing our day. Thank

744
00:27:47,039 --> 00:27:49,919
you each and every one of you. And I

745
00:27:48,480 --> 00:27:51,840
know there's many of you, probably

746
00:27:49,919 --> 00:27:53,760
hundreds of you online from what I hear.

747
00:27:51,840 --> 00:27:55,600
So, thank you all for being here with us

748
00:27:53,760 --> 00:27:57,679
today. It was a wonderful day of

749
00:27:55,600 --> 00:28:00,559
exchange of ideas. Um, I hope that we

750
00:27:57,679 --> 00:28:03,520
were able to celebrate um some of the uh

751
00:28:00,559 --> 00:28:04,880
recent uh successes of these early seed

752
00:28:03,520 --> 00:28:07,200
programs that the latter seekers

753
00:28:04,880 --> 00:28:09,919
accelerator has been advancing. There's

754
00:28:07,200 --> 00:28:12,320
much much more to do of course uh but

755
00:28:09,919 --> 00:28:14,000
we're hoping to uh grow this initiative,

756
00:28:12,320 --> 00:28:16,240
develop uh relationships with everyone

757
00:28:14,000 --> 00:28:18,080
in the ecosystem. So, to each and every

758
00:28:16,240 --> 00:28:21,799
one of you, thank you and see you next

759
00:28:18,080 --> 00:28:21,799
time. Have a good evening.

