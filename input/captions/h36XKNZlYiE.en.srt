1
00:00:04,000 --> 00:00:08,720
Thanks so much Asia and thanks to um

2
00:00:06,799 --> 00:00:11,360
everyone who invited me here. I'm really

3
00:00:08,720 --> 00:00:13,120
happy to be here and it's a privilege to

4
00:00:11,360 --> 00:00:15,120
be able to talk to you all and I'm

5
00:00:13,120 --> 00:00:18,320
looking forward to hearing your thoughts

6
00:00:15,120 --> 00:00:19,680
at the end about this topic as well. So

7
00:00:18,320 --> 00:00:22,400
I'm going to speak today about

8
00:00:19,680 --> 00:00:25,680
algorithmic monoculture and the ethics

9
00:00:22,400 --> 00:00:29,240
of systemic exclusion. Um so as A Asia

10
00:00:25,680 --> 00:00:31,039
mentioned I am lucky enough to be uh a

11
00:00:29,240 --> 00:00:33,600
computationallyincclined philosopher who

12
00:00:31,039 --> 00:00:35,520
gets to uh exist in both computer

13
00:00:33,600 --> 00:00:37,719
science and philosophy world and I

14
00:00:35,520 --> 00:00:40,879
couldn't do that without my fantastic

15
00:00:37,719 --> 00:00:42,719
co-authors. Um so you see them here in

16
00:00:40,879 --> 00:00:45,920
their full glory but their little heads

17
00:00:42,719 --> 00:00:47,680
will pop up on the slides uh as I've

18
00:00:45,920 --> 00:00:50,239
drawn projects that I've done with them.

19
00:00:47,680 --> 00:00:52,160
So look forward to my majestic

20
00:00:50,239 --> 00:00:54,960
co-authors return m returning and thank

21
00:00:52,160 --> 00:00:57,239
you to all of them for um the joyful

22
00:00:54,960 --> 00:01:00,800
work that we've been able to do

23
00:00:57,239 --> 00:01:03,840
together. So imagine you are a hiring

24
00:01:00,800 --> 00:01:06,159
manager and out of hundreds or thousands

25
00:01:03,840 --> 00:01:09,760
of résumés you have to find three people

26
00:01:06,159 --> 00:01:13,520
to interview and one to hire. And this

27
00:01:09,760 --> 00:01:15,840
in our computer science terms is a very

28
00:01:13,520 --> 00:01:17,840
uh information constrained problem. We

29
00:01:15,840 --> 00:01:19,280
have time constraints. We have

30
00:01:17,840 --> 00:01:21,680
information constraints. There are some

31
00:01:19,280 --> 00:01:23,360
things that we we might want to know and

32
00:01:21,680 --> 00:01:25,759
we're not allowed to know perhaps for

33
00:01:23,360 --> 00:01:27,520
good reason. Uh there are privacy

34
00:01:25,759 --> 00:01:29,200
constraints. And perhaps most

35
00:01:27,520 --> 00:01:31,360
importantly, the future space is really

36
00:01:29,200 --> 00:01:33,600
undefined. We don't actually know what

37
00:01:31,360 --> 00:01:35,360
makes a good employee right now or what

38
00:01:33,600 --> 00:01:38,640
will make a good employee in the

39
00:01:35,360 --> 00:01:41,280
uncertain future. And so into this

40
00:01:38,640 --> 00:01:44,960
morass of uncertainty step, automated

41
00:01:41,280 --> 00:01:47,439
hiring systems. Um most systems will do

42
00:01:44,960 --> 00:01:49,360
this first round screening. So discard

43
00:01:47,439 --> 00:01:51,759
about half of applications before a

44
00:01:49,360 --> 00:01:55,040
human being ever looks at them. Um and

45
00:01:51,759 --> 00:01:57,040
98% of Fortune 5 companies use some

46
00:01:55,040 --> 00:01:59,280
version of an applicant tracking system

47
00:01:57,040 --> 00:02:01,119
which has some kind of sorting. Uh but

48
00:01:59,280 --> 00:02:03,200
many of them use this more extensive

49
00:02:01,119 --> 00:02:03,960
first round screening algorithmic system

50
00:02:03,200 --> 00:02:07,119
as

51
00:02:03,960 --> 00:02:10,239
well. And the hope is that uh these

52
00:02:07,119 --> 00:02:12,599
hiring systems will save time and money

53
00:02:10,239 --> 00:02:15,200
uh reduce bias perhaps choose better

54
00:02:12,599 --> 00:02:17,599
candidates and most importantly produce

55
00:02:15,200 --> 00:02:20,000
more consistent judgments. And it's that

56
00:02:17,599 --> 00:02:22,560
consistency that I want to focus on in

57
00:02:20,000 --> 00:02:25,440
the talk today. So imagine from the

58
00:02:22,560 --> 00:02:28,480
candidate perspective you're applying to

59
00:02:25,440 --> 00:02:30,959
what I hope are three MIT area uh

60
00:02:28,480 --> 00:02:32,879
grocery stores. The Target, the Daily

61
00:02:30,959 --> 00:02:35,040
Table, and the HMart. I'm getting some

62
00:02:32,879 --> 00:02:37,280
of this. So, I'm hearing these are not

63
00:02:35,040 --> 00:02:38,800
the best grocery stores in the area, but

64
00:02:37,280 --> 00:02:40,720
maybe you didn't know that. Maybe there

65
00:02:38,800 --> 00:02:43,040
are yet better grocery stores that you

66
00:02:40,720 --> 00:02:47,040
should have applied to. Uh, I am going

67
00:02:43,040 --> 00:02:50,400
to defend the HMart, however. Um, okay.

68
00:02:47,040 --> 00:02:52,640
So you imagine perhaps naively that when

69
00:02:50,400 --> 00:02:54,400
you submit these three separate

70
00:02:52,640 --> 00:02:56,720
applications to three completely

71
00:02:54,400 --> 00:02:59,280
separate business entities that your

72
00:02:56,720 --> 00:03:03,440
file will be screened by three different

73
00:02:59,280 --> 00:03:06,239
sets of managers, actors, whoever. Um

74
00:03:03,440 --> 00:03:08,319
human beings will look at these files.

75
00:03:06,239 --> 00:03:10,720
But it might be that all three companies

76
00:03:08,319 --> 00:03:13,800
have purchased software from um an

77
00:03:10,720 --> 00:03:16,319
automated resume screening provider like

78
00:03:13,800 --> 00:03:18,400
HireView. And so you are unbeknownst to

79
00:03:16,319 --> 00:03:20,319
you experiencing algorithmic

80
00:03:18,400 --> 00:03:22,080
standardization where decisions that

81
00:03:20,319 --> 00:03:25,000
were previously made by many different

82
00:03:22,080 --> 00:03:27,440
people are now made by one screening

83
00:03:25,000 --> 00:03:29,519
company. And so we can say that whatever

84
00:03:27,440 --> 00:03:30,879
else they do, automated decision-making

85
00:03:29,519 --> 00:03:32,799
systems have the potential to

86
00:03:30,879 --> 00:03:34,400
standardize by enforcing the same

87
00:03:32,799 --> 00:03:36,319
classification on the same token

88
00:03:34,400 --> 00:03:37,879
applicant file every time it's

89
00:03:36,319 --> 00:03:41,080
encountered at

90
00:03:37,879 --> 00:03:45,120
scale. And then we might ask, is this

91
00:03:41,080 --> 00:03:47,599
standardization a problem? um in a

92
00:03:45,120 --> 00:03:50,080
context where the largest algorithmic

93
00:03:47,599 --> 00:03:53,040
screening providers uh have clients that

94
00:03:50,080 --> 00:03:56,159
include 60% of the Fortune 100 and eight

95
00:03:53,040 --> 00:03:58,319
of the 10 largest federal agencies. Is

96
00:03:56,159 --> 00:04:00,280
it a problem if the same applicants

97
00:03:58,319 --> 00:04:03,120
re-encounter the same systems over and

98
00:04:00,280 --> 00:04:06,159
over? And the reason I think this is an

99
00:04:03,120 --> 00:04:08,400
interesting uh AI and society problem,

100
00:04:06,159 --> 00:04:10,239
an interesting ethics problem, is that

101
00:04:08,400 --> 00:04:12,680
standardization at scale is one of the

102
00:04:10,239 --> 00:04:14,560
features of automated but not human

103
00:04:12,680 --> 00:04:16,799
decision-making. Uh if you've ever

104
00:04:14,560 --> 00:04:19,199
worked with a bureaucracy, you know that

105
00:04:16,799 --> 00:04:21,519
it's actually very hard to get humans to

106
00:04:19,199 --> 00:04:25,280
make the same decisions even on the same

107
00:04:21,519 --> 00:04:27,520
cases. Um if you show people whose whole

108
00:04:25,280 --> 00:04:30,960
job it is to make decisions like

109
00:04:27,520 --> 00:04:33,120
doctors, lawyers, judges, um the same

110
00:04:30,960 --> 00:04:36,240
file three months later with names and

111
00:04:33,120 --> 00:04:38,400
other details re removed, they'll make a

112
00:04:36,240 --> 00:04:40,880
different decision about 25 to 30% of

113
00:04:38,400 --> 00:04:42,960
the time. So even the same individual

114
00:04:40,880 --> 00:04:45,280
will sometimes make a different decision

115
00:04:42,960 --> 00:04:46,960
on the same case over time. And that's

116
00:04:45,280 --> 00:04:51,040
not even counting the diversity of

117
00:04:46,960 --> 00:04:52,800
between decisionmaker uh choices.

118
00:04:51,040 --> 00:04:53,840
So this is a case where algorithmic

119
00:04:52,800 --> 00:04:55,759
decision-making and human

120
00:04:53,840 --> 00:04:58,320
decision-making regimes might have

121
00:04:55,759 --> 00:05:00,639
importantly different features uh rather

122
00:04:58,320 --> 00:05:02,800
than cases that we often talk about uh

123
00:05:00,639 --> 00:05:05,280
which are also important in which it's

124
00:05:02,800 --> 00:05:07,120
just that algorithmic decision-m perhaps

125
00:05:05,280 --> 00:05:09,199
exacerbates patterns that are already

126
00:05:07,120 --> 00:05:10,960
present in the other decision-making

127
00:05:09,199 --> 00:05:12,880
regime.

128
00:05:10,960 --> 00:05:14,880
So in the talk today I want to go

129
00:05:12,880 --> 00:05:17,199
through what is it that standardizes

130
00:05:14,880 --> 00:05:19,840
algorithmic decisions when should we

131
00:05:17,199 --> 00:05:22,320
consider standardized decisions to be of

132
00:05:19,840 --> 00:05:25,240
moral concern and when they're of moral

133
00:05:22,320 --> 00:05:28,320
concern what is it that we should

134
00:05:25,240 --> 00:05:30,960
do so in this room I don't need to go

135
00:05:28,320 --> 00:05:32,960
through uh one source of standardization

136
00:05:30,960 --> 00:05:35,360
which is of course just the basic nature

137
00:05:32,960 --> 00:05:38,160
of a deterministic algorithm put in the

138
00:05:35,360 --> 00:05:41,600
same inputs you get the same outputs but

139
00:05:38,160 --> 00:05:43,680
I want to talk about a more uh subtle

140
00:05:41,600 --> 00:05:45,320
source of algorithmic standardization

141
00:05:43,680 --> 00:05:47,440
which is algorithmic

142
00:05:45,320 --> 00:05:49,520
monoculture. So monoculture is a

143
00:05:47,440 --> 00:05:52,639
metaphor drawn from agricultural

144
00:05:49,520 --> 00:05:55,120
science. Uh we have um the case of

145
00:05:52,639 --> 00:05:57,840
industrial farming where previously you

146
00:05:55,120 --> 00:05:59,680
might have had uh gardens that were

147
00:05:57,840 --> 00:06:01,199
intercropped with many different plants.

148
00:05:59,680 --> 00:06:04,319
Each of those plants drew different

149
00:06:01,199 --> 00:06:07,039
things from the soil uh used different

150
00:06:04,319 --> 00:06:09,600
nutrients, put different um fixed

151
00:06:07,039 --> 00:06:11,160
nutrients back in the soil, etc. Then

152
00:06:09,600 --> 00:06:13,199
when we move to industrialized

153
00:06:11,160 --> 00:06:15,440
agriculture, it becomes more efficient

154
00:06:13,199 --> 00:06:18,160
to take a tractor and run it down the

155
00:06:15,440 --> 00:06:20,240
same line of plants and harvest the same

156
00:06:18,160 --> 00:06:22,800
plants at the same time. But we know

157
00:06:20,240 --> 00:06:24,880
that if we plant the same field with the

158
00:06:22,800 --> 00:06:27,840
same crops over and over, eventually we

159
00:06:24,880 --> 00:06:29,759
get uh nutrient collapse.

160
00:06:27,840 --> 00:06:32,720
So metaphorically we might think of this

161
00:06:29,759 --> 00:06:34,880
as an algorithmic monoculture as uh

162
00:06:32,720 --> 00:06:37,199
Manish and co-author John Kleinberg have

163
00:06:34,880 --> 00:06:39,440
said uh algorithmic monoculture is a

164
00:06:37,199 --> 00:06:41,600
state in which many decision makers all

165
00:06:39,440 --> 00:06:44,319
rely on the same algorithm and in this

166
00:06:41,600 --> 00:06:46,720
lovely uh 2021 paper they point out that

167
00:06:44,319 --> 00:06:48,880
this can have medium-term collapse

168
00:06:46,720 --> 00:06:50,880
effects where even though initially it

169
00:06:48,880 --> 00:06:53,199
might be better for the decision m

170
00:06:50,880 --> 00:06:55,600
decision makers themselves it might end

171
00:06:53,199 --> 00:06:58,479
up in the medium-term uh causing them

172
00:06:55,600 --> 00:07:01,360
problems by their own

173
00:06:58,479 --> 00:07:04,240
So what are some other sources of

174
00:07:01,360 --> 00:07:05,840
algorithmic monoculture? So the first of

175
00:07:04,240 --> 00:07:09,919
course is we literally use the same

176
00:07:05,840 --> 00:07:11,039
model uh in the one domain. Um and we're

177
00:07:09,919 --> 00:07:13,039
going to call this mechanical

178
00:07:11,039 --> 00:07:15,039
algorithmic monoculture. So necessarily

179
00:07:13,039 --> 00:07:16,639
you have to get the same outcome. And

180
00:07:15,039 --> 00:07:20,080
this is not as unusual as you might

181
00:07:16,639 --> 00:07:22,720
think given rollups and mergers by uh

182
00:07:20,080 --> 00:07:27,160
various private equity firms that are

183
00:07:22,720 --> 00:07:29,840
trying to create one actor in an entire

184
00:07:27,160 --> 00:07:32,560
domain. But since this isn't a political

185
00:07:29,840 --> 00:07:35,039
economy talk, we will move on uh to

186
00:07:32,560 --> 00:07:38,319
think instead about a relaxed definition

187
00:07:35,039 --> 00:07:40,160
of algorithmic monoculture. So if uh

188
00:07:38,319 --> 00:07:42,479
Kleinberg and Raavon's definition is

189
00:07:40,160 --> 00:07:45,160
this pure algorithmic monoculture where

190
00:07:42,479 --> 00:07:47,840
all decision makers rely on the same

191
00:07:45,160 --> 00:07:49,280
algorithm, it might not not that be that

192
00:07:47,840 --> 00:07:51,840
realistic in all of our decision

193
00:07:49,280 --> 00:07:53,280
settings. So when higher view builds an

194
00:07:51,840 --> 00:07:55,199
applicant screening algorithms for

195
00:07:53,280 --> 00:07:57,360
grocery stores, it's not actually going

196
00:07:55,199 --> 00:08:00,160
to give the same algorithm to all the

197
00:07:57,360 --> 00:08:02,319
grocery stores. Sufficiently big actors

198
00:08:00,160 --> 00:08:05,120
desire customization. It has to be

199
00:08:02,319 --> 00:08:06,960
tailored to them. So instead of this

200
00:08:05,120 --> 00:08:09,599
case, we're going to have a relaxed

201
00:08:06,960 --> 00:08:11,840
monoculture where, you know, these

202
00:08:09,599 --> 00:08:13,120
houses sure look very similar, but if

203
00:08:11,840 --> 00:08:14,879
you look closely, they have different

204
00:08:13,120 --> 00:08:16,560
doors, they have different windows, they

205
00:08:14,879 --> 00:08:20,000
have different awnings, they're not

206
00:08:16,560 --> 00:08:21,440
exactly the same. And in a similar way,

207
00:08:20,000 --> 00:08:23,919
we're going to define algorithmic

208
00:08:21,440 --> 00:08:25,840
monoculture as the state in which many

209
00:08:23,919 --> 00:08:27,919
decision makers rely on similar

210
00:08:25,840 --> 00:08:29,280
algorithms in a way that I'll specify in

211
00:08:27,919 --> 00:08:31,440
a moment.

212
00:08:29,280 --> 00:08:33,760
So here we might be thinking of the same

213
00:08:31,440 --> 00:08:35,120
provider is still building algorithms

214
00:08:33,760 --> 00:08:39,120
but they're slightly different for each

215
00:08:35,120 --> 00:08:40,800
of the grocery stores or even uh there's

216
00:08:39,120 --> 00:08:44,399
a different provider who's providing

217
00:08:40,800 --> 00:08:47,519
algorithms to a third grocery store. So

218
00:08:44,399 --> 00:08:51,160
in that case why might we think that

219
00:08:47,519 --> 00:08:53,600
decision-m outcomes would still be

220
00:08:51,160 --> 00:08:55,920
correlated? So one reason is that

221
00:08:53,600 --> 00:09:00,560
machine learning is built on a wonderful

222
00:08:55,920 --> 00:09:03,200
tradition of component sharing. So uh we

223
00:09:00,560 --> 00:09:04,959
work really hard to get good data sets

224
00:09:03,200 --> 00:09:07,680
and then it would be rational for many

225
00:09:04,959 --> 00:09:10,240
people in the same decision-m ecosystem

226
00:09:07,680 --> 00:09:12,800
who want to do the same task to rely on

227
00:09:10,240 --> 00:09:16,240
the same data set. So for a long time

228
00:09:12,800 --> 00:09:18,720
imageet was the largest labeled uh data

229
00:09:16,240 --> 00:09:20,800
set of images in town. So it made sense

230
00:09:18,720 --> 00:09:22,120
that a lot of computer vision papers

231
00:09:20,800 --> 00:09:25,600
would all use

232
00:09:22,120 --> 00:09:28,320
imageet. Likewise, um for a certain time

233
00:09:25,600 --> 00:09:31,680
period when BERT was the best model, uh

234
00:09:28,320 --> 00:09:33,519
for certain language tasks, a lot of uh

235
00:09:31,680 --> 00:09:37,279
papers at the same conference would all

236
00:09:33,519 --> 00:09:39,839
rely on BERT. Um we might also have

237
00:09:37,279 --> 00:09:44,000
libraries that are best in class at a

238
00:09:39,839 --> 00:09:46,560
moment at a time and evaluations. Um

239
00:09:44,000 --> 00:09:48,160
academics are competitive people. At

240
00:09:46,560 --> 00:09:50,480
least I hope that's not too

241
00:09:48,160 --> 00:09:52,640
controversial to say. So if someone puts

242
00:09:50,480 --> 00:09:54,240
out a big shiny benchmark, a lot of

243
00:09:52,640 --> 00:09:56,959
people are going to try to meet that

244
00:09:54,240 --> 00:09:58,959
benchmark if it gains social prestige.

245
00:09:56,959 --> 00:10:01,200
And so attempts to meet the same

246
00:09:58,959 --> 00:10:03,360
benchmark might also homogenize the

247
00:10:01,200 --> 00:10:04,279
outputs because we're aiming towards the

248
00:10:03,360 --> 00:10:07,760
same

249
00:10:04,279 --> 00:10:10,560
goal. So we have these standardizing uh

250
00:10:07,760 --> 00:10:13,440
forces that are built on the tradition

251
00:10:10,560 --> 00:10:15,200
of sharing in machine learning. And

252
00:10:13,440 --> 00:10:17,920
anecdotally, we've seen that previous

253
00:10:15,200 --> 00:10:20,640
data curation efforts like imageet have

254
00:10:17,920 --> 00:10:22,880
standardized training corpora. And in

255
00:10:20,640 --> 00:10:26,000
doing so, standardized errors such as

256
00:10:22,880 --> 00:10:27,920
reliance on the same spurious cues or

257
00:10:26,000 --> 00:10:31,440
shortcuts like background textures to

258
00:10:27,920 --> 00:10:34,480
predict foreground objects. So here uh

259
00:10:31,440 --> 00:10:37,519
we have an image of a fox squirrel on

260
00:10:34,480 --> 00:10:40,240
the left. Um but imagenet models are

261
00:10:37,519 --> 00:10:42,640
very likely to say that it's a sea lion.

262
00:10:40,240 --> 00:10:45,120
Uh on the right we have a focal image of

263
00:10:42,640 --> 00:10:47,120
a dragonfly, but imagenet models are

264
00:10:45,120 --> 00:10:50,160
very likely to say it's a manhole cover.

265
00:10:47,120 --> 00:10:50,920
So why is that? Take a minute, look at

266
00:10:50,160 --> 00:10:54,640
the

267
00:10:50,920 --> 00:10:56,000
pictures. Okay, great. So we think it's

268
00:10:54,640 --> 00:10:57,839
because of these strong background

269
00:10:56,000 --> 00:11:00,480
textures. The background texture of the

270
00:10:57,839 --> 00:11:02,640
wet rock is similar to a sea lion. The

271
00:11:00,480 --> 00:11:05,680
cross-hatching of I assume the lawn

272
00:11:02,640 --> 00:11:08,640
chair is similar to a manhole cover. And

273
00:11:05,680 --> 00:11:10,240
so what's important here is not that uh

274
00:11:08,640 --> 00:11:12,240
models make mistakes. of course they

275
00:11:10,240 --> 00:11:14,880
make mistakes. It's that training on the

276
00:11:12,240 --> 00:11:16,760
data set means they mean make the same

277
00:11:14,880 --> 00:11:19,240
particular

278
00:11:16,760 --> 00:11:22,079
mistake. Um there can also be

279
00:11:19,240 --> 00:11:24,720
standardization due to use of shared

280
00:11:22,079 --> 00:11:27,600
data in regimes where there's only one

281
00:11:24,720 --> 00:11:31,040
data set. So common crawl is an attempt

282
00:11:27,600 --> 00:11:33,120
to uh compile and clean the data from

283
00:11:31,040 --> 00:11:35,360
the entire internet. But unfortunately

284
00:11:33,120 --> 00:11:37,120
there's only one entire internet. So

285
00:11:35,360 --> 00:11:40,079
even if we had a competitor to common

286
00:11:37,120 --> 00:11:42,839
crawl, it would be attempting to get all

287
00:11:40,079 --> 00:11:46,320
the same data from the same

288
00:11:42,839 --> 00:11:48,800
source. Um likewise, we might have uh

289
00:11:46,320 --> 00:11:51,440
very large models that are trained with

290
00:11:48,800 --> 00:11:54,720
significant computational resources

291
00:11:51,440 --> 00:11:56,640
often across multiod modalities. And I

292
00:11:54,720 --> 00:11:59,120
don't know about you, but if I built a

293
00:11:56,640 --> 00:12:00,640
$10 million model, which I have not, I

294
00:11:59,120 --> 00:12:02,800
wouldn't want to just immediately throw

295
00:12:00,640 --> 00:12:04,240
it in the trash after I used it once. I

296
00:12:02,800 --> 00:12:06,279
would probably try to use it for as many

297
00:12:04,240 --> 00:12:08,720
different things as I possibly

298
00:12:06,279 --> 00:12:10,560
could. So the concern here is that

299
00:12:08,720 --> 00:12:12,880
adapting the same model for many

300
00:12:10,560 --> 00:12:16,240
different tasks could lead to this sing

301
00:12:12,880 --> 00:12:18,480
a single point of failure. And in some

302
00:12:16,240 --> 00:12:21,920
co-authored work uh a paper called

303
00:12:18,480 --> 00:12:25,360
ecosystems graphs we've tried to um show

304
00:12:21,920 --> 00:12:27,600
this by looking at dependencies in the

305
00:12:25,360 --> 00:12:31,279
foundation model ecosystem and showing

306
00:12:27,600 --> 00:12:33,120
that there are significant hubs where a

307
00:12:31,279 --> 00:12:34,800
whole bunch of different models from

308
00:12:33,120 --> 00:12:37,920
different companies will train on the

309
00:12:34,800 --> 00:12:40,880
same data set like the pile um or

310
00:12:37,920 --> 00:12:42,320
different APIs across many different

311
00:12:40,880 --> 00:12:44,920
companies will rely on the same

312
00:12:42,320 --> 00:12:49,360
underlying base

313
00:12:44,920 --> 00:12:52,639
model. All right. Wonderful. So, um I've

314
00:12:49,360 --> 00:12:54,360
suggested that there are many reasons

315
00:12:52,639 --> 00:12:56,560
why algorithmic decisions might be

316
00:12:54,360 --> 00:12:59,600
standardized. But why do we care about

317
00:12:56,560 --> 00:13:01,279
this in this context? It's because we

318
00:12:59,600 --> 00:13:03,959
think that algorithmic monoculture has

319
00:13:01,279 --> 00:13:06,639
the potential to homogenize

320
00:13:03,959 --> 00:13:09,839
outcomes. So what does this look like?

321
00:13:06,639 --> 00:13:11,760
Um so imagine people are interacting

322
00:13:09,839 --> 00:13:13,760
with some kind of deployed machine

323
00:13:11,760 --> 00:13:15,839
learning system. So there are multiple

324
00:13:13,760 --> 00:13:18,320
people and they're interacting with

325
00:13:15,839 --> 00:13:20,240
multiple decision makers. This is

326
00:13:18,320 --> 00:13:22,720
different from the context that we often

327
00:13:20,240 --> 00:13:25,040
look at in let's say algorithmic

328
00:13:22,720 --> 00:13:28,079
fairness where we're imagining a regime

329
00:13:25,040 --> 00:13:30,639
of one decision maker and a population

330
00:13:28,079 --> 00:13:32,160
that has a bunch of subpopuls. Now we're

331
00:13:30,639 --> 00:13:35,560
thinking about what's the cumulative

332
00:13:32,160 --> 00:13:37,760
effect of multiple decision makers on a

333
00:13:35,560 --> 00:13:39,360
population. So we're switching the frame

334
00:13:37,760 --> 00:13:41,440
a little bit.

335
00:13:39,360 --> 00:13:43,839
So now we want to say what is the

336
00:13:41,440 --> 00:13:45,920
cumulative pattern of the outcomes that

337
00:13:43,839 --> 00:13:47,600
they receive. And we can either think

338
00:13:45,920 --> 00:13:50,079
about these just in terms of their

339
00:13:47,600 --> 00:13:53,360
positive or negative aspects. Uh did

340
00:13:50,079 --> 00:13:54,720
they get you'll say the good or not? Or

341
00:13:53,360 --> 00:13:56,480
we can think about these in terms of

342
00:13:54,720 --> 00:13:59,360
their errors. Was the correct judgment

343
00:13:56,480 --> 00:14:02,040
made on them? And is there a pattern of

344
00:13:59,360 --> 00:14:06,120
systemic failure or systemic

345
00:14:02,040 --> 00:14:08,800
error? So we're going to call uh outcome

346
00:14:06,120 --> 00:14:11,279
homogenization when an individual or a

347
00:14:08,800 --> 00:14:12,800
subgroup receives the same outcomes from

348
00:14:11,279 --> 00:14:15,440
decision makers and those could be

349
00:14:12,800 --> 00:14:17,440
positive or negative. Then we're going

350
00:14:15,440 --> 00:14:19,440
to say an individual experiences

351
00:14:17,440 --> 00:14:22,120
systemic failure if every

352
00:14:19,440 --> 00:14:24,800
decision-making system fails for that

353
00:14:22,120 --> 00:14:26,800
person. And in classification settings,

354
00:14:24,800 --> 00:14:29,360
that's going to mean they experience a

355
00:14:26,800 --> 00:14:33,000
systemic failure or error if every

356
00:14:29,360 --> 00:14:36,000
decision-making system mclassifies that

357
00:14:33,000 --> 00:14:38,320
person. So how do we measure this? We

358
00:14:36,000 --> 00:14:40,959
can't just count, right? Because if we

359
00:14:38,320 --> 00:14:43,279
count, we would overindex on the fact

360
00:14:40,959 --> 00:14:45,519
that the systems do make some errors and

361
00:14:43,279 --> 00:14:46,959
those errors would naturally collide

362
00:14:45,519 --> 00:14:49,120
even if the systems were completely

363
00:14:46,959 --> 00:14:51,120
independent of each other. So we just

364
00:14:49,120 --> 00:14:53,680
minimally need to normalize for that

365
00:14:51,120 --> 00:14:56,399
error rate. So we need to say compared

366
00:14:53,680 --> 00:14:58,320
to what we would have expected if these

367
00:14:56,399 --> 00:15:00,959
systems had been totally independent,

368
00:14:58,320 --> 00:15:02,519
how much more homogeneous are the

369
00:15:00,959 --> 00:15:05,600
outcomes than

370
00:15:02,519 --> 00:15:08,480
that? So you can imagine if each of the

371
00:15:05,600 --> 00:15:10,560
systems is very bad, it makes 40%

372
00:15:08,480 --> 00:15:12,639
errors, there's going to be a high sort

373
00:15:10,560 --> 00:15:14,160
of baseline error rate, but we still

374
00:15:12,639 --> 00:15:16,560
might want to know are they more

375
00:15:14,160 --> 00:15:18,959
homogeneous than that?

376
00:15:16,560 --> 00:15:20,880
Now this doesn't mean by calling it a

377
00:15:18,959 --> 00:15:22,560
baseline this doesn't mean that we're

378
00:15:20,880 --> 00:15:26,320
committed to saying that independence is

379
00:15:22,560 --> 00:15:28,880
actually the right uh the right answer.

380
00:15:26,320 --> 00:15:30,320
In fact we suspect in a lot of cases the

381
00:15:28,880 --> 00:15:32,560
sort of right degree of correlation

382
00:15:30,320 --> 00:15:34,639
would be somewhere in the middle. But in

383
00:15:32,560 --> 00:15:36,560
a lot of regimes we may not know what is

384
00:15:34,639 --> 00:15:38,160
the correct degree of correlation. So

385
00:15:36,560 --> 00:15:40,480
we're just you viewing this as a

386
00:15:38,160 --> 00:15:42,639
baseline where we're saying empirically

387
00:15:40,480 --> 00:15:43,880
how much more correlated are they than

388
00:15:42,639 --> 00:15:49,360
this

389
00:15:43,880 --> 00:15:53,920
baseline. Okay. So um in a uh study of

390
00:15:49,360 --> 00:15:55,920
this we looked at the happy data set and

391
00:15:53,920 --> 00:15:59,199
uh we looked at predictions from

392
00:15:55,920 --> 00:16:02,399
deployed APIs in three modalities image

393
00:15:59,199 --> 00:16:04,480
text and speech. Uh we had 11

394
00:16:02,399 --> 00:16:07,199
classification data sets roughly four

395
00:16:04,480 --> 00:16:09,120
per modality and three APIs per

396
00:16:07,199 --> 00:16:11,759
modality. So three predictions for every

397
00:16:09,120 --> 00:16:14,279
data point and luckily we also had this

398
00:16:11,759 --> 00:16:17,839
over time from 2020 to

399
00:16:14,279 --> 00:16:21,040
2022. So what we wanted to find out is

400
00:16:17,839 --> 00:16:23,839
how often were model errors correlated

401
00:16:21,040 --> 00:16:26,880
and it turned out much more than if the

402
00:16:23,839 --> 00:16:29,759
model errors uh were independent which

403
00:16:26,880 --> 00:16:31,839
is what we would sort of expect. So the

404
00:16:29,759 --> 00:16:34,240
observed is the dark blue. Uh you can

405
00:16:31,839 --> 00:16:36,480
see on the zero so all models get it

406
00:16:34,240 --> 00:16:38,399
wrong. There are a lot more all models

407
00:16:36,480 --> 00:16:40,959
get it wrong than we would have expected

408
00:16:38,399 --> 00:16:42,880
based on independence. Now sometimes

409
00:16:40,959 --> 00:16:45,279
when I present this to computer

410
00:16:42,880 --> 00:16:47,600
scientists they say aha that's right.

411
00:16:45,279 --> 00:16:49,600
That's because those examples are hard.

412
00:16:47,600 --> 00:16:51,279
And I say oh great okay what does it

413
00:16:49,600 --> 00:16:54,480
mean to be hard? And they say well

414
00:16:51,279 --> 00:16:57,360
hardness means that all the models get

415
00:16:54,480 --> 00:16:59,360
it wrong. And I say wait a second that's

416
00:16:57,360 --> 00:17:01,600
what I'm showing you. But is there a

417
00:16:59,360 --> 00:17:03,040
model independent property of hardness?

418
00:17:01,600 --> 00:17:05,600
Because if you and I looked at these

419
00:17:03,040 --> 00:17:07,760
samples, they wouldn't be hard for us.

420
00:17:05,600 --> 00:17:09,600
So I think what we're agreeing on is

421
00:17:07,760 --> 00:17:12,559
that there is a model dependent property

422
00:17:09,600 --> 00:17:14,959
of hardness where these samples are hard

423
00:17:12,559 --> 00:17:16,799
for this class of models, but they might

424
00:17:14,959 --> 00:17:19,360
not be hard for us and they might not be

425
00:17:16,799 --> 00:17:21,839
hard for some as yet uncreated or

426
00:17:19,360 --> 00:17:23,799
perhaps not measured uh type of model

427
00:17:21,839 --> 00:17:27,199
that would be classifying

428
00:17:23,799 --> 00:17:28,799
this. Um and what we notice could I ask

429
00:17:27,199 --> 00:17:30,559
one question? Yes.

430
00:17:28,799 --> 00:17:32,000
um were these cases when they're all

431
00:17:30,559 --> 00:17:34,480
wrong, are they all confident they're

432
00:17:32,000 --> 00:17:37,039
wrong? They we don't have a confidence

433
00:17:34,480 --> 00:17:40,000
measure, but that's a great question.

434
00:17:37,039 --> 00:17:41,280
Yeah, thank you. Um I'll get Yeah, there

435
00:17:40,000 --> 00:17:42,919
is a case later where we do have

436
00:17:41,280 --> 00:17:46,320
confidence. So, thank

437
00:17:42,919 --> 00:17:48,320
you. Um we also notice over time that

438
00:17:46,320 --> 00:17:51,600
model improvements don't substantially

439
00:17:48,320 --> 00:17:54,720
reduce these failures. So here again we

440
00:17:51,600 --> 00:17:57,760
have um out of all the samples that

441
00:17:54,720 --> 00:18:00,000
originally uh the models all got wrong

442
00:17:57,760 --> 00:18:03,520
over the next two years how many of them

443
00:18:00,000 --> 00:18:04,960
are improved and compared to the

444
00:18:03,520 --> 00:18:08,080
potential improvements which is the

445
00:18:04,960 --> 00:18:09,919
light blue many fewer of the models are

446
00:18:08,080 --> 00:18:12,640
improved than you would have expected.

447
00:18:09,919 --> 00:18:13,880
So the systemic failures tend to persist

448
00:18:12,640 --> 00:18:17,039
over

449
00:18:13,880 --> 00:18:19,039
time. Um, so in ongoing work that I

450
00:18:17,039 --> 00:18:21,200
unfortunately can't share yet, we are

451
00:18:19,039 --> 00:18:23,919
trying to audit a real algorithmic

452
00:18:21,200 --> 00:18:26,200
hiring vendor and try to figure out uh

453
00:18:23,919 --> 00:18:28,320
how much correlation is in their

454
00:18:26,200 --> 00:18:32,960
predictions. But what I want to get to

455
00:18:28,320 --> 00:18:35,200
now is the moral question. So uh if

456
00:18:32,960 --> 00:18:37,840
standardizing architectures and data

457
00:18:35,200 --> 00:18:40,000
sets improves decision quality, if

458
00:18:37,840 --> 00:18:41,760
there's a reason that we have this

459
00:18:40,000 --> 00:18:44,760
tradition of component sharing and

460
00:18:41,760 --> 00:18:46,960
machine learning, um if it improves

461
00:18:44,760 --> 00:18:49,280
accuracy, when should we nevertheless

462
00:18:46,960 --> 00:18:52,559
think that outcome homogenization is of

463
00:18:49,280 --> 00:18:54,320
moral concern? And I think this is um

464
00:18:52,559 --> 00:18:55,840
this is a question that I found

465
00:18:54,320 --> 00:18:57,280
interesting for some time because I

466
00:18:55,840 --> 00:18:59,520
think the answer is not completely

467
00:18:57,280 --> 00:19:01,760
obvious.

468
00:18:59,520 --> 00:19:04,640
So this is a challenge. So just to spell

469
00:19:01,760 --> 00:19:07,200
it out a little bit more, uh we often

470
00:19:04,640 --> 00:19:08,960
think epistemically, so in terms in our

471
00:19:07,200 --> 00:19:11,280
in our regime of knowledge and what we

472
00:19:08,960 --> 00:19:13,200
should know and what we should believe,

473
00:19:11,280 --> 00:19:15,760
uh we often think that each decision

474
00:19:13,200 --> 00:19:17,600
maker has a responsibility to act on

475
00:19:15,760 --> 00:19:20,640
their best available evidence, all other

476
00:19:17,600 --> 00:19:23,280
things being equal. But what if they

477
00:19:20,640 --> 00:19:24,640
also have reason to believe as I think a

478
00:19:23,280 --> 00:19:26,960
lot of decision makers should have

479
00:19:24,640 --> 00:19:29,440
reason to believe in the machine

480
00:19:26,960 --> 00:19:32,400
learning ecosystem. Whatever data I

481
00:19:29,440 --> 00:19:34,640
have, others also have. Uh whatever

482
00:19:32,400 --> 00:19:37,520
learning strategies I consider best,

483
00:19:34,640 --> 00:19:39,679
others probably also consider best. Um

484
00:19:37,520 --> 00:19:41,840
others are likely to combine their data

485
00:19:39,679 --> 00:19:44,720
and learning strategies in similar ways

486
00:19:41,840 --> 00:19:46,720
and get similar predictive outcomes. And

487
00:19:44,720 --> 00:19:49,200
those same predictive outputs might

488
00:19:46,720 --> 00:19:51,840
include the same mistakes. And so I

489
00:19:49,200 --> 00:19:54,240
should expect that in by acting in

490
00:19:51,840 --> 00:19:56,799
accordance with the predictions produced

491
00:19:54,240 --> 00:19:59,799
by my best data and learning strategy, I

492
00:19:56,799 --> 00:20:02,080
might compound errors on the same

493
00:19:59,799 --> 00:20:04,400
individuals. And these compounded errors

494
00:20:02,080 --> 00:20:06,559
in a high stakes decision-making regime

495
00:20:04,400 --> 00:20:07,480
could systemically exclude individuals

496
00:20:06,559 --> 00:20:09,360
from

497
00:20:07,480 --> 00:20:11,440
opportunities. So that's the kind of

498
00:20:09,360 --> 00:20:14,320
challenge that I want to address in this

499
00:20:11,440 --> 00:20:16,320
next section of the talk.

500
00:20:14,320 --> 00:20:18,400
And when we think about why it's a moral

501
00:20:16,320 --> 00:20:22,360
concern in the algorithmic fairness

502
00:20:18,400 --> 00:20:25,200
space, we often reach first for bias and

503
00:20:22,360 --> 00:20:28,000
discrimination. So you could say maybe

504
00:20:25,200 --> 00:20:32,000
this matters because uh it's a special

505
00:20:28,000 --> 00:20:35,280
kind of group based bias or uh maybe it

506
00:20:32,000 --> 00:20:37,520
only matters when the individuals who

507
00:20:35,280 --> 00:20:41,039
are excluded primarily belong to the

508
00:20:37,520 --> 00:20:45,760
same socially salient groups.

509
00:20:41,039 --> 00:20:48,559
Um so a challenge to this has been uh in

510
00:20:45,760 --> 00:20:51,679
my past work and also in uh other

511
00:20:48,559 --> 00:20:54,400
people's past work. What happens if the

512
00:20:51,679 --> 00:20:56,760
features that end up being the ones that

513
00:20:54,400 --> 00:20:59,159
underly these correlations are not

514
00:20:56,760 --> 00:21:01,360
necessarily standard group based

515
00:20:59,159 --> 00:21:02,640
characteristics? Um so it could be that

516
00:21:01,360 --> 00:21:04,559
there's some kind of consistent

517
00:21:02,640 --> 00:21:07,080
decision-m based on arbitrary

518
00:21:04,559 --> 00:21:10,159
characteristics.

519
00:21:07,080 --> 00:21:12,720
um what the sort of joking examples that

520
00:21:10,159 --> 00:21:14,480
people use are wearing purple shoelaces

521
00:21:12,720 --> 00:21:16,720
or being born on a particular day of the

522
00:21:14,480 --> 00:21:18,960
month. But you could also imagine this I

523
00:21:16,720 --> 00:21:21,520
think more realistically as some

524
00:21:18,960 --> 00:21:24,159
difficult to interpret uh constellation

525
00:21:21,520 --> 00:21:26,159
of features that maybe is just hard to

526
00:21:24,159 --> 00:21:27,480
find and we don't know what those

527
00:21:26,159 --> 00:21:30,720
features

528
00:21:27,480 --> 00:21:33,200
are. So instead of thinking about for

529
00:21:30,720 --> 00:21:35,760
example biases based on a social

530
00:21:33,200 --> 00:21:40,080
characteristic like gender in a large

531
00:21:35,760 --> 00:21:42,320
model. So instead of cases like uh label

532
00:21:40,080 --> 00:21:44,320
prediction where you have two possible

533
00:21:42,320 --> 00:21:46,720
labels is this a portrait of an

534
00:21:44,320 --> 00:21:49,280
astronaut with the American flag or is

535
00:21:46,720 --> 00:21:51,120
this a photograph of a smiling housewife

536
00:21:49,280 --> 00:21:53,600
in an orange jumpsuit with the American

537
00:21:51,120 --> 00:21:56,559
flag? Uh you might look at these photos

538
00:21:53,600 --> 00:21:58,640
as a as a human being who has seen a

539
00:21:56,559 --> 00:22:01,520
NASA formal po portrait before and say,

540
00:21:58,640 --> 00:22:03,679
"Wow, uh she has the orange, you know,

541
00:22:01,520 --> 00:22:05,360
NASA suit. She has a giant spaceship.

542
00:22:03,679 --> 00:22:08,480
She has a giant space helmet. She has

543
00:22:05,360 --> 00:22:10,880
the official NA NASA badge. Seems like

544
00:22:08,480 --> 00:22:13,760
probably an astronaut, but a model might

545
00:22:10,880 --> 00:22:16,240
say no. It's more likely that she is a

546
00:22:13,760 --> 00:22:18,480
smiling housewife."

547
00:22:16,240 --> 00:22:22,760
Um, but we could also imagine name

548
00:22:18,480 --> 00:22:25,679
artifacts that are more individual or

549
00:22:22,760 --> 00:22:28,080
particular. So, because a lot of large

550
00:22:25,679 --> 00:22:30,799
models are trained on data from the

551
00:22:28,080 --> 00:22:33,120
internet that includes uh forums like

552
00:22:30,799 --> 00:22:35,280
Reddit, uh it turns out that one thing

553
00:22:33,120 --> 00:22:37,520
that people like to do on Reddit is talk

554
00:22:35,280 --> 00:22:39,280
about politicians they don't like. And

555
00:22:37,520 --> 00:22:43,200
it turns out they don't like most

556
00:22:39,280 --> 00:22:47,320
politicians. Uh, and so regardless of

557
00:22:43,200 --> 00:22:49,919
party, if your name is Donald, Bernie,

558
00:22:47,320 --> 00:22:54,640
Bill, Hillary,

559
00:22:49,919 --> 00:22:56,960
uh, Donald, Mitch, Chuck, Barack, any of

560
00:22:54,640 --> 00:22:59,440
these sounding familiar, or did you get

561
00:22:56,960 --> 00:23:02,640
that complete memory wipe that I've been

562
00:22:59,440 --> 00:23:04,320
trying to inquire about recently? Um,

563
00:23:02,640 --> 00:23:07,200
okay. So, all of these names have

564
00:23:04,320 --> 00:23:10,400
negative sentiment associated with them.

565
00:23:07,200 --> 00:23:12,640
And uh you can imagine a certain kind of

566
00:23:10,400 --> 00:23:14,559
training task where if you're doing

567
00:23:12,640 --> 00:23:16,960
sentiment- based analysis of someone who

568
00:23:14,559 --> 00:23:19,120
has one of these names, they get a very

569
00:23:16,960 --> 00:23:21,600
slight negative bump. And if they're a

570
00:23:19,120 --> 00:23:25,440
51% candidate, maybe that bumps them

571
00:23:21,600 --> 00:23:27,120
down to a 50% or a 49% candidate. So

572
00:23:25,440 --> 00:23:29,039
we're not talking about effect sizes

573
00:23:27,120 --> 00:23:32,240
probably large enough to move someone

574
00:23:29,039 --> 00:23:33,919
from a 99% candidate to a 1% candidate.

575
00:23:32,240 --> 00:23:36,720
We're talking about small threshold

576
00:23:33,919 --> 00:23:38,440
effect bumps based on sort of arbitrary

577
00:23:36,720 --> 00:23:40,799
features like

578
00:23:38,440 --> 00:23:43,600
names. And then we might ask this

579
00:23:40,799 --> 00:23:45,840
question if name artifacts have some

580
00:23:43,600 --> 00:23:48,159
negative consequences. For example, if

581
00:23:45,840 --> 00:23:50,320
they cause a candidate who is already a

582
00:23:48,159 --> 00:23:52,880
borderline candidate to consistently

583
00:23:50,320 --> 00:23:54,720
drop below a decision threshold, is it

584
00:23:52,880 --> 00:23:57,679
wrong to concentrate these kinds of

585
00:23:54,720 --> 00:24:00,720
errors on Donald's or Bernie's or Bills

586
00:23:57,679 --> 00:24:02,559
or Alys's or Kevins?

587
00:24:00,720 --> 00:24:06,559
Um, anyone who has those names, come

588
00:24:02,559 --> 00:24:08,480
talk to me afterwards. Uh, okay. So, in

589
00:24:06,559 --> 00:24:10,320
the little flowchart I laid out, we can

590
00:24:08,480 --> 00:24:14,559
imagine a couple different responses to

591
00:24:10,320 --> 00:24:17,000
this. Uh, one is we could say systemic

592
00:24:14,559 --> 00:24:19,120
exclusion shouldn't qualify as bias or

593
00:24:17,000 --> 00:24:21,200
discrimination unless it involves

594
00:24:19,120 --> 00:24:24,000
membership in some kind of socially

595
00:24:21,200 --> 00:24:25,600
salient group. So, we could say, hey,

596
00:24:24,000 --> 00:24:28,240
I'm just not interested in this

597
00:24:25,600 --> 00:24:30,240
Donaldbased problem. you know, call me

598
00:24:28,240 --> 00:24:32,880
when we're uh looking at one of the

599
00:24:30,240 --> 00:24:35,440
seven federally protected categories.

600
00:24:32,880 --> 00:24:37,880
Um, and I think it makes sense to say

601
00:24:35,440 --> 00:24:39,840
this is not a case of bias or

602
00:24:37,880 --> 00:24:42,240
discrimination. And the reason is that

603
00:24:39,840 --> 00:24:44,480
if we look at standard definitions of

604
00:24:42,240 --> 00:24:48,080
bias and discrimination,

605
00:24:44,480 --> 00:24:50,240
uh it seems like both the scope and the

606
00:24:48,080 --> 00:24:52,400
scale of the phenomenon we're

607
00:24:50,240 --> 00:24:54,320
particularly talking about is much more

608
00:24:52,400 --> 00:24:56,480
extensive. So, we're talking about

609
00:24:54,320 --> 00:24:58,640
perpetual subordination of specially

610
00:24:56,480 --> 00:25:01,200
disadvantaged groups whose political

611
00:24:58,640 --> 00:25:03,679
power is severely circumscribed. Doesn't

612
00:25:01,200 --> 00:25:06,159
seem like that's happening to alyses.

613
00:25:03,679 --> 00:25:08,640
Uh, discrimination consists of actions,

614
00:25:06,159 --> 00:25:10,640
practices, or policies that are based on

615
00:25:08,640 --> 00:25:12,640
perceived social group to which they

616
00:25:10,640 --> 00:25:16,080
belong. And the groups must be socially

617
00:25:12,640 --> 00:25:18,960
salient. So here the idea is whatever

618
00:25:16,080 --> 00:25:20,400
your group membership is whether or not

619
00:25:18,960 --> 00:25:23,279
we don't care whether or not it's sort

620
00:25:20,400 --> 00:25:24,880
of metaphysically underlying true of you

621
00:25:23,279 --> 00:25:26,799
if it's something that's perpetually

622
00:25:24,880 --> 00:25:28,640
attributed to you across a bunch of

623
00:25:26,799 --> 00:25:30,559
different spheres of your life. It's

624
00:25:28,640 --> 00:25:31,559
structuring your activities in those

625
00:25:30,559 --> 00:25:33,679
different

626
00:25:31,559 --> 00:25:36,320
spheres. But it doesn't seem like these

627
00:25:33,679 --> 00:25:38,159
artifacts are going to swap between

628
00:25:36,320 --> 00:25:40,400
spheres or at least we don't currently

629
00:25:38,159 --> 00:25:42,000
have any evidence that we should think

630
00:25:40,400 --> 00:25:44,159
they do.

631
00:25:42,000 --> 00:25:46,640
There's also this sense of historicity.

632
00:25:44,159 --> 00:25:49,200
So maybe an action is compounding a

633
00:25:46,640 --> 00:25:51,039
prior injustice. Uh so there has to be

634
00:25:49,200 --> 00:25:52,200
some kind of past injustice that it's

635
00:25:51,039 --> 00:25:55,440
building

636
00:25:52,200 --> 00:25:57,200
on. And this kind of anti-donald semin

637
00:25:55,440 --> 00:25:59,600
sentiment would only count as

638
00:25:57,200 --> 00:26:01,840
discriminatory bias if we had a very

639
00:25:59,600 --> 00:26:04,240
permissive definition of bias.

640
00:26:01,840 --> 00:26:06,640
Interestingly, some of the original uh

641
00:26:04,240 --> 00:26:09,520
people to talk about uh bias and

642
00:26:06,640 --> 00:26:11,919
discrimination in computer systems uh

643
00:26:09,520 --> 00:26:13,600
Bach Friedman and Helen Nissenbomb do

644
00:26:11,919 --> 00:26:17,279
have a very permissive definition of

645
00:26:13,600 --> 00:26:19,080
this kind. uh they are thinking about

646
00:26:17,279 --> 00:26:21,360
systematic and unfair

647
00:26:19,080 --> 00:26:24,320
discrimination and they say it happens

648
00:26:21,360 --> 00:26:27,039
when uh a system unfairly discriminates

649
00:26:24,320 --> 00:26:30,000
if it denies an opportunity or good or

650
00:26:27,039 --> 00:26:32,120
if it assigns any undesirable outcome to

651
00:26:30,000 --> 00:26:34,240
an individual or groups of

652
00:26:32,120 --> 00:26:36,880
individuals on grounds that are

653
00:26:34,240 --> 00:26:38,400
unreasonable or inappropriate. Okay, so

654
00:26:36,880 --> 00:26:40,400
that could be anything. It doesn't have

655
00:26:38,400 --> 00:26:43,240
to have this history. It doesn't have to

656
00:26:40,400 --> 00:26:46,320
be across context.

657
00:26:43,240 --> 00:26:48,880
Uh, so if you think that's plausible,

658
00:26:46,320 --> 00:26:50,880
then I think you would think that

659
00:26:48,880 --> 00:26:55,760
anti-donald sentiment is a

660
00:26:50,880 --> 00:26:57,919
discriminatory bias. Um there's also a

661
00:26:55,760 --> 00:26:59,679
recent paper that takes this stance

662
00:26:57,919 --> 00:27:02,320
talking specifically about novel

663
00:26:59,679 --> 00:27:05,360
algorithmic groups and building on uh

664
00:27:02,320 --> 00:27:08,880
some of our recent work suggesting that

665
00:27:05,360 --> 00:27:10,640
uh al algorithmic groups um should be

666
00:27:08,880 --> 00:27:13,159
thought of as being discriminated

667
00:27:10,640 --> 00:27:15,279
against whenever they're systematically

668
00:27:13,159 --> 00:27:17,200
disadvantaged. I don't find this super

669
00:27:15,279 --> 00:27:19,200
compelling personally because I think

670
00:27:17,200 --> 00:27:22,000
we're losing the characteristic harm of

671
00:27:19,200 --> 00:27:25,120
what discrimination is. I think it makes

672
00:27:22,000 --> 00:27:28,000
more sense to reserve discrimination for

673
00:27:25,120 --> 00:27:31,400
cases that have this you know cross

674
00:27:28,000 --> 00:27:35,120
modal cross context timeextended

675
00:27:31,400 --> 00:27:36,880
character where it is continuous with

676
00:27:35,120 --> 00:27:39,080
socially salient discrimination that

677
00:27:36,880 --> 00:27:41,600
structures interactions of human

678
00:27:39,080 --> 00:27:44,000
beings. Um, and one way to talk about

679
00:27:41,600 --> 00:27:46,400
this is to say, look, if we take one of

680
00:27:44,000 --> 00:27:49,120
these overly permissive definitions,

681
00:27:46,400 --> 00:27:51,279
maybe as Karns and Roth say, we're doing

682
00:27:49,120 --> 00:27:53,360
a kind of fairness gerrymandering. So,

683
00:27:51,279 --> 00:27:55,279
we're picking whoever happens to have

684
00:27:53,360 --> 00:27:58,159
gotten left out and we're saying, well,

685
00:27:55,279 --> 00:28:00,559
it was unfair to them. Uh, but really

686
00:27:58,159 --> 00:28:02,640
that's kind of post talk. It's not

687
00:28:00,559 --> 00:28:05,159
necessarily the case that we should

688
00:28:02,640 --> 00:28:08,559
think of fairness as being

689
00:28:05,159 --> 00:28:10,440
um about whoever just happens to be in

690
00:28:08,559 --> 00:28:14,000
this algorithmic

691
00:28:10,440 --> 00:28:16,559
valley. So we have a few different ways

692
00:28:14,000 --> 00:28:19,760
uh of thinking about why systemic

693
00:28:16,559 --> 00:28:22,000
exclusion is a moral concern. So I'm

694
00:28:19,760 --> 00:28:24,320
going to propose that it's not because

695
00:28:22,000 --> 00:28:26,720
it's a type of discriminatory bias.

696
00:28:24,320 --> 00:28:28,240
However, if you decide you want to think

697
00:28:26,720 --> 00:28:30,640
it is, I think I've given you some

698
00:28:28,240 --> 00:28:31,880
resources to structure how you might

699
00:28:30,640 --> 00:28:34,320
make that

700
00:28:31,880 --> 00:28:36,640
argument. Um, you could say, well, it

701
00:28:34,320 --> 00:28:39,440
matters only if bias and systemic

702
00:28:36,640 --> 00:28:42,240
exclusion stack in some way. So, maybe

703
00:28:39,440 --> 00:28:44,559
there's some kind of bias. Uh, we

704
00:28:42,240 --> 00:28:46,480
already think that's a moral concern in

705
00:28:44,559 --> 00:28:48,960
this setting. Uh, there's a

706
00:28:46,480 --> 00:28:50,559
discriminatory um treatment of two

707
00:28:48,960 --> 00:28:53,360
different groups. And then we might

708
00:28:50,559 --> 00:28:54,720
think, well, it's even worse if the

709
00:28:53,360 --> 00:28:57,760
people in the group that are

710
00:28:54,720 --> 00:29:00,320
experiencing the lesser inclusion rate

711
00:28:57,760 --> 00:29:02,159
also it's concentrated on individuals

712
00:29:00,320 --> 00:29:04,520
within that group and not spread evenly

713
00:29:02,159 --> 00:29:07,200
over that group. I think that's

714
00:29:04,520 --> 00:29:09,120
possible, but being a philosopher, I'm

715
00:29:07,200 --> 00:29:11,840
going to try to argue for what I think

716
00:29:09,120 --> 00:29:16,799
is the hardest and the sort of base case

717
00:29:11,840 --> 00:29:19,919
example and then hope that that uh will

718
00:29:16,799 --> 00:29:21,520
um flow upwards to these other cases

719
00:29:19,919 --> 00:29:24,000
because if I can show that it matters

720
00:29:21,520 --> 00:29:25,120
for any excluded individuals but not

721
00:29:24,000 --> 00:29:28,080
because they're subject to

722
00:29:25,120 --> 00:29:30,480
discriminatory bias, then necessarily it

723
00:29:28,080 --> 00:29:33,480
also matters for individuals who are in

724
00:29:30,480 --> 00:29:36,559
these more specific cases.

725
00:29:33,480 --> 00:29:38,640
Okay. So when is excl systemic exclusion

726
00:29:36,559 --> 00:29:40,440
of individuals because of algorithmic

727
00:29:38,640 --> 00:29:43,279
monoculture

728
00:29:40,440 --> 00:29:44,640
wrong. Now uh there are some

729
00:29:43,279 --> 00:29:46,640
philosophers in the room. There are some

730
00:29:44,640 --> 00:29:49,279
non- philosophers in the room. One

731
00:29:46,640 --> 00:29:51,279
interesting thing about uh attempts to

732
00:29:49,279 --> 00:29:53,840
show that something is wrong is that for

733
00:29:51,279 --> 00:29:57,360
most topics we would expect most moral

734
00:29:53,840 --> 00:30:00,240
theories to agree. So if you say uh

735
00:29:57,360 --> 00:30:02,799
should I kick an innocent puppy just for

736
00:30:00,240 --> 00:30:05,520
fun? uh if your moral theory doesn't end

737
00:30:02,799 --> 00:30:06,960
up saying that's probably wrong, we

738
00:30:05,520 --> 00:30:08,480
would we would have this process of

739
00:30:06,960 --> 00:30:09,679
reflective equilibrium where we would

740
00:30:08,480 --> 00:30:11,760
say well something's wrong with the

741
00:30:09,679 --> 00:30:13,600
moral theory. That's a problem with the

742
00:30:11,760 --> 00:30:16,399
theory that like the sentiment that that

743
00:30:13,600 --> 00:30:18,480
is wrong should be borne out by all

744
00:30:16,399 --> 00:30:20,880
moral theories.

745
00:30:18,480 --> 00:30:22,799
But systemic exclusion of individuals I

746
00:30:20,880 --> 00:30:26,840
think it's an interesting case where you

747
00:30:22,799 --> 00:30:29,600
can see quite a spread of opinions uh

748
00:30:26,840 --> 00:30:31,399
from that I'm extrapolating from

749
00:30:29,600 --> 00:30:34,159
different

750
00:30:31,399 --> 00:30:36,320
theories. So at first I thought okay

751
00:30:34,159 --> 00:30:38,159
maybe systemic exclusion is wrong

752
00:30:36,320 --> 00:30:40,240
because it's unjust. So here we're in

753
00:30:38,159 --> 00:30:41,960
the realm of political philosophy. We're

754
00:30:40,240 --> 00:30:45,840
thinking about what do we owe to each

755
00:30:41,960 --> 00:30:47,399
other? Um, and we might think about this

756
00:30:45,840 --> 00:30:50,200
uh first in terms of democratic

757
00:30:47,399 --> 00:30:53,360
equality. So maybe it's fine for

758
00:30:50,200 --> 00:30:55,840
individuals, companies, organizations to

759
00:30:53,360 --> 00:30:58,159
rank candidates for opportunities

760
00:30:55,840 --> 00:30:59,919
uh by their own metrics. And it's even

761
00:30:58,159 --> 00:31:00,760
fine if some people's rankings are kind

762
00:30:59,919 --> 00:31:02,720
of

763
00:31:00,760 --> 00:31:05,039
arbitrary. But it's when the same

764
00:31:02,720 --> 00:31:07,440
hierarchy you're ranking is used across

765
00:31:05,039 --> 00:31:09,039
the whole sector that it sets rules of

766
00:31:07,440 --> 00:31:12,399
interaction with the domain and

767
00:31:09,039 --> 00:31:14,640
monopolizes access to opportunities. And

768
00:31:12,399 --> 00:31:16,799
when consistent exclusion of people

769
00:31:14,640 --> 00:31:19,039
because of this leads to some kind of

770
00:31:16,799 --> 00:31:19,960
pernicious social hierarchy, that is

771
00:31:19,039 --> 00:31:22,640
when it's

772
00:31:19,960 --> 00:31:25,120
unjust. But notice we have to get to the

773
00:31:22,640 --> 00:31:27,559
status of this sad cat. We have to

774
00:31:25,120 --> 00:31:29,760
actually create a pernicious social

775
00:31:27,559 --> 00:31:31,760
hierarchy. Um so it has to be the case

776
00:31:29,760 --> 00:31:34,240
that the people who are algorithmically

777
00:31:31,760 --> 00:31:36,080
excluded are on the bottom of a social

778
00:31:34,240 --> 00:31:37,840
hierarchy.

779
00:31:36,080 --> 00:31:40,000
And that means this threshold wouldn't

780
00:31:37,840 --> 00:31:42,440
support action on most of the

781
00:31:40,000 --> 00:31:44,320
empirically demonstrated cases of

782
00:31:42,440 --> 00:31:45,679
homogenization. So this is one of the

783
00:31:44,320 --> 00:31:48,320
reasons that I think it's really

784
00:31:45,679 --> 00:31:51,679
important to try to do kind of a cycle

785
00:31:48,320 --> 00:31:54,000
of empirical uh study and then

786
00:31:51,679 --> 00:31:55,360
philosophical theorizing because I think

787
00:31:54,000 --> 00:31:57,600
this would be a much more compelling

788
00:31:55,360 --> 00:31:59,519
talk if I could stand up here and say

789
00:31:57,600 --> 00:32:00,799
there's an algorithmic underclass of

790
00:31:59,519 --> 00:32:03,840
people who will never get any

791
00:32:00,799 --> 00:32:06,159
opportunity across any domain of life.

792
00:32:03,840 --> 00:32:09,039
And so yes, Elizabeth Anderson is

793
00:32:06,159 --> 00:32:10,880
correct that it's unjust to them. But I

794
00:32:09,039 --> 00:32:12,880
don't think we see that, at least not

795
00:32:10,880 --> 00:32:14,640
now. And I, you know, I would need that

796
00:32:12,880 --> 00:32:15,720
to be empirically demonstrated before I

797
00:32:14,640 --> 00:32:18,720
said that to

798
00:32:15,720 --> 00:32:21,519
you. So what instead, what if instead we

799
00:32:18,720 --> 00:32:23,760
think about something like uh wallser?

800
00:32:21,519 --> 00:32:26,080
So we say, hey, there are different

801
00:32:23,760 --> 00:32:28,880
domains, spheres of life, different

802
00:32:26,080 --> 00:32:30,960
kinds of social good, and we think that

803
00:32:28,880 --> 00:32:32,720
these spheres should be separable. So

804
00:32:30,960 --> 00:32:35,840
one thing a theory of justice should do

805
00:32:32,720 --> 00:32:39,360
is that it should decorrelate uh social

806
00:32:35,840 --> 00:32:41,360
goods. So if you lack a certain social

807
00:32:39,360 --> 00:32:42,960
good like healthare it shouldn't mean

808
00:32:41,360 --> 00:32:46,159
that it's impossible for you to get a

809
00:32:42,960 --> 00:32:49,000
loan. And that's one of the roles of the

810
00:32:46,159 --> 00:32:51,679
welfare state is to decorrelate those

811
00:32:49,000 --> 00:32:53,600
things. Okay. So, interestingly, one of

812
00:32:51,679 --> 00:32:55,279
the examples Walzer uses is credit

813
00:32:53,600 --> 00:32:58,080
score, which I would think is kind of an

814
00:32:55,279 --> 00:33:00,240
abstract gateway to other social goods,

815
00:32:58,080 --> 00:33:02,399
not itself a social good, but he uses

816
00:33:00,240 --> 00:33:04,399
it. So, let's run with that a little bit

817
00:33:02,399 --> 00:33:06,640
and say, what if there's this property

818
00:33:04,399 --> 00:33:09,200
of being legible to and correctly judged

819
00:33:06,640 --> 00:33:12,000
by an algorithm? Shouldn't that be

820
00:33:09,200 --> 00:33:15,360
decorated from your ability to access

821
00:33:12,000 --> 00:33:18,480
employment, welfare, healthcare?

822
00:33:15,360 --> 00:33:20,960
Um again I think you have to reach a

823
00:33:18,480 --> 00:33:22,919
pretty high threshold where you were

824
00:33:20,960 --> 00:33:25,600
genuinely shut out of employment

825
00:33:22,919 --> 00:33:27,080
altogether. Uh not just barred from

826
00:33:25,600 --> 00:33:29,279
perhaps your preferred employment

827
00:33:27,080 --> 00:33:30,039
opportunities. So this is also a really

828
00:33:29,279 --> 00:33:33,159
high

829
00:33:30,039 --> 00:33:35,600
threshold. Um so we have a

830
00:33:33,159 --> 00:33:38,000
similar worry on a lot of these justice

831
00:33:35,600 --> 00:33:40,720
based theories that thresholds would not

832
00:33:38,000 --> 00:33:44,000
support action on most cases of systemic

833
00:33:40,720 --> 00:33:45,519
exclusion due to outcome homogenization.

834
00:33:44,000 --> 00:33:48,279
In order to make it work, we would have

835
00:33:45,519 --> 00:33:51,279
to move to like an industrial strength

836
00:33:48,279 --> 00:33:52,960
egalitarianism. So RWS, the a person

837
00:33:51,279 --> 00:33:55,200
that liberal political philosophers

838
00:33:52,960 --> 00:33:57,760
often turn to is not good enough because

839
00:33:55,200 --> 00:33:59,600
he says fair equality of opportunity

840
00:33:57,760 --> 00:34:02,080
shouldn't be contingent on social class

841
00:33:59,600 --> 00:34:04,880
or standing. Um well, it would have to

842
00:34:02,080 --> 00:34:06,880
be equality of opportunity shouldn't be

843
00:34:04,880 --> 00:34:10,960
subject to any morally arbitrary

844
00:34:06,880 --> 00:34:13,040
features. But this means you know uh in

845
00:34:10,960 --> 00:34:14,960
a lot of cases where we currently think

846
00:34:13,040 --> 00:34:16,960
that this it would be too cumbersome or

847
00:34:14,960 --> 00:34:19,919
burdensome for the state to intervene

848
00:34:16,960 --> 00:34:22,399
the state would have to intervene. So if

849
00:34:19,919 --> 00:34:24,240
someone at a local hardware store uh

850
00:34:22,399 --> 00:34:25,520
says I'm going to interview all the

851
00:34:24,240 --> 00:34:27,839
candidates and then I'm going to have a

852
00:34:25,520 --> 00:34:29,919
pool of qualified individuals and then

853
00:34:27,839 --> 00:34:31,359
out of those qualified individuals I'm

854
00:34:29,919 --> 00:34:33,119
just going to pick the one who prefers

855
00:34:31,359 --> 00:34:34,720
to talk about fishing because they're

856
00:34:33,119 --> 00:34:36,560
all qualified and I'm that's going to be

857
00:34:34,720 --> 00:34:38,399
my tiebreaker. that would not be

858
00:34:36,560 --> 00:34:39,879
acceptable because that's a morally

859
00:34:38,399 --> 00:34:43,919
arbitrary

860
00:34:39,879 --> 00:34:45,800
feature. Um, so I think not only would

861
00:34:43,919 --> 00:34:48,079
this sort of industrial strength

862
00:34:45,800 --> 00:34:50,720
egalitarianism require much broader

863
00:34:48,079 --> 00:34:52,320
sweeping social revisions, it would also

864
00:34:50,720 --> 00:34:54,240
doesn't seem to capture the important

865
00:34:52,320 --> 00:34:56,000
features of homogenization due to

866
00:34:54,240 --> 00:34:57,960
monoculture which is that there's

867
00:34:56,000 --> 00:35:01,680
something correlated important about the

868
00:34:57,960 --> 00:35:03,280
correlation across multiple actors.

869
00:35:01,680 --> 00:35:07,119
So instead, I think we should look to

870
00:35:03,280 --> 00:35:08,880
theories like uh contractualism. So here

871
00:35:07,119 --> 00:35:10,920
we're imagining we're all considering

872
00:35:08,880 --> 00:35:12,960
adopting some set of rules or decision

873
00:35:10,920 --> 00:35:16,079
principles. Which ones should we think

874
00:35:12,960 --> 00:35:18,480
are acceptable? Well, Scan brings out

875
00:35:16,079 --> 00:35:20,400
the big red hammer and he says, "A

876
00:35:18,480 --> 00:35:22,960
decision-making principle is acceptable

877
00:35:20,400 --> 00:35:26,960
if no one affected may reasonably reject

878
00:35:22,960 --> 00:35:28,800
it." What is reasonable rejection?

879
00:35:26,960 --> 00:35:32,680
Well, it's reasonable when the burden

880
00:35:28,800 --> 00:35:35,119
imposed on the rejector and is

881
00:35:32,680 --> 00:35:37,280
substantial, but alternative principles

882
00:35:35,119 --> 00:35:40,720
don't impose greater burdens on others.

883
00:35:37,280 --> 00:35:43,200
So imagine you're a nimi and you say, I

884
00:35:40,720 --> 00:35:44,960
don't want this cell tower in my

885
00:35:43,200 --> 00:35:46,960
backyard, move it to someone else's

886
00:35:44,960 --> 00:35:48,960
backyard, but it would impose an equally

887
00:35:46,960 --> 00:35:50,880
great burden on them. We can't just like

888
00:35:48,960 --> 00:35:52,320
shift it around the town. There has to

889
00:35:50,880 --> 00:35:54,320
be some reason why it's uniquely

890
00:35:52,320 --> 00:35:56,160
burdensome to you in order for it to be

891
00:35:54,320 --> 00:35:58,480
shifted.

892
00:35:56,160 --> 00:36:01,200
Um but interestingly the contractualist

893
00:35:58,480 --> 00:36:03,760
also has reason to reject a principle on

894
00:36:01,200 --> 00:36:05,520
which the same small group is

895
00:36:03,760 --> 00:36:07,920
consistently sacrifice for the greater

896
00:36:05,520 --> 00:36:10,520
good. Um and that I think is very

897
00:36:07,920 --> 00:36:13,520
analogous to our case

898
00:36:10,520 --> 00:36:14,880
here. Um instead we should accept a

899
00:36:13,520 --> 00:36:17,359
lottery principle where different

900
00:36:14,880 --> 00:36:19,200
randomly selected groups are chosen in

901
00:36:17,359 --> 00:36:21,119
different counters because there we've

902
00:36:19,200 --> 00:36:24,240
reduced the error burden. we've spread

903
00:36:21,119 --> 00:36:26,240
it over more people and so the

904
00:36:24,240 --> 00:36:28,560
reasonable rejection of the concentrated

905
00:36:26,240 --> 00:36:30,440
errors can be substituted for this

906
00:36:28,560 --> 00:36:33,760
distributed error

907
00:36:30,440 --> 00:36:36,800
model. Interestingly, uh I think this

908
00:36:33,760 --> 00:36:39,200
lottery principle is also shown in the

909
00:36:36,800 --> 00:36:41,119
claims framework. So I'll go through

910
00:36:39,200 --> 00:36:42,800
this a little more quickly, but we could

911
00:36:41,119 --> 00:36:44,640
also imagine we're not now thinking

912
00:36:42,800 --> 00:36:46,480
about a theory of justice. We're

913
00:36:44,640 --> 00:36:49,920
thinking about individual fairness

914
00:36:46,480 --> 00:36:51,119
claims. So, um, you have a claim to

915
00:36:49,920 --> 00:36:53,440
something when you're owed it for

916
00:36:51,119 --> 00:36:55,040
reasons of fairness. And we're going to

917
00:36:53,440 --> 00:36:56,880
say that you have equal claims to the

918
00:36:55,040 --> 00:36:59,599
same good perhaps when you have

919
00:36:56,880 --> 00:37:01,280
equivalent properties. So, for a kidney,

920
00:36:59,599 --> 00:37:02,640
maybe you're the same age and you've

921
00:37:01,280 --> 00:37:05,920
been on the waiting list for the same

922
00:37:02,640 --> 00:37:08,160
amount of time or, you know, morally

923
00:37:05,920 --> 00:37:09,599
equivalent. So, it doesn't matter

924
00:37:08,160 --> 00:37:11,440
whether you've been on the wait list for

925
00:37:09,599 --> 00:37:13,720
four years or 3.8 years. We're going to

926
00:37:11,440 --> 00:37:16,720
say those are roughly

927
00:37:13,720 --> 00:37:18,800
equivalent. Or maybe depending on how we

928
00:37:16,720 --> 00:37:21,119
uh choose morally relevant goods, we

929
00:37:18,800 --> 00:37:23,200
might say, "Hey, just being a human

930
00:37:21,119 --> 00:37:24,960
being is the right qualification." And

931
00:37:23,200 --> 00:37:27,200
so everyone is going to be equally in

932
00:37:24,960 --> 00:37:29,160
the lottery pool. And those all work

933
00:37:27,200 --> 00:37:31,280
from the claims

934
00:37:29,160 --> 00:37:33,200
perspective. And the sources of our

935
00:37:31,280 --> 00:37:36,760
claims could be dessert, that you

936
00:37:33,200 --> 00:37:39,440
deserve it, need or

937
00:37:36,760 --> 00:37:41,760
merit. Um but notice these are a little

938
00:37:39,440 --> 00:37:43,359
bit different from utilities. So

939
00:37:41,760 --> 00:37:44,480
utilities, which is something that I

940
00:37:43,359 --> 00:37:46,960
think we're more comfortable talking

941
00:37:44,480 --> 00:37:51,839
about in the computer science context,

942
00:37:46,960 --> 00:37:54,480
uh are comparable similar to claims. Um

943
00:37:51,839 --> 00:37:58,000
but claims can linger even after the

944
00:37:54,480 --> 00:38:00,560
decision is made. So if I take the

945
00:37:58,000 --> 00:38:02,560
greatest utility action, I'm done. I

946
00:38:00,560 --> 00:38:06,320
don't owe anything to the people who

947
00:38:02,560 --> 00:38:08,640
didn't get the outcome. But claims uh

948
00:38:06,320 --> 00:38:10,720
suggest that I do owe something to the

949
00:38:08,640 --> 00:38:15,200
people who had a claim but who didn't

950
00:38:10,720 --> 00:38:17,920
end up getting the thing. And so uh

951
00:38:15,200 --> 00:38:20,079
they're also different from rights um

952
00:38:17,920 --> 00:38:22,000
because rights aren't something that we

953
00:38:20,079 --> 00:38:24,240
can weigh up whereas claims are

954
00:38:22,000 --> 00:38:26,880
something that we can weigh up. So

955
00:38:24,240 --> 00:38:29,680
claims are essentially comparative. And

956
00:38:26,880 --> 00:38:32,520
interestingly um the claims framework

957
00:38:29,680 --> 00:38:36,040
also suggests that we should allocate

958
00:38:32,520 --> 00:38:39,200
undivided undivisible goods with a

959
00:38:36,040 --> 00:38:42,200
lottery. So I think this idea of using

960
00:38:39,200 --> 00:38:44,880
randomization or a lottery to spread

961
00:38:42,200 --> 00:38:47,440
chances over these different groups that

962
00:38:44,880 --> 00:38:50,320
might otherwise excluded is the right

963
00:38:47,440 --> 00:38:53,240
way to go here. And so what should we

964
00:38:50,320 --> 00:38:56,960
do? We should do something like

965
00:38:53,240 --> 00:38:58,960
that. So how are we gonna do this? Well,

966
00:38:56,960 --> 00:39:01,599
uh, when I first started working with my

967
00:38:58,960 --> 00:39:03,760
awesome collaborators Asian Shomik, uh,

968
00:39:01,599 --> 00:39:06,480
I thought, okay, let's just make a bunch

969
00:39:03,760 --> 00:39:08,480
of models. The models are different. Uh,

970
00:39:06,480 --> 00:39:10,400
and then we use those models to make

971
00:39:08,480 --> 00:39:12,000
different predictions. And it turns out

972
00:39:10,400 --> 00:39:14,800
there is a good way for us to talk about

973
00:39:12,000 --> 00:39:17,040
that and formalize it. We can say, hey,

974
00:39:14,800 --> 00:39:19,200
in fact, there are many models that have

975
00:39:17,040 --> 00:39:22,320
the same accuracy, but different token

976
00:39:19,200 --> 00:39:24,720
predictions. Great. Exciting. And we can

977
00:39:22,320 --> 00:39:26,960
even produce them on purpose. So we can

978
00:39:24,720 --> 00:39:28,880
use the phenomenon of model multiplicity

979
00:39:26,960 --> 00:39:30,720
to intervene at many different places in

980
00:39:28,880 --> 00:39:34,079
the modeling pipeline make different

981
00:39:30,720 --> 00:39:36,079
choices and create different models and

982
00:39:34,079 --> 00:39:38,800
this set of models is sometimes called

983
00:39:36,079 --> 00:39:41,839
the Rashimon set. So Rashimon is a

984
00:39:38,800 --> 00:39:44,000
wonderful movie in which uh the same

985
00:39:41,839 --> 00:39:47,359
event is viewed by different people who

986
00:39:44,000 --> 00:39:50,240
participated in it and they each tell uh

987
00:39:47,359 --> 00:39:53,440
materially conflicting stories that all

988
00:39:50,240 --> 00:39:55,520
fit the final facts. So the same guy was

989
00:39:53,440 --> 00:39:57,720
killed, the same murder weapon was found

990
00:39:55,520 --> 00:40:00,160
in the same place, but different people

991
00:39:57,720 --> 00:40:03,440
didn't. So in a similar way, we could

992
00:40:00,160 --> 00:40:06,359
generate a set of equally wellperforming

993
00:40:03,440 --> 00:40:10,400
models that have different token

994
00:40:06,359 --> 00:40:12,400
features. Uh and we can arrange this in

995
00:40:10,400 --> 00:40:14,720
sort of like philosopher logical

996
00:40:12,400 --> 00:40:16,800
possibility style where we can say the

997
00:40:14,720 --> 00:40:18,800
original model that's going to be our at

998
00:40:16,800 --> 00:40:21,119
that's going to be our home model. We

999
00:40:18,800 --> 00:40:22,800
can have a same accuracy model class. We

1000
00:40:21,119 --> 00:40:25,119
can have an equivalent accuracy model

1001
00:40:22,800 --> 00:40:27,119
class. So close enough. And then we can

1002
00:40:25,119 --> 00:40:28,839
think about all the possible models

1003
00:40:27,119 --> 00:40:31,040
regardless of their

1004
00:40:28,839 --> 00:40:34,320
accuracy. But what happens if we do

1005
00:40:31,040 --> 00:40:36,280
that? Well, um we create them multiply

1006
00:40:34,320 --> 00:40:39,280
equally accurate models. We choose

1007
00:40:36,280 --> 00:40:40,760
randomly among the models. We have this

1008
00:40:39,280 --> 00:40:44,079
possibility of being classified

1009
00:40:40,760 --> 00:40:46,640
otherwise. Uh looks good so far. We can

1010
00:40:44,079 --> 00:40:48,480
have one model that gets nine out of the

1011
00:40:46,640 --> 00:40:51,200
10 qualified candidates but

1012
00:40:48,480 --> 00:40:53,599
unfortunately rejects Amara and then we

1013
00:40:51,200 --> 00:40:55,520
have a second model that again same

1014
00:40:53,599 --> 00:40:57,400
accuracy nine out of the 10 but

1015
00:40:55,520 --> 00:40:59,920
unfortunately rejects

1016
00:40:57,400 --> 00:41:02,640
Josephine. However

1017
00:40:59,920 --> 00:41:05,599
uh it turns out that what we imagine

1018
00:41:02,640 --> 00:41:07,359
this uh Rashimon set of models might be

1019
00:41:05,599 --> 00:41:09,920
doing is not everything that we hoped

1020
00:41:07,359 --> 00:41:12,640
that it would. So if we think about the

1021
00:41:09,920 --> 00:41:14,800
Rashimon set of models, no matter how

1022
00:41:12,640 --> 00:41:17,119
fancy we get with our tricks, we're only

1023
00:41:14,800 --> 00:41:20,160
going to recover a small proportion of

1024
00:41:17,119 --> 00:41:22,480
the models that you know God or some

1025
00:41:20,160 --> 00:41:24,720
better model generating entity could

1026
00:41:22,480 --> 00:41:27,760
have made. So that's our dotted line

1027
00:41:24,720 --> 00:41:32,240
within the possible rashimon set and we

1028
00:41:27,760 --> 00:41:34,720
might hope that we are then uh getting

1029
00:41:32,240 --> 00:41:37,440
equal accuracy allocations

1030
00:41:34,720 --> 00:41:40,160
um when we end up running the model

1031
00:41:37,440 --> 00:41:42,319
forward and doing our predictions.

1032
00:41:40,160 --> 00:41:45,119
But it turns out we don't. Uh it turns

1033
00:41:42,319 --> 00:41:47,680
out unfortunately um the allocations we

1034
00:41:45,119 --> 00:41:50,079
get not only some of them are worse than

1035
00:41:47,680 --> 00:41:53,119
others, some of them are not in an equal

1036
00:41:50,079 --> 00:41:56,000
utility allocation set uh but also they

1037
00:41:53,119 --> 00:41:58,119
sample pretty unevenly from the set of

1038
00:41:56,000 --> 00:42:01,839
possible equal utility

1039
00:41:58,119 --> 00:42:04,400
allocations. And so it might be that we

1040
00:42:01,839 --> 00:42:07,359
don't get uh this even spread over

1041
00:42:04,400 --> 00:42:10,000
outcomes that we wanted. we can get to

1042
00:42:07,359 --> 00:42:12,280
equal utility allocations where four out

1043
00:42:10,000 --> 00:42:15,280
of the five qualified people are

1044
00:42:12,280 --> 00:42:18,000
selected. Um, but it might be that there

1045
00:42:15,280 --> 00:42:20,720
are yet more equal utility allocations

1046
00:42:18,000 --> 00:42:22,720
that we just never find. Um, and we

1047
00:42:20,720 --> 00:42:24,319
aren't going to find with the Rashimon

1048
00:42:22,720 --> 00:42:26,040
set that we can create with the data

1049
00:42:24,319 --> 00:42:29,040
that we actually

1050
00:42:26,040 --> 00:42:32,160
have. So when we tried to do this, we

1051
00:42:29,040 --> 00:42:33,839
indeed noted this. So if you look here

1052
00:42:32,160 --> 00:42:36,079
at the cumulative proportions of

1053
00:42:33,839 --> 00:42:38,560
qualified, so everyone on this graph is

1054
00:42:36,079 --> 00:42:40,800
qualified. Uh and so you might say,

1055
00:42:38,560 --> 00:42:43,200
okay, uh how likely is that they're

1056
00:42:40,800 --> 00:42:46,880
selected? Well, there are some people in

1057
00:42:43,200 --> 00:42:48,560
this yellow circle who even in our uh

1058
00:42:46,880 --> 00:42:51,440
different ways of creating the rashimon

1059
00:42:48,560 --> 00:42:52,960
set never get selected. And that's very

1060
00:42:51,440 --> 00:42:54,560
unfortunate.

1061
00:42:52,960 --> 00:42:55,599
And one way of thinking about this is

1062
00:42:54,560 --> 00:42:57,280
that there are some qualified

1063
00:42:55,599 --> 00:43:00,000
individuals who are consistently viewed

1064
00:42:57,280 --> 00:43:01,160
as riskier than others even within this

1065
00:43:00,000 --> 00:43:04,480
rashimon

1066
00:43:01,160 --> 00:43:06,160
set. And so our conclusion here is that

1067
00:43:04,480 --> 00:43:08,880
multiple models created from the same

1068
00:43:06,160 --> 00:43:10,200
data can fail to evenly sample the space

1069
00:43:08,880 --> 00:43:13,000
of equally good

1070
00:43:10,200 --> 00:43:15,200
outcomes. And so from an ethical

1071
00:43:13,000 --> 00:43:18,319
perspective, we have to stop here and

1072
00:43:15,200 --> 00:43:21,280
say, okay, this prompts a question. What

1073
00:43:18,319 --> 00:43:23,800
were the good applicants owed? Did we

1074
00:43:21,280 --> 00:43:27,680
owe them that they actually got the

1075
00:43:23,800 --> 00:43:29,200
thing? Uh, did we owe them that they got

1076
00:43:27,680 --> 00:43:31,599
it a certain percentage of the time?

1077
00:43:29,200 --> 00:43:33,280
Let's say they applied to 100 jobs. What

1078
00:43:31,599 --> 00:43:36,160
percentage do they actually need to move

1079
00:43:33,280 --> 00:43:39,040
on to the interview stage? Um, do we owe

1080
00:43:36,160 --> 00:43:41,040
them a chance at the good? If so, is it

1081
00:43:39,040 --> 00:43:43,640
a proportional chance proportional to

1082
00:43:41,040 --> 00:43:47,160
their goodness or just a nonzero

1083
00:43:43,640 --> 00:43:49,359
chance? And when do we owe them that

1084
00:43:47,160 --> 00:43:52,400
chance? So there's an interesting

1085
00:43:49,359 --> 00:43:55,200
discussion uh in a paper called good

1086
00:43:52,400 --> 00:43:58,240
reasons for losers. I hope to make uh

1087
00:43:55,200 --> 00:44:01,040
such a great paper title someday. Uh

1088
00:43:58,240 --> 00:44:03,760
where he argues something I think uh

1089
00:44:01,040 --> 00:44:06,640
very extreme which is um if you're

1090
00:44:03,760 --> 00:44:08,240
giving someone a chance, it's a chance

1091
00:44:06,640 --> 00:44:11,920
that you owe them at the time of

1092
00:44:08,240 --> 00:44:14,079
decision. So we would have to pseudo

1093
00:44:11,920 --> 00:44:16,680
randomly or perhaps truly randomly in

1094
00:44:14,079 --> 00:44:18,800
some way choose between our different

1095
00:44:16,680 --> 00:44:21,040
allocations at the time that we're

1096
00:44:18,800 --> 00:44:24,240
actually deciding. It couldn't be the

1097
00:44:21,040 --> 00:44:26,160
case that we uh have already done this

1098
00:44:24,240 --> 00:44:27,720
random allocation because then they

1099
00:44:26,160 --> 00:44:30,400
never really had a

1100
00:44:27,720 --> 00:44:32,800
chance. Okay.

1101
00:44:30,400 --> 00:44:35,760
But I think uh the critique of the

1102
00:44:32,800 --> 00:44:37,040
multiple model perspective suggests that

1103
00:44:35,760 --> 00:44:38,960
we're not going to get the even

1104
00:44:37,040 --> 00:44:41,680
distribution of chances that we wanted

1105
00:44:38,960 --> 00:44:43,760
out of the rashimon set anyway. So

1106
00:44:41,680 --> 00:44:45,920
instead, what if we just do a lottery?

1107
00:44:43,760 --> 00:44:48,079
So we have these likelihoods that we

1108
00:44:45,920 --> 00:44:50,079
think people are an appropriate fit for

1109
00:44:48,079 --> 00:44:52,640
the good. What if we just randomize

1110
00:44:50,079 --> 00:44:56,640
based on those likelihoods? So if you

1111
00:44:52,640 --> 00:45:00,400
are uh 79% likely to be good, we give

1112
00:44:56,640 --> 00:45:01,720
you 79 lottery tickets in our lottery to

1113
00:45:00,400 --> 00:45:03,920
get the actual

1114
00:45:01,720 --> 00:45:05,599
outcome. I'm fine with this, but this

1115
00:45:03,920 --> 00:45:07,440
makes a lot of people queasy because it

1116
00:45:05,599 --> 00:45:09,680
means there are some cases where we give

1117
00:45:07,440 --> 00:45:11,839
the 1% candidate the thing and there are

1118
00:45:09,680 --> 00:45:13,839
some cases where we don't give the 99%

1119
00:45:11,839 --> 00:45:16,240
candidate the thing. So depending on

1120
00:45:13,839 --> 00:45:18,640
your tolerance for exploration as

1121
00:45:16,240 --> 00:45:20,800
opposed to exploitation, we could also

1122
00:45:18,640 --> 00:45:22,560
think about having a quality threshold.

1123
00:45:20,800 --> 00:45:24,480
So, we could say, "Hey, we're going to

1124
00:45:22,560 --> 00:45:26,880
cut off the extremes. We're always going

1125
00:45:24,480 --> 00:45:28,960
to give it to the 99% person. We're

1126
00:45:26,880 --> 00:45:30,880
never going to give it to the 1% person,

1127
00:45:28,960 --> 00:45:32,400
but there's a big band about the middle

1128
00:45:30,880 --> 00:45:34,079
that we should admit we're not that

1129
00:45:32,400 --> 00:45:36,960
certain about, and that's where we're

1130
00:45:34,079 --> 00:45:39,440
going to run the lottery." Now, this is

1131
00:45:36,960 --> 00:45:41,839
a little bit weird because from a

1132
00:45:39,440 --> 00:45:44,240
normative perspective, uh, we're saying

1133
00:45:41,839 --> 00:45:46,560
that we do have beliefs about the

1134
00:45:44,240 --> 00:45:48,640
likelihood of these candidates and we're

1135
00:45:46,560 --> 00:45:50,560
just kind of arbitrarily, uh,

1136
00:45:48,640 --> 00:45:53,359
thresholding them. So this is what's

1137
00:45:50,560 --> 00:45:55,440
called a mixed strategy. Um which

1138
00:45:53,359 --> 00:45:57,599
philosophers have some opinions about.

1139
00:45:55,440 --> 00:45:59,760
Uh but I think it is a lot more

1140
00:45:57,599 --> 00:46:02,240
palatable in a lot of actual decision-m

1141
00:45:59,760 --> 00:46:05,680
contexts. And as you can see when

1142
00:46:02,240 --> 00:46:07,680
compared with uh the dotted line so just

1143
00:46:05,680 --> 00:46:10,800
always giving the stuff to the people

1144
00:46:07,680 --> 00:46:12,800
and the straight line running um a pure

1145
00:46:10,800 --> 00:46:14,240
lottery it's going to be in the middle.

1146
00:46:12,800 --> 00:46:17,839
It is going to reduce outcome

1147
00:46:14,240 --> 00:46:19,920
homogenization but not as much.

1148
00:46:17,839 --> 00:46:22,560
And finally, we could do something that

1149
00:46:19,920 --> 00:46:24,720
does respect our evidence. Uh, and we

1150
00:46:22,560 --> 00:46:27,520
could use conformal prediction or

1151
00:46:24,720 --> 00:46:29,920
variance. So, we could say, hey, uh, we

1152
00:46:27,520 --> 00:46:31,839
could set up the model so that we know

1153
00:46:29,920 --> 00:46:34,000
which points the model is most uncertain

1154
00:46:31,839 --> 00:46:35,839
about and flip a coin to randomize those

1155
00:46:34,000 --> 00:46:37,920
points. And it turns out many of those

1156
00:46:35,839 --> 00:46:40,880
points are around the decision threshold

1157
00:46:37,920 --> 00:46:42,640
anyway. And then maybe uh we don't have

1158
00:46:40,880 --> 00:46:45,200
to lose a lot of accuracy depending on

1159
00:46:42,640 --> 00:46:47,520
how many points we do this for but we

1160
00:46:45,200 --> 00:46:52,160
can drop outcome homogenization faster

1161
00:46:47,520 --> 00:46:54,079
than we lose accuracy. Uh so in summary

1162
00:46:52,160 --> 00:46:56,480
shared data algorithms and foundation

1163
00:46:54,079 --> 00:46:58,240
models can all create or contribute to

1164
00:46:56,480 --> 00:47:01,200
algorithmic monoculture. This

1165
00:46:58,240 --> 00:47:02,800
monoculture can homogenize outcomes. uh

1166
00:47:01,200 --> 00:47:04,560
and we should consider this to be a

1167
00:47:02,800 --> 00:47:06,560
problem when it concentrates an

1168
00:47:04,560 --> 00:47:08,560
unreasonable burden of errors or

1169
00:47:06,560 --> 00:47:10,880
consistent rejections on individuals

1170
00:47:08,560 --> 00:47:12,520
compared to other possible models. And

1171
00:47:10,880 --> 00:47:14,800
to address it, each actor in the

1172
00:47:12,520 --> 00:47:17,040
ecosystem should randomize their

1173
00:47:14,800 --> 00:47:18,400
predictions. And with that, we hope that

1174
00:47:17,040 --> 00:47:21,400
we can go from the algorithmic

1175
00:47:18,400 --> 00:47:24,130
monoculture to a beautiful field of wild

1176
00:47:21,400 --> 00:47:27,300
flowers. Thank you.

1177
00:47:24,130 --> 00:47:27,300
[Applause]

