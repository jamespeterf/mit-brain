1
00:00:04,440 --> 00:00:09,320
so I'm Anor ala and welcome to the Deep

2
00:00:07,799 --> 00:00:11,519
learning day it's going to be a little

3
00:00:09,320 --> 00:00:13,000
bit more than just deep learning and uh

4
00:00:11,519 --> 00:00:15,040
so we will start with an introduction

5
00:00:13,000 --> 00:00:17,720
and Philip will kind of set the stage

6
00:00:15,040 --> 00:00:22,320
and tell you what the day today will

7
00:00:17,720 --> 00:00:22,320
consist of and you know I hope that you

8
00:00:22,640 --> 00:00:29,080
enjoy okay um yeah thanks for coming so

9
00:00:27,160 --> 00:00:31,160
just going to get a show of hands of who

10
00:00:29,080 --> 00:00:33,800
has taken deep learning class studied

11
00:00:31,160 --> 00:00:35,680
deep learning programmed deep Nets okay

12
00:00:33,800 --> 00:00:37,320
and who has never touched a deep net

13
00:00:35,680 --> 00:00:38,960
doesn't know what a deep net is at all

14
00:00:37,320 --> 00:00:40,120
okay good so I thought we would have a

15
00:00:38,960 --> 00:00:41,719
mix and we're going to try to make this

16
00:00:40,120 --> 00:00:43,320
accessible to everyone but also include

17
00:00:41,719 --> 00:00:46,079
some of the latest technical

18
00:00:43,320 --> 00:00:47,840
content um okay but that helps so many

19
00:00:46,079 --> 00:00:49,840
of you have seen this um I'm sure all of

20
00:00:47,840 --> 00:00:51,440
you have heard of you know deep learning

21
00:00:49,840 --> 00:00:54,440
this is the big one of the big

22
00:00:51,440 --> 00:00:56,840
Technologies driving AI these days okay

23
00:00:54,440 --> 00:00:59,079
so in this first like 15 minutes I'm

24
00:00:56,840 --> 00:01:01,559
going to um kind of give you the

25
00:00:59,079 --> 00:01:03,280
schedule for the day but also give you a

26
00:01:01,559 --> 00:01:05,280
few of the basic building blocks of what

27
00:01:03,280 --> 00:01:08,040
a deep net looks like that we'll then

28
00:01:05,280 --> 00:01:11,759
see elaborated in the remaining

29
00:01:08,040 --> 00:01:13,600
lectures okay so um here's the schedule

30
00:01:11,759 --> 00:01:16,439
and it follows kind of the timeline of

31
00:01:13,600 --> 00:01:18,560
deep learning from 1986 which was when

32
00:01:16,439 --> 00:01:20,640
the uh back propagation algorithm was

33
00:01:18,560 --> 00:01:24,040
published uh which was one of the kind

34
00:01:20,640 --> 00:01:25,759
of first moments in getting an algorithm

35
00:01:24,040 --> 00:01:28,360
to be able to train a neural net to do

36
00:01:25,759 --> 00:01:32,200
something interesting up until

37
00:01:28,360 --> 00:01:33,840
today and um the first 15 minutes is

38
00:01:32,200 --> 00:01:35,240
going to be deep learning and then the

39
00:01:33,840 --> 00:01:36,560
rest of the day will be everything that

40
00:01:35,240 --> 00:01:37,799
you kind of build on top of that which

41
00:01:36,560 --> 00:01:39,119
you could call Deep learning It's

42
00:01:37,799 --> 00:01:42,759
associated with deep learning but it

43
00:01:39,119 --> 00:01:45,960
really goes beyond um the 1980s version

44
00:01:42,759 --> 00:01:48,719
of Deep learning okay so I will give you

45
00:01:45,960 --> 00:01:52,479
the basic U idea of training a neural

46
00:01:48,719 --> 00:01:56,240
network to fit data uh and then kaiming

47
00:01:52,479 --> 00:01:57,520
will come in for the next lecture um on

48
00:01:56,240 --> 00:01:58,920
uh the next part of this text stack

49
00:01:57,520 --> 00:02:01,119
we're going to build up this text stack

50
00:01:58,920 --> 00:02:03,680
which is the stack that powers modern

51
00:02:01,119 --> 00:02:05,759
systems like chat GPT so the next piece

52
00:02:03,680 --> 00:02:07,759
that you really need to understand is

53
00:02:05,759 --> 00:02:10,440
generative modeling this is the main way

54
00:02:07,759 --> 00:02:11,680
that we train our deep Nets these days

55
00:02:10,440 --> 00:02:12,599
is with what we're called generative

56
00:02:11,680 --> 00:02:15,040
modeling

57
00:02:12,599 --> 00:02:17,080
objectives um and then on top of that we

58
00:02:15,040 --> 00:02:19,040
don't just do any old type of model on

59
00:02:17,080 --> 00:02:20,280
any old type of data uh for the really

60
00:02:19,040 --> 00:02:22,640
powerful systems that you're seeing

61
00:02:20,280 --> 00:02:25,000
today uh they're mostly large language

62
00:02:22,640 --> 00:02:26,480
models so we'll have you talk about that

63
00:02:25,000 --> 00:02:28,000
uh of course there are other ideas too

64
00:02:26,480 --> 00:02:29,239
but this is the tech stock uh that we're

65
00:02:28,000 --> 00:02:31,599
going to tell you about today which is

66
00:02:29,239 --> 00:02:34,000
for the kind of modern really powerful

67
00:02:31,599 --> 00:02:38,879
popular systems like language models uh

68
00:02:34,000 --> 00:02:40,760
like chat GPD okay so then um generative

69
00:02:38,879 --> 00:02:43,800
modeling really got to be a big Topic in

70
00:02:40,760 --> 00:02:45,760
deep learning in the 2010s I put down

71
00:02:43,800 --> 00:02:49,599
2014 because that's when two Landmark

72
00:02:45,760 --> 00:02:52,000
papers were published VA vaes and Gans

73
00:02:49,599 --> 00:02:54,959
um but you know it's fuzzy these years

74
00:02:52,000 --> 00:02:57,120
could be shifted around a little bit um

75
00:02:54,959 --> 00:02:59,519
and large language models started

76
00:02:57,120 --> 00:03:02,959
becoming popular around 2020 that was

77
00:02:59,519 --> 00:03:04,519
when gpt3 was published and we're now

78
00:03:02,959 --> 00:03:06,280
currently in the large language model

79
00:03:04,519 --> 00:03:09,840
era of things like chat

80
00:03:06,280 --> 00:03:11,159
GPT okay but things have progressed

81
00:03:09,840 --> 00:03:13,120
beyond that like that's not the latest

82
00:03:11,159 --> 00:03:14,720
thing so the the even more recent thing

83
00:03:13,120 --> 00:03:19,040
which started maybe around

84
00:03:14,720 --> 00:03:21,280
2022 is uh the idea of not just directly

85
00:03:19,040 --> 00:03:23,319
like sampling from the language model to

86
00:03:21,280 --> 00:03:24,920
write random stories but doing what's

87
00:03:23,319 --> 00:03:27,400
called reasoning so I'll talk about that

88
00:03:24,920 --> 00:03:28,799
in the afternoon and reasoning models

89
00:03:27,400 --> 00:03:30,519
are uh what are starting to make

90
00:03:28,799 --> 00:03:33,000
headlines today so for example if you

91
00:03:30,519 --> 00:03:34,439
heard about deep seek R1 uh this is a

92
00:03:33,000 --> 00:03:35,239
reasoning model so I'll tell you what

93
00:03:34,439 --> 00:03:38,400
that

94
00:03:35,239 --> 00:03:39,920
means okay and then the the very last

95
00:03:38,400 --> 00:03:41,840
piece on top of this stock which Antonio

96
00:03:39,920 --> 00:03:42,959
will talk about uh he'll actually talk

97
00:03:41,840 --> 00:03:45,120
before me because we switched the

98
00:03:42,959 --> 00:03:47,680
schedule around but it's it's agents

99
00:03:45,120 --> 00:03:48,959
making systems that uh do reasoning on

100
00:03:47,680 --> 00:03:50,319
top of language models that are built

101
00:03:48,959 --> 00:03:53,040
out of generative models that

102
00:03:50,319 --> 00:03:55,120
fundamentally rest on deep neural Nets

103
00:03:53,040 --> 00:03:56,680
uh but making actual agentic AI systems

104
00:03:55,120 --> 00:03:58,720
out of that and Antonio will talk about

105
00:03:56,680 --> 00:04:00,360
that and that's um becoming the thing

106
00:03:58,720 --> 00:04:02,400
around 2023

107
00:04:00,360 --> 00:04:04,720
now of course all these ideas have their

108
00:04:02,400 --> 00:04:06,280
Roots much in much earlier work but this

109
00:04:04,720 --> 00:04:08,439
is this is kind of the the slope that

110
00:04:06,280 --> 00:04:09,879
I've seen uh and the interesting thing

111
00:04:08,439 --> 00:04:12,159
if you look at the years I've drawn them

112
00:04:09,879 --> 00:04:14,000
in kind of logarithmic scale so why are

113
00:04:12,159 --> 00:04:15,239
we you know all trying to clamor to

114
00:04:14,000 --> 00:04:16,639
learn this because this is the real

115
00:04:15,239 --> 00:04:19,400
curve this is the real timeline okay

116
00:04:16,639 --> 00:04:21,639
it's just exponential uh okay so so much

117
00:04:19,400 --> 00:04:23,120
is going on right now and it's just a

118
00:04:21,639 --> 00:04:24,840
really exciting time you can see that

119
00:04:23,120 --> 00:04:26,360
we're going pretty far beyond uh kind of

120
00:04:24,840 --> 00:04:28,400
traditional deep learning in this in

121
00:04:26,360 --> 00:04:29,960
this course uh some people choose to

122
00:04:28,400 --> 00:04:31,759
define deep learning as just the blue

123
00:04:29,960 --> 00:04:33,960
part which I'll spend the first short

124
00:04:31,759 --> 00:04:36,520
talk on um but you you know the whole

125
00:04:33,960 --> 00:04:41,240
stack at places like open AI deep seek

126
00:04:36,520 --> 00:04:44,520
Google Etc is is this current stack

127
00:04:41,240 --> 00:04:45,600
okay okay so here are the the times um I

128
00:04:44,520 --> 00:04:47,280
believe there's some print outs of the

129
00:04:45,600 --> 00:04:49,759
schedule in the back or you might be

130
00:04:47,280 --> 00:04:51,919
able to find it online um so we'll have

131
00:04:49,759 --> 00:04:54,120
generative models large language models

132
00:04:51,919 --> 00:04:56,120
then we'll have a lunch break uh then

133
00:04:54,120 --> 00:04:57,759
we'll do agents and reasoning and then

134
00:04:56,120 --> 00:05:01,400
we'll have a Q&A session at the end when

135
00:04:57,759 --> 00:05:01,400
we can take your questions

136
00:05:01,560 --> 00:05:06,680
okay so let me now tell you uh the basic

137
00:05:05,440 --> 00:05:08,000
building blocks which many of you have

138
00:05:06,680 --> 00:05:10,720
seen before but just so that we get on

139
00:05:08,000 --> 00:05:12,600
the same page to start off so so what is

140
00:05:10,720 --> 00:05:14,520
a deep net a deep net is roughly

141
00:05:12,600 --> 00:05:15,960
inspired by how the neural Nets in our

142
00:05:14,520 --> 00:05:19,199
brain work right the name is the same

143
00:05:15,960 --> 00:05:20,960
it's a neural net and um in our brains

144
00:05:19,199 --> 00:05:23,400
neuroscientists have a pretty good

145
00:05:20,960 --> 00:05:26,080
understanding of the kind of pipeline

146
00:05:23,400 --> 00:05:28,840
from raw sensory inputs data coming into

147
00:05:26,080 --> 00:05:30,440
the uh into the eye into the ears uh

148
00:05:28,840 --> 00:05:32,800
being processed by different layers of

149
00:05:30,440 --> 00:05:34,680
the brain until at the kind of back at

150
00:05:32,800 --> 00:05:36,720
the top layer of the brain uh which is

151
00:05:34,680 --> 00:05:39,280
actually in the front of your brain uh

152
00:05:36,720 --> 00:05:41,880
you end up having recognition and

153
00:05:39,280 --> 00:05:43,919
decision- making and uh more like highle

154
00:05:41,880 --> 00:05:46,840
intelligence so this is the visual

155
00:05:43,919 --> 00:05:49,280
hierarchy in the brain uh you start with

156
00:05:46,840 --> 00:05:51,400
photons you have Edge detectors which

157
00:05:49,280 --> 00:05:53,120
get like added up into conjunctions of

158
00:05:51,400 --> 00:05:56,360
edges and detected as patterns and

159
00:05:53,120 --> 00:05:58,680
Spirals and triangles and eventually in

160
00:05:56,360 --> 00:06:01,080
uh an area more frontal in the brain the

161
00:05:58,680 --> 00:06:03,400
it cortex you get get the recognition of

162
00:06:01,080 --> 00:06:07,240
cats and dogs and foxes and so

163
00:06:03,400 --> 00:06:09,000
forth okay so what does that look like

164
00:06:07,240 --> 00:06:10,599
when you make a machine do it it looks

165
00:06:09,000 --> 00:06:13,199
very similar it's going to be a

166
00:06:10,599 --> 00:06:14,880
hierarchy of going from the raw data to

167
00:06:13,199 --> 00:06:16,120
a good representation that from which

168
00:06:14,880 --> 00:06:18,000
you can make a decision or

169
00:06:16,120 --> 00:06:19,720
classification so we start with the raw

170
00:06:18,000 --> 00:06:22,720
data we might detect things about that

171
00:06:19,720 --> 00:06:24,440
data we then will process these

172
00:06:22,720 --> 00:06:26,639
subcomponents into higher order

173
00:06:24,440 --> 00:06:28,120
structures like segments and parts and

174
00:06:26,639 --> 00:06:30,800
then we'll eventually classify well

175
00:06:28,120 --> 00:06:33,000
given that I see a blob of gray over a

176
00:06:30,800 --> 00:06:34,680
background of dark and there's a circle

177
00:06:33,000 --> 00:06:36,280
in the with a like a dot in the middle

178
00:06:34,680 --> 00:06:40,000
that might be an ey then I'll recognize

179
00:06:36,280 --> 00:06:41,120
as a herin okay so this uh hierarchical

180
00:06:40,000 --> 00:06:43,919
kind of pipeline that you might have

181
00:06:41,120 --> 00:06:46,759
coded by hand 20 years ago uh now we

182
00:06:43,919 --> 00:06:49,479
simply replace all this boxes with um

183
00:06:46,759 --> 00:06:52,639
kind of empty learnable parameterized

184
00:06:49,479 --> 00:06:54,360
functions and we learn the mapping to go

185
00:06:52,639 --> 00:06:57,360
from the input to the output but it's

186
00:06:54,360 --> 00:07:00,360
still this hierarchical computational

187
00:06:57,360 --> 00:07:03,680
flow okay

188
00:07:00,360 --> 00:07:06,199
so in modern neural Nets we don't use

189
00:07:03,680 --> 00:07:07,280
exactly the same hierarchy that Vision

190
00:07:06,199 --> 00:07:08,479
scientists might have come up with in

191
00:07:07,280 --> 00:07:10,720
the past we have something a little more

192
00:07:08,479 --> 00:07:13,520
generic we just have these layers very

193
00:07:10,720 --> 00:07:16,879
simple functions um that transform the

194
00:07:13,520 --> 00:07:18,639
data layer by layer uh one by one and a

195
00:07:16,879 --> 00:07:21,599
deep net is just a lot of those okay so

196
00:07:18,639 --> 00:07:24,120
we neural Nets deep Nets same same

197
00:07:21,599 --> 00:07:26,479
thing okay so the key thing is I'm going

198
00:07:24,120 --> 00:07:28,840
to have functions that are parameterized

199
00:07:26,479 --> 00:07:30,800
by weights and biases I'll tell you what

200
00:07:28,840 --> 00:07:33,039
those are that map the input data to an

201
00:07:30,800 --> 00:07:35,240
output uh representation of the data

202
00:07:33,039 --> 00:07:37,639
like the label

203
00:07:35,240 --> 00:07:39,759
Heron okay so here's how the whole

204
00:07:37,639 --> 00:07:42,120
system looks uh when you're going to

205
00:07:39,759 --> 00:07:45,479
train it so to train this network to

206
00:07:42,120 --> 00:07:47,520
recognize images you simply provide it

207
00:07:45,479 --> 00:07:50,000
an image of a heron you provide it the

208
00:07:47,520 --> 00:07:51,800
label of what the target you want the

209
00:07:50,000 --> 00:07:53,360
network to Output at the end the target

210
00:07:51,800 --> 00:07:55,440
representation is going to be the label

211
00:07:53,360 --> 00:07:56,720
herin and you provide it with what's

212
00:07:55,440 --> 00:07:58,319
called a loss function which will

213
00:07:56,720 --> 00:08:00,080
penalize the difference between the

214
00:07:58,319 --> 00:08:02,680
output of the network and the Target

215
00:08:00,080 --> 00:08:06,560
that you wanted it to Output so this is

216
00:08:02,680 --> 00:08:08,120
called supervised learning and um if you

217
00:08:06,560 --> 00:08:09,720
haven't seen these equations uh don't

218
00:08:08,120 --> 00:08:12,120
worry if you have there to remind you of

219
00:08:09,720 --> 00:08:14,159
the the basic math here uh we're going

220
00:08:12,120 --> 00:08:15,479
to try to find the parameters which are

221
00:08:14,159 --> 00:08:16,759
going to be the weights and biases these

222
00:08:15,479 --> 00:08:19,039
are like knobs I'm going to fiddle with

223
00:08:16,759 --> 00:08:20,720
these knobs to change the transformation

224
00:08:19,039 --> 00:08:23,000
so that it will achieve what I want

225
00:08:20,720 --> 00:08:24,720
it'll achieve a low loss low loss means

226
00:08:23,000 --> 00:08:26,840
low error it's going to achieve what I

227
00:08:24,720 --> 00:08:29,039
want okay so when I see the herin it

228
00:08:26,840 --> 00:08:32,000
will output heren and if it doesn't I'll

229
00:08:29,039 --> 00:08:34,839
get a l and I'll update the

230
00:08:32,000 --> 00:08:36,000
parameters okay if I see a guitar fish

231
00:08:34,839 --> 00:08:37,399
this is the training process I'll show

232
00:08:36,000 --> 00:08:39,360
it more and more images next I'll show

233
00:08:37,399 --> 00:08:41,360
it a guitar fish and I'll say what did

234
00:08:39,360 --> 00:08:43,760
you think this is well it'll say I think

235
00:08:41,360 --> 00:08:45,120
it's a you know a clown fish I'll say

236
00:08:43,760 --> 00:08:46,560
well that was a little bit off I'll you

237
00:08:45,120 --> 00:08:48,279
know have an error and I'll try to

238
00:08:46,560 --> 00:08:51,080
update the parameters to the knobs in

239
00:08:48,279 --> 00:08:52,360
order to minimize that error and then I

240
00:08:51,080 --> 00:08:54,360
showed a car and I showed a million

241
00:08:52,360 --> 00:08:56,600
images and once I've done done that it

242
00:08:54,360 --> 00:08:59,800
will be able to um see a new image and

243
00:08:56,600 --> 00:09:01,959
recognize what it is

244
00:08:59,800 --> 00:09:03,760
okay and the way that it does that is uh

245
00:09:01,959 --> 00:09:06,920
the basic training algorithm is gradient

246
00:09:03,760 --> 00:09:10,000
descent so that simply means take that

247
00:09:06,920 --> 00:09:12,200
um objective of minimizing the loss and

248
00:09:10,000 --> 00:09:14,560
try to tune the parameters fiddle with

249
00:09:12,200 --> 00:09:17,600
the knobs in the direction that will uh

250
00:09:14,560 --> 00:09:19,720
reduce the loss uh locally and that's uh

251
00:09:17,600 --> 00:09:22,000
based on the gradient of the loss with

252
00:09:19,720 --> 00:09:23,600
respect to the parameters okay so that

253
00:09:22,000 --> 00:09:25,200
looks like this I have some setting of

254
00:09:23,600 --> 00:09:27,160
The parameters that incurs high loss

255
00:09:25,200 --> 00:09:29,920
that doesn't recognize the you know the

256
00:09:27,160 --> 00:09:31,680
guitar fish and the car and so forth and

257
00:09:29,920 --> 00:09:33,279
I simply set it up as an optimization

258
00:09:31,680 --> 00:09:35,000
problem where I'm trying to make that

259
00:09:33,279 --> 00:09:36,720
loss go down via gradient descent I'm

260
00:09:35,000 --> 00:09:38,800
just going to walk down the slope to

261
00:09:36,720 --> 00:09:41,560
lower and lower amounts of error until I

262
00:09:38,800 --> 00:09:44,760
get a good solution to the

263
00:09:41,560 --> 00:09:46,760
problem okay so this is just to kind of

264
00:09:44,760 --> 00:09:48,440
lay out that the the basic the basic

265
00:09:46,760 --> 00:09:50,839
ideas we'll go into a lot more um

266
00:09:48,440 --> 00:09:52,720
interesting detail soon okay so the way

267
00:09:50,839 --> 00:09:53,760
I'd like to think about the elementary

268
00:09:52,720 --> 00:09:55,040
operations so you can think about a

269
00:09:53,760 --> 00:09:56,800
neural network now it's just a

270
00:09:55,040 --> 00:09:59,399
parameterized function that is fit to

271
00:09:56,800 --> 00:10:01,240
data to minimize the errors you may come

272
00:09:59,399 --> 00:10:03,600
predicting properties of the data that's

273
00:10:01,240 --> 00:10:05,600
one level of understanding it uh the

274
00:10:03,600 --> 00:10:07,040
next level is well internally what are

275
00:10:05,600 --> 00:10:08,600
the transformations of this

276
00:10:07,040 --> 00:10:11,079
parameterized function they're achieving

277
00:10:08,600 --> 00:10:12,640
that interesting mapping and the way I

278
00:10:11,079 --> 00:10:14,920
like to think of it is that layer by

279
00:10:12,640 --> 00:10:17,200
layer you're changing how you represent

280
00:10:14,920 --> 00:10:19,160
the data you're transforming the data so

281
00:10:17,200 --> 00:10:21,160
you start with the raw data and you do

282
00:10:19,160 --> 00:10:23,160
something to it to make it into a better

283
00:10:21,160 --> 00:10:26,240
format and you do this over and over

284
00:10:23,160 --> 00:10:28,880
again until at the end you get your

285
00:10:26,240 --> 00:10:30,920
decision okay so so this is going to be

286
00:10:28,880 --> 00:10:32,600
um a kind of mathematical like little

287
00:10:30,920 --> 00:10:34,399
abstract picture of what's going on

288
00:10:32,600 --> 00:10:36,279
inside a neural net uh but I think it

289
00:10:34,399 --> 00:10:37,839
helps helps us understand it so let's

290
00:10:36,279 --> 00:10:40,680
just look at a neural net that's trained

291
00:10:37,839 --> 00:10:41,959
to classify the red points apart from

292
00:10:40,680 --> 00:10:45,560
the Blue

293
00:10:41,959 --> 00:10:48,120
Points okay so you might have done some

294
00:10:45,560 --> 00:10:50,279
basic linear classifiers in math classes

295
00:10:48,120 --> 00:10:51,639
in the past well this is a little tricky

296
00:10:50,279 --> 00:10:53,480
because it's not actually linearly

297
00:10:51,639 --> 00:10:54,760
separable you can't separate that red

298
00:10:53,480 --> 00:10:56,519
set of points from the blue set of

299
00:10:54,760 --> 00:10:58,360
points with a line so you need a

300
00:10:56,519 --> 00:11:02,240
nonlinear classifier luckily deep Nets

301
00:10:58,360 --> 00:11:04,399
can do that uh so this is showing the uh

302
00:11:02,240 --> 00:11:06,600
the data represented as a scatter plot

303
00:11:04,399 --> 00:11:08,200
so each data point is a two-dimensional

304
00:11:06,600 --> 00:11:10,360
Vector two attributes and you can draw

305
00:11:08,200 --> 00:11:12,920
that in a two-dimensional plane on the

306
00:11:10,360 --> 00:11:15,760
left and the whole goal of a classifier

307
00:11:12,920 --> 00:11:18,839
is simply to find a way of moving the

308
00:11:15,760 --> 00:11:20,959
red points all to the output

309
00:11:18,839 --> 00:11:23,839
representation of a cat which we'll call

310
00:11:20,959 --> 00:11:28,079
the label one so the label one will be

311
00:11:23,839 --> 00:11:31,839
represented as um yal 1 and xals 0 so Y

312
00:11:28,079 --> 00:11:34,720
is like catness and X is dogness and um

313
00:11:31,839 --> 00:11:37,320
so I want all the red points to move to

314
00:11:34,720 --> 00:11:39,839
the point 1 Z that all interpret as a

315
00:11:37,320 --> 00:11:41,560
label of the data being a cat and all

316
00:11:39,839 --> 00:11:45,639
the blue points should move to the label

317
00:11:41,560 --> 00:11:48,959
01 it should be not at all a cat 100% a

318
00:11:45,639 --> 00:11:50,920
dog so a classifier's output which is is

319
00:11:48,959 --> 00:11:51,920
it what's is a cat or dog can be

320
00:11:50,920 --> 00:11:54,360
represented as this kind of

321
00:11:51,920 --> 00:11:56,320
two-dimensional picture of its belief to

322
00:11:54,360 --> 00:11:59,360
the degree that it's a cat the y- Axis

323
00:11:56,320 --> 00:12:00,800
or it's a dog the x-axis

324
00:11:59,360 --> 00:12:03,200
okay so all the deepnet is doing is

325
00:12:00,800 --> 00:12:05,279
finding a way of moving those data

326
00:12:03,200 --> 00:12:06,760
points around and the interesting thing

327
00:12:05,279 --> 00:12:09,519
is that the way it moves them around is

328
00:12:06,760 --> 00:12:11,839
very very simple each layer of that

329
00:12:09,519 --> 00:12:14,040
neural network is just going to make a

330
00:12:11,839 --> 00:12:16,800
very simple modification to where the

331
00:12:14,040 --> 00:12:18,160
data points live uh and eventually at

332
00:12:16,800 --> 00:12:19,560
the very top they'll all live at the

333
00:12:18,160 --> 00:12:21,240
right Point all the red points will have

334
00:12:19,560 --> 00:12:23,040
moved to the cat label and all the blue

335
00:12:21,240 --> 00:12:25,160
points will have moved to the the dog

336
00:12:23,040 --> 00:12:28,120
label so what does that look like

337
00:12:25,160 --> 00:12:29,560
there's two basic layer types and uh

338
00:12:28,120 --> 00:12:31,320
then there's a lot of other traditional

339
00:12:29,560 --> 00:12:33,399
fancy ones but the two basic ones that

340
00:12:31,320 --> 00:12:35,079
are really the workor of deep learning

341
00:12:33,399 --> 00:12:36,560
are called the linear layer and the

342
00:12:35,079 --> 00:12:39,320
pointwise

343
00:12:36,560 --> 00:12:40,839
nonlinearity so the linear layer is one

344
00:12:39,320 --> 00:12:43,399
of those Transformations okay it's going

345
00:12:40,839 --> 00:12:45,160
to be a linear transformation of the

346
00:12:43,399 --> 00:12:46,480
data here a geometric linear

347
00:12:45,160 --> 00:12:49,440
transformation so what does that look

348
00:12:46,480 --> 00:12:51,519
like so in math it looks like this I

349
00:12:49,440 --> 00:12:54,160
take my input data which is just you

350
00:12:51,519 --> 00:12:57,560
know some image or some raw data I'm

351
00:12:54,160 --> 00:13:00,000
processing X in I uh multiply that by

352
00:12:57,560 --> 00:13:02,079
matrix W that's a linear transform

353
00:13:00,000 --> 00:13:04,680
we call the Matrix W the weights of the

354
00:13:02,079 --> 00:13:06,800
neural network I add a bias um so it's

355
00:13:04,680 --> 00:13:08,560
an aine transformation and I get an

356
00:13:06,800 --> 00:13:10,560
output which has moved the data points

357
00:13:08,560 --> 00:13:13,199
to new location X

358
00:13:10,560 --> 00:13:14,880
out so people in neural netland often

359
00:13:13,199 --> 00:13:16,320
like to draw these as networks of

360
00:13:14,880 --> 00:13:18,920
neurons but this is just a

361
00:13:16,320 --> 00:13:21,240
representation of that equation I have a

362
00:13:18,920 --> 00:13:23,560
vector xn of numbers I have a linear

363
00:13:21,240 --> 00:13:26,279
transformation which is uh like every

364
00:13:23,560 --> 00:13:28,399
output is a linear combination of all of

365
00:13:26,279 --> 00:13:30,000
the input numbers okay that's just a

366
00:13:28,399 --> 00:13:32,199
matrix a linear combination and then I

367
00:13:30,000 --> 00:13:35,440
have a bias which I can add a kind of

368
00:13:32,199 --> 00:13:37,720
constant to each of the um output

369
00:13:35,440 --> 00:13:39,320
Dimensions okay so this is two different

370
00:13:37,720 --> 00:13:41,880
ways of looking at the network my

371
00:13:39,320 --> 00:13:44,680
favorite way is actually this so here is

372
00:13:41,880 --> 00:13:47,360
a picture of that plane that represents

373
00:13:44,680 --> 00:13:49,920
um the input data X in and how does that

374
00:13:47,360 --> 00:13:51,600
data get transformed to become X out so

375
00:13:49,920 --> 00:13:52,680
a linear transformation looks like that

376
00:13:51,600 --> 00:13:54,079
it's it's actually an aine

377
00:13:52,680 --> 00:13:56,519
transformation it's a geometric

378
00:13:54,079 --> 00:13:58,839
transformation of that input data

379
00:13:56,519 --> 00:14:01,160
distribution if the data lives on a grid

380
00:13:58,839 --> 00:14:04,360
then will make this picture here if the

381
00:14:01,160 --> 00:14:06,040
data is like a cloud of red points and I

382
00:14:04,360 --> 00:14:08,519
have some particular linear

383
00:14:06,040 --> 00:14:10,720
transformation uh it will look like this

384
00:14:08,519 --> 00:14:13,839
it can rotate and it can shift and

385
00:14:10,720 --> 00:14:15,759
translate and scale uh the distribution

386
00:14:13,839 --> 00:14:17,399
of the data so it'll move that data

387
00:14:15,759 --> 00:14:20,240
around that data is the

388
00:14:17,399 --> 00:14:21,480
representation um of the neural Lance

389
00:14:20,240 --> 00:14:23,199
representation of the data it's

390
00:14:21,480 --> 00:14:26,320
processing on Layer

391
00:14:23,199 --> 00:14:28,199
Two okay so that's the first function

392
00:14:26,320 --> 00:14:31,839
that it uses to move all the red points

393
00:14:28,199 --> 00:14:33,800
to one dot okay the second function it

394
00:14:31,839 --> 00:14:36,480
uses called the reu this is just going

395
00:14:33,800 --> 00:14:38,920
to be for each Dimension each uh each

396
00:14:36,480 --> 00:14:40,680
neuron will I'll take its value and if

397
00:14:38,920 --> 00:14:43,440
it's below zero I'll set it to zero and

398
00:14:40,680 --> 00:14:46,360
it's above zero I won't do anything okay

399
00:14:43,440 --> 00:14:49,360
so it's like a um a clipping type of

400
00:14:46,360 --> 00:14:50,800
function Clips off the negative values

401
00:14:49,360 --> 00:14:52,920
and here's what it looks like in terms

402
00:14:50,800 --> 00:14:55,279
of how it transforms data all the data

403
00:14:52,920 --> 00:14:57,680
in the negative part of that plane in

404
00:14:55,279 --> 00:15:00,920
Negative X or negative y gets clamped to

405
00:14:57,680 --> 00:15:02,759
the axes xal 0 or y equals 0 and the

406
00:15:00,920 --> 00:15:04,240
data in the positive part is an identity

407
00:15:02,759 --> 00:15:05,440
nothing changes there it just is it

408
00:15:04,240 --> 00:15:08,000
doesn't do

409
00:15:05,440 --> 00:15:10,160
anything okay and so if I apply that to

410
00:15:08,000 --> 00:15:11,880
a cloud of red points then this will be

411
00:15:10,160 --> 00:15:14,759
the effect the reu tries to kind of push

412
00:15:11,880 --> 00:15:16,639
everything onto the axes um of this High

413
00:15:14,759 --> 00:15:18,240
dimensional space uh this neural net

414
00:15:16,639 --> 00:15:19,880
representation

415
00:15:18,240 --> 00:15:22,399
space

416
00:15:19,880 --> 00:15:24,720
okay so those are two very simple ways

417
00:15:22,399 --> 00:15:26,079
of moving around the points and there's

418
00:15:24,720 --> 00:15:27,959
a bunch of other ones I'm not going to

419
00:15:26,079 --> 00:15:30,440
talk about all of these but we have the

420
00:15:27,959 --> 00:15:31,639
linear the reu we have normalization

421
00:15:30,440 --> 00:15:34,680
layers which kind of push everything

422
00:15:31,639 --> 00:15:37,120
onto a sphere and we have softmax layers

423
00:15:34,680 --> 00:15:38,600
which kind of um are another type of

424
00:15:37,120 --> 00:15:42,560
normalization

425
00:15:38,600 --> 00:15:44,959
effectively okay so simple geometric

426
00:15:42,560 --> 00:15:46,759
Transformations that take your raw data

427
00:15:44,959 --> 00:15:49,399
distribution and move it around until

428
00:15:46,759 --> 00:15:50,839
you have achieved a um an arrangement of

429
00:15:49,399 --> 00:15:52,480
the data you've moved the Blue Points

430
00:15:50,839 --> 00:15:54,000
apart from the red points so I'm only

431
00:15:52,480 --> 00:15:56,399
showing the red points here but what if

432
00:15:54,000 --> 00:15:59,120
we look at the Blue Points too so here

433
00:15:56,399 --> 00:16:01,319
is a uh like you know five layer uh

434
00:15:59,120 --> 00:16:03,040
Network this is being trained with

435
00:16:01,319 --> 00:16:05,480
gradient descent and I'm going to show

436
00:16:03,040 --> 00:16:08,079
you a movie of how gradient descent will

437
00:16:05,480 --> 00:16:09,880
update the transformations to move the

438
00:16:08,079 --> 00:16:11,399
red points apart from the Blue Points so

439
00:16:09,880 --> 00:16:14,480
the top of the network we're going to

440
00:16:11,399 --> 00:16:16,920
want the blue points to all live on xal

441
00:16:14,480 --> 00:16:19,160
1 yals 0 and the red points all live at

442
00:16:16,920 --> 00:16:22,199
xals 0 y equals 1 that's label for cat

443
00:16:19,160 --> 00:16:25,360
and dog okay or two two classes

444
00:16:22,199 --> 00:16:26,880
abstractly the input the representations

445
00:16:25,360 --> 00:16:28,720
are all entangled you can't separate

446
00:16:26,880 --> 00:16:31,160
them and then layer by layer we're going

447
00:16:28,720 --> 00:16:33,319
to achieve this transformation okay at

448
00:16:31,160 --> 00:16:34,800
initialization it's basically not doing

449
00:16:33,319 --> 00:16:36,480
anything interesting it's just kind of

450
00:16:34,800 --> 00:16:38,800
smushing all the red and the blue points

451
00:16:36,480 --> 00:16:39,839
on top of each other um and it looks

452
00:16:38,800 --> 00:16:42,199
like all blue but that's just a

453
00:16:39,839 --> 00:16:44,959
rendering artifact at the top okay so

454
00:16:42,199 --> 00:16:46,800
here's what it looks like so this is

455
00:16:44,959 --> 00:16:49,560
exact this is not hiding anything this

456
00:16:46,800 --> 00:16:51,040
is really the uh the the this is a

457
00:16:49,560 --> 00:16:52,440
neural network where every layer is a

458
00:16:51,040 --> 00:16:53,839
two-dimensional transformation usually

459
00:16:52,440 --> 00:16:55,240
they're high dimensional Transformations

460
00:16:53,839 --> 00:16:57,480
this is just a two-dimensional

461
00:16:55,240 --> 00:16:58,959
transformation and first the linear

462
00:16:57,480 --> 00:17:00,160
transformation at the bottom the first

463
00:16:58,959 --> 00:17:02,880
linear layer kind of stretches

464
00:17:00,160 --> 00:17:05,520
everything out then the reu snaps

465
00:17:02,880 --> 00:17:06,760
everything onto the two axes and then

466
00:17:05,520 --> 00:17:08,240
another linear transformation kind of

467
00:17:06,760 --> 00:17:09,640
stretches it in a different direction

468
00:17:08,240 --> 00:17:11,400
it's trying to kind of stretch and pull

469
00:17:09,640 --> 00:17:12,720
things such that somehow we're going to

470
00:17:11,400 --> 00:17:14,600
get the red points apart from the Blue

471
00:17:12,720 --> 00:17:16,720
Points you might be thinking this seems

472
00:17:14,600 --> 00:17:19,240
really impoverished like how could it

473
00:17:16,720 --> 00:17:20,880
possibly um you know disentangle

474
00:17:19,240 --> 00:17:23,160
complicated data distributions where the

475
00:17:20,880 --> 00:17:24,600
classes are all overlapping in different

476
00:17:23,160 --> 00:17:28,039
uh different parts of the space and and

477
00:17:24,600 --> 00:17:30,520
the interesting thing is yes a neural

478
00:17:28,039 --> 00:17:31,679
network with only two dimensional you

479
00:17:30,520 --> 00:17:33,720
know two neurons per layer two

480
00:17:31,679 --> 00:17:35,320
dimensional Transformations is not

481
00:17:33,720 --> 00:17:38,559
capable of doing everything it works in

482
00:17:35,320 --> 00:17:40,160
this case but uh interestingly a neural

483
00:17:38,559 --> 00:17:41,919
network in high Dimensions just using

484
00:17:40,160 --> 00:17:43,919
these linear and point was nonlinear

485
00:17:41,919 --> 00:17:45,520
Transformations can fit any function

486
00:17:43,919 --> 00:17:47,760
it's called the universal approximation

487
00:17:45,520 --> 00:17:49,559
property it can fit any any continuous

488
00:17:47,760 --> 00:17:51,400
function okay so at the top of the

489
00:17:49,559 --> 00:17:54,559
network we now have all the Blue Points

490
00:17:51,400 --> 00:17:57,440
shifted to uh one one cluster where x

491
00:17:54,559 --> 00:18:00,799
equals z meaning full belief it's a dog

492
00:17:57,440 --> 00:18:03,240
and all the red points to uh y equals 1

493
00:18:00,799 --> 00:18:05,159
uh which all means 100% belief it's a

494
00:18:03,240 --> 00:18:06,400
cat Okay so I'll show you that one more

495
00:18:05,159 --> 00:18:07,880
time this is what it looks like to train

496
00:18:06,400 --> 00:18:10,640
a

497
00:18:07,880 --> 00:18:12,440
network okay so this is the way I like

498
00:18:10,640 --> 00:18:14,159
to think about neural nuts and the uh

499
00:18:12,440 --> 00:18:15,679
the the fundamental building blocks

500
00:18:14,159 --> 00:18:17,480
geometric transformations of a data

501
00:18:15,679 --> 00:18:19,360
distribution that go from raw entangled

502
00:18:17,480 --> 00:18:21,640
data to better organized data from which

503
00:18:19,360 --> 00:18:23,400
you can make a decision a prediction or

504
00:18:21,640 --> 00:18:25,200
potentially just like generate pixels

505
00:18:23,400 --> 00:18:28,880
which Tim being we'll talk

506
00:18:25,200 --> 00:18:30,240
about okay so um I think that that would

507
00:18:28,880 --> 00:18:31,480
the Whirlwind tour of just getting you

508
00:18:30,240 --> 00:18:32,840
to think about neural networks let me

509
00:18:31,480 --> 00:18:35,440
take one or two questions and then King

510
00:18:32,840 --> 00:18:35,440
will come on

511
00:18:35,960 --> 00:18:41,320
yeah yes uh reu is rectified

512
00:18:38,720 --> 00:18:44,600
nonlinearity or rectified linear unit is

513
00:18:41,320 --> 00:18:44,600
is the is the

514
00:18:46,240 --> 00:18:51,720
acronymic so uh linear and uh a very

515
00:18:49,880 --> 00:18:53,720
broad class of pointwise nonlinearities

516
00:18:51,720 --> 00:18:55,120
uh have the property that you can they

517
00:18:53,720 --> 00:18:56,880
have Universal approximation property

518
00:18:55,120 --> 00:18:59,120
you can fit any function with those if

519
00:18:56,880 --> 00:19:04,159
you stack enough of them and you have

520
00:18:59,120 --> 00:19:05,679
enough neurons per layer um but it's not

521
00:19:04,159 --> 00:19:07,760
anything it's not absolutely anything

522
00:19:05,679 --> 00:19:10,080
yeah you can you can have some functions

523
00:19:07,760 --> 00:19:13,080
that are not Universal

524
00:19:10,080 --> 00:19:16,159
yeah okay any more questions and then I

525
00:19:13,080 --> 00:19:16,159
think we'll do timing yeah one

526
00:19:22,480 --> 00:19:30,640
more all the faces uh is that the way

527
00:19:27,720 --> 00:19:33,000
this then to the edges what does the

528
00:19:30,640 --> 00:19:35,120
whole Edge represent so the the whole

529
00:19:33,000 --> 00:19:37,480
Edge actually represents in this case uh

530
00:19:35,120 --> 00:19:39,120
technically the belief the probability

531
00:19:37,480 --> 00:19:40,919
that you think the image is a dog so

532
00:19:39,120 --> 00:19:42,880
it's not saying like what type of dog or

533
00:19:40,919 --> 00:19:46,039
all the different types of faces it's

534
00:19:42,880 --> 00:19:47,880
just uh if the point had landed uh like

535
00:19:46,039 --> 00:19:49,120
here I would be saying I have 50% Bel

536
00:19:47,880 --> 00:19:51,520
it's a dog I don't know if it's a cat or

537
00:19:49,120 --> 00:19:53,440
a dog uh so really what I'm showing you

538
00:19:51,520 --> 00:19:55,840
is is called a Simplex but I didn't get

539
00:19:53,440 --> 00:19:57,919
into the the full details here like up

540
00:19:55,840 --> 00:19:59,720
and down yeah up and down is like my

541
00:19:57,919 --> 00:20:04,760
belief of cat and left and right is my

542
00:19:59,720 --> 00:20:06,080
belief of dog yeah get 100% c%g that's

543
00:20:04,760 --> 00:20:07,320
the goal yeah so that's what I'm showing

544
00:20:06,080 --> 00:20:10,240
you as the goal

545
00:20:07,320 --> 00:20:13,360
yeah there is a point yeah yeah that's

546
00:20:10,240 --> 00:20:15,799
yeah like because you're not just P to

547
00:20:13,360 --> 00:20:18,919
point to the

548
00:20:15,799 --> 00:20:18,919
edge The

549
00:20:19,559 --> 00:20:24,200
Edge this whole it's it's actually this

550
00:20:22,360 --> 00:20:25,960
line is going from my belief that it's a

551
00:20:24,200 --> 00:20:27,400
cat to my belief it's a dog and

552
00:20:25,960 --> 00:20:29,559
intermediate points mean I don't know

553
00:20:27,400 --> 00:20:31,559
means 50/50 and I want all the red

554
00:20:29,559 --> 00:20:32,679
points to be I'm I'm sure it's a it's a

555
00:20:31,559 --> 00:20:34,320
cat and all the blue points to be I'm

556
00:20:32,679 --> 00:20:35,880
sure it's a dog but during training it's

557
00:20:34,320 --> 00:20:37,400
not sure so it's going to try to

558
00:20:35,880 --> 00:20:41,240
gradually become morer and pull them

559
00:20:37,400 --> 00:20:44,520
apart yeah that's the idea okay one more

560
00:20:41,240 --> 00:20:47,159
and then we'll go yeah oh yeah go ahead

561
00:20:44,520 --> 00:20:49,200
you go back a little TOS of yeah right

562
00:20:47,159 --> 00:20:51,320
there how those

563
00:20:49,200 --> 00:20:54,000
wirings uh the wiring graphs are just

564
00:20:51,320 --> 00:20:57,520
showing like this is a vector of inputs

565
00:20:54,000 --> 00:20:59,960
xn uh so it has indices I and uh this is

566
00:20:57,520 --> 00:21:04,080
a vector of outputs and the the

567
00:20:59,960 --> 00:21:06,080
dependencies like uh the jth index or

568
00:21:04,080 --> 00:21:08,720
you know the ith index of X Out is a

569
00:21:06,080 --> 00:21:11,880
function of the ith index of X of X in

570
00:21:08,720 --> 00:21:13,440
plus this um Global kind of potential

571
00:21:11,880 --> 00:21:14,799
here and that's what that layer is doing

572
00:21:13,440 --> 00:21:16,480
so it's kind of showing the dependency

573
00:21:14,799 --> 00:21:18,760
structure okay I think we should move on

574
00:21:16,480 --> 00:21:20,159
to Kim Ming um and I'll be back uh in

575
00:21:18,760 --> 00:21:24,360
the afternoon for

576
00:21:20,159 --> 00:21:24,360
more thank you

