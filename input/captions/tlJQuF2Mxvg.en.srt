1
00:00:05,920 --> 00:00:11,160
I'd like to introduce you to Sam Seer

2
00:00:08,000 --> 00:00:14,360
this is a picture of Sam over here Sam

3
00:00:11,160 --> 00:00:19,039
lives in a a small rural area in New

4
00:00:14,360 --> 00:00:21,320
Hampshire not too far from MIT and Sam's

5
00:00:19,039 --> 00:00:23,439
Community has struggled economically it

6
00:00:21,320 --> 00:00:25,119
hasn't really um experienced a lot of

7
00:00:23,439 --> 00:00:27,439
the benefits of technological change

8
00:00:25,119 --> 00:00:30,679
that places like Bangkok or or Boston

9
00:00:27,439 --> 00:00:32,520
have and when Sam graduated from high

10
00:00:30,679 --> 00:00:34,239
school not long thereafter he went to

11
00:00:32,520 --> 00:00:36,480
work in manufacturing he's been working

12
00:00:34,239 --> 00:00:38,879
in factories ever since and his job in

13
00:00:36,480 --> 00:00:40,680
the factory traditionally had been to

14
00:00:38,879 --> 00:00:42,480
operate a machine this is the

15
00:00:40,680 --> 00:00:44,719
entry-level job for a lot of

16
00:00:42,480 --> 00:00:47,199
manufacturing workers in America or or

17
00:00:44,719 --> 00:00:50,079
Europe and even in in Thailand and his

18
00:00:47,199 --> 00:00:51,800
role day-to-day was to load and unload

19
00:00:50,079 --> 00:00:53,960
metal parts for a cutting tool

20
00:00:51,800 --> 00:00:55,640
manufacturer inspect them for quality

21
00:00:53,960 --> 00:00:58,280
and really make sure that if anything

22
00:00:55,640 --> 00:01:02,800
broke he would get his supervisor

23
00:00:58,280 --> 00:01:04,320
involved now one day Sam's bosses came

24
00:01:02,800 --> 00:01:06,720
to him and told him they were going to

25
00:01:04,320 --> 00:01:08,720
introduce robots robotic automation on

26
00:01:06,720 --> 00:01:11,320
the factory floor and his job was going

27
00:01:08,720 --> 00:01:13,400
to change and and this is a a pretty

28
00:01:11,320 --> 00:01:16,159
common story for a lot of workers like

29
00:01:13,400 --> 00:01:18,560
Sam and I think there are two ways he

30
00:01:16,159 --> 00:01:21,119
could have thought about it one is that

31
00:01:18,560 --> 00:01:23,600
this is a threat to his job this is

32
00:01:21,119 --> 00:01:25,479
threatening to automate out his role

33
00:01:23,600 --> 00:01:27,759
within the company that he might either

34
00:01:25,479 --> 00:01:31,040
lose his job or become less important as

35
00:01:27,759 --> 00:01:33,399
a machinist and the second thought was

36
00:01:31,040 --> 00:01:35,399
maybe this is an opportunity actually to

37
00:01:33,399 --> 00:01:38,079
take that the automation could take away

38
00:01:35,399 --> 00:01:40,159
some of my tasks but allow me to move in

39
00:01:38,079 --> 00:01:42,560
to a type of job that required more

40
00:01:40,159 --> 00:01:45,640
skill more creativity and maybe could

41
00:01:42,560 --> 00:01:47,680
earn a higher wage and this sort of

42
00:01:45,640 --> 00:01:49,799
dilemma these two different directions

43
00:01:47,680 --> 00:01:52,560
that Automation and new technologies can

44
00:01:49,799 --> 00:01:54,040
take workers is is the subject of of my

45
00:01:52,560 --> 00:01:56,560
research and the research of others at

46
00:01:54,040 --> 00:01:58,479
MIT a and we think about it as kind of a

47
00:01:56,560 --> 00:02:00,280
fork in the road it's a fork in the road

48
00:01:58,479 --> 00:02:01,799
for people like Sam but it's also a fork

49
00:02:00,280 --> 00:02:03,240
in the road for the companies that

50
00:02:01,799 --> 00:02:05,960
employ them that there's a choice

51
00:02:03,240 --> 00:02:08,319
between adopting technology in a way

52
00:02:05,960 --> 00:02:11,080
that might increase productivity but

53
00:02:08,319 --> 00:02:12,959
actually makes jobs lower quality it Des

54
00:02:11,080 --> 00:02:15,120
skills them it doesn't necessarily

55
00:02:12,959 --> 00:02:17,440
contribute to increased wages and we've

56
00:02:15,120 --> 00:02:19,680
seen this type of commodification of

57
00:02:17,440 --> 00:02:21,599
work in a lot of cases in some cases

58
00:02:19,680 --> 00:02:22,640
with the introduction of computers some

59
00:02:21,599 --> 00:02:24,680
cases with the introduction of new

60
00:02:22,640 --> 00:02:26,319
machines that took craft out of

61
00:02:24,680 --> 00:02:28,000
manufacturing that made these jobs less

62
00:02:26,319 --> 00:02:30,400
skilled and you could certainly imagine

63
00:02:28,000 --> 00:02:32,160
robots doing that for a worker like Sam

64
00:02:30,400 --> 00:02:34,319
that his knowledge that he's acquired

65
00:02:32,160 --> 00:02:36,720
from years in manufacturing isn't really

66
00:02:34,319 --> 00:02:38,840
codified in the robot and he is really

67
00:02:36,720 --> 00:02:41,239
just a glorified babysitter for the

68
00:02:38,840 --> 00:02:42,319
robot in his role and his wages go down

69
00:02:41,239 --> 00:02:44,280
and we've actually seen this in the

70
00:02:42,319 --> 00:02:46,360
automotive industry in the US where

71
00:02:44,280 --> 00:02:48,720
there was a really dramatic introduction

72
00:02:46,360 --> 00:02:51,920
of robotic Automation in the 1980s and

73
00:02:48,720 --> 00:02:54,080
1990s inspired in part by uh Toyota and

74
00:02:51,920 --> 00:02:55,360
other companies but wages didn't go up

75
00:02:54,080 --> 00:02:57,800
for manufacturing workers in the

76
00:02:55,360 --> 00:03:00,000
automotive industry the required skills

77
00:02:57,800 --> 00:03:01,519
didn't necessarily go up either so so we

78
00:03:00,000 --> 00:03:03,480
didn't necessarily see we saw more of

79
00:03:01,519 --> 00:03:05,519
this path of the commodification of work

80
00:03:03,480 --> 00:03:07,280
in the automotive sector but there's an

81
00:03:05,519 --> 00:03:09,159
alternative story and a story that I

82
00:03:07,280 --> 00:03:12,000
think most of us in the room would agree

83
00:03:09,159 --> 00:03:14,239
is is more desirable and it's a story in

84
00:03:12,000 --> 00:03:16,000
which workers like Sam when automation's

85
00:03:14,239 --> 00:03:17,480
introduced they experience more

86
00:03:16,000 --> 00:03:20,440
opportunities opportunities to grow in

87
00:03:17,480 --> 00:03:22,760
their jobs grow into higher skill higher

88
00:03:20,440 --> 00:03:24,879
wage careers I think this is kind of the

89
00:03:22,760 --> 00:03:27,879
socially desirable outcome from

90
00:03:24,879 --> 00:03:29,920
Technologies like robotics or or AI so

91
00:03:27,879 --> 00:03:31,799
the question is I think the question for

92
00:03:29,920 --> 00:03:33,680
me at least and I hope some of you are

93
00:03:31,799 --> 00:03:36,360
interested as well is how do we take

94
00:03:33,680 --> 00:03:39,200
this right path versus the left path and

95
00:03:36,360 --> 00:03:41,080
also is this right path both better for

96
00:03:39,200 --> 00:03:42,519
workers and better for organizations or

97
00:03:41,080 --> 00:03:44,280
is there some kind of trade-off so

98
00:03:42,519 --> 00:03:46,319
that's been the kind of driving question

99
00:03:44,280 --> 00:03:49,439
for a lot of our research um over the

100
00:03:46,319 --> 00:03:51,280
past decade or so and what I think is is

101
00:03:49,439 --> 00:03:53,519
interesting about this current moment is

102
00:03:51,280 --> 00:03:55,920
that we we have some pretty good data on

103
00:03:53,519 --> 00:03:57,640
this fork in the road when it comes to

104
00:03:55,920 --> 00:03:59,079
robots we have some pretty good data

105
00:03:57,640 --> 00:04:01,200
when it comes to computers and even the

106
00:03:59,079 --> 00:04:04,319
internet but we don't really have much

107
00:04:01,200 --> 00:04:06,920
data at all much Insight on this

108
00:04:04,319 --> 00:04:09,680
Avalanche of new technological progress

109
00:04:06,920 --> 00:04:11,439
in in the area of AI so I want to talk a

110
00:04:09,680 --> 00:04:13,799
little bit about the lessons we can

111
00:04:11,439 --> 00:04:15,799
learn from the first couple years of of

112
00:04:13,799 --> 00:04:19,359
llms being on the market and also from

113
00:04:15,799 --> 00:04:21,079
more narrow AI Technologies so many of

114
00:04:19,359 --> 00:04:23,160
my colleagues have talked about this you

115
00:04:21,079 --> 00:04:25,560
heard from Eric about kind of the the

116
00:04:23,160 --> 00:04:27,720
impact of AI as a a technological system

117
00:04:25,560 --> 00:04:30,080
at a society level you heard from George

118
00:04:27,720 --> 00:04:32,320
and even the previous panel about how

119
00:04:30,080 --> 00:04:33,880
organizations are adapting to Ai and I

120
00:04:32,320 --> 00:04:35,759
want to zoom in even further on the

121
00:04:33,880 --> 00:04:37,919
individual level and think about the

122
00:04:35,759 --> 00:04:40,360
impact of these Technologies on workers

123
00:04:37,919 --> 00:04:42,800
like Sam and I think there are kind of

124
00:04:40,360 --> 00:04:45,880
three hypotheses that I want to walk

125
00:04:42,800 --> 00:04:47,360
through with you the first is Will AI or

126
00:04:45,880 --> 00:04:49,639
under what conditions will these

127
00:04:47,360 --> 00:04:50,919
Technologies improve worker productivity

128
00:04:49,639 --> 00:04:52,680
will they make workers I think about

129
00:04:50,919 --> 00:04:55,240
productivity in a pretty specific way

130
00:04:52,680 --> 00:04:57,120
will they allow workers to provide more

131
00:04:55,240 --> 00:04:59,320
value added for the company's bottom

132
00:04:57,120 --> 00:05:01,160
line per hour that they work so this

133
00:04:59,320 --> 00:05:03,720
isn't just being able to do a task

134
00:05:01,160 --> 00:05:06,320
faster doing that task faster needs to

135
00:05:03,720 --> 00:05:08,639
translate into more value for the

136
00:05:06,320 --> 00:05:10,160
company right and this isn't necessarily

137
00:05:08,639 --> 00:05:11,560
a priority for Sam but it's a priority

138
00:05:10,160 --> 00:05:14,840
for his employer and it might translate

139
00:05:11,560 --> 00:05:17,199
into higher wages the second is what

140
00:05:14,840 --> 00:05:19,039
jobs are affected all jobs aren't

141
00:05:17,199 --> 00:05:20,680
necessarily going to be affected equally

142
00:05:19,039 --> 00:05:24,199
they aren't by any technology so the

143
00:05:20,680 --> 00:05:25,800
question is will the jobs of you know of

144
00:05:24,199 --> 00:05:27,479
today what which jobs will be more

145
00:05:25,800 --> 00:05:29,919
affected than others and there's some

146
00:05:27,479 --> 00:05:31,600
conjecture that office jobs even higher

147
00:05:29,919 --> 00:05:33,479
wage office jobs might be more affected

148
00:05:31,600 --> 00:05:35,440
by AI Technologies compared to

149
00:05:33,479 --> 00:05:37,400
Technologies like robots so so I want to

150
00:05:35,440 --> 00:05:39,440
investigate that with you a little bit

151
00:05:37,400 --> 00:05:41,080
and then finally there's this this

152
00:05:39,440 --> 00:05:42,880
question of skills and training that's

153
00:05:41,080 --> 00:05:45,240
come up over and over again today which

154
00:05:42,880 --> 00:05:47,919
is you know what type of skills might

155
00:05:45,240 --> 00:05:50,400
you need to work successfully with AI

156
00:05:47,919 --> 00:05:52,319
there's this uh Trope that's gone around

157
00:05:50,400 --> 00:05:53,919
with companies is that you're you're not

158
00:05:52,319 --> 00:05:55,240
going to be replaced by an algorithm

159
00:05:53,919 --> 00:05:56,720
you're going to be replaced by a worker

160
00:05:55,240 --> 00:05:58,840
who knows how to use that algorithm

161
00:05:56,720 --> 00:06:00,400
really well that the skill is really to

162
00:05:58,840 --> 00:06:02,919
be a successful operator of the

163
00:06:00,400 --> 00:06:04,639
technology and and I I want to push back

164
00:06:02,919 --> 00:06:08,199
a little bit and question that in in the

165
00:06:04,639 --> 00:06:10,400
course of uh today so so H how might we

166
00:06:08,199 --> 00:06:12,199
evaluate these hypothesis what what uh

167
00:06:10,400 --> 00:06:14,840
resources do we have to talk to them so

168
00:06:12,199 --> 00:06:17,199
one is we have history so we have some

169
00:06:14,840 --> 00:06:19,280
experience with understanding the impact

170
00:06:17,199 --> 00:06:21,080
of technology on productivity uh the

171
00:06:19,280 --> 00:06:23,199
second we have some original research

172
00:06:21,080 --> 00:06:25,560
that comes from our surveys of workers

173
00:06:23,199 --> 00:06:28,360
and our work with uh about 60 companies

174
00:06:25,560 --> 00:06:30,080
tracking their um their initial

175
00:06:28,360 --> 00:06:32,400
experiments with generative AI

176
00:06:30,080 --> 00:06:35,560
and then also we have a lot of a legacy

177
00:06:32,400 --> 00:06:38,599
of doing really interesting human AI

178
00:06:35,560 --> 00:06:40,759
interactive work at at MIT um this is a

179
00:06:38,599 --> 00:06:43,160
a course that I co-taught last year with

180
00:06:40,759 --> 00:06:44,400
Julie Shaw on how humans interact most

181
00:06:43,160 --> 00:06:47,080
successfully with automated systems so

182
00:06:44,400 --> 00:06:49,520
we have a framework from the past that

183
00:06:47,080 --> 00:06:51,880
was applied in uh autopilot situations

184
00:06:49,520 --> 00:06:54,080
for uh in in airplanes that's been

185
00:06:51,880 --> 00:06:55,800
applied to air traffic controllers um

186
00:06:54,080 --> 00:06:57,440
that's been applied to uh security

187
00:06:55,800 --> 00:06:59,960
agents at the airport a lot of air

188
00:06:57,440 --> 00:07:01,639
airport airplane uh examples part as is

189
00:06:59,960 --> 00:07:03,400
taught in Aeronautics and astronautics

190
00:07:01,639 --> 00:07:05,680
at MIT but nonetheless we have a

191
00:07:03,400 --> 00:07:07,319
framework for thinking about how these

192
00:07:05,680 --> 00:07:10,080
might play out in the future even though

193
00:07:07,319 --> 00:07:12,680
this is a new technology okay so what

194
00:07:10,080 --> 00:07:14,319
makes this this new or different why

195
00:07:12,680 --> 00:07:17,199
might some of these past Frameworks not

196
00:07:14,319 --> 00:07:19,240
apply to the present and you know the

197
00:07:17,199 --> 00:07:21,000
the really terrific introductions to the

198
00:07:19,240 --> 00:07:22,800
technology from from Eric and George

199
00:07:21,000 --> 00:07:24,800
were were useful if I could summarize

200
00:07:22,800 --> 00:07:27,120
them in one plot the difference is

201
00:07:24,800 --> 00:07:29,639
really that Technologies like you know

202
00:07:27,120 --> 00:07:31,160
Sam's robots that are replacing him

203
00:07:29,639 --> 00:07:32,919
they're really good at doing the same

204
00:07:31,160 --> 00:07:34,919
thing over and over again so they can

205
00:07:32,919 --> 00:07:36,680
load and unload a machine very

206
00:07:34,919 --> 00:07:38,919
consistently and reliably as long as

207
00:07:36,680 --> 00:07:41,520
they have a dedicated power source but

208
00:07:38,919 --> 00:07:44,360
the role for Sam and the role for humans

209
00:07:41,520 --> 00:07:45,479
is to add flexibility to robots so for

210
00:07:44,360 --> 00:07:47,840
for industrial robots to be really

211
00:07:45,479 --> 00:07:49,599
useful they need a human to reprogram

212
00:07:47,840 --> 00:07:51,240
them to understand where in a process

213
00:07:49,599 --> 00:07:52,759
they might be most effective and to

214
00:07:51,240 --> 00:07:54,479
engineer around them right so that's the

215
00:07:52,759 --> 00:07:57,000
role of the engineer or the really

216
00:07:54,479 --> 00:07:59,520
skilled technician now as you've heard

217
00:07:57,000 --> 00:08:01,840
from example after example generative AI

218
00:07:59,520 --> 00:08:03,520
is very flexible it's it's almost a

219
00:08:01,840 --> 00:08:05,479
uniquely flexible technology in that it

220
00:08:03,520 --> 00:08:07,680
can take in all sorts of input can

221
00:08:05,479 --> 00:08:09,639
provide all sorts of output answer any

222
00:08:07,680 --> 00:08:12,199
type of question in different languages

223
00:08:09,639 --> 00:08:14,039
but it's not very robust it might be

224
00:08:12,199 --> 00:08:16,000
becoming more robust over time a lot of

225
00:08:14,039 --> 00:08:17,960
these cases of robustness actually

226
00:08:16,000 --> 00:08:19,879
sacrifice some flexibility so you could

227
00:08:17,960 --> 00:08:21,840
see some AI technologies that are over

228
00:08:19,879 --> 00:08:23,639
here but to in order to make the

229
00:08:21,840 --> 00:08:25,960
technology become much more useful or

230
00:08:23,639 --> 00:08:28,000
robust often times you have to rely on

231
00:08:25,960 --> 00:08:30,360
human skills right so the the role of

232
00:08:28,000 --> 00:08:31,919
the Human Instead of providing f ability

233
00:08:30,360 --> 00:08:33,640
and problem solving in the area of an

234
00:08:31,919 --> 00:08:35,719
engineer the role of the human with

235
00:08:33,640 --> 00:08:38,440
generative AI is often to add a layer of

236
00:08:35,719 --> 00:08:39,760
robustness to interpret what the gener

237
00:08:38,440 --> 00:08:41,039
what the algorithm is giving you and

238
00:08:39,760 --> 00:08:44,279
making sure that it's correct or that

239
00:08:41,039 --> 00:08:45,839
it's useful and to act upon it so you

240
00:08:44,279 --> 00:08:47,519
might think this this could change with

241
00:08:45,839 --> 00:08:48,920
agentic a I think we should wait and see

242
00:08:47,519 --> 00:08:51,519
on that so I might I might table that

243
00:08:48,920 --> 00:08:53,480
for for questions um and and I think an

244
00:08:51,519 --> 00:08:55,880
illustration of this which uh some of

245
00:08:53,480 --> 00:08:57,640
you might be familiar with is uh the

246
00:08:55,880 --> 00:08:59,120
strawberry problem that's manifested for

247
00:08:57,640 --> 00:09:00,839
open AI over and over again with their

248
00:08:59,120 --> 00:09:02,680
GP PT products how many of you have

249
00:09:00,839 --> 00:09:05,040
heard or encountered the strawberry

250
00:09:02,680 --> 00:09:07,560
problem well okay so some some people

251
00:09:05,040 --> 00:09:08,760
know it um you might I don't know how

252
00:09:07,560 --> 00:09:10,839
they originally discovered this because

253
00:09:08,760 --> 00:09:12,959
I don't know who who asked how many RS

254
00:09:10,839 --> 00:09:15,800
are there in Strawberry but nonetheless

255
00:09:12,959 --> 00:09:18,079
when you ask um open when you ask chat

256
00:09:15,800 --> 00:09:19,839
GPT how many RS are there in Strawberry

257
00:09:18,079 --> 00:09:21,399
inevitably if you ask in a particular

258
00:09:19,839 --> 00:09:23,560
way it'll get the answer wrong very

259
00:09:21,399 --> 00:09:25,160
basic question and then if you ask again

260
00:09:23,560 --> 00:09:26,839
in a different way it'll get it right

261
00:09:25,160 --> 00:09:29,399
it'll give you inconsistent answers over

262
00:09:26,839 --> 00:09:30,839
time so this is just kind of an ongoing

263
00:09:29,399 --> 00:09:33,200
conversation that i' I've had about

264
00:09:30,839 --> 00:09:34,800
strawberries with uh an algorithm which

265
00:09:33,200 --> 00:09:36,640
you know might require additional

266
00:09:34,800 --> 00:09:38,200
therapy or something to to investigate

267
00:09:36,640 --> 00:09:40,720
why I had such a long conversation about

268
00:09:38,200 --> 00:09:43,600
strawberries but nonetheless what I

269
00:09:40,720 --> 00:09:45,519
think this this problem indicates is a

270
00:09:43,600 --> 00:09:48,000
broader challenge for these sorts of

271
00:09:45,519 --> 00:09:49,560
tools the broader challenge is this that

272
00:09:48,000 --> 00:09:51,200
the engineers at open AI which have

273
00:09:49,560 --> 00:09:53,279
built such a brilliant technology that

274
00:09:51,200 --> 00:09:55,200
has so many different applications know

275
00:09:53,279 --> 00:09:57,000
that this problem exists and nonetheless

276
00:09:55,200 --> 00:09:58,600
they can't fix it because they don't

277
00:09:57,000 --> 00:10:00,160
really know why it manifests what's

278
00:09:58,600 --> 00:10:01,399
going on under the hood this is the

279
00:10:00,160 --> 00:10:02,920
problem of explainability and

280
00:10:01,399 --> 00:10:04,959
transparency that's come up in previous

281
00:10:02,920 --> 00:10:07,440
conversations that even a simple problem

282
00:10:04,959 --> 00:10:08,880
like this isn't necessarily fixable

283
00:10:07,440 --> 00:10:11,279
without engineering an entirely

284
00:10:08,880 --> 00:10:12,680
different model so I think for

285
00:10:11,279 --> 00:10:15,240
organizations as long as you know when

286
00:10:12,680 --> 00:10:17,200
these errors come up it might be okay to

287
00:10:15,240 --> 00:10:20,399
use it or if these errors are low cost

288
00:10:17,200 --> 00:10:22,959
but nonetheless the inability to explain

289
00:10:20,399 --> 00:10:24,519
why these errors manifest and avoid them

290
00:10:22,959 --> 00:10:26,120
is a consistent Challenge and that's the

291
00:10:24,519 --> 00:10:28,040
challenge of robustness that that I

292
00:10:26,120 --> 00:10:30,600
mentioned okay so to give a little

293
00:10:28,040 --> 00:10:33,200
background on on how we tried to

294
00:10:30,600 --> 00:10:35,440
understand these problems myself and and

295
00:10:33,200 --> 00:10:38,519
two faculty members at MIT Kate Kellogg

296
00:10:35,440 --> 00:10:40,639
and and Julie Shaw um you know we we had

297
00:10:38,519 --> 00:10:43,639
all done research on automation its

298
00:10:40,639 --> 00:10:46,639
impact on work in uh in in like the pre

299
00:10:43,639 --> 00:10:48,680
period you know before 2021 um myself in

300
00:10:46,639 --> 00:10:50,320
manufacturing Julie in in manufacturing

301
00:10:48,680 --> 00:10:52,160
and Aeronautics and Kate Kellogg in

302
00:10:50,320 --> 00:10:54,560
healthcare and what we decided was that

303
00:10:52,160 --> 00:10:55,920
we wanted to study uh in a systematic

304
00:10:54,560 --> 00:10:58,160
way the impact that these new

305
00:10:55,920 --> 00:10:59,399
technologies were having on workers and

306
00:10:58,160 --> 00:11:01,440
we wanted to see if a lot of our

307
00:10:59,399 --> 00:11:03,160
assumptions from these other Industries

308
00:11:01,440 --> 00:11:05,600
and contexts and Technologies would

309
00:11:03,160 --> 00:11:06,880
apply to llm so what we did is we

310
00:11:05,600 --> 00:11:08,880
started to call up companies with the

311
00:11:06,880 --> 00:11:12,000
help of ILP we convened a group of about

312
00:11:08,880 --> 00:11:13,600
60 uh companies to to track just how

313
00:11:12,000 --> 00:11:14,880
were they using these tools what effects

314
00:11:13,600 --> 00:11:17,040
were they having what trade-offs were

315
00:11:14,880 --> 00:11:18,399
they seeing and then we also did some

316
00:11:17,040 --> 00:11:20,839
some additional research and leaned on

317
00:11:18,399 --> 00:11:23,920
the research of our colleagues um over

318
00:11:20,839 --> 00:11:27,040
the past you know year or so and I think

319
00:11:23,920 --> 00:11:29,360
the um lessons from this research now

320
00:11:27,040 --> 00:11:30,399
almost a year and a half on are really

321
00:11:29,360 --> 00:11:32,560
instructive so what I want to walk

322
00:11:30,399 --> 00:11:34,240
through are Five Lessons From This

323
00:11:32,560 --> 00:11:37,200
research and give you some some of the

324
00:11:34,240 --> 00:11:38,959
data behind each so the first and and

325
00:11:37,200 --> 00:11:40,200
these might be of no surprise to you is

326
00:11:38,959 --> 00:11:41,959
that there are some use cases where

327
00:11:40,200 --> 00:11:44,079
these tools are are seem to be really

328
00:11:41,959 --> 00:11:46,760
effective and delivering on their

329
00:11:44,079 --> 00:11:47,920
promises now there's a bigger question

330
00:11:46,760 --> 00:11:49,680
of whether these use cases can

331
00:11:47,920 --> 00:11:52,120
generalize to the economy as a whole but

332
00:11:49,680 --> 00:11:54,920
I I'll go through a few use cases so

333
00:11:52,120 --> 00:11:57,680
early on you 20 2023 so now you know

334
00:11:54,920 --> 00:11:59,760
almost two years ago fully um there were

335
00:11:57,680 --> 00:12:01,839
initial studies of where the these tools

336
00:11:59,760 --> 00:12:04,519
could improve productivity so this on

337
00:12:01,839 --> 00:12:06,880
the left is uh a study from two

338
00:12:04,519 --> 00:12:09,440
economics graduate students at at MIT of

339
00:12:06,880 --> 00:12:12,360
a writing tasks so essentially they had

340
00:12:09,440 --> 00:12:14,519
uh one group of people who were not

341
00:12:12,360 --> 00:12:16,800
professional writers uh try to write a

342
00:12:14,519 --> 00:12:18,800
press release so the one group wrote it

343
00:12:16,800 --> 00:12:21,440
just on their own with a little bit of

344
00:12:18,800 --> 00:12:22,839
information given from uh the the

345
00:12:21,440 --> 00:12:24,680
researchers and then the other group

346
00:12:22,839 --> 00:12:27,000
used it with the assistance of of a chat

347
00:12:24,680 --> 00:12:29,000
GPT like tool and what they found was

348
00:12:27,000 --> 00:12:32,040
not only that did chat GPT made make the

349
00:12:29,000 --> 00:12:33,920
right faster um they also made the the

350
00:12:32,040 --> 00:12:35,880
press release higher quality when judged

351
00:12:33,920 --> 00:12:37,360
by independent experts right you know

352
00:12:35,880 --> 00:12:38,800
not surprising now this is one of the

353
00:12:37,360 --> 00:12:40,639
earliest studies you published in

354
00:12:38,800 --> 00:12:42,000
science is kind of the Baseline of what

355
00:12:40,639 --> 00:12:43,920
happens to your productivity when you

356
00:12:42,000 --> 00:12:45,560
use this tool the other finding that

357
00:12:43,920 --> 00:12:47,639
they had which is really interesting but

358
00:12:45,560 --> 00:12:49,880
has been challenged by later work is

359
00:12:47,639 --> 00:12:52,720
that the the poorer writers so writers

360
00:12:49,880 --> 00:12:54,720
with lower quality uh content without

361
00:12:52,720 --> 00:12:57,360
Chachi PT actually saw the biggest

362
00:12:54,720 --> 00:12:59,480
benefit and and they concluded that if

363
00:12:57,360 --> 00:13:01,399
you were actually a lesser skilled

364
00:12:59,480 --> 00:13:03,360
person in writing you could see more

365
00:13:01,399 --> 00:13:04,800
benefit by using the tool and this is

366
00:13:03,360 --> 00:13:06,639
different from something like robots

367
00:13:04,800 --> 00:13:08,880
where robots seem to benefit higher

368
00:13:06,639 --> 00:13:11,040
skilled individuals they said this this

369
00:13:08,880 --> 00:13:13,320
tool looked fundamentally different and

370
00:13:11,040 --> 00:13:15,440
that that study was really kind of

371
00:13:13,320 --> 00:13:17,480
confirmed and expanded in in this study

372
00:13:15,440 --> 00:13:19,920
which was of call center workers so this

373
00:13:17,480 --> 00:13:22,199
is the first study of generative a in

374
00:13:19,920 --> 00:13:23,519
the wild at a real call center facility

375
00:13:22,199 --> 00:13:25,839
and what they tracked with this plot is

376
00:13:23,519 --> 00:13:28,160
showing is the comparison between three

377
00:13:25,839 --> 00:13:30,240
groups one group that had a kind of an

378
00:13:28,160 --> 00:13:33,760
AI assistant as a the call center worker

379
00:13:30,240 --> 00:13:36,279
one group that started out uh you know

380
00:13:33,760 --> 00:13:37,839
that started out trained to deal with

381
00:13:36,279 --> 00:13:39,800
calls without an AI assist and were

382
00:13:37,839 --> 00:13:41,839
giving that AI assistant uh a little

383
00:13:39,800 --> 00:13:43,519
ways in to their job and then the final

384
00:13:41,839 --> 00:13:45,560
group The the blue group that did this

385
00:13:43,519 --> 00:13:46,720
job always manually and what they found

386
00:13:45,560 --> 00:13:48,880
they found two things that were really

387
00:13:46,720 --> 00:13:50,279
interesting one was consistently the red

388
00:13:48,880 --> 00:13:52,240
group the group that was trained to do

389
00:13:50,279 --> 00:13:53,839
their job with the AI assistant they

390
00:13:52,240 --> 00:13:55,279
learned to get to a higher level of

391
00:13:53,839 --> 00:13:57,079
productivity much quicker so if you

392
00:13:55,279 --> 00:13:58,279
think about this is their learning curve

393
00:13:57,079 --> 00:14:00,639
uh how many calls they were able to

394
00:13:58,279 --> 00:14:02,040
resolve per hour the red group gets to

395
00:14:00,639 --> 00:14:03,839
this like three and a half level of

396
00:14:02,040 --> 00:14:06,000
calls per hour very quickly whereas it

397
00:14:03,839 --> 00:14:08,320
takes the the green group and the blue

398
00:14:06,000 --> 00:14:10,560
group much longer almost double the time

399
00:14:08,320 --> 00:14:12,120
to get to that level the second thing

400
00:14:10,560 --> 00:14:14,079
that they learned was that there's

401
00:14:12,120 --> 00:14:16,120
something fundamentally different

402
00:14:14,079 --> 00:14:18,480
between getting trained with this tool

403
00:14:16,120 --> 00:14:20,600
being sort of an AI native in a job

404
00:14:18,480 --> 00:14:22,079
versus learning to do the job one way

405
00:14:20,600 --> 00:14:23,839
learning to do the job on your own and

406
00:14:22,079 --> 00:14:25,279
then having this tool introduced that

407
00:14:23,839 --> 00:14:27,440
somehow there might be something that's

408
00:14:25,279 --> 00:14:30,199
hard it's hard to transform how you do

409
00:14:27,440 --> 00:14:32,639
your job because the group that received

410
00:14:30,199 --> 00:14:34,440
the uh AI tool 5 to six months into

411
00:14:32,639 --> 00:14:36,480
their job they didn't see really any

412
00:14:34,440 --> 00:14:38,079
prod significant productivity benefit

413
00:14:36,480 --> 00:14:39,600
right so that was a really I think

414
00:14:38,079 --> 00:14:41,600
interesting Insight then the final

415
00:14:39,600 --> 00:14:43,959
question that I have looking at this is

416
00:14:41,600 --> 00:14:45,759
does the red line continue to go up you

417
00:14:43,959 --> 00:14:47,320
know does when you have this tool can

418
00:14:45,759 --> 00:14:49,800
you become far more productive than

419
00:14:47,320 --> 00:14:51,920
someone who has a year of experience or

420
00:14:49,800 --> 00:14:53,120
is the real benefit of using this tool

421
00:14:51,920 --> 00:14:55,000
to get to this high level of

422
00:14:53,120 --> 00:14:56,160
productivity and then you flatten out

423
00:14:55,000 --> 00:14:58,000
and converge and it's really just a

424
00:14:56,160 --> 00:14:59,399
matter of learning faster so this I

425
00:14:58,000 --> 00:15:00,680
think is an interesting study that

426
00:14:59,399 --> 00:15:02,399
leaves some unanswered questions but

427
00:15:00,680 --> 00:15:04,800
really shows customer service and this

428
00:15:02,399 --> 00:15:08,440
is consistent in our companies is a is a

429
00:15:04,800 --> 00:15:10,399
real um potential use case for uh these

430
00:15:08,440 --> 00:15:12,839
Technologies and and has the potential

431
00:15:10,399 --> 00:15:14,560
to scale up and and improve productivity

432
00:15:12,839 --> 00:15:17,800
now another one that that had brought up

433
00:15:14,560 --> 00:15:19,360
earlier GitHub co-pilot um I I saw many

434
00:15:17,800 --> 00:15:21,160
people in the audience have have used

435
00:15:19,360 --> 00:15:23,120
this in your uh either your teams or you

436
00:15:21,160 --> 00:15:25,600
have used this individually um here's

437
00:15:23,120 --> 00:15:27,480
one of the first studies of uh it's a

438
00:15:25,600 --> 00:15:29,920
randomized control trial of GitHub

439
00:15:27,480 --> 00:15:31,680
co-pilot used at several large companies

440
00:15:29,920 --> 00:15:34,560
one the the third column here is the

441
00:15:31,680 --> 00:15:36,279
anonymous uh is a Anonymous financial

442
00:15:34,560 --> 00:15:39,120
institution and then you have Microsoft

443
00:15:36,279 --> 00:15:40,920
and Accenture now the study had a tough

444
00:15:39,120 --> 00:15:42,360
time measuring productivity of software

445
00:15:40,920 --> 00:15:44,399
engineering it's so what indicator do

446
00:15:42,360 --> 00:15:46,639
you choose of of how software

447
00:15:44,399 --> 00:15:48,199
contributes to the bottom line so they

448
00:15:46,639 --> 00:15:51,199
chose three instead of just one they

449
00:15:48,199 --> 00:15:53,800
chose how many uh how many uh times

450
00:15:51,199 --> 00:15:55,199
people are pulling uh code requests how

451
00:15:53,800 --> 00:15:56,639
many commits and how many builds and

452
00:15:55,199 --> 00:15:59,279
then their measure of quality was the

453
00:15:56,639 --> 00:16:01,079
success rate of those builds and what I

454
00:15:59,279 --> 00:16:02,759
found really interesting was that every

455
00:16:01,079 --> 00:16:04,440
time I talk to someone who's using G

456
00:16:02,759 --> 00:16:06,440
GitHub co-pilot they talk about how

457
00:16:04,440 --> 00:16:08,959
amazing it is how transformative so I

458
00:16:06,440 --> 00:16:11,480
was expecting to see consistently

459
00:16:08,959 --> 00:16:14,040
positive and really large productivity

460
00:16:11,480 --> 00:16:15,560
gains across the companies but one thing

461
00:16:14,040 --> 00:16:17,680
that was I found fascinating was at

462
00:16:15,560 --> 00:16:19,440
Microsoft where this you know technology

463
00:16:17,680 --> 00:16:21,160
was developed in Partnership they didn't

464
00:16:19,440 --> 00:16:23,800
actually see a significant increase in

465
00:16:21,160 --> 00:16:25,399
in builds and you know they only saw a

466
00:16:23,800 --> 00:16:26,839
significant increase in in poll requests

467
00:16:25,399 --> 00:16:28,680
so people were taking on more work they

468
00:16:26,839 --> 00:16:30,480
weren't necessarily delivering more work

469
00:16:28,680 --> 00:16:32,079
and Accenture where they did see a

470
00:16:30,480 --> 00:16:34,279
significant actually a huge increase in

471
00:16:32,079 --> 00:16:35,560
the uh amount of new applications they

472
00:16:34,279 --> 00:16:37,399
were building they actually saw that

473
00:16:35,560 --> 00:16:39,240
those builds were of a lower quality

474
00:16:37,399 --> 00:16:40,959
significantly lower quality now when you

475
00:16:39,240 --> 00:16:42,959
pull all these together you see what

476
00:16:40,959 --> 00:16:44,120
look like productivity increases but

477
00:16:42,959 --> 00:16:46,160
there's a certain trade-off and

478
00:16:44,120 --> 00:16:48,120
inconsistency here so I think there's

479
00:16:46,160 --> 00:16:49,839
again these are suggestions of of

480
00:16:48,120 --> 00:16:51,680
positive results but we need to dig a

481
00:16:49,839 --> 00:16:53,399
little bit deeper and understand why are

482
00:16:51,680 --> 00:16:54,839
these results inconsistent why aren't

483
00:16:53,399 --> 00:16:57,079
these companies which are using the same

484
00:16:54,839 --> 00:16:59,000
tool experiencing some of the same

485
00:16:57,079 --> 00:17:01,240
benefits

486
00:16:59,000 --> 00:17:03,800
so that leads me to the the second big

487
00:17:01,240 --> 00:17:05,799
lesson is understanding how individuals

488
00:17:03,800 --> 00:17:08,079
are using these tools in different ways

489
00:17:05,799 --> 00:17:10,760
and how their performance might differ

490
00:17:08,079 --> 00:17:12,600
so here is some of of our original

491
00:17:10,760 --> 00:17:15,439
research um you could look at the whole

492
00:17:12,600 --> 00:17:17,400
paper online um and and we really tried

493
00:17:15,439 --> 00:17:19,360
to understand how workers were using

494
00:17:17,400 --> 00:17:20,720
these tools by asking workers themselves

495
00:17:19,360 --> 00:17:23,439
so we had this great opportunity to feel

496
00:17:20,720 --> 00:17:25,480
the survey of 9,000 workers across nine

497
00:17:23,439 --> 00:17:27,360
different countries and to try to

498
00:17:25,480 --> 00:17:29,600
understand why were some workers seeing

499
00:17:27,360 --> 00:17:30,960
more benefits from these tools and why

500
00:17:29,600 --> 00:17:33,280
were some workers more resistant to

501
00:17:30,960 --> 00:17:35,799
these tools um and and I include in

502
00:17:33,280 --> 00:17:37,919
these tools generative AI more narrow AI

503
00:17:35,799 --> 00:17:39,799
applications robotics we asked about all

504
00:17:37,919 --> 00:17:43,240
sorts of different Technologies and

505
00:17:39,799 --> 00:17:45,160
here's a bit of what we found so overall

506
00:17:43,240 --> 00:17:46,720
we asked about how workers saw the

507
00:17:45,160 --> 00:17:49,240
impact of Automation and new

508
00:17:46,720 --> 00:17:50,760
technologies like AI on five different

509
00:17:49,240 --> 00:17:52,080
aspects of their job and I'm going to

510
00:17:50,760 --> 00:17:53,480
come back to these these factors a

511
00:17:52,080 --> 00:17:55,559
little bit over over again so I'll go

512
00:17:53,480 --> 00:17:58,080
over them one was how does it affect

513
00:17:55,559 --> 00:17:59,280
your safety and comfort at work one was

514
00:17:58,080 --> 00:18:01,440
how do you think it affects fects your

515
00:17:59,280 --> 00:18:02,760
pay your your just kind of your earnings

516
00:18:01,440 --> 00:18:05,080
at work that we consider that an

517
00:18:02,760 --> 00:18:06,880
important piece of job quality how does

518
00:18:05,080 --> 00:18:07,880
it affect your Independence on the job

519
00:18:06,880 --> 00:18:10,000
this is particularly important to

520
00:18:07,880 --> 00:18:11,960
American workers um how does it affect

521
00:18:10,000 --> 00:18:13,919
your ability to uh develop new skills

522
00:18:11,960 --> 00:18:15,799
grow in your career upward mobility and

523
00:18:13,919 --> 00:18:17,919
then finally and this is kind of the the

524
00:18:15,799 --> 00:18:19,799
kicker for us how does it affect your

525
00:18:17,919 --> 00:18:21,600
job security this was our proxy for do

526
00:18:19,799 --> 00:18:23,919
you feel threatened by this technology

527
00:18:21,600 --> 00:18:25,520
or does this technology not affect you

528
00:18:23,919 --> 00:18:27,320
does it feel make you feel more secure

529
00:18:25,520 --> 00:18:29,080
for some reason and what was really

530
00:18:27,320 --> 00:18:31,880
striking that across the n countries

531
00:18:29,080 --> 00:18:33,840
when we added everything up we found

532
00:18:31,880 --> 00:18:35,320
significant net positives of how workers

533
00:18:33,840 --> 00:18:36,799
felt about new technology that workers

534
00:18:35,320 --> 00:18:38,440
were actually by and large very

535
00:18:36,799 --> 00:18:39,679
optimistic about new technologies and

536
00:18:38,440 --> 00:18:42,400
its impact on their work particularly

537
00:18:39,679 --> 00:18:44,280
optimistic about the ability for

538
00:18:42,400 --> 00:18:46,799
automation to improve their safety

539
00:18:44,280 --> 00:18:48,280
improve their autonomy and even um

540
00:18:46,799 --> 00:18:49,919
improve their upper Mobility their

541
00:18:48,280 --> 00:18:51,720
ability like that right path show the

542
00:18:49,919 --> 00:18:53,679
ability to gain new skills move into

543
00:18:51,720 --> 00:18:56,919
higher wage jobs they were a little more

544
00:18:53,679 --> 00:18:58,280
skeptical of its impact on job security

545
00:18:56,919 --> 00:18:59,280
but really they were still more positive

546
00:18:58,280 --> 00:19:00,840
than negative

547
00:18:59,280 --> 00:19:03,159
right so this was I think a really

548
00:19:00,840 --> 00:19:05,480
surprising finding for us that despite a

549
00:19:03,159 --> 00:19:08,480
media narrative about all this negative

550
00:19:05,480 --> 00:19:10,320
impact that AI uh and other Technologies

551
00:19:08,480 --> 00:19:11,880
uh seem to have on work that workers

552
00:19:10,320 --> 00:19:14,559
themselves when you ask them aren't

553
00:19:11,880 --> 00:19:15,880
negative at all or at least a large a

554
00:19:14,559 --> 00:19:17,320
majority of workers aren't negative at

555
00:19:15,880 --> 00:19:18,520
all so then we wanted to dig a Little

556
00:19:17,320 --> 00:19:19,880
Deeper like what's happening which

557
00:19:18,520 --> 00:19:22,559
workers are positive which workers are

558
00:19:19,880 --> 00:19:24,000
negative and what differentiates them so

559
00:19:22,559 --> 00:19:26,480
first I want to break this down by by

560
00:19:24,000 --> 00:19:29,799
country and maybe uh throw a little

561
00:19:26,480 --> 00:19:32,159
shade at at the United States so workers

562
00:19:29,799 --> 00:19:33,840
in uh European countries were far more

563
00:19:32,159 --> 00:19:35,080
optimistic than workers in the United

564
00:19:33,840 --> 00:19:36,960
States about the impact of new

565
00:19:35,080 --> 00:19:38,919
technologies on their jobs very

566
00:19:36,960 --> 00:19:40,720
surprising to us we did not anticipate

567
00:19:38,919 --> 00:19:42,400
this at all we thought that workers in

568
00:19:40,720 --> 00:19:43,840
the United States where a lot of these

569
00:19:42,400 --> 00:19:46,360
Technologies were developed or being

570
00:19:43,840 --> 00:19:48,640
applied early on would be given the

571
00:19:46,360 --> 00:19:50,200
reputation for you know openness to new

572
00:19:48,640 --> 00:19:51,960
technology and the lack of regulation of

573
00:19:50,200 --> 00:19:54,039
new technology that American workers

574
00:19:51,960 --> 00:19:56,240
would be very optimistic but not so

575
00:19:54,039 --> 00:19:57,440
American workers were just to compare

576
00:19:56,240 --> 00:19:59,600
with Germany which is kind of middle of

577
00:19:57,440 --> 00:20:02,000
the pack American workers were 15

578
00:19:59,600 --> 00:20:03,760
percentage points uh more pessimistic

579
00:20:02,000 --> 00:20:06,360
about automation's impact on their job

580
00:20:03,760 --> 00:20:08,280
security and 10 percentage points more

581
00:20:06,360 --> 00:20:10,120
pessimistic about the impact of these

582
00:20:08,280 --> 00:20:11,919
Technologies on their pay really

583
00:20:10,120 --> 00:20:13,880
significant differences and the United

584
00:20:11,919 --> 00:20:15,919
States is very much an outlier in terms

585
00:20:13,880 --> 00:20:17,960
of this pessimism and then you get

586
00:20:15,919 --> 00:20:19,480
places like Poland that are just

587
00:20:17,960 --> 00:20:21,120
overwhelmingly positive workers in

588
00:20:19,480 --> 00:20:23,559
Poland where you know there's a 40

589
00:20:21,120 --> 00:20:25,720
percentage point gap between uh optimism

590
00:20:23,559 --> 00:20:27,919
and pessimism so so what's going on here

591
00:20:25,720 --> 00:20:29,039
we have a few hypothesis we can't say

592
00:20:27,919 --> 00:20:30,320
for sure we actually have a graduate

593
00:20:29,039 --> 00:20:32,000
student looking into this trying to

594
00:20:30,320 --> 00:20:33,960
understand the European and American

595
00:20:32,000 --> 00:20:35,720
differences at a a more granular level

596
00:20:33,960 --> 00:20:38,799
but the the best we can guess is that

597
00:20:35,720 --> 00:20:41,120
the job security uh Factor really is

598
00:20:38,799 --> 00:20:42,919
influenced by institutions in the United

599
00:20:41,120 --> 00:20:44,400
States versus other countries that

600
00:20:42,919 --> 00:20:46,039
because there's not the same social

601
00:20:44,400 --> 00:20:47,880
safety net for people who lose their

602
00:20:46,039 --> 00:20:50,200
jobs in the United States the prospect

603
00:20:47,880 --> 00:20:52,120
of being displaced by a technology might

604
00:20:50,200 --> 00:20:53,360
feel much more real than let's say if

605
00:20:52,120 --> 00:20:54,679
you live in France or Germany where

606
00:20:53,360 --> 00:20:57,240
there are a lot more protections a lot

607
00:20:54,679 --> 00:20:58,919
more job security longer tenure for jobs

608
00:20:57,240 --> 00:21:01,000
so that's one explanation po potentially

609
00:20:58,919 --> 00:21:03,039
for for vulnerability um but one thing

610
00:21:01,000 --> 00:21:04,600
that I I want to underline is that uh of

611
00:21:03,039 --> 00:21:07,240
all the studies we have of the impact of

612
00:21:04,600 --> 00:21:09,159
robots and it on work suggests that

613
00:21:07,240 --> 00:21:10,679
early adopters of those Technologies so

614
00:21:09,159 --> 00:21:12,120
companies that adopt robots they

615
00:21:10,679 --> 00:21:13,360
actually end up hiring more people and

616
00:21:12,120 --> 00:21:14,559
becoming more productive so we don't

617
00:21:13,360 --> 00:21:16,559
really have good evidence that this

618
00:21:14,559 --> 00:21:19,080
displacement would happen at any scale

619
00:21:16,559 --> 00:21:22,000
among companies that um adopt generative

620
00:21:19,080 --> 00:21:24,080
AI okay so the next hypothesis that we

621
00:21:22,000 --> 00:21:27,480
wanted to test here was the workers like

622
00:21:24,080 --> 00:21:29,240
Sam workers in um in positions where

623
00:21:27,480 --> 00:21:31,200
they are doing routine tasks that might

624
00:21:29,240 --> 00:21:33,320
easily be automated we thought that they

625
00:21:31,200 --> 00:21:35,240
would feel more sensitive more

626
00:21:33,320 --> 00:21:37,200
vulnerable to Automation and they might

627
00:21:35,240 --> 00:21:39,039
be more negative about new technologies

628
00:21:37,200 --> 00:21:41,240
so we we asked them so what what types

629
00:21:39,039 --> 00:21:42,520
of tasks do you do at work um kind of

630
00:21:41,240 --> 00:21:44,679
what's your job like what industry are

631
00:21:42,520 --> 00:21:46,120
you in and we tried to piece this apart

632
00:21:44,679 --> 00:21:48,360
and what we found again was was

633
00:21:46,120 --> 00:21:50,159
surprising that we thought that physical

634
00:21:48,360 --> 00:21:52,320
workers so workers in you know let's say

635
00:21:50,159 --> 00:21:53,480
warehouses talked about food processing

636
00:21:52,320 --> 00:21:55,640
before

637
00:21:53,480 --> 00:21:57,480
manufacturing with a high level of

638
00:21:55,640 --> 00:21:58,960
routine tasks we thought that these

639
00:21:57,480 --> 00:22:00,559
would be the most pessimistic workers

640
00:21:58,960 --> 00:22:02,520
toward Automation and technological

641
00:22:00,559 --> 00:22:05,039
change that actually wasn't the key

642
00:22:02,520 --> 00:22:07,320
factor so yes if you have a high share

643
00:22:05,039 --> 00:22:09,919
of routine tasks and you don't do much

644
00:22:07,320 --> 00:22:12,159
problem solving or cognitive work um in

645
00:22:09,919 --> 00:22:14,240
your job you're you're more likely to be

646
00:22:12,159 --> 00:22:16,240
negative right you have a low perception

647
00:22:14,240 --> 00:22:17,880
of its impact on your job security and

648
00:22:16,240 --> 00:22:20,799
an even lower perception of its impact

649
00:22:17,880 --> 00:22:23,360
on your pay but if you had that same set

650
00:22:20,799 --> 00:22:25,240
of routine tasks but you do some problem

651
00:22:23,360 --> 00:22:26,880
solving if your job is designed slightly

652
00:22:25,240 --> 00:22:29,159
differently such that you are in charge

653
00:22:26,880 --> 00:22:30,440
of let's say a cell of robots where you

654
00:22:29,159 --> 00:22:32,440
have to make decisions about how it's

655
00:22:30,440 --> 00:22:34,960
oriented then you actually become the

656
00:22:32,440 --> 00:22:37,080
most optimistic type of worker so just

657
00:22:34,960 --> 00:22:39,080
that change in the kind of the cognitive

658
00:22:37,080 --> 00:22:40,760
side of your job flips your perception

659
00:22:39,080 --> 00:22:43,000
of of technology which we found very

660
00:22:40,760 --> 00:22:44,840
interesting that it's actually the the

661
00:22:43,000 --> 00:22:46,279
level of your cognitive tasks level of

662
00:22:44,840 --> 00:22:47,760
problem solving that you do that's much

663
00:22:46,279 --> 00:22:50,080
more predictive of how you feel about

664
00:22:47,760 --> 00:22:52,799
these Technologies than your level of

665
00:22:50,080 --> 00:22:54,760
routineness okay and then finally you

666
00:22:52,799 --> 00:22:56,440
know and if there are questions I'm

667
00:22:54,760 --> 00:22:58,039
happy to go into this in more detail we

668
00:22:56,440 --> 00:23:00,360
found a few other factors that seem to

669
00:22:58,039 --> 00:23:03,520
be really important and I'll I'll just

670
00:23:00,360 --> 00:23:05,159
mention one and one of the major factors

671
00:23:03,520 --> 00:23:07,919
that comes up over and over again in

672
00:23:05,159 --> 00:23:10,279
studies of human AI interaction is your

673
00:23:07,919 --> 00:23:12,320
propensity to trust that we ask a

674
00:23:10,279 --> 00:23:14,200
battery of questions of people about

675
00:23:12,320 --> 00:23:15,799
their how much they believe others when

676
00:23:14,200 --> 00:23:18,279
they tell them something before it's

677
00:23:15,799 --> 00:23:20,159
proven otherwise how much they naturally

678
00:23:18,279 --> 00:23:21,480
trust people we have a battery of five

679
00:23:20,159 --> 00:23:23,799
questions it's now become standardized

680
00:23:21,480 --> 00:23:25,640
in the research and that set of

681
00:23:23,799 --> 00:23:28,720
questions how you respond to that those

682
00:23:25,640 --> 00:23:30,400
questions is super predictive of how you

683
00:23:28,720 --> 00:23:31,960
think about these Technologies how you

684
00:23:30,400 --> 00:23:33,919
think about Ai and automation impa on

685
00:23:31,960 --> 00:23:35,480
your work the more you trust the more

686
00:23:33,919 --> 00:23:37,039
likely you are to think a very positive

687
00:23:35,480 --> 00:23:39,440
impact of these Technologies and engage

688
00:23:37,039 --> 00:23:41,000
with them perhaps unsurprising another

689
00:23:39,440 --> 00:23:43,600
factor that we found interesting this is

690
00:23:41,000 --> 00:23:45,440
more surprising is that the more exposed

691
00:23:43,600 --> 00:23:47,279
you are to these Technologies at work so

692
00:23:45,440 --> 00:23:49,960
if you've had experience with robots or

693
00:23:47,279 --> 00:23:52,000
AI or generative AI the more optimistic

694
00:23:49,960 --> 00:23:54,520
you are about their impact on your job

695
00:23:52,000 --> 00:23:56,640
security this this might be because

696
00:23:54,520 --> 00:23:58,799
you're uh a worker who's already high

697
00:23:56,640 --> 00:24:00,039
performing and you're not uh not worried

698
00:23:58,799 --> 00:24:01,760
about it or it could be because you've

699
00:24:00,039 --> 00:24:03,039
seen the reality of how hard it is to

700
00:24:01,760 --> 00:24:04,600
implement these technologies that you

701
00:24:03,039 --> 00:24:05,960
really don't think it's they uh your

702
00:24:04,600 --> 00:24:08,760
company's going to be successful at

703
00:24:05,960 --> 00:24:10,919
displacing you so there are a few kind

704
00:24:08,760 --> 00:24:12,279
of takeaways of this for for practice

705
00:24:10,919 --> 00:24:13,840
that I can come back to at the end but I

706
00:24:12,279 --> 00:24:16,039
think that uh really the the main

707
00:24:13,840 --> 00:24:18,520
takeaway I emphasized for this paper is

708
00:24:16,039 --> 00:24:20,600
job design that this difference between

709
00:24:18,520 --> 00:24:22,080
whether you have a routine job with

710
00:24:20,600 --> 00:24:24,039
without much cognitive work or whether

711
00:24:22,080 --> 00:24:25,440
you have some cognitive work in your job

712
00:24:24,039 --> 00:24:27,080
can really make a difference of how

713
00:24:25,440 --> 00:24:29,000
individuals interact with this

714
00:24:27,080 --> 00:24:31,520
technology okay

715
00:24:29,000 --> 00:24:33,360
so the third third big Point that's come

716
00:24:31,520 --> 00:24:34,960
out of our research is about the

717
00:24:33,360 --> 00:24:37,640
trade-offs with generative AI I'm not

718
00:24:34,960 --> 00:24:40,679
sure if the the free lunch phrase is is

719
00:24:37,640 --> 00:24:42,240
uh kind of too slang but the the phrase

720
00:24:40,679 --> 00:24:43,720
in economics is there's no such thing as

721
00:24:42,240 --> 00:24:45,640
a free lunch and that's one big

722
00:24:43,720 --> 00:24:48,000
explanation of of tradeoffs when it

723
00:24:45,640 --> 00:24:49,559
comes to economic activity and with

724
00:24:48,000 --> 00:24:51,200
generative AI I feel like the early

725
00:24:49,559 --> 00:24:52,640
discussions have been that there's going

726
00:24:51,200 --> 00:24:55,200
to be this productivity boom it's going

727
00:24:52,640 --> 00:24:58,399
to be wonderful and workers can benefit

728
00:24:55,200 --> 00:24:59,679
too because they um they they might be

729
00:24:58,399 --> 00:25:01,600
able to move into higher wage jobs or

730
00:24:59,679 --> 00:25:03,600
more jobs would be created but the

731
00:25:01,600 --> 00:25:05,000
question that I ask immediately is well

732
00:25:03,600 --> 00:25:07,240
what are the tradeoffs what are we

733
00:25:05,000 --> 00:25:09,240
sacrificing here by uh adopting this

734
00:25:07,240 --> 00:25:10,799
technology so we found a few things and

735
00:25:09,240 --> 00:25:12,799
I I think this paper sums it up really

736
00:25:10,799 --> 00:25:15,039
well another graduate student paper

737
00:25:12,799 --> 00:25:16,720
there's I I would say um some of the

738
00:25:15,039 --> 00:25:18,640
most interesting work in in this field

739
00:25:16,720 --> 00:25:19,880
is coming out of young graduate students

740
00:25:18,640 --> 00:25:21,600
with creative approaches to these

741
00:25:19,880 --> 00:25:24,559
problems and this is from a graduate

742
00:25:21,600 --> 00:25:27,520
student named a aen Tona Rogers who

743
00:25:24,559 --> 00:25:30,679
worked with a materials R&D company and

744
00:25:27,520 --> 00:25:33,240
he studied how this company that the job

745
00:25:30,679 --> 00:25:35,520
was to discover new materials that were

746
00:25:33,240 --> 00:25:37,880
commercializable how their introduction

747
00:25:35,520 --> 00:25:40,720
of an AI tool to help them discover

748
00:25:37,880 --> 00:25:42,679
these materials would affect their um

749
00:25:40,720 --> 00:25:45,520
their productivity and ultimately how

750
00:25:42,679 --> 00:25:48,000
their workers uh went about their R&D

751
00:25:45,520 --> 00:25:50,600
now mind you this is a very different

752
00:25:48,000 --> 00:25:52,279
case than Sam right so Sam is considered

753
00:25:50,600 --> 00:25:54,880
you a lower wage worker a Frontline

754
00:25:52,279 --> 00:25:57,480
worker these are PhD R&D scientists some

755
00:25:54,880 --> 00:26:00,159
of you might have a similar job and what

756
00:25:57,480 --> 00:26:02,840
happened and focus on the the lower left

757
00:26:00,159 --> 00:26:04,919
here was he measured productivity in

758
00:26:02,840 --> 00:26:06,760
three ways how many new materials did

759
00:26:04,919 --> 00:26:09,039
they discover once this tool was

760
00:26:06,760 --> 00:26:11,320
introduced how many patents did those

761
00:26:09,039 --> 00:26:13,200
materials turn into and then finally and

762
00:26:11,320 --> 00:26:14,760
I think this is the key one how many new

763
00:26:13,200 --> 00:26:17,440
products actually came out of it because

764
00:26:14,760 --> 00:26:19,000
patents you know not not not super

765
00:26:17,440 --> 00:26:20,240
valuable always for the company in the

766
00:26:19,000 --> 00:26:22,320
short term really what they wanted were

767
00:26:20,240 --> 00:26:24,880
commercializable products and what they

768
00:26:22,320 --> 00:26:27,679
found is this huge increase in new

769
00:26:24,880 --> 00:26:29,279
materials more the 50% growth but a a

770
00:26:27,679 --> 00:26:31,320
much small but still significant

771
00:26:29,279 --> 00:26:33,399
increase in product prototypes so what

772
00:26:31,320 --> 00:26:35,360
you can imagine here is they're finding

773
00:26:33,399 --> 00:26:37,399
a lot of new materials and then it's the

774
00:26:35,360 --> 00:26:40,559
job of the people to winnow them down

775
00:26:37,399 --> 00:26:42,600
into product prototypes and it this

776
00:26:40,559 --> 00:26:44,159
assumption that he had that this work is

777
00:26:42,600 --> 00:26:46,480
really going to change as a as a

778
00:26:44,159 --> 00:26:48,360
function of this AI being introduced was

779
00:26:46,480 --> 00:26:50,440
validated when he asked people much like

780
00:26:48,360 --> 00:26:52,200
we did in our survey how what were what

781
00:26:50,440 --> 00:26:54,279
kind of tasks did you do and how did you

782
00:26:52,200 --> 00:26:56,880
allocate your work between these tasks

783
00:26:54,279 --> 00:26:59,720
he found that um in terms of idea

784
00:26:56,880 --> 00:27:01,120
generation the workers Prett previously

785
00:26:59,720 --> 00:27:03,000
had to generate a lot of ideas that was

786
00:27:01,120 --> 00:27:06,039
a fundamental J part of their job as an

787
00:27:03,000 --> 00:27:08,000
R&D scientists but now that the AI was

788
00:27:06,039 --> 00:27:09,799
suggesting new materials they spent a

789
00:27:08,000 --> 00:27:12,520
lot less of their time on idea

790
00:27:09,799 --> 00:27:15,039
generation much more of their time on

791
00:27:12,520 --> 00:27:17,520
exercising judgment is the material that

792
00:27:15,039 --> 00:27:20,720
this algorithm suggesting is it actually

793
00:27:17,520 --> 00:27:21,679
worthwhile now that that might be okay

794
00:27:20,720 --> 00:27:22,520
that they're spending more time on

795
00:27:21,679 --> 00:27:25,240
judgment it's contributing to

796
00:27:22,520 --> 00:27:27,799
productivity maybe that's an okay shift

797
00:27:25,240 --> 00:27:29,720
in their jobs but when he ask them how

798
00:27:27,799 --> 00:27:31,480
did you like this shift in your job how

799
00:27:29,720 --> 00:27:33,640
do you like your new job they said it

800
00:27:31,480 --> 00:27:35,960
was horrible they hated it so you look

801
00:27:33,640 --> 00:27:38,720
at this really extraordinary CH it's

802
00:27:35,960 --> 00:27:40,480
change and satisfaction is consistently

803
00:27:38,720 --> 00:27:42,519
lower they they they really don't like

804
00:27:40,480 --> 00:27:45,320
their new task allocation so this is one

805
00:27:42,519 --> 00:27:47,159
of the stories where there's a clear and

806
00:27:45,320 --> 00:27:48,600
significant productivity boost for this

807
00:27:47,159 --> 00:27:51,320
company as a result of introducing the

808
00:27:48,600 --> 00:27:53,360
tool it's in an area that could be great

809
00:27:51,320 --> 00:27:55,120
for society discovering new materials

810
00:27:53,360 --> 00:27:56,360
perhaps for clean energy or for

811
00:27:55,120 --> 00:27:58,360
improvements in manufacturing

812
00:27:56,360 --> 00:27:59,919
productivity whatever it is but there's

813
00:27:58,360 --> 00:28:01,960
a clear trade-off that the workers

814
00:27:59,919 --> 00:28:03,240
themselves aren't really happy with it

815
00:28:01,960 --> 00:28:05,000
and I know a lot of you from your

816
00:28:03,240 --> 00:28:06,159
experiences could probably imagine that

817
00:28:05,000 --> 00:28:07,519
this technology is going to have

818
00:28:06,159 --> 00:28:09,480
difficulty scaling if this is the

819
00:28:07,519 --> 00:28:11,200
reality that workers particularly

820
00:28:09,480 --> 00:28:12,640
valuable workers for the company aren't

821
00:28:11,200 --> 00:28:17,240
on board with it and aren't championing

822
00:28:12,640 --> 00:28:19,760
it in their own domains okay so finally

823
00:28:17,240 --> 00:28:21,200
you know I I I want to illustrate some

824
00:28:19,760 --> 00:28:23,039
from our experience one of the things we

825
00:28:21,200 --> 00:28:24,720
we found early on when we were talking

826
00:28:23,039 --> 00:28:26,480
to companies about their experiments

827
00:28:24,720 --> 00:28:28,159
with generative AI we would ask them

828
00:28:26,480 --> 00:28:29,640
well how are you measuring success can

829
00:28:28,159 --> 00:28:31,440
what does success look like can you hand

830
00:28:29,640 --> 00:28:33,000
can you give us some of your indicators

831
00:28:31,440 --> 00:28:35,120
and over and over again these companies

832
00:28:33,000 --> 00:28:37,240
would say we have this many active users

833
00:28:35,120 --> 00:28:38,880
we have this many pilot cases and that

834
00:28:37,240 --> 00:28:40,519
would be their measure of success we

835
00:28:38,880 --> 00:28:43,000
have 10,000 people on our platform we

836
00:28:40,519 --> 00:28:44,760
have this many active users and it was

837
00:28:43,000 --> 00:28:46,840
so so dissatisfying because I wanted to

838
00:28:44,760 --> 00:28:49,240
know well how is that really helping you

839
00:28:46,840 --> 00:28:52,360
how is that helping you do the job that

840
00:28:49,240 --> 00:28:56,679
your organization is is designed to do

841
00:28:52,360 --> 00:28:58,640
so over time we we wanted to get uh a

842
00:28:56,679 --> 00:29:00,679
better sense of this like what what

843
00:28:58,640 --> 00:29:02,159
could actually look like in practice and

844
00:29:00,679 --> 00:29:05,519
and what we found is that there really

845
00:29:02,159 --> 00:29:07,640
limits to this uh I would say sole focus

846
00:29:05,519 --> 00:29:10,519
on on productivity use cases or doing a

847
00:29:07,640 --> 00:29:11,760
task faster so I want to you know

848
00:29:10,519 --> 00:29:13,120
highlight there there are a few papers

849
00:29:11,760 --> 00:29:15,159
that speak to this but one that's that's

850
00:29:13,120 --> 00:29:18,240
brand new that I find interesting is

851
00:29:15,159 --> 00:29:20,320
this study of of medical doctors so they

852
00:29:18,240 --> 00:29:22,840
gave in the study medical doctors a

853
00:29:20,320 --> 00:29:25,320
series of cases uh that they had to

854
00:29:22,840 --> 00:29:27,640
analyze and then they had some of the

855
00:29:25,320 --> 00:29:29,559
doctors had the benefits of an AI

856
00:29:27,640 --> 00:29:31,880
assistant to analyze the cases some

857
00:29:29,559 --> 00:29:34,799
didn't and then they had their answers

858
00:29:31,880 --> 00:29:36,600
were evaluated for their quality and

859
00:29:34,799 --> 00:29:38,960
they also measured the time it took for

860
00:29:36,600 --> 00:29:42,320
doctors to analyze and perhaps

861
00:29:38,960 --> 00:29:43,720
unsurprising to you now the um the AI

862
00:29:42,320 --> 00:29:46,120
assisted doctors actually did much

863
00:29:43,720 --> 00:29:47,960
better on their diagnostic judgment they

864
00:29:46,120 --> 00:29:50,760
did much better in analyzing the cases

865
00:29:47,960 --> 00:29:53,880
overall but surprisingly it took them

866
00:29:50,760 --> 00:29:55,960
longer it took the AI assisted doctors

867
00:29:53,880 --> 00:29:57,880
longer to analyze the case perhaps

868
00:29:55,960 --> 00:29:59,200
because the AI helped them question

869
00:29:57,880 --> 00:30:01,399
their their initial judgment and their

870
00:29:59,200 --> 00:30:03,720
gut reaction and when they deliberated

871
00:30:01,399 --> 00:30:07,240
more they actually were able to provide

872
00:30:03,720 --> 00:30:08,760
a much higher quality um analysis now

873
00:30:07,240 --> 00:30:10,519
here if you were analyzing this in in

874
00:30:08,760 --> 00:30:12,600
productivity terms this would be a

875
00:30:10,519 --> 00:30:14,000
failure of generative Ai and a failure

876
00:30:12,600 --> 00:30:15,840
of the process but really when it comes

877
00:30:14,000 --> 00:30:17,039
to Medicine I think that's the not not

878
00:30:15,840 --> 00:30:19,399
necessar what you're looking for the key

879
00:30:17,039 --> 00:30:21,519
indicator wouldn't be just productivity

880
00:30:19,399 --> 00:30:24,640
and speed you would actually want

881
00:30:21,519 --> 00:30:27,240
quality over speed because the the error

882
00:30:24,640 --> 00:30:28,760
the consequences of an error is is so

883
00:30:27,240 --> 00:30:32,399
much higher than the consequences of

884
00:30:28,760 --> 00:30:34,120
moving a little bit slowly okay finally

885
00:30:32,399 --> 00:30:36,399
you know we talked a bit about trust and

886
00:30:34,120 --> 00:30:38,960
I think trust in these systems is really

887
00:30:36,399 --> 00:30:40,919
important and again one of the things we

888
00:30:38,960 --> 00:30:42,159
hear from the organizations we work with

889
00:30:40,919 --> 00:30:44,519
is particularly when we've been studying

890
00:30:42,159 --> 00:30:46,399
workers how do I get work our workers to

891
00:30:44,519 --> 00:30:49,080
adopt this technology how do I get

892
00:30:46,399 --> 00:30:50,919
Workers to trust it and I think that's

893
00:30:49,080 --> 00:30:52,880
the wrong question because that they're

894
00:30:50,919 --> 00:30:56,240
really looking at how do you

895
00:30:52,880 --> 00:30:58,240
optimize for uh how do you optimize for

896
00:30:56,240 --> 00:30:59,799
user adoption that you want work to

897
00:30:58,240 --> 00:31:01,519
trust the tool you want workers to use

898
00:30:59,799 --> 00:31:03,080
it wherever they can because you think

899
00:31:01,519 --> 00:31:05,320
this tool is going to translate into

900
00:31:03,080 --> 00:31:07,840
improved performance productivity Etc

901
00:31:05,320 --> 00:31:10,360
but really and what we've learned from

902
00:31:07,840 --> 00:31:12,840
Aerospace and other examples is that you

903
00:31:10,360 --> 00:31:14,399
need not just a blind Trust of these

904
00:31:12,840 --> 00:31:16,840
tools you need to trust them when

905
00:31:14,399 --> 00:31:18,240
they're appropriate now one the cases

906
00:31:16,840 --> 00:31:21,240
that Eric brought up earlier which I

907
00:31:18,240 --> 00:31:24,000
really like is this case is from a paper

908
00:31:21,240 --> 00:31:26,399
U called the jagged Frontier and what

909
00:31:24,000 --> 00:31:28,000
the authors mean by the jagged Frontier

910
00:31:26,399 --> 00:31:30,559
is that there are this these use cas

911
00:31:28,000 --> 00:31:32,440
cases that generative AI can do well and

912
00:31:30,559 --> 00:31:33,919
there are use cases that these tools

913
00:31:32,440 --> 00:31:36,240
can't do well at all and they might be

914
00:31:33,919 --> 00:31:37,880
misleading so this is a case of uh boss

915
00:31:36,240 --> 00:31:41,360
and Consulting Group Consultants given

916
00:31:37,880 --> 00:31:44,559
two tasks one both both are typical

917
00:31:41,360 --> 00:31:46,440
Consulting Group tasks and one the AI

918
00:31:44,559 --> 00:31:48,960
can outperform humans and then the other

919
00:31:46,440 --> 00:31:51,679
the AI can't and when they tried to

920
00:31:48,960 --> 00:31:54,080
understand why did the AI assisted uh

921
00:31:51,679 --> 00:31:55,880
Consultants fail in this task they

922
00:31:54,080 --> 00:31:58,159
considered outside the frontier of what

923
00:31:55,880 --> 00:32:00,279
AI can do they went in and they looked

924
00:31:58,159 --> 00:32:02,360
at the chat logs of how these

925
00:32:00,279 --> 00:32:04,240
Consultants interacted with the the

926
00:32:02,360 --> 00:32:05,480
algorithm and they said well why would

927
00:32:04,240 --> 00:32:07,159
they why were they led to the wrong

928
00:32:05,480 --> 00:32:09,240
answer like was it the AI telling them

929
00:32:07,159 --> 00:32:11,519
what was wrong and it wasn't actually

930
00:32:09,240 --> 00:32:13,399
just that the the chatbot was telling

931
00:32:11,519 --> 00:32:15,760
them that the the wrong answer it was

932
00:32:13,399 --> 00:32:17,960
when they pushed back that chapot was so

933
00:32:15,760 --> 00:32:19,799
persuasive and convincing them for the

934
00:32:17,960 --> 00:32:23,399
on behalf of the wrong answer that even

935
00:32:19,799 --> 00:32:25,480
these highly trained Consultants were uh

936
00:32:23,399 --> 00:32:27,279
were convinced of to to move in the

937
00:32:25,480 --> 00:32:30,200
wrong direction so this is kind of the

938
00:32:27,279 --> 00:32:31,799
key uh I think lesson is is trust but

939
00:32:30,200 --> 00:32:33,559
verify that you might want to use these

940
00:32:31,799 --> 00:32:35,360
tools but maintain your skepticism of

941
00:32:33,559 --> 00:32:37,320
them enough that you calibrate your

942
00:32:35,360 --> 00:32:38,799
trust appropriately to what these tools

943
00:32:37,320 --> 00:32:41,960
can actually be really good

944
00:32:38,799 --> 00:32:43,360
at and and finally you know getting into

945
00:32:41,960 --> 00:32:44,720
some of your questions and I I really

946
00:32:43,360 --> 00:32:46,279
loved reading some of the questions from

947
00:32:44,720 --> 00:32:48,799
the other session so if your question

948
00:32:46,279 --> 00:32:50,399
wasn't answered please resubmit it um

949
00:32:48,799 --> 00:32:53,440
but one of the things that that we've

950
00:32:50,399 --> 00:32:55,440
learned you know now more than a year in

951
00:32:53,440 --> 00:32:57,320
is that there are a lot of these really

952
00:32:55,440 --> 00:32:58,679
interesting unanswered questions that

953
00:32:57,320 --> 00:33:00,559
are going to motivate our research but

954
00:32:58,679 --> 00:33:02,200
we're also interested to hear what

955
00:33:00,559 --> 00:33:04,399
industry is is thinking about with them

956
00:33:02,200 --> 00:33:05,480
so one of the big questions we wanted to

957
00:33:04,399 --> 00:33:07,600
know and I think a lot of folks want to

958
00:33:05,480 --> 00:33:09,000
know is that who is who are these tools

959
00:33:07,600 --> 00:33:11,159
most useful for who should we be

960
00:33:09,000 --> 00:33:12,639
training assuming we can't focus on and

961
00:33:11,159 --> 00:33:14,880
everyone adopting them are they more

962
00:33:12,639 --> 00:33:16,519
useful for novices or experts initially

963
00:33:14,880 --> 00:33:18,159
we thought they might be more useful for

964
00:33:16,519 --> 00:33:20,000
the lowest skilled individuals but in

965
00:33:18,159 --> 00:33:21,720
software engineering in medicine and

966
00:33:20,000 --> 00:33:23,600
other contexts it seems that the skill

967
00:33:21,720 --> 00:33:25,960
of being able to interpret the results

968
00:33:23,600 --> 00:33:28,080
that you're being given that that uh

969
00:33:25,960 --> 00:33:29,679
that skill which requires expertise

970
00:33:28,080 --> 00:33:31,639
which requires experience is actually

971
00:33:29,679 --> 00:33:33,760
really critical so this there's no clear

972
00:33:31,639 --> 00:33:36,360
answer to number one yet um does

973
00:33:33,760 --> 00:33:37,360
training on generative AI help um I've

974
00:33:36,360 --> 00:33:39,279
been shocked that a lot of the

975
00:33:37,360 --> 00:33:40,880
organizations that are deploying this

976
00:33:39,279 --> 00:33:42,399
these Technologies at scale don't

977
00:33:40,880 --> 00:33:45,200
provide a lot of training at all and how

978
00:33:42,399 --> 00:33:46,600
to use them and you know in one large

979
00:33:45,200 --> 00:33:47,919
insurance company we study they have a

980
00:33:46,600 --> 00:33:49,440
seven minute training before their

981
00:33:47,919 --> 00:33:51,159
employees can get on the platform and

982
00:33:49,440 --> 00:33:53,799
start using it for for whatever they'

983
00:33:51,159 --> 00:33:55,240
like so my assumption has been that

984
00:33:53,799 --> 00:33:56,760
training on generative AI might not be

985
00:33:55,240 --> 00:33:59,320
that useful and and you saw from the

986
00:33:56,760 --> 00:34:01,279
previous study that Eric spoke to as

987
00:33:59,320 --> 00:34:02,480
well that workers with generative AI

988
00:34:01,279 --> 00:34:04,320
weren't actually more like that much

989
00:34:02,480 --> 00:34:05,760
more likely to answer correctly and when

990
00:34:04,320 --> 00:34:06,840
they were steered in the wrong direction

991
00:34:05,760 --> 00:34:08,839
they're actually more likely to get the

992
00:34:06,840 --> 00:34:10,919
answer wrong so this is I think an open

993
00:34:08,839 --> 00:34:13,000
question does training help or what

994
00:34:10,919 --> 00:34:14,760
training helps might be more appropriate

995
00:34:13,000 --> 00:34:16,440
and then finally and this the panel

996
00:34:14,760 --> 00:34:18,159
discussed this is like we don't actually

997
00:34:16,440 --> 00:34:19,839
know what the dominant models will be

998
00:34:18,159 --> 00:34:21,399
the models are changing and what they're

999
00:34:19,839 --> 00:34:23,679
good at changing so this idea of the

1000
00:34:21,399 --> 00:34:25,200
frontier use cases are actually shifting

1001
00:34:23,679 --> 00:34:26,679
in ways that might be unsettling to

1002
00:34:25,200 --> 00:34:28,320
companies that just want to lock in on a

1003
00:34:26,679 --> 00:34:30,520
model and have an application that they

1004
00:34:28,320 --> 00:34:33,440
know can work um so that I feel like is

1005
00:34:30,520 --> 00:34:35,520
a um I hear a lot of users of these

1006
00:34:33,440 --> 00:34:38,240
tools say just I just want to I just

1007
00:34:35,520 --> 00:34:39,760
want a dominant design and I want um I

1008
00:34:38,240 --> 00:34:41,720
want to know that it can work reliably

1009
00:34:39,760 --> 00:34:44,399
instead of the it it changing under my

1010
00:34:41,720 --> 00:34:46,800
feet and then finally I think that this

1011
00:34:44,399 --> 00:34:48,399
is a a key for for us as a university

1012
00:34:46,800 --> 00:34:52,119
and also for the education system in

1013
00:34:48,399 --> 00:34:55,200
general is you know do the skills that

1014
00:34:52,119 --> 00:34:57,839
we provide need to shift to support

1015
00:34:55,200 --> 00:35:00,800
people to thrive in a an augmented envir

1016
00:34:57,839 --> 00:35:02,920
enironment and my initial assumption is

1017
00:35:00,800 --> 00:35:04,240
is not really that this the same skills

1018
00:35:02,920 --> 00:35:05,800
that help people thrive in today's

1019
00:35:04,240 --> 00:35:08,240
Workforce are going to be by and larg

1020
00:35:05,800 --> 00:35:10,520
the same skills that uh help people

1021
00:35:08,240 --> 00:35:12,480
survive and and thrive in a a Workforce

1022
00:35:10,520 --> 00:35:14,359
that's augmented by AI because you still

1023
00:35:12,480 --> 00:35:17,280
need to understand the underlying

1024
00:35:14,359 --> 00:35:18,920
processes the underlying Technologies um

1025
00:35:17,280 --> 00:35:21,720
that this that these tools are being

1026
00:35:18,920 --> 00:35:24,720
plugged into in order to use them

1027
00:35:21,720 --> 00:35:26,119
successfully okay so I'm going to uh

1028
00:35:24,720 --> 00:35:27,400
given we don't have a ton of time left

1029
00:35:26,119 --> 00:35:28,599
I'm going to pause here and if you're

1030
00:35:27,400 --> 00:35:30,160
interested interested in cockpit

1031
00:35:28,599 --> 00:35:31,960
automation I'm happy to talk about this

1032
00:35:30,160 --> 00:35:33,960
more but I'm going to turn to the the

1033
00:35:31,960 --> 00:35:36,280
questions um starting now and I'll be

1034
00:35:33,960 --> 00:35:39,040
able to draw in some of the other

1035
00:35:36,280 --> 00:35:39,800
material so if we could now I'm

1036
00:35:39,040 --> 00:35:41,680
understanding what the other speakers

1037
00:35:39,800 --> 00:35:43,400
were saying about the clock um if we

1038
00:35:41,680 --> 00:35:45,880
could throw up the

1039
00:35:43,400 --> 00:35:47,119
questions did the St oh I see one did

1040
00:35:45,880 --> 00:35:48,520
the study design include workers that

1041
00:35:47,119 --> 00:35:50,800
have been laid off due to automation or

1042
00:35:48,520 --> 00:35:52,400
just the ones left on the job so as

1043
00:35:50,800 --> 00:35:54,440
someone who studies the prospect of

1044
00:35:52,400 --> 00:35:56,160
workers being laid off due to automation

1045
00:35:54,440 --> 00:35:59,480
there aren't that many clear cases that

1046
00:35:56,160 --> 00:36:01,680
you can point to um long shoreman is the

1047
00:35:59,480 --> 00:36:04,680
classic case Port workers in the United

1048
00:36:01,680 --> 00:36:06,760
States are workers that in the 1960s

1049
00:36:04,680 --> 00:36:09,040
there was automation introduced um at

1050
00:36:06,760 --> 00:36:10,200
West Coast ports and there was a huge

1051
00:36:09,040 --> 00:36:12,040
reduction in the workforce due to

1052
00:36:10,200 --> 00:36:13,720
attrition and also just because they

1053
00:36:12,040 --> 00:36:15,720
became far more productive what the

1054
00:36:13,720 --> 00:36:18,000
Union in that case did was they

1055
00:36:15,720 --> 00:36:20,280
negotiated that the benefits of

1056
00:36:18,000 --> 00:36:22,200
automation the new revenues that came in

1057
00:36:20,280 --> 00:36:23,440
would be shared with the workers so

1058
00:36:22,200 --> 00:36:25,319
these workers got laid off but they

1059
00:36:23,440 --> 00:36:28,000
actually got paid really well um as they

1060
00:36:25,319 --> 00:36:30,440
found other jobs so the the workers did

1061
00:36:28,000 --> 00:36:32,560
the study design included workers you

1062
00:36:30,440 --> 00:36:33,440
know people in the workforce um but I

1063
00:36:32,560 --> 00:36:35,160
actually don't think it would be that

1064
00:36:33,440 --> 00:36:38,560
much different if we included people who

1065
00:36:35,160 --> 00:36:41,839
identified as being displaced by

1066
00:36:38,560 --> 00:36:41,839
automation other

1067
00:36:45,920 --> 00:36:50,280
questions practical approaches to

1068
00:36:47,839 --> 00:36:52,119
training individuals um enhances their

1069
00:36:50,280 --> 00:36:53,599
performance rather than simply relying

1070
00:36:52,119 --> 00:36:57,680
on AI as a

1071
00:36:53,599 --> 00:36:59,920
tool I think so so I've seen a few

1072
00:36:57,680 --> 00:37:01,880
approaches so I I don't um again the

1073
00:36:59,920 --> 00:37:04,359
training point I haven't seen a lot of

1074
00:37:01,880 --> 00:37:05,839
good models of of training so we we have

1075
00:37:04,359 --> 00:37:08,160
a partnership with Google there the

1076
00:37:05,839 --> 00:37:10,000
google.org funded our uh has sponsored

1077
00:37:08,160 --> 00:37:12,280
our our working group and they have this

1078
00:37:10,000 --> 00:37:14,040
interesting approach where they have a a

1079
00:37:12,280 --> 00:37:15,800
series of online courses some are the

1080
00:37:14,040 --> 00:37:17,040
more advanced in data science and

1081
00:37:15,800 --> 00:37:18,760
they're going to do another more

1082
00:37:17,040 --> 00:37:20,280
advanced course in in AI related

1083
00:37:18,760 --> 00:37:22,240
Technologies then they have this AI

1084
00:37:20,280 --> 00:37:24,040
Essentials course which is really to

1085
00:37:22,240 --> 00:37:25,520
understand I think Eric actually could

1086
00:37:24,040 --> 00:37:27,960
have taught the a Essentials course with

1087
00:37:25,520 --> 00:37:30,880
his um lecture earlier but it's it's a

1088
00:37:27,960 --> 00:37:32,440
series of uh interactive steps to get

1089
00:37:30,880 --> 00:37:34,000
the the basics of what these tools can

1090
00:37:32,440 --> 00:37:35,880
do where you should be skeptical of them

1091
00:37:34,000 --> 00:37:37,040
and to help calibrate that trust which I

1092
00:37:35,880 --> 00:37:39,359
think Could Be an Effective training

1093
00:37:37,040 --> 00:37:41,960
tool but it's not really AI skills like

1094
00:37:39,359 --> 00:37:44,319
no one's going to be able to um you know

1095
00:37:41,960 --> 00:37:45,720
work with uh work with open ai's API

1096
00:37:44,319 --> 00:37:49,000
after that no one's going to be able to

1097
00:37:45,720 --> 00:37:50,359
help develop or or you know um you know

1098
00:37:49,000 --> 00:37:53,400
fiddle fiddle with the weights in a

1099
00:37:50,359 --> 00:37:55,640
model so it's not um it's not Advanced

1100
00:37:53,400 --> 00:37:57,200
but it does seem to be useful and again

1101
00:37:55,640 --> 00:37:58,760
I've been struck by how little training

1102
00:37:57,200 --> 00:38:00,359
or organizations even organizations

1103
00:37:58,760 --> 00:38:02,440
using these tools successfully have done

1104
00:38:00,359 --> 00:38:04,640
for their

1105
00:38:02,440 --> 00:38:07,520
workers are these studies inclusive of

1106
00:38:04,640 --> 00:38:09,119
agentic tools uh no partly because none

1107
00:38:07,520 --> 00:38:10,680
of the companies have really used those

1108
00:38:09,119 --> 00:38:13,720
tools successfully yet that we've been

1109
00:38:10,680 --> 00:38:15,480
studying um I think the the maybe you

1110
00:38:13,720 --> 00:38:17,920
could feel this as an undercurrent you

1111
00:38:15,480 --> 00:38:20,079
know we're studying some very Advanced

1112
00:38:17,920 --> 00:38:21,640
companies uh technologically we're

1113
00:38:20,079 --> 00:38:24,400
studying some very big you know kind of

1114
00:38:21,640 --> 00:38:26,560
Fortune 100 type companies and it's it's

1115
00:38:24,400 --> 00:38:29,040
not had these tools have not reached

1116
00:38:26,560 --> 00:38:30,720
significant scale in these companies um

1117
00:38:29,040 --> 00:38:33,079
to have much of an impact yet even even

1118
00:38:30,720 --> 00:38:35,400
a year on so the what we're seeing on

1119
00:38:33,079 --> 00:38:37,640
the ground is is hugely different than

1120
00:38:35,400 --> 00:38:40,119
the hype that we're seeing around kind

1121
00:38:37,640 --> 00:38:42,760
of the the breathless uh discussions of

1122
00:38:40,119 --> 00:38:45,960
agentic tools and AGI it's just not the

1123
00:38:42,760 --> 00:38:45,960
reality for the companies that we've

1124
00:38:50,319 --> 00:38:57,200
studied I feel a bit ashamed we had so

1125
00:38:52,800 --> 00:38:57,200
many questions for the earlier folks

1126
00:38:59,359 --> 00:39:02,400
how would the AI Evolution reshape the

1127
00:39:01,000 --> 00:39:03,520
education landscape what would it look

1128
00:39:02,400 --> 00:39:05,119
like and consequently how would this

1129
00:39:03,520 --> 00:39:08,880
impact on

1130
00:39:05,119 --> 00:39:10,599
employment so I I I feel like I'm going

1131
00:39:08,880 --> 00:39:12,520
to be hesitant to speculate but I can

1132
00:39:10,599 --> 00:39:14,240
talk about what I've heard um some

1133
00:39:12,520 --> 00:39:16,079
colleagues discuss and and I know you

1134
00:39:14,240 --> 00:39:18,119
know Eric might have other uh

1135
00:39:16,079 --> 00:39:19,079
perspectives he works with folks in

1136
00:39:18,119 --> 00:39:21,400
computer science and artificial

1137
00:39:19,079 --> 00:39:24,240
intelligence laboratory I think the

1138
00:39:21,400 --> 00:39:26,359
question is really acute in uh computer

1139
00:39:24,240 --> 00:39:28,359
science Fields where I think there are

1140
00:39:26,359 --> 00:39:30,319
early indications that what it means to

1141
00:39:28,359 --> 00:39:32,319
be an entry-level programmer in a lot of

1142
00:39:30,319 --> 00:39:35,200
large software organizations is very

1143
00:39:32,319 --> 00:39:38,760
different so traditionally when a school

1144
00:39:35,200 --> 00:39:40,960
like MIT would train a software engineer

1145
00:39:38,760 --> 00:39:42,920
to not only take an entry-level job but

1146
00:39:40,960 --> 00:39:44,240
to move up to a leadership role where

1147
00:39:42,920 --> 00:39:46,200
you really had to understand the

1148
00:39:44,240 --> 00:39:47,800
architecture of a system you had to make

1149
00:39:46,200 --> 00:39:49,119
leadership decisions you had to

1150
00:39:47,800 --> 00:39:51,319
understand kind of the overarching

1151
00:39:49,119 --> 00:39:54,319
technical landscape now a software

1152
00:39:51,319 --> 00:39:57,000
engineer might be lower paid and also

1153
00:39:54,319 --> 00:39:59,440
require less skill so the question I

1154
00:39:57,000 --> 00:40:01,400
think from schools like mits do we

1155
00:39:59,440 --> 00:40:02,920
really still want to uh educate our

1156
00:40:01,400 --> 00:40:04,960
students to go into that entry level

1157
00:40:02,920 --> 00:40:06,839
role or are we going to educate them for

1158
00:40:04,960 --> 00:40:08,520
for leadership and understand that that

1159
00:40:06,839 --> 00:40:10,640
entry level role might change but still

1160
00:40:08,520 --> 00:40:13,079
be a step along the way so I think

1161
00:40:10,640 --> 00:40:14,920
that's an an ongoing conversation that

1162
00:40:13,079 --> 00:40:16,680
as entry level jobs change or at the

1163
00:40:14,920 --> 00:40:19,000
skills required to do these jobs change

1164
00:40:16,680 --> 00:40:20,160
the educa the the curriculum to train

1165
00:40:19,000 --> 00:40:23,920
students to go into these entry level

1166
00:40:20,160 --> 00:40:23,920
roles will also have to change

1167
00:40:30,040 --> 00:40:35,880
okay in the absence of of more questions

1168
00:40:33,480 --> 00:40:39,319
I'm going to go into just a a final

1169
00:40:35,880 --> 00:40:41,520
let's say Parable to to leave you with

1170
00:40:39,319 --> 00:40:43,280
so I'm going to only talk about one of

1171
00:40:41,520 --> 00:40:45,079
these but on on the left you know I'm

1172
00:40:43,280 --> 00:40:47,160
from Chicago oh they didn't even pull it

1173
00:40:45,079 --> 00:40:48,599
up okay there we go you know I'm from

1174
00:40:47,160 --> 00:40:50,839
Chicago and and one of the things that

1175
00:40:48,599 --> 00:40:52,280
Chicago is known for is a particular

1176
00:40:50,839 --> 00:40:53,599
type particular hot dog company I don't

1177
00:40:52,280 --> 00:40:54,760
know if any of you have been to Chicago

1178
00:40:53,599 --> 00:40:58,119
it's a wonderful place I would really

1179
00:40:54,760 --> 00:41:00,000
recommend it um and and we these uh

1180
00:40:58,119 --> 00:41:01,839
Vienna beef hot dogs that are are known

1181
00:41:00,000 --> 00:41:03,119
for their red color I don't know why

1182
00:41:01,839 --> 00:41:04,319
they're called Vienna Beef you'd think

1183
00:41:03,119 --> 00:41:06,599
that that would come from Vienna and not

1184
00:41:04,319 --> 00:41:08,400
Chicago but nonetheless we we we known

1185
00:41:06,599 --> 00:41:09,839
for producing this hot dog uh and given

1186
00:41:08,400 --> 00:41:12,319
that you talked about food processing

1187
00:41:09,839 --> 00:41:14,560
before um the factory that produced this

1188
00:41:12,319 --> 00:41:16,520
hot dog for the longest time um was in

1189
00:41:14,560 --> 00:41:19,200
the city and it was one of those older

1190
00:41:16,520 --> 00:41:21,040
factories where the equipment was from

1191
00:41:19,200 --> 00:41:22,240
another generation but it was produced

1192
00:41:21,040 --> 00:41:23,520
in this particular way that people

1193
00:41:22,240 --> 00:41:25,440
really loved it they loved it at

1194
00:41:23,520 --> 00:41:27,640
baseball games and and it sold really

1195
00:41:25,440 --> 00:41:29,599
well and one of the things that the

1196
00:41:27,640 --> 00:41:31,359
Vienna beef hot dog company did was they

1197
00:41:29,599 --> 00:41:33,119
realized that they the times were

1198
00:41:31,359 --> 00:41:34,560
changing and they wanted to automate

1199
00:41:33,119 --> 00:41:36,920
their processes partly because of

1200
00:41:34,560 --> 00:41:39,599
increased standards in the food industry

1201
00:41:36,920 --> 00:41:41,040
so they brought in automated material

1202
00:41:39,599 --> 00:41:43,319
movement they brought in robotic

1203
00:41:41,040 --> 00:41:45,200
Automation and they they created a whole

1204
00:41:43,319 --> 00:41:47,119
new facility that was very kind of clean

1205
00:41:45,200 --> 00:41:50,200
you could imagine stainless steel

1206
00:41:47,119 --> 00:41:51,839
everything and they they started

1207
00:41:50,200 --> 00:41:52,920
producing hot dogs in the new facility

1208
00:41:51,839 --> 00:41:54,839
and the hot dogs looked totally

1209
00:41:52,920 --> 00:41:56,920
different they didn't have this kind of

1210
00:41:54,839 --> 00:41:59,240
red shiny color they were gray they

1211
00:41:56,920 --> 00:42:00,400
didn't taste the same and what they

1212
00:41:59,240 --> 00:42:02,599
found was that they introduced

1213
00:42:00,400 --> 00:42:03,960
Automation and something went wrong in

1214
00:42:02,599 --> 00:42:05,240
their process something broke down even

1215
00:42:03,960 --> 00:42:07,440
though they thought they had replicated

1216
00:42:05,240 --> 00:42:09,520
everything step by step and when they

1217
00:42:07,440 --> 00:42:11,240
went back and they tried to do a root

1218
00:42:09,520 --> 00:42:13,359
cause analysis said why did this

1219
00:42:11,240 --> 00:42:16,359
automation kind of lead us to such poor

1220
00:42:13,359 --> 00:42:18,800
quality they found that what they hadn't

1221
00:42:16,359 --> 00:42:21,960
realized as part of their process was

1222
00:42:18,800 --> 00:42:24,040
that being slow at moving the hot dogs

1223
00:42:21,960 --> 00:42:25,880
from the station where they were cased

1224
00:42:24,040 --> 00:42:28,119
to the station where they were smoked it

1225
00:42:25,880 --> 00:42:29,760
used to be a guy named Irving who would

1226
00:42:28,119 --> 00:42:32,200
push the cart and he'd talk to people

1227
00:42:29,760 --> 00:42:33,720
and he moved nice and slow and it it

1228
00:42:32,200 --> 00:42:36,000
took a long time for the process to

1229
00:42:33,720 --> 00:42:38,760
happen but once the hot dogs got into

1230
00:42:36,000 --> 00:42:40,079
the smoker they had sufficiently thought

1231
00:42:38,760 --> 00:42:41,520
that they were at the temperature to

1232
00:42:40,079 --> 00:42:42,960
produce this red color to produce the

1233
00:42:41,520 --> 00:42:45,400
taste that it was actually the slowness

1234
00:42:42,960 --> 00:42:48,800
of the process and Irving's maybe you

1235
00:42:45,400 --> 00:42:50,119
know uh lazy ways that helped them do

1236
00:42:48,800 --> 00:42:52,480
the very thing that they had become so

1237
00:42:50,119 --> 00:42:54,680
good at and once the automated system

1238
00:42:52,480 --> 00:42:56,000
cut this time of the process in half and

1239
00:42:54,680 --> 00:42:57,800
just dunk the hot dogs right in the

1240
00:42:56,000 --> 00:42:59,720
smoker they had lost what made them

1241
00:42:57,800 --> 00:43:02,119
special in the first place so I think

1242
00:42:59,720 --> 00:43:04,240
this is I I think of this this real

1243
00:43:02,119 --> 00:43:07,040
story as a parable for what many

1244
00:43:04,240 --> 00:43:09,240
organizations are doing now with uh with

1245
00:43:07,040 --> 00:43:11,880
AI tools that they're they're in this

1246
00:43:09,240 --> 00:43:13,520
desire for Speed in some cases losing

1247
00:43:11,880 --> 00:43:15,440
what brings their business value in the

1248
00:43:13,520 --> 00:43:17,280
first place and loses what makes their

1249
00:43:15,440 --> 00:43:19,119
workers and their skills so valuable as

1250
00:43:17,280 --> 00:43:21,240
well so I think understanding kind of

1251
00:43:19,119 --> 00:43:22,960
the core processes the core technologies

1252
00:43:21,240 --> 00:43:24,720
that makes organizations great is

1253
00:43:22,960 --> 00:43:26,760
important far before you figure out what

1254
00:43:24,720 --> 00:43:28,640
technologies might be helpful for those

1255
00:43:26,760 --> 00:43:31,079
organiz ation so thank you so much for

1256
00:43:28,640 --> 00:43:33,210
your time happy to talk more and and

1257
00:43:31,079 --> 00:43:34,880
really appreciate you

1258
00:43:33,210 --> 00:43:39,599
[Applause]

1259
00:43:34,880 --> 00:43:39,599
coming thank you very much Ben

