1
00:00:08,400 --> 00:00:12,080
I was going to ask my colleague Steve to

2
00:00:10,080 --> 00:00:13,920
pop up. Uh we've been the kind of

3
00:00:12,080 --> 00:00:15,759
co-architects

4
00:00:13,920 --> 00:00:18,720
nominally of the kind of the mele that

5
00:00:15,759 --> 00:00:20,880
we've had today. Um so I just really

6
00:00:18,720 --> 00:00:23,199
like

7
00:00:20,880 --> 00:00:27,519
what do you think you've heard today?

8
00:00:23,199 --> 00:00:30,640
What was what's your takeaway from?

9
00:00:27,519 --> 00:00:33,640
>> Nothing like being put on the spot.

10
00:00:30,640 --> 00:00:33,640
Um

11
00:00:34,800 --> 00:00:39,040
>> oh

12
00:00:37,520 --> 00:00:41,360
um

13
00:00:39,040 --> 00:00:44,079
so I think ultimately before was we

14
00:00:41,360 --> 00:00:48,239
started with collaboration um we started

15
00:00:44,079 --> 00:00:50,160
with students uh I would say from a a

16
00:00:48,239 --> 00:00:51,760
human resources perspective that is sort

17
00:00:50,160 --> 00:00:53,120
of the backbone in the future of so

18
00:00:51,760 --> 00:00:56,160
there's a lot of technology right now

19
00:00:53,120 --> 00:00:58,879
that we are addressing that I am old and

20
00:00:56,160 --> 00:01:00,640
gray and I probably don't understand um

21
00:00:58,879 --> 00:01:03,039
Michael Shre and I had a conversation

22
00:01:00,640 --> 00:01:05,439
sidebar about getting on an airplane

23
00:01:03,039 --> 00:01:07,520
that Masha had spoken about and not

24
00:01:05,439 --> 00:01:08,720
understanding how how it works, what it

25
00:01:07,520 --> 00:01:10,000
goes into, but we take it for granted

26
00:01:08,720 --> 00:01:13,119
and I think we're doing the same with

27
00:01:10,000 --> 00:01:16,320
LLMs and AI. Um, and so I think having

28
00:01:13,119 --> 00:01:19,040
the student population uh in a rise

29
00:01:16,320 --> 00:01:21,840
framework which guy mentioned before

30
00:01:19,040 --> 00:01:24,640
talking about research uh innovation,

31
00:01:21,840 --> 00:01:27,040
students and education uh as a backbone

32
00:01:24,640 --> 00:01:29,600
of some sort of a baseline as where we

33
00:01:27,040 --> 00:01:31,600
start. I that's really insightful. I

34
00:01:29,600 --> 00:01:33,600
think so from my perspective, I think

35
00:01:31,600 --> 00:01:36,159
what we've been on is a bit of a

36
00:01:33,600 --> 00:01:39,200
trajectory that says so much is that the

37
00:01:36,159 --> 00:01:42,960
the challenges are real.

38
00:01:39,200 --> 00:01:46,159
Um the capabilities and the innovation

39
00:01:42,960 --> 00:01:47,920
and the excitement to confront them and

40
00:01:46,159 --> 00:01:49,759
to work through them and to innovate

41
00:01:47,920 --> 00:01:52,880
through them to grow through them is

42
00:01:49,759 --> 00:01:55,119
real. We have the talent, we have the

43
00:01:52,880 --> 00:01:57,119
technologies, but the, you know, that

44
00:01:55,119 --> 00:01:59,920
has to be approached with, as Lee was

45
00:01:57,119 --> 00:02:02,479
saying this morning, a realism around

46
00:01:59,920 --> 00:02:04,479
the limitations of some of the tools we

47
00:02:02,479 --> 00:02:07,920
had with that capital depth or the

48
00:02:04,479 --> 00:02:10,800
ability to do um shared governance even

49
00:02:07,920 --> 00:02:14,000
within alliances. Um so it's going to

50
00:02:10,800 --> 00:02:15,680
be, you know, an an interesting journey,

51
00:02:14,000 --> 00:02:17,680
but it's like it's not as if it's not

52
00:02:15,680 --> 00:02:20,400
not all doom and gloom. The promise of

53
00:02:17,680 --> 00:02:23,280
these things is so great that if we can

54
00:02:20,400 --> 00:02:25,120
coordinate or or compete or experiment

55
00:02:23,280 --> 00:02:27,120
or learn our way through the current

56
00:02:25,120 --> 00:02:29,360
wave of challenges, you know, there kind

57
00:02:27,120 --> 00:02:31,680
of there's a there is a bright future

58
00:02:29,360 --> 00:02:34,400
ahead. We may a few rough years, but

59
00:02:31,680 --> 00:02:36,480
this is this is what we trained for.

60
00:02:34,400 --> 00:02:38,879
This is what we went to school for. This

61
00:02:36,480 --> 00:02:41,200
is what we work for. Um, and I'm, you

62
00:02:38,879 --> 00:02:42,480
know, leave it a pretty optimistic

63
00:02:41,200 --> 00:02:44,640
generally that we got to be, you know,

64
00:02:42,480 --> 00:02:46,640
we approaching the future with caution.

65
00:02:44,640 --> 00:02:48,720
uh change innovation at time of change

66
00:02:46,640 --> 00:02:51,040
is going to be hard but it's like it's

67
00:02:48,720 --> 00:02:52,400
it's is well within our capabilities I

68
00:02:51,040 --> 00:02:53,840
think we just have to understand the

69
00:02:52,400 --> 00:02:54,720
constraints and the and the routes to

70
00:02:53,840 --> 00:02:58,160
that

71
00:02:54,720 --> 00:03:01,280
>> um the constraints and then

72
00:02:58,160 --> 00:03:03,200
Fiona had mentioned prioritization

73
00:03:01,280 --> 00:03:06,159
so not everybody can do everything at

74
00:03:03,200 --> 00:03:07,760
all times everywhere trying to perhaps

75
00:03:06,159 --> 00:03:09,760
use technology whether it's AI or

76
00:03:07,760 --> 00:03:11,840
elsewhere

77
00:03:09,760 --> 00:03:14,239
to work collaborative collaboratively

78
00:03:11,840 --> 00:03:16,640
while somebody is maybe ch tackling

79
00:03:14,239 --> 00:03:18,480
quantum and somebody's tackling energy

80
00:03:16,640 --> 00:03:19,120
and we're leveraging the the shared

81
00:03:18,480 --> 00:03:21,920
learnings.

82
00:03:19,120 --> 00:03:24,239
>> Excellent. Thank you. So with that draw

83
00:03:21,920 --> 00:03:25,440
this afternoon to close. So I want to

84
00:03:24,239 --> 00:03:26,720
say thank you for everybody for

85
00:03:25,440 --> 00:03:29,599
especially for sticking with us for the

86
00:03:26,720 --> 00:03:30,879
entire day. Um I hope that was you know

87
00:03:29,599 --> 00:03:32,959
you learned something that you didn't

88
00:03:30,879 --> 00:03:35,040
real didn't see before or at least

89
00:03:32,959 --> 00:03:36,640
enjoyed the conversation. Uh we're

90
00:03:35,040 --> 00:03:37,920
looking very much to seeing everybody in

91
00:03:36,640 --> 00:03:39,840
London again on the next occasion

92
00:03:37,920 --> 00:03:42,560
hopefully if you'll have us back. We

93
00:03:39,840 --> 00:03:44,560
didn't upset too many people. Um, and so

94
00:03:42,560 --> 00:03:46,640
thank you all very much indeed and thank

95
00:03:44,560 --> 00:03:49,280
you particularly to the backroom crew.

96
00:03:46,640 --> 00:03:51,840
>> Yeah. So to BT for hosting us here, to

97
00:03:49,280 --> 00:03:54,000
the AV crew for doing a fantastic job,

98
00:03:51,840 --> 00:03:56,239
uh, for Quan who's hiding somewhere, for

99
00:03:54,000 --> 00:03:57,599
uh, making keeping us all in line, uh,

100
00:03:56,239 --> 00:04:00,080
for Gatri for bringing this all

101
00:03:57,599 --> 00:04:02,480
together, uh, to Simon Godfrey, who's

102
00:04:00,080 --> 00:04:04,239
around somewhere, who's been helping me

103
00:04:02,480 --> 00:04:06,400
to work with the UK speakers and pulling

104
00:04:04,239 --> 00:04:08,319
the the narrative together, thought

105
00:04:06,400 --> 00:04:09,519
partner for the last three, four years.

106
00:04:08,319 --> 00:04:12,080
Uh, great guy. If you haven't met him,

107
00:04:09,519 --> 00:04:14,560
you should meet him. Um, and to all the

108
00:04:12,080 --> 00:04:15,840
speakers, all the presenters and the,

109
00:04:14,560 --> 00:04:18,160
you know, the staff outside that did the

110
00:04:15,840 --> 00:04:20,560
catering, it really is a family exercise

111
00:04:18,160 --> 00:04:23,040
to bring these things together. Um,

112
00:04:20,560 --> 00:04:24,240
we've had fun. Uh, it's always stressful

113
00:04:23,040 --> 00:04:26,560
on the way into these things, but once

114
00:04:24,240 --> 00:04:29,120
you, it generally gets going on the day.

115
00:04:26,560 --> 00:04:32,280
Um, so I release you all the rest of

116
00:04:29,120 --> 00:04:32,280
your days.

