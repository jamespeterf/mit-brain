1
00:00:00,160 --> 00:00:05,839
So with that, let me introduce our first

2
00:00:02,480 --> 00:00:08,559
speaker, Professor Hussein Ranima. He's

3
00:00:05,839 --> 00:00:11,679
a professor at the MIT Media Lab and

4
00:00:08,559 --> 00:00:14,080
Toronto Metropolitan University. He's

5
00:00:11,679 --> 00:00:17,520
also the founder of Flybeats, which is

6
00:00:14,080 --> 00:00:19,760
an AI and data science company.

7
00:00:17,520 --> 00:00:22,480
Hussein's research is focused on the

8
00:00:19,760 --> 00:00:25,199
design of human AI systems, data

9
00:00:22,480 --> 00:00:27,760
governance, human computer interaction,

10
00:00:25,199 --> 00:00:31,119
and the design of missionritical AI

11
00:00:27,760 --> 00:00:33,920
systems. He works on many super exciting

12
00:00:31,119 --> 00:00:36,480
projects but today the title of his talk

13
00:00:33,920 --> 00:00:42,559
is unlocking human potential with

14
00:00:36,480 --> 00:00:44,719
perspective aware AI agents over to you.

15
00:00:42,559 --> 00:00:47,760
Thank you very much Ireina and it's a

16
00:00:44,719 --> 00:00:51,039
pleasure to be with you all um and a big

17
00:00:47,760 --> 00:00:54,399
thank you to our friends at ILP to

18
00:00:51,039 --> 00:00:56,239
continue to host these uh great events.

19
00:00:54,399 --> 00:00:58,559
Uh I'm looking forward to our

20
00:00:56,239 --> 00:01:00,559
conversation today and especially

21
00:00:58,559 --> 00:01:04,000
sharing the great research that my

22
00:01:00,559 --> 00:01:08,159
teammates are doing at the uh MIT media

23
00:01:04,000 --> 00:01:12,159
lab. I will be taking you through a uh

24
00:01:08,159 --> 00:01:15,520
presentation uh with a focus on not just

25
00:01:12,159 --> 00:01:18,240
our thinking but also a new program that

26
00:01:15,520 --> 00:01:21,600
we have created at the MIT media lab

27
00:01:18,240 --> 00:01:25,040
called Sapion which is a new way to

28
00:01:21,600 --> 00:01:28,479
think about the future of AI and human

29
00:01:25,040 --> 00:01:32,799
collaboration especially when you have a

30
00:01:28,479 --> 00:01:35,119
lot of people a lot of AI models and the

31
00:01:32,799 --> 00:01:38,240
ecosystem between them and the

32
00:01:35,119 --> 00:01:43,200
complexity that comes with that. I will

33
00:01:38,240 --> 00:01:46,560
start with um just going to shrink my

34
00:01:43,200 --> 00:01:48,880
um yeah I will start with some of our

35
00:01:46,560 --> 00:01:52,320
older projects. Some of them are 10

36
00:01:48,880 --> 00:01:55,119
years old. Like we had coffee machines

37
00:01:52,320 --> 00:01:57,920
that could sense the stress level and

38
00:01:55,119 --> 00:02:00,399
the sleep level of our labmates and

39
00:01:57,920 --> 00:02:02,560
based on that it changed the change the

40
00:02:00,399 --> 00:02:05,040
intensity of their espresso and sent

41
00:02:02,560 --> 00:02:07,600
them a message that their coffee was

42
00:02:05,040 --> 00:02:10,160
available. We built the very first

43
00:02:07,600 --> 00:02:13,040
navigational system for Paris Metro

44
00:02:10,160 --> 00:02:15,680
Authority that it navigated people not

45
00:02:13,040 --> 00:02:17,840
just based on location but based on a

46
00:02:15,680 --> 00:02:20,480
number of context elements that

47
00:02:17,840 --> 00:02:22,800
surrounded that passengers. We had

48
00:02:20,480 --> 00:02:25,280
augmented reality solutions that instead

49
00:02:22,800 --> 00:02:28,160
of using radio signals, we could

50
00:02:25,280 --> 00:02:31,760
navigate people in an airport using

51
00:02:28,160 --> 00:02:34,080
augmented reality uh on signage systems

52
00:02:31,760 --> 00:02:36,400
and then we navigated people accordingly

53
00:02:34,080 --> 00:02:38,239
to their gates to restaurants based on

54
00:02:36,400 --> 00:02:41,440
the needs that they had at an airport

55
00:02:38,239 --> 00:02:44,080
and many many other things. But since

56
00:02:41,440 --> 00:02:46,800
then many things have changed and

57
00:02:44,080 --> 00:02:50,720
especially that's our relationship with

58
00:02:46,800 --> 00:02:54,239
data. In almost all those projects, my

59
00:02:50,720 --> 00:02:56,480
mindset as a graduate student and then a

60
00:02:54,239 --> 00:02:58,000
new faculty member and many of our

61
00:02:56,480 --> 00:03:00,160
students

62
00:02:58,000 --> 00:03:03,280
where the fact that hey we have all the

63
00:03:00,160 --> 00:03:06,080
data give me all the data and when I

64
00:03:03,280 --> 00:03:09,599
have the data I'll do my work or I'll

65
00:03:06,080 --> 00:03:12,800
write my thesis that is changing now. We

66
00:03:09,599 --> 00:03:15,920
are now surrounded by zetabytes of data

67
00:03:12,800 --> 00:03:19,120
not just within our organizations but

68
00:03:15,920 --> 00:03:21,599
also outside our organizations.

69
00:03:19,120 --> 00:03:24,560
Uh and many of that data does not belong

70
00:03:21,599 --> 00:03:26,560
to us. They come with different legal

71
00:03:24,560 --> 00:03:29,360
contracts. They belong to different

72
00:03:26,560 --> 00:03:31,360
people. Of course we need to synthesize

73
00:03:29,360 --> 00:03:34,480
and model them and use them for

74
00:03:31,360 --> 00:03:37,680
inference and learning. But it's not as

75
00:03:34,480 --> 00:03:40,159
easy as before. So what I always say is

76
00:03:37,680 --> 00:03:41,680
that we are moving from an era of

77
00:03:40,159 --> 00:03:44,400
personalization

78
00:03:41,680 --> 00:03:47,120
which was all about putting all the data

79
00:03:44,400 --> 00:03:50,400
in one centralized location inferent

80
00:03:47,120 --> 00:03:53,360
model and push things to people to a

81
00:03:50,400 --> 00:03:56,000
model which is based on participation

82
00:03:53,360 --> 00:03:59,599
collaborating and co-creating with the

83
00:03:56,000 --> 00:04:03,040
machine. And that drives a fundamentally

84
00:03:59,599 --> 00:04:05,120
different way on how we design systems,

85
00:04:03,040 --> 00:04:07,760
build infrastructures,

86
00:04:05,120 --> 00:04:10,400
manage data relationships, trade mo

87
00:04:07,760 --> 00:04:13,040
train models. So the program that we

88
00:04:10,400 --> 00:04:16,479
created at the MIT Media Lab is

89
00:04:13,040 --> 00:04:19,759
addressing these six pillars. moving

90
00:04:16,479 --> 00:04:21,919
from personalization to participation,

91
00:04:19,759 --> 00:04:25,120
breaking data silos and forming

92
00:04:21,919 --> 00:04:27,440
alliances not just between units in a

93
00:04:25,120 --> 00:04:29,759
company but also with their partnerships

94
00:04:27,440 --> 00:04:32,479
outside the organization

95
00:04:29,759 --> 00:04:35,680
all the way to things such as making AI

96
00:04:32,479 --> 00:04:38,800
auditable like when you deliver an

97
00:04:35,680 --> 00:04:41,520
answer or a generated prompt to an

98
00:04:38,800 --> 00:04:43,680
individual or a group of people. What

99
00:04:41,520 --> 00:04:45,919
about your audit track? What if someone

100
00:04:43,680 --> 00:04:48,800
calls a call center and say excuse me

101
00:04:45,919 --> 00:04:51,199
why did you give me that answer or that

102
00:04:48,800 --> 00:04:53,360
information? So similar to what we

103
00:04:51,199 --> 00:04:56,240
learned in the financial industry we are

104
00:04:53,360 --> 00:05:01,199
also doing a lot of work to be able to

105
00:04:56,240 --> 00:05:04,240
audit um generative AI uh interventions.

106
00:05:01,199 --> 00:05:07,039
The program that we created looks at uh

107
00:05:04,240 --> 00:05:09,840
five key pillars looking at looking at

108
00:05:07,039 --> 00:05:13,280
AI using an ecological model rather than

109
00:05:09,840 --> 00:05:17,360
just a technological or a design model.

110
00:05:13,280 --> 00:05:19,840
We focus a lot on AI literacy to empower

111
00:05:17,360 --> 00:05:23,360
people not just boards of directors and

112
00:05:19,840 --> 00:05:26,800
seuitees but also K12 students to

113
00:05:23,360 --> 00:05:29,280
understand what AI is and is not and how

114
00:05:26,800 --> 00:05:32,000
best they can use it as an enabler and

115
00:05:29,280 --> 00:05:34,720
an augmentation tool rather than a force

116
00:05:32,000 --> 00:05:37,280
that will replace uh people's cognition

117
00:05:34,720 --> 00:05:40,080
and their ability. We have a strong

118
00:05:37,280 --> 00:05:42,560
focus on the importance of data and how

119
00:05:40,080 --> 00:05:46,000
we control the data, own it and even

120
00:05:42,560 --> 00:05:48,560
monetize it correctly with transparency.

121
00:05:46,000 --> 00:05:51,199
And also a very big focus at the MIT

122
00:05:48,560 --> 00:05:54,400
media lab is the cross-disciplinary

123
00:05:51,199 --> 00:05:57,280
aspect of AI. Not just technology, but

124
00:05:54,400 --> 00:05:59,440
how do you design with AI? We are at an

125
00:05:57,280 --> 00:06:03,039
era in which we are throwing terabytes

126
00:05:59,440 --> 00:06:05,120
of data on these two-dimensional screens

127
00:06:03,039 --> 00:06:07,919
and we want them to be trackable,

128
00:06:05,120 --> 00:06:10,160
auditable, verifiable. It's very

129
00:06:07,919 --> 00:06:13,600
different than how we used to design

130
00:06:10,160 --> 00:06:15,680
user experience and user interfaces in

131
00:06:13,600 --> 00:06:17,680
the past.

132
00:06:15,680 --> 00:06:20,560
similar to how we created a

133
00:06:17,680 --> 00:06:22,720
communication spectrum in the past 25

134
00:06:20,560 --> 00:06:25,039
years that allows us to have this

135
00:06:22,720 --> 00:06:27,440
amazing highdefinition

136
00:06:25,039 --> 00:06:30,319
uh zoom call from all around the world

137
00:06:27,440 --> 00:06:33,280
and we take that for granted. We also

138
00:06:30,319 --> 00:06:35,919
believe that we need to invest on an AI

139
00:06:33,280 --> 00:06:38,720
spectrum. We need to focus on new

140
00:06:35,919 --> 00:06:41,600
experience architectures, on our ability

141
00:06:38,720 --> 00:06:43,759
to abstract data and systems without

142
00:06:41,600 --> 00:06:46,720
keep removing old systems and putting

143
00:06:43,759 --> 00:06:50,240
new systems in. and a big focus on our

144
00:06:46,720 --> 00:06:53,280
research team on human AI service agents

145
00:06:50,240 --> 00:06:56,000
which is not necessarily building and

146
00:06:53,280 --> 00:06:59,520
incorporating LLMs but it's about

147
00:06:56,000 --> 00:07:02,160
encapsulating people's knowledge in AI

148
00:06:59,520 --> 00:07:04,880
agents allowing them to share this

149
00:07:02,160 --> 00:07:07,440
expertise with a decentralized learning

150
00:07:04,880 --> 00:07:10,240
network and even down the rope benefit

151
00:07:07,440 --> 00:07:12,479
from it or monetize it. You're

152
00:07:10,240 --> 00:07:15,599
interested in forming data alliances.

153
00:07:12,479 --> 00:07:18,240
How can a telecom carrier, a bank, a

154
00:07:15,599 --> 00:07:22,000
travel company, a utility company, a

155
00:07:18,240 --> 00:07:25,759
clinic can all come together to share

156
00:07:22,000 --> 00:07:28,479
signals and decision patterns on behalf

157
00:07:25,759 --> 00:07:32,000
of the user with the consent of the user

158
00:07:28,479 --> 00:07:34,639
to generate new value. So our programs

159
00:07:32,000 --> 00:07:36,800
are based on six pillars. generating

160
00:07:34,639 --> 00:07:39,039
moonshot ideas, not necessarily

161
00:07:36,800 --> 00:07:41,520
interested about what's coming tomorrow,

162
00:07:39,039 --> 00:07:43,840
but envisioning the future of industries

163
00:07:41,520 --> 00:07:46,479
using our moonshot thinking at the media

164
00:07:43,840 --> 00:07:49,840
lab on the future of industries in the

165
00:07:46,479 --> 00:07:52,400
next 10, 15 and 20 years. big focus on

166
00:07:49,840 --> 00:07:55,360
data mechanics, building products and

167
00:07:52,400 --> 00:07:58,879
platforms, building new experiences and

168
00:07:55,360 --> 00:08:01,919
design and most importantly empowering

169
00:07:58,879 --> 00:08:04,720
people, empowering organization and

170
00:08:01,919 --> 00:08:07,520
improve their AI readiness.

171
00:08:04,720 --> 00:08:11,039
MIT Media Lab has been very strong in

172
00:08:07,520 --> 00:08:13,360
the area of human computer interaction.

173
00:08:11,039 --> 00:08:16,879
In the literature, this is usually being

174
00:08:13,360 --> 00:08:19,199
defined as a literature in which a human

175
00:08:16,879 --> 00:08:22,960
collaborates with a machine or a few

176
00:08:19,199 --> 00:08:25,280
people collaborate with a few machines.

177
00:08:22,960 --> 00:08:29,440
What we are interested in are what I

178
00:08:25,280 --> 00:08:32,479
call HCI 2, humane, calm, and

179
00:08:29,440 --> 00:08:35,279
intelligent interfaces. We are actually

180
00:08:32,479 --> 00:08:39,519
interested in AI systems that can run in

181
00:08:35,279 --> 00:08:42,159
the background allowing us the human the

182
00:08:39,519 --> 00:08:44,159
people to communicate more effectively

183
00:08:42,159 --> 00:08:46,080
with with each other. We are not

184
00:08:44,159 --> 00:08:49,120
necessarily interested to put you

185
00:08:46,080 --> 00:08:51,279
behind, you know, um goggles and

186
00:08:49,120 --> 00:08:54,080
headsets and locking you down in these

187
00:08:51,279 --> 00:08:56,800
digital echo chambers. We want you to

188
00:08:54,080 --> 00:08:59,279
use the best possible user interface

189
00:08:56,800 --> 00:09:02,399
that has ever been invented and that's

190
00:08:59,279 --> 00:09:06,480
human. We are empowering human to human

191
00:09:02,399 --> 00:09:09,360
interfacing using AI in the background.

192
00:09:06,480 --> 00:09:11,600
Now I want to show you some examples.

193
00:09:09,360 --> 00:09:14,160
Our project started using some of the

194
00:09:11,600 --> 00:09:16,560
early software and AI agents. People

195
00:09:14,160 --> 00:09:18,959
think that AI agents are new. They are

196
00:09:16,560 --> 00:09:21,200
not. They were actually invented at MIT

197
00:09:18,959 --> 00:09:24,399
in 1994.

198
00:09:21,200 --> 00:09:26,160
We use them to create a social GPS a few

199
00:09:24,399 --> 00:09:28,720
years ago in collaboration with a

200
00:09:26,160 --> 00:09:31,120
telecom carrier. So instead of using

201
00:09:28,720 --> 00:09:33,279
cartisian coordinates, you can use the

202
00:09:31,120 --> 00:09:35,600
knowledge base of a celebrity or a

203
00:09:33,279 --> 00:09:38,000
famous person. You can apply that

204
00:09:35,600 --> 00:09:40,560
knowledge graph on Google maps, a food

205
00:09:38,000 --> 00:09:43,519
ordering app, a media channel. And let's

206
00:09:40,560 --> 00:09:46,959
say in the case of Google maps, I could

207
00:09:43,519 --> 00:09:50,720
go to Paris and I could walk the shoes

208
00:09:46,959 --> 00:09:53,200
of Gwennet Paltro and see what would she

209
00:09:50,720 --> 00:09:55,279
do if she was here, what bakery she

210
00:09:53,200 --> 00:09:59,120
would go to, what music she would listen

211
00:09:55,279 --> 00:10:01,600
to on Spotify. We interconnected to open

212
00:09:59,120 --> 00:10:04,000
banking rail so people can actually

213
00:10:01,600 --> 00:10:06,000
could use the category of their

214
00:10:04,000 --> 00:10:08,720
transactions to recommend what they

215
00:10:06,000 --> 00:10:12,080
bought and what they like. Through this

216
00:10:08,720 --> 00:10:14,800
we learned a lot and we realized that AI

217
00:10:12,080 --> 00:10:17,200
agents are not ready to deliver on their

218
00:10:14,800 --> 00:10:19,760
promises. They are very good in doing

219
00:10:17,200 --> 00:10:22,320
one or two things very specifically

220
00:10:19,760 --> 00:10:27,360
related to functions. But when it comes

221
00:10:22,320 --> 00:10:29,600
to the complexity of um building these

222
00:10:27,360 --> 00:10:32,240
workflows in which human element is

223
00:10:29,600 --> 00:10:34,160
active there they were not there. So we

224
00:10:32,240 --> 00:10:37,120
decided to build these new data

225
00:10:34,160 --> 00:10:39,440
structures called chronicles.

226
00:10:37,120 --> 00:10:42,160
These are data structures that can

227
00:10:39,440 --> 00:10:44,800
capture the behavioral pattern using

228
00:10:42,160 --> 00:10:47,760
causal and correlational model of

229
00:10:44,800 --> 00:10:50,640
individuals using ontologies and then

230
00:10:47,760 --> 00:10:53,839
they will be used to provide a much

231
00:10:50,640 --> 00:10:57,440
better humane interfaces when you're

232
00:10:53,839 --> 00:10:59,440
building AI agents. We have written a

233
00:10:57,440 --> 00:11:01,839
lot of papers on that that you can read

234
00:10:59,440 --> 00:11:04,800
about around the notion of perspective

235
00:11:01,839 --> 00:11:07,040
aware AI and the role of these new

236
00:11:04,800 --> 00:11:09,600
software engineering paradigms using

237
00:11:07,040 --> 00:11:12,720
chronicles and demonstrating how they

238
00:11:09,600 --> 00:11:15,760
bring value to the future of AI system.

239
00:11:12,720 --> 00:11:18,560
So essentially learning from the user

240
00:11:15,760 --> 00:11:21,200
and keep adding subgraphs to a master

241
00:11:18,560 --> 00:11:24,800
chronicle representing your knowledge or

242
00:11:21,200 --> 00:11:26,800
even an organizational knowledge. An

243
00:11:24,800 --> 00:11:28,800
example that I can give you is a work

244
00:11:26,800 --> 00:11:32,160
that we have done with the US government

245
00:11:28,800 --> 00:11:34,800
for a particular US agency and we use

246
00:11:32,160 --> 00:11:36,560
these chronicles to capture human

247
00:11:34,800 --> 00:11:38,959
dynamics

248
00:11:36,560 --> 00:11:41,600
capturing expert knowledge, role,

249
00:11:38,959 --> 00:11:43,920
workload, competency and then we use

250
00:11:41,600 --> 00:11:47,040
that to figure out things that AI is

251
00:11:43,920 --> 00:11:51,040
actually not very good at. If I have a a

252
00:11:47,040 --> 00:11:53,600
a a company of about 100,000 people, how

253
00:11:51,040 --> 00:11:56,320
do I form teams that can achieve a

254
00:11:53,600 --> 00:11:58,480
particular project very well and if I

255
00:11:56,320 --> 00:12:01,279
optimize for that, what are the risks

256
00:11:58,480 --> 00:12:03,200
that I need to be aware of? Or if I want

257
00:12:01,279 --> 00:12:05,440
to figure out mentor mentee

258
00:12:03,200 --> 00:12:08,320
relationships between people to form

259
00:12:05,440 --> 00:12:11,040
groups, how do I do that?

260
00:12:08,320 --> 00:12:13,200
uh we actually outperformed many of the

261
00:12:11,040 --> 00:12:15,680
research out there using generative

262
00:12:13,200 --> 00:12:18,480
agent-based modeling by incorporating

263
00:12:15,680 --> 00:12:21,200
our chronicles into them and we have

264
00:12:18,480 --> 00:12:24,160
seen significant improvements that if

265
00:12:21,200 --> 00:12:27,680
you compare them with the incumbency we

266
00:12:24,160 --> 00:12:31,120
can now capture soft human dynamics in

267
00:12:27,680 --> 00:12:34,560
AI models on a multimodal basis on a

268
00:12:31,120 --> 00:12:38,560
much more effective way for example look

269
00:12:34,560 --> 00:12:42,079
at a video and an image an email, Slack

270
00:12:38,560 --> 00:12:45,200
channels and figure out who may work

271
00:12:42,079 --> 00:12:47,440
better with whom or who likes you know

272
00:12:45,200 --> 00:12:49,760
to form a team that is delivering a

273
00:12:47,440 --> 00:12:51,839
particular objective. So we are getting

274
00:12:49,760 --> 00:12:54,639
some very good results for that. And now

275
00:12:51,839 --> 00:12:57,279
I want to show you some examples.

276
00:12:54,639 --> 00:12:59,600
The ORC chart on the left is very

277
00:12:57,279 --> 00:13:02,320
familiar to all of us. A two-dimensional

278
00:12:59,600 --> 00:13:04,720
ORC chart project comes in. We get

279
00:13:02,320 --> 00:13:08,160
together based on budget, availability,

280
00:13:04,720 --> 00:13:11,120
calendar and we deliver that work. We

281
00:13:08,160 --> 00:13:13,680
are now creating a human AI agent for

282
00:13:11,120 --> 00:13:16,480
every single member of that uh

283
00:13:13,680 --> 00:13:18,880
organization. So when a project comes in

284
00:13:16,480 --> 00:13:21,600
in form of a prompt with a large context

285
00:13:18,880 --> 00:13:23,519
window, it's actually the human AI

286
00:13:21,600 --> 00:13:25,519
agents of the employees that are

287
00:13:23,519 --> 00:13:28,560
collaborating with each other to deliver

288
00:13:25,519 --> 00:13:31,040
the first part of that task. They they

289
00:13:28,560 --> 00:13:33,760
the agents bring the project to a level

290
00:13:31,040 --> 00:13:36,480
of maturity that essentially most of the

291
00:13:33,760 --> 00:13:39,440
grunt and mechanical work is being done

292
00:13:36,480 --> 00:13:41,760
and then the AI will notify people to

293
00:13:39,440 --> 00:13:44,320
join and make the project more polished,

294
00:13:41,760 --> 00:13:46,800
better, innovative and our thesis in

295
00:13:44,320 --> 00:13:50,079
this project is that AI is not

296
00:13:46,800 --> 00:13:53,279
necessarily here to replace us. If used

297
00:13:50,079 --> 00:13:55,920
correctly, it is giving us more time to

298
00:13:53,279 --> 00:13:58,560
learn, to collaborate, to meet with each

299
00:13:55,920 --> 00:14:00,720
other, and our human AI agents will get

300
00:13:58,560 --> 00:14:03,120
better. Now, I'm going to take you

301
00:14:00,720 --> 00:14:05,600
quickly through a number of examples in

302
00:14:03,120 --> 00:14:07,760
the industry. We have built a tool

303
00:14:05,600 --> 00:14:10,399
called Open Dome that allows us to

304
00:14:07,760 --> 00:14:13,519
create a digital twin of a space using

305
00:14:10,399 --> 00:14:15,680
Gaussian splatting within about an hour.

306
00:14:13,519 --> 00:14:19,519
We work with this organization to

307
00:14:15,680 --> 00:14:23,040
capture data from uh HR systems, payroll

308
00:14:19,519 --> 00:14:25,440
systems, tax systems, CRM systems and we

309
00:14:23,040 --> 00:14:28,959
applied that chronicle to the digital

310
00:14:25,440 --> 00:14:31,760
twin of that uh office. So now you can

311
00:14:28,959 --> 00:14:34,320
go to that office and say today I want

312
00:14:31,760 --> 00:14:36,160
to come to the office digital twin but I

313
00:14:34,320 --> 00:14:39,519
want to see the office through the lens

314
00:14:36,160 --> 00:14:42,480
of the CEO. we now go capture data from

315
00:14:39,519 --> 00:14:44,880
across the chronicles and create a lens

316
00:14:42,480 --> 00:14:48,160
for this employee to understand what's

317
00:14:44,880 --> 00:14:50,079
important for the CEO that day or you

318
00:14:48,160 --> 00:14:52,240
can change the lens to the ESG

319
00:14:50,079 --> 00:14:56,399
specialist in this case and see how the

320
00:14:52,240 --> 00:14:59,600
organization is doing in terms of um

321
00:14:56,399 --> 00:15:02,560
carbon footprints and and and uh some of

322
00:14:59,600 --> 00:15:05,600
their sustainability metrics.

323
00:15:02,560 --> 00:15:07,760
I welcome all of you to visit us at the

324
00:15:05,600 --> 00:15:10,399
media lab. We are a very unique

325
00:15:07,760 --> 00:15:12,639
interdisciplinary organization and one

326
00:15:10,399 --> 00:15:15,040
of my incoming graduate students just

327
00:15:12,639 --> 00:15:17,040
used this technology to create a tour

328
00:15:15,040 --> 00:15:20,000
guide for the lab using perspective

329
00:15:17,040 --> 00:15:21,760
aware AI. So when you come to the lab

330
00:15:20,000 --> 00:15:24,160
you can wear the lens of different

331
00:15:21,760 --> 00:15:27,120
professors and faculty members or even

332
00:15:24,160 --> 00:15:30,079
staff members we have and by choosing

333
00:15:27,120 --> 00:15:32,399
that lens you can now go and read about

334
00:15:30,079 --> 00:15:34,720
all the projects but as you read about

335
00:15:32,399 --> 00:15:37,600
the project you can also see the

336
00:15:34,720 --> 00:15:39,760
perspective of that professor. So this

337
00:15:37,600 --> 00:15:42,560
way you can choose a professor who is

338
00:15:39,760 --> 00:15:45,360
interested in climate or technology or

339
00:15:42,560 --> 00:15:48,720
user interfaces and you can develop a

340
00:15:45,360 --> 00:15:50,639
much better perspective in terms of uh

341
00:15:48,720 --> 00:15:52,320
their viewpoints and we are measuring

342
00:15:50,639 --> 00:15:53,839
that this actually brings people

343
00:15:52,320 --> 00:15:56,480
together.

344
00:15:53,839 --> 00:15:59,680
Another great example is how we can

345
00:15:56,480 --> 00:16:02,240
generate new ideas. I have a dear

346
00:15:59,680 --> 00:16:05,199
colleague Joe Jacobson who is very good

347
00:16:02,240 --> 00:16:07,120
with genome engineering. My background

348
00:16:05,199 --> 00:16:10,240
of course is not genetics. I'm more

349
00:16:07,120 --> 00:16:12,320
focused on AI, but there are lots of

350
00:16:10,240 --> 00:16:14,560
interesting synergies between our work,

351
00:16:12,320 --> 00:16:16,160
but I need some ideas to figure out what

352
00:16:14,560 --> 00:16:17,680
it could be the reference for our

353
00:16:16,160 --> 00:16:20,160
collaboration.

354
00:16:17,680 --> 00:16:22,800
So now we have fine-tuned models from

355
00:16:20,160 --> 00:16:25,360
knowledge base in the lab, fine-tune

356
00:16:22,800 --> 00:16:27,199
them, orchestrate them, control them,

357
00:16:25,360 --> 00:16:29,680
and now if I'm looking for a base

358
00:16:27,199 --> 00:16:32,639
collaboration, I can use Gen AI using

359
00:16:29,680 --> 00:16:35,279
audio and generative signals. And then I

360
00:16:32,639 --> 00:16:38,160
can interconnect two projects together

361
00:16:35,279 --> 00:16:41,040
and come up with a new project that is

362
00:16:38,160 --> 00:16:45,519
interfacing between his area of

363
00:16:41,040 --> 00:16:48,079
excellence and my uh focus. We are now

364
00:16:45,519 --> 00:16:50,639
applying this to many verticals from

365
00:16:48,079 --> 00:16:53,360
working with loyalty program providers.

366
00:16:50,639 --> 00:16:55,279
Then when you choose your seats you can

367
00:16:53,360 --> 00:16:58,000
use you can see the plane through the

368
00:16:55,279 --> 00:16:59,920
lens of others. One of my favorite ones

369
00:16:58,000 --> 00:17:02,160
is what we are doing in the medical

370
00:16:59,920 --> 00:17:05,120
sector. So when a doctor is looking at a

371
00:17:02,160 --> 00:17:07,600
medical doseier, he or she can activate

372
00:17:05,120 --> 00:17:10,000
the lenses of two other doctors from

373
00:17:07,600 --> 00:17:12,559
across other hospitals, see their

374
00:17:10,000 --> 00:17:15,439
diagnosis and misdiagnosis risks, but

375
00:17:12,559 --> 00:17:18,160
make a better decision uh with more

376
00:17:15,439 --> 00:17:20,079
calculated decision uh when making the

377
00:17:18,160 --> 00:17:23,360
diagnosis. This is a reason that we

378
00:17:20,079 --> 00:17:26,959
always say biases are actually good in

379
00:17:23,360 --> 00:17:29,840
AI as long as we are transparent on it.

380
00:17:26,959 --> 00:17:32,320
We are working now with triathlon gold

381
00:17:29,840 --> 00:17:36,799
medalists in the US to get information

382
00:17:32,320 --> 00:17:38,480
from these new um health companies that

383
00:17:36,799 --> 00:17:42,080
are giving you a lot of access to

384
00:17:38,480 --> 00:17:45,200
biomarkers, genome data, health trackers

385
00:17:42,080 --> 00:17:47,120
to essentially give you a lens h as a

386
00:17:45,200 --> 00:17:49,679
health concier. So when you go to a

387
00:17:47,120 --> 00:17:52,720
restaurant, you can basically look at a

388
00:17:49,679 --> 00:17:54,720
menu and based on all your needs, we

389
00:17:52,720 --> 00:17:57,120
will be able to fetch things from you

390
00:17:54,720 --> 00:18:00,480
for the menu that is aligned with your

391
00:17:57,120 --> 00:18:02,559
health goals and your uh plans. We do a

392
00:18:00,480 --> 00:18:04,320
lot of work with financial institutions.

393
00:18:02,559 --> 00:18:07,520
One of the things about the Sapion

394
00:18:04,320 --> 00:18:09,120
program is we build these fundamental

395
00:18:07,520 --> 00:18:12,320
technologies that we think are

396
00:18:09,120 --> 00:18:14,400
gamechanging and then we partner with

397
00:18:12,320 --> 00:18:16,799
organizations around the world that

398
00:18:14,400 --> 00:18:18,799
really want to change their sector and

399
00:18:16,799 --> 00:18:21,200
we work with them to apply this

400
00:18:18,799 --> 00:18:23,840
technology to give them competitive

401
00:18:21,200 --> 00:18:26,000
advantage. So we do number of things

402
00:18:23,840 --> 00:18:29,280
with financial institutions to build

403
00:18:26,000 --> 00:18:31,520
let's say a decision support space for

404
00:18:29,280 --> 00:18:33,520
your financial needs. This is in

405
00:18:31,520 --> 00:18:36,880
collaboration with some of the work we

406
00:18:33,520 --> 00:18:39,520
do on um Apple Vision Pro. You can go to

407
00:18:36,880 --> 00:18:42,480
this space. You have access to Genai and

408
00:18:39,520 --> 00:18:45,520
you can talk to your uh finance agent

409
00:18:42,480 --> 00:18:47,679
and say, "Hey, activate my home. Show me

410
00:18:45,520 --> 00:18:49,600
how much mortgage I have left. Give me a

411
00:18:47,679 --> 00:18:52,320
retirement plan that allows me to pay

412
00:18:49,600 --> 00:18:55,679
for my mortgage and pay it off by this

413
00:18:52,320 --> 00:18:57,840
time or uh manage my retirement savings

414
00:18:55,679 --> 00:19:00,559
or I want to buy that car. Show me how

415
00:18:57,840 --> 00:19:03,039
to do it." It's a very interesting way

416
00:19:00,559 --> 00:19:05,520
for us to bring a regulated complex

417
00:19:03,039 --> 00:19:08,480
industry, combine generative AI and

418
00:19:05,520 --> 00:19:10,799
spatial computing and rethink how the

419
00:19:08,480 --> 00:19:13,840
future of these enterprise services can

420
00:19:10,799 --> 00:19:16,320
look like. My team is very much focused

421
00:19:13,840 --> 00:19:18,720
on embodied AI. We actually want down

422
00:19:16,320 --> 00:19:22,240
the road for the phones to go away and

423
00:19:18,720 --> 00:19:24,480
objects to become active. Uh we have now

424
00:19:22,240 --> 00:19:27,840
built a mobile banking app that doesn't

425
00:19:24,480 --> 00:19:30,320
even need an app or a mobile phone. We

426
00:19:27,840 --> 00:19:33,360
have partnered with a French company to

427
00:19:30,320 --> 00:19:37,280
build biometrics in a card. You can now

428
00:19:33,360 --> 00:19:40,000
use um augmented reality glasses and

429
00:19:37,280 --> 00:19:43,679
voice and you can use gestures of the

430
00:19:40,000 --> 00:19:47,200
card to activate balances, offers

431
00:19:43,679 --> 00:19:50,160
looking at your transactions. You can um

432
00:19:47,200 --> 00:19:52,880
share bills uh or or split bills if you

433
00:19:50,160 --> 00:19:54,960
go to a restaurant without the usage of

434
00:19:52,880 --> 00:19:57,919
any app. And although the hardware

435
00:19:54,960 --> 00:20:00,240
requirements are still not fully

436
00:19:57,919 --> 00:20:02,160
commercialized but we are really

437
00:20:00,240 --> 00:20:06,000
building the research foundation for

438
00:20:02,160 --> 00:20:08,240
these for the future. We do wick robots.

439
00:20:06,000 --> 00:20:10,480
So uh this is for us the future of

440
00:20:08,240 --> 00:20:13,840
electronics that instead of setting up

441
00:20:10,480 --> 00:20:15,600
configurations on machines and robots.

442
00:20:13,840 --> 00:20:18,240
We are now incorporating these

443
00:20:15,600 --> 00:20:22,000
chronicles into robots and we measure

444
00:20:18,240 --> 00:20:24,480
things such as empathic values when kids

445
00:20:22,000 --> 00:20:27,520
and robots interact on a multimodal

446
00:20:24,480 --> 00:20:31,840
basis. So parents can now incorporate a

447
00:20:27,520 --> 00:20:34,000
lot of their value systems um when their

448
00:20:31,840 --> 00:20:36,559
kids interact with robots. And we have a

449
00:20:34,000 --> 00:20:39,360
very big interest in helping especially

450
00:20:36,559 --> 00:20:41,840
kids with autism to use these ro

451
00:20:39,360 --> 00:20:45,200
humanoid robots to develop better social

452
00:20:41,840 --> 00:20:48,240
skills. One of the last projects I show

453
00:20:45,200 --> 00:20:51,679
you is what we have done recently uh in

454
00:20:48,240 --> 00:20:53,760
collaboration with Bloomberg. Um Emily

455
00:20:51,679 --> 00:20:56,480
Chang is a well-known technology

456
00:20:53,760 --> 00:20:59,280
reporter. Uh we worked with her to

457
00:20:56,480 --> 00:21:01,200
develop a program on uh Bloomberg

458
00:20:59,280 --> 00:21:04,000
originals. You can watch it. It's called

459
00:21:01,200 --> 00:21:06,720
Postthuman. and we connected our

460
00:21:04,000 --> 00:21:09,679
chronicles essentially to create a

461
00:21:06,720 --> 00:21:12,000
digital version of Emily. So we used our

462
00:21:09,679 --> 00:21:14,880
chronicle builders, we used her

463
00:21:12,000 --> 00:21:17,840
interviews with Natalie Portman, with

464
00:21:14,880 --> 00:21:21,520
Cheryl Sandberg, her books, her social

465
00:21:17,840 --> 00:21:23,840
media. We built her chronicle and then

466
00:21:21,520 --> 00:21:27,120
of course we could run this as a chatbot

467
00:21:23,840 --> 00:21:29,600
or textbased interfaces but we also

468
00:21:27,120 --> 00:21:32,080
reached out to a group in Hollywood that

469
00:21:29,600 --> 00:21:33,840
were experts in building facial

470
00:21:32,080 --> 00:21:37,360
expressions because a lot of these

471
00:21:33,840 --> 00:21:41,200
synthetic characters and uh deep fakes

472
00:21:37,360 --> 00:21:43,360
are either not secure and safe or they

473
00:21:41,200 --> 00:21:46,640
are essentially gimmicks are not really

474
00:21:43,360 --> 00:21:49,039
giving you that empic response that you

475
00:21:46,640 --> 00:21:53,600
need. So we them

476
00:21:49,039 --> 00:21:56,480
fine tuned our model their facial models

477
00:21:53,600 --> 00:22:00,799
and that's the inter that we created. So

478
00:21:56,480 --> 00:22:05,280
we could now have the your colleagues or

479
00:22:00,799 --> 00:22:08,400
friends available to you. Um you can

480
00:22:05,280 --> 00:22:10,480
choose your ideal chronicle. You will

481
00:22:08,400 --> 00:22:12,960
parse their graph based on you have

482
00:22:10,480 --> 00:22:15,919
based on what you have access to and

483
00:22:12,960 --> 00:22:18,400
then you almost logical in the workplace

484
00:22:15,919 --> 00:22:21,360
enhancing productivity and streamlining

485
00:22:18,400 --> 00:22:23,679
tasks but it brings significant

486
00:22:21,360 --> 00:22:26,720
concerns. The training data driving

487
00:22:23,679 --> 00:22:29,520
these systems can be biased. And now if

488
00:22:26,720 --> 00:22:32,080
you look at here this the most important

489
00:22:29,520 --> 00:22:35,440
thing here is an audit confidence

490
00:22:32,080 --> 00:22:38,799
dashboard. As she talks, we measure

491
00:22:35,440 --> 00:22:40,320
things such as provenence of the data,

492
00:22:38,799 --> 00:22:42,880
um,

493
00:22:40,320 --> 00:22:46,960
uh, provenence of the data, lift of the

494
00:22:42,880 --> 00:22:50,559
data, uh, confidence on data. As she

495
00:22:46,960 --> 00:22:53,919
talks, we can tell you how we are about

496
00:22:50,559 --> 00:22:56,480
the answer that she's giving you. You

497
00:22:53,919 --> 00:22:59,120
can also now activate different agents

498
00:22:56,480 --> 00:23:02,000
in in your network. activate multiple

499
00:22:59,120 --> 00:23:04,960
agents and chronicles and you can ask

500
00:23:02,000 --> 00:23:08,080
them a question and observe their

501
00:23:04,960 --> 00:23:10,640
conversation and learn from that. We

502
00:23:08,080 --> 00:23:14,000
recently have a very large media company

503
00:23:10,640 --> 00:23:17,440
working with us to use this of election

504
00:23:14,000 --> 00:23:20,320
debates on TV channels. We are now

505
00:23:17,440 --> 00:23:22,480
working for organizations to prepare for

506
00:23:20,320 --> 00:23:25,200
board meetings when an executive wants

507
00:23:22,480 --> 00:23:27,919
to present to the board or in capital

508
00:23:25,200 --> 00:23:30,640
markets when negotiation happens. You

509
00:23:27,919 --> 00:23:33,360
can now simulate these social behaviors

510
00:23:30,640 --> 00:23:36,880
not just based on one or two but you can

511
00:23:33,360 --> 00:23:39,600
activate tens of agents to do that. The

512
00:23:36,880 --> 00:23:41,600
last thing I say is even the work that

513
00:23:39,600 --> 00:23:43,600
we have done with Deepak Chopra and

514
00:23:41,600 --> 00:23:45,760
programs that we have built called

515
00:23:43,600 --> 00:23:47,760
artificial immortality.

516
00:23:45,760 --> 00:23:51,039
Something is becoming more and more

517
00:23:47,760 --> 00:23:54,559
clear to us. The biggest problem is not

518
00:23:51,039 --> 00:23:58,640
technology. It's us. It's how we

519
00:23:54,559 --> 00:24:01,919
understand emotions, human values,

520
00:23:58,640 --> 00:24:03,919
empathy and incorporating them into AI

521
00:24:01,919 --> 00:24:06,640
systems. And those are also some of the

522
00:24:03,919 --> 00:24:09,760
work that we do not just with technology

523
00:24:06,640 --> 00:24:12,400
people but people who even let's say ran

524
00:24:09,760 --> 00:24:15,039
paliatic care centers for Mount Sinai

525
00:24:12,400 --> 00:24:18,159
hospitals for years and understanding

526
00:24:15,039 --> 00:24:20,480
some of those um learnings and applying

527
00:24:18,159 --> 00:24:22,400
it to AI.

528
00:24:20,480 --> 00:24:25,360
And the last thing is our human AI

529
00:24:22,400 --> 00:24:28,159
empowerment programs at SAPN. We bring

530
00:24:25,360 --> 00:24:30,480
some of the largest organizations to the

531
00:24:28,159 --> 00:24:32,480
lab. We participate with them. We

532
00:24:30,480 --> 00:24:36,000
connect them with our students. We run

533
00:24:32,480 --> 00:24:37,840
hackathons that have Fortune50 CEOs, but

534
00:24:36,000 --> 00:24:40,400
they also have grade three and grade

535
00:24:37,840 --> 00:24:43,200
four students as part of one team

536
00:24:40,400 --> 00:24:45,760
because we believe it's this AI as a

537
00:24:43,200 --> 00:24:48,400
fabric that brings a lot of our uh

538
00:24:45,760 --> 00:24:51,200
capabilities together and bridge our

539
00:24:48,400 --> 00:24:55,039
gaps. And the way we think about it is

540
00:24:51,200 --> 00:24:57,840
that we believe in a hub model. Sapion

541
00:24:55,039 --> 00:25:00,960
is unlike any labs. We connect to

542
00:24:57,840 --> 00:25:04,159
startups, projects, ideas and we create

543
00:25:00,960 --> 00:25:07,200
these eological hubs for everyone to

544
00:25:04,159 --> 00:25:09,679
learn uh from each other and they have

545
00:25:07,200 --> 00:25:12,960
built tools that you can use to really

546
00:25:09,679 --> 00:25:16,480
measure the AI readiness of your company

547
00:25:12,960 --> 00:25:19,360
oration and with that decide what you

548
00:25:16,480 --> 00:25:22,960
want to do next. Thank you very much for

549
00:25:19,360 --> 00:25:25,039
your time and uh I am very uh much

550
00:25:22,960 --> 00:25:28,880
looking forward to staying in touch with

551
00:25:25,039 --> 00:25:32,960
you and a big thank you to our friends

552
00:25:28,880 --> 00:25:34,960
uh at ILP for hosting us.

553
00:25:32,960 --> 00:25:37,440
Thank you so much. This is absolutely

554
00:25:34,960 --> 00:25:39,679
fascinating. Obviously we have uh great

555
00:25:37,440 --> 00:25:43,120
questions. So let's get at least some of

556
00:25:39,679 --> 00:25:45,360
them answer it. Uh question number one.

557
00:25:43,120 --> 00:25:47,120
Normally to get to know someone you

558
00:25:45,360 --> 00:25:50,320
speak to them or you listen to them

559
00:25:47,120 --> 00:25:53,600
speak. Are these AI lenses perspectives

560
00:25:50,320 --> 00:25:56,400
trying to remove social interactions?

561
00:25:53,600 --> 00:25:58,400
Uh absolutely not. I think it's trying

562
00:25:56,400 --> 00:26:02,400
to address some of the challenges that

563
00:25:58,400 --> 00:26:06,080
we have seen before that people try to

564
00:26:02,400 --> 00:26:09,840
replicate people by using linear systems

565
00:26:06,080 --> 00:26:11,679
to incorporate personas and and clusters

566
00:26:09,840 --> 00:26:14,080
and associate people with certain

567
00:26:11,679 --> 00:26:16,320
things. What we are doing with these

568
00:26:14,080 --> 00:26:19,520
chronicles is that with the permission

569
00:26:16,320 --> 00:26:23,120
of the user, we allow the user to go

570
00:26:19,520 --> 00:26:26,720
about their daily lives interact

571
00:26:23,120 --> 00:26:29,279
especially in a work setting and these

572
00:26:26,720 --> 00:26:31,919
agents in the background will learn from

573
00:26:29,279 --> 00:26:34,960
these people. they are in a much better

574
00:26:31,919 --> 00:26:38,480
than learning your preferences and

575
00:26:34,960 --> 00:26:41,200
context than an algorithm deciding for

576
00:26:38,480 --> 00:26:43,120
you and putting you in a cluster just

577
00:26:41,200 --> 00:26:46,000
let's say for the sake of selling you

578
00:26:43,120 --> 00:26:47,840
ads. So uh if you I'm happy to share

579
00:26:46,000 --> 00:26:50,480
some of our papers or you can look them

580
00:26:47,840 --> 00:26:52,960
up on on Google Scholar. That's the

581
00:26:50,480 --> 00:26:56,080
whole point that you trust these

582
00:26:52,960 --> 00:26:58,480
chronicles to learn from you what to

583
00:26:56,080 --> 00:27:00,559
learn from you and they understand let's

584
00:26:58,480 --> 00:27:02,640
say what belongs to you that is personal

585
00:27:00,559 --> 00:27:04,799
that should be completely encrypted

586
00:27:02,640 --> 00:27:08,159
versus what belongs to your organization

587
00:27:04,799 --> 00:27:11,440
and should contribute to uh a broader

588
00:27:08,159 --> 00:27:14,000
collective um chronicle.

589
00:27:11,440 --> 00:27:16,480
Great. Another question, uh, what is the

590
00:27:14,000 --> 00:27:19,679
future role of junior employees with

591
00:27:16,480 --> 00:27:23,039
human AI agents?

592
00:27:19,679 --> 00:27:26,400
Question. One of the funniest LinkedIn

593
00:27:23,039 --> 00:27:28,159
job post I saw two months ago was this

594
00:27:26,400 --> 00:27:30,559
very well-known company that of course

595
00:27:28,159 --> 00:27:34,080
I'm not going to mention their name. Uh,

596
00:27:30,559 --> 00:27:38,159
that was looking for a Genai

597
00:27:34,080 --> 00:27:42,159
expert with experience

598
00:27:38,159 --> 00:27:44,720
doesn't exist. M uh so what I always say

599
00:27:42,159 --> 00:27:46,240
to companies the CEOs the boards of

600
00:27:44,720 --> 00:27:50,159
directors

601
00:27:46,240 --> 00:27:53,200
invest in your new generation of talent.

602
00:27:50,159 --> 00:27:55,200
These kids we grew up with MS DOS. I

603
00:27:53,200 --> 00:27:57,520
don't know who remembers MS DOS but I

604
00:27:55,200 --> 00:28:00,320
remember it like you use prompts to type

605
00:27:57,520 --> 00:28:03,279
and tell the machine what to do. One of

606
00:28:00,320 --> 00:28:05,200
the reasons that chat GPT learning curve

607
00:28:03,279 --> 00:28:07,600
was so low is that we were very familiar

608
00:28:05,200 --> 00:28:09,840
with that interface. The generation that

609
00:28:07,600 --> 00:28:13,120
is coming is not growing up with MSUS.

610
00:28:09,840 --> 00:28:15,600
They're growing up with Roblox. They are

611
00:28:13,120 --> 00:28:19,039
already being trained on multimodal

612
00:28:15,600 --> 00:28:21,520
interface, multitasking, spatial comput.

613
00:28:19,039 --> 00:28:24,000
How as a company you're preparing for

614
00:28:21,520 --> 00:28:27,120
this generation, not just as your

615
00:28:24,000 --> 00:28:29,279
customers, but as people who will truly

616
00:28:27,120 --> 00:28:32,480
move your company forward to the next

617
00:28:29,279 --> 00:28:35,120
generation. So the role for those and

618
00:28:32,480 --> 00:28:37,840
the reason I want to bring K12 and

619
00:28:35,120 --> 00:28:40,480
universities together is that we need to

620
00:28:37,840 --> 00:28:43,600
understand the next generation of our

621
00:28:40,480 --> 00:28:47,039
employees are in high schools now are in

622
00:28:43,600 --> 00:28:50,640
junior schools now invest in them. They

623
00:28:47,039 --> 00:28:52,640
are current junior associate does but

624
00:28:50,640 --> 00:28:54,720
their type of jobs will be very

625
00:28:52,640 --> 00:28:57,840
different and companies who will be the

626
00:28:54,720 --> 00:29:01,200
ones who can identify that right now.

627
00:28:57,840 --> 00:29:02,640
Yeah. Uh we have many more questions. I

628
00:29:01,200 --> 00:29:06,559
have many more questions but

629
00:29:02,640 --> 00:29:06,559
unfortunately we have to move on.

