1
00:00:00,160 --> 00:00:04,400
yeah thanks for that kind introduction

2
00:00:02,120 --> 00:00:06,359
and thanks for inviting me I'm really

3
00:00:04,400 --> 00:00:09,040
excited to be here because I was once a

4
00:00:06,359 --> 00:00:10,719
student at litz as well like Q like over

5
00:00:09,040 --> 00:00:13,360
10 years ago not I'm not going to say

6
00:00:10,719 --> 00:00:15,920
exactly how long I I was advised by Yuri

7
00:00:13,360 --> 00:00:19,720
pansi and Mario madart so I had a foot

8
00:00:15,920 --> 00:00:22,199
at lids and a foot at Rite and when I

9
00:00:19,720 --> 00:00:23,640
look back those were like some of the I

10
00:00:22,199 --> 00:00:26,160
guess some of the best and most

11
00:00:23,640 --> 00:00:28,679
intellectually stimulating times in my

12
00:00:26,160 --> 00:00:30,599
research career so I hope you all enjoy

13
00:00:28,679 --> 00:00:31,840
it and it's great to be back here to

14
00:00:30,599 --> 00:00:34,840
share some of the work that we've been

15
00:00:31,840 --> 00:00:36,440
doing down and now across the river at

16
00:00:34,840 --> 00:00:39,760
the School of Engineering and applied

17
00:00:36,440 --> 00:00:41,600
sciences at Harvard so since this is a

18
00:00:39,760 --> 00:00:43,920
student conference I thought it would

19
00:00:41,600 --> 00:00:45,559
take the first about 10 minutes of the

20
00:00:43,920 --> 00:00:48,000
talk just to say a little bit about what

21
00:00:45,559 --> 00:00:50,039
my research group does at Harvard and

22
00:00:48,000 --> 00:00:51,680
particularly how we work both with

23
00:00:50,039 --> 00:00:54,320
information Theory and also applications

24
00:00:51,680 --> 00:00:56,800
in machine learning AI focusing on a

25
00:00:54,320 --> 00:00:59,640
private responsible design of these

26
00:00:56,800 --> 00:01:01,000
algorithms and one of the reasons we use

27
00:00:59,640 --> 00:01:03,280
information theory is because

28
00:01:01,000 --> 00:01:05,159
information is everywhere in machine

29
00:01:03,280 --> 00:01:07,479
learning when you think about the

30
00:01:05,159 --> 00:01:10,600
information in in a machine learning

31
00:01:07,479 --> 00:01:13,080
algorithm or now I guess an AI algorithm

32
00:01:10,600 --> 00:01:15,280
usually what comes to mind is the data

33
00:01:13,080 --> 00:01:17,520
set the data that you use to train this

34
00:01:15,280 --> 00:01:20,280
algorithm and how information from the

35
00:01:17,520 --> 00:01:22,720
data is somehow mapped on to the weights

36
00:01:20,280 --> 00:01:24,240
of your model in order to produce some

37
00:01:22,720 --> 00:01:27,479
do some kind of classification or

38
00:01:24,240 --> 00:01:29,960
prediction task but the truth is in the

39
00:01:27,479 --> 00:01:31,920
pipeline of Designing these models and

40
00:01:29,960 --> 00:01:33,479
there are many other information sources

41
00:01:31,920 --> 00:01:35,720
for example there design choices that

42
00:01:33,479 --> 00:01:37,360
are made by the people developing those

43
00:01:35,720 --> 00:01:38,960
algorithms like the model class the

44
00:01:37,360 --> 00:01:41,360
number of parameters your regularization

45
00:01:38,960 --> 00:01:42,799
strength your train test split u the

46
00:01:41,360 --> 00:01:45,320
choice of training algorithm and

47
00:01:42,799 --> 00:01:47,640
optimization even the random seed that

48
00:01:45,320 --> 00:01:49,240
you choose to initialize your weights

49
00:01:47,640 --> 00:01:51,040
right determines which local Minima

50
00:01:49,240 --> 00:01:52,640
you're eventually going to converge to

51
00:01:51,040 --> 00:01:54,759
and ultimately how your model is going

52
00:01:52,640 --> 00:01:56,840
to perform and what kind of predictions

53
00:01:54,759 --> 00:02:00,079
it's going to make and all that

54
00:01:56,840 --> 00:02:02,799
information is mapped to the model after

55
00:02:00,079 --> 00:02:04,680
training um and that could lead to

56
00:02:02,799 --> 00:02:07,399
unintended consequences when these

57
00:02:04,680 --> 00:02:10,560
models are deployed U one such

58
00:02:07,399 --> 00:02:12,360
consequence are privacy risks U models

59
00:02:10,560 --> 00:02:14,120
can contain information about the

60
00:02:12,360 --> 00:02:15,440
training data and just by examining

61
00:02:14,120 --> 00:02:17,280
these weights or the input and the

62
00:02:15,440 --> 00:02:19,080
output of the model you might be able to

63
00:02:17,280 --> 00:02:22,239
reverse engineer potentially privacy

64
00:02:19,080 --> 00:02:24,319
sensitive data used during training um

65
00:02:22,239 --> 00:02:27,239
uh such privacy attacks have been widely

66
00:02:24,319 --> 00:02:29,920
documented in the literature U models

67
00:02:27,239 --> 00:02:31,840
can also include biases and have dis

68
00:02:29,920 --> 00:02:33,360
performance on different population

69
00:02:31,840 --> 00:02:36,080
groups specifically even though your

70
00:02:33,360 --> 00:02:38,599
model might be accurate on average when

71
00:02:36,080 --> 00:02:40,800
you evaluate that model on different

72
00:02:38,599 --> 00:02:42,319
groups you might see a disparity of

73
00:02:40,800 --> 00:02:43,959
performance and these kinds of

74
00:02:42,319 --> 00:02:45,879
disparities have also been widely

75
00:02:43,959 --> 00:02:50,400
documented in applications that go from

76
00:02:45,879 --> 00:02:54,000
facial recognition to policing to um to

77
00:02:50,400 --> 00:02:57,000
uh education and so on um generative

78
00:02:54,000 --> 00:02:59,840
models right uh they can produce outputs

79
00:02:57,000 --> 00:03:01,800
that also inaccurately represent uh

80
00:02:59,840 --> 00:03:03,560
population statistic this is one example

81
00:03:01,800 --> 00:03:05,720
that we did in our group where if you

82
00:03:03,560 --> 00:03:08,239
query the uh if you input the query a

83
00:03:05,720 --> 00:03:11,120
photo of a computer program for the ANC

84
00:03:08,239 --> 00:03:13,080
on Google image search uh historically

85
00:03:11,120 --> 00:03:15,360
uh the main programmers of the ANC were

86
00:03:13,080 --> 00:03:17,280
all women but if you uh add the same

87
00:03:15,360 --> 00:03:18,680
prompt to stable diffusion you get a

88
00:03:17,280 --> 00:03:21,480
bunch of pictures black and white

89
00:03:18,680 --> 00:03:24,280
pictures of men um and this could be a

90
00:03:21,480 --> 00:03:26,239
challenge not only from a kind of U

91
00:03:24,280 --> 00:03:27,519
moral sense but this is a challenge also

92
00:03:26,239 --> 00:03:29,040
when you're deploying these models in

93
00:03:27,519 --> 00:03:31,239
practice because when you don't account

94
00:03:29,040 --> 00:03:32,640
for this uh you can get headlines

95
00:03:31,239 --> 00:03:34,920
written about the models they are deplin

96
00:03:32,640 --> 00:03:36,799
in your company like Gemini's AI image

97
00:03:34,920 --> 00:03:39,319
results might offend your users this is

98
00:03:36,799 --> 00:03:42,080
an incident that happened with Gemini's

99
00:03:39,319 --> 00:03:44,000
AI image generation algorithm about a

100
00:03:42,080 --> 00:03:47,080
year ago where these algorithms would in

101
00:03:44,000 --> 00:03:49,360
accurately represent certain populations

102
00:03:47,080 --> 00:03:51,879
and in some cases these models can have

103
00:03:49,360 --> 00:03:54,760
no information at all for certain inputs

104
00:03:51,879 --> 00:03:57,280
uh the model output can be um arbitrary

105
00:03:54,760 --> 00:03:58,920
an artifact of the randomness used

106
00:03:57,280 --> 00:04:00,760
during training this is an another

107
00:03:58,920 --> 00:04:03,640
example that we did of a few years uh

108
00:04:00,760 --> 00:04:05,519
back where um we trained a computional

109
00:04:03,640 --> 00:04:07,360
neur network to detect different

110
00:04:05,519 --> 00:04:09,920
spectrograms and detect what kind of

111
00:04:07,360 --> 00:04:11,599
sound is in that spectrogram and simply

112
00:04:09,920 --> 00:04:13,040
by changing the random initialization of

113
00:04:11,599 --> 00:04:16,199
training you keep everything the same

114
00:04:13,040 --> 00:04:18,000
the data is the same the the the model

115
00:04:16,199 --> 00:04:20,680
architecture is the same and so on but

116
00:04:18,000 --> 00:04:22,400
just changing the random seed that uh

117
00:04:20,680 --> 00:04:25,199
you use to initialize your training

118
00:04:22,400 --> 00:04:27,240
process uh you end up with models that

119
00:04:25,199 --> 00:04:29,160
have different weights that on average

120
00:04:27,240 --> 00:04:31,919
have approximately the same performance

121
00:04:29,160 --> 00:04:35,320
but on individual samples they make

122
00:04:31,919 --> 00:04:37,440
conflicting predictions um and sure in

123
00:04:35,320 --> 00:04:38,840
an application like uh audio recognition

124
00:04:37,440 --> 00:04:41,320
that might not seem like a big deal

125
00:04:38,840 --> 00:04:43,160
that's known as the Rashon effect but

126
00:04:41,320 --> 00:04:44,520
when you look at other applications of

127
00:04:43,160 --> 00:04:46,759
social consequence like content

128
00:04:44,520 --> 00:04:48,400
moderation and so on um decisions

129
00:04:46,759 --> 00:04:50,919
supported by Machine learning models may

130
00:04:48,400 --> 00:04:53,360
depend on arbitrary and unjustified

131
00:04:50,919 --> 00:04:55,320
choices during their

132
00:04:53,360 --> 00:04:57,000
development so what I'm going to do

133
00:04:55,320 --> 00:04:59,720
today is to talk a little bit about some

134
00:04:57,000 --> 00:05:02,600
of these challenges and how we um use

135
00:04:59,720 --> 00:05:06,000
tools from um information Theory to uh

136
00:05:02,600 --> 00:05:08,320
address challenges in privacy uh In

137
00:05:06,000 --> 00:05:10,720
fairness and in arbitrariness

138
00:05:08,320 --> 00:05:13,240
information theory is a tool set of

139
00:05:10,720 --> 00:05:15,000
choice um because information Theory

140
00:05:13,240 --> 00:05:17,280
allows us to delineate the limits of

141
00:05:15,000 --> 00:05:19,319
these machine learning systems that are

142
00:05:17,280 --> 00:05:21,400
independent of specific models

143
00:05:19,319 --> 00:05:25,400
applications and semantics we can really

144
00:05:21,400 --> 00:05:28,240
try to get at some of the heart of these

145
00:05:25,400 --> 00:05:29,840
problems so and in order to understand

146
00:05:28,240 --> 00:05:31,400
how we apply information theory in the

147
00:05:29,840 --> 00:05:33,080
cases there are two questions that I'm

148
00:05:31,400 --> 00:05:34,800
going to answer if you're already

149
00:05:33,080 --> 00:05:36,680
already taken an information Theory

150
00:05:34,800 --> 00:05:38,840
class here at uh MIT this is going to be

151
00:05:36,680 --> 00:05:40,440
pretty obvious for you um so just sit

152
00:05:38,840 --> 00:05:42,479
back back and relax if not you're going

153
00:05:40,440 --> 00:05:44,600
to learn something new and we have to

154
00:05:42,479 --> 00:05:47,319
answer how do we measure information and

155
00:05:44,600 --> 00:05:49,600
how do we model information right so how

156
00:05:47,319 --> 00:05:53,680
can we measure information how do we

157
00:05:49,600 --> 00:05:56,000
quantify it so information in

158
00:05:53,680 --> 00:05:56,919
information theory is measured as a

159
00:05:56,000 --> 00:05:59,280
change in

160
00:05:56,919 --> 00:06:00,800
uncertainty so if I tell you that the

161
00:05:59,280 --> 00:06:02,360
Sun will rise tomorrow that's a

162
00:06:00,800 --> 00:06:04,319
statement that's significantly less

163
00:06:02,360 --> 00:06:06,759
informative than when I tell you that

164
00:06:04,319 --> 00:06:09,840
the outcome of the dice row of a row of

165
00:06:06,759 --> 00:06:11,039
truece the sum of the faces of chudis 12

166
00:06:09,840 --> 00:06:13,080
and the reason why the first statement

167
00:06:11,039 --> 00:06:15,440
is less informative than the second is

168
00:06:13,080 --> 00:06:17,319
because I'm pretty sure the sun will

169
00:06:15,440 --> 00:06:19,759
rise tomorrow right with probability

170
00:06:17,319 --> 00:06:22,199
overwhelmingly close to one um but if

171
00:06:19,759 --> 00:06:24,039
you ask me what's my prior belief of the

172
00:06:22,199 --> 00:06:26,039
outcome of the sum of uh the roow of

173
00:06:24,039 --> 00:06:28,639
true dice I have much more uncertainty

174
00:06:26,039 --> 00:06:30,120
and once I observe that dice row that U

175
00:06:28,639 --> 00:06:33,160
distribution collapses

176
00:06:30,120 --> 00:06:35,000
to a single distribution here so in

177
00:06:33,160 --> 00:06:37,199
general information is measured and

178
00:06:35,000 --> 00:06:39,280
Quantified in terms of these difference

179
00:06:37,199 --> 00:06:41,360
between beliefs as represented by

180
00:06:39,280 --> 00:06:45,639
probability distributions between the

181
00:06:41,360 --> 00:06:47,479
state of a system and in and one way of

182
00:06:45,639 --> 00:06:49,120
quantifying this change in distributions

183
00:06:47,479 --> 00:06:51,160
is using something called a Divergence

184
00:06:49,120 --> 00:06:53,000
measure and there is a world of

185
00:06:51,160 --> 00:06:55,800
divergences to choose from there's

186
00:06:53,000 --> 00:06:58,840
integral probability metrics uh F

187
00:06:55,800 --> 00:07:00,280
divergences regman divergences and so on

188
00:06:58,840 --> 00:07:02,160
uh should they this talk we're going to

189
00:07:00,280 --> 00:07:04,919
spend a lot of time talking about F

190
00:07:02,160 --> 00:07:06,960
divergences these are divergences of the

191
00:07:04,919 --> 00:07:09,240
following form where if you're give me

192
00:07:06,960 --> 00:07:11,240
true distributions p and q i compute the

193
00:07:09,240 --> 00:07:15,319
expected value in Q of this function of

194
00:07:11,240 --> 00:07:17,479
the uh likelihood ratio between p and Q

195
00:07:15,319 --> 00:07:20,039
and there are many divergences of this

196
00:07:17,479 --> 00:07:21,520
type that depend on this function f um

197
00:07:20,039 --> 00:07:24,240
our constraint is that this function f

198
00:07:21,520 --> 00:07:27,720
is convex that at one it's evaluated to

199
00:07:24,240 --> 00:07:29,160
zero and F is also finite for all T

200
00:07:27,720 --> 00:07:31,160
greater than zero like some of these

201
00:07:29,160 --> 00:07:33,360
diver es you've probably heard of like K

202
00:07:31,160 --> 00:07:35,919
Divergence you can recover it for f

203
00:07:33,360 --> 00:07:37,319
equals t loog t AMA Divergence this is

204
00:07:35,919 --> 00:07:40,639
going to be a very important Divergence

205
00:07:37,319 --> 00:07:41,840
for us toal variation K square and so on

206
00:07:40,639 --> 00:07:43,160
and what we're going to be doing in a

207
00:07:41,840 --> 00:07:44,879
few minutes is that we're going to be

208
00:07:43,160 --> 00:07:48,639
analyzing challenges in privacy and

209
00:07:44,879 --> 00:07:50,919
fairness in machine learning using these

210
00:07:48,639 --> 00:07:52,720
divergences so we can use these

211
00:07:50,919 --> 00:07:54,560
Divergence metrics between distributions

212
00:07:52,720 --> 00:07:57,080
to measure information to measure these

213
00:07:54,560 --> 00:07:58,960
changes and beliefs now the second

214
00:07:57,080 --> 00:08:01,680
question that I ask is how do we model

215
00:07:58,960 --> 00:08:04,199
information in machine learning um so in

216
00:08:01,680 --> 00:08:06,080
this case what we do is that we measure

217
00:08:04,199 --> 00:08:08,680
uh data sets or we represent and model

218
00:08:06,080 --> 00:08:11,039
data sets as samples from a sar p ofx

219
00:08:08,680 --> 00:08:12,840
and we when we think about a model or a

220
00:08:11,039 --> 00:08:14,400
training algorithm we think about them

221
00:08:12,840 --> 00:08:16,159
as a channel you can think about an

222
00:08:14,400 --> 00:08:18,280
algorithm for example a Transformer

223
00:08:16,159 --> 00:08:20,280
model as a channel where a sample comes

224
00:08:18,280 --> 00:08:22,159
in or a text comes in and the output is

225
00:08:20,280 --> 00:08:23,960
a probability over the next token or a

226
00:08:22,159 --> 00:08:25,840
classifier as a channel as well where an

227
00:08:23,960 --> 00:08:27,960
image comes in and the output has a

228
00:08:25,840 --> 00:08:29,080
probability distribution over classes we

229
00:08:27,960 --> 00:08:30,639
can also think about a training

230
00:08:29,080 --> 00:08:32,680
algorithm as a channel randomized

231
00:08:30,639 --> 00:08:34,680
training algorithm a data set comes in

232
00:08:32,680 --> 00:08:37,159
and the output is a distribution over

233
00:08:34,680 --> 00:08:40,440
weights and this abstraction allows us

234
00:08:37,159 --> 00:08:42,560
to um do an analysis that is uh

235
00:08:40,440 --> 00:08:45,240
independent of semantics and specific

236
00:08:42,560 --> 00:08:47,600
applications and also find patterns and

237
00:08:45,240 --> 00:08:51,360
questions of privacy and fairness that

238
00:08:47,600 --> 00:08:54,720
hold across model classes and training

239
00:08:51,360 --> 00:08:56,680
algorithms so these information measures

240
00:08:54,720 --> 00:08:58,160
and models they're mathematical

241
00:08:56,680 --> 00:09:00,480
definitions right and historically

242
00:08:58,160 --> 00:09:02,800
they've been used to quantify the limits

243
00:09:00,480 --> 00:09:05,000
of many engineering and Computing

244
00:09:02,800 --> 00:09:06,560
systems and the use of tools from

245
00:09:05,000 --> 00:09:09,839
information theory of course dates back

246
00:09:06,560 --> 00:09:11,760
to Claude Shannon and what his

247
00:09:09,839 --> 00:09:14,200
breakthroughs allowed among many other

248
00:09:11,760 --> 00:09:16,240
things is for us to move away from ad

249
00:09:14,200 --> 00:09:18,360
hoc engineering from like designing

250
00:09:16,240 --> 00:09:20,160
systems that transmit and communicate

251
00:09:18,360 --> 00:09:22,600
data that were tailored for different

252
00:09:20,160 --> 00:09:23,920
kinds of specific applications and

253
00:09:22,600 --> 00:09:25,720
really get it at the heart of the

254
00:09:23,920 --> 00:09:28,160
problem and distill the problem down to

255
00:09:25,720 --> 00:09:29,560
like a common little model that was

256
00:09:28,160 --> 00:09:31,240
common across these different

257
00:09:29,560 --> 00:09:33,240
applications in this case the question

258
00:09:31,240 --> 00:09:34,839
of um data compression and data

259
00:09:33,240 --> 00:09:36,760
transmission could be boiled down to

260
00:09:34,839 --> 00:09:38,920
figure one in his paper where you have

261
00:09:36,760 --> 00:09:41,120
an information Source a transmitter a

262
00:09:38,920 --> 00:09:43,320
receiver and so on and by analyzing

263
00:09:41,120 --> 00:09:45,640
these models you can find uh he found

264
00:09:43,320 --> 00:09:46,959
limits of how far you can compress data

265
00:09:45,640 --> 00:09:48,480
and limits on how fast you can

266
00:09:46,959 --> 00:09:52,040
communicate or how many bits you can

267
00:09:48,480 --> 00:09:54,160
transmit per Channel use and obviously

268
00:09:52,040 --> 00:09:56,240
there is a gap between theory and

269
00:09:54,160 --> 00:09:58,760
practice right you can't say that oh

270
00:09:56,240 --> 00:10:00,880
here's a bunch of diagram blocks in a

271
00:09:58,760 --> 00:10:02,519
PowerPoint slide and by there and just

272
00:10:00,880 --> 00:10:05,720
by looking at that we can understand how

273
00:10:02,519 --> 00:10:08,800
to engineer a Wi-Fi router right but I

274
00:10:05,720 --> 00:10:10,240
think for me and um the power of

275
00:10:08,800 --> 00:10:13,000
information Theory and this is something

276
00:10:10,240 --> 00:10:15,440
I learned at at Lids is that it gives us

277
00:10:13,000 --> 00:10:19,360
operational limits that engineers and

278
00:10:15,440 --> 00:10:22,200
computer scientists can aim for and for

279
00:10:19,360 --> 00:10:24,160
the next uh for the following decades

280
00:10:22,200 --> 00:10:25,760
after Shannon introduced his paper folks

281
00:10:24,160 --> 00:10:27,480
have been reaching for these limits

282
00:10:25,760 --> 00:10:30,600
right developing better coding schemes

283
00:10:27,480 --> 00:10:32,839
better communication protocols and so on

284
00:10:30,600 --> 00:10:37,680
um these Technologies led to the

285
00:10:32,839 --> 00:10:40,600
widespread deployment of meth of um of

286
00:10:37,680 --> 00:10:45,079
of algorithms and devices that collect

287
00:10:40,600 --> 00:10:47,240
and store data at a global scale and

288
00:10:45,079 --> 00:10:48,920
since early 2000s this data has been

289
00:10:47,240 --> 00:10:51,040
fueling the fast development of machine

290
00:10:48,920 --> 00:10:53,320
learning which today exposes many

291
00:10:51,040 --> 00:10:55,079
emerging challenges and what I do in my

292
00:10:53,320 --> 00:10:57,040
research what my research group does is

293
00:10:55,079 --> 00:11:00,040
that we ask can we apply information

294
00:10:57,040 --> 00:11:02,040
Theory to address these

295
00:11:00,040 --> 00:11:04,360
challenges and this is very much in the

296
00:11:02,040 --> 00:11:06,480
spirit of the field like if you this is

297
00:11:04,360 --> 00:11:09,160
a picture borrowed or adapted from

298
00:11:06,480 --> 00:11:10,839
cover's book where um information Theory

299
00:11:09,160 --> 00:11:12,240
historically has found connections with

300
00:11:10,839 --> 00:11:13,760
many different fields that range from

301
00:11:12,240 --> 00:11:15,760
probability Theory to statistics

302
00:11:13,760 --> 00:11:18,240
communication theory of course physics

303
00:11:15,760 --> 00:11:20,839
computer science and economics and what

304
00:11:18,240 --> 00:11:23,399
my research does is that uh we' add

305
00:11:20,839 --> 00:11:25,000
another petal to this flower and ask how

306
00:11:23,399 --> 00:11:26,880
can we apply information Theory to

307
00:11:25,000 --> 00:11:29,000
understand these emerging challenges in

308
00:11:26,880 --> 00:11:32,120
machine learning related to questions of

309
00:11:29,000 --> 00:11:34,639
private fairness arbitrariness and so

310
00:11:32,120 --> 00:11:36,560
on so I want to mention one last

311
00:11:34,639 --> 00:11:40,360
Divergence which is this Divergence

312
00:11:36,560 --> 00:11:42,920
between um theory and practice like I

313
00:11:40,360 --> 00:11:45,160
mentioned um I'm not going to claim that

314
00:11:42,920 --> 00:11:46,480
just by looking at simple models you can

315
00:11:45,160 --> 00:11:49,320
understand the complexity of like

316
00:11:46,480 --> 00:11:51,639
regulating 5G networks right this is

317
00:11:49,320 --> 00:11:53,600
this is a much more complicated problem

318
00:11:51,639 --> 00:11:56,079
and in the same way just by looking at

319
00:11:53,600 --> 00:11:57,639
these mathematical models behind privacy

320
00:11:56,079 --> 00:11:59,760
and behind questions of fairness that

321
00:11:57,639 --> 00:12:03,120
doesn't perfectly as far from perfectly

322
00:11:59,760 --> 00:12:04,880
capturing a regulations be uh uh in

323
00:12:03,120 --> 00:12:06,680
surrounding the topic or even the

324
00:12:04,880 --> 00:12:08,720
personal feeling that someone has when

325
00:12:06,680 --> 00:12:12,000
they're discriminated by by an

326
00:12:08,720 --> 00:12:13,839
algorithm but theory is um only one

327
00:12:12,000 --> 00:12:15,959
piece of this puzzle right but it's an

328
00:12:13,839 --> 00:12:17,920
important piece it's an important piece

329
00:12:15,959 --> 00:12:19,800
because by looking at Theory it helps us

330
00:12:17,920 --> 00:12:22,079
understand the limits of these systems

331
00:12:19,800 --> 00:12:23,760
optimize and critique their design and

332
00:12:22,079 --> 00:12:27,600
create algorithms and cods that

333
00:12:23,760 --> 00:12:29,279
massively outperform horis sixs um and

334
00:12:27,600 --> 00:12:31,399
for me the most precious moments in

335
00:12:29,279 --> 00:12:34,720
research is when we can make these this

336
00:12:31,399 --> 00:12:37,360
Divergence between theory and practice

337
00:12:34,720 --> 00:12:39,680
small so that's what we're going to talk

338
00:12:37,360 --> 00:12:41,120
about next um now we're going to talk a

339
00:12:39,680 --> 00:12:43,000
little bit about privacy and fairness I

340
00:12:41,120 --> 00:12:44,320
won't have time to talk about our work

341
00:12:43,000 --> 00:12:45,959
on predictive multiplicity and

342
00:12:44,320 --> 00:12:47,720
arbitrariness I'll leave that for next

343
00:12:45,959 --> 00:12:50,519
time you can look up our papers if

344
00:12:47,720 --> 00:12:54,040
you're interested in that um but for the

345
00:12:50,519 --> 00:12:55,600
next about 40 minutes uh maybe a little

346
00:12:54,040 --> 00:12:57,920
bit less we're going to be talking about

347
00:12:55,600 --> 00:13:01,240
design and limits of privacy mechanisms

348
00:12:57,920 --> 00:13:04,160
and design of limits of group fairness

349
00:13:01,240 --> 00:13:07,279
interventions wait are youall with me

350
00:13:04,160 --> 00:13:10,399
yeah cool so let's start with a little

351
00:13:07,279 --> 00:13:12,959
bit about design and limits of privacy

352
00:13:10,399 --> 00:13:15,160
mechanisms so the flavor of privacy that

353
00:13:12,959 --> 00:13:17,000
we're going to be looking at today is

354
00:13:15,160 --> 00:13:19,959
differential privacy who here has heard

355
00:13:17,000 --> 00:13:22,680
about differential privacy he quite a

356
00:13:19,959 --> 00:13:24,760
few of you great so if you haven't the

357
00:13:22,680 --> 00:13:27,560
core idea behind differential privacy is

358
00:13:24,760 --> 00:13:29,240
the following you have a data set D and

359
00:13:27,560 --> 00:13:31,199
you compute a query over this data set

360
00:13:29,240 --> 00:13:33,519
set for example you ask what's the

361
00:13:31,199 --> 00:13:36,639
number of chefs or the average number of

362
00:13:33,519 --> 00:13:38,760
chefs in this data set and the goal of

363
00:13:36,639 --> 00:13:41,480
this uh of differential privacy is to

364
00:13:38,760 --> 00:13:43,720
measure how much your belief about the

365
00:13:41,480 --> 00:13:46,240
data set changes or how much this output

366
00:13:43,720 --> 00:13:48,160
distribution changes uh if I change only

367
00:13:46,240 --> 00:13:50,079
one entry of the data set say I change

368
00:13:48,160 --> 00:13:52,120
the data of the of the chef for an

369
00:13:50,079 --> 00:13:54,680
engineer here so the goal is that a

370
00:13:52,120 --> 00:13:58,120
quiry does not significantly depend on

371
00:13:54,680 --> 00:14:00,600
any single individual's data and the way

372
00:13:58,120 --> 00:14:02,040
you can Ensure privacy in practice how

373
00:14:00,600 --> 00:14:04,000
differential privacy is insured in

374
00:14:02,040 --> 00:14:06,079
practice is by engineering this green

375
00:14:04,000 --> 00:14:08,199
box here something known as a privacy

376
00:14:06,079 --> 00:14:09,800
mechanism you can think about a privacy

377
00:14:08,199 --> 00:14:12,639
mechanism as a conditional distribution

378
00:14:09,800 --> 00:14:15,160
or a channel where the input is say a

379
00:14:12,639 --> 00:14:17,199
query or maybe a data set itself and the

380
00:14:15,160 --> 00:14:19,519
output is a randomized version of that

381
00:14:17,199 --> 00:14:21,560
query the simplest form of ensuring

382
00:14:19,519 --> 00:14:23,639
privacy is by adding noise to your query

383
00:14:21,560 --> 00:14:25,480
so I you ask me how many chefs are in

384
00:14:23,639 --> 00:14:27,399
this data set then I respond a number

385
00:14:25,480 --> 00:14:29,800
plus some gaussian noise that was

386
00:14:27,399 --> 00:14:31,880
calibrated in order to hide the

387
00:14:29,800 --> 00:14:34,440
dependence in terms of individual

388
00:14:31,880 --> 00:14:36,320
samples and in general this differential

389
00:14:34,440 --> 00:14:38,920
privacy can be measured abstractly in

390
00:14:36,320 --> 00:14:41,000
terms of a Divergence right in in terms

391
00:14:38,920 --> 00:14:45,720
of the change in your output

392
00:14:41,000 --> 00:14:50,120
distribution across neighboring inputs

393
00:14:45,720 --> 00:14:51,720
here so um the way this is uh

394
00:14:50,120 --> 00:14:53,480
differential one of the most popular

395
00:14:51,720 --> 00:14:55,720
flavors of differential privacy is known

396
00:14:53,480 --> 00:14:57,839
as Epsilon Delta differential privacy or

397
00:14:55,720 --> 00:14:59,680
approximate differential privacy where

398
00:14:57,839 --> 00:15:01,839
we say that this green box here this

399
00:14:59,680 --> 00:15:04,160
privacy mechanism is differentially

400
00:15:01,839 --> 00:15:06,079
private satisfies Epsilon Delta

401
00:15:04,160 --> 00:15:08,199
differential privacy for every Epsilon

402
00:15:06,079 --> 00:15:10,800
greater than zero and Delta between Z

403
00:15:08,199 --> 00:15:14,160
and one uh if for all neighboring data

404
00:15:10,800 --> 00:15:16,360
sets the probability of any output event

405
00:15:14,160 --> 00:15:18,560
doesn't change by more than an additive

406
00:15:16,360 --> 00:15:22,639
factor of Delta and a multiplicative

407
00:15:18,560 --> 00:15:24,759
factor of e to the Epsilon so for the

408
00:15:22,639 --> 00:15:27,040
rest of the stock just to simplify a

409
00:15:24,759 --> 00:15:29,800
little bit the notation we're going to

410
00:15:27,040 --> 00:15:33,000
substitute the condition of neighboring

411
00:15:29,800 --> 00:15:34,600
in terms of um uh neighboring data sets

412
00:15:33,000 --> 00:15:36,720
inducing neighboring queries so if your

413
00:15:34,600 --> 00:15:38,680
data sets are neighboring your query

414
00:15:36,720 --> 00:15:40,720
doesn't change my the norm of your query

415
00:15:38,680 --> 00:15:44,079
doesn't change more by a sensitivity

416
00:15:40,720 --> 00:15:47,000
parameter s and in this case we can

417
00:15:44,079 --> 00:15:50,120
express differential privacy in terms of

418
00:15:47,000 --> 00:15:52,519
one specific uh F Divergence known as

419
00:15:50,120 --> 00:15:54,480
the egma or hockey stick Divergence

420
00:15:52,519 --> 00:15:57,199
where we choose our function f to be a

421
00:15:54,480 --> 00:15:59,959
function that looks like a hockey

422
00:15:57,199 --> 00:16:01,440
stick and in this case you can rewrite

423
00:15:59,959 --> 00:16:04,079
the differential privacy in a form that

424
00:16:01,440 --> 00:16:06,079
is not as common but um is entirely

425
00:16:04,079 --> 00:16:07,720
equivalent where we say that a privacy

426
00:16:06,079 --> 00:16:10,079
mechanism is differentially private if

427
00:16:07,720 --> 00:16:11,639
the egma Divergence or this hockey stick

428
00:16:10,079 --> 00:16:13,759
Divergence a kind of f Divergence

429
00:16:11,639 --> 00:16:15,680
between neighboring distributions is not

430
00:16:13,759 --> 00:16:19,360
greater for a parameter each of the

431
00:16:15,680 --> 00:16:20,959
Epsilon is not greater than Delta and

432
00:16:19,360 --> 00:16:23,199
one thing that's really important is

433
00:16:20,959 --> 00:16:25,279
that any privacy mechanism is

434
00:16:23,199 --> 00:16:27,839
characterized by a range of these

435
00:16:25,279 --> 00:16:30,040
Epsilon Delta values in practice when

436
00:16:27,839 --> 00:16:33,199
you uh read about differential privacy

437
00:16:30,040 --> 00:16:35,480
disclosures usually folks uh fix Delta

438
00:16:33,199 --> 00:16:37,880
to be a small value like 10 to Theus 6

439
00:16:35,480 --> 00:16:40,120
or 10 to Theus 7 usually one over the

440
00:16:37,880 --> 00:16:42,240
size of the data set and Report the

441
00:16:40,120 --> 00:16:44,639
corresponding Epsilon but it's important

442
00:16:42,240 --> 00:16:46,519
for you to understand that any privacy

443
00:16:44,639 --> 00:16:48,480
mechanism any one of these channels is

444
00:16:46,519 --> 00:16:50,240
characterized by a range of these

445
00:16:48,480 --> 00:16:52,240
Epsilon and Delta values and ideally we

446
00:16:50,240 --> 00:16:54,240
want to make these very small smaller

447
00:16:52,240 --> 00:16:55,759
they are the more private your mechanism

448
00:16:54,240 --> 00:16:57,199
right the closer true neighboring

449
00:16:55,759 --> 00:17:00,440
distributions

450
00:16:57,199 --> 00:17:03,560
are and differential privacy is widely

451
00:17:00,440 --> 00:17:05,640
adopted uh I think it's not an underst

452
00:17:03,560 --> 00:17:07,520
understatement to say that in many

453
00:17:05,640 --> 00:17:10,039
places in applied machine learning in

454
00:17:07,520 --> 00:17:12,600
Industry differential privacy has become

455
00:17:10,039 --> 00:17:15,919
the standard privacy metric uh it's

456
00:17:12,600 --> 00:17:18,199
adopted at Google at meta at Apple uh in

457
00:17:15,919 --> 00:17:21,319
fact it was adopted also by the US

458
00:17:18,199 --> 00:17:24,839
government in the 2020 desial

459
00:17:21,319 --> 00:17:26,439
census so the goal and the question that

460
00:17:24,839 --> 00:17:28,199
I'm going to be exploring in the rest of

461
00:17:26,439 --> 00:17:30,080
this part of the talk is how can we

462
00:17:28,199 --> 00:17:32,160
ensure private Acy and how can we

463
00:17:30,080 --> 00:17:34,600
measure and understand the behavior of

464
00:17:32,160 --> 00:17:36,919
these privacy guarantees when you access

465
00:17:34,600 --> 00:17:38,600
data multiple times because when you

466
00:17:36,919 --> 00:17:40,880
apply a privacy mechanism you usually

467
00:17:38,600 --> 00:17:43,320
don't apply it only once right you have

468
00:17:40,880 --> 00:17:44,520
your data set and you ask a query and

469
00:17:43,320 --> 00:17:46,320
you feed it through your privacy

470
00:17:44,520 --> 00:17:47,960
mechanism and you get a response but

471
00:17:46,320 --> 00:17:49,799
then you're not done usually that that

472
00:17:47,960 --> 00:17:51,600
data set is hosted somewhere or you're

473
00:17:49,799 --> 00:17:53,360
using acquiring this data multiple times

474
00:17:51,600 --> 00:17:56,240
to train a machine learning model and

475
00:17:53,360 --> 00:17:58,679
you ask multiple queries right either by

476
00:17:56,240 --> 00:18:01,880
the same or by different users and every

477
00:17:58,679 --> 00:18:04,320
time time this data is accessed your

478
00:18:01,880 --> 00:18:07,159
privacy curve becomes looser and looser

479
00:18:04,320 --> 00:18:10,600
like your privacy guarantees they become

480
00:18:07,159 --> 00:18:13,799
less and less strict so one example

481
00:18:10,600 --> 00:18:15,280
where you data is accessed uh uh tens of

482
00:18:13,799 --> 00:18:17,559
thousands or hundreds of thousands of

483
00:18:15,280 --> 00:18:19,080
times and a privacy mechanism is applied

484
00:18:17,559 --> 00:18:20,520
many times is when you're doing

485
00:18:19,080 --> 00:18:22,039
differentially private training of

486
00:18:20,520 --> 00:18:23,760
machine learning models like when you're

487
00:18:22,039 --> 00:18:26,480
doing differentially private stochastic

488
00:18:23,760 --> 00:18:29,600
gradient descent in this case each query

489
00:18:26,480 --> 00:18:32,280
for the data set is a computation of a

490
00:18:29,600 --> 00:18:34,400
gradient at a given parameter point and

491
00:18:32,280 --> 00:18:36,159
a popular algorithm is dpsg where you

492
00:18:34,400 --> 00:18:38,679
clip your gradient and you add the

493
00:18:36,159 --> 00:18:39,960
gussan noise in order to ensure privacy

494
00:18:38,679 --> 00:18:41,679
and you can assume that in this case

495
00:18:39,960 --> 00:18:43,880
this privacy mechanism in this case

496
00:18:41,679 --> 00:18:45,919
clipping the gradient and adding noise

497
00:18:43,880 --> 00:18:47,960
is applied T of thousands or hundreds of

498
00:18:45,919 --> 00:18:51,320
thousands or maybe millions of times

499
00:18:47,960 --> 00:18:52,840
during training so the idea that we had

500
00:18:51,320 --> 00:18:54,799
me and my

501
00:18:52,840 --> 00:18:57,559
collaborators was well how can we

502
00:18:54,799 --> 00:18:59,280
optimize privacy mechanisms for this

503
00:18:57,559 --> 00:19:01,400
High composition Reg

504
00:18:59,280 --> 00:19:03,360
right so if we're designing these green

505
00:19:01,400 --> 00:19:05,640
boxes these privacy mechanisms that add

506
00:19:03,360 --> 00:19:08,840
noise to the data how should we engineer

507
00:19:05,640 --> 00:19:11,880
them so that we can have the best

508
00:19:08,840 --> 00:19:13,480
privacy for the best utility possible

509
00:19:11,880 --> 00:19:15,159
and in order to answer that we need to

510
00:19:13,480 --> 00:19:19,320
answer two questions the first question

511
00:19:15,159 --> 00:19:21,039
is well how do we measure uh privacy

512
00:19:19,320 --> 00:19:23,400
loss under many compositions how does

513
00:19:21,039 --> 00:19:26,080
this privacy curve behave right how do

514
00:19:23,400 --> 00:19:28,480
these divergences increase and the

515
00:19:26,080 --> 00:19:30,200
second question is shall how should we

516
00:19:28,480 --> 00:19:31,960
design design privacy mechanisms that

517
00:19:30,200 --> 00:19:33,240
operate under many compositions once we

518
00:19:31,960 --> 00:19:35,200
can measure privacy when you're

519
00:19:33,240 --> 00:19:37,159
accessing data multiple times how should

520
00:19:35,200 --> 00:19:40,360
you optimize the noise that you add to

521
00:19:37,159 --> 00:19:43,679
the data in order to achieve the best

522
00:19:40,360 --> 00:19:46,080
utility so the first question that we're

523
00:19:43,679 --> 00:19:48,679
going to look at is this one is how do

524
00:19:46,080 --> 00:19:50,880
we measure privacy laws under many

525
00:19:48,679 --> 00:19:52,760
compositions so in order to understand

526
00:19:50,880 --> 00:19:55,080
this we need a slightly technical uh

527
00:19:52,760 --> 00:19:57,440
Mark technical definition which is this

528
00:19:55,080 --> 00:19:59,600
notion of a universal privacy curve like

529
00:19:57,440 --> 00:20:01,520
at the end of the day what we want to

530
00:19:59,600 --> 00:20:04,039
understand is the worst case Divergence

531
00:20:01,520 --> 00:20:06,159
across these two distributions for the

532
00:20:04,039 --> 00:20:09,240
worst case pair of possible queries that

533
00:20:06,159 --> 00:20:11,039
someone can ask and we want you

534
00:20:09,240 --> 00:20:13,640
understand how this privacy curve

535
00:20:11,039 --> 00:20:15,840
behaves and how it increases as you ask

536
00:20:13,640 --> 00:20:18,000
more and more and more and more car

537
00:20:15,840 --> 00:20:20,200
queries and ever since you have

538
00:20:18,000 --> 00:20:23,120
differential privacy there are methods

539
00:20:20,200 --> 00:20:24,720
that do privacy accounting which is

540
00:20:23,120 --> 00:20:26,760
essentially this process of keeping

541
00:20:24,720 --> 00:20:29,960
track of your privacy

542
00:20:26,760 --> 00:20:31,880
loss um the first methods were basic

543
00:20:29,960 --> 00:20:33,320
composition results so basically what

544
00:20:31,880 --> 00:20:35,720
they did is that instead of considering

545
00:20:33,320 --> 00:20:38,600
an entirely entire privacy curve they

546
00:20:35,720 --> 00:20:40,640
considered uh for each use of the

547
00:20:38,600 --> 00:20:43,320
mechanism a specific pair of Epsilon and

548
00:20:40,640 --> 00:20:45,000
Delta VAR values and asked well how can

549
00:20:43,320 --> 00:20:47,960
we combine these Epsilon and Delta

550
00:20:45,000 --> 00:20:49,720
values in order to get a new or an

551
00:20:47,960 --> 00:20:51,760
overall privacy guarantee you can think

552
00:20:49,720 --> 00:20:54,320
about this as well each time I use a

553
00:20:51,760 --> 00:20:57,159
mechanism I report a given Epsilon Delta

554
00:20:54,320 --> 00:20:59,480
and now if I query the data say a 100

555
00:20:57,159 --> 00:21:02,840
times how can I combine these in Deltas

556
00:20:59,480 --> 00:21:05,679
to get a guarantee on my privacy

557
00:21:02,840 --> 00:21:08,480
leakage over the past uh especially over

558
00:21:05,679 --> 00:21:11,159
the past 5 years for years you've seen a

559
00:21:08,480 --> 00:21:14,000
flurry of work that tries to refine this

560
00:21:11,159 --> 00:21:16,279
analysis by considering the uh the

561
00:21:14,000 --> 00:21:18,360
entire privacy curve like uh there are

562
00:21:16,279 --> 00:21:20,679
large deviations approaches CLT based

563
00:21:18,360 --> 00:21:22,799
approaches numerical approaches and

564
00:21:20,679 --> 00:21:25,240
specifically I'm going to focus on one

565
00:21:22,799 --> 00:21:27,360
approach we presented at icml a few

566
00:21:25,240 --> 00:21:30,320
years back called the sample Point

567
00:21:27,360 --> 00:21:33,039
accountant and really what made these

568
00:21:30,320 --> 00:21:36,200
methods work is that they focus on the

569
00:21:33,039 --> 00:21:38,600
concentration of one specific random

570
00:21:36,200 --> 00:21:40,679
variable known as the Privacy loss

571
00:21:38,600 --> 00:21:42,200
random variable so bear with me on this

572
00:21:40,679 --> 00:21:44,120
because if you understand this part

573
00:21:42,200 --> 00:21:47,480
you're going to understand really what's

574
00:21:44,120 --> 00:21:51,480
at the heart of differential

575
00:21:47,480 --> 00:21:53,880
privacy so what is this privacy loss

576
00:21:51,480 --> 00:21:55,880
random variable right so remember that

577
00:21:53,880 --> 00:21:57,919
we have this green box this green box is

578
00:21:55,880 --> 00:21:59,400
our channel it's our privacy mechanism

579
00:21:57,919 --> 00:22:01,039
we're going to add ask a query on the

580
00:21:59,400 --> 00:22:04,320
data and then we're going to randomize

581
00:22:01,039 --> 00:22:06,080
that query in order to Output uh a

582
00:22:04,320 --> 00:22:08,559
randomized value in order to ensure

583
00:22:06,080 --> 00:22:11,640
privacy um and we measure privacy in

584
00:22:08,559 --> 00:22:13,880
terms of neighboring queries right so we

585
00:22:11,640 --> 00:22:16,279
can Define this random variable known as

586
00:22:13,880 --> 00:22:18,080
the Privacy loss random variable which

587
00:22:16,279 --> 00:22:20,520
is nothing more than the log likelihood

588
00:22:18,080 --> 00:22:23,640
ratio between the output distributions

589
00:22:20,520 --> 00:22:25,159
if the input were X or X Prime so it's a

590
00:22:23,640 --> 00:22:27,600
log likelihood ratio that you would see

591
00:22:25,159 --> 00:22:29,159
when you're testing between these two

592
00:22:27,600 --> 00:22:31,720
distributions and here we're assuming

593
00:22:29,159 --> 00:22:34,279
that Y is distributed ACC Accord to P of

594
00:22:31,720 --> 00:22:37,200
Y given xal Little

595
00:22:34,279 --> 00:22:40,080
X so the mean of this random

596
00:22:37,200 --> 00:22:43,279
variable is

597
00:22:40,080 --> 00:22:45,720
anyone KO Divergence great that's how

598
00:22:43,279 --> 00:22:47,279
you know you're at MIT so uh the mean of

599
00:22:45,720 --> 00:22:48,799
this random variable is K Divergence

600
00:22:47,279 --> 00:22:51,320
right it's the K Divergence between

601
00:22:48,799 --> 00:22:52,679
these two neighboring distributions so

602
00:22:51,320 --> 00:22:54,960
remember this because this will be

603
00:22:52,679 --> 00:22:57,279
important for us later why because

604
00:22:54,960 --> 00:22:59,200
eventually we're going to be accessing

605
00:22:57,279 --> 00:23:03,000
the data multiple times and the random

606
00:22:59,200 --> 00:23:06,679
variable will concentrate through its

607
00:23:03,000 --> 00:23:08,720
mean and we can write these this privacy

608
00:23:06,679 --> 00:23:10,600
curve in terms of the Privacy loss ROM

609
00:23:08,720 --> 00:23:12,720
variable it's a pretty straightforward

610
00:23:10,600 --> 00:23:14,480
exercise in algebra just you rewrite

611
00:23:12,720 --> 00:23:17,000
this egma Divergence that captures your

612
00:23:14,480 --> 00:23:20,240
absolent Delta guarantees in terms of

613
00:23:17,000 --> 00:23:22,919
this privacy loss random variable um we

614
00:23:20,240 --> 00:23:24,640
still have this annoying subterm here to

615
00:23:22,919 --> 00:23:26,000
get the exact Delta where you have to

616
00:23:24,640 --> 00:23:28,080
look at the supremum across all

617
00:23:26,000 --> 00:23:30,679
neighboring distributions you kind of

618
00:23:28,080 --> 00:23:32,679
you can get over that by looking at

619
00:23:30,679 --> 00:23:34,760
something known as a tight privacy loss

620
00:23:32,679 --> 00:23:37,000
random variable basically you create a

621
00:23:34,760 --> 00:23:39,440
pair of random variables who Z GMA

622
00:23:37,000 --> 00:23:41,200
Divergence whose privacy curve dominates

623
00:23:39,440 --> 00:23:43,760
those for any pair of neighboring

624
00:23:41,200 --> 00:23:46,279
distributions so we can in many cases

625
00:23:43,760 --> 00:23:48,480
create a tight privacy loss ROM variable

626
00:23:46,279 --> 00:23:49,960
often it's just the the distribution

627
00:23:48,480 --> 00:23:53,000
that you get for the Privacy loss ROM

628
00:23:49,960 --> 00:23:54,799
variable for the worst case inputs and

629
00:23:53,000 --> 00:23:57,200
in this case you can write your privacy

630
00:23:54,799 --> 00:23:59,039
curve as a function of this Privacy Law

631
00:23:57,200 --> 00:24:00,880
strend and variable

632
00:23:59,039 --> 00:24:02,600
and since since this is a log likelihood

633
00:24:00,880 --> 00:24:04,159
ratio uh when you're quiring the data

634
00:24:02,600 --> 00:24:06,600
multiple times and you're considering

635
00:24:04,159 --> 00:24:09,640
this uh worst case uh

636
00:24:06,600 --> 00:24:12,159
PV um your privacy guarantee becomes a

637
00:24:09,640 --> 00:24:14,120
sum of these privacy loss random

638
00:24:12,159 --> 00:24:16,000
variables right so the problem of

639
00:24:14,120 --> 00:24:19,520
privacy accounting boils down to

640
00:24:16,000 --> 00:24:21,600
quantifying the sum of these usually IID

641
00:24:19,520 --> 00:24:22,880
privacy loss random variables and over

642
00:24:21,600 --> 00:24:25,520
the past few years you've seen several

643
00:24:22,880 --> 00:24:27,640
papers that try to do exactly that um so

644
00:24:25,520 --> 00:24:29,360
one natural approach is for you to uh

645
00:24:27,640 --> 00:24:31,039
approximate this some using the central

646
00:24:29,360 --> 00:24:32,679
limit theorem in this case you

647
00:24:31,039 --> 00:24:35,559
approximate the sum of the Privacy loss

648
00:24:32,679 --> 00:24:37,240
random variable by a gaussian um

649
00:24:35,559 --> 00:24:39,480
naturally another approach is that you

650
00:24:37,240 --> 00:24:41,080
look at the cumulant generating function

651
00:24:39,480 --> 00:24:43,480
of this privacy loss rendom variable

652
00:24:41,080 --> 00:24:44,600
this is a large deviations approach this

653
00:24:43,480 --> 00:24:46,240
is what's at the heart of something

654
00:24:44,600 --> 00:24:48,520
called Rene differential privacy and the

655
00:24:46,240 --> 00:24:52,039
moments accountant um in this case you

656
00:24:48,520 --> 00:24:53,960
apply a turnoff Bound for um for the

657
00:24:52,039 --> 00:24:55,200
accumul Jing fun using the cumul Jing

658
00:24:53,960 --> 00:24:57,200
function of your privacy loss random

659
00:24:55,200 --> 00:24:59,559
variable which is essentially R

660
00:24:57,200 --> 00:25:01,440
Divergence for for neighboring

661
00:24:59,559 --> 00:25:03,039
distributions but the challenge of these

662
00:25:01,440 --> 00:25:05,120
approaches is that they're nice because

663
00:25:03,039 --> 00:25:07,000
they give us an analytical form of your

664
00:25:05,120 --> 00:25:08,240
privacy loss and remember ultimately

665
00:25:07,000 --> 00:25:10,880
where we're going with this is that we

666
00:25:08,240 --> 00:25:13,480
want to optimize our noise distributions

667
00:25:10,880 --> 00:25:16,000
U but they can be not very tight like

668
00:25:13,480 --> 00:25:17,960
RDP can overestimate your privacy loss

669
00:25:16,000 --> 00:25:21,320
and the calcium differential privacy can

670
00:25:17,960 --> 00:25:23,240
underestimate your privacy loss so if

671
00:25:21,320 --> 00:25:25,720
you have a bunch of distributions right

672
00:25:23,240 --> 00:25:27,960
and you're adding them up and you want

673
00:25:25,720 --> 00:25:29,960
to compute the density of this sum you

674
00:25:27,960 --> 00:25:33,200
can involve the individual densities and

675
00:25:29,960 --> 00:25:36,679
what's a fast algorithm for computing

676
00:25:33,200 --> 00:25:38,760
convolution the fft yeah and this is

677
00:25:36,679 --> 00:25:40,919
another set of approaches known as the

678
00:25:38,760 --> 00:25:43,720
fft approaches that does exactly that

679
00:25:40,919 --> 00:25:46,840
that you numerically compute um the

680
00:25:43,720 --> 00:25:48,720
convolution via the fft so here you have

681
00:25:46,840 --> 00:25:50,440
a set of accounting procedures one that

682
00:25:48,720 --> 00:25:53,159
gives you nice analytical forms that

683
00:25:50,440 --> 00:25:55,200
later we can optimize but has relatively

684
00:25:53,159 --> 00:25:56,559
low precision and the other one that's

685
00:25:55,200 --> 00:25:58,080
purely numerical so it's something

686
00:25:56,559 --> 00:25:58,760
that's purely computational but gives

687
00:25:58,080 --> 00:26:01,240
you

688
00:25:58,760 --> 00:26:03,960
incredibly precise approximations of

689
00:26:01,240 --> 00:26:05,919
your privacy loss fromom variable so we

690
00:26:03,960 --> 00:26:07,440
proposed an alternative approach a

691
00:26:05,919 --> 00:26:10,720
couple of years back

692
00:26:07,440 --> 00:26:11,960
ATL called the saddleo accountant uh

693
00:26:10,720 --> 00:26:14,600
which is nice because it has an

694
00:26:11,960 --> 00:26:17,320
analytical form and it has high

695
00:26:14,600 --> 00:26:18,919
precision and it's just one key trick

696
00:26:17,320 --> 00:26:21,080
that we use and one key idea is that

697
00:26:18,919 --> 00:26:23,360
instead of looking at the concentration

698
00:26:21,080 --> 00:26:25,880
of the Privacy loss random variable we

699
00:26:23,360 --> 00:26:27,880
look at an exponential tilting of the

700
00:26:25,880 --> 00:26:30,440
Privacy loss random variable so what do

701
00:26:27,880 --> 00:26:32,960
I mean by exponential tilting that we

702
00:26:30,440 --> 00:26:35,159
create a new random variable L tiled

703
00:26:32,960 --> 00:26:38,240
whose distribution is the Tilted

704
00:26:35,159 --> 00:26:41,120
distribution of your random variable L

705
00:26:38,240 --> 00:26:42,640
and here K is your cumulant generating

706
00:26:41,120 --> 00:26:44,120
function so for example if you have a

707
00:26:42,640 --> 00:26:46,120
continuous random variable if you have a

708
00:26:44,120 --> 00:26:48,240
density for your privacy loss random

709
00:26:46,120 --> 00:26:50,080
variable uh the distribution of your

710
00:26:48,240 --> 00:26:52,399
tilted distribution is just your

711
00:26:50,080 --> 00:26:55,080
original density times this term where

712
00:26:52,399 --> 00:26:57,320
again K is your cumulant generating

713
00:26:55,080 --> 00:26:58,760
function and cting distributions are

714
00:26:57,320 --> 00:27:00,840
very nice this is a a common trick to

715
00:26:58,760 --> 00:27:02,399
use in large deviations Theory uh

716
00:27:00,840 --> 00:27:03,960
because the first property that they

717
00:27:02,399 --> 00:27:05,880
satisfy is that if your underlying

718
00:27:03,960 --> 00:27:08,520
distributions are independent the Tilted

719
00:27:05,880 --> 00:27:10,320
distributions are also independent that

720
00:27:08,520 --> 00:27:14,520
uh the tilting of the sum is the sum of

721
00:27:10,320 --> 00:27:17,159
the tilts um and we can also measure the

722
00:27:14,520 --> 00:27:20,799
value uh of of certain functions of this

723
00:27:17,159 --> 00:27:22,720
random variable in terms of its tilted

724
00:27:20,799 --> 00:27:24,640
distribution and it's particularly this

725
00:27:22,720 --> 00:27:26,480
third property that we use here because

726
00:27:24,640 --> 00:27:29,000
what we do is that we reexpress your

727
00:27:26,480 --> 00:27:32,559
privacy curve in terms terms of the

728
00:27:29,000 --> 00:27:35,679
Tilted distribution of your privacy loss

729
00:27:32,559 --> 00:27:37,880
random variable and when you look at

730
00:27:35,679 --> 00:27:39,240
this uh at your resulting Delta

731
00:27:37,880 --> 00:27:40,720
guarantee for given Epsilon in

732
00:27:39,240 --> 00:27:43,880
differential privacy for the Tilted

733
00:27:40,720 --> 00:27:45,159
version you can uh your in the large

734
00:27:43,880 --> 00:27:47,279
composition regime you're going to have

735
00:27:45,159 --> 00:27:49,880
the sum of multiple random variables and

736
00:27:47,279 --> 00:27:51,240
you can approximate it using the central

737
00:27:49,880 --> 00:27:54,640
limit theorem but now applying the

738
00:27:51,240 --> 00:27:56,640
central limit theorem to your tilted

739
00:27:54,640 --> 00:27:59,399
distribution and that's at the heart of

740
00:27:56,640 --> 00:28:02,559
the SLE Point accounting um when you

741
00:27:59,399 --> 00:28:05,159
look at this formula um here you can

742
00:28:02,559 --> 00:28:07,399
actually apply Paro and write it in the

743
00:28:05,159 --> 00:28:11,360
Fier domain so you can write it in terms

744
00:28:07,399 --> 00:28:13,080
of the uh um in terms of the moment

745
00:28:11,360 --> 00:28:14,720
generating function of your tilted

746
00:28:13,080 --> 00:28:16,679
distribution or the characteristic

747
00:28:14,720 --> 00:28:17,919
function of your tilted distribution and

748
00:28:16,679 --> 00:28:20,080
then you can write your privacy

749
00:28:17,919 --> 00:28:22,760
guarantees in terms of an integral where

750
00:28:20,080 --> 00:28:24,279
T here is exactly the tilting parameter

751
00:28:22,760 --> 00:28:27,159
that you choose so in a sense the

752
00:28:24,279 --> 00:28:29,559
tilting parameter this determines the

753
00:28:27,159 --> 00:28:32,880
real intercept of this line integral

754
00:28:29,559 --> 00:28:35,960
that you're solving and again K here is

755
00:28:32,880 --> 00:28:39,039
the is your cumul generating function so

756
00:28:35,960 --> 00:28:42,039
if you look at this exponent here uh and

757
00:28:39,039 --> 00:28:45,080
you it's uh it's imaginary and it's a

758
00:28:42,039 --> 00:28:48,279
complex uh exponent and if you plot it

759
00:28:45,080 --> 00:28:52,360
in the complex plane uh this function

760
00:28:48,279 --> 00:28:55,480
has a saddle point here um and this

761
00:28:52,360 --> 00:28:58,720
saddle point is where we choose our

762
00:28:55,480 --> 00:29:00,159
integration uh our integration point and

763
00:28:58,720 --> 00:29:01,840
that's something that's very common in

764
00:29:00,159 --> 00:29:04,200
mathematical physics it's known as

765
00:29:01,840 --> 00:29:06,200
lass's integration method and we apply

766
00:29:04,200 --> 00:29:08,320
it here and once you do this you get a

767
00:29:06,200 --> 00:29:10,080
very precise approximation of your Delta

768
00:29:08,320 --> 00:29:12,399
Epsilon curve you can derive bounds for

769
00:29:10,080 --> 00:29:14,440
this as well where the point that we

770
00:29:12,399 --> 00:29:17,440
choose are tilting is exactly the

771
00:29:14,440 --> 00:29:19,960
solution the unique solution um of that

772
00:29:17,440 --> 00:29:21,640
CLE Point um if you're familiar with

773
00:29:19,960 --> 00:29:23,200
Rene differential privacy or the moments

774
00:29:21,640 --> 00:29:25,960
accountant that was proposed a few years

775
00:29:23,200 --> 00:29:28,120
ago um this you can think about this as

776
00:29:25,960 --> 00:29:29,960
a correction of the moments accountant

777
00:29:28,120 --> 00:29:32,960
where you add this additional uh

778
00:29:29,960 --> 00:29:34,080
denominator here in order to um and

779
00:29:32,960 --> 00:29:37,080
these additional terms when you're

780
00:29:34,080 --> 00:29:39,159
Computing your your

781
00:29:37,080 --> 00:29:41,960
integral and the result is that this

782
00:29:39,159 --> 00:29:44,080
gives us uh approximations that are very

783
00:29:41,960 --> 00:29:48,720
very accurate including in these in the

784
00:29:44,080 --> 00:29:51,399
tails for uh your privacy loss so uh

785
00:29:48,720 --> 00:29:53,320
what you see here are um uh the black

786
00:29:51,399 --> 00:29:55,559
line or upper and lower bounds given by

787
00:29:53,320 --> 00:29:57,640
the Sal Point accountant the Blue Line

788
00:29:55,559 --> 00:29:59,919
Is What You Get by Computing the fft

789
00:29:57,640 --> 00:30:01,720
eventually it stalls out because of

790
00:29:59,919 --> 00:30:03,440
numerical precisions of precision of

791
00:30:01,720 --> 00:30:06,120
course you can always improve your fft

792
00:30:03,440 --> 00:30:08,360
to handle larger Precision but this is

793
00:30:06,120 --> 00:30:09,720
what we use out of the box in Python and

794
00:30:08,360 --> 00:30:11,880
we can see that our approximation is

795
00:30:09,720 --> 00:30:14,840
very precise and more precise than the

796
00:30:11,880 --> 00:30:14,840
moments accountant for

797
00:30:14,919 --> 00:30:19,080
example

798
00:30:16,480 --> 00:30:21,080
so because in part of your information

799
00:30:19,080 --> 00:30:24,159
theorist as well a natural question to

800
00:30:21,080 --> 00:30:26,080
ask is well how does privacy behave when

801
00:30:24,159 --> 00:30:28,159
you're in the regime where you have an

802
00:30:26,080 --> 00:30:30,360
extremely large number of compositions

803
00:30:28,159 --> 00:30:32,360
right if you're accessing data hundreds

804
00:30:30,360 --> 00:30:35,039
of thousands or millions of times right

805
00:30:32,360 --> 00:30:38,360
this very asymptotic regime where you're

806
00:30:35,039 --> 00:30:41,240
accessing data multiple times

807
00:30:38,360 --> 00:30:42,519
and at and you can ask well can we

808
00:30:41,240 --> 00:30:45,279
derive something that looks like a

809
00:30:42,519 --> 00:30:48,760
capacity result here so if you were to

810
00:30:45,279 --> 00:30:50,440
fix a given Delta at what so usually in

811
00:30:48,760 --> 00:30:52,600
practice as I mentioned Delta is fixed

812
00:30:50,440 --> 00:30:55,159
through a value of 10 to Theus 6 or tus

813
00:30:52,600 --> 00:30:58,480
7 one over the size of your data set you

814
00:30:55,159 --> 00:31:01,360
can ask well how does your Epsilon grow

815
00:30:58,480 --> 00:31:04,159
with the number of composition K and can

816
00:31:01,360 --> 00:31:06,840
we characterize this linear growth rate

817
00:31:04,159 --> 00:31:10,559
of your Epsilon guarantee and Epsilon

818
00:31:06,840 --> 00:31:13,279
Delta differential privacy um so the

819
00:31:10,559 --> 00:31:15,639
goal is that for a fixed Delta can we

820
00:31:13,279 --> 00:31:18,519
characterize the growth rate of this

821
00:31:15,639 --> 00:31:20,200
privacy curve and specifically we want

822
00:31:18,519 --> 00:31:22,880
to characterize this growth rate across

823
00:31:20,200 --> 00:31:25,200
a range of possible mechanisms right so

824
00:31:22,880 --> 00:31:27,919
the constraint that we ask is what is

825
00:31:25,200 --> 00:31:31,279
the smallest privacy per composition uh

826
00:31:27,919 --> 00:31:34,240
epon here given a distortion constraint

827
00:31:31,279 --> 00:31:36,600
right so over all possible mechanisms

828
00:31:34,240 --> 00:31:39,440
where the Distortion at the output is

829
00:31:36,600 --> 00:31:41,360
bounded for example the msse is bounded

830
00:31:39,440 --> 00:31:45,000
what is the smallest possible growth

831
00:31:41,360 --> 00:31:47,360
rate that we can achieve in terms of

832
00:31:45,000 --> 00:31:49,799
Epsilon per

833
00:31:47,360 --> 00:31:51,440
composition so and we Define the set of

834
00:31:49,799 --> 00:31:53,639
mechanisms that satisfies this

835
00:31:51,440 --> 00:31:55,440
Distortion constraint by c r here this

836
00:31:53,639 --> 00:31:58,080
is so this is the set of mechanisms that

837
00:31:55,440 --> 00:31:59,559
satisfies our Target Distortion and we

838
00:31:58,080 --> 00:32:01,039
can ask again the question that we're

839
00:31:59,559 --> 00:32:02,600
asking is what is the smallest privacy

840
00:32:01,039 --> 00:32:05,240
per composition given across all

841
00:32:02,600 --> 00:32:07,679
mechanisms that satisfy this Distortion

842
00:32:05,240 --> 00:32:09,960
constraint and we Define this quantity

843
00:32:07,679 --> 00:32:14,039
as privacy capacity so this Epsilon

844
00:32:09,960 --> 00:32:16,039
store uh for a given um Alpha Norm here

845
00:32:14,039 --> 00:32:17,840
and for a given cost constraint C in

846
00:32:16,039 --> 00:32:19,559
your Distortion our goal should

847
00:32:17,840 --> 00:32:22,799
characterize the infimum across all

848
00:32:19,559 --> 00:32:25,440
achievable privacy growth rates in this

849
00:32:22,799 --> 00:32:30,440
problem um this is something that we

850
00:32:25,440 --> 00:32:32,600
presented at isit last year and um we

851
00:32:30,440 --> 00:32:34,840
say that a growth rate Epsilon is

852
00:32:32,600 --> 00:32:37,519
achievable if there is we can design or

853
00:32:34,840 --> 00:32:40,080
there exists a mechanism P of Y given x

854
00:32:37,519 --> 00:32:41,919
uh such that this growth rate in epsilon

855
00:32:40,080 --> 00:32:46,559
is smaller than Epsilon for some Delta

856
00:32:41,919 --> 00:32:48,360
that doesn't decrease CH true quickly so

857
00:32:46,559 --> 00:32:50,679
there's an analogy for privacy capacity

858
00:32:48,360 --> 00:32:53,039
in terms of rate Distortion Theory where

859
00:32:50,679 --> 00:32:54,399
here uh C plays a roow of distortion

860
00:32:53,039 --> 00:32:57,360
rate Distortion Theory you can think

861
00:32:54,399 --> 00:33:00,600
about K as your block length and Delta

862
00:32:57,360 --> 00:33:03,480
here almost as your decoding error

863
00:33:00,600 --> 00:33:05,840
probability so it turns out that privacy

864
00:33:03,480 --> 00:33:09,320
capacity uh this growth rate of your

865
00:33:05,840 --> 00:33:11,440
Epsilon as you query data multiple times

866
00:33:09,320 --> 00:33:14,039
is related to another quantity which is

867
00:33:11,440 --> 00:33:16,639
the minmax scale Divergence so you can

868
00:33:14,039 --> 00:33:17,960
ask across all mechanisms in this set

869
00:33:16,639 --> 00:33:20,760
that satisfy a given Distortion

870
00:33:17,960 --> 00:33:22,799
constraint what is the worst case scale

871
00:33:20,760 --> 00:33:24,279
Divergence across all neighboring

872
00:33:22,799 --> 00:33:26,880
distributions so you can think for

873
00:33:24,279 --> 00:33:28,159
example that um what across all

874
00:33:26,880 --> 00:33:30,240
distributions that set by a given

875
00:33:28,159 --> 00:33:31,760
constraint for example true gaussians

876
00:33:30,240 --> 00:33:35,360
what is the worst case

877
00:33:31,760 --> 00:33:37,919
scale um across all possible

878
00:33:35,360 --> 00:33:40,679
shifts and it turns out that in this

879
00:33:37,919 --> 00:33:43,880
limit of many many accesses through the

880
00:33:40,679 --> 00:33:46,320
data your privacy rate um uh your

881
00:33:43,880 --> 00:33:48,799
privacy capacity the linear growth rate

882
00:33:46,320 --> 00:33:53,360
of Epsilon is equivalent to your minia

883
00:33:48,799 --> 00:33:54,760
Max K Divergence rate so in a sense in

884
00:33:53,360 --> 00:33:57,919
this case your privacy guarantee single

885
00:33:54,760 --> 00:33:59,399
letter isers and the the row of like if

886
00:33:57,919 --> 00:34:01,519
you would have like MRA information and

887
00:33:59,399 --> 00:34:03,720
rate Distortion Theory here you have the

888
00:34:01,519 --> 00:34:06,000
minmax scale Divergence across

889
00:34:03,720 --> 00:34:09,040
neighboring distributions but not only

890
00:34:06,000 --> 00:34:10,839
that we can show that for um if you're

891
00:34:09,040 --> 00:34:13,159
uh for the cost constraint that we're

892
00:34:10,839 --> 00:34:15,359
considering these Norm cost constraints

893
00:34:13,159 --> 00:34:17,440
we have that um additive continuous and

894
00:34:15,359 --> 00:34:20,280
spherically symmetric mechanisms can

895
00:34:17,440 --> 00:34:21,520
achieve privacy capacity so what we're

896
00:34:20,280 --> 00:34:24,040
what this means is that even though

897
00:34:21,520 --> 00:34:27,000
we're optimizing over an abstract set of

898
00:34:24,040 --> 00:34:29,839
channels it's sufficient to design an

899
00:34:27,000 --> 00:34:31,639
additive noise to your query in order to

900
00:34:29,839 --> 00:34:33,520
achieve this capacity that there in

901
00:34:31,639 --> 00:34:37,000
other words there exist additive noise

902
00:34:33,520 --> 00:34:39,000
distributions that achieve privacy

903
00:34:37,000 --> 00:34:41,079
capacity this is interesting for us

904
00:34:39,000 --> 00:34:43,480
because now we can try to optimize the

905
00:34:41,079 --> 00:34:45,119
PDF of this noise in order to achieve

906
00:34:43,480 --> 00:34:46,720
favorable performance in this large

907
00:34:45,119 --> 00:34:48,440
composition regime and this is the

908
00:34:46,720 --> 00:34:51,200
second question that we ask so how

909
00:34:48,440 --> 00:34:53,679
should we design privacy mechanisms that

910
00:34:51,200 --> 00:34:55,919
operate under many

911
00:34:53,679 --> 00:34:57,839
compositions so just to recap where we

912
00:34:55,919 --> 00:34:59,480
are so again the goal is should design

913
00:34:57,839 --> 00:35:02,119
privacy mechanisms that minimize privacy

914
00:34:59,480 --> 00:35:04,079
loss and maximize utility ideally we

915
00:35:02,119 --> 00:35:05,760
would want you do this by Computing the

916
00:35:04,079 --> 00:35:07,400
Privacy curve exactly but that's very

917
00:35:05,760 --> 00:35:10,200
hard we can only compute it using things

918
00:35:07,400 --> 00:35:12,079
like the fft um ongoing work is actually

919
00:35:10,200 --> 00:35:14,000
optimizing the saddle point accountant

920
00:35:12,079 --> 00:35:16,440
method it's kind of complicated because

921
00:35:14,000 --> 00:35:18,920
you have to back propagate through OD so

922
00:35:16,440 --> 00:35:20,880
it's not super simple to do that what

923
00:35:18,920 --> 00:35:22,720
I'm going to show you today are results

924
00:35:20,880 --> 00:35:24,640
that we already published where we

925
00:35:22,720 --> 00:35:27,200
optimize for privacy capacity so we

926
00:35:24,640 --> 00:35:30,599
optimize for this like limit of the

927
00:35:27,200 --> 00:35:32,720
growth rate of Epsilon and in this case

928
00:35:30,599 --> 00:35:35,280
you have this optimization problem where

929
00:35:32,720 --> 00:35:37,920
you're minimizing K Divergence subject

930
00:35:35,280 --> 00:35:39,839
to a cost constraint as I mentioned

931
00:35:37,920 --> 00:35:41,920
before uh you can show that it's

932
00:35:39,839 --> 00:35:44,440
sufficient to consider additive

933
00:35:41,920 --> 00:35:47,920
mechanisms so the optimal thing to do is

934
00:35:44,440 --> 00:35:50,040
to optimize the PDF or pmf in the

935
00:35:47,920 --> 00:35:52,839
discrete case of the noise that you're

936
00:35:50,040 --> 00:35:56,920
adding to your data and that's exactly

937
00:35:52,839 --> 00:35:59,720
what we do like we quantize this uh PDF

938
00:35:56,920 --> 00:36:02,480
and solve it

939
00:35:59,720 --> 00:36:04,160
numerically uh so if you squint at this

940
00:36:02,480 --> 00:36:06,079
problem you can probably see that it's a

941
00:36:04,160 --> 00:36:07,720
convex optimization problem so we can

942
00:36:06,079 --> 00:36:10,720
just kind of boot Force numerically

943
00:36:07,720 --> 00:36:12,520
solve it so you can quantize this PDF

944
00:36:10,720 --> 00:36:14,240
right you can just quantize it in

945
00:36:12,520 --> 00:36:17,560
multiple buckets in the one dimensional

946
00:36:14,240 --> 00:36:19,440
case and uh solve this very complicated

947
00:36:17,560 --> 00:36:21,319
this optimization problem that is looks

948
00:36:19,440 --> 00:36:23,280
pretty complicated but it's still convex

949
00:36:21,319 --> 00:36:26,160
so you can solve it using standard

950
00:36:23,280 --> 00:36:28,400
interior Point methods um this is the

951
00:36:26,160 --> 00:36:30,839
convergence of your optimization problem

952
00:36:28,400 --> 00:36:32,640
you can see that it converges to a very

953
00:36:30,839 --> 00:36:35,119
kind of crazily shaped noise

954
00:36:32,640 --> 00:36:37,720
distribution uh we call this the cactus

955
00:36:35,119 --> 00:36:39,440
distribution because of like the the

956
00:36:37,720 --> 00:36:42,560
other authors of this paper they're from

957
00:36:39,440 --> 00:36:45,680
ASU so they think a lot about Cactus I

958
00:36:42,560 --> 00:36:47,480
guess and it has this nice spiky

959
00:36:45,680 --> 00:36:49,680
distribution you can do this for

960
00:36:47,480 --> 00:36:52,280
multiple Dimensions as well uh this is

961
00:36:49,680 --> 00:36:54,800
the the PDF of the radius of the noise

962
00:36:52,280 --> 00:36:56,720
in 10 dimensions and the optimized noise

963
00:36:54,800 --> 00:36:58,200
distributions they do not look like gin

964
00:36:56,720 --> 00:36:59,599
they look like something

965
00:36:58,200 --> 00:37:01,280
something different and here we're

966
00:36:59,599 --> 00:37:05,200
looking at 10 Dimensions Delta equal

967
00:37:01,280 --> 00:37:07,119
10us 8 and a MSC Distortion constraint

968
00:37:05,200 --> 00:37:10,319
and you can see that it achieves a more

969
00:37:07,119 --> 00:37:13,560
favorable privacy Decay for the same

970
00:37:10,319 --> 00:37:16,920
Distortion level and as I mentioned this

971
00:37:13,560 --> 00:37:20,280
is how it looks in one dimension this is

972
00:37:16,920 --> 00:37:22,560
the the the the PDF of the noise in in

973
00:37:20,280 --> 00:37:25,359
one dimension it looks like a

974
00:37:22,560 --> 00:37:27,200
cactus you these these noises are

975
00:37:25,359 --> 00:37:28,359
optimized should be better in terms of K

976
00:37:27,200 --> 00:37:30,839
diver

977
00:37:28,359 --> 00:37:33,359
right so you can you can compare the

978
00:37:30,839 --> 00:37:36,119
maximal K Divergence across worst case

979
00:37:33,359 --> 00:37:39,119
neighboring um inputs

980
00:37:36,119 --> 00:37:41,800
for our optimized noise distributions

981
00:37:39,119 --> 00:37:44,160
and gussan and if you look at this curve

982
00:37:41,800 --> 00:37:46,319
so here on the x-axis you have the MSC

983
00:37:44,160 --> 00:37:49,280
right the the the standard deviation of

984
00:37:46,319 --> 00:37:50,800
the noise on the y- axis you have your

985
00:37:49,280 --> 00:37:53,040
maximal K

986
00:37:50,800 --> 00:37:55,640
Divergence and here we're considering a

987
00:37:53,040 --> 00:37:58,200
sensitivity of one a shift of

988
00:37:55,640 --> 00:38:00,440
one but what's interesting is that not

989
00:37:58,200 --> 00:38:02,640
surprisingly the the optimized noise

990
00:38:00,440 --> 00:38:05,440
distribution is better and it has a

991
00:38:02,640 --> 00:38:07,319
lower maximal K than the gaussin

992
00:38:05,440 --> 00:38:10,240
distribution but it seems that there are

993
00:38:07,319 --> 00:38:14,119
certain regimes where gussan is

994
00:38:10,240 --> 00:38:16,359
optimal right and so what's when we

995
00:38:14,119 --> 00:38:18,000
first saw this graph it got us thinking

996
00:38:16,359 --> 00:38:19,760
like what's happening here like we went

997
00:38:18,000 --> 00:38:21,400
through all this trouble of like

998
00:38:19,760 --> 00:38:25,000
understanding how privacy behaves in the

999
00:38:21,400 --> 00:38:26,480
large composition regime we quanti we

1000
00:38:25,000 --> 00:38:28,599
found a setting where like additive

1001
00:38:26,480 --> 00:38:30,520
mechanisms are optimal we can quantize

1002
00:38:28,599 --> 00:38:32,240
the PDF and formulate this giant convex

1003
00:38:30,520 --> 00:38:34,680
optimization problem we get a noise

1004
00:38:32,240 --> 00:38:36,079
distribution out of it but it seems that

1005
00:38:34,680 --> 00:38:37,599
in certain regimes it's hard to beat

1006
00:38:36,079 --> 00:38:39,680
gaussin it seems like gaussin is pretty

1007
00:38:37,599 --> 00:38:40,680
good so what's happening and it turns

1008
00:38:39,680 --> 00:38:43,040
out that there's something really

1009
00:38:40,680 --> 00:38:45,680
interesting happening in this regime

1010
00:38:43,040 --> 00:38:47,640
that in the small sensitivity High noise

1011
00:38:45,680 --> 00:38:49,960
regime optimizing differential privacy

1012
00:38:47,640 --> 00:38:52,920
mechanisms is equivalent to finding igen

1013
00:38:49,960 --> 00:38:56,920
functions of the shinger operator so

1014
00:38:52,920 --> 00:38:58,760
surprise physics and um so let's go

1015
00:38:56,920 --> 00:39:01,480
through this

1016
00:38:58,760 --> 00:39:03,560
so this follows from a a property of K

1017
00:39:01,480 --> 00:39:04,880
Divergence so if you look at K

1018
00:39:03,560 --> 00:39:06,280
Divergence so if you look at the

1019
00:39:04,880 --> 00:39:10,560
optimization problem that we're looking

1020
00:39:06,280 --> 00:39:13,000
at um here TFA is the shift operator and

1021
00:39:10,560 --> 00:39:14,839
we're looking at the PDF that minimizes

1022
00:39:13,000 --> 00:39:16,720
the worst Cas scale Divergence across

1023
00:39:14,839 --> 00:39:18,359
all possible shifts of a distribution

1024
00:39:16,720 --> 00:39:20,280
with itself this is the simplest form in

1025
00:39:18,359 --> 00:39:21,960
one dimension subject to a cost

1026
00:39:20,280 --> 00:39:23,880
constraint where assuming in the simple

1027
00:39:21,960 --> 00:39:25,280
onedimensional case that the cost

1028
00:39:23,880 --> 00:39:26,480
constraint is something of this form

1029
00:39:25,280 --> 00:39:28,520
like you can think about Alpha equals

1030
00:39:26,480 --> 00:39:30,839
true and we're minimize

1031
00:39:28,520 --> 00:39:32,680
MSC but what happens when you make your

1032
00:39:30,839 --> 00:39:34,680
your sensitivity very very small when

1033
00:39:32,680 --> 00:39:36,839
you make the shift very very small you

1034
00:39:34,680 --> 00:39:38,440
can approximate a kale Divergence up the

1035
00:39:36,839 --> 00:39:41,960
first starter term of its Tor series

1036
00:39:38,440 --> 00:39:43,920
expansion on the shift in terms of fer

1037
00:39:41,960 --> 00:39:46,599
information so for small shift scale

1038
00:39:43,920 --> 00:39:48,720
Divergence behaves locally like Fisher

1039
00:39:46,599 --> 00:39:50,720
information where in Fisher information

1040
00:39:48,720 --> 00:39:53,160
is defined

1041
00:39:50,720 --> 00:39:54,880
here so in this case if your shift is

1042
00:39:53,160 --> 00:39:57,560
very very very small or equivalently the

1043
00:39:54,880 --> 00:39:59,280
noise that you add is very very large

1044
00:39:57,560 --> 00:40:01,760
you can approximate this optimization

1045
00:39:59,280 --> 00:40:04,640
Problem by minimizing the Fisher

1046
00:40:01,760 --> 00:40:06,720
information subject to a cost constraint

1047
00:40:04,640 --> 00:40:09,800
and you can show that this optimization

1048
00:40:06,720 --> 00:40:11,839
also has a unique optimizing

1049
00:40:09,800 --> 00:40:14,079
distribution now the thing that's

1050
00:40:11,839 --> 00:40:16,160
surprising here is that again in this

1051
00:40:14,079 --> 00:40:17,839
small sensitivity regime or cently large

1052
00:40:16,160 --> 00:40:22,280
noise regime that you're adding to the

1053
00:40:17,839 --> 00:40:24,200
data um the fure minimization problem it

1054
00:40:22,280 --> 00:40:26,480
boils down to uh that solving this

1055
00:40:24,200 --> 00:40:29,760
optimization is equivalent to finding

1056
00:40:26,480 --> 00:40:32,119
igen functions of the shinger operator

1057
00:40:29,760 --> 00:40:34,640
and what I mean by that is the falling

1058
00:40:32,119 --> 00:40:37,280
operator given here so if you have a

1059
00:40:34,640 --> 00:40:40,000
measurable function V uh the shinger

1060
00:40:37,280 --> 00:40:42,359
operator h of V uh over this uh the

1061
00:40:40,000 --> 00:40:44,560
space of squared integrable functions

1062
00:40:42,359 --> 00:40:45,880
with the potential function V is given

1063
00:40:44,560 --> 00:40:47,400
by this operator here you take a

1064
00:40:45,880 --> 00:40:49,920
function y and you compute it second

1065
00:40:47,400 --> 00:40:53,560
derivative and add it change the sign

1066
00:40:49,920 --> 00:40:55,400
and add it to V times Y and we say that

1067
00:40:53,560 --> 00:40:57,760
a function is an igen function for this

1068
00:40:55,400 --> 00:40:59,520
operator if there exists an e such that

1069
00:40:57,760 --> 00:41:01,280
when you apply the sh gr operator to the

1070
00:40:59,520 --> 00:41:05,599
function you get the function

1071
00:41:01,280 --> 00:41:08,920
back so it turns out that you can relate

1072
00:41:05,599 --> 00:41:11,960
the optimization problem to finding igen

1073
00:41:08,920 --> 00:41:13,720
functions of this shinger operator where

1074
00:41:11,960 --> 00:41:15,960
you adjust this potential function to be

1075
00:41:13,720 --> 00:41:17,359
related to your cost function here and

1076
00:41:15,960 --> 00:41:19,680
Theta you can think about this almost

1077
00:41:17,359 --> 00:41:21,640
like a lrange multiplier that def

1078
00:41:19,680 --> 00:41:24,599
defines what value of the cost that

1079
00:41:21,640 --> 00:41:26,400
you're going to achieve and the theorem

1080
00:41:24,599 --> 00:41:28,319
here and this is a theorem that is not

1081
00:41:26,400 --> 00:41:31,560
ours it has been a uh observed in other

1082
00:41:28,319 --> 00:41:34,680
work as well that um in this case we can

1083
00:41:31,560 --> 00:41:36,880
Define um a new noise mechanism that we

1084
00:41:34,680 --> 00:41:39,920
call the shinger mechanism where we add

1085
00:41:36,880 --> 00:41:42,400
noise to your query where the noise is

1086
00:41:39,920 --> 00:41:44,880
taken from a PDF whose distribution is

1087
00:41:42,400 --> 00:41:48,280
the is equivalent to the square of the

1088
00:41:44,880 --> 00:41:51,200
igen um of the of the igen function that

1089
00:41:48,280 --> 00:41:54,000
achieves the unique smallest igen value

1090
00:41:51,200 --> 00:41:55,480
of this shinger operator um and you can

1091
00:41:54,000 --> 00:41:58,000
show that in this case the square of

1092
00:41:55,480 --> 00:42:00,160
this sig function is exactly

1093
00:41:58,000 --> 00:42:02,200
the the PDF of the noise that you would

1094
00:42:00,160 --> 00:42:03,960
add so the takeaway that I want you to

1095
00:42:02,200 --> 00:42:05,720
get from here is that KO optimal privacy

1096
00:42:03,960 --> 00:42:08,520
mechanisms in the small sensitivity

1097
00:42:05,720 --> 00:42:10,079
regime are given by igen functions of

1098
00:42:08,520 --> 00:42:12,880
the shinger

1099
00:42:10,079 --> 00:42:15,319
operator so when you choose your cost

1100
00:42:12,880 --> 00:42:18,760
function should be quadratic

1101
00:42:15,319 --> 00:42:20,640
cost then um and find the corresponding

1102
00:42:18,760 --> 00:42:23,040
igen function the this is a known result

1103
00:42:20,640 --> 00:42:25,720
in mathematical physics the function is

1104
00:42:23,040 --> 00:42:27,800
exactly the gaussin so this explains why

1105
00:42:25,720 --> 00:42:30,079
gaussian is optimal right why we

1106
00:42:27,800 --> 00:42:33,040
observed this favorable performance of

1107
00:42:30,079 --> 00:42:35,800
of gaussians in this small sensitivity

1108
00:42:33,040 --> 00:42:37,079
or large noise regime what's interesting

1109
00:42:35,800 --> 00:42:38,800
is should explore other kinds of

1110
00:42:37,079 --> 00:42:42,000
constraints so you can ask what happens

1111
00:42:38,800 --> 00:42:42,760
if I add an L1 constraint here so in

1112
00:42:42,000 --> 00:42:46,520
this

1113
00:42:42,760 --> 00:42:48,280
case um one might expect that you're

1114
00:42:46,520 --> 00:42:50,480
going to look of get something that

1115
00:42:48,280 --> 00:42:51,839
looks like laast noise which is a very

1116
00:42:50,480 --> 00:42:54,000
popular mechanism in differential

1117
00:42:51,839 --> 00:42:55,359
privacy but in fact you don't the

1118
00:42:54,000 --> 00:42:57,400
optimal noise distribution is something

1119
00:42:55,359 --> 00:42:59,920
known as an Airy distribution that's in

1120
00:42:57,400 --> 00:43:01,520
terms of these ay functions we haven't

1121
00:42:59,920 --> 00:43:05,760
seen this in differential privacy and we

1122
00:43:01,520 --> 00:43:07,880
call this the a mechanism um compared to

1123
00:43:05,760 --> 00:43:09,920
true laast to the laast noise to the

1124
00:43:07,880 --> 00:43:11,200
standard laas mechanism um these

1125
00:43:09,920 --> 00:43:13,160
distributions are actually quite

1126
00:43:11,200 --> 00:43:15,280
different not only in terms of their

1127
00:43:13,160 --> 00:43:17,559
shape but also in terms of their tail so

1128
00:43:15,280 --> 00:43:20,280
you get so if you look at a laass taale

1129
00:43:17,559 --> 00:43:21,760
like it falls as e to the minus X if you

1130
00:43:20,280 --> 00:43:23,839
were to look at a gaus the it's each to

1131
00:43:21,760 --> 00:43:28,440
the minus x square and for the AA

1132
00:43:23,839 --> 00:43:31,119
mechanism you get something in between

1133
00:43:28,440 --> 00:43:32,599
so okay so these are the two questions

1134
00:43:31,119 --> 00:43:34,599
that we were asking and we've been

1135
00:43:32,599 --> 00:43:36,079
studying for a few years now so the

1136
00:43:34,599 --> 00:43:37,559
first one is how do we measure privacy

1137
00:43:36,079 --> 00:43:39,760
loss under many compositions and how

1138
00:43:37,559 --> 00:43:41,920
should we design privacy mechanisms that

1139
00:43:39,760 --> 00:43:43,520
operate under many compositions and I

1140
00:43:41,920 --> 00:43:45,720
think by now we have a really good

1141
00:43:43,520 --> 00:43:47,359
handle of how these absolon Delta

1142
00:43:45,720 --> 00:43:48,400
privacy guarantees behave in the limit

1143
00:43:47,359 --> 00:43:51,079
of large

1144
00:43:48,400 --> 00:43:52,880
compositions um the question of how to

1145
00:43:51,079 --> 00:43:54,839
design optimal mechanisms is still

1146
00:43:52,880 --> 00:43:57,400
ongoing work what we've been observing

1147
00:43:54,839 --> 00:43:59,160
in our experiments is that many many

1148
00:43:57,400 --> 00:44:01,040
many machine learning settings of

1149
00:43:59,160 --> 00:44:03,880
interest is this kind of small

1150
00:44:01,040 --> 00:44:05,760
sensitivity regime so um effectively

1151
00:44:03,880 --> 00:44:09,200
your data is very large or you have many

1152
00:44:05,760 --> 00:44:13,440
parameters and um and in that case

1153
00:44:09,200 --> 00:44:15,160
gussan is um performs really well so it

1154
00:44:13,440 --> 00:44:17,640
it might turn out that what people have

1155
00:44:15,160 --> 00:44:19,960
been doing all along of adding gin to to

1156
00:44:17,640 --> 00:44:22,400
data is actually optimal or really close

1157
00:44:19,960 --> 00:44:24,079
to Optimal but at least now we can show

1158
00:44:22,400 --> 00:44:26,880
that Beyond just

1159
00:44:24,079 --> 00:44:29,520
theistic so in the 10 minutes that I

1160
00:44:26,880 --> 00:44:31,200
have left I just want you switch your

1161
00:44:29,520 --> 00:44:33,559
gears a little bit and just give some

1162
00:44:31,200 --> 00:44:36,000
high Lev overview of some of the work

1163
00:44:33,559 --> 00:44:38,520
that we've been doing on fairness um so

1164
00:44:36,000 --> 00:44:41,000
particularly if anyone is interesting in

1165
00:44:38,520 --> 00:44:42,440
collaborating with folks down massav and

1166
00:44:41,000 --> 00:44:44,960
across the

1167
00:44:42,440 --> 00:44:46,400
river so just for the last 10 minutes

1168
00:44:44,960 --> 00:44:49,680
I'm just going to highlight some of our

1169
00:44:46,400 --> 00:44:52,400
contributions here so the challenge

1170
00:44:49,680 --> 00:44:54,880
we've been asking for a while is the

1171
00:44:52,400 --> 00:44:57,359
challenge of group fairness which is if

1172
00:44:54,880 --> 00:44:59,280
you give me a classifier

1173
00:44:57,359 --> 00:45:01,440
and a data set right I train this

1174
00:44:59,280 --> 00:45:03,559
classifier and I want you ensure that

1175
00:45:01,440 --> 00:45:06,119
this classifier is not only accurate on

1176
00:45:03,559 --> 00:45:08,400
average but across different population

1177
00:45:06,119 --> 00:45:10,640
groups there's no significant difference

1178
00:45:08,400 --> 00:45:12,839
in performance and by difference in

1179
00:45:10,640 --> 00:45:14,680
performance here I mean for example

1180
00:45:12,839 --> 00:45:17,160
differences in accuracy or false

1181
00:45:14,680 --> 00:45:19,200
positive rates or false negative rates

1182
00:45:17,160 --> 00:45:21,480
and what I mean by population groups I

1183
00:45:19,200 --> 00:45:23,440
might address this again towards the end

1184
00:45:21,480 --> 00:45:25,640
but um what I mean by that or groups for

1185
00:45:23,440 --> 00:45:28,440
example um this has been since taken

1186
00:45:25,640 --> 00:45:31,520
down I Believe by the by the White House

1187
00:45:28,440 --> 00:45:34,040
but um uh to avoid discrimination

1188
00:45:31,520 --> 00:45:38,160
against groups determined for example by

1189
00:45:34,040 --> 00:45:41,920
uh race color ethnicity sex and um so

1190
00:45:38,160 --> 00:45:43,520
on so um there are many methods that uh

1191
00:45:41,920 --> 00:45:46,319
and many metrics should measure this

1192
00:45:43,520 --> 00:45:48,920
disparity in performance uh in so-called

1193
00:45:46,319 --> 00:45:50,960
group fairness and usually they follow

1194
00:45:48,920 --> 00:45:52,839
the following recipe at least for binary

1195
00:45:50,960 --> 00:45:54,880
classification tasks where you have an

1196
00:45:52,839 --> 00:45:57,319
input example and you have a classifier

1197
00:45:54,880 --> 00:45:59,880
output and you have a binary group group

1198
00:45:57,319 --> 00:46:03,000
attribute and then you can if um denote

1199
00:45:59,880 --> 00:46:04,520
it by S for example two groups here and

1200
00:46:03,000 --> 00:46:07,160
if you're doing binary classification

1201
00:46:04,520 --> 00:46:09,000
you can build the group fairness metrix

1202
00:46:07,160 --> 00:46:11,240
by building this confusion Matrix

1203
00:46:09,000 --> 00:46:13,720
between both groups and comp and

1204
00:46:11,240 --> 00:46:15,160
comparing their entries so example for

1205
00:46:13,720 --> 00:46:17,599
example if you trained a classifier and

1206
00:46:15,160 --> 00:46:20,720
you want the average number of ones uh

1207
00:46:17,599 --> 00:46:22,520
the average number of of inputs marked

1208
00:46:20,720 --> 00:46:23,880
as one across both groups are trying to

1209
00:46:22,520 --> 00:46:26,559
ensure something known as statistical

1210
00:46:23,880 --> 00:46:28,319
parity you might want to equalize false

1211
00:46:26,559 --> 00:46:29,800
positive positive rates that's known as

1212
00:46:28,319 --> 00:46:31,920
equalized odds but you can already see

1213
00:46:29,800 --> 00:46:36,280
that there is combinator many different

1214
00:46:31,920 --> 00:46:38,079
ways of defining fairness metrics uh in

1215
00:46:36,280 --> 00:46:41,839
terms of how your Model

1216
00:46:38,079 --> 00:46:43,920
S um when your classifier outputs a a a

1217
00:46:41,839 --> 00:46:46,040
risk score or a distribution over

1218
00:46:43,920 --> 00:46:48,040
classes uh so for example when you think

1219
00:46:46,040 --> 00:46:49,480
about your classifier as a as a

1220
00:46:48,040 --> 00:46:51,119
conditional distribution that outputs a

1221
00:46:49,480 --> 00:46:54,240
distribution over a certain set of

1222
00:46:51,119 --> 00:46:56,280
classes um you can also extend these

1223
00:46:54,240 --> 00:46:58,640
privacy def these fairness definitions

1224
00:46:56,280 --> 00:47:01,960
that is scar equalized odds and so on in

1225
00:46:58,640 --> 00:47:03,880
terms of this output distribution here

1226
00:47:01,960 --> 00:47:06,319
so something that will be important for

1227
00:47:03,880 --> 00:47:09,040
us in just a few slides is that several

1228
00:47:06,319 --> 00:47:11,640
score based group fairness metrics can

1229
00:47:09,040 --> 00:47:13,839
be written as linear constraints on the

1230
00:47:11,640 --> 00:47:15,520
classifier so this will this is an

1231
00:47:13,839 --> 00:47:17,520
important Insight because it'll allow us

1232
00:47:15,520 --> 00:47:19,359
to formulate optimization problems where

1233
00:47:17,520 --> 00:47:20,760
we add these group fairness constraints

1234
00:47:19,359 --> 00:47:23,839
as linear constraints and try to

1235
00:47:20,760 --> 00:47:26,400
minimize the change in the output of the

1236
00:47:23,839 --> 00:47:29,440
classifier but over the past like 10

1237
00:47:26,400 --> 00:47:31,079
years there's been just Myriad like

1238
00:47:29,440 --> 00:47:33,480
fairness interventions they can be

1239
00:47:31,079 --> 00:47:35,440
broadly divided into pre-processing and

1240
00:47:33,480 --> 00:47:36,800
processing and post-processing methods

1241
00:47:35,440 --> 00:47:38,839
where you change the data before you

1242
00:47:36,800 --> 00:47:41,319
train in processing methods where you

1243
00:47:38,839 --> 00:47:43,000
change the classifier while training and

1244
00:47:41,319 --> 00:47:45,640
postprocessing methods where you correct

1245
00:47:43,000 --> 00:47:47,480
the outputs of a classifier and usually

1246
00:47:45,640 --> 00:47:50,160
these interventions are compared in

1247
00:47:47,480 --> 00:47:53,319
terms of these fairness utility

1248
00:47:50,160 --> 00:47:55,720
trade-offs so here on the uh y- AIS you

1249
00:47:53,319 --> 00:47:57,480
have accuracy on the x-axis you have

1250
00:47:55,720 --> 00:48:00,480
some kind of fairness metric

1251
00:47:57,480 --> 00:48:01,960
and you compare what accuracy on average

1252
00:48:00,480 --> 00:48:04,400
does your model preserve while closing

1253
00:48:01,960 --> 00:48:07,160
some kind of disparity Gap in fact there

1254
00:48:04,400 --> 00:48:08,920
was a study recently that showed that

1255
00:48:07,160 --> 00:48:09,920
this is already from last year so this

1256
00:48:08,920 --> 00:48:12,960
number probably increased that there

1257
00:48:09,920 --> 00:48:15,920
were 341 Publications with over 400

1258
00:48:12,960 --> 00:48:18,440
methods proposing different fairness

1259
00:48:15,920 --> 00:48:20,000
interventions so our goal was like well

1260
00:48:18,440 --> 00:48:21,720
can we design a better fairness

1261
00:48:20,000 --> 00:48:24,040
intervention and specifically how can we

1262
00:48:21,720 --> 00:48:26,400
show that maybe we're approaching some

1263
00:48:24,040 --> 00:48:29,079
kind of paral Frontier of the best that

1264
00:48:26,400 --> 00:48:31,599
you can do so our goal was to design a

1265
00:48:29,079 --> 00:48:33,920
fairness intervention that worked for

1266
00:48:31,599 --> 00:48:35,920
large data sets multiple classes and

1267
00:48:33,920 --> 00:48:39,119
multiple potentially intersectional

1268
00:48:35,920 --> 00:48:40,880
groups for ensuring U performance and

1269
00:48:39,119 --> 00:48:43,800
there are many fairness interventions

1270
00:48:40,880 --> 00:48:46,160
out there but most of them have certain

1271
00:48:43,800 --> 00:48:48,359
limitations they were benchmarked on

1272
00:48:46,160 --> 00:48:50,680
overused data sets like abon compass

1273
00:48:48,359 --> 00:48:53,480
that were pretty small um they only

1274
00:48:50,680 --> 00:48:55,520
consider either binary classification or

1275
00:48:53,480 --> 00:48:57,760
only consider disparities in performance

1276
00:48:55,520 --> 00:49:00,599
in two groups

1277
00:48:57,760 --> 00:49:02,240
so um in Europe a couple of years back

1278
00:49:00,599 --> 00:49:04,400
we proposed a new fairness intervention

1279
00:49:02,240 --> 00:49:06,599
called Fair projection that overcomes

1280
00:49:04,400 --> 00:49:08,920
these limitations we Benchmark it on a

1281
00:49:06,599 --> 00:49:10,520
data set with over a million samples it

1282
00:49:08,920 --> 00:49:11,920
can be applied training classifier

1283
00:49:10,520 --> 00:49:13,640
without retraining it's a

1284
00:49:11,920 --> 00:49:15,520
post-processing method so the complexity

1285
00:49:13,640 --> 00:49:17,520
is the same if you're training a a

1286
00:49:15,520 --> 00:49:19,760
neural network or if you're training a

1287
00:49:17,520 --> 00:49:21,520
logistic regression and can be used for

1288
00:49:19,760 --> 00:49:24,000
any finite number of classes and

1289
00:49:21,520 --> 00:49:25,319
protected groups and F projection is

1290
00:49:24,000 --> 00:49:27,160
actually based on an optimization

1291
00:49:25,319 --> 00:49:28,839
formulation known as information

1292
00:49:27,160 --> 00:49:30,680
projection that you might be familiar

1293
00:49:28,839 --> 00:49:35,640
from your information Theory

1294
00:49:30,680 --> 00:49:39,000
class um where the vanilla information

1295
00:49:35,640 --> 00:49:41,240
projection you have a distribution p and

1296
00:49:39,000 --> 00:49:42,880
you want to project onto a convex set of

1297
00:49:41,240 --> 00:49:45,920
distribution scal of C that are

1298
00:49:42,880 --> 00:49:48,200
determined by a finite set of moment

1299
00:49:45,920 --> 00:49:49,599
constraints so the idea is that if you

1300
00:49:48,200 --> 00:49:51,359
have a reference distribution P you want

1301
00:49:49,599 --> 00:49:53,520
to find a new distribution Q in the set

1302
00:49:51,359 --> 00:49:55,799
that minimize minimizes the quote

1303
00:49:53,520 --> 00:49:57,599
unquote distance often formulated in

1304
00:49:55,799 --> 00:50:00,200
terms of K diers

1305
00:49:57,599 --> 00:50:01,599
subject to uh being in this class and

1306
00:50:00,200 --> 00:50:03,240
this finds many applications in large

1307
00:50:01,599 --> 00:50:05,480
deviation Theory Universal are scolding

1308
00:50:03,240 --> 00:50:08,240
compression and Beyond you can actually

1309
00:50:05,480 --> 00:50:10,960
generalize this this this projection

1310
00:50:08,240 --> 00:50:13,160
operation in terms of a f Divergence so

1311
00:50:10,960 --> 00:50:15,480
you can ask what is the closest uh

1312
00:50:13,160 --> 00:50:17,839
distribution in terms of a given F

1313
00:50:15,480 --> 00:50:19,640
Divergence um and you can further

1314
00:50:17,839 --> 00:50:21,760
General this was done by cheer in the

1315
00:50:19,640 --> 00:50:23,480
70s and you can actually further

1316
00:50:21,760 --> 00:50:25,640
generalize this and instead of

1317
00:50:23,480 --> 00:50:27,200
projecting a single distribution you

1318
00:50:25,640 --> 00:50:29,400
project a conditional distribution a

1319
00:50:27,200 --> 00:50:31,599
channel right and you can ask well what

1320
00:50:29,400 --> 00:50:34,920
is the channel within a set of channels

1321
00:50:31,599 --> 00:50:37,559
that satisfies a convex set a convex set

1322
00:50:34,920 --> 00:50:39,880
of moment constraints that minimizes the

1323
00:50:37,559 --> 00:50:42,680
this conditional F

1324
00:50:39,880 --> 00:50:44,440
Divergence um we presented this at IIT

1325
00:50:42,680 --> 00:50:46,119
already now a few years back it's not a

1326
00:50:44,440 --> 00:50:47,880
direct extension of information

1327
00:50:46,119 --> 00:50:49,720
projection you have to be care take a

1328
00:50:47,880 --> 00:50:51,920
little bit of care to show that this the

1329
00:50:49,720 --> 00:50:54,119
solution to this optimization is well

1330
00:50:51,920 --> 00:50:56,079
defined and model projection is very

1331
00:50:54,119 --> 00:50:58,160
useful for ensuring fairness because in

1332
00:50:56,079 --> 00:51:00,079
this case as I mentioned earlier in the

1333
00:50:58,160 --> 00:51:02,000
talk you can think about your

1334
00:51:00,079 --> 00:51:04,040
conditional distribution as your model

1335
00:51:02,000 --> 00:51:06,160
right as your classifier so you can

1336
00:51:04,040 --> 00:51:07,720
think about a classifier as a channel

1337
00:51:06,160 --> 00:51:10,640
where a sample comes in and a

1338
00:51:07,720 --> 00:51:14,760
distribution over classes comes out

1339
00:51:10,640 --> 00:51:16,839
right um so a dist um and as I mentioned

1340
00:51:14,760 --> 00:51:18,400
earlier the set of group fairness uh

1341
00:51:16,839 --> 00:51:21,200
several group fairness constraints can

1342
00:51:18,400 --> 00:51:23,319
be written as linear constraints on this

1343
00:51:21,200 --> 00:51:25,200
classifier so you can formulate the

1344
00:51:23,319 --> 00:51:27,200
problem of minimizing the changes of the

1345
00:51:25,200 --> 00:51:29,040
output of a classifier

1346
00:51:27,200 --> 00:51:31,240
um or finding a new classifier that

1347
00:51:29,040 --> 00:51:33,200
minimizes the average change of a base

1348
00:51:31,240 --> 00:51:35,839
classifier but also satisfying your

1349
00:51:33,200 --> 00:51:38,480
fairness constraint as an F Divergence

1350
00:51:35,839 --> 00:51:40,240
minimization problem that looks like

1351
00:51:38,480 --> 00:51:43,960
model

1352
00:51:40,240 --> 00:51:46,559
projection and you can prove that the um

1353
00:51:43,960 --> 00:51:48,240
optimal output the optimal classifier

1354
00:51:46,559 --> 00:51:49,839
that you get is actually a tilting just

1355
00:51:48,240 --> 00:51:53,400
like we saw earlier but now an F

1356
00:51:49,839 --> 00:51:55,000
Divergence dependent tilting of your of

1357
00:51:53,400 --> 00:51:57,119
your original classifier where these

1358
00:51:55,000 --> 00:52:00,160
dual parameters these parameters of the

1359
00:51:57,119 --> 00:52:03,160
tilting depend on the your F Divergence

1360
00:52:00,160 --> 00:52:05,160
of choice and on your fairness

1361
00:52:03,160 --> 00:52:06,960
constraints and this is the basis of an

1362
00:52:05,160 --> 00:52:10,440
algorithm that we proposed that was an

1363
00:52:06,960 --> 00:52:12,440
oral ad neurs a couple of years ago um

1364
00:52:10,440 --> 00:52:14,920
called fur projection which is a model

1365
00:52:12,440 --> 00:52:16,200
agnostic uh post-processing approach

1366
00:52:14,920 --> 00:52:18,079
that can be applied to any based

1367
00:52:16,200 --> 00:52:19,839
classifiers so if you buy this model

1368
00:52:18,079 --> 00:52:22,839
projection framework this is that

1369
00:52:19,839 --> 00:52:26,040
post-processing is the best thing to do

1370
00:52:22,839 --> 00:52:28,440
and um we tested this as well uh you can

1371
00:52:26,040 --> 00:52:29,960
implement it this um this algorithm is

1372
00:52:28,440 --> 00:52:32,920
highly paralyzable you can solve it

1373
00:52:29,960 --> 00:52:34,240
using admm this is the the algorithm uh

1374
00:52:32,920 --> 00:52:36,319
for fair projection I'm not going to go

1375
00:52:34,240 --> 00:52:38,119
into details but the important thing is

1376
00:52:36,319 --> 00:52:41,640
that it you can paralyze it and run it

1377
00:52:38,119 --> 00:52:44,839
on a GPU using standard like uh proximal

1378
00:52:41,640 --> 00:52:46,920
admm like algorithms and we benchmarked

1379
00:52:44,839 --> 00:52:49,040
it on several data sets data sets that

1380
00:52:46,920 --> 00:52:50,520
are of a nation state scale with like

1381
00:52:49,040 --> 00:52:52,559
this is a data set from the Brazilian

1382
00:52:50,520 --> 00:52:55,599
University entrance exam with over a

1383
00:52:52,559 --> 00:52:57,880
million samples and this is restricted

1384
00:52:55,599 --> 00:52:59,599
to binary classif ification because most

1385
00:52:57,880 --> 00:53:01,079
competing Methods at the time were only

1386
00:52:59,599 --> 00:53:03,960
restricted to Binary classification

1387
00:53:01,079 --> 00:53:06,799
tasks and and you can see here the dark

1388
00:53:03,960 --> 00:53:09,319
red line a which is for projection is

1389
00:53:06,799 --> 00:53:10,839
competitive with the state-ofthe-art um

1390
00:53:09,319 --> 00:53:12,000
in this case our base model is a random

1391
00:53:10,839 --> 00:53:14,880
Force

1392
00:53:12,000 --> 00:53:16,920
classifier um the most competitive

1393
00:53:14,880 --> 00:53:19,359
method to ours was the reductions

1394
00:53:16,920 --> 00:53:20,839
approach but our method ran like a 100

1395
00:53:19,359 --> 00:53:24,119
times faster exactly because we can

1396
00:53:20,839 --> 00:53:26,799
paralyze it on GPU and since then we've

1397
00:53:24,119 --> 00:53:28,720
tded on other methods also for

1398
00:53:26,799 --> 00:53:30,839
correcting your networks and things like

1399
00:53:28,720 --> 00:53:34,720
that so the last thing I'm going to

1400
00:53:30,839 --> 00:53:36,599
mention is that if you look at this this

1401
00:53:34,720 --> 00:53:38,680
plot it seems like there's some

1402
00:53:36,599 --> 00:53:42,480
diminishing returns here right it seems

1403
00:53:38,680 --> 00:53:44,200
like oh we we're optimizing this this

1404
00:53:42,480 --> 00:53:46,400
fairness correction process but it seems

1405
00:53:44,200 --> 00:53:49,280
like we're not getting a lot out of it

1406
00:53:46,400 --> 00:53:51,799
so there's a question if are we close to

1407
00:53:49,280 --> 00:53:53,200
this best fairness accuracy Frontier

1408
00:53:51,799 --> 00:53:55,960
like for example is there something like

1409
00:53:53,200 --> 00:53:57,599
a Converse bound that we could show that

1410
00:53:55,960 --> 00:53:59,920
proves that maybe we're already very

1411
00:53:57,599 --> 00:54:01,520
close to the best that we can do and

1412
00:53:59,920 --> 00:54:05,559
this is something we presented at

1413
00:54:01,520 --> 00:54:07,680
Europe's 2023 that in fact we can for

1414
00:54:05,559 --> 00:54:09,319
certain semisynthetic experiments derive

1415
00:54:07,680 --> 00:54:11,160
this Converse bound it's based on a

1416
00:54:09,319 --> 00:54:14,400
result by black B comparison of

1417
00:54:11,160 --> 00:54:17,079
experiments and for popular data sets

1418
00:54:14,400 --> 00:54:19,880
you can show that we are actually very

1419
00:54:17,079 --> 00:54:24,040
close to

1420
00:54:19,880 --> 00:54:27,079
Optimal so yeah I'm going to wrap up

1421
00:54:24,040 --> 00:54:28,079
here so if you found this in interesting

1422
00:54:27,079 --> 00:54:31,599
and you're looking for folks to

1423
00:54:28,079 --> 00:54:34,040
collaborate around the river um we have

1424
00:54:31,599 --> 00:54:36,000
also other ongoing work and that range

1425
00:54:34,040 --> 00:54:37,400
from like pure kind of Shannon Theory

1426
00:54:36,000 --> 00:54:40,640
kind of things on measuring information

1427
00:54:37,400 --> 00:54:42,640
from moments to things that are uh more

1428
00:54:40,640 --> 00:54:44,160
uh coding theoretic flavored like

1429
00:54:42,640 --> 00:54:46,599
approximate coding Computing and private

1430
00:54:44,160 --> 00:54:47,799
Vector data database retrieval work on

1431
00:54:46,599 --> 00:54:51,680
like trying to understand the

1432
00:54:47,799 --> 00:54:53,440
generalization properties of of of

1433
00:54:51,680 --> 00:54:54,960
learning algorithms based on these

1434
00:54:53,440 --> 00:54:58,520
information theoretic

1435
00:54:54,960 --> 00:55:00,319
quantities and um to wrap up just

1436
00:54:58,520 --> 00:55:03,760
talking going back to the Divergence

1437
00:55:00,319 --> 00:55:06,799
between theory and practice

1438
00:55:03,760 --> 00:55:08,920
so information Theory can help us can

1439
00:55:06,799 --> 00:55:10,240
help guide the design of trustworthy or

1440
00:55:08,920 --> 00:55:13,760
more trustworthy machine learning

1441
00:55:10,240 --> 00:55:15,880
algorithms through for formulations that

1442
00:55:13,760 --> 00:55:17,599
are imperfect right specifically when

1443
00:55:15,880 --> 00:55:21,319
you talk about like regulation of

1444
00:55:17,599 --> 00:55:24,599
privacy and and of I don't know ethical

1445
00:55:21,319 --> 00:55:26,480
aspects of fairness but they are good

1446
00:55:24,599 --> 00:55:30,400
because they can be they're formal they

1447
00:55:26,480 --> 00:55:31,440
can be analyzed critiqued and tested um

1448
00:55:30,400 --> 00:55:32,720
another thing we've been doing in my

1449
00:55:31,440 --> 00:55:35,119
group is also trying to bring these

1450
00:55:32,720 --> 00:55:37,440
things in practice both with um Outreach

1451
00:55:35,119 --> 00:55:40,000
programs in Latin America to

1452
00:55:37,440 --> 00:55:42,319
collaborations with like the op DP uh

1453
00:55:40,000 --> 00:55:43,799
project at Harvard where uh which is a

1454
00:55:42,319 --> 00:55:46,200
big project for developing open

1455
00:55:43,799 --> 00:55:48,280
differential privacy software um we

1456
00:55:46,200 --> 00:55:50,319
actually wrote a policy brief that was

1457
00:55:48,280 --> 00:55:51,680
published at the G20 meeting in Brazil

1458
00:55:50,319 --> 00:55:55,480
about arbitrariness and fairness and

1459
00:55:51,680 --> 00:55:57,160
machine learning and so on um we have a

1460
00:55:55,480 --> 00:56:00,039
big research group I have a my research

1461
00:55:57,160 --> 00:56:02,280
group is now about 12 people uh all

1462
00:56:00,039 --> 00:56:04,799
thinking about things that are very uh

1463
00:56:02,280 --> 00:56:07,200
close to the lit student heart like

1464
00:56:04,799 --> 00:56:10,160
learning data optimization uh

1465
00:56:07,200 --> 00:56:12,839
information Theory and so on um and I

1466
00:56:10,160 --> 00:56:15,140
would be happy to answer any questions

1467
00:56:12,839 --> 00:56:19,370
thanks

1468
00:56:15,140 --> 00:56:19,370
[Applause]

