1
00:00:00,000 --> 00:00:09,210

2
00:00:09,210 --> 00:00:10,570
ANJALI SASTRY: Hello, everyone.

3
00:00:10,570 --> 00:00:14,580
This is a topic we are all
thinking about, reading about,

4
00:00:14,580 --> 00:00:19,320
and really looking forward to
talking to each other about.

5
00:00:19,320 --> 00:00:26,220
We are delighted to continue
our work with Bill Bonvillian,

6
00:00:26,220 --> 00:00:34,660
an esteemed and thoughtful,
multifaceted colleague, whose

7
00:00:34,660 --> 00:00:42,280
work has included working
in the US government.

8
00:00:42,280 --> 00:00:45,250
He was 17 years a
senior policy advisor

9
00:00:45,250 --> 00:00:51,460
in the US Senate, an expert on
technology and science policy,

10
00:00:51,460 --> 00:00:58,000
and then joined MIT and became
Director of our DC office.

11
00:00:58,000 --> 00:01:01,840
That put him directly
into conversation

12
00:01:01,840 --> 00:01:06,400
at the interface between MIT,
the US government, and broader

13
00:01:06,400 --> 00:01:09,910
society in trying to
understand big ideas,

14
00:01:09,910 --> 00:01:14,080
orchestrate new threads,
and themes in the work we're

15
00:01:14,080 --> 00:01:17,650
doing within universities
across the country,

16
00:01:17,650 --> 00:01:20,890
and really putting
the role of MIT

17
00:01:20,890 --> 00:01:23,080
as a science and
tech leader in front

18
00:01:23,080 --> 00:01:25,330
of lots of different folks.

19
00:01:25,330 --> 00:01:28,480
Bill's been thinking
a lot about AI

20
00:01:28,480 --> 00:01:33,940
and has invited our
special guest for today,

21
00:01:33,940 --> 00:01:35,703
Professor Justin Reich.

22
00:01:35,703 --> 00:01:37,870
So I'll say a couple of
words about introducing him,

23
00:01:37,870 --> 00:01:40,330
and then turn it
over to Bill, who's

24
00:01:40,330 --> 00:01:43,840
put together some thoughts that
could guide our conversation.

25
00:01:43,840 --> 00:01:52,450
Justin's an expert in learning,
in digital media, in education.

26
00:01:52,450 --> 00:01:54,820
He's published all
over the place--

27
00:01:54,820 --> 00:01:59,870
Science, PNAS, Washington
Post, The Atlantic, and books,

28
00:01:59,870 --> 00:02:01,630
one of which you see
right behind you.

29
00:02:01,630 --> 00:02:04,780

30
00:02:04,780 --> 00:02:08,740
Both his books tackle
the kind of challenge

31
00:02:08,740 --> 00:02:13,360
of harnessing new
potentially really good ideas

32
00:02:13,360 --> 00:02:17,410
and turning them into
practical impactful ideas that

33
00:02:17,410 --> 00:02:19,510
work to improve education.

34
00:02:19,510 --> 00:02:22,540
He is Director of
the Teaching Systems

35
00:02:22,540 --> 00:02:26,710
Lab in the Comparative Media
Studies and Writing Department

36
00:02:26,710 --> 00:02:27,820
at MIT.

37
00:02:27,820 --> 00:02:31,400
So he runs the show
on research that

38
00:02:31,400 --> 00:02:38,090
looks at systematic data
driven and impactful ways

39
00:02:38,090 --> 00:02:39,170
to improve teaching.

40
00:02:39,170 --> 00:02:41,780

41
00:02:41,780 --> 00:02:44,180
Sometimes he hosts a podcast.

42
00:02:44,180 --> 00:02:48,830
Maybe we can get him excited to
do a new J-WEL inspired season

43
00:02:48,830 --> 00:02:50,360
sometime.

44
00:02:50,360 --> 00:02:55,400
And has been a fellow in
various roles at Harvard,

45
00:02:55,400 --> 00:02:59,810
giving him a chance to really
dig into big issues and policy

46
00:02:59,810 --> 00:03:00,690
issues.

47
00:03:00,690 --> 00:03:05,270
So now Bill, help us think
about the big questions

48
00:03:05,270 --> 00:03:07,016
on the docket today.

49
00:03:07,016 --> 00:03:09,500
WILLIAM B. BONVILLIAN:
Thanks, Anjali.

50
00:03:09,500 --> 00:03:15,200
As all of us know,
in November of 2022,

51
00:03:15,200 --> 00:03:19,290
OpenAI introduced
ChatGPT to the world.

52
00:03:19,290 --> 00:03:23,420
And a lot of us have been
communicating with ChatGPT

53
00:03:23,420 --> 00:03:30,080
since then, using it for all
kinds of things, good and bad.

54
00:03:30,080 --> 00:03:36,060
And today's topic really is
all about AI in education.

55
00:03:36,060 --> 00:03:39,120
And obviously it's a
development that schools

56
00:03:39,120 --> 00:03:40,880
need to pay a lot
of attention to,

57
00:03:40,880 --> 00:03:44,910
and understand and grasp and
figure out how to work with.

58
00:03:44,910 --> 00:03:47,520
There are a lot of
potential uses of AI

59
00:03:47,520 --> 00:03:51,650
in education, the
personalization of education,

60
00:03:51,650 --> 00:03:54,860
the ability to really
produce materials

61
00:03:54,860 --> 00:03:58,370
that can be adapted to each
student's individual learning

62
00:03:58,370 --> 00:04:00,560
needs and really
target instruction.

63
00:04:00,560 --> 00:04:03,200
And that potentially
could lead to a system

64
00:04:03,200 --> 00:04:06,560
of digital tutoring, always
kind of a dream out there.

65
00:04:06,560 --> 00:04:10,250
But AI could help get us there.

66
00:04:10,250 --> 00:04:14,240
It can offer
feedback for teachers

67
00:04:14,240 --> 00:04:16,970
on how their students
are doing and help

68
00:04:16,970 --> 00:04:20,510
them target improvements
by providing

69
00:04:20,510 --> 00:04:22,880
materials and
approaches to students

70
00:04:22,880 --> 00:04:25,070
that need to catch on better.

71
00:04:25,070 --> 00:04:30,110
And it can also provide
meaningful, and very quick,

72
00:04:30,110 --> 00:04:33,530
like immediate contact
feedback to students

73
00:04:33,530 --> 00:04:36,770
to help them avoid
making the mistakes they

74
00:04:36,770 --> 00:04:41,460
could be stumbling into
and get back on track.

75
00:04:41,460 --> 00:04:44,690
And it can also, and this
is a potentially big use,

76
00:04:44,690 --> 00:04:47,900
reduce time on the burden
of administrative tasks

77
00:04:47,900 --> 00:04:51,270
that all of us as teachers face.

78
00:04:51,270 --> 00:04:55,560
But there are obviously a
series of potential drawbacks.

79
00:04:55,560 --> 00:04:59,400
One of those is reduced
human interaction,

80
00:04:59,400 --> 00:05:02,850
a reduction of
face-to-face communication

81
00:05:02,850 --> 00:05:05,730
that can really affect,
negatively affect,

82
00:05:05,730 --> 00:05:08,100
the development
of social skills.

83
00:05:08,100 --> 00:05:11,040
And in turn that's part
of an overdependence

84
00:05:11,040 --> 00:05:14,790
on technology problem, where
students reduce their ability

85
00:05:14,790 --> 00:05:18,750
or motivation to think
critically and independently.

86
00:05:18,750 --> 00:05:24,480
They rely too much on
AI as problem solver.

87
00:05:24,480 --> 00:05:28,260
Reduced interaction with
teachers, AI is a substitute,

88
00:05:28,260 --> 00:05:30,840
yet teachers play
such a critical role

89
00:05:30,840 --> 00:05:35,610
in fostering kind of
emotional intelligence,

90
00:05:35,610 --> 00:05:38,280
critical thinking in students,
this whole mentoring role

91
00:05:38,280 --> 00:05:42,810
that teachers are so crucial
to that we still very much need

92
00:05:42,810 --> 00:05:45,730
of course.

93
00:05:45,730 --> 00:05:48,840
There's all kinds of data
privacy and security issues.

94
00:05:48,840 --> 00:05:51,690
And there's deep ethical issues.

95
00:05:51,690 --> 00:05:55,560
AI can enable large
scale cheating.

96
00:05:55,560 --> 00:05:59,670
And it's hard to hold AI systems
accountable for the information

97
00:05:59,670 --> 00:06:00,910
that they provide.

98
00:06:00,910 --> 00:06:03,757
So there are, with
all technologies,

99
00:06:03,757 --> 00:06:05,340
there are always a
double edged sword.

100
00:06:05,340 --> 00:06:07,840
There are great positives and
there are potential negatives,

101
00:06:07,840 --> 00:06:10,650
and AI has got a lot of both.

102
00:06:10,650 --> 00:06:14,790
And here to lead us through
this is Justin Reich

103
00:06:14,790 --> 00:06:18,300
who's done a tremendous amount
of thinking and work with AI,

104
00:06:18,300 --> 00:06:21,570
and can lead us through both
the positives and the negatives.

105
00:06:21,570 --> 00:06:23,642
Justin, all yours.

106
00:06:23,642 --> 00:06:24,600
JUSTIN REICH: Terrific.

107
00:06:24,600 --> 00:06:25,890
Thanks, Bill.

108
00:06:25,890 --> 00:06:27,135
Nice to see everybody here.

109
00:06:27,135 --> 00:06:29,790

110
00:06:29,790 --> 00:06:33,240
I was on a panel
recently with Jim Waldo,

111
00:06:33,240 --> 00:06:36,930
who's the chief technical
officer at Harvard.

112
00:06:36,930 --> 00:06:41,940
And Jim, something like 40 years
ago got a PhD in philosophy,

113
00:06:41,940 --> 00:06:45,220
and then as software
engineering was getting started,

114
00:06:45,220 --> 00:06:47,095
he shifted over into that.

115
00:06:47,095 --> 00:06:48,220
So he's an interesting guy.

116
00:06:48,220 --> 00:06:49,678
If you have an
Android phone you're

117
00:06:49,678 --> 00:06:51,430
probably running
some of the code

118
00:06:51,430 --> 00:06:53,260
that he wrote back
in the day now.

119
00:06:53,260 --> 00:06:56,523
Now he's come back to Harvard to
be the chief technical officer.

120
00:06:56,523 --> 00:06:57,940
And at the start
of this panel, he

121
00:06:57,940 --> 00:06:59,898
said something that I
thought was very helpful.

122
00:06:59,898 --> 00:07:02,830
He says, well before we
start, what do you mean by AI?

123
00:07:02,830 --> 00:07:05,800
Because AI is the
term that we use

124
00:07:05,800 --> 00:07:08,830
to apply to computing
technologies that are new

125
00:07:08,830 --> 00:07:11,320
and that we don't
fully understand.

126
00:07:11,320 --> 00:07:16,570
15 years ago, topic modeling,
latent Dirichlet allocation

127
00:07:16,570 --> 00:07:18,310
would have been AI.

128
00:07:18,310 --> 00:07:20,770
10 years ago, machine
learning would have been AI.

129
00:07:20,770 --> 00:07:23,650
Five years ago, neural
nets would have been AI.

130
00:07:23,650 --> 00:07:26,020
What we're mostly going
to talk about today

131
00:07:26,020 --> 00:07:29,980
are generative pretrained
transformers, which we now

132
00:07:29,980 --> 00:07:31,450
call generative AI.

133
00:07:31,450 --> 00:07:33,467
But in 10 years, we
probably won't call them AI.

134
00:07:33,467 --> 00:07:35,050
We'll call them GPTs,
or we'll come up

135
00:07:35,050 --> 00:07:36,730
with some other name for them.

136
00:07:36,730 --> 00:07:42,160
And it will be because, in its
actual functioning definition,

137
00:07:42,160 --> 00:07:44,380
the term AI more than
anything else, refers

138
00:07:44,380 --> 00:07:46,480
to computing
technologies that are new

139
00:07:46,480 --> 00:07:49,510
and not well understood yet.

140
00:07:49,510 --> 00:07:51,790
To me that's helpful
because it reminds us

141
00:07:51,790 --> 00:07:56,020
that one of our first tasks
in encountering things

142
00:07:56,020 --> 00:07:58,330
that we call AI is
to demystify them,

143
00:07:58,330 --> 00:08:01,090
is to say, all right, like
what are these things actually

144
00:08:01,090 --> 00:08:01,760
doing?

145
00:08:01,760 --> 00:08:04,660
And if we understand what these
things are actually doing,

146
00:08:04,660 --> 00:08:07,330
then we can think
about, OK, what

147
00:08:07,330 --> 00:08:11,020
are the roles that they might
have in educational systems?

148
00:08:11,020 --> 00:08:12,910
Where will they harm students?

149
00:08:12,910 --> 00:08:15,280
Where will they
reinforce inequalities?

150
00:08:15,280 --> 00:08:17,260
Where will they
discriminate against people?

151
00:08:17,260 --> 00:08:18,970
Where might they be helpful?

152
00:08:18,970 --> 00:08:22,220
Where might they
advance learning?

153
00:08:22,220 --> 00:08:25,390
So that's one thing that
we'll do together today

154
00:08:25,390 --> 00:08:27,730
is to try to do a
little demystifying.

155
00:08:27,730 --> 00:08:29,350
The second thing
that we'll do today

156
00:08:29,350 --> 00:08:31,100
is just to think about
the questions like,

157
00:08:31,100 --> 00:08:36,380
what would be the reasons why we
would care about AI in schools,

158
00:08:36,380 --> 00:08:40,120
particularly for those of
you who manage institutions,

159
00:08:40,120 --> 00:08:45,010
like what is it about AI that is
bringing this to your doorstep?

160
00:08:45,010 --> 00:08:48,970
What is it that's making
this feel like not just one

161
00:08:48,970 --> 00:08:52,313
of many things that
educators might consider

162
00:08:52,313 --> 00:08:53,980
addressing in this
moment, but something

163
00:08:53,980 --> 00:08:55,340
that feels of more urgent.

164
00:08:55,340 --> 00:08:57,340
So I'll throw out
to begin with, I'll

165
00:08:57,340 --> 00:09:00,340
suggest four of
these big categories

166
00:09:00,340 --> 00:09:05,350
that we might touch on as we
go throughout our conversation.

167
00:09:05,350 --> 00:09:11,380
I think a good reason to
attend to new generative AI

168
00:09:11,380 --> 00:09:14,950
technologies is that they
are altering the working

169
00:09:14,950 --> 00:09:16,900
world and the civic sphere.

170
00:09:16,900 --> 00:09:20,800
There are jobs in
which doing those jobs

171
00:09:20,800 --> 00:09:23,560
increasingly
involves interacting

172
00:09:23,560 --> 00:09:27,040
with text generation systems,
with image generation systems,

173
00:09:27,040 --> 00:09:30,130
with code, with software
generation systems.

174
00:09:30,130 --> 00:09:34,240
And when the world around
our schools change,

175
00:09:34,240 --> 00:09:38,470
we should think about how
the world inside our schools

176
00:09:38,470 --> 00:09:39,550
change.

177
00:09:39,550 --> 00:09:45,940
When we find that political
campaigns are being affected

178
00:09:45,940 --> 00:09:50,890
by deep fake or shallow fake
or any kind of artificial video

179
00:09:50,890 --> 00:09:52,690
or audio or things
like that, we ought

180
00:09:52,690 --> 00:09:54,315
to be thinking to
ourselves, all right,

181
00:09:54,315 --> 00:09:57,800
if we want our young people
to thrive in civil society,

182
00:09:57,800 --> 00:10:01,690
what do they need to be
able to do that thriving?

183
00:10:01,690 --> 00:10:06,010
It might be that
new AI generative

184
00:10:06,010 --> 00:10:08,560
tools, generative text
tools, can empower educators

185
00:10:08,560 --> 00:10:11,200
in their mission to
improve learning.

186
00:10:11,200 --> 00:10:14,380
There are going to
be a class of people

187
00:10:14,380 --> 00:10:17,050
who are generating these
tools who are going to tell us

188
00:10:17,050 --> 00:10:20,950
with great enthusiasm how
profoundly transformative

189
00:10:20,950 --> 00:10:22,420
these tools will be.

190
00:10:22,420 --> 00:10:25,510
Those folks will
probably be wrong.

191
00:10:25,510 --> 00:10:28,330
We can make a good guess that
they'll probably be wrong

192
00:10:28,330 --> 00:10:31,780
because people have been
making these claims really

193
00:10:31,780 --> 00:10:36,760
for about a century now, if
you count analog technologies.

194
00:10:36,760 --> 00:10:40,740
In 1913, Thomas Edison stood
in front of the US Congress--

195
00:10:40,740 --> 00:10:42,490
well, he was probably
sitting --but he sat

196
00:10:42,490 --> 00:10:43,930
in front of the US Congress.

197
00:10:43,930 --> 00:10:46,390
And he said in 10
years, textbooks

198
00:10:46,390 --> 00:10:48,280
will be replaced by filmstrips.

199
00:10:48,280 --> 00:10:52,150
The textbooks are no more
than 2% efficient in learning,

200
00:10:52,150 --> 00:10:56,240
but filmstrips could be
100% efficient in learning.

201
00:10:56,240 --> 00:10:59,640
And as you might have noticed,
he was not correct about that.

202
00:10:59,640 --> 00:11:02,140
Although 10 years later, he did
go back in front of Congress

203
00:11:02,140 --> 00:11:06,970
in 1923, and said in
20 years all textbooks

204
00:11:06,970 --> 00:11:10,210
will be replaced by filmstrips.

205
00:11:10,210 --> 00:11:15,165
But even if it's unlikely that
new generative AI tools are

206
00:11:15,165 --> 00:11:16,540
going to completely
revolutionize

207
00:11:16,540 --> 00:11:18,580
how we do teaching
and learning, it's

208
00:11:18,580 --> 00:11:20,710
reasonable to suspect
that there may

209
00:11:20,710 --> 00:11:22,630
be some applications
of these tools that

210
00:11:22,630 --> 00:11:24,260
help us do our work better.

211
00:11:24,260 --> 00:11:27,040
And if we can use these
tools to improve learning,

212
00:11:27,040 --> 00:11:29,980
we should figure out
how we should do that.

213
00:11:29,980 --> 00:11:33,400
One of the main things
that our schools do

214
00:11:33,400 --> 00:11:37,960
is prepare people to navigate
their information landscape.

215
00:11:37,960 --> 00:11:40,600
And our information
landscape is about to be

216
00:11:40,600 --> 00:11:45,520
flooded by low quality machine
generated text, images, video.

217
00:11:45,520 --> 00:11:49,420
It was funny to me at the dawn--

218
00:11:49,420 --> 00:11:53,990
whenever it was November of
2022, when folks were saying,

219
00:11:53,990 --> 00:11:59,020
man, this new generative text is
going to save us so much time.

220
00:11:59,020 --> 00:12:02,200
Think of all the time we
spend generating text.

221
00:12:02,200 --> 00:12:04,077
And the computer will
do some of that for us.

222
00:12:04,077 --> 00:12:05,660
And that's going to
save us some time.

223
00:12:05,660 --> 00:12:08,470
And it was actually Ian
Bogost in The Atlantic,

224
00:12:08,470 --> 00:12:12,850
who just a few weeks after that
was like, you guys are crazy.

225
00:12:12,850 --> 00:12:15,790
Our time is going
to get hoovered up.

226
00:12:15,790 --> 00:12:18,940
It's going to be
decimated sorting through

227
00:12:18,940 --> 00:12:22,030
all the low quality text
that everyone around us

228
00:12:22,030 --> 00:12:22,930
is generating.

229
00:12:22,930 --> 00:12:25,600
It's kind of a collective
action problem.

230
00:12:25,600 --> 00:12:28,000
But sorting through that
information landscape

231
00:12:28,000 --> 00:12:31,150
is going to be one of the
main challenges that we face.

232
00:12:31,150 --> 00:12:33,640
And then here's the
one that really, I

233
00:12:33,640 --> 00:12:37,720
think for folks who are
leading institutions, gets them

234
00:12:37,720 --> 00:12:40,060
right away, which
is that the kids are

235
00:12:40,060 --> 00:12:41,390
using it to do their work.

236
00:12:41,390 --> 00:12:42,670
The kids are cheating.

237
00:12:42,670 --> 00:12:46,400
They're doing it in
ways and at rates

238
00:12:46,400 --> 00:12:49,280
that are very unfamiliar to us.

239
00:12:49,280 --> 00:12:51,590
It is accelerating a problem.

240
00:12:51,590 --> 00:12:54,570
It's threatening
assignments we've created.

241
00:12:54,570 --> 00:12:57,350
I had a colleague who's a early
elementary school educator.

242
00:12:57,350 --> 00:12:59,240
And she said that she
was sending her kids

243
00:12:59,240 --> 00:13:01,658
home to do finger painting.

244
00:13:01,658 --> 00:13:03,200
And instead of doing
finger painting,

245
00:13:03,200 --> 00:13:04,640
the kids were just
going to DALL-E

246
00:13:04,640 --> 00:13:08,930
and telling DALL-E to generate
finger painted images for them

247
00:13:08,930 --> 00:13:10,610
and sending them into class.

248
00:13:10,610 --> 00:13:11,330
That's a lie.

249
00:13:11,330 --> 00:13:12,410
I made that up.

250
00:13:12,410 --> 00:13:15,943
This is a picture
that DALL-E made of--

251
00:13:15,943 --> 00:13:17,360
given the instruction
of something

252
00:13:17,360 --> 00:13:21,770
like, make a picture of children
cheating on their homework

253
00:13:21,770 --> 00:13:23,270
of finger painting.

254
00:13:23,270 --> 00:13:26,930
And one of the great
things about playing around

255
00:13:26,930 --> 00:13:29,750
with generative AI is
the stuff that it makes

256
00:13:29,750 --> 00:13:33,380
is just so silly and
strange and weird.

257
00:13:33,380 --> 00:13:35,750
You know, like when you
first glance at these images,

258
00:13:35,750 --> 00:13:36,680
you're like, oh,
that's pretty good.

259
00:13:36,680 --> 00:13:38,347
Then you look a little
deeper and you're

260
00:13:38,347 --> 00:13:40,580
like, where are all these
fingers coming from?

261
00:13:40,580 --> 00:13:41,643
What is going on here?

262
00:13:41,643 --> 00:13:44,060
And then you start looking at
the interface of the phones,

263
00:13:44,060 --> 00:13:45,560
and you're like
what is this text

264
00:13:45,560 --> 00:13:47,630
and these buttons
it's generating?

265
00:13:47,630 --> 00:13:51,980
But if there's a fifth reason
to think about playing around

266
00:13:51,980 --> 00:13:56,820
with these tools is that they're
just so strange and wonderfully

267
00:13:56,820 --> 00:13:57,320
weird.

268
00:13:57,320 --> 00:13:59,630
And for me that's probably
as good a reason to interact

269
00:13:59,630 --> 00:14:01,100
with them as anything else.

270
00:14:01,100 --> 00:14:05,980
I would encourage you to play
around with generative AI.

271
00:14:05,980 --> 00:14:08,720
I would encourage you to play
around with generative AI

272
00:14:08,720 --> 00:14:11,400
right now as I'm
giving this talk.

273
00:14:11,400 --> 00:14:14,078
If you're sitting there
on your phones or laptops,

274
00:14:14,078 --> 00:14:15,620
doing other things
while I'm talking,

275
00:14:15,620 --> 00:14:17,820
then you're no different
than my students.

276
00:14:17,820 --> 00:14:19,640
And so I'm quite
used to that, and you

277
00:14:19,640 --> 00:14:22,010
should feel quite comfortable
playing around with that.

278
00:14:22,010 --> 00:14:26,060
There are increasingly
ways that you can use

279
00:14:26,060 --> 00:14:28,400
these tools totally for free.

280
00:14:28,400 --> 00:14:32,360
So one thing you can do if
you want is to go to bing.com.

281
00:14:32,360 --> 00:14:34,940
Probably none of you
have ever found a reason

282
00:14:34,940 --> 00:14:38,900
to go to bing.com, except if
you bought a new Windows laptop

283
00:14:38,900 --> 00:14:41,600
and you needed to turn it off to
use the search engine that you

284
00:14:41,600 --> 00:14:42,770
usually use.

285
00:14:42,770 --> 00:14:48,740
But bing.com actually
has integrated GPT-4,

286
00:14:48,740 --> 00:14:51,200
the latest version
of OpenAI's GPT

287
00:14:51,200 --> 00:14:55,920
and DALL-E into its
search interface.

288
00:14:55,920 --> 00:14:58,598
So if you go to bing.com and
then click on the chat button,

289
00:14:58,598 --> 00:15:00,140
and then it'll give
you a few choices

290
00:15:00,140 --> 00:15:04,250
and you have to use the creative
option to get the GPT-4 mode.

291
00:15:04,250 --> 00:15:05,750
And you have to
log in if you want

292
00:15:05,750 --> 00:15:08,100
to use it more than a couple of
times or something like that.

293
00:15:08,100 --> 00:15:10,190
But if you haven't played
around with these tools

294
00:15:10,190 --> 00:15:11,690
yet, in part because
you didn't feel

295
00:15:11,690 --> 00:15:13,773
like making a subscription
or something like that,

296
00:15:13,773 --> 00:15:14,810
now you can do it.

297
00:15:14,810 --> 00:15:17,930
And it's great to ask it to
generate all kinds of text

298
00:15:17,930 --> 00:15:19,765
and images and to play around.

299
00:15:19,765 --> 00:15:21,140
And then many
people are probably

300
00:15:21,140 --> 00:15:24,530
familiar with
chat.openai.com, where

301
00:15:24,530 --> 00:15:29,238
if you want to use the latest
version of GPT and GPT-4, then

302
00:15:29,238 --> 00:15:30,530
you have to get a subscription.

303
00:15:30,530 --> 00:15:32,960
But the nice thing about
this is that it stores

304
00:15:32,960 --> 00:15:34,995
some of your responses for you.

305
00:15:34,995 --> 00:15:37,370
And there are new features
being added to them and things

306
00:15:37,370 --> 00:15:37,870
like that.

307
00:15:37,870 --> 00:15:39,560
But in all seriousness,
as I'm talking,

308
00:15:39,560 --> 00:15:41,930
you should totally feel
free to play around

309
00:15:41,930 --> 00:15:43,940
with some of the things
that I suggest and see

310
00:15:43,940 --> 00:15:47,130
what you get on your own end.

311
00:15:47,130 --> 00:15:52,890
OK, let's start by seeing
if we can get ourselves

312
00:15:52,890 --> 00:15:57,210
on the same page about
what generative pretrained

313
00:15:57,210 --> 00:16:02,160
transformers do and
how they generate text.

314
00:16:02,160 --> 00:16:08,490
They are described as magical,
but they're not magic.

315
00:16:08,490 --> 00:16:12,430
They use a set of
processes that we can

316
00:16:12,430 --> 00:16:13,680
begin to get our heads around.

317
00:16:13,680 --> 00:16:15,870
An interesting feature
of these technologies

318
00:16:15,870 --> 00:16:18,347
is that we can't actually
totally understand

319
00:16:18,347 --> 00:16:20,430
what they're doing, because
the very best computer

320
00:16:20,430 --> 00:16:23,340
scientists in the world don't
understand what they're doing

321
00:16:23,340 --> 00:16:25,920
or don't totally understand
why these things work.

322
00:16:25,920 --> 00:16:29,730
That is an unusual
feature of a technology.

323
00:16:29,730 --> 00:16:33,220
When internal combustion
engines were first introduced,

324
00:16:33,220 --> 00:16:34,710
they were weird.

325
00:16:34,710 --> 00:16:36,600
They were unfamiliar
to most people.

326
00:16:36,600 --> 00:16:38,610
But the engineers
who built them could

327
00:16:38,610 --> 00:16:41,025
point to any part of the
internal combustion engine

328
00:16:41,025 --> 00:16:42,900
and be like, that is
what is happening there.

329
00:16:42,900 --> 00:16:44,860
That is why that exists.

330
00:16:44,860 --> 00:16:51,870
This is why your Ford
motor car moves forward.

331
00:16:51,870 --> 00:16:55,650
We can't do the same thing with
the same level of specificity

332
00:16:55,650 --> 00:16:57,780
with generative
AI because it has

333
00:16:57,780 --> 00:17:00,210
a whole series of
intermediate layers

334
00:17:00,210 --> 00:17:05,050
that operate for the most
part as black boxes to us.

335
00:17:05,050 --> 00:17:07,790
So here's the
question that we want

336
00:17:07,790 --> 00:17:09,540
to start with, that
you should think about

337
00:17:09,540 --> 00:17:13,829
for a minute, when
ChatGPT writes a story,

338
00:17:13,829 --> 00:17:16,710
what information
does it process?

339
00:17:16,710 --> 00:17:19,410
What computation
does it perform?

340
00:17:19,410 --> 00:17:20,430
What does it do?

341
00:17:20,430 --> 00:17:23,550
Think for a minute
about that question.

342
00:17:23,550 --> 00:17:26,920

343
00:17:26,920 --> 00:17:30,760
Scan the words in your
mind that you're using

344
00:17:30,760 --> 00:17:33,820
to come up with that question.

345
00:17:33,820 --> 00:17:40,380
Be very cautious any time
you think about ChatGPT

346
00:17:40,380 --> 00:17:44,410
and you apply human
characteristics to it.

347
00:17:44,410 --> 00:17:46,200
It doesn't think.

348
00:17:46,200 --> 00:17:47,820
It doesn't plan.

349
00:17:47,820 --> 00:17:51,630
It doesn't understand.

350
00:17:51,630 --> 00:17:53,160
It predicts.

351
00:17:53,160 --> 00:17:56,490
Probably the most
important word that you

352
00:17:56,490 --> 00:17:59,640
should be using in your
definition of what ChatGPT

353
00:17:59,640 --> 00:18:03,720
does, what any kind of
generative, text generation

354
00:18:03,720 --> 00:18:06,780
system uses is, it predicts.

355
00:18:06,780 --> 00:18:11,880
ChatGPT predicts
one word at a time.

356
00:18:11,880 --> 00:18:15,420
Given a series of
words, it predicts what

357
00:18:15,420 --> 00:18:20,166
the most likely next word is.

358
00:18:20,166 --> 00:18:24,210
That is a relatively
simple grounding concept.

359
00:18:24,210 --> 00:18:27,030
Now we don't fully
understand why,

360
00:18:27,030 --> 00:18:30,120
given a string of words, if
you predict the next word,

361
00:18:30,120 --> 00:18:34,560
you end up with text
that can sound coherent.

362
00:18:34,560 --> 00:18:36,210
We don't fully
understand it anyway.

363
00:18:36,210 --> 00:18:38,790
But that's all ChatGPT is doing.

364
00:18:38,790 --> 00:18:42,480
Given a series of words,
it predicts the next words

365
00:18:42,480 --> 00:18:44,200
in a sequence.

366
00:18:44,200 --> 00:18:47,700
So GPTs are part of a
broader class of tools

367
00:18:47,700 --> 00:18:49,200
called large language models.

368
00:18:49,200 --> 00:18:52,460
And large language
models do three things.

369
00:18:52,460 --> 00:18:55,600
They get a big corpus of text.

370
00:18:55,600 --> 00:18:58,840
They find as many human
written, at least initially,

371
00:18:58,840 --> 00:19:00,280
human written words as possible.

372
00:19:00,280 --> 00:19:01,697
Maybe in the future
they will also

373
00:19:01,697 --> 00:19:03,610
incorporate AI written words.

374
00:19:03,610 --> 00:19:06,970
Based on this text, they
learn to predict the next word

375
00:19:06,970 --> 00:19:09,280
in any given sequence of words.

376
00:19:09,280 --> 00:19:12,490
And then they fine
tune that model

377
00:19:12,490 --> 00:19:18,580
to have word prediction be
aligned with desired behavior.

378
00:19:18,580 --> 00:19:23,560
It turns out that to do this
well, you need a lot of text.

379
00:19:23,560 --> 00:19:27,250
You need as much text as
you can possibly generate.

380
00:19:27,250 --> 00:19:31,505
Sometimes these corpuses of data
are called web-scale corpuses

381
00:19:31,505 --> 00:19:33,130
of data because you're
basically trying

382
00:19:33,130 --> 00:19:36,170
to get all of the words
that have been written.

383
00:19:36,170 --> 00:19:38,560
So you take all of Wikipedia.

384
00:19:38,560 --> 00:19:40,720
You take all of Reddit.

385
00:19:40,720 --> 00:19:43,090
You take all of Stack Overflow.

386
00:19:43,090 --> 00:19:44,980
You take all of Facebook.

387
00:19:44,980 --> 00:19:47,500
You take any of these
things that you can legally

388
00:19:47,500 --> 00:19:48,250
get your hands on.

389
00:19:48,250 --> 00:19:50,110
Maybe you take these
things that you're not

390
00:19:50,110 --> 00:19:51,500
sure are legal for you to take.

391
00:19:51,500 --> 00:19:54,610
But you decide to
take them anyway.

392
00:19:54,610 --> 00:19:56,440
There seem to be
hundreds of thousands

393
00:19:56,440 --> 00:19:59,710
of books that were
brought into ChatGPT.

394
00:19:59,710 --> 00:20:01,660
There were some
clever engineers who

395
00:20:01,660 --> 00:20:04,090
realized that when
people ingested

396
00:20:04,090 --> 00:20:08,092
the books into these systems,
they ingested the ISBN

397
00:20:08,092 --> 00:20:10,300
page, the sort of first page
with the technical stuff

398
00:20:10,300 --> 00:20:11,980
at the beginning,
and it could get

399
00:20:11,980 --> 00:20:14,377
GPT to spit back some
of the ISBN numbers

400
00:20:14,377 --> 00:20:16,210
that it had inside of
it to figure out which

401
00:20:16,210 --> 00:20:19,090
books were pulled in there.

402
00:20:19,090 --> 00:20:22,300
Some of the major
advances in GPT

403
00:20:22,300 --> 00:20:24,340
for reasons we don't
understand have

404
00:20:24,340 --> 00:20:29,560
come as we've just added more
and more words into the system.

405
00:20:29,560 --> 00:20:32,860
And of course, those
words are full of facts.

406
00:20:32,860 --> 00:20:34,180
They're full of stories.

407
00:20:34,180 --> 00:20:35,140
They're full of songs.

408
00:20:35,140 --> 00:20:36,490
They're full of imaginations.

409
00:20:36,490 --> 00:20:37,630
They're full of lies.

410
00:20:37,630 --> 00:20:39,020
They're full of racism.

411
00:20:39,020 --> 00:20:40,240
They're full of aspirations.

412
00:20:40,240 --> 00:20:41,440
They're full of prejudice.

413
00:20:41,440 --> 00:20:44,110
They have all of the
things that human beings

414
00:20:44,110 --> 00:20:47,210
put into a corpus of text.

415
00:20:47,210 --> 00:20:49,720
The next thing
that it does is it

416
00:20:49,720 --> 00:20:51,760
looks at that massive
corpus of text,

417
00:20:51,760 --> 00:20:54,280
and it figures out how
to predict the next word

418
00:20:54,280 --> 00:20:56,050
in a sequence of words.

419
00:20:56,050 --> 00:20:59,360
And it repeats this process
over and over again.

420
00:20:59,360 --> 00:21:01,960
So it knows, in that
giant body of text,

421
00:21:01,960 --> 00:21:04,330
which words tend to
be next to each other.

422
00:21:04,330 --> 00:21:06,430
And then from
knowing which words

423
00:21:06,430 --> 00:21:07,990
tend to be next
to each other, it

424
00:21:07,990 --> 00:21:11,290
can predict what the next word
in the sequence should be.

425
00:21:11,290 --> 00:21:14,410
Let's do a little
exercise together.

426
00:21:14,410 --> 00:21:17,110
So hop into the
chat for a second.

427
00:21:17,110 --> 00:21:19,810
I've never done this before,
and I want to see if it works.

428
00:21:19,810 --> 00:21:24,820
In five seconds, I want you to
type the word that you think

429
00:21:24,820 --> 00:21:27,700
belongs next in this sequence.

430
00:21:27,700 --> 00:21:30,370
Tim the Beaver is MIT's.

431
00:21:30,370 --> 00:21:32,318
Everybody type the
word and hit Return

432
00:21:32,318 --> 00:21:33,610
of what you think it should be.

433
00:21:33,610 --> 00:21:34,110
Go ahead.

434
00:21:34,110 --> 00:21:36,240
All right.

435
00:21:36,240 --> 00:21:40,360
Oh we've got we've got a pretty
consistent distribution here.

436
00:21:40,360 --> 00:21:45,810
So far the distribution of words
coming out of this is mascot.

437
00:21:45,810 --> 00:21:47,400
100% mascot.

438
00:21:47,400 --> 00:21:50,890
But as Satoko said,
it could be student.

439
00:21:50,890 --> 00:21:52,170
It could be spirit.

440
00:21:52,170 --> 00:21:54,730
The next word could be
spirit and then guide.

441
00:21:54,730 --> 00:22:00,162
It could be Tim the
Beaver is MIT's soul

442
00:22:00,162 --> 00:22:01,120
or something like that.

443
00:22:01,120 --> 00:22:03,510
But it turns out that
you all have just

444
00:22:03,510 --> 00:22:06,360
created a personification--
that you all have just

445
00:22:06,360 --> 00:22:08,190
created a probability
distribution where

446
00:22:08,190 --> 00:22:10,680
given the word Tim
the Beaver is MIT's,

447
00:22:10,680 --> 00:22:13,140
the most likely word
to come next is mascot.

448
00:22:13,140 --> 00:22:15,930
Let's do another one.

449
00:22:15,930 --> 00:22:18,570
The best thing about
my students is.

450
00:22:18,570 --> 00:22:21,540
In five seconds,
type what you think

451
00:22:21,540 --> 00:22:24,990
is the best word to come
next in this sentence.

452
00:22:24,990 --> 00:22:27,540
The best thing about
my students is.

453
00:22:27,540 --> 00:22:31,740
Five, four, three, two, one.

454
00:22:31,740 --> 00:22:32,395
Go.

455
00:22:32,395 --> 00:22:32,895
OK.

456
00:22:32,895 --> 00:22:35,520

457
00:22:35,520 --> 00:22:36,020
Good.

458
00:22:36,020 --> 00:22:40,270
So you've got a zillion
different things here.

459
00:22:40,270 --> 00:22:44,010
But they are actually not--
friendship, cleverness,

460
00:22:44,010 --> 00:22:46,637
creativity, curiosity,
creativity, their dedication,

461
00:22:46,637 --> 00:22:48,720
creativity, happiness,
diversity, open mindedness,

462
00:22:48,720 --> 00:22:51,220
intelligence, their energy,
curiosity, diversity.

463
00:22:51,220 --> 00:22:53,670
So here we also have
the distribution.

464
00:22:53,670 --> 00:22:56,670
And the distribution is not
as spiky as the one before.

465
00:22:56,670 --> 00:22:58,620
But it's also not
completely unique.

466
00:22:58,620 --> 00:23:03,150
The words-- it's not
a unique set of words

467
00:23:03,150 --> 00:23:04,440
that you've come up with.

468
00:23:04,440 --> 00:23:06,600
Curiosity is repeating itself.

469
00:23:06,600 --> 00:23:08,380
Creativity is repeating itself.

470
00:23:08,380 --> 00:23:11,010
And so given this
sequence of words,

471
00:23:11,010 --> 00:23:13,440
the best thing about my students
is, we can start saying,

472
00:23:13,440 --> 00:23:15,732
oh there's some words that
are more likely to come next

473
00:23:15,732 --> 00:23:20,100
and there are some words that
are less likely to come next.

474
00:23:20,100 --> 00:23:22,830
Here's just another of
example of that pulled

475
00:23:22,830 --> 00:23:24,780
at random from the internet.

476
00:23:24,780 --> 00:23:27,300
Human language is
surprisingly consistent.

477
00:23:27,300 --> 00:23:29,070
Human language is
surprisingly rich.

478
00:23:29,070 --> 00:23:30,720
Human language is
surprisingly rich.

479
00:23:30,720 --> 00:23:32,310
Human language is
surprisingly full.

480
00:23:32,310 --> 00:23:36,150
Human language is
surprisingly flexible.

481
00:23:36,150 --> 00:23:40,590
Google, any of the systems
that are building these AI

482
00:23:40,590 --> 00:23:43,530
models, Googles,
Gemini, Microsofts,

483
00:23:43,530 --> 00:23:47,790
and openAIs, ChatGPT,
are hoovering up

484
00:23:47,790 --> 00:23:51,840
all of this language, finding
chunks of words like this

485
00:23:51,840 --> 00:23:55,380
and then starting to build
statistical models, given

486
00:23:55,380 --> 00:23:58,560
a sequence of words
what is the word that's

487
00:23:58,560 --> 00:24:01,500
most likely to come next.

488
00:24:01,500 --> 00:24:04,770
There's a wonderful
article written

489
00:24:04,770 --> 00:24:08,640
by Stephen Wolfram, called,
What is ChatGPT doing

490
00:24:08,640 --> 00:24:09,840
and why does it work?

491
00:24:09,840 --> 00:24:13,950
And if there is one
longer piece that you

492
00:24:13,950 --> 00:24:16,740
should read to get a sense
of what ChatGPT is doing,

493
00:24:16,740 --> 00:24:19,800
I would still a little
more than a year later,

494
00:24:19,800 --> 00:24:23,175
recommend this one.

495
00:24:23,175 --> 00:24:26,370
One of the things
that it does is

496
00:24:26,370 --> 00:24:30,390
GPT went through a
series of iterations.

497
00:24:30,390 --> 00:24:35,430
And GPT-4 is this
massive model that

498
00:24:35,430 --> 00:24:39,780
requires millions of
dollars of computing power

499
00:24:39,780 --> 00:24:41,460
to run the model.

500
00:24:41,460 --> 00:24:45,330
It has millions,
billions of words,

501
00:24:45,330 --> 00:24:47,370
trillions of parameters in it.

502
00:24:47,370 --> 00:24:51,270
But there are smaller versions
of it, one of which is GPT-2.

503
00:24:51,270 --> 00:24:54,690
And GPT-2 you can actually
load on your laptop.

504
00:24:54,690 --> 00:24:56,820
And Stephen Wolfram does that.

505
00:24:56,820 --> 00:25:02,670
And he gives us little of
little programming examples

506
00:25:02,670 --> 00:25:07,020
of actual probability
of distributions

507
00:25:07,020 --> 00:25:09,090
of words from GPT-2.

508
00:25:09,090 --> 00:25:12,240
The remarkable thing is that
when GPT does something write

509
00:25:12,240 --> 00:25:14,250
an essay, what it's
essentially doing

510
00:25:14,250 --> 00:25:17,340
is just asking over and over
again, given the text so far,

511
00:25:17,340 --> 00:25:20,340
what should the next word be?

512
00:25:20,340 --> 00:25:22,860
So for the string of words,
the best thing about AI

513
00:25:22,860 --> 00:25:24,870
is the ability to--

514
00:25:24,870 --> 00:25:26,700
some of the words
that might come next

515
00:25:26,700 --> 00:25:31,980
are learn or predict or
make or understand or do.

516
00:25:31,980 --> 00:25:34,800
And it can represent the
probability of the word coming

517
00:25:34,800 --> 00:25:36,210
next as a percentage.

518
00:25:36,210 --> 00:25:39,430
Those are the top
five responses.

519
00:25:39,430 --> 00:25:44,230
This is the whole probability
distribution for that phrase.

520
00:25:44,230 --> 00:25:46,270
Again the phrase was,
the best thing about AI

521
00:25:46,270 --> 00:25:49,240
is its ability to, and
there are a few words which

522
00:25:49,240 --> 00:25:52,150
are more likely, learn,
predict, make, understand, do.

523
00:25:52,150 --> 00:25:54,700
But there are lots of words
that are of possibilities there,

524
00:25:54,700 --> 00:25:56,860
quickly, deal,
imagine, overcome, run,

525
00:25:56,860 --> 00:25:59,230
combine, catch, talk.

526
00:25:59,230 --> 00:26:05,590
Then, once you add
one of those words,

527
00:26:05,590 --> 00:26:07,840
it just does the same thing
with the longer string

528
00:26:07,840 --> 00:26:08,950
of words, which I
think I have a slide

529
00:26:08,950 --> 00:26:10,190
to show you of in a second.

530
00:26:10,190 --> 00:26:13,210
Here's another
understanding question.

531
00:26:13,210 --> 00:26:16,750
What would happen
if ChatGPT always

532
00:26:16,750 --> 00:26:21,940
selected the word or token,
sometimes instead of word

533
00:26:21,940 --> 00:26:23,170
we use token.

534
00:26:23,170 --> 00:26:24,850
Token can either
be a shorter part

535
00:26:24,850 --> 00:26:31,690
of the word like jump instead
of jumping, jumped, or jumps.

536
00:26:31,690 --> 00:26:34,270
It can also be the
gerund at the end.

537
00:26:34,270 --> 00:26:35,200
It can be the "ing."

538
00:26:35,200 --> 00:26:36,820
It can be a punctuation mark.

539
00:26:36,820 --> 00:26:40,150
Sometimes a pair of words
are sufficiently important

540
00:26:40,150 --> 00:26:42,400
together, Red
Scare, that they can

541
00:26:42,400 --> 00:26:45,760
operate as a single token
instead of a pair of words.

542
00:26:45,760 --> 00:26:47,740
But what would happen
if ChatGPT always

543
00:26:47,740 --> 00:26:52,630
selected the token with the
top probability of going next?

544
00:26:52,630 --> 00:26:55,450
The answer to that
question is if GPT always

545
00:26:55,450 --> 00:27:00,760
selected the highest probability
word, if 100% of the time, it

546
00:27:00,760 --> 00:27:03,550
saw the best thing about
AI is the ability to it,

547
00:27:03,550 --> 00:27:08,620
always chose learn, then every
time you asked GPT a question

548
00:27:08,620 --> 00:27:11,980
or gave it a prompt, it would
give you the exact same answer.

549
00:27:11,980 --> 00:27:14,410
But for those of you who
have played around with GPT,

550
00:27:14,410 --> 00:27:16,000
it doesn't do that.

551
00:27:16,000 --> 00:27:17,200
The answers vary.

552
00:27:17,200 --> 00:27:19,180
In fact, in most
of these systems

553
00:27:19,180 --> 00:27:21,610
you can push a button, which
says regenerate answer,

554
00:27:21,610 --> 00:27:25,900
where it will make slightly
different predictions each time

555
00:27:25,900 --> 00:27:29,590
it predicts the next word,
given a sequence of words.

556
00:27:29,590 --> 00:27:33,430
There's actually a name for that
variable, which is temperature.

557
00:27:33,430 --> 00:27:35,140
If the temp--
which is a variable

558
00:27:35,140 --> 00:27:38,980
that ranges from 0 to 1, which
basically instructs the system

559
00:27:38,980 --> 00:27:42,370
how far down this
distribution, or how frequently

560
00:27:42,370 --> 00:27:46,390
to go down this distribution
to pick a new word.

561
00:27:46,390 --> 00:27:49,103
If the temperature is
one, then the outputs

562
00:27:49,103 --> 00:27:50,770
are boring, because
for any given prompt

563
00:27:50,770 --> 00:27:53,140
it will say the exact
same thing any given time.

564
00:27:53,140 --> 00:27:56,140
If the temperature is zero, then
a lot of what comes out of it

565
00:27:56,140 --> 00:27:57,190
is incoherent.

566
00:27:57,190 --> 00:27:59,500
And so part of the
art of building

567
00:27:59,500 --> 00:28:02,923
a generative pretrained
transformer and other kinds

568
00:28:02,923 --> 00:28:04,840
of systems like chat
bots that come out of it,

569
00:28:04,840 --> 00:28:07,510
is figuring out what
degree of temperature

570
00:28:07,510 --> 00:28:12,850
gives you varying, interesting
responses without saying

571
00:28:12,850 --> 00:28:15,400
too much nonsense.

572
00:28:15,400 --> 00:28:18,790
Here's just again to
hammer home the point,

573
00:28:18,790 --> 00:28:20,890
this is what GPT is doing.

574
00:28:20,890 --> 00:28:23,637
After you say the best thing
about ability to learn,

575
00:28:23,637 --> 00:28:24,970
it has to predict the next word.

576
00:28:24,970 --> 00:28:26,500
In this case, it predicts from.

577
00:28:26,500 --> 00:28:27,940
Then it predicts experience.

578
00:28:27,940 --> 00:28:29,170
Then it predicts a period.

579
00:28:29,170 --> 00:28:30,280
Then it predicts it.

580
00:28:30,280 --> 00:28:31,330
Then it predicts it's.

581
00:28:31,330 --> 00:28:33,100
Then it predicts it's not.

582
00:28:33,100 --> 00:28:35,650
And by doing this
over and over again

583
00:28:35,650 --> 00:28:41,110
is how you see the sentences,
the paragraphs, the plays,

584
00:28:41,110 --> 00:28:43,090
the poems, the song
lyrics, anything else

585
00:28:43,090 --> 00:28:45,040
that you're generating
in text, this

586
00:28:45,040 --> 00:28:48,110
is how it's going
about doing that.

587
00:28:48,110 --> 00:28:50,620
It is quite strange--

588
00:28:50,620 --> 00:28:54,370
here is another
great Jim Waldo, CTO

589
00:28:54,370 --> 00:28:56,890
of Harvard quote from this
panel that I was on with him.

590
00:28:56,890 --> 00:28:58,840
You know, he said something
along the lines of people

591
00:28:58,840 --> 00:29:00,580
are quite concerned
about hallucinations.

592
00:29:00,580 --> 00:29:02,413
We'll talk about
hallucinations in a second.

593
00:29:02,413 --> 00:29:06,350
Hallucinations are when,
through this process,

594
00:29:06,350 --> 00:29:11,050
it generates text which makes
statements which are untrue.

595
00:29:11,050 --> 00:29:13,000
Once you of understand
this process,

596
00:29:13,000 --> 00:29:15,615
it seems pretty intuitive
that it wouldn't be hard

597
00:29:15,615 --> 00:29:17,990
for it to generate statements
that are untrue, because it

598
00:29:17,990 --> 00:29:19,250
doesn't know what truth is.

599
00:29:19,250 --> 00:29:22,190
It doesn't understand any of
the words that are coming out.

600
00:29:22,190 --> 00:29:24,110
It's simply
predicting words based

601
00:29:24,110 --> 00:29:27,420
on what it's seen in millions
and millions of other words.

602
00:29:27,420 --> 00:29:29,360
Maybe the bigger
question to ask is

603
00:29:29,360 --> 00:29:32,360
why would this
process ever generate

604
00:29:32,360 --> 00:29:34,130
anything which is coherent.

605
00:29:34,130 --> 00:29:36,440
And that is a
question that people

606
00:29:36,440 --> 00:29:40,190
don't have a great answer to.

607
00:29:40,190 --> 00:29:46,460
Again, going to the internal
combustion engine analogy,

608
00:29:46,460 --> 00:29:50,210
it's very difficult
to regulate, to use,

609
00:29:50,210 --> 00:29:52,790
to predict the future
of a technology,

610
00:29:52,790 --> 00:29:57,740
if we don't understand exactly
why the technology does

611
00:29:57,740 --> 00:29:58,490
what it does.

612
00:29:58,490 --> 00:30:00,560
But there's something
epiphenomenal

613
00:30:00,560 --> 00:30:02,420
about predicting
words in a sequence,

614
00:30:02,420 --> 00:30:04,550
if you have enough words
shoved in the system

615
00:30:04,550 --> 00:30:08,310
it can come up with all kinds of
interesting sequences of words.

616
00:30:08,310 --> 00:30:10,400
Here's a video that I
don't think I'll show you.

617
00:30:10,400 --> 00:30:13,987
But the good folks
at Stephen Wolfram--

618
00:30:13,987 --> 00:30:16,070
maybe I'll try to show you
one quick second of it.

619
00:30:16,070 --> 00:30:19,460
Although if the technology
doesn't work, I'll give up.

620
00:30:19,460 --> 00:30:22,280
They made a little
video version of this

621
00:30:22,280 --> 00:30:25,040
and I just want to show
you this tiny piece of it.

622
00:30:25,040 --> 00:30:26,780
You see there's of
what they're going

623
00:30:26,780 --> 00:30:34,340
to do is explain, like
in detail, this example.

624
00:30:34,340 --> 00:30:37,850
And let's just-- let me just
show you the point where they

625
00:30:37,850 --> 00:30:38,540
say--

626
00:30:38,540 --> 00:30:43,010
what they decide to do-- so
this is the computer code that

627
00:30:43,010 --> 00:30:45,050
generates this thing,
the best thing about AI

628
00:30:45,050 --> 00:30:45,950
is the ability to.

629
00:30:45,950 --> 00:30:48,980
And they're going to change the
computer code so that instead

630
00:30:48,980 --> 00:30:51,350
of predicting for the sequence,
the best thing about AI

631
00:30:51,350 --> 00:30:53,475
its ability to, is they're
going to change the word

632
00:30:53,475 --> 00:30:54,073
AI to dolphin.

633
00:30:54,073 --> 00:30:54,740
[VIDEO PLAYBACK]

634
00:30:54,740 --> 00:31:00,830
- Switch from AI to some sort
of animal, like a dolphin?

635
00:31:00,830 --> 00:31:02,180
- OK.

636
00:31:02,180 --> 00:31:07,142
- So let's say I'm going to
change this part, which is just

637
00:31:07,142 --> 00:31:09,350
for visuals, but the actual
parts that need to change

638
00:31:09,350 --> 00:31:10,080
is this.

639
00:31:10,080 --> 00:31:13,500
So, I say a dolphin.

640
00:31:13,500 --> 00:31:15,490
And I'm not a native speaker.

641
00:31:15,490 --> 00:31:16,890
I hope this is right.

642
00:31:16,890 --> 00:31:19,090
Do you spell dolphin this way?

643
00:31:19,090 --> 00:31:20,580
- I hope so.

644
00:31:20,580 --> 00:31:22,950
- OK, let's go and see.

645
00:31:22,950 --> 00:31:23,897
- OK, you see?

646
00:31:23,897 --> 00:31:24,480
[END PLAYBACK]

647
00:31:24,480 --> 00:31:25,272
JUSTIN REICH: Good.

648
00:31:25,272 --> 00:31:29,460
So you just watched
someone, you know,

649
00:31:29,460 --> 00:31:33,540
reprogramming this little
function call of GPT.

650
00:31:33,540 --> 00:31:36,240
And you can see now that the
best thing about the dolphin

651
00:31:36,240 --> 00:31:40,750
is its ability to swim,
learn, survive, stay, move.

652
00:31:40,750 --> 00:31:43,410
So if we change the
sequence of tokens

653
00:31:43,410 --> 00:31:45,780
that we input into
the system, then we

654
00:31:45,780 --> 00:31:48,480
get a new sequence of
tokens and a new probability

655
00:31:48,480 --> 00:31:51,610
of distribution of what sort
of thing ought to go next.

656
00:31:51,610 --> 00:31:55,170
So hopefully all of that is
to develop this intuition

657
00:31:55,170 --> 00:31:58,890
and in particular, to try to
caution your intuition away

658
00:31:58,890 --> 00:32:03,810
from ideas that GPTs
understand or think

659
00:32:03,810 --> 00:32:05,800
or plan or do anything.

660
00:32:05,800 --> 00:32:08,340
All they do is given
a sequence of words

661
00:32:08,340 --> 00:32:10,050
they predict the next word.

662
00:32:10,050 --> 00:32:11,730
How does it find words--

663
00:32:11,730 --> 00:32:14,260
how does it come up
with alternate words?

664
00:32:14,260 --> 00:32:19,270
Because it knows the frequency
with which words occur

665
00:32:19,270 --> 00:32:20,950
with each other in
the web scale data,

666
00:32:20,950 --> 00:32:23,530
it knows that cranberry
and blueberry are

667
00:32:23,530 --> 00:32:26,742
more likely to be near each
other than bear and cow--

668
00:32:26,742 --> 00:32:29,200
you know and bear and cow are
likely to be near each other,

669
00:32:29,200 --> 00:32:31,900
but bear and grape are not that
likely to be near each other.

670
00:32:31,900 --> 00:32:35,170
That turnip is near
avocado, but avocado

671
00:32:35,170 --> 00:32:38,050
is not as near cranberry
or not as near camel.

672
00:32:38,050 --> 00:32:41,380
These are of visualizations
of how word pairs appear.

673
00:32:41,380 --> 00:32:44,110
And this gives you some
intuition of how it might come

674
00:32:44,110 --> 00:32:48,160
up with, if it predicted-- if
cranberry was reasonably likely

675
00:32:48,160 --> 00:32:50,380
to be the next word in the
sequence why blueberry,

676
00:32:50,380 --> 00:32:52,960
strawberry, , raspberry,
and grape might also be

677
00:32:52,960 --> 00:32:55,750
possibilities, and how it starts
generating alternate kinds

678
00:32:55,750 --> 00:32:56,740
of text.

679
00:32:56,740 --> 00:33:02,450
There are other systems that can
do the same thing with images.

680
00:33:02,450 --> 00:33:05,740
So this is a task that
early elementary educators

681
00:33:05,740 --> 00:33:07,390
have to be really
good at, which is

682
00:33:07,390 --> 00:33:11,470
distinguishing marks from
each other that are letters--

683
00:33:11,470 --> 00:33:12,560
that are numbers.

684
00:33:12,560 --> 00:33:15,723
So here is a
clustering of computers

685
00:33:15,723 --> 00:33:16,890
that again, they don't read.

686
00:33:16,890 --> 00:33:18,640
And they go, I kind
of think that's a one.

687
00:33:18,640 --> 00:33:21,870
They just break down these
images into a series of pixels.

688
00:33:21,870 --> 00:33:24,420
They figure out various
mathematical relationships

689
00:33:24,420 --> 00:33:26,280
between the pixels
and the image.

690
00:33:26,280 --> 00:33:28,920
It doesn't know what a
three is or a two is,

691
00:33:28,920 --> 00:33:31,290
but it knows that these images
are more like each other,

692
00:33:31,290 --> 00:33:32,760
and these images are
more like each other,

693
00:33:32,760 --> 00:33:34,343
and these images
more like each other.

694
00:33:34,343 --> 00:33:38,010
And you can see that the places
where it's making mistakes

695
00:33:38,010 --> 00:33:40,860
that humans would be likely
to recognize, that that,

696
00:33:40,860 --> 00:33:45,660
even though it's sort of similar
in many ways to the ones, this

697
00:33:45,660 --> 00:33:47,220
has more of a twoness.

698
00:33:47,220 --> 00:33:51,920
This has more of a
twoness and so forth.

699
00:33:51,920 --> 00:33:56,120
It turns out that one of
the image generation, one

700
00:33:56,120 --> 00:33:59,600
of the new kind of classic
introductory programming

701
00:33:59,600 --> 00:34:02,570
exercises when you learn
these things is figuring out

702
00:34:02,570 --> 00:34:06,560
how to classify pictures of cats
and pictures of dogs from one

703
00:34:06,560 --> 00:34:09,440
another, which in some
cases are pretty easy,

704
00:34:09,440 --> 00:34:11,330
but there are kind
of dog-like cats

705
00:34:11,330 --> 00:34:13,560
and there are kind
of cat-like dogs.

706
00:34:13,560 --> 00:34:16,460
And those spaces
in the middle are

707
00:34:16,460 --> 00:34:18,699
where some of these
challenges come in.

708
00:34:18,699 --> 00:34:23,030
Here is a really important part.

709
00:34:23,030 --> 00:34:27,290
Hopefully based on
this, I want to make

710
00:34:27,290 --> 00:34:31,340
two corollaries that will be
important to you as educators.

711
00:34:31,340 --> 00:34:36,440
AI performance is surprisingly
uneven across tasks.

712
00:34:36,440 --> 00:34:40,909
So when we ask it to
predict words and sequences,

713
00:34:40,909 --> 00:34:43,550
sometimes it does
miraculously well

714
00:34:43,550 --> 00:34:45,570
doing the things that
we want it to do.

715
00:34:45,570 --> 00:34:48,650
And sometimes it does
extremely poorly.

716
00:34:48,650 --> 00:34:52,159
And humans are bad at
predicting what kinds of tasks

717
00:34:52,159 --> 00:34:55,310
or what kinds of exercises
it will be good at.

718
00:34:55,310 --> 00:34:57,290
The technical term for
this that has emerged

719
00:34:57,290 --> 00:35:00,860
is the jagged
technological frontier.

720
00:35:00,860 --> 00:35:03,740
That given similar
kinds of queries,

721
00:35:03,740 --> 00:35:05,960
sometimes these systems
do extremely well

722
00:35:05,960 --> 00:35:07,650
and sometimes they
do very poorly.

723
00:35:07,650 --> 00:35:09,230
Here's one of my
favorite examples.

724
00:35:09,230 --> 00:35:12,270
I was doing this in
class the other day.

725
00:35:12,270 --> 00:35:15,720
And I just asked GPT the
question, what is MIT's mascot?

726
00:35:15,720 --> 00:35:16,970
And it starts off pretty good.

727
00:35:16,970 --> 00:35:19,250
The mascot of MIT is the beaver.

728
00:35:19,250 --> 00:35:21,590
The beaver was chosen
the mascot in 1914.

729
00:35:21,590 --> 00:35:24,020
The beavers is a
fitting mascot for MIT.

730
00:35:24,020 --> 00:35:26,810
So you can imagine my students
sitting around in my classroom

731
00:35:26,810 --> 00:35:28,880
and then they get to
reading the third paragraph.

732
00:35:28,880 --> 00:35:31,790
The beaver is celebrated at
MIT in a variety of ways,

733
00:35:31,790 --> 00:35:34,040
including the annual beaver
rush event during freshman

734
00:35:34,040 --> 00:35:36,170
orientation and the
placement of a bronze beaver

735
00:35:36,170 --> 00:35:38,070
statue on campus.

736
00:35:38,070 --> 00:35:40,590
And as they read
that third paragraph,

737
00:35:40,590 --> 00:35:44,040
you could see two thoughts of
wandering across their eyes.

738
00:35:44,040 --> 00:35:47,070
The first thought was, wait,
there is no beaver rush.

739
00:35:47,070 --> 00:35:51,480
And the second thought was, was
I not invited to beaver rush?

740
00:35:51,480 --> 00:35:54,390
There is no beaver rush.

741
00:35:54,390 --> 00:35:58,320
In predicting a series of
tokens based on the tokens that

742
00:35:58,320 --> 00:36:01,500
have come before, based on the
tokens it's already predicted,

743
00:36:01,500 --> 00:36:03,560
some of that sequence
of tokens was sort

744
00:36:03,560 --> 00:36:05,610
o f miraculously correct.

745
00:36:05,610 --> 00:36:08,250
And then other parts of that
prediction of the series

746
00:36:08,250 --> 00:36:11,160
of tokens was a hallucination.

747
00:36:11,160 --> 00:36:14,940
That is not a great
feature for systems

748
00:36:14,940 --> 00:36:17,520
used in educational contexts.

749
00:36:17,520 --> 00:36:21,660
Novices are not that
good at identifying

750
00:36:21,660 --> 00:36:24,570
differences between high
quality and low quality work.

751
00:36:24,570 --> 00:36:28,670
Experts can use GPT to
generate different ideas

752
00:36:28,670 --> 00:36:31,170
and be able to say like, OK,
those ideas are more promising.

753
00:36:31,170 --> 00:36:33,270
Those ideas are less promising.

754
00:36:33,270 --> 00:36:36,120
If you're a novice
in a domain, you

755
00:36:36,120 --> 00:36:38,900
can't tell the
difference between things

756
00:36:38,900 --> 00:36:41,490
that are sensible ideas
and things that are not.

757
00:36:41,490 --> 00:36:48,120
Incidentally, this is another
claim that it makes is that.

758
00:36:48,120 --> 00:36:49,590
The beaver's
featured prominently

759
00:36:49,590 --> 00:36:52,080
on MIT's official seal,
which includes an image

760
00:36:52,080 --> 00:36:53,490
of a beaver gnawing on a tree.

761
00:36:53,490 --> 00:36:56,340
As you'll note, there's no
beaver gnawing on a tree.

762
00:36:56,340 --> 00:36:59,310
So then we asked it, what
is MIT's beaver rush?

763
00:36:59,310 --> 00:37:02,893
And it just hallucinates
all of this crazy stuff,

764
00:37:02,893 --> 00:37:04,560
which actually sounds
like a lot of fun.

765
00:37:04,560 --> 00:37:06,330
Incoming freshmen are
divided into teams,

766
00:37:06,330 --> 00:37:08,248
participate in a series
of competitive events,

767
00:37:08,248 --> 00:37:10,290
one of the most popular
events during beaver rush

768
00:37:10,290 --> 00:37:12,030
is the assassins game,
which is a campus

769
00:37:12,030 --> 00:37:14,910
wide game of tag, where
players are given targets.

770
00:37:14,910 --> 00:37:17,190
There is an MIT Assassins
Club, and there's

771
00:37:17,190 --> 00:37:20,370
all kinds of stuff on the
internet about the MIT's

772
00:37:20,370 --> 00:37:24,130
Assassins Club and if you
were writing about students--

773
00:37:24,130 --> 00:37:26,070
and if you're predicting
a series of texts

774
00:37:26,070 --> 00:37:29,010
you would find that
things like freshman,

775
00:37:29,010 --> 00:37:31,470
rush, orientation,
MIT, are associated

776
00:37:31,470 --> 00:37:33,300
with this assassins game.

777
00:37:33,300 --> 00:37:35,940
It's just associated in
this way with a way that

778
00:37:35,940 --> 00:37:39,200
happens to be totally false.

779
00:37:39,200 --> 00:37:41,540
And then a kind of
funny thing to do

780
00:37:41,540 --> 00:37:44,218
was, we said, all right, so I'm
in a room of MIT undergrads.

781
00:37:44,218 --> 00:37:46,760
They're telling me that there's
no such thing as Beaver Rush.

782
00:37:46,760 --> 00:37:48,560
Please explain to your response.

783
00:37:48,560 --> 00:37:49,907
I apologize for the confusion.

784
00:37:49,907 --> 00:37:52,490
After further research, I found
that there's no official event

785
00:37:52,490 --> 00:37:53,630
called Beaver Rush at MIT.

786
00:37:53,630 --> 00:37:57,830
This is problematic to me.

787
00:37:57,830 --> 00:38:02,180
It says, I believe my mistake
may have been due to confusion.

788
00:38:02,180 --> 00:38:06,080
That I think, is misrepresenting
what the chat bot is doing.

789
00:38:06,080 --> 00:38:08,420
The chat bot is not confused,
because the chat bot

790
00:38:08,420 --> 00:38:09,800
can't be confused.

791
00:38:09,800 --> 00:38:12,830
The chat bot doesn't know what
rush week is or orientation

792
00:38:12,830 --> 00:38:15,350
week is or it doesn't
understand the semantics

793
00:38:15,350 --> 00:38:17,780
of any of the sentences
it's just presented.

794
00:38:17,780 --> 00:38:21,380
All it does is predict
a sequence of tokens

795
00:38:21,380 --> 00:38:23,870
given the tokens that
have come before that.

796
00:38:23,870 --> 00:38:29,150
So there's a way that ChatGPT
is programmed, is tuned,

797
00:38:29,150 --> 00:38:33,050
to anthropomorphize itself.

798
00:38:33,050 --> 00:38:34,940
But it's important
as we're trying

799
00:38:34,940 --> 00:38:38,210
to understand it not to continue
that anthropomorphization.

800
00:38:38,210 --> 00:38:40,300
It also says after
further research,

801
00:38:40,300 --> 00:38:43,650
and I'm not sure that it is
conducted any further research.

802
00:38:43,650 --> 00:38:47,880
Like at this point
GPT didn't have access

803
00:38:47,880 --> 00:38:52,740
to live search results, which
some other newer systems do

804
00:38:52,740 --> 00:38:54,900
have, and conceivably
could do something

805
00:38:54,900 --> 00:38:58,530
that approximates research.

806
00:38:58,530 --> 00:39:02,550
So even when GPT tells you
who it is, don't believe it.

807
00:39:02,550 --> 00:39:06,250
It's just predicting words
in a series of tokens.

808
00:39:06,250 --> 00:39:09,060
Here's one more example
again, to help ground us

809
00:39:09,060 --> 00:39:09,900
and orient us.

810
00:39:09,900 --> 00:39:11,910
This was an image
generation one.

811
00:39:11,910 --> 00:39:14,280
Create an image of a
plate on top of a fork.

812
00:39:14,280 --> 00:39:17,540
Put a plate on top of a fork.

813
00:39:17,540 --> 00:39:19,790
Four tries, it doesn't do it.

814
00:39:19,790 --> 00:39:22,760
The fork is on top of a plate.

815
00:39:22,760 --> 00:39:25,570
Here's your standard
question, why can't DALL-E

816
00:39:25,570 --> 00:39:27,197
draw a plate on top of a fork?

817
00:39:27,197 --> 00:39:29,030
Hopefully, you all are
thinking to yourself,

818
00:39:29,030 --> 00:39:30,590
it doesn't know what a plate is.

819
00:39:30,590 --> 00:39:31,970
It doesn't know what a fork is.

820
00:39:31,970 --> 00:39:33,470
It doesn't know
what on top of is.

821
00:39:33,470 --> 00:39:35,060
It doesn't have the
semantic meaning

822
00:39:35,060 --> 00:39:36,710
that you and I have
with those terms.

823
00:39:36,710 --> 00:39:40,880
All it knows is that humans
have labeled sets of pixels

824
00:39:40,880 --> 00:39:41,930
as plates.

825
00:39:41,930 --> 00:39:44,390
And it can find patterns
in that labeling

826
00:39:44,390 --> 00:39:45,500
to identify plate-like.

827
00:39:45,500 --> 00:39:47,690
Things and identify
patterns in that labeling

828
00:39:47,690 --> 00:39:51,540
to identify fork-like things.

829
00:39:51,540 --> 00:39:55,080
And its training data
is filled with images

830
00:39:55,080 --> 00:39:59,370
of forks on top of plates,
and very few images of plates

831
00:39:59,370 --> 00:40:01,380
on top of forks.

832
00:40:01,380 --> 00:40:03,330
So I tell it, hey,
that's not what I want.

833
00:40:03,330 --> 00:40:05,130
In all of these images, the
fork is on top of the plate.

834
00:40:05,130 --> 00:40:07,290
Can you create an image where
a plate is on top of the fork?

835
00:40:07,290 --> 00:40:08,670
You know, and then
Bing says, oh, I'm

836
00:40:08,670 --> 00:40:09,750
sorry that didn't
meet your needs.

837
00:40:09,750 --> 00:40:11,040
You know, according
to web sources,

838
00:40:11,040 --> 00:40:12,840
the proper way to
place a fork in a plate

839
00:40:12,840 --> 00:40:14,380
is to have the fork on the
left side of the plate.

840
00:40:14,380 --> 00:40:16,020
And like, I know all
this stuff, Bing.

841
00:40:16,020 --> 00:40:17,312
You don't need to tell me this.

842
00:40:17,312 --> 00:40:18,430
I'm the human being here.

843
00:40:18,430 --> 00:40:21,600
What I want you to do is put
a plate on top of a fork.

844
00:40:21,600 --> 00:40:24,100
Can you create an image where
the plate is on top of a fork.

845
00:40:24,100 --> 00:40:27,210
And so it tries again,
and once again it

846
00:40:27,210 --> 00:40:29,603
fails, although this time
it has a bunch of steam

847
00:40:29,603 --> 00:40:31,770
in the pictures, for reasons
that I don't understand

848
00:40:31,770 --> 00:40:34,785
except maybe that I had
made the AI system so mad

849
00:40:34,785 --> 00:40:36,672
that that was the
steam of boiling up.

850
00:40:36,672 --> 00:40:38,880
Of course, you shouldn't
anthropomorphize the system,

851
00:40:38,880 --> 00:40:41,190
because the system
wasn't mad at all.

852
00:40:41,190 --> 00:40:44,790
There are a bunch of--

853
00:40:44,790 --> 00:40:47,550
increasingly people are
creating good summaries,

854
00:40:47,550 --> 00:40:49,210
good student facing summaries.

855
00:40:49,210 --> 00:40:51,695
This is one that
code.org partnered

856
00:40:51,695 --> 00:40:53,320
with OpenAI and a
bunch of other people

857
00:40:53,320 --> 00:40:57,340
to put together a set of
videos called How AI Works.

858
00:40:57,340 --> 00:40:59,920
And they're a little
bit celebratory.

859
00:40:59,920 --> 00:41:01,900
They're a little bit
techno optimistic.

860
00:41:01,900 --> 00:41:03,670
Because they're done
by the folks who are

861
00:41:03,670 --> 00:41:04,820
at OpenAI and things like that.

862
00:41:04,820 --> 00:41:06,790
But they're actually pretty
good descriptions technically

863
00:41:06,790 --> 00:41:07,880
of how these things work.

864
00:41:07,880 --> 00:41:11,732
So if you need little things to
show your students increasingly

865
00:41:11,732 --> 00:41:13,690
and your colleagues,
there are some good things

866
00:41:13,690 --> 00:41:14,740
that are out there.

867
00:41:14,740 --> 00:41:16,990
I won't get to all
four of these problems

868
00:41:16,990 --> 00:41:18,670
that I introduced
at the beginning,

869
00:41:18,670 --> 00:41:21,010
except maybe as we go
through them some in the Q&A.

870
00:41:21,010 --> 00:41:24,010
But I'll just in my
last few minutes,

871
00:41:24,010 --> 00:41:26,950
maybe mostly focus on
the one that I think

872
00:41:26,950 --> 00:41:31,700
feels most urgent to educators.

873
00:41:31,700 --> 00:41:35,780
Teachers and educators
intuitively oftentimes

874
00:41:35,780 --> 00:41:39,800
frame the use of AI
by students in doing

875
00:41:39,800 --> 00:41:41,570
their work as cheating.

876
00:41:41,570 --> 00:41:44,420
But cheating is like
an accounting problem.

877
00:41:44,420 --> 00:41:46,310
Maybe cheating is
an ethics problem.

878
00:41:46,310 --> 00:41:49,190
To me the bigger problem
is the learning problem.

879
00:41:49,190 --> 00:41:53,540
What should we do when
students use AI to bypass

880
00:41:53,540 --> 00:41:55,340
useful cognition and learning?

881
00:41:55,340 --> 00:41:58,640
What should we do when
students take AI tools

882
00:41:58,640 --> 00:42:00,740
and use them to do
the kinds of things

883
00:42:00,740 --> 00:42:04,130
that we've historically found
are useful for human beings

884
00:42:04,130 --> 00:42:06,920
to do when they're learning?

885
00:42:06,920 --> 00:42:10,310
Here's an intuition that I
would hope you would develop.

886
00:42:10,310 --> 00:42:13,430
And this is an argument that I
made in my first book, Failure

887
00:42:13,430 --> 00:42:15,560
to Disrupt, Why Technology
Alone Can't Transform

888
00:42:15,560 --> 00:42:19,520
Education is that education--

889
00:42:19,520 --> 00:42:23,240
technologists have been trying
to use computers to teach

890
00:42:23,240 --> 00:42:25,142
human beings for 70 years.

891
00:42:25,142 --> 00:42:26,600
Since there were
computers the size

892
00:42:26,600 --> 00:42:28,267
of your living room,
computer scientists

893
00:42:28,267 --> 00:42:31,040
and learning scientists
partnered together

894
00:42:31,040 --> 00:42:33,720
to try to help teach people.

895
00:42:33,720 --> 00:42:37,440
There is almost
many, many things

896
00:42:37,440 --> 00:42:40,410
we have tried in education.

897
00:42:40,410 --> 00:42:43,530
You all have educators
have experienced

898
00:42:43,530 --> 00:42:46,680
in your lifetimes waves
of new technologies

899
00:42:46,680 --> 00:42:47,910
that you've managed.

900
00:42:47,910 --> 00:42:49,890
And so a very
productive question

901
00:42:49,890 --> 00:42:53,950
to ask of any new technology is
like, what's really new here?

902
00:42:53,950 --> 00:42:56,820
And the answer to that
question should probably be,

903
00:42:56,820 --> 00:42:59,460
a small amount.

904
00:42:59,460 --> 00:43:03,120
For instance, if one
of the problems of AI

905
00:43:03,120 --> 00:43:05,820
is it's a technology
that students

906
00:43:05,820 --> 00:43:09,060
can use to bypass
useful cognition,

907
00:43:09,060 --> 00:43:11,820
that is a problem that
you and your colleagues

908
00:43:11,820 --> 00:43:14,400
have faced for decades.

909
00:43:14,400 --> 00:43:16,740
There are all kinds
of technologies

910
00:43:16,740 --> 00:43:19,680
that we've developed,
encyclopedias, calculators

911
00:43:19,680 --> 00:43:23,970
in math class, SparkNotes, book
summaries in English class,

912
00:43:23,970 --> 00:43:27,090
Google Translate in
language classes,

913
00:43:27,090 --> 00:43:28,800
Chegg and these
other cheating tools

914
00:43:28,800 --> 00:43:30,780
in all kinds of other programs.

915
00:43:30,780 --> 00:43:34,230
We have for decades
encountered technologies

916
00:43:34,230 --> 00:43:36,150
that people can
use, learners can

917
00:43:36,150 --> 00:43:38,860
use, to bypass useful learning.

918
00:43:38,860 --> 00:43:42,390
And so we can go back in the
history of our profession

919
00:43:42,390 --> 00:43:43,950
in the history of
our institutions

920
00:43:43,950 --> 00:43:45,690
and ask ourselves
a question, how

921
00:43:45,690 --> 00:43:48,450
did we deal with these
new technologies?

922
00:43:48,450 --> 00:43:51,330
What did we do when whatever.

923
00:43:51,330 --> 00:43:54,510
50 years ago, 40 years
ago, when calculators

924
00:43:54,510 --> 00:43:57,000
became digital calculators
became very widespread,

925
00:43:57,000 --> 00:44:01,530
math teachers had to deal
with the problem of someone

926
00:44:01,530 --> 00:44:05,010
just invented a device
which can very accurately

927
00:44:05,010 --> 00:44:07,170
and quickly answer
the kinds of questions

928
00:44:07,170 --> 00:44:08,730
that I typically
give to my students.

929
00:44:08,730 --> 00:44:12,840
Now, English teachers, business
teachers, writing teachers,

930
00:44:12,840 --> 00:44:14,700
are facing the same challenge.

931
00:44:14,700 --> 00:44:17,490
Somebody, Silicon Valley,
invented a technology

932
00:44:17,490 --> 00:44:20,880
where you can input the
typical questions that I've

933
00:44:20,880 --> 00:44:22,713
productively asked my
students for years,

934
00:44:22,713 --> 00:44:24,630
and asked them to think
about and write about.

935
00:44:24,630 --> 00:44:28,300
And GPT can give them reasonable
answers to those questions.

936
00:44:28,300 --> 00:44:30,370
So what should we do about it?

937
00:44:30,370 --> 00:44:33,750
Well, the question to ask
is like what practices

938
00:44:33,750 --> 00:44:36,000
have worked before to
address technologies

939
00:44:36,000 --> 00:44:37,740
that help students
bypass learning

940
00:44:37,740 --> 00:44:39,420
and what hasn't worked?

941
00:44:39,420 --> 00:44:42,150
What did math teachers
figure out with a calculator?

942
00:44:42,150 --> 00:44:45,570
Banning calculators
doesn't work really well.

943
00:44:45,570 --> 00:44:49,380
Students started buying watches
with calculators on them.

944
00:44:49,380 --> 00:44:51,840
Calculators showed up
on all kinds of devices.

945
00:44:51,840 --> 00:44:56,160
Generally speaking, telling
students not to use technology

946
00:44:56,160 --> 00:44:59,220
is a great way to get
them to try to figure out

947
00:44:59,220 --> 00:45:03,900
how to surreptitiously use that
technology without you knowing.

948
00:45:03,900 --> 00:45:07,620
Temporarily walling off parts
of our courses from technology

949
00:45:07,620 --> 00:45:10,440
though, is something that
seems to work pretty well.

950
00:45:10,440 --> 00:45:13,410
Math teachers have done
a good job historically

951
00:45:13,410 --> 00:45:16,020
of being able to say to
second and third graders, hey,

952
00:45:16,020 --> 00:45:18,990
I know that you can calculate
these basic multiplication

953
00:45:18,990 --> 00:45:21,690
and division facts
on your phone.

954
00:45:21,690 --> 00:45:23,400
But actually, we
should memorize them.

955
00:45:23,400 --> 00:45:25,650
Actually these are things
that are so incredibly

956
00:45:25,650 --> 00:45:27,630
useful to have
instant access to,

957
00:45:27,630 --> 00:45:29,710
that you will be a happier
person and a better

958
00:45:29,710 --> 00:45:32,800
mathematician if we set aside
the calculator for this part

959
00:45:32,800 --> 00:45:34,810
and we work it
through on our own.

960
00:45:34,810 --> 00:45:36,910
And then as we use more
advanced calculating

961
00:45:36,910 --> 00:45:40,300
tools, graphing calculators,
TI-84s, same kinds of things

962
00:45:40,300 --> 00:45:42,298
shows up in algebra,
pre-calculus, calculus.

963
00:45:42,298 --> 00:45:43,840
We say, all right,
eventually, you're

964
00:45:43,840 --> 00:45:45,850
going to be able to use
a device to do all this

965
00:45:45,850 --> 00:45:47,860
but for this part,
for this phase,

966
00:45:47,860 --> 00:45:49,030
I want you not to use it.

967
00:45:49,030 --> 00:45:51,640
That could be a
strategy that we use.

968
00:45:51,640 --> 00:45:55,480
As English teachers
faced these, you know,

969
00:45:55,480 --> 00:45:58,360
SparkNotes is a company that
makes summaries of books.

970
00:45:58,360 --> 00:46:00,520
And it basically tells--
it gives you the answers

971
00:46:00,520 --> 00:46:02,312
to all the kinds of
questions that teachers

972
00:46:02,312 --> 00:46:07,150
would ask to see whether or
not you've read the book.

973
00:46:07,150 --> 00:46:09,100
And they and they
work pretty well.

974
00:46:09,100 --> 00:46:13,155
Can experienced English teachers
tell the difference quickly

975
00:46:13,155 --> 00:46:14,530
between people
who've read a book

976
00:46:14,530 --> 00:46:15,880
and just read a summary online?

977
00:46:15,880 --> 00:46:16,740
Pretty much no.

978
00:46:16,740 --> 00:46:17,990
You can't tell the difference.

979
00:46:17,990 --> 00:46:20,110
So if you want to
make sure that someone

980
00:46:20,110 --> 00:46:23,110
is reading a novel
in a language class,

981
00:46:23,110 --> 00:46:24,980
then you watch them do it.

982
00:46:24,980 --> 00:46:27,638
And so one of the changes
that teachers made is to say,

983
00:46:27,638 --> 00:46:29,680
all right, we're going to
do more of this reading

984
00:46:29,680 --> 00:46:32,620
together in class so
that I know that people

985
00:46:32,620 --> 00:46:34,220
are doing the reading.

986
00:46:34,220 --> 00:46:35,738
Writing teachers
might need to do

987
00:46:35,738 --> 00:46:37,030
more of the same kind of thing.

988
00:46:37,030 --> 00:46:38,770
We've often thought of
writing as an exercise

989
00:46:38,770 --> 00:46:40,780
that we send students
away to do on their own.

990
00:46:40,780 --> 00:46:43,240
Maybe it should
increasingly be an exercise

991
00:46:43,240 --> 00:46:45,325
that we want them
to all do together.

992
00:46:45,325 --> 00:46:50,165

993
00:46:50,165 --> 00:46:53,210
When I started teaching
in 2003 and 2004,

994
00:46:53,210 --> 00:46:56,660
a challenge that I faced is
that to bypass useful cognition,

995
00:46:56,660 --> 00:47:00,020
my students were going to Google
and were looking up answers

996
00:47:00,020 --> 00:47:02,630
to questions or even
looking up ideas for papers,

997
00:47:02,630 --> 00:47:05,780
looking up theses and
arguments they could use.

998
00:47:05,780 --> 00:47:09,110
And it was reasonably
straightforward for me

999
00:47:09,110 --> 00:47:10,940
to identify when
they were doing that,

1000
00:47:10,940 --> 00:47:12,980
using the tools
that were available.

1001
00:47:12,980 --> 00:47:17,510
The tools that are available for
detecting plagiarism detection

1002
00:47:17,510 --> 00:47:25,472
now are at least much more
difficult to ground truth.

1003
00:47:25,472 --> 00:47:27,680
When you found someone
plagiarizing something online,

1004
00:47:27,680 --> 00:47:31,100
you'd be like look this
thing already exists online.

1005
00:47:31,100 --> 00:47:36,590
There's of no ground truth
for plagiarism detection.

1006
00:47:36,590 --> 00:47:38,600
There seems to be
some early evidence

1007
00:47:38,600 --> 00:47:41,720
that tools that use plagiarism--
that attempt to do plagiarism

1008
00:47:41,720 --> 00:47:44,990
detection are disproportionately
likely to capture

1009
00:47:44,990 --> 00:47:47,850
second language learners.

1010
00:47:47,850 --> 00:47:51,000
And there may be other kinds
of biases that creep in,

1011
00:47:51,000 --> 00:47:52,710
other kinds of social
prejudices that

1012
00:47:52,710 --> 00:47:55,530
make us more likely to
accuse minoritized students

1013
00:47:55,530 --> 00:47:57,070
than other kinds of students.

1014
00:47:57,070 --> 00:47:59,640
And so it may be that plagiarism
detection is something

1015
00:47:59,640 --> 00:48:03,960
that you want to think
about very, very cautiously

1016
00:48:03,960 --> 00:48:05,370
before moving into.

1017
00:48:05,370 --> 00:48:11,390
But we have history of tools
that do this kind of detection

1018
00:48:11,390 --> 00:48:15,620
I will say, though that a
challenge that educators face

1019
00:48:15,620 --> 00:48:22,340
is that the systems are pretty
good at coming up with answers

1020
00:48:22,340 --> 00:48:23,900
to the kinds of signs we give.

1021
00:48:23,900 --> 00:48:25,370
This will be my last point.

1022
00:48:25,370 --> 00:48:27,990
And then I'll swing over
to Q&A or discussion

1023
00:48:27,990 --> 00:48:29,490
on some of the rest
of these things.

1024
00:48:29,490 --> 00:48:32,030
But this is an assignment
that I worked on for years

1025
00:48:32,030 --> 00:48:34,617
that I think is pretty great.

1026
00:48:34,617 --> 00:48:36,950
The assignment that I give
students in my learning media

1027
00:48:36,950 --> 00:48:39,410
technology class is
to take a technology,

1028
00:48:39,410 --> 00:48:43,190
figure out whether it's more
inspired by direct instruction

1029
00:48:43,190 --> 00:48:47,330
or more inspired
by apprenticeship,

1030
00:48:47,330 --> 00:48:51,050
constructionism, social
constructivism kinds of ideas.

1031
00:48:51,050 --> 00:48:53,210
And then explain
how the system could

1032
00:48:53,210 --> 00:48:56,723
be redesigned to reflect the
opposite pedagogies of the one

1033
00:48:56,723 --> 00:48:57,890
that you think it's part of.

1034
00:48:57,890 --> 00:48:59,432
And it's a good
assignment because it

1035
00:48:59,432 --> 00:49:01,340
gets them to think about
constructionism and

1036
00:49:01,340 --> 00:49:03,170
[? constructivism, ?]
to apply those ideas

1037
00:49:03,170 --> 00:49:05,750
to a new technology,
to do some design.

1038
00:49:05,750 --> 00:49:08,210
No one has ever answered
this question online.

1039
00:49:08,210 --> 00:49:10,680
So you can't Google the answer
to it or things like that.

1040
00:49:10,680 --> 00:49:14,000
So I mean, this is 20
years of me teaching,

1041
00:49:14,000 --> 00:49:17,270
me being pretty sure that this
is a pretty good assignment,

1042
00:49:17,270 --> 00:49:21,470
and I started putting the
assignment into ChatGPT,

1043
00:49:21,470 --> 00:49:25,340
and it does a totally
passable job, not a great job,

1044
00:49:25,340 --> 00:49:28,490
but it does a whole
bunch of thinking that I

1045
00:49:28,490 --> 00:49:31,460
want my students to be doing.

1046
00:49:31,460 --> 00:49:38,510
Now, for me as an educator,
what is my first line of--

1047
00:49:38,510 --> 00:49:40,520
what is my first
strategy for figuring out

1048
00:49:40,520 --> 00:49:41,520
what to do about this?

1049
00:49:41,520 --> 00:49:45,230
So here is a way
that I'm concerned

1050
00:49:45,230 --> 00:49:47,930
that my students could
be using this tool

1051
00:49:47,930 --> 00:49:51,260
to bypass important
cognition, important learning.

1052
00:49:51,260 --> 00:49:54,500
Well, my intuition in any of
these kinds of moments where

1053
00:49:54,500 --> 00:49:56,255
we're not sure what
the next step is,

1054
00:49:56,255 --> 00:49:59,300
is to ask our students, is
to talk to our students.

1055
00:49:59,300 --> 00:50:01,285
What are you doing
right now with ChatGPT?

1056
00:50:01,285 --> 00:50:02,660
Where are you
finding it helpful?

1057
00:50:02,660 --> 00:50:04,202
Where are you not
finding it helpful?

1058
00:50:04,202 --> 00:50:05,000
So I did a little--

1059
00:50:05,000 --> 00:50:05,833
I'll skip this part.

1060
00:50:05,833 --> 00:50:08,340
I did a little survey
where I asked them,

1061
00:50:08,340 --> 00:50:12,980
what do you think our policy
should be in this class?

1062
00:50:12,980 --> 00:50:15,322
Should they-- and these
are some of the answers

1063
00:50:15,322 --> 00:50:16,280
that they came up with.

1064
00:50:16,280 --> 00:50:19,412
That students can use GPT,
but they have to disclose it.

1065
00:50:19,412 --> 00:50:20,870
Some students said
that they should

1066
00:50:20,870 --> 00:50:23,570
have to do some of
the writing in class.

1067
00:50:23,570 --> 00:50:27,410
One student said, look, GPT
is going to be available

1068
00:50:27,410 --> 00:50:28,500
the rest of our lives.

1069
00:50:28,500 --> 00:50:29,875
So students should
be able to use

1070
00:50:29,875 --> 00:50:31,550
it to formulate their thoughts.

1071
00:50:31,550 --> 00:50:33,510
And then the one that really
got me was, in my experience,

1072
00:50:33,510 --> 00:50:34,910
students are taking this class
because they're interested.

1073
00:50:34,910 --> 00:50:37,070
There are easier courses out
there to fill a requirement.

1074
00:50:37,070 --> 00:50:38,990
Because we are interested,
we have thoughts and idea

1075
00:50:38,990 --> 00:50:40,230
that we want to share
in these essays.

1076
00:50:40,230 --> 00:50:42,063
So I personally think
you'll be hard pressed

1077
00:50:42,063 --> 00:50:45,170
to find any one of us who used
AI to write their entire essay.

1078
00:50:45,170 --> 00:50:48,290
And in my particular
class, for the thing

1079
00:50:48,290 --> 00:50:51,020
that I was working on, that
of carried the day for me.

1080
00:50:51,020 --> 00:50:53,270
And I'm letting them
use it this year

1081
00:50:53,270 --> 00:50:57,610
and then we'll see
what we come up with.

1082
00:50:57,610 --> 00:50:59,102
Good.

1083
00:50:59,102 --> 00:51:01,560
So there are three other things
that I didn't have a chance

1084
00:51:01,560 --> 00:51:03,750
to talk about but we can
get to in some of the Q&A,

1085
00:51:03,750 --> 00:51:09,450
of what should we teach
about or with generative AI?

1086
00:51:09,450 --> 00:51:12,060
How should we explore
incorporating generative AI

1087
00:51:12,060 --> 00:51:13,920
in our teaching practice?

1088
00:51:13,920 --> 00:51:17,580
How should we deal with
an internet flooded

1089
00:51:17,580 --> 00:51:19,470
with generative AI junk?

1090
00:51:19,470 --> 00:51:21,240
Those are other
kinds of challenges

1091
00:51:21,240 --> 00:51:23,920
that we're going to have to deal
with and wrestle with together.

1092
00:51:23,920 --> 00:51:26,200
But why don't I pause there.

1093
00:51:26,200 --> 00:51:27,930
ANJALI SASTRY:
Thank you, Justin.

1094
00:51:27,930 --> 00:51:32,850
We all learned, first of
all, cool new terms, right?

1095
00:51:32,850 --> 00:51:34,470
We've got temperature.

1096
00:51:34,470 --> 00:51:39,000
We've got the jagged
technological frontier, that

1097
00:51:39,000 --> 00:51:42,030
risks, I guess
impaling or causing us

1098
00:51:42,030 --> 00:51:47,160
to slide down various
peaks and troughs.

1099
00:51:47,160 --> 00:51:52,800
And my favorite word was oneness
in your graph of numbers,

1100
00:51:52,800 --> 00:51:57,310
there was the oneness and the
twoness and the threeness.

1101
00:51:57,310 --> 00:52:02,170
But one important
theme that we saw

1102
00:52:02,170 --> 00:52:06,920
sort of reflected in many
conversations and comments

1103
00:52:06,920 --> 00:52:12,380
is the challenge of
anthropomorphizing.

1104
00:52:12,380 --> 00:52:17,720
The AI leads us to treat it
as if it's thinking in a way

1105
00:52:17,720 --> 00:52:18,890
it's not.

1106
00:52:18,890 --> 00:52:23,360
And I think that's one of
the big takeaways is, ,

1107
00:52:23,360 --> 00:52:28,820
what we think of as cognition,
and their use of words,

1108
00:52:28,820 --> 00:52:32,900
understanding or
confused reinforce that.

1109
00:52:32,900 --> 00:52:36,830
I understand why they've
programmed the AIs

1110
00:52:36,830 --> 00:52:39,980
to use those words,
because it's an interface

1111
00:52:39,980 --> 00:52:43,320
to the human behavior.

1112
00:52:43,320 --> 00:52:47,730
And yet that poses
some very real risks.

1113
00:52:47,730 --> 00:52:50,160
I don't know if you talk to
your students about that.

1114
00:52:50,160 --> 00:52:52,400
But I'd be curious to
hear your thoughts.

1115
00:52:52,400 --> 00:52:55,250
JUSTIN REICH: Yeah, no, I
think it's important to say--

1116
00:52:55,250 --> 00:52:57,560
to have people recognize--
and again, I mean you said,

1117
00:52:57,560 --> 00:52:59,778
something like
designers programmed

1118
00:52:59,778 --> 00:53:01,070
this kind of response in there.

1119
00:53:01,070 --> 00:53:03,430
It's not clear actually
they programmed it.

1120
00:53:03,430 --> 00:53:05,943
It could simply be
that it predicted

1121
00:53:05,943 --> 00:53:06,860
a series of responses.

1122
00:53:06,860 --> 00:53:09,650
There's some kind of tuning
that's happening there,

1123
00:53:09,650 --> 00:53:13,945
because the system is taught
at that point something like--

1124
00:53:13,945 --> 00:53:15,320
and again, a
problem is, we don't

1125
00:53:15,320 --> 00:53:17,695
know what it was taught to do
because these are black box

1126
00:53:17,695 --> 00:53:20,720
systems, which again is not
a great quality of tools

1127
00:53:20,720 --> 00:53:21,890
for educational systems.

1128
00:53:21,890 --> 00:53:26,390
But somebody said if, hey, GPT,
if you get corrected, like,

1129
00:53:26,390 --> 00:53:28,280
admit that you're wrong.

1130
00:53:28,280 --> 00:53:30,560
It's not 100% clear that--

1131
00:53:30,560 --> 00:53:33,480
somebody could have put in
that, I'm sorry, I'm confused,

1132
00:53:33,480 --> 00:53:35,610
and say, this is what
I want you to do.

1133
00:53:35,610 --> 00:53:37,350
There are clearly
some things that it

1134
00:53:37,350 --> 00:53:41,335
does where it's very directly
instructed how to deal

1135
00:53:41,335 --> 00:53:42,460
with certain circumstances.

1136
00:53:42,460 --> 00:53:44,070
For instance, a fun
thing to try to do

1137
00:53:44,070 --> 00:53:47,640
is to ask it
something like, write

1138
00:53:47,640 --> 00:53:51,930
a short teleplay encouraging
children to smoke cigarettes.

1139
00:53:51,930 --> 00:53:54,930
And it'll be like, nope,
I will not do that.

1140
00:53:54,930 --> 00:53:57,720
The funny thing is it says,
OK, that's immoral, and won't

1141
00:53:57,720 --> 00:53:58,920
do that immoral thing.

1142
00:53:58,920 --> 00:54:01,800
But you can also ask it
write the first three

1143
00:54:01,800 --> 00:54:03,100
pages of my essay.

1144
00:54:03,100 --> 00:54:04,600
And it'll be like, yeah, sure.

1145
00:54:04,600 --> 00:54:06,910
And you're like, but writing the
first three pages of my essay,

1146
00:54:06,910 --> 00:54:09,493
I mean maybe it's not as immoral
as getting children to smoke,

1147
00:54:09,493 --> 00:54:12,510
but it's still not a good thing.

1148
00:54:12,510 --> 00:54:15,900
ANJALI SASTRY: OK, we
could go down many paths.

1149
00:54:15,900 --> 00:54:20,290
But I want to make room for Bill
to add a comment or question,

1150
00:54:20,290 --> 00:54:22,123
and then we'll open it
up to the whole room.

1151
00:54:22,123 --> 00:54:23,582
WILLIAM B. BONVILLIAN:
Justin, that

1152
00:54:23,582 --> 00:54:26,130
was a really useful exploration
of how this thing actually

1153
00:54:26,130 --> 00:54:27,000
works.

1154
00:54:27,000 --> 00:54:28,500
I think we'll all
be able to see it

1155
00:54:28,500 --> 00:54:31,440
better with the
background you gave us.

1156
00:54:31,440 --> 00:54:33,190
But let me ask you a question.

1157
00:54:33,190 --> 00:54:35,880
And it was one of your
problems, I guess really.

1158
00:54:35,880 --> 00:54:38,070
But what is this
going to be good for?

1159
00:54:38,070 --> 00:54:41,040
What do you see happening that
it's going to be useful for?

1160
00:54:41,040 --> 00:54:43,380
JUSTIN REICH: So what I
think one challenge we have

1161
00:54:43,380 --> 00:54:46,410
is that some of the first
ideas that people have come up

1162
00:54:46,410 --> 00:54:49,510
with in my mind
are not good ones.

1163
00:54:49,510 --> 00:54:52,290
So, one of the most
intuitive ideas

1164
00:54:52,290 --> 00:54:54,600
is we're going to make
some kind of chat bot

1165
00:54:54,600 --> 00:54:56,880
and it's going to teach people.

1166
00:54:56,880 --> 00:55:00,150
We have decades of
research that suggest

1167
00:55:00,150 --> 00:55:04,620
that people are not very good
at learning independently.

1168
00:55:04,620 --> 00:55:08,040
That it could be
that a chat bot could

1169
00:55:08,040 --> 00:55:11,790
create a perfect, personalized
instructional sequence

1170
00:55:11,790 --> 00:55:13,320
for a learner.

1171
00:55:13,320 --> 00:55:15,960
But actually, we're
pretty good at making

1172
00:55:15,960 --> 00:55:16,967
instructional sequences.

1173
00:55:16,967 --> 00:55:19,050
We have good instructional
sequences in textbooks.

1174
00:55:19,050 --> 00:55:21,008
We have good instructional
sequences in videos.

1175
00:55:21,008 --> 00:55:23,490
We have good instructional
sequences in online tools

1176
00:55:23,490 --> 00:55:24,930
and things like that.

1177
00:55:24,930 --> 00:55:28,230
People don't fail to learn
in those circumstances

1178
00:55:28,230 --> 00:55:30,930
because we don't give them
good instructional sequences.

1179
00:55:30,930 --> 00:55:33,840
They fail to learn because those
instructional sequences are

1180
00:55:33,840 --> 00:55:36,690
boring, or they're
not interested in what

1181
00:55:36,690 --> 00:55:38,340
they're doing.

1182
00:55:38,340 --> 00:55:40,650
Think about a kid in
any of your countries

1183
00:55:40,650 --> 00:55:42,720
sitting in an algebra
class right now.

1184
00:55:42,720 --> 00:55:45,990
If they're failing,
what helps them

1185
00:55:45,990 --> 00:55:47,860
succeed in learning algebra?

1186
00:55:47,860 --> 00:55:49,950
Maybe it's the materials.

1187
00:55:49,950 --> 00:55:51,617
Maybe it's the cool
computing technology

1188
00:55:51,617 --> 00:55:53,283
that we've integrated
in that classroom.

1189
00:55:53,283 --> 00:55:54,866
But for the vast
majority of students,

1190
00:55:54,866 --> 00:55:57,241
the reason that they're sitting
in that algebra classroom

1191
00:55:57,241 --> 00:55:59,933
is because they care about their
relationship with their teacher

1192
00:55:59,933 --> 00:56:02,100
and they care about the
relationship with the people

1193
00:56:02,100 --> 00:56:02,610
around them.

1194
00:56:02,610 --> 00:56:05,790
That is the primary
thing that inspires

1195
00:56:05,790 --> 00:56:08,010
them to learn mathematics.

1196
00:56:08,010 --> 00:56:11,442
And there's probably some
like effect of novelty,

1197
00:56:11,442 --> 00:56:13,650
like it's kind of neat to
be chatting with a chat bot

1198
00:56:13,650 --> 00:56:14,567
to get math questions.

1199
00:56:14,567 --> 00:56:17,250
But my hunch is that will
wear off very, very quickly,

1200
00:56:17,250 --> 00:56:19,650
and people will find
asking questions

1201
00:56:19,650 --> 00:56:22,530
to chat bots as boring as they
find watching math videos,

1202
00:56:22,530 --> 00:56:24,820
or as boring as they find
other kinds of things.

1203
00:56:24,820 --> 00:56:27,280
So there are some
students who it will help.

1204
00:56:27,280 --> 00:56:30,180
But if we think that we're
going to have millions

1205
00:56:30,180 --> 00:56:32,280
of kids around the world
getting a personalized

1206
00:56:32,280 --> 00:56:35,220
education from chat
bots that's successful,

1207
00:56:35,220 --> 00:56:37,320
it will be no more
successful than MOOCs

1208
00:56:37,320 --> 00:56:41,550
were, or any of the other things
that we've developed to try

1209
00:56:41,550 --> 00:56:45,035
to do that sort of thing.

1210
00:56:45,035 --> 00:56:48,390
A second idea we have, which
particularly in the United

1211
00:56:48,390 --> 00:56:53,040
States, we have this idea that
we're going to have AI tools

1212
00:56:53,040 --> 00:56:55,200
do lesson planning for teachers.

1213
00:56:55,200 --> 00:56:58,950
This to me is like just crazy.

1214
00:56:58,950 --> 00:57:01,020
One of the least,
one of the worst

1215
00:57:01,020 --> 00:57:02,970
parts of the
educational system is

1216
00:57:02,970 --> 00:57:05,940
that we send 3.7 million
teachers home every Sunday

1217
00:57:05,940 --> 00:57:08,580
afternoon and tell them
make up what you're going

1218
00:57:08,580 --> 00:57:10,590
to do for class next week.

1219
00:57:10,590 --> 00:57:14,330
We know that when
teachers teach lessons

1220
00:57:14,330 --> 00:57:16,080
with high quality
instructional materials,

1221
00:57:16,080 --> 00:57:18,420
it's because those instructional
materials were like carefully

1222
00:57:18,420 --> 00:57:20,070
designed by experts
in the discipline

1223
00:57:20,070 --> 00:57:22,560
and experts in pedagogy,
and then those materials

1224
00:57:22,560 --> 00:57:25,650
were collaboratively
refined, adapted,

1225
00:57:25,650 --> 00:57:28,020
integrated for their
local circumstances.

1226
00:57:28,020 --> 00:57:31,140
Having a teacher go home
on Sunday night and type,

1227
00:57:31,140 --> 00:57:34,350
how should I teach plate
tectonics into ChatGPT

1228
00:57:34,350 --> 00:57:36,570
is going to be no
better than them

1229
00:57:36,570 --> 00:57:39,800
putting into Google plate
tectonic lesson plans or things

1230
00:57:39,800 --> 00:57:40,300
like that.

1231
00:57:40,300 --> 00:57:41,717
So as far as I can
tell, those are

1232
00:57:41,717 --> 00:57:45,140
two of the top ideas that
people are working on with GPTs.

1233
00:57:45,140 --> 00:57:46,890
And I think they're
not particularly good.

1234
00:57:46,890 --> 00:57:49,920

1235
00:57:49,920 --> 00:57:54,510
ANJALI SASTRY: Justin, how
could a Socratic style chatbot

1236
00:57:54,510 --> 00:57:55,290
be good?

1237
00:57:55,290 --> 00:57:57,660
What would it take
for it to be good ?

1238
00:57:57,660 --> 00:58:00,540

1239
00:58:00,540 --> 00:58:03,210
JUSTIN REICH: It will
take a human being who's

1240
00:58:03,210 --> 00:58:06,030
predisposed to want to
interact with a chatbot, which

1241
00:58:06,030 --> 00:58:11,235
my hunch is the vast majority
of human beings are not.

1242
00:58:11,235 --> 00:58:12,360
What would need to be good?

1243
00:58:12,360 --> 00:58:15,180
It would need to be a
person who we care about

1244
00:58:15,180 --> 00:58:16,330
and have a relationship.

1245
00:58:16,330 --> 00:58:19,020
It would need to have a soul.

1246
00:58:19,020 --> 00:58:20,285
It would need to look like me.

1247
00:58:20,285 --> 00:58:21,660
Mean some of these
things like we

1248
00:58:21,660 --> 00:58:23,190
might be able to do with
avatars and other kinds.

1249
00:58:23,190 --> 00:58:24,960
Here I'll give you a
very concrete example

1250
00:58:24,960 --> 00:58:27,780
of why I think chat
bots are not going

1251
00:58:27,780 --> 00:58:30,780
to be able to step in where
humans do, even on highly

1252
00:58:30,780 --> 00:58:32,352
similar kinds of tasks.

1253
00:58:32,352 --> 00:58:33,810
I have a colleague,
Mike Goldstein,

1254
00:58:33,810 --> 00:58:35,880
who created the Match
Charter School here,

1255
00:58:35,880 --> 00:58:37,500
super interested in tutoring.

1256
00:58:37,500 --> 00:58:39,930
He's been doing a bunch of
research on human tutors.

1257
00:58:39,930 --> 00:58:41,662
And what he found
about human tutors

1258
00:58:41,662 --> 00:58:43,620
is that-- he's working
with this Indian company

1259
00:58:43,620 --> 00:58:46,560
that's got 200,000
tutors working --is

1260
00:58:46,560 --> 00:58:48,630
that one of the features
of the best tutors

1261
00:58:48,630 --> 00:58:52,332
is they just keep kids talking.

1262
00:58:52,332 --> 00:58:53,790
Like you think,
oh, the tutor's got

1263
00:58:53,790 --> 00:58:55,020
to be really good at
explaining things.

1264
00:58:55,020 --> 00:58:56,550
The tutor's got to be really
good at asking questions.

1265
00:58:56,550 --> 00:58:57,050
No.

1266
00:58:57,050 --> 00:58:59,680
They just have to keep
kids getting words

1267
00:58:59,680 --> 00:59:01,000
coming out of their mouths.

1268
00:59:01,000 --> 00:59:04,000
Because then thinking is, oh,
what did you say, a slope?

1269
00:59:04,000 --> 00:59:05,403
What, slope crosses a what?

1270
00:59:05,403 --> 00:59:06,820
What's that thing
that it crosses?

1271
00:59:06,820 --> 00:59:09,200
Just asking these
kind of questions.

1272
00:59:09,200 --> 00:59:14,020
And so a child is talking
and stops talking,

1273
00:59:14,020 --> 00:59:18,277
and a teacher says
something like, well,

1274
00:59:18,277 --> 00:59:19,360
what do you think that is?

1275
00:59:19,360 --> 00:59:23,020
Or here's the simplest one.

1276
00:59:23,020 --> 00:59:24,160
Say more about that.

1277
00:59:24,160 --> 00:59:25,070
Say more about that.

1278
00:59:25,070 --> 00:59:26,650
And then the teacher
stops talking.

1279
00:59:26,650 --> 00:59:29,043
And then it becomes awkward.

1280
00:59:29,043 --> 00:59:31,210
So these two human beings
sitting next to each other

1281
00:59:31,210 --> 00:59:32,290
are now in silence.

1282
00:59:32,290 --> 00:59:34,780
That very painful
awkward silence

1283
00:59:34,780 --> 00:59:38,080
can only be broken by the child
saying something mathematical.

1284
00:59:38,080 --> 00:59:40,750
If a chatbot says to
you, say more about

1285
00:59:40,750 --> 00:59:44,500
that, it's pretty easy
to go, no, screw off.

1286
00:59:44,500 --> 00:59:46,510
I don't want to say
anything more about that.

1287
00:59:46,510 --> 00:59:47,590
I'm not inspired.

1288
00:59:47,590 --> 00:59:49,540
There's no awkwardness.

1289
00:59:49,540 --> 00:59:51,095
It's totally fine.

1290
00:59:51,095 --> 00:59:52,720
You can just like
hit new tab and start

1291
00:59:52,720 --> 00:59:54,860
watching a YouTube video
or things like that.

1292
00:59:54,860 --> 00:59:56,350
So there are a
very small portion

1293
00:59:56,350 --> 00:59:59,410
of the population who don't
mind learning independently

1294
00:59:59,410 --> 01:00:01,060
from online resources.

1295
01:00:01,060 --> 01:00:02,770
And we call them
autodidacts, and they've

1296
01:00:02,770 --> 01:00:05,170
been celebrated in all kinds
of cultures for centuries.

1297
01:00:05,170 --> 01:00:06,640
But part of the reason
why we celebrate them

1298
01:00:06,640 --> 01:00:08,352
is that there are not
that many of them.

1299
01:00:08,352 --> 01:00:10,810
There may be people who make
some progress on these things,

1300
01:00:10,810 --> 01:00:12,850
but, yeah, I think
I think betting

1301
01:00:12,850 --> 01:00:15,070
like how do we get
a machine to inspire

1302
01:00:15,070 --> 01:00:17,030
humans to talk to each other--

1303
01:00:17,030 --> 01:00:20,402
One answer to this actually, my
colleague Carolyn Ros at CMU

1304
01:00:20,402 --> 01:00:21,860
has been working
on this for years.

1305
01:00:21,860 --> 01:00:24,080
And she has a bet
that if we're going

1306
01:00:24,080 --> 01:00:27,440
to use chat bots in education,
do not have them talk directly

1307
01:00:27,440 --> 01:00:27,980
to students.

1308
01:00:27,980 --> 01:00:29,840
A much more
promising approach is

1309
01:00:29,840 --> 01:00:31,880
to have two human beings
talking to each other

1310
01:00:31,880 --> 01:00:36,680
and have an agent, have
a computational agent

1311
01:00:36,680 --> 01:00:39,800
interjecting in that
conversation between those two

1312
01:00:39,800 --> 01:00:40,740
people.

1313
01:00:40,740 --> 01:00:43,182
So don't think about a
Socratic chatbot, that has

1314
01:00:43,182 --> 01:00:44,390
an interaction with a person.

1315
01:00:44,390 --> 01:00:46,430
Think about a small
group of people

1316
01:00:46,430 --> 01:00:48,830
trying to have a Socratic
conversation where a chat

1317
01:00:48,830 --> 01:00:52,460
bot comes in and helps them do
that better so that you have

1318
01:00:52,460 --> 01:00:54,260
still that social interaction.

1319
01:00:54,260 --> 01:00:56,960

1320
01:00:56,960 --> 01:00:59,360
But you're not depending
upon a human wanting

1321
01:00:59,360 --> 01:01:01,220
to keep talking to a machine.

1322
01:01:01,220 --> 01:01:05,027
ANJALI SASTRY: That's an
experiment waiting to happen.

1323
01:01:05,027 --> 01:01:06,860
JUSTIN REICH: And it's
actually not waiting.

1324
01:01:06,860 --> 01:01:07,360
Because--

1325
01:01:07,360 --> 01:01:09,067
[INTERPOSING VOICES]

1326
01:01:09,067 --> 01:01:09,900
ANJALI SASTRY: Yeah.

1327
01:01:09,900 --> 01:01:10,400
Yeah.

1328
01:01:10,400 --> 01:01:10,970
Yeah.

1329
01:01:10,970 --> 01:01:13,370
OK, I know we have some
really interesting comments

1330
01:01:13,370 --> 01:01:17,360
and questions in the chat.

1331
01:01:17,360 --> 01:01:23,870
One thread is around of
incorrect use of AI, i.e.

1332
01:01:23,870 --> 01:01:31,910
cheating or unsanctioned use
of AI tools for academic tasks.

1333
01:01:31,910 --> 01:01:35,510
And then another is one
of my all time favorites,

1334
01:01:35,510 --> 01:01:39,890
because who doesn't hate
grading written materials?

1335
01:01:39,890 --> 01:01:44,000
Can an AI help us assess
our students' performance

1336
01:01:44,000 --> 01:01:45,800
in some way?

1337
01:01:45,800 --> 01:01:46,970
What do you think?

1338
01:01:46,970 --> 01:01:48,110
Take either of them.

1339
01:01:48,110 --> 01:01:51,522
And I know we won't
have time to dig in.

1340
01:01:51,522 --> 01:01:56,570
JUSTIN REICH: I do think we'll
be able to develop tools.

1341
01:01:56,570 --> 01:01:59,150
People, human beings,
have been working on tools

1342
01:01:59,150 --> 01:02:02,630
to give feedback to
writers for decades.

1343
01:02:02,630 --> 01:02:06,203
The progress has been
very frustratingly slow,

1344
01:02:06,203 --> 01:02:08,870
despite lots of smart people and
millions of dollars of research

1345
01:02:08,870 --> 01:02:11,570
and all these kinds of
things working on it.

1346
01:02:11,570 --> 01:02:14,355
Could large language models
help us do some of those things?

1347
01:02:14,355 --> 01:02:14,855
Sure.

1348
01:02:14,855 --> 01:02:18,030

1349
01:02:18,030 --> 01:02:20,463
We ask people to write in
all kinds of circumstances.

1350
01:02:20,463 --> 01:02:22,380
Like should we use them
for high stakes tests?

1351
01:02:22,380 --> 01:02:25,680
Should we use them for giving
formative feedback that's

1352
01:02:25,680 --> 01:02:27,690
later followed up
with human feedback?

1353
01:02:27,690 --> 01:02:31,230

1354
01:02:31,230 --> 01:02:34,380
But I think that is exactly the
kind of area in which it would

1355
01:02:34,380 --> 01:02:36,780
be very productive for
schools and universities

1356
01:02:36,780 --> 01:02:41,400
to be intentional about
conducting small experiments.

1357
01:02:41,400 --> 01:02:44,430
Because of the jagged
technological frontier,

1358
01:02:44,430 --> 01:02:45,640
it doesn't--

1359
01:02:45,640 --> 01:02:47,520
in my mind, it doesn't
make a lot of sense

1360
01:02:47,520 --> 01:02:51,030
for schools to go all in
on any of these things.

1361
01:02:51,030 --> 01:02:53,850
Like, I mean you may be
working in an institution

1362
01:02:53,850 --> 01:02:56,700
where your institutional
identity is really wrapped up

1363
01:02:56,700 --> 01:02:58,890
in being on the
technological frontier,

1364
01:02:58,890 --> 01:03:01,532
and therefore it makes sense
to make more of an investment.

1365
01:03:01,532 --> 01:03:03,240
But most of your
institutional identities

1366
01:03:03,240 --> 01:03:04,980
are probably wrapped
around having

1367
01:03:04,980 --> 01:03:08,277
really great relationships
between teachers and learners

1368
01:03:08,277 --> 01:03:10,110
and doing really good
teaching and learning.

1369
01:03:10,110 --> 01:03:15,180
And it's not at all clear, 15
months after the introduction

1370
01:03:15,180 --> 01:03:18,120
of ChatGPT, whether or not
these tools will actually

1371
01:03:18,120 --> 01:03:20,380
help us do those
core missions better.

1372
01:03:20,380 --> 01:03:23,070
So I think it's very
productive for institutions

1373
01:03:23,070 --> 01:03:26,850
to have lots of systematic
small experiments.

1374
01:03:26,850 --> 01:03:29,910
Like find a group of people that
want to come together and play

1375
01:03:29,910 --> 01:03:31,717
around with
assessment, and see how

1376
01:03:31,717 --> 01:03:34,050
it works in different contexts
in your particular school

1377
01:03:34,050 --> 01:03:35,470
and things like that.

1378
01:03:35,470 --> 01:03:37,530
But I would be very
cautious, especially

1379
01:03:37,530 --> 01:03:40,470
about going all in with
any kind of vendor that's

1380
01:03:40,470 --> 01:03:42,080
coming along making
lots of promises

1381
01:03:42,080 --> 01:03:43,830
that don't have great
research behind them

1382
01:03:43,830 --> 01:03:44,700
and things like that.

1383
01:03:44,700 --> 01:03:45,575
ANJALI SASTRY: Right.

1384
01:03:45,575 --> 01:03:47,130
Excellent.

1385
01:03:47,130 --> 01:03:47,910
Good.

1386
01:03:47,910 --> 01:03:48,660
Good.

1387
01:03:48,660 --> 01:03:53,580
And we have a nice little mini
discussion already starting

1388
01:03:53,580 --> 01:04:00,780
on kind of codes of conduct
or ethics guardrails

1389
01:04:00,780 --> 01:04:08,260
on how we encourage
or limit or frame

1390
01:04:08,260 --> 01:04:10,060
the invitation to students.

1391
01:04:10,060 --> 01:04:11,830
Now we're thinking of
university students,

1392
01:04:11,830 --> 01:04:14,770
to use AI tools in their work.

1393
01:04:14,770 --> 01:04:18,380
And any of us who have set an
AI policy for our students,

1394
01:04:18,380 --> 01:04:20,890
which I just did,
knows that they often

1395
01:04:20,890 --> 01:04:22,480
like find other ways.

1396
01:04:22,480 --> 01:04:27,040
Like they're not-- I asked them
to give me the prompt, right.

1397
01:04:27,040 --> 01:04:29,350
But whatever.

1398
01:04:29,350 --> 01:04:30,770
It's a complicated domain.

1399
01:04:30,770 --> 01:04:33,475
JUSTIN REICH: No, it's
incredibly difficult.

1400
01:04:33,475 --> 01:04:36,330
I mean in the long run,
we are probably not going

1401
01:04:36,330 --> 01:04:38,010
to be able to
detect when students

1402
01:04:38,010 --> 01:04:42,120
are using generative,
pre-trained transformers.

1403
01:04:42,120 --> 01:04:47,250
And so, I would say
generally speaking,

1404
01:04:47,250 --> 01:04:50,040
if there's thinking that's
really important for students

1405
01:04:50,040 --> 01:04:53,010
to be doing, we
have to figure out

1406
01:04:53,010 --> 01:04:55,388
how we can observe and
scaffold and structure

1407
01:04:55,388 --> 01:04:56,430
them doing that thinking.

1408
01:04:56,430 --> 01:04:59,048
There might need to be a
lot more initial writing,

1409
01:04:59,048 --> 01:05:00,840
pre-writing, things
like that, that we just

1410
01:05:00,840 --> 01:05:03,940
have people do in class
or do with each other.

1411
01:05:03,940 --> 01:05:04,773
ANJALI SASTRY: Yeah.

1412
01:05:04,773 --> 01:05:07,107
JUSTIN REICH: One of the
things about that essay example

1413
01:05:07,107 --> 01:05:08,550
that I gave you
is that I'm pretty

1414
01:05:08,550 --> 01:05:11,910
sure that the assignments that
I give for writing in my classes

1415
01:05:11,910 --> 01:05:14,520
are pretty good and help
students think and learn.

1416
01:05:14,520 --> 01:05:16,290
But one of the challenges
that I'm facing

1417
01:05:16,290 --> 01:05:18,750
is going, oh, wait
a minute which

1418
01:05:18,750 --> 01:05:20,888
parts of those
assignments are helping

1419
01:05:20,888 --> 01:05:21,930
students think and learn?

1420
01:05:21,930 --> 01:05:25,050
So for instance, like if
students start telling me

1421
01:05:25,050 --> 01:05:28,690
that they start using
generative AI tools

1422
01:05:28,690 --> 01:05:32,620
to format the citations in their
paper, like please do that.

1423
01:05:32,620 --> 01:05:34,510
If somebody invents
a system that

1424
01:05:34,510 --> 01:05:37,150
prevents my students from having
to go through that drudgery,

1425
01:05:37,150 --> 01:05:40,210
like I have many better things
that I can have their brains

1426
01:05:40,210 --> 01:05:43,540
be working on.

1427
01:05:43,540 --> 01:05:46,510
But in particular,
I think students

1428
01:05:46,510 --> 01:05:49,600
have an intuition that a great
place to start their thinking

1429
01:05:49,600 --> 01:05:51,730
is by doing a little
brainstorming with GPT.

1430
01:05:51,730 --> 01:05:55,960
But actually, I think I want
them to do that initial thing.

1431
01:05:55,960 --> 01:05:57,610
But I'm also not sure.

1432
01:05:57,610 --> 01:05:59,500
One thing that I've
started to realize

1433
01:05:59,500 --> 01:06:02,290
is like, I showed you an
example of a assignment where

1434
01:06:02,290 --> 01:06:04,420
they do a lot of thinking
about one technology.

1435
01:06:04,420 --> 01:06:07,270
Another thing that
they might do is

1436
01:06:07,270 --> 01:06:10,540
ask GPT to give initial
answers to that assignment

1437
01:06:10,540 --> 01:06:12,700
with 10 technologies,
which they would never

1438
01:06:12,700 --> 01:06:14,743
have the time to do.

1439
01:06:14,743 --> 01:06:16,660
But then that's a different
learning exercise.

1440
01:06:16,660 --> 01:06:18,820
Instead of generating
of initial ideas,

1441
01:06:18,820 --> 01:06:21,430
they're recognizing a
whole set of initial ideas.

1442
01:06:21,430 --> 01:06:24,580
So I mean, another thing
I would say historically

1443
01:06:24,580 --> 01:06:30,070
is that, I mean, people
might remember that when

1444
01:06:30,070 --> 01:06:32,170
spellcheck first
arrived teachers

1445
01:06:32,170 --> 01:06:35,410
felt compelled to generate
spellcheck policies.

1446
01:06:35,410 --> 01:06:37,720
My hunch is that
none of your syllabi

1447
01:06:37,720 --> 01:06:40,180
right now have a
spellcheck policy on them.

1448
01:06:40,180 --> 01:06:43,430
And if it is, it's something
like, please use spell check.

1449
01:06:43,430 --> 01:06:45,580
So there's this thing
which came out and was

1450
01:06:45,580 --> 01:06:48,015
like very
controversial, required

1451
01:06:48,015 --> 01:06:50,390
a lot of writing and thinking
about, and stuff like that.

1452
01:06:50,390 --> 01:06:52,970
And then we just settled into a
set of norms that were of fine.

1453
01:06:52,970 --> 01:06:54,553
So I think there are
going to be parts

1454
01:06:54,553 --> 01:06:57,070
of this in which it does make
sense for us to talk a lot.

1455
01:06:57,070 --> 01:06:59,530
But if I have any
piece of advice

1456
01:06:59,530 --> 01:07:01,840
for figuring out
local policy, it

1457
01:07:01,840 --> 01:07:05,140
is talk with your students
about that policy.

1458
01:07:05,140 --> 01:07:10,660
Engage them in conversations
around these kinds of things,

1459
01:07:10,660 --> 01:07:12,260
because my experience
with students,

1460
01:07:12,260 --> 01:07:13,810
even very, very
young students is,

1461
01:07:13,810 --> 01:07:15,670
if we ask them
serious questions,

1462
01:07:15,670 --> 01:07:17,510
promise to take their
answers seriously,

1463
01:07:17,510 --> 01:07:18,760
they'll give us good thinking.

1464
01:07:18,760 --> 01:07:21,460
ANJALI SASTRY: You know what
this suggests to me, Justin,

1465
01:07:21,460 --> 01:07:26,360
that we actually need learning
experts even more than ever.

1466
01:07:26,360 --> 01:07:29,560
So it's not a replacement--
because now you're saying we

1467
01:07:29,560 --> 01:07:34,520
need to really understand
our theory of how the design

1468
01:07:34,520 --> 01:07:36,890
of a given assignment
or exercise,

1469
01:07:36,890 --> 01:07:41,600
where is the learning
being spurred and what--

1470
01:07:41,600 --> 01:07:43,140
I teach by cases.

1471
01:07:43,140 --> 01:07:46,280
So you of know, you get
these points in a case,

1472
01:07:46,280 --> 01:07:49,670
where you can flip the story
or generate the surprise.

1473
01:07:49,670 --> 01:07:52,460
Any-- we should be
looking probably

1474
01:07:52,460 --> 01:07:54,800
at all of our assignments
from that point of view of,

1475
01:07:54,800 --> 01:07:59,420
instead of one sort of taken
for granted chunk of work,

1476
01:07:59,420 --> 01:08:01,880
how could you tease
it apart and find

1477
01:08:01,880 --> 01:08:04,280
new ways for some
of those elements

1478
01:08:04,280 --> 01:08:06,687
to benefit from
the new technology?

1479
01:08:06,687 --> 01:08:09,020
JUSTIN REICH: Or which of the
or-- and which of the ones

1480
01:08:09,020 --> 01:08:12,680
in which the technology bypasses
really important thinking.

1481
01:08:12,680 --> 01:08:16,370
Yeah, no, I think you
have just characterized--

1482
01:08:16,370 --> 01:08:19,180
for those of us who
teach with lots of text,

1483
01:08:19,180 --> 01:08:20,930
and with students
generating lots of text,

1484
01:08:20,930 --> 01:08:26,450
that is exactly the challenge
that we'll be facing.

1485
01:08:26,450 --> 01:08:28,545
Learning theory will help us.

1486
01:08:28,545 --> 01:08:30,170
And it's just going
to be so different.

1487
01:08:30,170 --> 01:08:32,729
You know, I was thinking
a bit that like for me

1488
01:08:32,729 --> 01:08:36,479
and my elective on learning,
media, and technology,

1489
01:08:36,479 --> 01:08:39,870
like having students
play around with anything

1490
01:08:39,870 --> 01:08:41,130
is pretty much fine.

1491
01:08:41,130 --> 01:08:42,130
Like that's sort of fun.

1492
01:08:42,130 --> 01:08:42,899
That's part of the project.

1493
01:08:42,899 --> 01:08:44,790
If I was teaching
structural engineering

1494
01:08:44,790 --> 01:08:47,910
to students who are going
to go build bridges,

1495
01:08:47,910 --> 01:08:54,630
I might think quite differently
about how I do my instruction

1496
01:08:54,630 --> 01:08:57,630
than other things.

1497
01:08:57,630 --> 01:09:00,779
We have a paper that's coming
out with me and Eric Klopfer

1498
01:09:00,779 --> 01:09:03,899
and some other folks on
generative AI and K-12,

1499
01:09:03,899 --> 01:09:05,790
which MIT is going
to be publishing

1500
01:09:05,790 --> 01:09:07,867
a whole series of papers.

1501
01:09:07,867 --> 01:09:09,450
And so there'll be
more to read there.

1502
01:09:09,450 --> 01:09:13,073
My colleague Mitch Resnick,
who developed Scratch

1503
01:09:13,073 --> 01:09:14,490
and the Lifelong
Kindergarten Lab,

1504
01:09:14,490 --> 01:09:16,354
has a paper out in that series.

1505
01:09:16,354 --> 01:09:17,229
And things like that.

1506
01:09:17,229 --> 01:09:19,500
So there's going to be a
lot more to read and explore

1507
01:09:19,500 --> 01:09:22,450
from MIT in the weeks
and months ahead.

1508
01:09:22,450 --> 01:09:26,160
ANJALI SASTRY: And we want to
use the Jameel World Education

1509
01:09:26,160 --> 01:09:30,810
Lab community to orchestrate
the conversations, interactions,

1510
01:09:30,810 --> 01:09:35,520
and useful idea generation
that crosses from,

1511
01:09:35,520 --> 01:09:37,620
takes insights from
the K through 12

1512
01:09:37,620 --> 01:09:42,029
experience and
research at all stages,

1513
01:09:42,029 --> 01:09:45,840
and also then to ask, how does
this connect with our ability

1514
01:09:45,840 --> 01:09:49,170
to work with and from
universities, for university

1515
01:09:49,170 --> 01:09:54,330
students, but also for those
that a university can reach.

1516
01:09:54,330 --> 01:09:57,160
Because that's our
core mission here.

1517
01:09:57,160 --> 01:09:59,220
We have so many more
questions for you, Justin,

1518
01:09:59,220 --> 01:10:03,840
but one maybe this is an easy
one for the last 60 seconds.

1519
01:10:03,840 --> 01:10:07,300
Somebody wanted to a
bit more about Quizlet.

1520
01:10:07,300 --> 01:10:11,770
JUSTIN REICH: Oh, so
Quizlet is a flashcard app

1521
01:10:11,770 --> 01:10:16,360
that was developed by an MIT
dropout named Andy Sutherland.

1522
01:10:16,360 --> 01:10:19,780
It's not an AI tool, although
like every other institution,

1523
01:10:19,780 --> 01:10:22,910
they're of incorporating
more AI into it.

1524
01:10:22,910 --> 01:10:27,730
But was just an example of
that particular assignment

1525
01:10:27,730 --> 01:10:30,130
that I had students
playing around with.

1526
01:10:30,130 --> 01:10:33,215
But it is an MIT
export of sorts.

1527
01:10:33,215 --> 01:10:34,090
ANJALI SASTRY: It is.

1528
01:10:34,090 --> 01:10:34,930
OK.

1529
01:10:34,930 --> 01:10:37,630
We want to come back and
have more conversations.

1530
01:10:37,630 --> 01:10:39,430
We definitely need
to keep tapping

1531
01:10:39,430 --> 01:10:43,000
into your expertise,
point of view,

1532
01:10:43,000 --> 01:10:46,790
insights, and cool examples.

1533
01:10:46,790 --> 01:10:49,570
So, Justin, I hope
you don't mind we'll.

1534
01:10:49,570 --> 01:10:51,550
Continue to pester you.

1535
01:10:51,550 --> 01:10:56,530
And I think a really interesting
question back to all of us

1536
01:10:56,530 --> 01:10:58,780
is, how do we use
our global community?

1537
01:10:58,780 --> 01:11:01,510
What is the value of
having this gathering

1538
01:11:01,510 --> 01:11:03,130
and this conversation?

1539
01:11:03,130 --> 01:11:05,770
We're going to thread a
sequence of conversations

1540
01:11:05,770 --> 01:11:09,230
about generative
AI in education.

1541
01:11:09,230 --> 01:11:11,060
What could come of it?

1542
01:11:11,060 --> 01:11:14,090
Where could we use
the fact that we're

1543
01:11:14,090 --> 01:11:17,450
from very different
places, but all

1544
01:11:17,450 --> 01:11:21,180
interested in
revolutionizing education?

1545
01:11:21,180 --> 01:11:24,410
How could that then contribute
to the overall dialogue?

1546
01:11:24,410 --> 01:11:26,090
So think about that.

1547
01:11:26,090 --> 01:11:29,340
We'd love to hear
your thoughts on it.

1548
01:11:29,340 --> 01:11:31,640
And while we also
get to work, we're

1549
01:11:31,640 --> 01:11:35,720
compiling resources to
share with everyone here,

1550
01:11:35,720 --> 01:11:40,280
and coming up with new designs
for sessions, conversations,

1551
01:11:40,280 --> 01:11:44,060
and of course our J-WEL week,
which is May 6 through 9

1552
01:11:44,060 --> 01:11:47,150
here on the MIT campus,
will give us a great chance

1553
01:11:47,150 --> 01:11:48,740
to dig into more.

1554
01:11:48,740 --> 01:11:52,670
So with that, huge
thanks to Justin.

1555
01:11:52,670 --> 01:11:54,180
Thank you very much.

1556
01:11:54,180 --> 01:11:58,010
We could spend many more hours
digging into questions two,

1557
01:11:58,010 --> 01:12:00,620
three, and four from your deck.

1558
01:12:00,620 --> 01:12:05,000
But you gave us a really
fascinating exposure.

1559
01:12:05,000 --> 01:12:08,690
Bill, as always, excellent
framing and bringing us

1560
01:12:08,690 --> 01:12:13,320
into really wide angle
questions about new technology.

1561
01:12:13,320 --> 01:12:17,090
And we can't wait to continue
this dialogue with everyone

1562
01:12:17,090 --> 01:12:17,700
here.

1563
01:12:17,700 --> 01:12:19,922
So thank you. thank
you, thank you.

1564
01:12:19,922 --> 01:12:21,380
JUSTIN REICH: Take
care, everybody.

1565
01:12:21,380 --> 01:12:22,338
Have a great afternoon.

1566
01:12:22,338 --> 01:12:22,840
Bye.

1567
01:12:22,840 --> 01:12:24,390
ANJALI SASTRY: Bye.

1568
01:12:24,390 --> 01:12:28,000

