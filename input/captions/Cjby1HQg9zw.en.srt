1
00:00:01,920 --> 00:00:06,560
Good morning everyone. Uh thank you for

2
00:00:03,919 --> 00:00:10,000
making it uh early in the morning. I'll

3
00:00:06,560 --> 00:00:12,800
um introduce our primary speaker Ben Fry

4
00:00:10,000 --> 00:00:15,040
who is a PhD student uh with Nick Pelosi

5
00:00:12,800 --> 00:00:18,160
who will give the seminar today in

6
00:00:15,040 --> 00:00:19,760
Harvard biopysics department. So with

7
00:00:18,160 --> 00:00:24,520
that I'm just going to hand over to

8
00:00:19,760 --> 00:00:24,520
Nick. I hope you enjoy the primer.

9
00:00:24,560 --> 00:00:28,240
>> Thank you. Thank you. Hopefully my voice

10
00:00:26,560 --> 00:00:32,480
is being amplified and everyone can hear

11
00:00:28,240 --> 00:00:34,480
me, especially people on Zoom. Um, yeah.

12
00:00:32,480 --> 00:00:37,520
So, thank you for the invitation to give

13
00:00:34,480 --> 00:00:40,879
the primer on, uh, sort of what we've

14
00:00:37,520 --> 00:00:43,440
been working on in the Paley lab uh, for

15
00:00:40,879 --> 00:00:47,680
the past couple years. uh I'm a fourth

16
00:00:43,440 --> 00:00:50,239
year PhD student for context and our lab

17
00:00:47,680 --> 00:00:52,559
generally works on problems that exist

18
00:00:50,239 --> 00:00:55,600
in the protein structural domain uh for

19
00:00:52,559 --> 00:00:57,680
context uh specifically at the interface

20
00:00:55,600 --> 00:00:59,440
of like protein lians molecular

21
00:00:57,680 --> 00:01:01,199
recognition

22
00:00:59,440 --> 00:01:03,920
um so sort of questions that we're

23
00:01:01,199 --> 00:01:05,840
interested in broadly are how can we

24
00:01:03,920 --> 00:01:07,760
design proteins from scratch that can

25
00:01:05,840 --> 00:01:10,560
bind specifically to any small molecule

26
00:01:07,760 --> 00:01:12,960
liant uh how can we maybe redesign

27
00:01:10,560 --> 00:01:16,240
existing proteins with novel functions

28
00:01:12,960 --> 00:01:18,479
to modulate metabolites or ligans. Uh

29
00:01:16,240 --> 00:01:20,479
and how can we maybe predict small

30
00:01:18,479 --> 00:01:22,880
molecule binding sites across the human

31
00:01:20,479 --> 00:01:26,159
proteome? And sort of that last question

32
00:01:22,880 --> 00:01:28,400
is sort of the key to the content that

33
00:01:26,159 --> 00:01:31,200
my PI Nick is going to present in the

34
00:01:28,400 --> 00:01:32,960
core seminar later today. Um and sort of

35
00:01:31,200 --> 00:01:35,040
on the right I'm showing some denovo

36
00:01:32,960 --> 00:01:37,520
designed helical bundle proteins that

37
00:01:35,040 --> 00:01:40,479
interact specifically with spa molecules

38
00:01:37,520 --> 00:01:44,079
in a way that's sort of a new to nature

39
00:01:40,479 --> 00:01:45,680
protein liant interface. Uh and that's

40
00:01:44,079 --> 00:01:49,479
represents work that we've done in the

41
00:01:45,680 --> 00:01:49,479
lab sort of prior.

42
00:01:52,079 --> 00:01:56,240
So as you may know uh the protein

43
00:01:54,240 --> 00:01:58,159
structure prediction and design fields

44
00:01:56,240 --> 00:02:01,759
have undergone sort of a revolution in

45
00:01:58,159 --> 00:02:04,000
the wake of the uh Nobel prize uh that

46
00:02:01,759 --> 00:02:06,320
was awarded to David Baker and Deus

47
00:02:04,000 --> 00:02:08,720
Habasus and John Jumper for protein

48
00:02:06,320 --> 00:02:10,640
structure prediction with Alphaold 2. Uh

49
00:02:08,720 --> 00:02:12,239
within the design field there have been

50
00:02:10,640 --> 00:02:14,000
some deep learning methods that have

51
00:02:12,239 --> 00:02:16,400
really revolutionized our methodology

52
00:02:14,000 --> 00:02:18,640
and our toolbox. Uh specifically I'll

53
00:02:16,400 --> 00:02:22,800
call attention to protein MPNN and this

54
00:02:18,640 --> 00:02:25,040
RF diffusion model. Um uh there's also

55
00:02:22,800 --> 00:02:27,520
been a lot of work uh on the language

56
00:02:25,040 --> 00:02:31,200
modeling side with the ESM family of

57
00:02:27,520 --> 00:02:32,800
models progen uh prot5 which you might

58
00:02:31,200 --> 00:02:36,560
be familiar with if you've worked in the

59
00:02:32,800 --> 00:02:38,480
protein uh sequence space at all sort of

60
00:02:36,560 --> 00:02:41,040
categorizing these models there's two

61
00:02:38,480 --> 00:02:42,959
different classes uh there's the 3D

62
00:02:41,040 --> 00:02:46,480
structure based so alpha fold protein

63
00:02:42,959 --> 00:02:48,160
mpn rf diffusion and the sequence based

64
00:02:46,480 --> 00:02:50,560
and something to sort of call out is

65
00:02:48,160 --> 00:02:52,640
just the many orders of magnitude

66
00:02:50,560 --> 00:02:54,080
difference between the amount of

67
00:02:52,640 --> 00:02:55,920
training data that you have for

68
00:02:54,080 --> 00:02:58,319
sequence-based modeling versus structure

69
00:02:55,920 --> 00:03:01,440
based modeling. Um so if you have a

70
00:02:58,319 --> 00:03:04,080
problem that can be tackled uh with

71
00:03:01,440 --> 00:03:07,280
sequence language model embed embeddings

72
00:03:04,080 --> 00:03:09,440
uh it's probably fruitful to try using

73
00:03:07,280 --> 00:03:11,519
the sequence-based modeling if you can.

74
00:03:09,440 --> 00:03:13,280
Um but we've seen very good and

75
00:03:11,519 --> 00:03:15,680
promising results from structure guided

76
00:03:13,280 --> 00:03:19,599
modeling as well.

77
00:03:15,680 --> 00:03:21,200
So sort of to review um

78
00:03:19,599 --> 00:03:23,360
a general principle for all deep

79
00:03:21,200 --> 00:03:24,879
learning models is that information is

80
00:03:23,360 --> 00:03:26,640
stored in highdimensional vector

81
00:03:24,879 --> 00:03:29,280
representations which I'm sure many of

82
00:03:26,640 --> 00:03:31,360
you are familiar with. Uh sort of a

83
00:03:29,280 --> 00:03:33,680
really nice canonical example of

84
00:03:31,360 --> 00:03:37,040
studying this is the wordtovec model

85
00:03:33,680 --> 00:03:39,200
that was published in 2013. Uh, and it's

86
00:03:37,040 --> 00:03:42,480
this sort of classical natural language

87
00:03:39,200 --> 00:03:44,720
processing example of if I embed words

88
00:03:42,480 --> 00:03:46,720
in like what we would now consider to be

89
00:03:44,720 --> 00:03:49,760
a relatively lowdimensional vector

90
00:03:46,720 --> 00:03:51,680
embedding. Um, you can do sort of simple

91
00:03:49,760 --> 00:03:54,640
vector arithmetic operations and then

92
00:03:51,680 --> 00:03:57,280
look up from your library of words what

93
00:03:54,640 --> 00:03:58,879
the closest resulting vector is. So like

94
00:03:57,280 --> 00:04:01,280
uh an example that's brought up

95
00:03:58,879 --> 00:04:03,519
frequently is this idea of like the king

96
00:04:01,280 --> 00:04:06,080
vector minus the man vector plus the

97
00:04:03,519 --> 00:04:07,920
woman vector equals queen or is the

98
00:04:06,080 --> 00:04:10,000
closest in the embedding space to the

99
00:04:07,920 --> 00:04:13,519
queen vector. And there's sort of all

100
00:04:10,000 --> 00:04:16,400
different kinds of uh words that you can

101
00:04:13,519 --> 00:04:18,239
replace uh king, man, woman, queen with

102
00:04:16,400 --> 00:04:20,479
uh that have similar behaviors. And this

103
00:04:18,239 --> 00:04:23,520
was also shown in 2013 with like a very

104
00:04:20,479 --> 00:04:25,840
simple uh wordtovec model. But obviously

105
00:04:23,520 --> 00:04:27,440
in 2017 we now have transformers. And

106
00:04:25,840 --> 00:04:29,360
this has sort of revolutionaliz

107
00:04:27,440 --> 00:04:31,759
revolutionized the way that we do a lot

108
00:04:29,360 --> 00:04:35,440
of natural language processing modeling.

109
00:04:31,759 --> 00:04:37,919
Uh and sort of recently with the advent

110
00:04:35,440 --> 00:04:41,840
of like large language models. Uh you

111
00:04:37,919 --> 00:04:44,800
can see we've gone from maybe 164

112
00:04:41,840 --> 00:04:46,479
dimensional embeddings to 7,168

113
00:04:44,800 --> 00:04:49,440
dimensional word embeddings which are

114
00:04:46,479 --> 00:04:52,479
used by like the deepseek R1 model. Uh

115
00:04:49,440 --> 00:04:54,240
so this embeddings being stored in

116
00:04:52,479 --> 00:04:56,000
highdimensional vector representations

117
00:04:54,240 --> 00:04:59,280
is a common thread that's been very

118
00:04:56,000 --> 00:05:00,800
persistent throughout uh deep learning

119
00:04:59,280 --> 00:05:02,800
methodology in the natural language

120
00:05:00,800 --> 00:05:05,520
processing space. But it's also

121
00:05:02,800 --> 00:05:07,199
something that's been uh key to the

122
00:05:05,520 --> 00:05:08,800
function of protein structure and

123
00:05:07,199 --> 00:05:10,479
protein sequence domain modeling as

124
00:05:08,800 --> 00:05:12,320
well.

125
00:05:10,479 --> 00:05:14,479
So representation learning as you might

126
00:05:12,320 --> 00:05:17,199
be aware is a field that specifically

127
00:05:14,479 --> 00:05:19,840
deals with building up representations

128
00:05:17,199 --> 00:05:22,080
uh for downstream tasks. So sort of how

129
00:05:19,840 --> 00:05:25,280
do you structure your vector space or

130
00:05:22,080 --> 00:05:27,280
latent space in such a way that uh your

131
00:05:25,280 --> 00:05:29,680
model understands

132
00:05:27,280 --> 00:05:32,160
um for example protein protein

133
00:05:29,680 --> 00:05:34,080
interactions protein lian interactions.

134
00:05:32,160 --> 00:05:35,680
uh so I'm showing atomica which is a

135
00:05:34,080 --> 00:05:37,840
Marinka Zitnik paper that was published

136
00:05:35,680 --> 00:05:39,759
in the last year um which has been sort

137
00:05:37,840 --> 00:05:42,320
of an exciting work uh trying to

138
00:05:39,759 --> 00:05:44,560
generalize these at the atomic level uh

139
00:05:42,320 --> 00:05:47,919
using threedimensional structure but

140
00:05:44,560 --> 00:05:50,560
sort of something that is hypothesized

141
00:05:47,919 --> 00:05:52,240
in the field is that alphafold 2 like

142
00:05:50,560 --> 00:05:53,680
models these graph neural nets that are

143
00:05:52,240 --> 00:05:55,680
trained on like the entire protein

144
00:05:53,680 --> 00:05:56,960
structural database uh and protein

145
00:05:55,680 --> 00:05:59,199
language models already use

146
00:05:56,960 --> 00:06:01,600
highdimensional embeddings for the tasks

147
00:05:59,199 --> 00:06:03,919
that they're trained to perform. So can

148
00:06:01,600 --> 00:06:06,240
we train simple models like regressions,

149
00:06:03,919 --> 00:06:07,919
decision trees, etc. uh that sort of

150
00:06:06,240 --> 00:06:09,840
bootstrap off of these rich feature

151
00:06:07,919 --> 00:06:14,400
embeddings that are already generated

152
00:06:09,840 --> 00:06:17,199
for um these orthogonal modeling tasks.

153
00:06:14,400 --> 00:06:18,720
Um so sort of to review the types of

154
00:06:17,199 --> 00:06:20,319
embeddings that you can get out of these

155
00:06:18,720 --> 00:06:22,400
different types of models, I'm going to

156
00:06:20,319 --> 00:06:24,880
sort of briefly cover the alphafold 2

157
00:06:22,400 --> 00:06:26,800
model. Um so as you probably know,

158
00:06:24,880 --> 00:06:28,479
Alphaold 2 is a model for predicting

159
00:06:26,800 --> 00:06:31,520
threedimensional protein structure from

160
00:06:28,479 --> 00:06:33,440
an amino acid sequence. Uh the inputs

161
00:06:31,520 --> 00:06:34,720
are sort of a protein sequence of

162
00:06:33,440 --> 00:06:37,039
interest that you want to predict the

163
00:06:34,720 --> 00:06:40,000
structure for uh and a multiple sequence

164
00:06:37,039 --> 00:06:41,280
alignment of similar multiple of similar

165
00:06:40,000 --> 00:06:43,520
protein sequences that are found

166
00:06:41,280 --> 00:06:46,000
throughout the tree of life. Uh and

167
00:06:43,520 --> 00:06:48,080
Alphold 2's task is to decode the

168
00:06:46,000 --> 00:06:50,319
evolutionary co-variation in the

169
00:06:48,080 --> 00:06:52,960
multiple sequence alignment uh and turn

170
00:06:50,319 --> 00:06:54,800
that into a threedimensional structure.

171
00:06:52,960 --> 00:06:56,960
So if you break down the architecture of

172
00:06:54,800 --> 00:06:59,280
AlphaFold 2, you can sort of see how the

173
00:06:56,960 --> 00:07:01,199
input sequence sort of flows through the

174
00:06:59,280 --> 00:07:03,440
model resulting in a threedimensional

175
00:07:01,199 --> 00:07:05,520
structure. Um, but there's two key

176
00:07:03,440 --> 00:07:08,000
things that I want to call attention to.

177
00:07:05,520 --> 00:07:10,479
Uh, there's basically two main blocks in

178
00:07:08,000 --> 00:07:11,919
Alphold 2 that are responsible for the

179
00:07:10,479 --> 00:07:13,599
transformations that are applied to this

180
00:07:11,919 --> 00:07:15,599
input sequence that turn it into a

181
00:07:13,599 --> 00:07:17,840
structure. The first of which is called

182
00:07:15,599 --> 00:07:19,599
the EvoFormer, uh, which we'll cover in

183
00:07:17,840 --> 00:07:21,280
a second, uh, and then the structure

184
00:07:19,599 --> 00:07:24,160
module which actually generates the 3D

185
00:07:21,280 --> 00:07:26,240
structure. But the Evoformer is tasked

186
00:07:24,160 --> 00:07:28,800
with generating embeddings that we can

187
00:07:26,240 --> 00:07:31,360
take out of the alpha 2 model and use

188
00:07:28,800 --> 00:07:33,120
for downstream tasks. Uh there's two

189
00:07:31,360 --> 00:07:34,720
main embeddings. One is the single

190
00:07:33,120 --> 00:07:38,080
representation which is sort of the

191
00:07:34,720 --> 00:07:39,759
embedding of every amino acid in the

192
00:07:38,080 --> 00:07:44,000
query sequence that's passed into

193
00:07:39,759 --> 00:07:46,319
alphafold 2. Uh so this is sort of n by

194
00:07:44,000 --> 00:07:48,400
c where c is channels of the embedding

195
00:07:46,319 --> 00:07:50,400
and n is just the number of amino acids

196
00:07:48,400 --> 00:07:52,160
in the sequence. Uh, and then there's

197
00:07:50,400 --> 00:07:55,599
also a pair representation which

198
00:07:52,160 --> 00:07:58,080
explicitly encodes how every I residue

199
00:07:55,599 --> 00:07:59,840
co-varies with every J residue and

200
00:07:58,080 --> 00:08:04,240
there's a specific embedding for that

201
00:07:59,840 --> 00:08:06,479
interaction between that pair. Um, so

202
00:08:04,240 --> 00:08:09,360
why is this pair-wise covariation

203
00:08:06,479 --> 00:08:10,560
information used in alphafold 2? Um so

204
00:08:09,360 --> 00:08:12,479
something that's really interesting

205
00:08:10,560 --> 00:08:14,560
that's fun to bring up is that prior to

206
00:08:12,479 --> 00:08:16,000
like the deep learning revolution simple

207
00:08:14,560 --> 00:08:17,759
protein structure prediction was

208
00:08:16,000 --> 00:08:21,280
actually able to be performed solely

209
00:08:17,759 --> 00:08:24,319
from pairwise uh co-variation using pots

210
00:08:21,280 --> 00:08:25,759
models or direct couplings analysis. So

211
00:08:24,319 --> 00:08:28,400
the way that this works is you would

212
00:08:25,759 --> 00:08:30,080
take your query amino acid sequence,

213
00:08:28,400 --> 00:08:32,000
generate a multiple sequence alignment

214
00:08:30,080 --> 00:08:34,000
and then sort of look at columns in the

215
00:08:32,000 --> 00:08:36,399
multiple sequence alignment that covary

216
00:08:34,000 --> 00:08:38,320
with each other and then there was a

217
00:08:36,399 --> 00:08:39,760
hypothesis that things that covary with

218
00:08:38,320 --> 00:08:42,080
each other implies some sort of

219
00:08:39,760 --> 00:08:44,000
structural constraint. So if you're

220
00:08:42,080 --> 00:08:46,959
covariing which you can formulate with

221
00:08:44,000 --> 00:08:50,320
this sort of pots model formulation

222
00:08:46,959 --> 00:08:52,320
um this coupling term J should imply

223
00:08:50,320 --> 00:08:54,640
some small uh distance in

224
00:08:52,320 --> 00:08:56,320
threedimensional space and then sort of

225
00:08:54,640 --> 00:08:59,200
energy based refinement could be applied

226
00:08:56,320 --> 00:09:00,160
to an initial structural map uh which

227
00:08:59,200 --> 00:09:01,839
allows you to generate a

228
00:09:00,160 --> 00:09:04,399
threedimensional structure. Um but sort

229
00:09:01,839 --> 00:09:06,880
of the key is that this methodology is

230
00:09:04,399 --> 00:09:10,240
restricted only to pairwise evolutionary

231
00:09:06,880 --> 00:09:13,040
co-ariation. So, Alphold 2's Evoformer

232
00:09:10,240 --> 00:09:15,519
is explicitly a transformer-based model.

233
00:09:13,040 --> 00:09:17,279
It can go beyond pair-wise information

234
00:09:15,519 --> 00:09:20,160
uh and extract higher order

235
00:09:17,279 --> 00:09:22,480
co-evolutionary signal from these MSAs.

236
00:09:20,160 --> 00:09:24,240
Um and like I said uh the result of this

237
00:09:22,480 --> 00:09:27,200
is that you get a pair representation

238
00:09:24,240 --> 00:09:30,720
where every amino acid uh in your input

239
00:09:27,200 --> 00:09:33,360
sequence uh has an explicit embedding

240
00:09:30,720 --> 00:09:35,279
for how it attends to other amino acids

241
00:09:33,360 --> 00:09:36,959
in that sequence uh which we can take

242
00:09:35,279 --> 00:09:38,640
advantage of for training downstream

243
00:09:36,959 --> 00:09:40,880
models.

244
00:09:38,640 --> 00:09:42,720
Um, and something that's a fun caveat is

245
00:09:40,880 --> 00:09:44,480
that the MSA information is actually

246
00:09:42,720 --> 00:09:46,959
optional. Uh, and if you work in the

247
00:09:44,480 --> 00:09:48,560
denovo protein design field, uh, we

248
00:09:46,959 --> 00:09:50,240
almost never use multiple sequence

249
00:09:48,560 --> 00:09:52,800
alignments to evaluate our denovo

250
00:09:50,240 --> 00:09:54,560
designs because we can rely directly on

251
00:09:52,800 --> 00:09:56,080
the sort of implicit energy function

252
00:09:54,560 --> 00:09:58,080
that alphaold has learned to predict

253
00:09:56,080 --> 00:10:01,279
threedimensional structure. I'm happy to

254
00:09:58,080 --> 00:10:03,519
talk more about that if you're curious.

255
00:10:01,279 --> 00:10:05,279
So if the evoiformer model is similar to

256
00:10:03,519 --> 00:10:07,519
a language model encoder building up

257
00:10:05,279 --> 00:10:09,040
these highdimensional representations uh

258
00:10:07,519 --> 00:10:11,519
of the multiple sequence alignment

259
00:10:09,040 --> 00:10:13,839
information query sequence the structure

260
00:10:11,519 --> 00:10:15,600
model is this 3D rotation and variant

261
00:10:13,839 --> 00:10:17,279
graph neural network that's tasked with

262
00:10:15,600 --> 00:10:18,399
decoding those highdimensional

263
00:10:17,279 --> 00:10:21,279
representations through a

264
00:10:18,399 --> 00:10:22,880
threedimensional structure. Um how it

265
00:10:21,279 --> 00:10:26,640
specifically works isn't super important

266
00:10:22,880 --> 00:10:28,480
for this talk. Um but

267
00:10:26,640 --> 00:10:30,000
yeah, it's basically tasked with just

268
00:10:28,480 --> 00:10:33,279
decoding these representations to

269
00:10:30,000 --> 00:10:35,519
threedimensional information.

270
00:10:33,279 --> 00:10:37,040
So again looking at this overview, we

271
00:10:35,519 --> 00:10:38,880
can take these single and pair

272
00:10:37,040 --> 00:10:40,880
representations from AlphaFold 2 for

273
00:10:38,880 --> 00:10:42,320
downstream applications and Nick is

274
00:10:40,880 --> 00:10:45,680
going to talk more about how we do this

275
00:10:42,320 --> 00:10:47,440
in a model called Alphault 2 bind. Um

276
00:10:45,680 --> 00:10:50,240
but something to call out is that

277
00:10:47,440 --> 00:10:52,399
Alphafold 2 only takes in an input amino

278
00:10:50,240 --> 00:10:54,560
acid sequence. It literally doesn't know

279
00:10:52,399 --> 00:10:56,480
anything about uh the presence or

280
00:10:54,560 --> 00:10:58,079
absence of small molecules. Any

281
00:10:56,480 --> 00:11:00,399
information that it might have about

282
00:10:58,079 --> 00:11:02,399
where small molecule binding sites are

283
00:11:00,399 --> 00:11:04,079
in a protein structure is purely from

284
00:11:02,399 --> 00:11:05,920
like information that's contained in

285
00:11:04,079 --> 00:11:08,720
that multiple sequence alignment and

286
00:11:05,920 --> 00:11:10,959
stored in those embeddings. So uh sort

287
00:11:08,720 --> 00:11:13,360
of an open question uh before this paper

288
00:11:10,959 --> 00:11:16,959
was published is does alpha 2 understand

289
00:11:13,360 --> 00:11:18,880
information about any uh implicit small

290
00:11:16,959 --> 00:11:22,000
molecules in a protein sequence? Uh and

291
00:11:18,880 --> 00:11:24,160
we'll talk more about this later.

292
00:11:22,000 --> 00:11:26,480
Um so sort of calling back to my

293
00:11:24,160 --> 00:11:28,240
previous slide uh there's significantly

294
00:11:26,480 --> 00:11:30,240
more sequencing data than there is

295
00:11:28,240 --> 00:11:32,720
protein structural data. So what about

296
00:11:30,240 --> 00:11:36,079
models that take advantage of this? Uh

297
00:11:32,720 --> 00:11:38,880
one such model is this ESM if where if

298
00:11:36,079 --> 00:11:40,480
stands for inverse folding. So inverse

299
00:11:38,880 --> 00:11:42,079
folding is the task of taking a

300
00:11:40,480 --> 00:11:43,839
three-dimensional structure and

301
00:11:42,079 --> 00:11:45,600
generating a protein sequence that's

302
00:11:43,839 --> 00:11:47,760
compatible with folding to that

303
00:11:45,600 --> 00:11:51,040
structure and ideally having whatever

304
00:11:47,760 --> 00:11:54,480
function uh you could also condition it

305
00:11:51,040 --> 00:11:58,000
with. Um for the ESM if model you can

306
00:11:54,480 --> 00:12:00,079
extract sort of an N or L by 512

307
00:11:58,000 --> 00:12:03,279
dimensional vector where L is just the

308
00:12:00,079 --> 00:12:05,360
number of amino acids. uh and this model

309
00:12:03,279 --> 00:12:07,200
was trained by leveraging the amount of

310
00:12:05,360 --> 00:12:09,360
protein sequence information in these

311
00:12:07,200 --> 00:12:11,519
large sequencing data sets uh using

312
00:12:09,360 --> 00:12:14,160
alphaold 2 to generate sort of synthetic

313
00:12:11,519 --> 00:12:16,800
crystal structures uh and then training

314
00:12:14,160 --> 00:12:19,760
the ESM model for the inverse task of

315
00:12:16,800 --> 00:12:21,920
generating amino acid sequences. Um so

316
00:12:19,760 --> 00:12:24,320
this is sort of just another way to

317
00:12:21,920 --> 00:12:26,240
extract embeddings solely from geometric

318
00:12:24,320 --> 00:12:28,399
information by doing a convolution over

319
00:12:26,240 --> 00:12:30,880
a 3D graph. Um which is sort of a

320
00:12:28,399 --> 00:12:33,120
different type of information than you

321
00:12:30,880 --> 00:12:34,880
might get from leveraging sequences

322
00:12:33,120 --> 00:12:36,480
directly.

323
00:12:34,880 --> 00:12:38,480
Um and then of course we have protein

324
00:12:36,480 --> 00:12:41,200
language models which encode protein

325
00:12:38,480 --> 00:12:43,440
sequences uh by pre-training with masked

326
00:12:41,200 --> 00:12:45,600
language modeling objectives. So you

327
00:12:43,440 --> 00:12:47,360
mask some amount of protein sequences in

328
00:12:45,600 --> 00:12:49,200
these giant databases and have the model

329
00:12:47,360 --> 00:12:51,279
try to recover the amino acids that were

330
00:12:49,200 --> 00:12:52,880
masked. Uh the result of this is that

331
00:12:51,279 --> 00:12:55,040
you get probabilities over the 20

332
00:12:52,880 --> 00:12:57,200
natural amino acids for each masked

333
00:12:55,040 --> 00:12:59,680
token. Uh and then sort of under the

334
00:12:57,200 --> 00:13:02,160
hood ESM2 is just using like this BERT

335
00:12:59,680 --> 00:13:04,560
style pre-training objective in order to

336
00:13:02,160 --> 00:13:06,079
uh generate embeddings. It's an encoder

337
00:13:04,560 --> 00:13:08,560
only model if you're familiar with the

338
00:13:06,079 --> 00:13:10,959
terminology.

339
00:13:08,560 --> 00:13:13,839
So along with the ESM2 model at its

340
00:13:10,959 --> 00:13:17,839
release, uh the meta team that worked on

341
00:13:13,839 --> 00:13:19,680
it also released uh ESM fold which uh in

342
00:13:17,839 --> 00:13:22,000
the motivation of taking embeddings and

343
00:13:19,680 --> 00:13:24,240
training them to do downstream tasks uh

344
00:13:22,000 --> 00:13:26,560
literally takes the embeddings from ESM2

345
00:13:24,240 --> 00:13:28,800
for your sequence uh and then recycles

346
00:13:26,560 --> 00:13:30,800
the Alphold 2 structure module that I

347
00:13:28,800 --> 00:13:33,040
introduced earlier uh in order to

348
00:13:30,800 --> 00:13:34,880
perform 3D protein structure prediction

349
00:13:33,040 --> 00:13:37,040
using the embeddings. And this allows

350
00:13:34,880 --> 00:13:39,120
you to circumvent the task of generating

351
00:13:37,040 --> 00:13:40,800
the multiple sequence alignments which

352
00:13:39,120 --> 00:13:42,399
is generally pretty computationally

353
00:13:40,800 --> 00:13:44,240
expensive and requires searching very

354
00:13:42,399 --> 00:13:47,440
large databases.

355
00:13:44,240 --> 00:13:50,399
Um so from this you can actually extract

356
00:13:47,440 --> 00:13:52,560
uh pair and sequence representations uh

357
00:13:50,399 --> 00:13:53,920
from ESM fold though that's very rarely

358
00:13:52,560 --> 00:13:56,160
done because these are sort of

359
00:13:53,920 --> 00:13:58,399
bootstrapped off of embeddings that are

360
00:13:56,160 --> 00:13:59,680
generated by the ESM2 model uh in the

361
00:13:58,399 --> 00:14:04,639
first place. So there's not really

362
00:13:59,680 --> 00:14:06,399
additional information in these. Um

363
00:14:04,639 --> 00:14:08,880
so the actual dimension that you can

364
00:14:06,399 --> 00:14:11,760
extract from the ESM2 embedding sort of

365
00:14:08,880 --> 00:14:14,240
depends on what size model uh you're

366
00:14:11,760 --> 00:14:16,000
working with. They trained uh a bunch of

367
00:14:14,240 --> 00:14:18,160
different models at different scales to

368
00:14:16,000 --> 00:14:21,279
see sort of empirical scaling laws for

369
00:14:18,160 --> 00:14:23,360
the protein domain. Um but the key is

370
00:14:21,279 --> 00:14:25,600
that you can take embeddings again from

371
00:14:23,360 --> 00:14:27,040
the ESM2 like models and use them for

372
00:14:25,600 --> 00:14:28,880
downstream tasks like structure

373
00:14:27,040 --> 00:14:30,639
prediction.

374
00:14:28,880 --> 00:14:31,920
uh and yeah you have both single impair

375
00:14:30,639 --> 00:14:34,320
representations in the same way that

376
00:14:31,920 --> 00:14:36,560
alpha full 2 did.

377
00:14:34,320 --> 00:14:39,199
They used the ESM fold model to predict

378
00:14:36,560 --> 00:14:40,959
structures for 600 million uh amino acid

379
00:14:39,199 --> 00:14:42,560
sequences across like metagenomic

380
00:14:40,959 --> 00:14:44,480
sequencing databases which is pretty

381
00:14:42,560 --> 00:14:46,800
cool. Uh and they show that their

382
00:14:44,480 --> 00:14:48,800
structure prediction methodology is

383
00:14:46,800 --> 00:14:51,680
comparable but maybe slightly worse than

384
00:14:48,800 --> 00:14:55,120
alphafold 2 despite leveraging only

385
00:14:51,680 --> 00:14:57,839
sequence domain information. Um so ESM

386
00:14:55,120 --> 00:14:59,600
fold is this very heavyweight 3D

387
00:14:57,839 --> 00:15:01,440
rotation equavariant graph neural

388
00:14:59,600 --> 00:15:05,279
network that takes these ESM embeddings

389
00:15:01,440 --> 00:15:06,720
and generates 3D structure. Um but other

390
00:15:05,279 --> 00:15:09,680
work that's been performed in the field

391
00:15:06,720 --> 00:15:11,920
using ESM embeddings uh has shown that

392
00:15:09,680 --> 00:15:14,000
simple models such as linear regression

393
00:15:11,920 --> 00:15:17,199
support vector machines uh decision

394
00:15:14,000 --> 00:15:19,360
trees random forests etc uh can also be

395
00:15:17,199 --> 00:15:21,519
used to leverage these highdimensional

396
00:15:19,360 --> 00:15:24,959
embeddings from language models. So

397
00:15:21,519 --> 00:15:27,199
here's a uh paper from Deborah Marx's

398
00:15:24,959 --> 00:15:30,800
lab using ESM embeddings to predict

399
00:15:27,199 --> 00:15:33,839
optimal enzyme pH.

400
00:15:30,800 --> 00:15:35,760
Um so sort of a key point though is that

401
00:15:33,839 --> 00:15:38,880
interesting proteins that we care about

402
00:15:35,760 --> 00:15:41,440
are dynamic. Um particularly if you're

403
00:15:38,880 --> 00:15:43,440
interested in lian binding function, uh

404
00:15:41,440 --> 00:15:46,639
you might be interested in proteins that

405
00:15:43,440 --> 00:15:49,279
possess so-called cryptic pockets. So

406
00:15:46,639 --> 00:15:52,000
proteins that sort of have a uh

407
00:15:49,279 --> 00:15:54,399
two-state equilibrium where in one state

408
00:15:52,000 --> 00:15:56,160
a pocket is not available for small

409
00:15:54,399 --> 00:15:58,399
molecule binding but in another it

410
00:15:56,160 --> 00:16:00,320
fluctuates open and it suddenly becomes

411
00:15:58,399 --> 00:16:02,560
uh available for small molecule binding.

412
00:16:00,320 --> 00:16:04,800
Uh there's like a change in pocket

413
00:16:02,560 --> 00:16:06,240
volume between the two states. Uh and if

414
00:16:04,800 --> 00:16:08,240
you're leveraging a structure prediction

415
00:16:06,240 --> 00:16:10,720
model like alpha 2, you're only getting

416
00:16:08,240 --> 00:16:14,560
like a single snapshot of the protein

417
00:16:10,720 --> 00:16:16,800
structure. So uh a hypothesis is that

418
00:16:14,560 --> 00:16:18,959
maybe these protein structure models or

419
00:16:16,800 --> 00:16:20,880
protein language models encode something

420
00:16:18,959 --> 00:16:23,199
about protein dynamics or local

421
00:16:20,880 --> 00:16:25,680
flexibility even if we don't have like

422
00:16:23,199 --> 00:16:27,440
massive training data for like gigantic

423
00:16:25,680 --> 00:16:30,480
long time scale molecular dynamics

424
00:16:27,440 --> 00:16:32,880
trajectories. Uh we can get at

425
00:16:30,480 --> 00:16:35,920
structural flexibility um or

426
00:16:32,880 --> 00:16:38,560
confirmational heterogeneity through uh

427
00:16:35,920 --> 00:16:41,279
interrogating these embeddings.

428
00:16:38,560 --> 00:16:43,839
So, Dino 1 is a model that came out

429
00:16:41,279 --> 00:16:45,680
recently in 2023. Uh, and actually my

430
00:16:43,839 --> 00:16:47,519
friend and colleague Gina El Nesser is

431
00:16:45,680 --> 00:16:49,839
giving a talk on this tonight at the

432
00:16:47,519 --> 00:16:51,680
Boston Protein Design and Modeling Club.

433
00:16:49,839 --> 00:16:53,759
So, if that sounds interesting to you,

434
00:16:51,680 --> 00:16:55,920
like how you can use labels from nuclear

435
00:16:53,759 --> 00:16:58,800
magnetic resonance spectroscopy

436
00:16:55,920 --> 00:17:01,120
experiments uh to try to infer protein

437
00:16:58,800 --> 00:17:02,880
structural flexibility. Um, check that

438
00:17:01,120 --> 00:17:05,199
out. I think it's at 7 p.m., but Nick

439
00:17:02,880 --> 00:17:07,280
can confirm that. Um, but something that

440
00:17:05,199 --> 00:17:08,880
was cool is that they evaluated a bunch

441
00:17:07,280 --> 00:17:10,720
of different representations that you

442
00:17:08,880 --> 00:17:13,280
can use to try to predict these

443
00:17:10,720 --> 00:17:14,799
confirmational flexibility labels, uh,

444
00:17:13,280 --> 00:17:17,439
including the alpha full 2 pair

445
00:17:14,799 --> 00:17:19,280
representation, ESM2 embeddings, ESM3

446
00:17:17,439 --> 00:17:20,799
embeddings, and then ESM3 with

447
00:17:19,280 --> 00:17:22,959
structural embeddings if you're familiar

448
00:17:20,799 --> 00:17:26,160
with that model. Uh, and they took the

449
00:17:22,959 --> 00:17:27,360
best performing uh, layer from the

450
00:17:26,160 --> 00:17:29,120
transformer. So, you can actually

451
00:17:27,360 --> 00:17:31,120
interrogate more than just the output

452
00:17:29,120 --> 00:17:34,160
layer uh, embeddings.

453
00:17:31,120 --> 00:17:35,520
um since different hierarchies of

454
00:17:34,160 --> 00:17:38,640
information might be passed up through

455
00:17:35,520 --> 00:17:43,280
as the model uh applies convolutions to

456
00:17:38,640 --> 00:17:46,000
the input data. Um yeah,

457
00:17:43,280 --> 00:17:48,320
so to answer my own question, uh you can

458
00:17:46,000 --> 00:17:50,320
definitely train simple models on the

459
00:17:48,320 --> 00:17:52,480
embeddings of these complicated deep

460
00:17:50,320 --> 00:17:55,120
learning models. uh and sort of doing

461
00:17:52,480 --> 00:17:56,960
this has the advantage of uh not

462
00:17:55,120 --> 00:17:58,880
necessarily needing a massive training

463
00:17:56,960 --> 00:18:01,360
data set uh in the way that you might

464
00:17:58,880 --> 00:18:03,520
need to train something like ESM fold uh

465
00:18:01,360 --> 00:18:05,440
you can use significantly fewer labels

466
00:18:03,520 --> 00:18:06,880
uh which for the case of protein small

467
00:18:05,440 --> 00:18:09,760
molecule interactions might be really

468
00:18:06,880 --> 00:18:12,080
important um so this begs the question

469
00:18:09,760 --> 00:18:14,240
what is this AF2 bind thing that I've

470
00:18:12,080 --> 00:18:15,679
been sort of dancing around uh I'm not

471
00:18:14,240 --> 00:18:17,440
going to go into too much detail because

472
00:18:15,679 --> 00:18:18,960
we have a whole like keynote seminar

473
00:18:17,440 --> 00:18:23,360
after this that Nick is going to present

474
00:18:18,960 --> 00:18:26,240
on it but uh to sort of entice you to

475
00:18:23,360 --> 00:18:27,840
stay for that. Um there are a bunch of

476
00:18:26,240 --> 00:18:29,520
interesting questions that we can

477
00:18:27,840 --> 00:18:31,440
potentially answer if we could predict

478
00:18:29,520 --> 00:18:34,000
small molecule interactions and binding

479
00:18:31,440 --> 00:18:36,160
sites across the proteome. Um one of

480
00:18:34,000 --> 00:18:39,679
which is can we discover novel function

481
00:18:36,160 --> 00:18:41,919
in unccharacterized natural proteins? Uh

482
00:18:39,679 --> 00:18:44,640
can we identify novel sites with which

483
00:18:41,919 --> 00:18:47,200
to target drug development um campaigns

484
00:18:44,640 --> 00:18:49,039
to? Uh, and then sort of an interesting

485
00:18:47,200 --> 00:18:52,240
philosophical question of like can we

486
00:18:49,039 --> 00:18:53,520
actually learn what alpha 2 understands

487
00:18:52,240 --> 00:18:54,880
about protein small molecule

488
00:18:53,520 --> 00:18:56,400
interactions even though it's never

489
00:18:54,880 --> 00:18:59,200
trained explicitly with this as an

490
00:18:56,400 --> 00:19:01,520
objective. Um, and this is a table that

491
00:18:59,200 --> 00:19:03,919
I grabbed directly from the AF2 bind

492
00:19:01,520 --> 00:19:05,600
paper. Uh, and hopefully the models that

493
00:19:03,919 --> 00:19:07,679
they're sort of using embeddings from

494
00:19:05,600 --> 00:19:10,080
look familiar at this point. uh and you

495
00:19:07,679 --> 00:19:12,000
can see how the uh ability of this model

496
00:19:10,080 --> 00:19:13,760
to predict binding site changes

497
00:19:12,000 --> 00:19:17,360
depending on what embeddings or

498
00:19:13,760 --> 00:19:19,679
combinations thereof you're applying.

499
00:19:17,360 --> 00:19:21,520
Um so AF2 bind under the hood is just a

500
00:19:19,679 --> 00:19:24,480
logistic regression where it's doing a

501
00:19:21,520 --> 00:19:26,080
binary classification task on the input

502
00:19:24,480 --> 00:19:29,520
embeddings

503
00:19:26,080 --> 00:19:33,440
um using the alpha alpha 2 pair

504
00:19:29,520 --> 00:19:35,919
embeddings and generating uh a label

505
00:19:33,440 --> 00:19:38,160
prediction one or zero for whether an

506
00:19:35,919 --> 00:19:41,039
amino acid is interacting with a small

507
00:19:38,160 --> 00:19:44,160
molecule.

508
00:19:41,039 --> 00:19:46,480
Um and then to sort of contrast between

509
00:19:44,160 --> 00:19:49,520
what existing methodologies for protein

510
00:19:46,480 --> 00:19:51,679
pocket prediction might do. Um I

511
00:19:49,520 --> 00:19:53,840
mentioned before that protein structure

512
00:19:51,679 --> 00:19:56,000
prediction generates these snapshots.

513
00:19:53,840 --> 00:19:58,320
X-ray crystalallography crym like the

514
00:19:56,000 --> 00:20:00,000
traditional uh methodologies that we've

515
00:19:58,320 --> 00:20:01,919
used to generate 3D structural

516
00:20:00,000 --> 00:20:05,039
information only give you sort of static

517
00:20:01,919 --> 00:20:07,200
snapshots of your protein. Um and

518
00:20:05,039 --> 00:20:09,919
F-Pocket is sort of an established

519
00:20:07,200 --> 00:20:12,480
method that uses uh 3D geometry

520
00:20:09,919 --> 00:20:14,080
information uh specifically voronoid

521
00:20:12,480 --> 00:20:17,039
tessillation in order to identify

522
00:20:14,080 --> 00:20:20,720
cavities in the surface of a protein and

523
00:20:17,039 --> 00:20:22,240
then sort of makes the leap that a

524
00:20:20,720 --> 00:20:24,480
cavity inside of a protein is a

525
00:20:22,240 --> 00:20:27,440
potential small molecule binding site.

526
00:20:24,480 --> 00:20:29,280
Uh because this is completely dependent

527
00:20:27,440 --> 00:20:31,280
on the 3D structure that you pass into

528
00:20:29,280 --> 00:20:32,880
the model. Um if you have anything like

529
00:20:31,280 --> 00:20:36,000
a cryptic pocket where you have a

530
00:20:32,880 --> 00:20:37,919
collapsed form uh of your protein when a

531
00:20:36,000 --> 00:20:39,520
small molecule is unbound like F-Pocket

532
00:20:37,919 --> 00:20:41,919
as a methodology is not going to be able

533
00:20:39,520 --> 00:20:45,120
to identify a small molecule binding

534
00:20:41,919 --> 00:20:47,440
site. Uh so we hypothesize that because

535
00:20:45,120 --> 00:20:49,760
alpha fold 2 binds takes these parametas

536
00:20:47,440 --> 00:20:51,760
embeddings from the alpha 2 model even

537
00:20:49,760 --> 00:20:54,080
though alphafold 2 doesn't generate

538
00:20:51,760 --> 00:20:55,440
dynamics information directly it might

539
00:20:54,080 --> 00:20:57,440
understand something about this

540
00:20:55,440 --> 00:21:00,240
equilibrium that a natural protein might

541
00:20:57,440 --> 00:21:01,919
have with its cryptic pocket uh and you

542
00:21:00,240 --> 00:21:03,840
can see its actual predictions over a

543
00:21:01,919 --> 00:21:05,280
cryptic pocket. it's still able to

544
00:21:03,840 --> 00:21:07,120
identify some residues that might be

545
00:21:05,280 --> 00:21:10,000
implicated in a binding interface uh

546
00:21:07,120 --> 00:21:13,440
despite being applied specifically to

547
00:21:10,000 --> 00:21:16,679
the uh structure where the binding site

548
00:21:13,440 --> 00:21:16,679
is oluded.

549
00:21:18,080 --> 00:21:23,120
Um so then just to sort of gratify

550
00:21:20,960 --> 00:21:25,600
myself uh I haven't worked on alpha fold

551
00:21:23,120 --> 00:21:27,039
2 bind personally but I have worked on

552
00:21:25,600 --> 00:21:31,360
uh graph neural nets for protein

553
00:21:27,039 --> 00:21:34,559
sequence design uh and pre-training uh

554
00:21:31,360 --> 00:21:36,640
part of my network called laser mpnn uh

555
00:21:34,559 --> 00:21:38,559
with an orthogonal task of predicting

556
00:21:36,640 --> 00:21:40,799
predicting partial charges from quantum

557
00:21:38,559 --> 00:21:43,120
mechanical calculations uh ended up

558
00:21:40,799 --> 00:21:44,799
being sort of a useful task uh that

559
00:21:43,120 --> 00:21:47,440
improved model performance for protein

560
00:21:44,799 --> 00:21:50,240
sequence generation. uh we were able to

561
00:21:47,440 --> 00:21:51,280
show that the laser ampana model that we

562
00:21:50,240 --> 00:21:52,880
developed outperforms the

563
00:21:51,280 --> 00:21:54,559
state-of-the-art allowed us to do

564
00:21:52,880 --> 00:21:57,360
essentially zero shot design for small

565
00:21:54,559 --> 00:21:58,880
molecule binding protein interactions uh

566
00:21:57,360 --> 00:22:01,120
when you apply the same filtering and

567
00:21:58,880 --> 00:22:03,600
ranking criteria uh and we were actually

568
00:22:01,120 --> 00:22:05,760
able to optimize using just predicted

569
00:22:03,600 --> 00:22:08,320
probabilities from our laser m&m model

570
00:22:05,760 --> 00:22:10,080
our best design from 120 nanomear to a

571
00:22:08,320 --> 00:22:13,280
one nanomer binder which was really

572
00:22:10,080 --> 00:22:16,000
exciting. Um so that's all I have for

573
00:22:13,280 --> 00:22:17,440
you today. I'm looking forward to the

574
00:22:16,000 --> 00:22:19,039
seminar that Nick is going to give later

575
00:22:17,440 --> 00:22:22,200
and I'm happy to take any questions you

576
00:22:19,039 --> 00:22:22,200
may have.

577
00:22:35,039 --> 00:22:40,640
>> Uh thank you for your presentation. Um

578
00:22:38,400 --> 00:22:42,159
so for many of your tasks you were

579
00:22:40,640 --> 00:22:44,640
prompting for example the different

580
00:22:42,159 --> 00:22:47,200
layers of the transformers but this bit

581
00:22:44,640 --> 00:22:49,120
this might be somehow very inefficient

582
00:22:47,200 --> 00:22:52,000
because still you have a lot of faults

583
00:22:49,120 --> 00:22:54,799
to try. Have you tried for example uh

584
00:22:52,000 --> 00:22:57,200
Laura where you just fine- tune for a

585
00:22:54,799 --> 00:23:00,240
few iterations and if that's improves

586
00:22:57,200 --> 00:23:01,280
for example over you over like frozen

587
00:23:00,240 --> 00:23:03,679
layers.

588
00:23:01,280 --> 00:23:06,159
>> Yeah absolutely. So uh have we applied

589
00:23:03,679 --> 00:23:08,640
Laura to the embeddings to maybe make

590
00:23:06,159 --> 00:23:11,360
the training of these like downstream

591
00:23:08,640 --> 00:23:13,200
simple models more efficient? Um I

592
00:23:11,360 --> 00:23:15,760
haven't personally worked on these

593
00:23:13,200 --> 00:23:18,480
problems myself. Um but something that I

594
00:23:15,760 --> 00:23:20,640
have seen people do is do dimensionality

595
00:23:18,480 --> 00:23:22,720
reduction techniques like PCA on the

596
00:23:20,640 --> 00:23:24,320
embeddings to sort of reduce the

597
00:23:22,720 --> 00:23:26,640
complexity before going into some

598
00:23:24,320 --> 00:23:28,159
downstream thing. uh if you're training

599
00:23:26,640 --> 00:23:29,360
something on like the deepseek model

600
00:23:28,159 --> 00:23:30,720
embeddings where you have 7,000

601
00:23:29,360 --> 00:23:33,440
dimensions like surely there's some

602
00:23:30,720 --> 00:23:35,600
redundant information in those um so you

603
00:23:33,440 --> 00:23:38,400
could absolutely try Laura uh in order

604
00:23:35,600 --> 00:23:41,880
to compress that information uh PCA

605
00:23:38,400 --> 00:23:41,880
might work as well.

606
00:23:43,520 --> 00:23:46,640
>> Thanks for that uh presentation. I'm

607
00:23:45,520 --> 00:23:48,320
wondering if you could just talk a

608
00:23:46,640 --> 00:23:50,159
little more about laser mn. It sounded

609
00:23:48,320 --> 00:23:52,400
intriguing but it was very uh uh brief

610
00:23:50,159 --> 00:23:54,799
so I wasn't quite clear on the on the

611
00:23:52,400 --> 00:23:57,679
method doing. I can talk about laser

612
00:23:54,799 --> 00:23:59,600
ampn more uh but I think I might just

613
00:23:57,679 --> 00:24:01,120
direct you to look up my name and the

614
00:23:59,600 --> 00:24:03,679
BPDMC

615
00:24:01,120 --> 00:24:06,320
acronym because I have a recorded talk

616
00:24:03,679 --> 00:24:08,000
uh that's only about laser mn that goes

617
00:24:06,320 --> 00:24:09,600
into all the details you might have. Uh

618
00:24:08,000 --> 00:24:11,200
and if you do end up watching it feel

619
00:24:09,600 --> 00:24:15,000
free to email me afterwards. I can

620
00:24:11,200 --> 00:24:15,000
answer any additional questions.

621
00:24:21,520 --> 00:24:27,520
Um thank you for the talk. So a quick

622
00:24:23,840 --> 00:24:29,440
question about the dynamic nature of

623
00:24:27,520 --> 00:24:32,000
cryptic pocket right. So you mentioned

624
00:24:29,440 --> 00:24:34,480
multiple times that protein is not

625
00:24:32,000 --> 00:24:36,720
static and then if pocket will give

626
00:24:34,480 --> 00:24:41,200
always a static identification of

627
00:24:36,720 --> 00:24:45,360
pocket. So um how static or stoastic the

628
00:24:41,200 --> 00:24:48,159
nature of the if to bind output is

629
00:24:45,360 --> 00:24:50,640
like is it possible that given one

630
00:24:48,159 --> 00:24:52,960
confirmation do we get one prediction

631
00:24:50,640 --> 00:24:55,679
and given multiple confirmation do we

632
00:24:52,960 --> 00:24:58,159
get different predictions

633
00:24:55,679 --> 00:25:00,640
>> so something that's interesting about

634
00:24:58,159 --> 00:25:03,520
alphafold as a model is that you can

635
00:25:00,640 --> 00:25:06,080
provide it templated information um

636
00:25:03,520 --> 00:25:08,799
which is I think what they did to make

637
00:25:06,080 --> 00:25:10,320
or force alpha fold to evaluate two

638
00:25:08,799 --> 00:25:12,880
different structures uh and get

639
00:25:10,320 --> 00:25:15,039
embeddings that change depending on

640
00:25:12,880 --> 00:25:17,039
those structures. So, Alphafold

641
00:25:15,039 --> 00:25:19,120
implicitly has some uncertainty when

642
00:25:17,039 --> 00:25:20,799
it's generating a structural model. Uh

643
00:25:19,120 --> 00:25:22,559
and if you just sample it a bunch of

644
00:25:20,799 --> 00:25:25,279
times with a bunch of different seeds,

645
00:25:22,559 --> 00:25:30,480
you can get like u some information

646
00:25:25,279 --> 00:25:32,400
about how the alphafold confidence uh

647
00:25:30,480 --> 00:25:33,840
about the structure uh is sort of like

648
00:25:32,400 --> 00:25:38,159
uncertain. Uh so you get like a

649
00:25:33,840 --> 00:25:40,000
confirmational ensemble. Um my guess is

650
00:25:38,159 --> 00:25:42,159
to generate these they provided

651
00:25:40,000 --> 00:25:44,559
templates from these PTB codes to

652
00:25:42,159 --> 00:25:46,799
AlphaFold that caused it to change its

653
00:25:44,559 --> 00:25:48,480
embeddings depending on uh what

654
00:25:46,799 --> 00:25:51,600
confirmations were fed in.

655
00:25:48,480 --> 00:25:55,200
>> I I was more referring to the AF2 bind.

656
00:25:51,600 --> 00:25:57,360
>> So can we generate embedding pair from

657
00:25:55,200 --> 00:26:00,000
different confirmation and give as an

658
00:25:57,360 --> 00:26:00,960
input to generate different pockets

659
00:26:00,000 --> 00:26:03,960
let's say?

660
00:26:00,960 --> 00:26:03,960
>> Yes.

661
00:26:13,840 --> 00:26:19,360
And for example, have you tried using

662
00:26:15,760 --> 00:26:20,320
DNA language models on these type of

663
00:26:19,360 --> 00:26:20,799
tasks?

664
00:26:20,320 --> 00:26:22,799
>> And

665
00:26:20,799 --> 00:26:23,600
>> no, but that could be an interesting

666
00:26:22,799 --> 00:26:25,440
future direction.

667
00:26:23,600 --> 00:26:28,240
>> And what's your intuition? Would it work

668
00:26:25,440 --> 00:26:30,400
or

669
00:26:28,240 --> 00:26:32,320
>> I know I'm at the bro, so I can't say

670
00:26:30,400 --> 00:26:34,880
that I'm skeptical of DNA language

671
00:26:32,320 --> 00:26:36,640
model. This is, but I think models that

672
00:26:34,880 --> 00:26:38,960
were trained specifically for the

673
00:26:36,640 --> 00:26:40,640
protein modeling structure domain might

674
00:26:38,960 --> 00:26:44,760
be better for this task, but that's

675
00:26:40,640 --> 00:26:44,760
purely based off of my biases.

676
00:26:48,960 --> 00:26:54,640
It it might work in so far as the DNA

677
00:26:51,360 --> 00:26:58,000
language models understand protein um

678
00:26:54,640 --> 00:27:00,000
sequence and structure. So it might be

679
00:26:58,000 --> 00:27:02,640
one step removed from just directly

680
00:27:00,000 --> 00:27:04,559
using a protein language model or a

681
00:27:02,640 --> 00:27:07,440
structure prediction model but it might

682
00:27:04,559 --> 00:27:09,679
work. There might be something

683
00:27:07,440 --> 00:27:11,679
>> maybe there's additional information in

684
00:27:09,679 --> 00:27:13,360
uh what organism the sequence comes from

685
00:27:11,679 --> 00:27:17,120
or the specific codons that are being

686
00:27:13,360 --> 00:27:19,520
used. Uh so I know people have trained

687
00:27:17,120 --> 00:27:21,600
inverse foldings like a protein MPN like

688
00:27:19,520 --> 00:27:25,200
model specifically to generate codon

689
00:27:21,600 --> 00:27:26,880
codon uh triplets instead of amino

690
00:27:25,200 --> 00:27:28,000
acids. Uh, so I think that was a George

691
00:27:26,880 --> 00:27:30,960
Church paper that came out. I don't know

692
00:27:28,000 --> 00:27:33,440
if anyone's using it, but uh, definitely

693
00:27:30,960 --> 00:27:36,159
there's additional information in the

694
00:27:33,440 --> 00:27:39,799
DNA that might be lost when you convert

695
00:27:36,159 --> 00:27:39,799
it to amino acids.

696
00:27:46,400 --> 00:27:51,159
>> I know that the database is not working.

697
00:27:51,679 --> 00:27:56,399
Um,

698
00:27:53,679 --> 00:28:01,880
uh, TPU bio.xyz

699
00:27:56,399 --> 00:28:01,880
human proteium that doesn't load up just

700
00:28:08,640 --> 00:28:13,679
>> if if RTM is listening to this talk and

701
00:28:11,520 --> 00:28:17,720
he's out there on Zoom, uh, your

702
00:28:13,679 --> 00:28:17,720
database isn't working, Art.

703
00:28:17,760 --> 00:28:23,600
>> Um, we'll get on that. Yeah. Um I will

704
00:28:20,880 --> 00:28:25,360
say that the um and yeah, I mean a lot

705
00:28:23,600 --> 00:28:27,520
of these questions you can just ask me I

706
00:28:25,360 --> 00:28:29,520
guess after my my talk and hopefully

707
00:28:27,520 --> 00:28:34,240
I'll answer some of them during my talk.

708
00:28:29,520 --> 00:28:36,000
But um the AF2 bind uh paper, it's been

709
00:28:34,240 --> 00:28:38,480
out in bioarchchive for a long time, but

710
00:28:36,000 --> 00:28:41,840
it's actually recently accepted in a

711
00:28:38,480 --> 00:28:45,440
journal. So um we we're going to um put

712
00:28:41,840 --> 00:28:47,679
out a big list of all of the buying site

713
00:28:45,440 --> 00:28:51,600
predictions basically as a CSV file. So

714
00:28:47,679 --> 00:28:53,440
you can just pull them from a from a a

715
00:28:51,600 --> 00:28:55,440
file repo. You don't have to rely on any

716
00:28:53,440 --> 00:28:58,440
databases that are online and

717
00:28:55,440 --> 00:28:58,440
interactive.

718
00:29:04,000 --> 00:29:09,120
When I say like wild type structure, a

719
00:29:06,080 --> 00:29:12,799
starting structure, if we could have the

720
00:29:09,120 --> 00:29:15,440
cryptic pockets or how the

721
00:29:12,799 --> 00:29:17,840
not the orthosic but alosic like alosic

722
00:29:15,440 --> 00:29:20,559
and cryptic pockets open up in different

723
00:29:17,840 --> 00:29:22,320
confirmation given one input or wild

724
00:29:20,559 --> 00:29:23,600
type structure if we can have that

725
00:29:22,320 --> 00:29:26,880
predictions and I think the

726
00:29:23,600 --> 00:29:30,480
stockasticity of the embedding should

727
00:29:26,880 --> 00:29:32,720
help with that that would be fun.

728
00:29:30,480 --> 00:29:35,360
So, I guess I'm just we're just going

729
00:29:32,720 --> 00:29:37,039
back and forth with the mic, but um so

730
00:29:35,360 --> 00:29:40,080
I'll talk about that a little bit in my

731
00:29:37,039 --> 00:29:43,760
in my talk. There's a lot um a lot more

732
00:29:40,080 --> 00:29:45,679
that we did uh testing out AF2 bind

733
00:29:43,760 --> 00:29:47,679
that's in the accepted version of the

734
00:29:45,679 --> 00:29:51,279
paper that's not in the bioarchchive

735
00:29:47,679 --> 00:29:54,559
paper. Um one of those was alisteric or

736
00:29:51,279 --> 00:29:57,520
cryptic pocket analysis. Uh and I agree

737
00:29:54,559 --> 00:30:00,720
that um it would be great to be able to

738
00:29:57,520 --> 00:30:02,559
predict those and one um and it can do

739
00:30:00,720 --> 00:30:05,520
that in some cases and I'll talk about

740
00:30:02,559 --> 00:30:09,200
that which is interesting because

741
00:30:05,520 --> 00:30:13,279
only 1% of the PTB has at least

742
00:30:09,200 --> 00:30:15,120
annotated um alossteric binding sites.

743
00:30:13,279 --> 00:30:20,159
Um so it's very underrepresented in

744
00:30:15,120 --> 00:30:21,840
training. Um, but I I think that one of

745
00:30:20,159 --> 00:30:24,799
the things that I'm really interested in

746
00:30:21,840 --> 00:30:29,200
exploring in terms of future steps with

747
00:30:24,799 --> 00:30:32,720
the AF2bind stuff is um an AF2 bind as

748
00:30:29,200 --> 00:30:35,200
Ben um you know described quite well

749
00:30:32,720 --> 00:30:39,919
during his primer uses these embeddings

750
00:30:35,200 --> 00:30:41,840
from Alphal 2 uh which is tasked to

751
00:30:39,919 --> 00:30:44,880
predict a single crystal structure given

752
00:30:41,840 --> 00:30:48,320
the sequence in an MSA. Uh but could

753
00:30:44,880 --> 00:30:51,120
other could other uh neural networks

754
00:30:48,320 --> 00:30:53,360
that um maybe are trained on uh

755
00:30:51,120 --> 00:30:56,640
predicting molecular dynamics snapshots

756
00:30:53,360 --> 00:30:59,760
or trained on predicting ensembles would

757
00:30:56,640 --> 00:31:01,520
embeddings from those neural networks

758
00:30:59,760 --> 00:31:05,039
maybe they understand a little bit more

759
00:31:01,520 --> 00:31:06,799
about where a pocket might open up and

760
00:31:05,039 --> 00:31:08,399
those embeddings might be richer for

761
00:31:06,799 --> 00:31:10,480
predicting cryptic sites and it's

762
00:31:08,399 --> 00:31:11,760
something we haven't done yet but we

763
00:31:10,480 --> 00:31:12,240
really want to do.

764
00:31:11,760 --> 00:31:14,159
>> Yeah.

765
00:31:12,240 --> 00:31:17,039
>> Yeah. Um I'm going to ask one more

766
00:31:14,159 --> 00:31:18,720
question. Uh I'm I suspect that you are

767
00:31:17,039 --> 00:31:20,240
going to get to it in your talk but

768
00:31:18,720 --> 00:31:21,279
because we have time I'll just ask

769
00:31:20,240 --> 00:31:24,320
anyway.

770
00:31:21,279 --> 00:31:26,399
>> One um very critical contribution of

771
00:31:24,320 --> 00:31:29,600
this type of model could be that if we

772
00:31:26,399 --> 00:31:31,360
can use the pocket information to infer

773
00:31:29,600 --> 00:31:34,320
small molecule design. I know that in

774
00:31:31,360 --> 00:31:36,399
this paper you used amino acid just as a

775
00:31:34,320 --> 00:31:39,360
proxy right as a probe.

776
00:31:36,399 --> 00:31:42,080
>> But eventually we want to use those um

777
00:31:39,360 --> 00:31:44,240
amino acid properties somehow if we so

778
00:31:42,080 --> 00:31:46,240
first is if the pocket property can

779
00:31:44,240 --> 00:31:48,080
infer which amino acid it is likely to

780
00:31:46,240 --> 00:31:50,720
bind and then from that amino acid

781
00:31:48,080 --> 00:31:51,440
property if we can infer small molecule

782
00:31:50,720 --> 00:31:53,440
design.

783
00:31:51,440 --> 00:31:55,200
>> Yeah. Yeah. I totally agree with that

784
00:31:53,440 --> 00:31:57,039
and that's a a big motivation. It's one

785
00:31:55,200 --> 00:31:58,720
of the tanalyzing things about the work

786
00:31:57,039 --> 00:32:01,039
that I'll talk about is there is some

787
00:31:58,720 --> 00:32:04,399
signal there uh for predicting what kind

788
00:32:01,039 --> 00:32:06,960
of a small molecule might bind. I've

789
00:32:04,399 --> 00:32:08,559
been um you know, if you really want to

790
00:32:06,960 --> 00:32:11,919
uh you want an answer to this, you

791
00:32:08,559 --> 00:32:16,880
should prod Ben because I've been um

792
00:32:11,919 --> 00:32:21,279
trying to um uh sort of light Ben's fire

793
00:32:16,880 --> 00:32:23,200
to train a new model uh that uses um uh

794
00:32:21,279 --> 00:32:25,200
his graph neural network architecture

795
00:32:23,200 --> 00:32:28,000
that he's trained for um for sequence

796
00:32:25,200 --> 00:32:30,799
design uh that's conditioned on lians

797
00:32:28,000 --> 00:32:33,360
and as well as protein structure and to

798
00:32:30,799 --> 00:32:35,440
use that to not only predict pockets but

799
00:32:33,360 --> 00:32:38,880
um predict predict which kinds of lians

800
00:32:35,440 --> 00:32:41,600
might bind into certain pockets and then

801
00:32:38,880 --> 00:32:44,159
um his his neural network it actually is

802
00:32:41,600 --> 00:32:45,679
very very fast for inference. So you

803
00:32:44,159 --> 00:32:48,480
should be able to cycle through many

804
00:32:45,679 --> 00:32:50,159
different kinds of lians and home in on

805
00:32:48,480 --> 00:32:52,640
what kind of lian might be compatible

806
00:32:50,159 --> 00:32:54,159
with a pocket. So just just talk to Ben

807
00:32:52,640 --> 00:32:56,559
later and say, "Ben, this is really

808
00:32:54,159 --> 00:32:59,720
valuable for me. I'd love if you could

809
00:32:56,559 --> 00:32:59,720
do this.

810
00:33:03,440 --> 00:33:07,519
And do you think for example we need to

811
00:33:05,120 --> 00:33:09,200
cycle across all the combinations or

812
00:33:07,519 --> 00:33:12,080
have you tried for example reinforcement

813
00:33:09,200 --> 00:33:14,559
learning and uh having like a reward

814
00:33:12,080 --> 00:33:19,000
function and maybe also looking into the

815
00:33:14,559 --> 00:33:19,000
literature with a language model

816
00:33:19,679 --> 00:33:23,919
for example when you get your output and

817
00:33:22,000 --> 00:33:27,159
then you in order to refine the output

818
00:33:23,919 --> 00:33:27,159
let's say

819
00:33:28,399 --> 00:33:33,200
>> so to to refine reinforcement learning

820
00:33:30,559 --> 00:33:36,399
to refine the output of is this amino

821
00:33:33,200 --> 00:33:36,799
acid possibly interacting with a a liant

822
00:33:36,399 --> 00:33:37,600
or not.

823
00:33:36,799 --> 00:33:40,559
>> Yeah. For example,

824
00:33:37,600 --> 00:33:44,480
>> task. Um we haven't done reinforcement

825
00:33:40,559 --> 00:33:47,600
learning on that. Um there's just not a

826
00:33:44,480 --> 00:33:50,080
lot of data and you'll see in my talk

827
00:33:47,600 --> 00:33:54,559
that there's not a lot of non-redundant

828
00:33:50,080 --> 00:33:57,600
data, experimental data for this residue

829
00:33:54,559 --> 00:34:00,559
is for sure interacting with a small

830
00:33:57,600 --> 00:34:02,880
molecule lian in some instance of its

831
00:34:00,559 --> 00:34:05,120
structure in a database. Um and so

832
00:34:02,880 --> 00:34:06,960
there's and it's easy to make synthetic

833
00:34:05,120 --> 00:34:09,359
data. I can dock small molecules to

834
00:34:06,960 --> 00:34:11,760
proteins all day, but it doesn't really

835
00:34:09,359 --> 00:34:13,599
change the labels of this residue might

836
00:34:11,760 --> 00:34:15,760
be interacting with any kind of lian or

837
00:34:13,599 --> 00:34:18,320
this residue might not be. Um, and so

838
00:34:15,760 --> 00:34:20,240
reinforcement learning in that context

839
00:34:18,320 --> 00:34:22,240
doesn't really

840
00:34:20,240 --> 00:34:24,399
wouldn't in my mind wouldn't really help

841
00:34:22,240 --> 00:34:26,159
too much because it's I can of course

842
00:34:24,399 --> 00:34:28,480
tell the model it did a good job or a

843
00:34:26,159 --> 00:34:31,679
bad job at predicting, but there's no

844
00:34:28,480 --> 00:34:32,879
new training data really to um so I

845
00:34:31,679 --> 00:34:35,280
think in reinforcement learning would

846
00:34:32,879 --> 00:34:36,879
just be to equivalent to just doing

847
00:34:35,280 --> 00:34:41,040
other

848
00:34:36,879 --> 00:34:43,599
um epochs of training in this sense. And

849
00:34:41,040 --> 00:34:45,599
um we sort of already and I think I'll

850
00:34:43,599 --> 00:34:48,480
show this in my talk but we've or maybe

851
00:34:45,599 --> 00:34:51,359
I won't but um for training the model we

852
00:34:48,480 --> 00:34:52,960
actually it was easy to even though we

853
00:34:51,359 --> 00:34:56,720
did a we went through great pains and

854
00:34:52,960 --> 00:34:58,640
efforts to make a a a really good data

855
00:34:56,720 --> 00:35:01,839
training set and a good split that

856
00:34:58,640 --> 00:35:03,440
didn't have any data leakage um and a

857
00:35:01,839 --> 00:35:07,040
number of examples of training were

858
00:35:03,440 --> 00:35:09,040
still small um and we knew the test set

859
00:35:07,040 --> 00:35:10,320
was completely different from anything

860
00:35:09,040 --> 00:35:13,040
in training. for a lot of different

861
00:35:10,320 --> 00:35:15,200
ways, a lot a lot of different angles.

862
00:35:13,040 --> 00:35:17,599
Um, and we could still overfit this

863
00:35:15,200 --> 00:35:21,200
logistic regression model quite easily.

864
00:35:17,599 --> 00:35:22,960
Um, and so we had a validation set. Um,

865
00:35:21,200 --> 00:35:24,400
and of course the classic way to to know

866
00:35:22,960 --> 00:35:28,240
if you're overfitting is to look at the

867
00:35:24,400 --> 00:35:30,880
loss of train versus validation set. uh

868
00:35:28,240 --> 00:35:32,240
and so we we used L2 regularization on

869
00:35:30,880 --> 00:35:34,480
the model weights for the logistic

870
00:35:32,240 --> 00:35:36,960
regression model and figured out the

871
00:35:34,480 --> 00:35:38,960
right weight for um the L2

872
00:35:36,960 --> 00:35:40,800
regularization so that the train loss

873
00:35:38,960 --> 00:35:42,880
and the validation loss were about the

874
00:35:40,800 --> 00:35:44,240
same but if we kept training we could

875
00:35:42,880 --> 00:35:47,119
keep training but then we knew we were

876
00:35:44,240 --> 00:35:48,640
overfitting so I think reinforcement

877
00:35:47,119 --> 00:35:51,119
learning in this context would just work

878
00:35:48,640 --> 00:35:54,680
like by going into the overfitting uh

879
00:35:51,119 --> 00:35:54,680
regime so

880
00:35:57,200 --> 00:36:03,800
yeah You're welcome. All right. Thanks,

881
00:35:59,520 --> 00:36:03,800
Ben. Appreciate the primer.

882
00:36:04,320 --> 00:36:10,160
>> So, now it's my pleasure to introduce uh

883
00:36:07,440 --> 00:36:12,800
Nick Pelosi. I'm saying it correctly.

884
00:36:10,160 --> 00:36:13,920
>> Pity, whatever you want. There's no way

885
00:36:12,800 --> 00:36:17,200
to no way to say it.

886
00:36:13,920 --> 00:36:19,839
>> Okay. So, let me introduce Nick. Um Nick

887
00:36:17,200 --> 00:36:22,160
is an assistant professor of biological

888
00:36:19,839 --> 00:36:25,359
chemistry and molecular pharmacology in

889
00:36:22,160 --> 00:36:28,079
Dana Farber Cancer Institute. Um Nick

890
00:36:25,359 --> 00:36:30,240
has received uh his PhD from the

891
00:36:28,079 --> 00:36:33,839
department of biochemistry at Duke

892
00:36:30,240 --> 00:36:37,440
University in 2016 where he worked on

893
00:36:33,839 --> 00:36:39,839
theory and time resolved spectroscopic

894
00:36:37,440 --> 00:36:43,680
measurements of biological and

895
00:36:39,839 --> 00:36:46,000
abiological electron transfer reactions.

896
00:36:43,680 --> 00:36:49,680
Through this work he became interested

897
00:36:46,000 --> 00:36:51,920
in denovo protein design and uh his uh

898
00:36:49,680 --> 00:36:54,079
and did his post-doal studies in the

899
00:36:51,920 --> 00:36:55,839
laboratory of um University of

900
00:36:54,079 --> 00:36:58,880
California San Francisco where he

901
00:36:55,839 --> 00:37:02,000
focused on developing new approaches for

902
00:36:58,880 --> 00:37:04,960
accurate pro protein design. So today

903
00:37:02,000 --> 00:37:07,440
Nick will present AF2 bind which is an

904
00:37:04,960 --> 00:37:09,599
exciting method. I hope you all enjoy it

905
00:37:07,440 --> 00:37:12,240
and with that I'll hand over to Nick.

906
00:37:09,599 --> 00:37:14,640
>> Okay thanks Maya. Uh so thanks for the

907
00:37:12,240 --> 00:37:17,119
invitation to talk about some of our

908
00:37:14,640 --> 00:37:19,920
work that we've been doing in the lab.

909
00:37:17,119 --> 00:37:23,280
Um and I want to thank Ben Fry for

910
00:37:19,920 --> 00:37:26,160
giving an excellent primer. Uh Ben is a

911
00:37:23,280 --> 00:37:29,599
a superstar in deep learning and machine

912
00:37:26,160 --> 00:37:31,680
learning as it uh you know as it uh um

913
00:37:29,599 --> 00:37:36,000
involves protein design. So be on the

914
00:37:31,680 --> 00:37:38,400
lookout for Ben. Um, and so I guess what

915
00:37:36,000 --> 00:37:40,000
I what I want to first do is is uh give

916
00:37:38,400 --> 00:37:43,040
a shout out to people who have done the

917
00:37:40,000 --> 00:37:46,480
work and uh and just describe the main

918
00:37:43,040 --> 00:37:49,040
question that that the work is probing.

919
00:37:46,480 --> 00:37:51,920
Um the big the big question is can

920
00:37:49,040 --> 00:37:53,839
pre-trained neural networks are they

921
00:37:51,920 --> 00:37:56,480
good at identifying small molecule

922
00:37:53,839 --> 00:37:58,480
binding sites? That's the question. Um

923
00:37:56,480 --> 00:38:00,800
we trained a model called AF2bind which

924
00:37:58,480 --> 00:38:05,119
I'll talk about but um on the bottom

925
00:38:00,800 --> 00:38:08,320
here is the mly crew uh of collaborators

926
00:38:05,119 --> 00:38:11,760
uh that did all the work um to train the

927
00:38:08,320 --> 00:38:15,520
model uh and iterate and so Sergey of

928
00:38:11,760 --> 00:38:17,839
Chinikov a lot of people know um he's at

929
00:38:15,520 --> 00:38:20,640
MIT and RTM

930
00:38:17,839 --> 00:38:23,520
uh was in in Sergey's lab at the time

931
00:38:20,640 --> 00:38:26,800
and now is in Debbie Marx's group um and

932
00:38:23,520 --> 00:38:28,960
RTM uh is responsible for the online

933
00:38:26,800 --> 00:38:31,760
interactive database which he has to

934
00:38:28,960 --> 00:38:33,520
turn back online apparently. Um, and

935
00:38:31,760 --> 00:38:35,440
then Anna and Jody are in my group and

936
00:38:33,520 --> 00:38:37,200
Casper is actually in Bruno Koreah's

937
00:38:35,440 --> 00:38:39,839
group and he was working on this a

938
00:38:37,200 --> 00:38:41,520
little bit when when Bruno was at

939
00:38:39,839 --> 00:38:43,920
Harvard for a sabatical a couple years

940
00:38:41,520 --> 00:38:45,680
ago. Okay, so that's the Mley crew. I'll

941
00:38:43,920 --> 00:38:49,280
thank them again later, but those are

942
00:38:45,680 --> 00:38:50,480
the the faces behind the work. Um, so I

943
00:38:49,280 --> 00:38:52,160
want to start with a little bit of

944
00:38:50,480 --> 00:38:54,800
motivation, a little bit of high level

945
00:38:52,160 --> 00:38:56,400
why do we want to do this at all? Um,

946
00:38:54,800 --> 00:38:58,480
and I think Ben did a good job at that

947
00:38:56,400 --> 00:39:01,440
already. But, but this idea that

948
00:38:58,480 --> 00:39:03,119
proteins bind almost anything. Uh, here

949
00:39:01,440 --> 00:39:06,320
I'm just showing a few proteins with

950
00:39:03,119 --> 00:39:08,480
their lians in purple. Um, and and

951
00:39:06,320 --> 00:39:10,960
binding is intimately tied to protein

952
00:39:08,480 --> 00:39:12,640
function. In a lot of cases, if you know

953
00:39:10,960 --> 00:39:15,520
a protein bind something, you might know

954
00:39:12,640 --> 00:39:17,040
what its function is. Um, and it

955
00:39:15,520 --> 00:39:20,480
basically underpins all of drug

956
00:39:17,040 --> 00:39:22,160
discovery, small molecule binding. Um,

957
00:39:20,480 --> 00:39:24,800
and so accurate prediction of binding

958
00:39:22,160 --> 00:39:27,440
sites. um you know if you knew where a

959
00:39:24,800 --> 00:39:28,480
small molecule pocket would be um you'd

960
00:39:27,440 --> 00:39:31,440
know where to target in a

961
00:39:28,480 --> 00:39:33,359
structure-based drug design program um

962
00:39:31,440 --> 00:39:34,880
and you might actually uh it might

963
00:39:33,359 --> 00:39:36,720
actually help to interpret disease

964
00:39:34,880 --> 00:39:39,760
mutations but you know for that we would

965
00:39:36,720 --> 00:39:43,280
need experts like Hillary to do um to do

966
00:39:39,760 --> 00:39:46,720
that that analysis later um so there are

967
00:39:43,280 --> 00:39:48,320
some a few ways that that uh the

968
00:39:46,720 --> 00:39:50,960
prediction of binding pockets because

969
00:39:48,320 --> 00:39:54,240
it's so important there are a lot of

970
00:39:50,960 --> 00:39:57,280
existing methods that do this um and

971
00:39:54,240 --> 00:39:59,280
that don't use neural networks to do it.

972
00:39:57,280 --> 00:40:01,599
Uh and so here's one that might be the

973
00:39:59,280 --> 00:40:04,160
most popular method for predicting

974
00:40:01,599 --> 00:40:06,560
binding pockets. This is actually

975
00:40:04,160 --> 00:40:08,560
adopted by the PDB. If you want to look

976
00:40:06,560 --> 00:40:13,119
at binding pockets in the PDB, it uses

977
00:40:08,560 --> 00:40:14,560
P2 rank. P2 rank takes a structure and

978
00:40:13,119 --> 00:40:16,560
um there's no there's no neural network

979
00:40:14,560 --> 00:40:18,160
here, but it takes the solvent

980
00:40:16,560 --> 00:40:19,680
accessible surface of a protein

981
00:40:18,160 --> 00:40:22,320
structure and it puts little probes

982
00:40:19,680 --> 00:40:24,400
around the spheres in green here.

983
00:40:22,320 --> 00:40:27,200
Um, and then there's a random forest

984
00:40:24,400 --> 00:40:30,320
classifier that classifies each one of

985
00:40:27,200 --> 00:40:32,400
these spheres according to if it's

986
00:40:30,320 --> 00:40:35,040
druggable or not. You know, what amino

987
00:40:32,400 --> 00:40:37,440
acids are around it? What's its what's

988
00:40:35,040 --> 00:40:42,480
its burial with respect to the protein

989
00:40:37,440 --> 00:40:44,880
surface? 40 or so features um and random

990
00:40:42,480 --> 00:40:47,440
force classifier that that takes those

991
00:40:44,880 --> 00:40:49,359
features as well as the density of those

992
00:40:47,440 --> 00:40:51,200
probes and says this is a pocket or not.

993
00:40:49,359 --> 00:40:54,000
Maybe even this is a druggable pocket.

994
00:40:51,200 --> 00:40:56,400
So it works really pretty well uh given

995
00:40:54,000 --> 00:40:57,839
a a structure. Um so one one question

996
00:40:56,400 --> 00:40:59,839
would be why would we want neural

997
00:40:57,839 --> 00:41:03,280
networks? Um and I'll hope to answer

998
00:40:59,839 --> 00:41:05,280
that in a in a couple of slides. Um

999
00:41:03,280 --> 00:41:07,280
another method is to use homology

1000
00:41:05,280 --> 00:41:09,599
modeling is just to say well this

1001
00:41:07,280 --> 00:41:11,280
protein is similar to this other protein

1002
00:41:09,599 --> 00:41:14,319
and I know this other protein binds

1003
00:41:11,280 --> 00:41:16,480
molecule X and this protein has a

1004
00:41:14,319 --> 00:41:18,079
similar pocket. So therefore it's a

1005
00:41:16,480 --> 00:41:19,520
pretty good guess that those residues

1006
00:41:18,079 --> 00:41:21,440
are binding residues and it's probably

1007
00:41:19,520 --> 00:41:25,839
binding a similar molecule. And when

1008
00:41:21,440 --> 00:41:28,240
AlphaFold 2 came out um a group actually

1009
00:41:25,839 --> 00:41:30,640
uh um use this approach of homology

1010
00:41:28,240 --> 00:41:33,520
modeling to transfer

1011
00:41:30,640 --> 00:41:36,160
lians found in proteins in the PDB in

1012
00:41:33,520 --> 00:41:38,000
the protein data bank to alphold

1013
00:41:36,160 --> 00:41:39,839
predicted structures of proteins that

1014
00:41:38,000 --> 00:41:41,520
aren't in the PDB but they're very

1015
00:41:39,839 --> 00:41:44,400
closely related. They had some criteria

1016
00:41:41,520 --> 00:41:46,960
of what is closely related. Um here's an

1017
00:41:44,400 --> 00:41:50,400
example of of a protein. I think this is

1018
00:41:46,960 --> 00:41:52,319
just myoglobin. Um and the alphafold

1019
00:41:50,400 --> 00:41:56,000
predicted structures on the first two

1020
00:41:52,319 --> 00:41:57,839
panels uh on the left. And then um they

1021
00:41:56,000 --> 00:42:00,400
could because there's so many structures

1022
00:41:57,839 --> 00:42:02,079
of myoglobin and PDB boundahem. They

1023
00:42:00,400 --> 00:42:03,839
could just superimpose one of those

1024
00:42:02,079 --> 00:42:06,079
structures onto the alpha predicted

1025
00:42:03,839 --> 00:42:08,319
structure and say, "Hey, this lian

1026
00:42:06,079 --> 00:42:10,800
that's in the crystal structure looks

1027
00:42:08,319 --> 00:42:13,119
like it should still fit into this

1028
00:42:10,800 --> 00:42:15,680
protein." And uh so we'll just transfer

1029
00:42:13,119 --> 00:42:20,160
that over. And it was one point I want

1030
00:42:15,680 --> 00:42:22,319
to make here is that alphafold 2 and Ben

1031
00:42:20,160 --> 00:42:25,599
made this point I think too. Alphold 2

1032
00:42:22,319 --> 00:42:27,440
wasn't trained at all on small molecule

1033
00:42:25,599 --> 00:42:30,319
lians. Of course alphaold 3 was and

1034
00:42:27,440 --> 00:42:32,800
other models like bolts and um and and

1035
00:42:30,319 --> 00:42:34,000
openfold are are were trained on on

1036
00:42:32,800 --> 00:42:35,839
small molecule lians as well. But

1037
00:42:34,000 --> 00:42:40,079
alphold 2 is not. All the lians were

1038
00:42:35,839 --> 00:42:42,400
stripped from the protein structures.

1039
00:42:40,079 --> 00:42:44,640
you just had protein there. But the task

1040
00:42:42,400 --> 00:42:47,440
for prediction was to predict the

1041
00:42:44,640 --> 00:42:49,760
structure of in this case the ligan

1042
00:42:47,440 --> 00:42:52,400
bound form with the side chain

1043
00:42:49,760 --> 00:42:54,240
confirmers that are in those

1044
00:42:52,400 --> 00:42:56,800
confirmations sometimes because the lian

1045
00:42:54,240 --> 00:42:58,480
is bound. And so under the hood,

1046
00:42:56,800 --> 00:43:00,000
alphafold should be able, I think, to

1047
00:42:58,480 --> 00:43:01,839
infer

1048
00:43:00,000 --> 00:43:04,480
small molecule binding confirmations

1049
00:43:01,839 --> 00:43:07,440
even without explicitly considering a

1050
00:43:04,480 --> 00:43:09,040
small molecule and and alphafill is an

1051
00:43:07,440 --> 00:43:11,440
example of this that that there's some

1052
00:43:09,040 --> 00:43:13,200
success there at transferring over um

1053
00:43:11,440 --> 00:43:14,640
small molecule binding sites. The

1054
00:43:13,200 --> 00:43:16,319
example here in the middle panel is that

1055
00:43:14,640 --> 00:43:17,599
these two histadines are in this

1056
00:43:16,319 --> 00:43:20,079
configuration so that they can

1057
00:43:17,599 --> 00:43:23,040
coordinate the metal and he alphafold

1058
00:43:20,079 --> 00:43:25,119
doesn't know about he explicitly in this

1059
00:43:23,040 --> 00:43:27,440
prediction. Okay, so that's that's

1060
00:43:25,119 --> 00:43:28,800
another way homology modeling. Um, and

1061
00:43:27,440 --> 00:43:30,240
then there's this question I sort of

1062
00:43:28,800 --> 00:43:31,839
brought up, which is can't neural

1063
00:43:30,240 --> 00:43:35,119
networks already find binding sites

1064
00:43:31,839 --> 00:43:37,760
well? Neural networks like um, alphafold

1065
00:43:35,119 --> 00:43:40,960
3, bolts 2,

1066
00:43:37,760 --> 00:43:43,760
um, other neural networks that handle

1067
00:43:40,960 --> 00:43:47,119
small molecules explicitly. Um, is this

1068
00:43:43,760 --> 00:43:49,280
problem already solved? um and and and

1069
00:43:47,119 --> 00:43:51,599
and what I want to point out. So here

1070
00:43:49,280 --> 00:43:55,119
I'm showing Rosetta all atom which was

1071
00:43:51,599 --> 00:43:56,560
the first neural network to um to get

1072
00:43:55,119 --> 00:43:58,000
published that could handle small

1073
00:43:56,560 --> 00:44:01,839
molecule lians at the same time as

1074
00:43:58,000 --> 00:44:03,760
protein structure and predicting it. Um

1075
00:44:01,839 --> 00:44:07,440
and I want to give you a little example

1076
00:44:03,760 --> 00:44:10,240
of of um some of the potential pitfalls

1077
00:44:07,440 --> 00:44:11,839
of always trusting these kinds of models

1078
00:44:10,240 --> 00:44:13,839
for for figuring out where a small

1079
00:44:11,839 --> 00:44:16,800
molecule might go and and how it might

1080
00:44:13,839 --> 00:44:18,319
bind. Um, these are great models, by the

1081
00:44:16,800 --> 00:44:20,319
way, and they're very useful, but it's I

1082
00:44:18,319 --> 00:44:22,240
think it's worth understanding where

1083
00:44:20,319 --> 00:44:24,720
they might go wrong so that you can

1084
00:44:22,240 --> 00:44:26,640
better use them uh for your own research

1085
00:44:24,720 --> 00:44:28,960
problems. So, here's an this is an

1086
00:44:26,640 --> 00:44:30,400
example straight out of their paper. And

1087
00:44:28,960 --> 00:44:33,200
this said, look, we trained our model

1088
00:44:30,400 --> 00:44:36,560
and we had a test set and we tested our

1089
00:44:33,200 --> 00:44:40,400
model on the test set and this is an

1090
00:44:36,560 --> 00:44:42,079
example of a protein from the test set

1091
00:44:40,400 --> 00:44:43,920
that's that's claimed to be completely

1092
00:44:42,079 --> 00:44:47,520
outside the training distribution of the

1093
00:44:43,920 --> 00:44:51,440
model for Zetapodal atom model. Um the

1094
00:44:47,520 --> 00:44:53,760
the technical claim is that the closest

1095
00:44:51,440 --> 00:44:56,880
protein sequence to the training set is

1096
00:44:53,760 --> 00:45:00,319
25% sequence identity. So that's you

1097
00:44:56,880 --> 00:45:02,160
know not not bad. Um the ligan was in

1098
00:45:00,319 --> 00:45:07,280
the training set. So this is I think a

1099
00:45:02,160 --> 00:45:10,880
flavven adinine um an FAD molecule um or

1100
00:45:07,280 --> 00:45:13,200
an FM f FMN molecule. And uh and so they

1101
00:45:10,880 --> 00:45:15,920
said oh look um this but our prediction

1102
00:45:13,200 --> 00:45:19,040
is spoton. So the so the the crystal

1103
00:45:15,920 --> 00:45:21,040
structure is in um white and the

1104
00:45:19,040 --> 00:45:22,720
prediction is in cyan and they look

1105
00:45:21,040 --> 00:45:23,920
indistinguishable. So the model did such

1106
00:45:22,720 --> 00:45:26,880
a good job even though this protein

1107
00:45:23,920 --> 00:45:29,040
wasn't in the training set.

1108
00:45:26,880 --> 00:45:31,520
Um but it's it's quite easy actually to

1109
00:45:29,040 --> 00:45:33,119
just search for other proteins to this

1110
00:45:31,520 --> 00:45:36,480
this protein that's outside the training

1111
00:45:33,119 --> 00:45:40,240
and look at proteins in the training set

1112
00:45:36,480 --> 00:45:42,640
um that that have that are less than 25%

1113
00:45:40,240 --> 00:45:44,560
identity but they still have identical

1114
00:45:42,640 --> 00:45:47,200
structures basically and identical

1115
00:45:44,560 --> 00:45:48,800
binding pockets. And so that cyan

1116
00:45:47,200 --> 00:45:51,040
protein that I'm or the the purple

1117
00:45:48,800 --> 00:45:53,760
protein that I'm showing you here is a

1118
00:45:51,040 --> 00:45:57,920
protein that is only 23% sequence

1119
00:45:53,760 --> 00:45:59,760
identity to that protein above. And yet

1120
00:45:57,920 --> 00:46:02,000
um I can overlay the structures which I

1121
00:45:59,760 --> 00:46:03,760
just did and they look very similar. And

1122
00:46:02,000 --> 00:46:06,160
the point here and even if you look at

1123
00:46:03,760 --> 00:46:07,839
the the pocket and and the um way the

1124
00:46:06,160 --> 00:46:10,400
ligan is bound is pretty much identical.

1125
00:46:07,839 --> 00:46:12,640
So so if you had a threshold of 25%

1126
00:46:10,400 --> 00:46:15,920
sequence identity means that there's no

1127
00:46:12,640 --> 00:46:18,000
data leakage. Think again. This was 23%

1128
00:46:15,920 --> 00:46:20,560
sequence identity. The point I want to

1129
00:46:18,000 --> 00:46:23,440
make here is that binding is so

1130
00:46:20,560 --> 00:46:25,839
important to protein function

1131
00:46:23,440 --> 00:46:27,599
that it is one of the parts of the

1132
00:46:25,839 --> 00:46:30,319
protein that is most conserved across

1133
00:46:27,599 --> 00:46:32,800
evolution. So the rest of the sequence

1134
00:46:30,319 --> 00:46:35,119
it can diverge and you can diverge and

1135
00:46:32,800 --> 00:46:36,960
still maintain those same structure but

1136
00:46:35,119 --> 00:46:38,960
oftent times the binding pocket doesn't

1137
00:46:36,960 --> 00:46:40,800
diverge. And so if you're considering

1138
00:46:38,960 --> 00:46:42,720
sequence identity of a whole protein as

1139
00:46:40,800 --> 00:46:45,680
your criterion for is a binding pocket

1140
00:46:42,720 --> 00:46:48,240
the same or not, you can run into issues

1141
00:46:45,680 --> 00:46:49,920
like this where you get data leakage and

1142
00:46:48,240 --> 00:46:51,520
you might think that your model is quote

1143
00:46:49,920 --> 00:46:53,520
unquote generalizing, but it's really

1144
00:46:51,520 --> 00:46:55,440
just memorizing.

1145
00:46:53,520 --> 00:46:56,800
Okay, so that that's one caveat. Of

1146
00:46:55,440 --> 00:46:58,800
course, these models are quite useful

1147
00:46:56,800 --> 00:47:01,119
still. Um, but if you want to make the

1148
00:46:58,800 --> 00:47:05,040
claim that the model is generalizing and

1149
00:47:01,119 --> 00:47:08,720
predicting denovo sites, um, it's hard

1150
00:47:05,040 --> 00:47:10,400
to, uh, um, to make that claim basically

1151
00:47:08,720 --> 00:47:13,520
without retraining the model. Here's the

1152
00:47:10,400 --> 00:47:15,280
point. I'm I'm going to, uh, uh, ask let

1153
00:47:13,520 --> 00:47:18,160
you ask your question in one second. Um,

1154
00:47:15,280 --> 00:47:20,079
but the point here is that it's hard to

1155
00:47:18,160 --> 00:47:24,560
know about these models like Alfold 3

1156
00:47:20,079 --> 00:47:26,800
and RFA and Boltz and how um, and how

1157
00:47:24,560 --> 00:47:30,319
well they generalize. So being able to

1158
00:47:26,800 --> 00:47:33,280
predict pockets and bind small molecules

1159
00:47:30,319 --> 00:47:35,680
and dock them accurately um if you can't

1160
00:47:33,280 --> 00:47:38,319
retrain them on a really rigorous data

1161
00:47:35,680 --> 00:47:40,160
split because a lot of those splits are

1162
00:47:38,319 --> 00:47:42,079
not extremely rigorous in term for this

1163
00:47:40,160 --> 00:47:44,720
task like it I'm just emphasizing that

1164
00:47:42,079 --> 00:47:47,680
this particular task you need to be very

1165
00:47:44,720 --> 00:47:50,720
rigorous I think about how you split um

1166
00:47:47,680 --> 00:47:52,960
and so this is the meme here is uh you

1167
00:47:50,720 --> 00:47:55,440
know training data test data they're the

1168
00:47:52,960 --> 00:47:57,119
same thing for a lot of these models um

1169
00:47:55,440 --> 00:47:58,319
for this particular task and there was

1170
00:47:57,119 --> 00:47:58,800
there was a question.

1171
00:47:58,319 --> 00:48:01,119
>> Yeah.

1172
00:47:58,800 --> 00:48:04,240
>> Yeah. Uh exactly great great

1173
00:48:01,119 --> 00:48:06,240
illustration just so why has the field

1174
00:48:04,240 --> 00:48:07,359
not figured out how to deal with this? I

1175
00:48:06,240 --> 00:48:09,040
mean you have like

1176
00:48:07,359 --> 00:48:11,040
>> it's really well understood in

1177
00:48:09,040 --> 00:48:13,520
biochemistry and biohysics that there's

1178
00:48:11,040 --> 00:48:15,119
convergent evolution and yada yada and

1179
00:48:13,520 --> 00:48:17,839
and somehow

1180
00:48:15,119 --> 00:48:20,640
>> the machine learning people still make

1181
00:48:17,839 --> 00:48:22,800
these really naive train test splits and

1182
00:48:20,640 --> 00:48:23,359
it keeps it's every paper and it's all

1183
00:48:22,800 --> 00:48:23,839
these

1184
00:48:23,359 --> 00:48:26,079
>> oh yeah

1185
00:48:23,839 --> 00:48:28,240
>> really good journals and you know people

1186
00:48:26,079 --> 00:48:29,119
acknowledge the problem but there's no

1187
00:48:28,240 --> 00:48:30,640
like

1188
00:48:29,119 --> 00:48:32,160
>> why hasn't the field kind of put forward

1189
00:48:30,640 --> 00:48:32,880
a systematic framework about how to do

1190
00:48:32,160 --> 00:48:35,839
train testing?

1191
00:48:32,880 --> 00:48:37,599
>> Yeah. Yeah, it's a good question and um

1192
00:48:35,839 --> 00:48:39,280
some people are, you know, some people

1193
00:48:37,599 --> 00:48:41,040
have recognized this and are trying to

1194
00:48:39,280 --> 00:48:44,559
make very rigorous data splits for this

1195
00:48:41,040 --> 00:48:45,839
task. Um I won't call out anybody by

1196
00:48:44,559 --> 00:48:47,680
name explicitly, but there's a few

1197
00:48:45,839 --> 00:48:49,839
groups that I know of in industry as

1198
00:48:47,680 --> 00:48:51,839
well as academia that are really trying

1199
00:48:49,839 --> 00:48:53,839
to do this well and recognize the

1200
00:48:51,839 --> 00:48:55,520
problem. Um but why is it still

1201
00:48:53,839 --> 00:48:59,040
prevalent? I think it's because it's

1202
00:48:55,520 --> 00:49:00,880
easy to split in different ways that are

1203
00:48:59,040 --> 00:49:03,040
e like just by sequence identity. It's

1204
00:49:00,880 --> 00:49:05,200
very easy to do that. people have done

1205
00:49:03,040 --> 00:49:08,240
it before. It's been published. They got

1206
00:49:05,200 --> 00:49:11,920
their paper accepted. They did that. Um,

1207
00:49:08,240 --> 00:49:15,359
and so it's very I'll show you in the

1208
00:49:11,920 --> 00:49:17,119
next couple of slides um how much work

1209
00:49:15,359 --> 00:49:19,440
it actually does take to make a rigorous

1210
00:49:17,119 --> 00:49:23,359
data split. And oftentimes making the

1211
00:49:19,440 --> 00:49:27,040
data split is, as I tell Ben, sometimes

1212
00:49:23,359 --> 00:49:30,559
uh it's it's thankless work. um you're

1213
00:49:27,040 --> 00:49:33,839
the unsung hero of the work if you make

1214
00:49:30,559 --> 00:49:35,760
a really good data split for training

1215
00:49:33,839 --> 00:49:38,640
but no but you don't really get the

1216
00:49:35,760 --> 00:49:41,040
medal uh because you know if you haven't

1217
00:49:38,640 --> 00:49:42,720
train the model yourself so

1218
00:49:41,040 --> 00:49:44,640
>> disincentivized because when you make

1219
00:49:42,720 --> 00:49:45,760
the split well your model performance

1220
00:49:44,640 --> 00:49:46,240
just crashes right

1221
00:49:45,760 --> 00:49:48,079
>> exactly

1222
00:49:46,240 --> 00:49:49,760
>> and then people write papers after the

1223
00:49:48,079 --> 00:49:51,599
fact where they're like oh we went back

1224
00:49:49,760 --> 00:49:53,520
to these models did the split correctly

1225
00:49:51,599 --> 00:49:55,920
and now they have no performance

1226
00:49:53,520 --> 00:49:58,160
>> so there's also bad incentive ex That's

1227
00:49:55,920 --> 00:50:01,359
a really good point is that um oftent

1228
00:49:58,160 --> 00:50:02,960
times there's uh competition for if I

1229
00:50:01,359 --> 00:50:06,000
publish my model it has to do better

1230
00:50:02,960 --> 00:50:07,920
than the previous model and um the

1231
00:50:06,000 --> 00:50:10,880
previous model was tested on a split

1232
00:50:07,920 --> 00:50:12,960
that had data leakage and your if you're

1233
00:50:10,880 --> 00:50:14,720
very rigorous about how you train your

1234
00:50:12,960 --> 00:50:16,800
model and do your split then your model

1235
00:50:14,720 --> 00:50:19,680
will perform likely worse than the one

1236
00:50:16,800 --> 00:50:21,119
that was cheating in a sense and um so

1237
00:50:19,680 --> 00:50:23,440
that does lead to saying well I'm just

1238
00:50:21,119 --> 00:50:25,599
going to do it on this other split and

1239
00:50:23,440 --> 00:50:27,440
see if I do a little bit better but how

1240
00:50:25,599 --> 00:50:29,920
but it but that leads to the problem of

1241
00:50:27,440 --> 00:50:32,000
how why am I doing better? Is it because

1242
00:50:29,920 --> 00:50:34,720
I'm cheating better or because my model

1243
00:50:32,000 --> 00:50:38,079
is generalizing better and and and this

1244
00:50:34,720 --> 00:50:40,400
is really the a hard problem because um

1245
00:50:38,079 --> 00:50:41,920
it's expensive to retrain models

1246
00:50:40,400 --> 00:50:44,400
especially the protein structure

1247
00:50:41,920 --> 00:50:46,960
prediction models. Not every academic

1248
00:50:44,400 --> 00:50:49,680
lab has access to

1249
00:50:46,960 --> 00:50:51,839
uh thousand H200s

1250
00:50:49,680 --> 00:50:54,000
and half a million dollars to retrain

1251
00:50:51,839 --> 00:50:56,960
the models and then two months of time.

1252
00:50:54,000 --> 00:50:58,240
Right? So um so so how do you do that?

1253
00:50:56,960 --> 00:51:00,240
And so that's actually one of the

1254
00:50:58,240 --> 00:51:04,920
motivations for what we did with AF2

1255
00:51:00,240 --> 00:51:04,920
buying. Um yeah, another question.

1256
00:51:05,040 --> 00:51:10,720
>> Sorry, can I have a very pedestrian

1257
00:51:07,359 --> 00:51:14,400
question for the previous slide? Uh

1258
00:51:10,720 --> 00:51:17,680
>> where you show 23% sequence identity. So

1259
00:51:14,400 --> 00:51:19,599
23% sequent identity in this case does

1260
00:51:17,680 --> 00:51:22,319
it mean that there is like a match every

1261
00:51:19,599 --> 00:51:24,880
fourth amino acid or there is actually a

1262
00:51:22,319 --> 00:51:27,599
stretch of matches that correspond to

1263
00:51:24,880 --> 00:51:30,079
this binding site between two proteins.

1264
00:51:27,599 --> 00:51:32,480
>> Um usually what you'll see is that a lot

1265
00:51:30,079 --> 00:51:34,880
of the surface residues are very

1266
00:51:32,480 --> 00:51:36,160
different. Um and it's usually how

1267
00:51:34,880 --> 00:51:38,640
proteins evolve. If you look at two

1268
00:51:36,160 --> 00:51:41,440
proteins in the same family, um they

1269
00:51:38,640 --> 00:51:43,520
might have a very similar core packing

1270
00:51:41,440 --> 00:51:45,440
uh and and the least conserved residues

1271
00:51:43,520 --> 00:51:47,280
are all on the surface of the protein,

1272
00:51:45,440 --> 00:51:48,400
but the most conserved residues are the

1273
00:51:47,280 --> 00:51:51,520
ones that are involved directly in

1274
00:51:48,400 --> 00:51:53,599
binding liant usually. And so and so in

1275
00:51:51,520 --> 00:51:55,440
this case, it's not every four amino

1276
00:51:53,599 --> 00:51:57,920
acids are conserved. It would be that

1277
00:51:55,440 --> 00:52:00,319
the binding pocket in for these two

1278
00:51:57,920 --> 00:52:03,680
proteins look very similar. And as you

1279
00:52:00,319 --> 00:52:06,400
move out to the outside of the onion um

1280
00:52:03,680 --> 00:52:08,400
the the residues get more different

1281
00:52:06,400 --> 00:52:11,119
between the two. So that's how one can

1282
00:52:08,400 --> 00:52:12,480
it's not a uniform uh metric. So you can

1283
00:52:11,119 --> 00:52:15,119
fool yourself.

1284
00:52:12,480 --> 00:52:18,160
>> Another quick question here is that um

1285
00:52:15,119 --> 00:52:20,160
is the conservation across species or we

1286
00:52:18,160 --> 00:52:22,400
are talking also about conservation

1287
00:52:20,160 --> 00:52:23,680
across paralogs because in small

1288
00:52:22,400 --> 00:52:27,839
molecule

1289
00:52:23,680 --> 00:52:28,640
>> field we actually want not so conserved

1290
00:52:27,839 --> 00:52:30,800
pocket.

1291
00:52:28,640 --> 00:52:32,720
>> Yeah. Yeah. Um you can get a lot of

1292
00:52:30,800 --> 00:52:35,200
conservation even across paralogues

1293
00:52:32,720 --> 00:52:38,160
different I mean species and paralogues

1294
00:52:35,200 --> 00:52:39,599
within a species. Um

1295
00:52:38,160 --> 00:52:42,480
yeah, I mean the thing that you

1296
00:52:39,599 --> 00:52:44,240
mentioned is is uh you know an important

1297
00:52:42,480 --> 00:52:47,599
one is that and that's one of the

1298
00:52:44,240 --> 00:52:49,680
problems with um trying to for drug

1299
00:52:47,599 --> 00:52:52,640
discovery to get a specific molecule

1300
00:52:49,680 --> 00:52:55,440
that'll bind to a specific target is

1301
00:52:52,640 --> 00:52:58,079
that the binding sites are so conserved

1302
00:52:55,440 --> 00:52:59,359
across different proteins paralogues of

1303
00:52:58,079 --> 00:53:03,359
you know the same species humans for

1304
00:52:59,359 --> 00:53:05,280
example um and that trying to design a

1305
00:53:03,359 --> 00:53:08,240
small molecule that binds to this pocket

1306
00:53:05,280 --> 00:53:10,000
and not that pocket is very challenging

1307
00:53:08,240 --> 00:53:11,280
because those those pockets are so the

1308
00:53:10,000 --> 00:53:14,160
residues in those pockets are so

1309
00:53:11,280 --> 00:53:16,800
conserved. Um and so but there it can be

1310
00:53:14,160 --> 00:53:18,480
one amino acid two amino acid difference

1311
00:53:16,800 --> 00:53:21,359
and that's what drug that's what you

1312
00:53:18,480 --> 00:53:23,839
know medicinal chemists and uh try to

1313
00:53:21,359 --> 00:53:25,760
take advantage of for specificity but

1314
00:53:23,839 --> 00:53:27,119
you could imagine that you know I mean a

1315
00:53:25,760 --> 00:53:28,800
lot of these neural networks if you

1316
00:53:27,119 --> 00:53:30,960
change one residue in the binding pocket

1317
00:53:28,800 --> 00:53:32,319
and then predict the same lian it'll

1318
00:53:30,960 --> 00:53:34,880
still put the lian there because it's

1319
00:53:32,319 --> 00:53:37,040
it's more of pattern matching than it is

1320
00:53:34,880 --> 00:53:39,680
about understanding really the physics

1321
00:53:37,040 --> 00:53:42,640
behind it. Um so if you classic thing

1322
00:53:39,680 --> 00:53:46,000
would be to do a bump hole you know um

1323
00:53:42,640 --> 00:53:47,200
do a bump hole uh um like you know since

1324
00:53:46,000 --> 00:53:49,520
we're at the broad you know Steuart

1325
00:53:47,200 --> 00:53:54,640
Shriber did bump holes all the time back

1326
00:53:49,520 --> 00:53:56,720
with uh um you know FKBP andrb and the

1327
00:53:54,640 --> 00:53:58,559
rapamy derivatives and to make these

1328
00:53:56,720 --> 00:54:01,440
these analoges very specific for a

1329
00:53:58,559 --> 00:54:04,079
protein you'd take a put a bump on the

1330
00:54:01,440 --> 00:54:05,920
ligant uh so so you have add a

1331
00:54:04,079 --> 00:54:07,520
functional group that would clash with

1332
00:54:05,920 --> 00:54:08,319
the binding pocket in the natural

1333
00:54:07,520 --> 00:54:10,480
protein

1334
00:54:08,319 --> 00:54:12,400
but then mutate the natural protein to

1335
00:54:10,480 --> 00:54:15,359
so that that residue that it would clash

1336
00:54:12,400 --> 00:54:17,599
with is now a smaller residue and now it

1337
00:54:15,359 --> 00:54:20,240
can fit. And so now your new lian can

1338
00:54:17,599 --> 00:54:22,480
fit into a a new pocket that's only one

1339
00:54:20,240 --> 00:54:25,200
amino acid different, but it allows for

1340
00:54:22,480 --> 00:54:27,280
specificity. If you did that for a

1341
00:54:25,200 --> 00:54:31,599
neural network and you took the bumped

1342
00:54:27,280 --> 00:54:33,119
lian and still predicted it with the

1343
00:54:31,599 --> 00:54:34,400
wild type pocket, it'll still put the

1344
00:54:33,119 --> 00:54:36,400
lian there, but it shouldn't bind. You

1345
00:54:34,400 --> 00:54:38,400
know, physically it should bind. So, um,

1346
00:54:36,400 --> 00:54:41,040
so that's that's one important thing to

1347
00:54:38,400 --> 00:54:42,720
note. Um, yeah. So, but the but the goal

1348
00:54:41,040 --> 00:54:44,400
of being able to find pockets that

1349
00:54:42,720 --> 00:54:46,559
aren't so conserved for drug discovery

1350
00:54:44,400 --> 00:54:49,440
is a good goal. And I'll get to that, I

1351
00:54:46,559 --> 00:54:52,319
think, a little bit, right? So, training

1352
00:54:49,440 --> 00:54:55,119
data, test data, um, you know, they're

1353
00:54:52,319 --> 00:54:56,800
the same picture. Good. So, how so how

1354
00:54:55,119 --> 00:54:59,119
did we go about doing this? Because our

1355
00:54:56,800 --> 00:55:02,800
motivation was to to actually train a

1356
00:54:59,119 --> 00:55:06,400
model that would be we would have

1357
00:55:02,800 --> 00:55:08,480
confidence deploying at scale to unknown

1358
00:55:06,400 --> 00:55:10,800
proteins and we want to be able to

1359
00:55:08,480 --> 00:55:14,079
predict denovo pockets. Denovo pockets

1360
00:55:10,800 --> 00:55:15,839
meaning ones that um you know alpha fill

1361
00:55:14,079 --> 00:55:17,440
wouldn't be able to predict by homology

1362
00:55:15,839 --> 00:55:19,599
simply for example. We want to be able

1363
00:55:17,440 --> 00:55:23,760
to know are there pockets in this

1364
00:55:19,599 --> 00:55:26,319
protein that um our model uh that we've

1365
00:55:23,760 --> 00:55:27,680
trained can pick up about features for

1366
00:55:26,319 --> 00:55:29,599
those pockets as opposed to just

1367
00:55:27,680 --> 00:55:31,440
cheating because that pocket was seen in

1368
00:55:29,599 --> 00:55:33,359
training. Okay. So we had to be very

1369
00:55:31,440 --> 00:55:35,280
rigorous about this. So Sergey and I

1370
00:55:33,359 --> 00:55:38,240
worked on this for several months I want

1371
00:55:35,280 --> 00:55:40,880
to say uh it was a we did many many

1372
00:55:38,240 --> 00:55:42,800
iterations of this training uh test data

1373
00:55:40,880 --> 00:55:44,880
set and refinement. I'll take you

1374
00:55:42,800 --> 00:55:46,880
through what we what we had to do. First

1375
00:55:44,880 --> 00:55:49,520
thing we did is we took every protein in

1376
00:55:46,880 --> 00:55:51,359
the PDB. There's about 200 200,000

1377
00:55:49,520 --> 00:55:53,680
proteins in the PDB, a little bit more

1378
00:55:51,359 --> 00:55:54,960
than that, crystal structures, cryom

1379
00:55:53,680 --> 00:55:57,200
structures, things like that. We focused

1380
00:55:54,960 --> 00:55:59,119
on crystal structures here. Um, but we

1381
00:55:57,200 --> 00:56:00,559
took proteins that in this case we we

1382
00:55:59,119 --> 00:56:02,079
whittleled it down to single chain

1383
00:56:00,559 --> 00:56:04,160
proteins

1384
00:56:02,079 --> 00:56:07,280
and uh proteins that were less than 500

1385
00:56:04,160 --> 00:56:08,880
amino acids. Uh, so single chain 500

1386
00:56:07,280 --> 00:56:11,440
amino acid or less. The average size was

1387
00:56:08,880 --> 00:56:12,799
about 300 amino acids. Protein had to be

1388
00:56:11,440 --> 00:56:15,040
greater than 40 amino acids. So, it

1389
00:56:12,799 --> 00:56:17,440
couldn't be a peptide. Um, it had to

1390
00:56:15,040 --> 00:56:20,799
bind one one small molecule liant in the

1391
00:56:17,440 --> 00:56:22,880
crystal structure. Um, and and that

1392
00:56:20,799 --> 00:56:24,559
small molecule couldn't be making

1393
00:56:22,880 --> 00:56:28,000
interactions with another protein in the

1394
00:56:24,559 --> 00:56:30,240
crystal structure or um another copy of

1395
00:56:28,000 --> 00:56:31,760
the same protein in the crystal via

1396
00:56:30,240 --> 00:56:33,359
crystal contact. We really wanted to

1397
00:56:31,760 --> 00:56:36,400
make sure that this small molecule was a

1398
00:56:33,359 --> 00:56:37,680
good binding site. Um, so we did that.

1399
00:56:36,400 --> 00:56:40,240
Uh, so that gives you from the

1400
00:56:37,680 --> 00:56:42,880
200,000ish, it gives you about 14,000

1401
00:56:40,240 --> 00:56:44,559
proteins. Uh we filtered by resolution

1402
00:56:42,880 --> 00:56:47,280
too. It was kind of modest resolution

1403
00:56:44,559 --> 00:56:49,599
like 3.5 angstrom resolution. So we

1404
00:56:47,280 --> 00:56:52,559
didn't we didn't get rid of many. Um and

1405
00:56:49,599 --> 00:56:55,040
then we clustered by here's that here's

1406
00:56:52,559 --> 00:56:57,520
that dirty word again sequence identity.

1407
00:56:55,040 --> 00:57:01,520
We clustered by sequence identity um

1408
00:56:57,520 --> 00:57:03,599
using M62 and that reduced the number of

1409
00:57:01,520 --> 00:57:05,839
um of protein structures to around

1410
00:57:03,599 --> 00:57:07,200
2,000. But we didn't stop there. A lot

1411
00:57:05,839 --> 00:57:09,200
of people would stop there and say we're

1412
00:57:07,200 --> 00:57:12,000
done. That's our that's our set. will

1413
00:57:09,200 --> 00:57:13,280
split the clusters um which are only 30%

1414
00:57:12,000 --> 00:57:16,640
similar. This is how something like

1415
00:57:13,280 --> 00:57:18,240
protein MPNM is trained. Uh split the

1416
00:57:16,640 --> 00:57:21,200
clusters and then that's our trained

1417
00:57:18,240 --> 00:57:23,119
test set. But for small molecule binding

1418
00:57:21,200 --> 00:57:25,040
prediction, we um needed to be a little

1419
00:57:23,119 --> 00:57:27,599
bit more rigorous. And so we did a few

1420
00:57:25,040 --> 00:57:30,960
things. We then took those and and uh

1421
00:57:27,599 --> 00:57:34,160
filtered them by similar families of of

1422
00:57:30,960 --> 00:57:36,400
sequences in PAM and interpro and then

1423
00:57:34,160 --> 00:57:39,280
we did structural similarity by TM align

1424
00:57:36,400 --> 00:57:41,200
and TM score. Uh so we wanted a we

1425
00:57:39,280 --> 00:57:43,920
wanted low structural similarity between

1426
00:57:41,200 --> 00:57:45,200
the the single chain proteins and then

1427
00:57:43,920 --> 00:57:46,559
then from there some of those single

1428
00:57:45,200 --> 00:57:50,079
chain proteins were multi-dommain

1429
00:57:46,559 --> 00:57:52,000
proteins and so we wanted to make sure

1430
00:57:50,079 --> 00:57:53,760
that um but many of them were single

1431
00:57:52,000 --> 00:57:56,319
domain uh we wanted to make sure that

1432
00:57:53,760 --> 00:57:59,839
there were no domains that were even the

1433
00:57:56,319 --> 00:58:02,880
same in our in our data set. Um and then

1434
00:57:59,839 --> 00:58:04,880
finally from that we uh just took the

1435
00:58:02,880 --> 00:58:06,079
pocket residues. You can identify

1436
00:58:04,880 --> 00:58:08,160
residues from the crystal structure

1437
00:58:06,079 --> 00:58:10,480
trivially by just saying these residues

1438
00:58:08,160 --> 00:58:12,240
are within five anstroms of a of a bound

1439
00:58:10,480 --> 00:58:15,040
liant and those are those are the pocket

1440
00:58:12,240 --> 00:58:17,040
residues and you can extract those

1441
00:58:15,040 --> 00:58:18,960
coordinates from the structure and then

1442
00:58:17,040 --> 00:58:21,599
instead of aligning the whole protein

1443
00:58:18,960 --> 00:58:24,319
you can align pockets to one another

1444
00:58:21,599 --> 00:58:25,520
using TM align as well and um and so we

1445
00:58:24,319 --> 00:58:27,520
wanted to make sure that the pockets

1446
00:58:25,520 --> 00:58:28,960
weren't similar and and you can see that

1447
00:58:27,520 --> 00:58:30,000
there would have been some pro if we

1448
00:58:28,960 --> 00:58:31,520
didn't do this there would have been

1449
00:58:30,000 --> 00:58:34,160
some proteins that passed all those

1450
00:58:31,520 --> 00:58:36,480
filters but yet still had similar

1451
00:58:34,160 --> 00:58:38,640
pockets. So, we lost some at the very

1452
00:58:36,480 --> 00:58:41,440
end. But look at look at the number that

1453
00:58:38,640 --> 00:58:45,680
we have at the end. We went from over

1454
00:58:41,440 --> 00:58:48,240
200,000 proteins in the PDB and filtered

1455
00:58:45,680 --> 00:58:51,599
by what we think are all necessary

1456
00:58:48,240 --> 00:58:54,079
criteria for eliminating data leakage

1457
00:58:51,599 --> 00:58:57,280
for this prediction task. And we ended

1458
00:58:54,079 --> 00:58:59,200
up with 200 proteins.

1459
00:58:57,280 --> 00:59:01,839
That's not a lot to train a deep neural

1460
00:58:59,200 --> 00:59:03,040
network, right? 200 data points. I don't

1461
00:59:01,839 --> 00:59:04,160
think anybody in this room would feel

1462
00:59:03,040 --> 00:59:05,839
comfortable trying to train a deep

1463
00:59:04,160 --> 00:59:08,960
neural network from scratch on 200 data

1464
00:59:05,839 --> 00:59:11,119
points. I hope not. Um and so okay, so

1465
00:59:08,960 --> 00:59:13,359
this could be an issue. And so um what

1466
00:59:11,119 --> 00:59:16,720
we did too, so we split that those 200

1467
00:59:13,359 --> 00:59:18,960
structures up into a training set and a

1468
00:59:16,720 --> 00:59:20,720
validation set and a test set. We

1469
00:59:18,960 --> 00:59:23,119
actually split them up into 11 different

1470
00:59:20,720 --> 00:59:26,960
groups so we could do 10-fold cross

1471
00:59:23,119 --> 00:59:30,559
validation where each group had about um

1472
00:59:26,960 --> 00:59:32,000
20 proteins in it. Um, and each group

1473
00:59:30,559 --> 00:59:33,680
was very different from the other group

1474
00:59:32,000 --> 00:59:35,839
by all those criteria that I just

1475
00:59:33,680 --> 00:59:37,599
mentioned. And then we enrich those a

1476
00:59:35,839 --> 00:59:38,799
little bit by saying, okay, within a

1477
00:59:37,599 --> 00:59:40,400
group, as long as it's different from

1478
00:59:38,799 --> 00:59:42,319
any other group, we can we can relax

1479
00:59:40,400 --> 00:59:44,480
some of those criteria from our 2,00

1480
00:59:42,319 --> 00:59:46,559
proteins. So just increase the numbers a

1481
00:59:44,480 --> 00:59:49,920
little bit. So now we have instead of

1482
00:59:46,559 --> 00:59:51,599
200, we have around 700 proteins. Okay?

1483
00:59:49,920 --> 00:59:53,200
But they're all but the these these 11

1484
00:59:51,599 --> 00:59:55,040
groups, which I'm not showing explicitly

1485
00:59:53,200 --> 00:59:56,960
here, these 11 groups are all very

1486
00:59:55,040 --> 00:59:59,200
different from one another.

1487
00:59:56,960 --> 01:00:01,040
um we always took one group as the test

1488
00:59:59,200 --> 01:00:03,200
set and then we would mix up the train

1489
01:00:01,040 --> 01:00:05,680
validation sets for cross validation for

1490
01:00:03,200 --> 01:00:08,000
training. Okay, so I I think I made the

1491
01:00:05,680 --> 01:00:09,680
point that um the amount of unique data

1492
01:00:08,000 --> 01:00:11,839
here, non-redundant data for this task

1493
01:00:09,680 --> 01:00:14,480
is just too small I think to train a a

1494
01:00:11,839 --> 01:00:16,880
neural network from scratch. Um and so

1495
01:00:14,480 --> 01:00:18,640
the question that that we we ask is can

1496
01:00:16,880 --> 01:00:20,559
we use embeddings from a pre-trained

1497
01:00:18,640 --> 01:00:22,240
model? And this is one of the reasons

1498
01:00:20,559 --> 01:00:25,599
why Ben took you through all these

1499
01:00:22,240 --> 01:00:27,680
models uh in the primer is all these

1500
01:00:25,599 --> 01:00:31,119
models, ESM2, protein sequence model,

1501
01:00:27,680 --> 01:00:33,280
alpha fold 2, ESM1

1502
01:00:31,119 --> 01:00:36,799
sequence prediction model, um even

1503
01:00:33,280 --> 01:00:38,240
protein MPN um these were all trained

1504
01:00:36,799 --> 01:00:40,640
and these were the ones available at the

1505
01:00:38,240 --> 01:00:42,400
time of this work anyway and um and

1506
01:00:40,640 --> 01:00:44,480
these were all trained not seeing ligans

1507
01:00:42,400 --> 01:00:47,119
at all but yet they were trained on much

1508
01:00:44,480 --> 01:00:49,520
more data uh on different tasks, right?

1509
01:00:47,119 --> 01:00:51,839
on the task of of sequence prediction or

1510
01:00:49,520 --> 01:00:55,200
structure prediction but on train on all

1511
01:00:51,839 --> 01:00:57,520
the PDB on all the proteins in um

1512
01:00:55,200 --> 01:01:00,799
sequence databases

1513
01:00:57,520 --> 01:01:02,160
uh and and even on you know self-dist

1514
01:01:00,799 --> 01:01:03,839
distilled proteins where you would

1515
01:01:02,160 --> 01:01:05,359
predict the structures of of real

1516
01:01:03,839 --> 01:01:07,680
proteins from sequence databases and

1517
01:01:05,359 --> 01:01:09,200
train on those um not train on lians at

1518
01:01:07,680 --> 01:01:11,440
all but yet could they develop a rich

1519
01:01:09,200 --> 01:01:13,680
enough embedding of what protein

1520
01:01:11,440 --> 01:01:15,839
structure is to be used for a downstream

1521
01:01:13,680 --> 01:01:17,920
prediction task like predicting is this

1522
01:01:15,839 --> 01:01:20,000
residue binding a lian you're not right.

1523
01:01:17,920 --> 01:01:21,040
That's the idea. So this you're not

1524
01:01:20,000 --> 01:01:22,319
trained on. So it's kind of

1525
01:01:21,040 --> 01:01:23,920
counterintuitive, but I want to take you

1526
01:01:22,319 --> 01:01:25,200
through why I think it might work and

1527
01:01:23,920 --> 01:01:28,319
why we thought this might work at the

1528
01:01:25,200 --> 01:01:32,160
time and why it does work. Uh I think

1529
01:01:28,319 --> 01:01:33,520
so. Um one of the reasons is that uh you

1530
01:01:32,160 --> 01:01:35,440
know the question is why why could these

1531
01:01:33,520 --> 01:01:37,200
embeddings be useful on models that

1532
01:01:35,440 --> 01:01:40,640
weren't trained on or don't haven't seen

1533
01:01:37,200 --> 01:01:42,880
leans before? uh and and the one of the

1534
01:01:40,640 --> 01:01:44,640
interesting uh things that I maybe I

1535
01:01:42,880 --> 01:01:48,640
want you to take away from this talk is

1536
01:01:44,640 --> 01:01:50,000
that protein small molecule interactions

1537
01:01:48,640 --> 01:01:52,400
um and I'm showing one on the left in

1538
01:01:50,000 --> 01:01:54,720
the bottom left here for example a green

1539
01:01:52,400 --> 01:01:57,680
glutamine interacting with a purple lian

1540
01:01:54,720 --> 01:02:00,400
small molecule drug these interactions

1541
01:01:57,680 --> 01:02:02,559
that proteins make with small molecules

1542
01:02:00,400 --> 01:02:06,559
look a heck of a lot like interactions

1543
01:02:02,559 --> 01:02:09,040
that proteins make with protein and in

1544
01:02:06,559 --> 01:02:11,440
scan it's just another that's just an

1545
01:02:09,040 --> 01:02:13,119
asparagene residue from another protein.

1546
01:02:11,440 --> 01:02:15,599
And these interaction geometries look

1547
01:02:13,119 --> 01:02:17,440
very similar. And so um you you could

1548
01:02:15,599 --> 01:02:20,799
think of drugs or small molecules or

1549
01:02:17,440 --> 01:02:23,119
metabolites as combinations of amino

1550
01:02:20,799 --> 01:02:25,599
acid features. You could think of that

1551
01:02:23,119 --> 01:02:27,680
and say that well if a model was trained

1552
01:02:25,599 --> 01:02:29,760
on predicting

1553
01:02:27,680 --> 01:02:31,680
interactions between residues even

1554
01:02:29,760 --> 01:02:33,920
within a chain

1555
01:02:31,680 --> 01:02:35,520
even within a chain of a protein as

1556
01:02:33,920 --> 01:02:37,760
opposed to even between different chains

1557
01:02:35,520 --> 01:02:39,680
of proteins that those interactions that

1558
01:02:37,760 --> 01:02:41,680
it's learned about could be transferable

1559
01:02:39,680 --> 01:02:43,680
interactions with small molecular drugs

1560
01:02:41,680 --> 01:02:46,240
or metabolites whatever molecules you

1561
01:02:43,680 --> 01:02:48,079
want. Um we actually took and we were

1562
01:02:46,240 --> 01:02:50,400
curious about this. If you take all the

1563
01:02:48,079 --> 01:02:53,040
lians in the PTB

1564
01:02:50,400 --> 01:02:55,119
about four 40,000 of them that we could

1565
01:02:53,040 --> 01:02:57,839
actually take the smile strings of and

1566
01:02:55,119 --> 01:03:00,000
RDKit would read them. So about 40,000

1567
01:02:57,839 --> 01:03:01,520
lians. Can we convert those to Morgan

1568
01:03:00,000 --> 01:03:03,200
fingerprints

1569
01:03:01,520 --> 01:03:05,200
uh for the small molecule lians just a

1570
01:03:03,200 --> 01:03:07,359
just a vector that's basically different

1571
01:03:05,200 --> 01:03:10,640
different functional groups of the ligan

1572
01:03:07,359 --> 01:03:12,799
a one or a zero and um and if we do the

1573
01:03:10,640 --> 01:03:14,799
same thing for the 20 amino acids

1574
01:03:12,799 --> 01:03:16,720
convert the 20 amino acids take a smile

1575
01:03:14,799 --> 01:03:18,400
string proteinated you know charge

1576
01:03:16,720 --> 01:03:20,640
uncharge for the 20 amino acids and

1577
01:03:18,400 --> 01:03:23,200
convert those to to um Morgan

1578
01:03:20,640 --> 01:03:25,200
fingerprints these vectors then you

1579
01:03:23,200 --> 01:03:27,599
could you could ask for any drug which

1580
01:03:25,200 --> 01:03:30,240
is represented as a vector

1581
01:03:27,599 --> 01:03:32,319
how many of its components

1582
01:03:30,240 --> 01:03:34,799
are represented in the 20 amino acid

1583
01:03:32,319 --> 01:03:38,000
vectors as a basis set for this drug,

1584
01:03:34,799 --> 01:03:42,160
the small molecule. And um and so this

1585
01:03:38,000 --> 01:03:44,559
histogram is uh the molecule counts by

1586
01:03:42,160 --> 01:03:46,720
just a fraction of the the molecule

1587
01:03:44,559 --> 01:03:49,760
covered by amino acids. So the average

1588
01:03:46,720 --> 01:03:52,720
is about 50%. Any molecule in the PTBE,

1589
01:03:49,760 --> 01:03:55,119
at least half of it on average can be

1590
01:03:52,720 --> 01:03:56,799
described by amino acids. And so I'm

1591
01:03:55,119 --> 01:03:59,599
showing you some of those examples down

1592
01:03:56,799 --> 01:04:02,160
below where the red circles are the bits

1593
01:03:59,599 --> 01:04:04,400
of it that were described by amino acids

1594
01:04:02,160 --> 01:04:05,920
via the Morgan fingerprint. So a lot of

1595
01:04:04,400 --> 01:04:08,160
these molecules can be described. So

1596
01:04:05,920 --> 01:04:10,240
it's serotonin, the drug down at the

1597
01:04:08,160 --> 01:04:14,000
bottom, a pixaban,

1598
01:04:10,240 --> 01:04:17,280
um, rap, what is the one in the the one

1599
01:04:14,000 --> 01:04:20,240
that's in the top second from the top on

1600
01:04:17,280 --> 01:04:22,720
the right? I guess I think that's LSD.

1601
01:04:20,240 --> 01:04:24,559
So but all these can be um can be

1602
01:04:22,720 --> 01:04:27,039
represented by amino acids. Okay. So

1603
01:04:24,559 --> 01:04:28,319
that's one motivation. So a lot of these

1604
01:04:27,039 --> 01:04:31,280
interaction you can go into any

1605
01:04:28,319 --> 01:04:33,200
individual crystal structure now and you

1606
01:04:31,280 --> 01:04:36,319
can mine for these kinds of interactions

1607
01:04:33,200 --> 01:04:39,280
with different fragments. Now so um so

1608
01:04:36,319 --> 01:04:40,960
here's an example. Um take any protein

1609
01:04:39,280 --> 01:04:42,880
from the PDB and you could say I want to

1610
01:04:40,960 --> 01:04:46,000
know how proteins interact with this

1611
01:04:42,880 --> 01:04:47,839
green functional group caroximate.

1612
01:04:46,000 --> 01:04:49,440
And there's a lot of caroxids in this

1613
01:04:47,839 --> 01:04:51,599
protein. They're amino acid side chains.

1614
01:04:49,440 --> 01:04:53,280
that's part of asparagene and glutamine.

1615
01:04:51,599 --> 01:04:55,280
So you can go into any single one of

1616
01:04:53,280 --> 01:04:56,960
them and you can say I want to know

1617
01:04:55,280 --> 01:04:59,200
what's interacting with this particular

1618
01:04:56,960 --> 01:05:00,799
asparagene in green and it looks like

1619
01:04:59,200 --> 01:05:03,920
there's three amino acids that interact

1620
01:05:00,799 --> 01:05:05,680
with it directly in white. And some of

1621
01:05:03,920 --> 01:05:07,359
those interactions within that single

1622
01:05:05,680 --> 01:05:09,839
chain protein are what I would call

1623
01:05:07,359 --> 01:05:11,359
local in sequence and some of them are

1624
01:05:09,839 --> 01:05:13,359
distant in sequence. And proteins are

1625
01:05:11,359 --> 01:05:15,760
like a polymer. They're just a polymer.

1626
01:05:13,359 --> 01:05:17,359
They have a persistence length where the

1627
01:05:15,760 --> 01:05:20,240
chain sort of forgets the direction that

1628
01:05:17,359 --> 01:05:22,480
it's going. And if you have two residues

1629
01:05:20,240 --> 01:05:24,000
interacting that are beyond the

1630
01:05:22,480 --> 01:05:26,559
persistence length of the polymer, it's

1631
01:05:24,000 --> 01:05:28,160
sort of like seeing um two different

1632
01:05:26,559 --> 01:05:31,440
molecules that are not part of the same

1633
01:05:28,160 --> 01:05:33,200
chain. Um so it's sort of like a protein

1634
01:05:31,440 --> 01:05:34,880
drug interaction and that would be the

1635
01:05:33,200 --> 01:05:37,200
one on the bottom. It's distant in

1636
01:05:34,880 --> 01:05:38,880
sequence. All right. So you could then

1637
01:05:37,200 --> 01:05:42,079
take those and say, okay, this is my

1638
01:05:38,880 --> 01:05:44,960
amino acid. It's interacting with a a a

1639
01:05:42,079 --> 01:05:46,880
the green caroximid at this position in

1640
01:05:44,960 --> 01:05:48,559
space. And you could do that for all the

1641
01:05:46,880 --> 01:05:51,200
proteins in the PDB and start to build

1642
01:05:48,559 --> 01:05:52,960
up massive data sets for how amino acids

1643
01:05:51,200 --> 01:05:55,119
interact with these kinds of fragments.

1644
01:05:52,960 --> 01:05:58,319
We made up a term for this called a

1645
01:05:55,119 --> 01:06:00,880
vandimer which is a um

1646
01:05:58,319 --> 01:06:02,960
it's related to the term is related to

1647
01:06:00,880 --> 01:06:05,280
amino acid rotimer a distinct side chain

1648
01:06:02,960 --> 01:06:06,720
configuration but a vandimer is a

1649
01:06:05,280 --> 01:06:08,079
configuration that a fragment might be

1650
01:06:06,720 --> 01:06:10,000
found and a fragment is coming from an

1651
01:06:08,079 --> 01:06:12,720
amino acid but that fragment

1652
01:06:10,000 --> 01:06:15,200
nevertheless could be used to describe

1653
01:06:12,720 --> 01:06:16,880
um uh we have a lot of these data sets

1654
01:06:15,200 --> 01:06:18,960
and and they could be used to describe

1655
01:06:16,880 --> 01:06:22,079
small molecules and their interactions

1656
01:06:18,960 --> 01:06:23,520
with uh proteins in the PDB. So here I'm

1657
01:06:22,079 --> 01:06:25,200
just showing a couple of examples of

1658
01:06:23,520 --> 01:06:28,400
crystal structures found to different

1659
01:06:25,200 --> 01:06:29,760
drugs. Um you have actually a pixaban

1660
01:06:28,400 --> 01:06:32,160
and then you have actually that's

1661
01:06:29,760 --> 01:06:33,760
adrenaline from a gpcr and and the

1662
01:06:32,160 --> 01:06:35,440
interactions with the small molecule can

1663
01:06:33,760 --> 01:06:37,280
be very well described by these

1664
01:06:35,440 --> 01:06:39,440
interactions just pulling out from

1665
01:06:37,280 --> 01:06:40,960
proteins how they interact with

1666
01:06:39,440 --> 01:06:43,520
functional groups from side chains of

1667
01:06:40,960 --> 01:06:45,440
amino acids. So that's kind of cool. Um

1668
01:06:43,520 --> 01:06:47,359
we just as an aside, this isn't really

1669
01:06:45,440 --> 01:06:48,960
the the point of this talk, but we took

1670
01:06:47,359 --> 01:06:52,640
that kind of a basis set and we designed

1671
01:06:48,960 --> 01:06:56,160
proteins from scratch in my group um

1672
01:06:52,640 --> 01:06:59,280
with Bill Drat too at UCSF um as a real

1673
01:06:56,160 --> 01:07:00,960
test for can we use these protein amino

1674
01:06:59,280 --> 01:07:02,799
acid interactions to design protein

1675
01:07:00,960 --> 01:07:04,640
small molecule interactions and this

1676
01:07:02,799 --> 01:07:07,039
worked. But this is a that's a different

1677
01:07:04,640 --> 01:07:09,359
talk, right? So, so that's what made us

1678
01:07:07,039 --> 01:07:12,000
believe that maybe these models that

1679
01:07:09,359 --> 01:07:13,599
were trained without lians but just to

1680
01:07:12,000 --> 01:07:16,240
predict maybe protein understanding of

1681
01:07:13,599 --> 01:07:17,760
protein protein interactions intrachain

1682
01:07:16,240 --> 01:07:20,000
interactions could be used to predict

1683
01:07:17,760 --> 01:07:22,720
small molecule binding sites. The last

1684
01:07:20,000 --> 01:07:24,640
thing that we thought was interesting uh

1685
01:07:22,720 --> 01:07:27,680
is that it's kind of well known that

1686
01:07:24,640 --> 01:07:29,039
when a small molecule binds a protein

1687
01:07:27,680 --> 01:07:31,760
that protein might get a little bit

1688
01:07:29,039 --> 01:07:33,119
smaller and it might finish folding. you

1689
01:07:31,760 --> 01:07:35,440
know, existing contacts might be more

1690
01:07:33,119 --> 01:07:37,440
enthalpically favorable after binding.

1691
01:07:35,440 --> 01:07:40,160
There's a really great review by Dudley

1692
01:07:37,440 --> 01:07:42,400
Williams from 20 years ago that I'm

1693
01:07:40,160 --> 01:07:43,839
citing here. And this is just showing if

1694
01:07:42,400 --> 01:07:45,440
you have a protein that's a big circle,

1695
01:07:43,839 --> 01:07:48,400
it gets a little small when it binds a

1696
01:07:45,440 --> 01:07:50,559
liant. And so can but so binding might

1697
01:07:48,400 --> 01:07:52,640
finish folding quote unquote the the

1698
01:07:50,559 --> 01:07:55,359
protein. And so if you had a a structure

1699
01:07:52,640 --> 01:07:57,599
prediction model like alphafold 2, could

1700
01:07:55,359 --> 01:07:59,520
you use the structure prediction model

1701
01:07:57,599 --> 01:08:01,920
somehow to finish folding the protein?

1702
01:07:59,520 --> 01:08:04,240
Could that signal be used and read out

1703
01:08:01,920 --> 01:08:05,520
to predict ligan binding sites? That was

1704
01:08:04,240 --> 01:08:07,359
another thing that we were wondering

1705
01:08:05,520 --> 01:08:08,559
about. And so here's one way you might

1706
01:08:07,359 --> 01:08:10,880
might go about doing that. Now I'm going

1707
01:08:08,559 --> 01:08:14,079
to get into like the guts of alpha AF2

1708
01:08:10,880 --> 01:08:16,480
bind and then the the the um and then

1709
01:08:14,079 --> 01:08:18,799
what we can do with it. So what we do is

1710
01:08:16,480 --> 01:08:21,199
we take and alphold 2 here is the single

1711
01:08:18,799 --> 01:08:22,799
chain alphaold model. So it's the first

1712
01:08:21,199 --> 01:08:24,159
one that came out. It's not alpha 2

1713
01:08:22,799 --> 01:08:25,679
multimer. It's the single chain. It's

1714
01:08:24,159 --> 01:08:28,719
only been trained on single chain

1715
01:08:25,679 --> 01:08:30,080
proteins. And um we use the target

1716
01:08:28,719 --> 01:08:32,400
sequence. So this is an example of a

1717
01:08:30,080 --> 01:08:34,960
four helix bundle. Um take the target

1718
01:08:32,400 --> 01:08:36,560
sequence of the protein and you take the

1719
01:08:34,960 --> 01:08:38,719
backbone coordinates. You could use the

1720
01:08:36,560 --> 01:08:42,159
side chain coordinates too of that

1721
01:08:38,719 --> 01:08:44,400
protein as a template to alpha fold

1722
01:08:42,159 --> 01:08:47,359
and um and then we use 20 bait amino

1723
01:08:44,400 --> 01:08:49,199
acids. beta amino acids then we can we

1724
01:08:47,359 --> 01:08:52,000
can in the single chain model of

1725
01:08:49,199 --> 01:08:56,400
alphafold you can represent different

1726
01:08:52,000 --> 01:08:58,239
chains um in a hacky way by saying by

1727
01:08:56,400 --> 01:09:00,799
giving the each each chain a large

1728
01:08:58,239 --> 01:09:02,159
residue offset and so each one of these

1729
01:09:00,799 --> 01:09:04,640
bait amino acids these are the 20

1730
01:09:02,159 --> 01:09:06,640
canonical amino acids we could just um

1731
01:09:04,640 --> 01:09:09,120
make a residue offset of 50 arbitrary

1732
01:09:06,640 --> 01:09:11,120
arbitrary number 50 residue offset and

1733
01:09:09,120 --> 01:09:12,319
alpha fold 2 sees it as sort of like a

1734
01:09:11,120 --> 01:09:14,319
separate chain it won't try to connect

1735
01:09:12,319 --> 01:09:16,880
them covealently

1736
01:09:14,319 --> 01:09:19,679
um and So, so we decided what if we ch

1737
01:09:16,880 --> 01:09:22,159
what if we use these beta amino acids as

1738
01:09:19,679 --> 01:09:24,400
surrogate lians that alphafold might be

1739
01:09:22,159 --> 01:09:25,759
able to finish folding with. There's no

1740
01:09:24,400 --> 01:09:28,080
multiple sequence alignment here because

1741
01:09:25,759 --> 01:09:30,799
we're using the template uh of the

1742
01:09:28,080 --> 01:09:32,560
protein. Um and alphafold if you give it

1743
01:09:30,799 --> 01:09:36,480
people don't know if you give alphafold

1744
01:09:32,560 --> 01:09:38,319
a template structure that has the same

1745
01:09:36,480 --> 01:09:40,719
sequence as the sequence you're

1746
01:09:38,319 --> 01:09:42,480
predicting with alphafold it'll

1747
01:09:40,719 --> 01:09:43,520
basically output the same structure as

1748
01:09:42,480 --> 01:09:44,960
the input.

1749
01:09:43,520 --> 01:09:47,279
So the STR and this so there was a

1750
01:09:44,960 --> 01:09:49,679
question before about can AF2 bind

1751
01:09:47,279 --> 01:09:52,239
handle multiple confirmations of

1752
01:09:49,679 --> 01:09:54,400
proteins and it can um if you give it

1753
01:09:52,239 --> 01:09:56,560
different templates uh because it'll

1754
01:09:54,400 --> 01:09:58,960
just output that same template structure

1755
01:09:56,560 --> 01:10:00,719
basically for identical sequences. Um

1756
01:09:58,960 --> 01:10:02,159
okay so this okay so this is the

1757
01:10:00,719 --> 01:10:03,679
question is could you just simply use it

1758
01:10:02,159 --> 01:10:05,440
and dock these lians and then use that

1759
01:10:03,679 --> 01:10:07,600
information. So this is what it looks

1760
01:10:05,440 --> 01:10:10,400
like if you do that, right? There's just

1761
01:10:07,600 --> 01:10:11,760
20 amino acids everywhere. Maybe some of

1762
01:10:10,400 --> 01:10:13,679
them go inside, some of them are

1763
01:10:11,760 --> 01:10:16,560
outside. This is also just a single pass

1764
01:10:13,679 --> 01:10:18,800
through the model. Um, we could maybe

1765
01:10:16,560 --> 01:10:21,199
try to make sense of this, but this is

1766
01:10:18,800 --> 01:10:23,360
kind this is a, you know, a little bit

1767
01:10:21,199 --> 01:10:26,640
messy. So we decided that maybe the best

1768
01:10:23,360 --> 01:10:28,640
signal to try to extract would be to go

1769
01:10:26,640 --> 01:10:30,480
into the guts of alpha 2. Not just look

1770
01:10:28,640 --> 01:10:31,840
at the output of alpha 2, but look go

1771
01:10:30,480 --> 01:10:34,480
into the guts of it and look at the

1772
01:10:31,840 --> 01:10:36,480
embeddings. and in particular look at

1773
01:10:34,480 --> 01:10:39,679
the pair representation and the single

1774
01:10:36,480 --> 01:10:41,360
representation Ben talked about. So

1775
01:10:39,679 --> 01:10:43,360
which alphafold converts the input

1776
01:10:41,360 --> 01:10:45,440
sequence into these embeddings in order

1777
01:10:43,360 --> 01:10:47,760
to predict structure.

1778
01:10:45,440 --> 01:10:49,440
Um and so in so instead of just docking

1779
01:10:47,760 --> 01:10:51,840
these molecules and then looking at the

1780
01:10:49,440 --> 01:10:54,000
output of alphafold we actually just run

1781
01:10:51,840 --> 01:10:55,840
alphafold 2 a single pass through the

1782
01:10:54,000 --> 01:10:58,000
model and we don't care about where

1783
01:10:55,840 --> 01:11:00,480
these beta amino acids actually go where

1784
01:10:58,000 --> 01:11:02,800
they're put by alphafold. What we care

1785
01:11:00,480 --> 01:11:06,239
about is how alphafold

1786
01:11:02,800 --> 01:11:08,320
um how alpha fold attends to each of

1787
01:11:06,239 --> 01:11:10,719
those bait residues with respect to any

1788
01:11:08,320 --> 01:11:13,440
residue in the template in the template

1789
01:11:10,719 --> 01:11:17,040
structure. And so the the alpha the pair

1790
01:11:13,440 --> 01:11:19,040
representation of alpha fold is a tensor

1791
01:11:17,040 --> 01:11:21,840
for any pair of molecu any pair of

1792
01:11:19,040 --> 01:11:24,159
residues that are that are folded. Um

1793
01:11:21,840 --> 01:11:27,920
you have an embedding

1794
01:11:24,159 --> 01:11:30,400
and that embedding is 128 numbers. And

1795
01:11:27,920 --> 01:11:33,600
so this tensor looks like this cube

1796
01:11:30,400 --> 01:11:36,239
where um in the in the um blue and in

1797
01:11:33,600 --> 01:11:39,040
the cyan it's actually not symmetric. So

1798
01:11:36,239 --> 01:11:41,199
the blue and the cyan um in a symmetric

1799
01:11:39,040 --> 01:11:42,400
mat matrix would be equal to one another

1800
01:11:41,199 --> 01:11:44,960
but in this case there's sort of

1801
01:11:42,400 --> 01:11:48,320
different numbers um and so for each of

1802
01:11:44,960 --> 01:11:50,480
the bait residues the 20 bait residues

1803
01:11:48,320 --> 01:11:52,960
we actually care how alphafold creates

1804
01:11:50,480 --> 01:11:55,040
an embedding for any of the residues in

1805
01:11:52,960 --> 01:11:56,400
the template structure and that those

1806
01:11:55,040 --> 01:11:58,480
are the features we're going to extract

1807
01:11:56,400 --> 01:12:00,320
from the from the pair representation

1808
01:11:58,480 --> 01:12:02,239
and train a model to predict if this is

1809
01:12:00,320 --> 01:12:04,400
a binding residue or not. I'll take you

1810
01:12:02,239 --> 01:12:06,400
through what that means. But this is um

1811
01:12:04,400 --> 01:12:08,640
this is basically the training objective

1812
01:12:06,400 --> 01:12:10,159
now is we take all these inputs on the

1813
01:12:08,640 --> 01:12:11,199
left for a particular protein. We don't

1814
01:12:10,159 --> 01:12:13,440
have to use a multiple sequence

1815
01:12:11,199 --> 01:12:15,760
alignment and we stick those into alpha

1816
01:12:13,440 --> 01:12:17,600
2 single pass through the model and then

1817
01:12:15,760 --> 01:12:20,640
we extract features from the pair

1818
01:12:17,600 --> 01:12:22,960
representation and linearly combine them

1819
01:12:20,640 --> 01:12:24,719
and train a logistic regression model in

1820
01:12:22,960 --> 01:12:26,400
order to predict which residues on that

1821
01:12:24,719 --> 01:12:28,239
protein are binding residues. We have

1822
01:12:26,400 --> 01:12:30,719
all those ground truth labels from that

1823
01:12:28,239 --> 01:12:32,400
data set that we painstakingly made. For

1824
01:12:30,719 --> 01:12:33,840
example, from a crystal structure of

1825
01:12:32,400 --> 01:12:35,760
this protein, we know it binds this

1826
01:12:33,840 --> 01:12:38,080
green lian. So, we know what the ground

1827
01:12:35,760 --> 01:12:39,600
truth labels are. They're in purple.

1828
01:12:38,080 --> 01:12:41,280
These are the these are the pocket

1829
01:12:39,600 --> 01:12:43,920
contact these are the ligan contacting

1830
01:12:41,280 --> 01:12:46,239
residues of the pocket. And um the goal

1831
01:12:43,920 --> 01:12:47,840
for um an AF2 bind like model is to

1832
01:12:46,239 --> 01:12:50,480
predict which those what those residues

1833
01:12:47,840 --> 01:12:52,560
are. All right. So, this is this is in

1834
01:12:50,480 --> 01:12:53,920
practice how it works. So what we do is

1835
01:12:52,560 --> 01:12:57,040
like I said we just take that pair

1836
01:12:53,920 --> 01:13:00,080
representation and we extract the um the

1837
01:12:57,040 --> 01:13:02,080
blue and the cyan parts of the pair

1838
01:13:00,080 --> 01:13:03,360
representation and we stack them. All

1839
01:13:02,080 --> 01:13:05,199
right so this is the best way I could

1840
01:13:03,360 --> 01:13:07,840
come up with an illustration of what it

1841
01:13:05,199 --> 01:13:10,480
is we're actually doing with AF2 bind

1842
01:13:07,840 --> 01:13:14,800
and and um and then I've highlighted a

1843
01:13:10,480 --> 01:13:17,040
single residue uh um in red as a slice

1844
01:13:14,800 --> 01:13:19,360
of the pair representation. I'm not sure

1845
01:13:17,040 --> 01:13:22,320
if you can see my you can't see my my

1846
01:13:19,360 --> 01:13:24,480
cursor but single residue in red um

1847
01:13:22,320 --> 01:13:26,400
residue let's call it residue n and so

1848
01:13:24,480 --> 01:13:28,960
we can extract that slice so if we're

1849
01:13:26,400 --> 01:13:30,800
interested in does residue n is there a

1850
01:13:28,960 --> 01:13:33,520
high probability of it binding a small

1851
01:13:30,800 --> 01:13:34,960
molecule liant or not then this is these

1852
01:13:33,520 --> 01:13:37,600
are the features we extract from the

1853
01:13:34,960 --> 01:13:41,199
pair representation for residue n we can

1854
01:13:37,600 --> 01:13:45,920
concatenate them and uh so we have about

1855
01:13:41,199 --> 01:13:49,679
so it's it's uh 20 by 128 um plus 128 so

1856
01:13:45,920 --> 01:13:53,600
it's around 5,28 features for just this

1857
01:13:49,679 --> 01:13:55,520
single uh the single residue because

1858
01:13:53,600 --> 01:13:58,480
it's it's the 20 different feature pair

1859
01:13:55,520 --> 01:14:01,440
features for the 20 different bait res.

1860
01:13:58,480 --> 01:14:02,960
Okay. And um and so then we we have

1861
01:14:01,440 --> 01:14:05,600
those those numbers which are just

1862
01:14:02,960 --> 01:14:07,840
numbers from AlphaFold's internal guts

1863
01:14:05,600 --> 01:14:10,480
and we train a weight matrix plus a

1864
01:14:07,840 --> 01:14:14,159
bias. That's the al that's the AF2 bind

1865
01:14:10,480 --> 01:14:16,560
model. a weight matrix is trained um and

1866
01:14:14,159 --> 01:14:18,960
a logistic regression model. Those the

1867
01:14:16,560 --> 01:14:23,600
weight matrix plus the bias converts

1868
01:14:18,960 --> 01:14:26,640
those 5,28 numbers into one number Z.

1869
01:14:23,600 --> 01:14:30,159
Z goes into a sigmoid and that's the

1870
01:14:26,640 --> 01:14:32,159
binding probability of the model. And we

1871
01:14:30,159 --> 01:14:35,760
know and it's just cross entropy loss,

1872
01:14:32,159 --> 01:14:40,000
right? So we uh or binary uh loss where

1873
01:14:35,760 --> 01:14:42,000
um where uh you get a one if it's the

1874
01:14:40,000 --> 01:14:44,080
correct prediction and it's a zero if

1875
01:14:42,000 --> 01:14:45,760
not and the weights are updated on the

1876
01:14:44,080 --> 01:14:48,239
model in order to predict the right

1877
01:14:45,760 --> 01:14:51,199
labels one or zero binding residue not

1878
01:14:48,239 --> 01:14:54,239
binding. Um so we call that probability

1879
01:14:51,199 --> 01:14:56,080
pbind probability of binding and um and

1880
01:14:54,239 --> 01:14:59,840
we train that in this cross validation

1881
01:14:56,080 --> 01:15:01,360
data set very rigorously. Um and this is

1882
01:14:59,840 --> 01:15:03,040
and it works on unseen folds. We

1883
01:15:01,360 --> 01:15:04,880
actually put these in our test set uh

1884
01:15:03,040 --> 01:15:06,960
because they're famous examples. The

1885
01:15:04,880 --> 01:15:09,120
muopioid receptor, it's a GPCR. Those

1886
01:15:06,960 --> 01:15:11,679
were not trained on nothing like GPCRs

1887
01:15:09,120 --> 01:15:13,280
were trained on. Chrommo domains bind

1888
01:15:11,679 --> 01:15:15,520
lots of lians. They weren't trained on

1889
01:15:13,280 --> 01:15:18,640
nothing like them. No similar pockets

1890
01:15:15,520 --> 01:15:19,920
were trained on. And AF2 binds using the

1891
01:15:18,640 --> 01:15:22,080
pair representation predicts those

1892
01:15:19,920 --> 01:15:25,199
pockets really really well. Um, but it

1893
01:15:22,080 --> 01:15:27,360
not only gives you just uh uh the

1894
01:15:25,199 --> 01:15:29,360
pocket, but what's what's what's you

1895
01:15:27,360 --> 01:15:30,640
know you could ask, well maybe uh P2

1896
01:15:29,360 --> 01:15:32,640
rank would do the same thing here

1897
01:15:30,640 --> 01:15:35,199
because there's a pocket is obvious.

1898
01:15:32,640 --> 01:15:38,400
What the subtlety here is that for every

1899
01:15:35,199 --> 01:15:40,080
residue in the pocket um AF2 bind gives

1900
01:15:38,400 --> 01:15:43,679
you a probability that that residue is

1901
01:15:40,080 --> 01:15:45,840
binding any small molecule liant. And

1902
01:15:43,679 --> 01:15:47,600
those probabilities are not the same for

1903
01:15:45,840 --> 01:15:49,280
every residue in the pocket. And so

1904
01:15:47,600 --> 01:15:52,080
there are some hotspot residues that

1905
01:15:49,280 --> 01:15:54,000
might be um more favorably interacting

1906
01:15:52,080 --> 01:15:55,440
with lians than other residues in this

1907
01:15:54,000 --> 01:15:57,360
pocket. And that we've actually this

1908
01:15:55,440 --> 01:15:58,640
isn't in the paper, but I I I was

1909
01:15:57,360 --> 01:16:02,000
playing around with this. And if you

1910
01:15:58,640 --> 01:16:05,360
just use the probabilities from AF2 bind

1911
01:16:02,000 --> 01:16:08,880
and a um a simple model that says rigid

1912
01:16:05,360 --> 01:16:10,560
body docking of a small molecule

1913
01:16:08,880 --> 01:16:11,679
um where it's a known small molecule. So

1914
01:16:10,560 --> 01:16:13,280
you kind of cheat and you take the right

1915
01:16:11,679 --> 01:16:14,719
configuration of the small molecule, but

1916
01:16:13,280 --> 01:16:16,960
it doesn't know where to go. All all

1917
01:16:14,719 --> 01:16:20,000
it's trying to do is maximize the

1918
01:16:16,960 --> 01:16:24,159
contacts of the small molecule with

1919
01:16:20,000 --> 01:16:26,320
residues that have high pbind. So so

1920
01:16:24,159 --> 01:16:28,960
maximize that score. You actually can

1921
01:16:26,320 --> 01:16:30,960
recover um in in in many examples you

1922
01:16:28,960 --> 01:16:32,080
can recover the true binding post which

1923
01:16:30,960 --> 01:16:33,120
is kind of cool because there's no

1924
01:16:32,080 --> 01:16:35,120
energy function other than just

1925
01:16:33,120 --> 01:16:37,600
maximizing speed but that's not in the

1926
01:16:35,120 --> 01:16:39,760
paper. Um no reviewer asked for that so

1927
01:16:37,600 --> 01:16:42,760
um we didn't put it in. Yeah. Couple

1928
01:16:39,760 --> 01:16:42,760
questions.

1929
01:16:45,360 --> 01:16:51,440
also learn about the uh the ligan itself

1930
01:16:48,560 --> 01:16:53,360
like do you have does

1931
01:16:51,440 --> 01:16:55,040
>> does it map at all to like the the

1932
01:16:53,360 --> 01:16:56,880
residues that it thinks in that

1933
01:16:55,040 --> 01:16:58,960
representation do they look do they have

1934
01:16:56,880 --> 01:16:59,440
similar functionality to the actual

1935
01:16:58,960 --> 01:17:00,880
opioid

1936
01:16:59,440 --> 01:17:02,080
>> yeah that that's a great question so can

1937
01:17:00,880 --> 01:17:04,640
we learn something about the actual

1938
01:17:02,080 --> 01:17:06,640
nature of the the um small molecule that

1939
01:17:04,640 --> 01:17:08,480
binds there and I'll skip I'll skip this

1940
01:17:06,640 --> 01:17:10,719
I'll go back to this slide in a second

1941
01:17:08,480 --> 01:17:12,880
but I'll go straight to your the answer

1942
01:17:10,719 --> 01:17:15,199
your question is that the model because

1943
01:17:12,880 --> 01:17:16,640
it's simple because it's just a simple

1944
01:17:15,199 --> 01:17:18,880
logistic regression model. we can go

1945
01:17:16,640 --> 01:17:21,840
immediately back to everything the 20

1946
01:17:18,880 --> 01:17:24,000
beta amino acids as the input and say so

1947
01:17:21,840 --> 01:17:25,520
um what we did here is this is the same

1948
01:17:24,000 --> 01:17:28,880
input that goes into the logistic

1949
01:17:25,520 --> 01:17:31,280
regression model these two um red boxes

1950
01:17:28,880 --> 01:17:33,760
of features but instead of converting

1951
01:17:31,280 --> 01:17:36,719
them all to one sum to one number Z that

1952
01:17:33,760 --> 01:17:40,080
goes into the sigmoid we can um do a do

1953
01:17:36,719 --> 01:17:43,040
a um a selective sum first and keep and

1954
01:17:40,080 --> 01:17:46,159
basically sum the uh the features so

1955
01:17:43,040 --> 01:17:49,120
that we collapse it from a 20 by 256 six

1956
01:17:46,159 --> 01:17:51,520
to a 20 by one. The 20 dimension being

1957
01:17:49,120 --> 01:17:53,840
each of the bait amino acids. So every

1958
01:17:51,520 --> 01:17:55,520
one of the canonical 20 amino acids and

1959
01:17:53,840 --> 01:17:59,840
we could say which of these 20 amino

1960
01:17:55,520 --> 01:18:03,760
acids actually is has an activation that

1961
01:17:59,840 --> 01:18:05,920
would be firing the sigmoid right to

1962
01:18:03,760 --> 01:18:09,520
predict binding or not binding. And so

1963
01:18:05,920 --> 01:18:11,760
the blue here is on the very right is uh

1964
01:18:09,520 --> 01:18:13,440
amino acids that would be firing high.

1965
01:18:11,760 --> 01:18:15,600
So being being correlated with a

1966
01:18:13,440 --> 01:18:16,640
prediction for this is a binding residue

1967
01:18:15,600 --> 01:18:19,679
and then the red are are

1968
01:18:16,640 --> 01:18:22,159
anti-correlated. Um and so what's kind

1969
01:18:19,679 --> 01:18:23,760
of cool is that you can do this for

1970
01:18:22,159 --> 01:18:25,360
every residue that's predicted in the

1971
01:18:23,760 --> 01:18:27,760
pocket, not just a single residue and

1972
01:18:25,360 --> 01:18:31,199
sort of build up these maps

1973
01:18:27,760 --> 01:18:35,199
of of um what of attributing bait

1974
01:18:31,199 --> 01:18:37,040
residues to the prediction of this is a

1975
01:18:35,199 --> 01:18:39,040
binding residue or not. And so you can

1976
01:18:37,040 --> 01:18:42,480
directly attribute. So the point of this

1977
01:18:39,040 --> 01:18:44,400
slide is that it it does correlate um

1978
01:18:42,480 --> 01:18:46,400
you know per as I would expect but it

1979
01:18:44,400 --> 01:18:49,360
was really great to see sort of maybe

1980
01:18:46,400 --> 01:18:52,480
unintuitive but um if you think of

1981
01:18:49,360 --> 01:18:54,080
pockets that bind apolar ligans like for

1982
01:18:52,480 --> 01:18:55,760
example the one in the bottom left here

1983
01:18:54,080 --> 01:18:57,440
this protein that binds this apolar

1984
01:18:55,760 --> 01:18:59,280
lipid

1985
01:18:57,440 --> 01:19:01,920
the bait residues that are responsible

1986
01:18:59,280 --> 01:19:05,840
for the prediction of that binding

1987
01:19:01,920 --> 01:19:07,760
pocket are I'm pointing out with a a a

1988
01:19:05,840 --> 01:19:10,480
black arrow are these hydrophobic amino

1989
01:19:07,760 --> 01:19:12,080
acids fenol alanine, tryptophan

1990
01:19:10,480 --> 01:19:13,760
and so those are the amino acids that

1991
01:19:12,080 --> 01:19:15,199
are that are leading to that prediction.

1992
01:19:13,760 --> 01:19:16,640
On the opposite on the right side of the

1993
01:19:15,199 --> 01:19:18,400
slide there's a a protein different

1994
01:19:16,640 --> 01:19:21,840
protein that binds to a very polar amino

1995
01:19:18,400 --> 01:19:23,520
acid. Um lot of different you can see a

1996
01:19:21,840 --> 01:19:25,840
lot of different oxygens and nitrogens

1997
01:19:23,520 --> 01:19:29,760
in that that that um it binds to a very

1998
01:19:25,840 --> 01:19:31,360
polar lian. Uh and and um the the beta

1999
01:19:29,760 --> 01:19:34,400
amino acids that actually lead to the

2000
01:19:31,360 --> 01:19:36,880
prediction there are polar beta amino

2001
01:19:34,400 --> 01:19:40,880
acids. So those are um in this case you

2002
01:19:36,880 --> 01:19:43,840
have glutamine asparagene um uh

2003
01:19:40,880 --> 01:19:45,520
aspartate and glutamate. So the blue

2004
01:19:43,840 --> 01:19:47,199
parts are the parts that are leading to

2005
01:19:45,520 --> 01:19:48,480
the predictions and there are different

2006
01:19:47,199 --> 01:19:50,159
the point is there are different beta

2007
01:19:48,480 --> 01:19:51,840
amino acids that do that depending on

2008
01:19:50,159 --> 01:19:53,600
the nature of the binding pocket kinds

2009
01:19:51,840 --> 01:19:56,480
of lians that are bind. One idea would

2010
01:19:53,600 --> 01:19:57,840
be to directly use these maps as input

2011
01:19:56,480 --> 01:20:00,400
features to predicting what kind of

2012
01:19:57,840 --> 01:20:01,760
lians. We haven't done that, but um one

2013
01:20:00,400 --> 01:20:04,480
could imagine doing that and taking

2014
01:20:01,760 --> 01:20:06,480
these as a a nice representation of the

2015
01:20:04,480 --> 01:20:08,080
pocket and saying, "Okay, I'm going to

2016
01:20:06,480 --> 01:20:10,960
figure out what drug or small molecule

2017
01:20:08,080 --> 01:20:13,199
might bind there." Future work. Um

2018
01:20:10,960 --> 01:20:14,880
that's one way to do it. Um and then

2019
01:20:13,199 --> 01:20:16,560
just the last point on this is we looked

2020
01:20:14,880 --> 01:20:18,400
at every those were just two anecdotal

2021
01:20:16,560 --> 01:20:20,800
examples but we looked at every protein

2022
01:20:18,400 --> 01:20:22,800
in our training set and test set all

2023
01:20:20,800 --> 01:20:24,640
across everything and um so many

2024
01:20:22,800 --> 01:20:27,440
thousands of of amino acids that are

2025
01:20:24,640 --> 01:20:29,920
predicted to be pockets and we knew the

2026
01:20:27,440 --> 01:20:31,679
nature of the lian uh because this is

2027
01:20:29,920 --> 01:20:32,719
ground truth. So we knew how many for

2028
01:20:31,679 --> 01:20:33,920
example the simplest thing is just to

2029
01:20:32,719 --> 01:20:36,400
count up how many carbons are in the

2030
01:20:33,920 --> 01:20:39,520
lian because that's sort of a a proxy

2031
01:20:36,400 --> 01:20:41,760
for a hydrophobicity of the ligan. And

2032
01:20:39,520 --> 01:20:46,159
so where what I'm plotting here here on

2033
01:20:41,760 --> 01:20:48,800
the y- axis um is fractions of um

2034
01:20:46,159 --> 01:20:52,320
non-carbon atoms. So the polar atoms I

2035
01:20:48,800 --> 01:20:55,120
guess and um and then the uh the x- axis

2036
01:20:52,320 --> 01:20:58,400
is the um the firing the fraction of

2037
01:20:55,120 --> 01:21:02,320
predicted bait residues um that that

2038
01:20:58,400 --> 01:21:04,320
correlate with um with these uh non the

2039
01:21:02,320 --> 01:21:06,239
amount of non-carbon polar atoms in the

2040
01:21:04,320 --> 01:21:07,520
ligan. And you can see that for on the

2041
01:21:06,239 --> 01:21:09,920
left, these are kind of hydrophobic

2042
01:21:07,520 --> 01:21:11,760
residues, fennel alanine and isolucine.

2043
01:21:09,920 --> 01:21:14,400
And they're um they're inversely

2044
01:21:11,760 --> 01:21:16,800
correlated as they start firing. You go

2045
01:21:14,400 --> 01:21:19,360
you go up the x- axis, the amount of

2046
01:21:16,800 --> 01:21:21,040
polar residues in the lians go goes down

2047
01:21:19,360 --> 01:21:24,159
and it's inversely correlated on the

2048
01:21:21,040 --> 01:21:26,320
right where you have a a glutamate and

2049
01:21:24,159 --> 01:21:28,719
histadine residues pretty polar. And

2050
01:21:26,320 --> 01:21:30,560
those um the the nature of the lians

2051
01:21:28,719 --> 01:21:34,000
become more polar when those amino acids

2052
01:21:30,560 --> 01:21:35,280
are starting to fire. Um let's see. I I

2053
01:21:34,000 --> 01:21:37,120
do want to go I want to go back to this

2054
01:21:35,280 --> 01:21:39,520
one slide first before I get into more

2055
01:21:37,120 --> 01:21:41,920
related results about that. Um but just

2056
01:21:39,520 --> 01:21:43,920
this is just to show that we looked at

2057
01:21:41,920 --> 01:21:46,960
one of the reasons why this was nice in

2058
01:21:43,920 --> 01:21:48,239
looking at um uh uh embeddings from

2059
01:21:46,960 --> 01:21:49,679
neural networks is we didn't have to

2060
01:21:48,239 --> 01:21:51,920
retrain the models. Number one, they

2061
01:21:49,679 --> 01:21:55,040
weren't trained on lians. So there's no

2062
01:21:51,920 --> 01:21:57,280
memorization in that sense. Um and we

2063
01:21:55,040 --> 01:21:58,639
could just extract the embeddings and

2064
01:21:57,280 --> 01:22:00,320
then we're just taking the embeddings

2065
01:21:58,639 --> 01:22:02,320
and training a logistic regression model

2066
01:22:00,320 --> 01:22:04,960
on those embeddings. So we can actually

2067
01:22:02,320 --> 01:22:06,480
just train the same logistic regression

2068
01:22:04,960 --> 01:22:08,239
model

2069
01:22:06,480 --> 01:22:09,679
um using the different embeddings from

2070
01:22:08,239 --> 01:22:12,080
the different neural networks on our

2071
01:22:09,679 --> 01:22:13,280
same data set. Right? So now we can sort

2072
01:22:12,080 --> 01:22:14,960
of compare apples to apples which

2073
01:22:13,280 --> 01:22:17,199
embeddings are are better for our data

2074
01:22:14,960 --> 01:22:19,120
set which has this rigorous split. Um so

2075
01:22:17,199 --> 01:22:22,159
we did the single representation alful

2076
01:22:19,120 --> 01:22:23,920
too. Um I usually don't show tables in

2077
01:22:22,159 --> 01:22:26,080
slides but um this is kind of a nice

2078
01:22:23,920 --> 01:22:29,120
table because it just shows you we did

2079
01:22:26,080 --> 01:22:31,679
um ESM2 protein sequence only ESM1

2080
01:22:29,120 --> 01:22:33,280
inverse folding structure only um alpha

2081
01:22:31,679 --> 01:22:35,760
fold 2 which is the AF2 the pair

2082
01:22:33,280 --> 01:22:37,280
representation which is AF2 bind um and

2083
01:22:35,760 --> 01:22:39,520
then these are the the combinations of

2084
01:22:37,280 --> 01:22:43,040
the models and the point here is that of

2085
01:22:39,520 --> 01:22:44,960
the single uh um uh embeddings from

2086
01:22:43,040 --> 01:22:48,000
different neural networks alphold 2 pair

2087
01:22:44,960 --> 01:22:49,520
representation does the best um if you

2088
01:22:48,000 --> 01:22:50,960
care about you know rock curves curves.

2089
01:22:49,520 --> 01:22:54,639
These are what the the rock curves look

2090
01:22:50,960 --> 01:22:57,360
like. Precision um recall curves and AF2

2091
01:22:54,639 --> 01:22:59,120
bind is up here and up here as well as

2092
01:22:57,360 --> 01:23:02,400
these linear combinations of the models.

2093
01:22:59,120 --> 01:23:06,000
And then ES ESM2 and ESM1 inverse

2094
01:23:02,400 --> 01:23:08,159
folding are these guys here. Um and so

2095
01:23:06,000 --> 01:23:09,280
we because of the interpretability, you

2096
01:23:08,159 --> 01:23:11,040
know, we can interpret which bait

2097
01:23:09,280 --> 01:23:13,199
residues belong to which we stuck with

2098
01:23:11,040 --> 01:23:14,400
AF2 the AF2 bind model. If you're

2099
01:23:13,199 --> 01:23:16,480
interested, all these other linear

2100
01:23:14,400 --> 01:23:18,639
combinations of models which we

2101
01:23:16,480 --> 01:23:20,400
retrained everything from scratch um for

2102
01:23:18,639 --> 01:23:22,159
a fair prediction. Um these are all

2103
01:23:20,400 --> 01:23:25,840
available to on GitHub if you want to

2104
01:23:22,159 --> 01:23:27,280
use these models. Yeah.

2105
01:23:25,840 --> 01:23:30,239
>> Yeah. So I have a question about the

2106
01:23:27,280 --> 01:23:32,000
input of your model. Uh you had these

2107
01:23:30,239 --> 01:23:33,840
two different pairs, the green one and

2108
01:23:32,000 --> 01:23:35,280
the blue one. I didn't get what's the

2109
01:23:33,840 --> 01:23:37,600
difference.

2110
01:23:35,280 --> 01:23:39,360
>> I had Sorry. Oh. Oh. Um what's the input

2111
01:23:37,600 --> 01:23:40,320
to this? What's the difference between

2112
01:23:39,360 --> 01:23:45,199
um between

2113
01:23:40,320 --> 01:23:47,920
>> the order the Yeah. So um they in in

2114
01:23:45,199 --> 01:23:50,320
theory they should be symmetric um but

2115
01:23:47,920 --> 01:23:52,639
they're not because it's just this is a

2116
01:23:50,320 --> 01:23:56,560
this is the um alphaold 2 attention

2117
01:23:52,639 --> 01:24:00,560
basically between bait residue one let's

2118
01:23:56,560 --> 01:24:03,199
say and amino acid n in the protein and

2119
01:24:00,560 --> 01:24:06,000
then this is the same attention bait

2120
01:24:03,199 --> 01:24:07,520
residue one and amino acid n up here. Um

2121
01:24:06,000 --> 01:24:09,120
so they but they're not actually

2122
01:24:07,520 --> 01:24:11,120
symmetric in the model.

2123
01:24:09,120 --> 01:24:13,520
>> And so and so instead of so you could

2124
01:24:11,120 --> 01:24:16,400
imagine just um taking one of them or

2125
01:24:13,520 --> 01:24:19,520
the other and training a model just on

2126
01:24:16,400 --> 01:24:21,120
one of them. Um and if they were the

2127
01:24:19,520 --> 01:24:22,480
exact same that's what you would get.

2128
01:24:21,120 --> 01:24:25,520
You would just get the same model. But

2129
01:24:22,480 --> 01:24:27,040
they're not exactly the same features.

2130
01:24:25,520 --> 01:24:28,880
They're very similar. And I'll show

2131
01:24:27,040 --> 01:24:31,440
actually if you do PCA on the input

2132
01:24:28,880 --> 01:24:33,520
features you don't need all 5,000

2133
01:24:31,440 --> 01:24:35,040
input features for the predictions. Um,

2134
01:24:33,520 --> 01:24:36,239
but the the motivation here is we were

2135
01:24:35,040 --> 01:24:37,760
just concatenating that because they're

2136
01:24:36,239 --> 01:24:39,360
actually different numbers although

2137
01:24:37,760 --> 01:24:41,760
they're they're pretty similar.

2138
01:24:39,360 --> 01:24:43,440
>> And for example, if you randomize the

2139
01:24:41,760 --> 01:24:45,199
concatenation,

2140
01:24:43,440 --> 01:24:45,760
is the performance going down? Did you

2141
01:24:45,199 --> 01:24:47,360
try?

2142
01:24:45,760 --> 01:24:49,440
>> If you randomize what?

2143
01:24:47,360 --> 01:24:51,040
>> The concatenation. So first, if you have

2144
01:24:49,440 --> 01:24:52,320
the green one, the blue one, or the blue

2145
01:24:51,040 --> 01:24:55,520
one, the green one?

2146
01:24:52,320 --> 01:24:57,920
>> Um, if you randomize them, so you don't

2147
01:24:55,520 --> 01:25:00,400
necessarily want to randomize them. um

2148
01:24:57,920 --> 01:25:02,000
because where the position is sort of

2149
01:25:00,400 --> 01:25:03,600
well actually once you have once you

2150
01:25:02,000 --> 01:25:04,719
have this already built up it doesn't m

2151
01:25:03,600 --> 01:25:06,639
the only thing that goes into the

2152
01:25:04,719 --> 01:25:08,239
logistic regression model is slices. So

2153
01:25:06,639 --> 01:25:10,480
we actually do randomize in the sense

2154
01:25:08,239 --> 01:25:12,080
that we we take this and we break it up

2155
01:25:10,480 --> 01:25:14,719
into all the slices and we can train in

2156
01:25:12,080 --> 01:25:16,719
parallel each one of these red slices.

2157
01:25:14,719 --> 01:25:18,159
Um but um but if you're talking about

2158
01:25:16,719 --> 01:25:20,239
putting this in front of this instead of

2159
01:25:18,159 --> 01:25:21,440
this behind it, I don't think that would

2160
01:25:20,239 --> 01:25:23,120
that would matter.

2161
01:25:21,440 --> 01:25:24,960
>> Yeah, because I was thinking maybe of a

2162
01:25:23,120 --> 01:25:26,480
deep set neuronet network.

2163
01:25:24,960 --> 01:25:30,159
>> You were thinking what? of a deep set

2164
01:25:26,480 --> 01:25:32,320
neuronet network where you have the only

2165
01:25:30,159 --> 01:25:34,400
the embedding. So you have you have half

2166
01:25:32,320 --> 01:25:36,560
the embedding size and then for for

2167
01:25:34,400 --> 01:25:39,360
example you one hoting code if if it's

2168
01:25:36,560 --> 01:25:42,320
the blue one or the green one.

2169
01:25:39,360 --> 01:25:44,239
>> Yeah. Um yeah I'm not sure. So but I I

2170
01:25:42,320 --> 01:25:46,239
yeah uh in this case in this case it

2171
01:25:44,239 --> 01:25:47,280
wouldn't matter. Um so I think just for

2172
01:25:46,239 --> 01:25:49,679
the sake of time we're going to keep

2173
01:25:47,280 --> 01:25:52,080
going um because I have a lot to to get

2174
01:25:49,679 --> 01:25:53,760
through I guess. So um I'm not going to

2175
01:25:52,080 --> 01:25:55,360
go through this is just to say yeah I

2176
01:25:53,760 --> 01:25:56,560
mean this point here in the scatter plot

2177
01:25:55,360 --> 01:25:58,159
is just to show you that the pair

2178
01:25:56,560 --> 01:26:01,440
representation

2179
01:25:58,159 --> 01:26:02,480
does better than ESM2 and ESM1 inverse

2180
01:26:01,440 --> 01:26:03,840
folding. I'll tell you a little bit

2181
01:26:02,480 --> 01:26:06,000
about this metric recall. What the

2182
01:26:03,840 --> 01:26:08,080
recall is is a threshold independent

2183
01:26:06,000 --> 01:26:10,560
metric where you take all the residues

2184
01:26:08,080 --> 01:26:14,400
in the protein predict binding

2185
01:26:10,560 --> 01:26:16,159
probability and just rank them based on

2186
01:26:14,400 --> 01:26:18,560
largest residues with the largest

2187
01:26:16,159 --> 01:26:20,239
binding probability to the lowest. And

2188
01:26:18,560 --> 01:26:22,639
then you if you know that this protein

2189
01:26:20,239 --> 01:26:25,840
has n residues that are binding

2190
01:26:22,639 --> 01:26:27,520
residues, let's say it was 20 20 of the

2191
01:26:25,840 --> 01:26:30,560
200 residues in the protein are binding

2192
01:26:27,520 --> 01:26:33,120
residues. Then you look at the top 20

2193
01:26:30,560 --> 01:26:35,280
that were ranked by AF2 bind or by any

2194
01:26:33,120 --> 01:26:38,000
of these models and you say are these

2195
01:26:35,280 --> 01:26:41,360
top 20 ranked by probability of binding,

2196
01:26:38,000 --> 01:26:42,719
are they the true binding residues? What

2197
01:26:41,360 --> 01:26:43,920
fraction of those are true binding

2198
01:26:42,719 --> 01:26:47,760
residues? That's that's what we're

2199
01:26:43,920 --> 01:26:50,159
calling um recovery here. Uh which is

2200
01:26:47,760 --> 01:26:53,199
this this term here. But if you wanted

2201
01:26:50,159 --> 01:26:54,960
more um more machine learning like uh

2202
01:26:53,199 --> 01:26:57,840
metrics, the rock curves are there and

2203
01:26:54,960 --> 01:26:59,199
the precision recall curves. Um okay, so

2204
01:26:57,840 --> 01:27:01,440
I went through this, I went through

2205
01:26:59,199 --> 01:27:05,280
this, I went through that. Uh the other

2206
01:27:01,440 --> 01:27:07,360
thing to just note is that um we did PCA

2207
01:27:05,280 --> 01:27:11,840
on we did principal component analysis

2208
01:27:07,360 --> 01:27:13,600
on these um these these pair features uh

2209
01:27:11,840 --> 01:27:15,840
on the dimension of the 20 bait. So we

2210
01:27:13,600 --> 01:27:17,920
could stack all these examples and then

2211
01:27:15,840 --> 01:27:19,520
do PCA and say what is what is cor are

2212
01:27:17,920 --> 01:27:21,679
the baits correlated with one another?

2213
01:27:19,520 --> 01:27:22,880
Do you need all 20 amino acids? Could

2214
01:27:21,679 --> 01:27:25,280
you could you get away with this

2215
01:27:22,880 --> 01:27:27,840
prediction by doing three baits instead

2216
01:27:25,280 --> 01:27:30,719
of 20 bait residues? Um, and what the

2217
01:27:27,840 --> 01:27:32,320
PCA shows is that um, at least for 95%

2218
01:27:30,719 --> 01:27:34,400
of the variants, you need about 17 out

2219
01:27:32,320 --> 01:27:36,719
of the 20 baits. So, you do need a lot

2220
01:27:34,400 --> 01:27:38,639
of these bait residues. Um, the only

2221
01:27:36,719 --> 01:27:41,520
bait res residue that really didn't seem

2222
01:27:38,639 --> 01:27:44,400
to matter uh, much was cyine. Like I

2223
01:27:41,520 --> 01:27:46,719
guess we could drop out cyine. Um, alpha

2224
01:27:44,400 --> 01:27:49,199
2 doesn't care about cyine at all. Um,

2225
01:27:46,719 --> 01:27:50,960
at least in this prediction task. Uh,

2226
01:27:49,199 --> 01:27:53,760
and um, and if you look at the principal

2227
01:27:50,960 --> 01:27:55,360
component vectors, um, you can see which

2228
01:27:53,760 --> 01:27:56,639
which amino which bait residues are

2229
01:27:55,360 --> 01:27:57,840
firing with one another. first principal

2230
01:27:56,639 --> 01:28:00,880
component, everything's firing together

2231
01:27:57,840 --> 01:28:02,800
except for cyine. Um, and then the other

2232
01:28:00,880 --> 01:28:05,360
two, which do explain a fair amount of

2233
01:28:02,800 --> 01:28:07,040
the variance, it's kind of bio sort of

2234
01:28:05,360 --> 01:28:08,960
biochemically related. It makes sense

2235
01:28:07,040 --> 01:28:10,480
that the the bait residues that are

2236
01:28:08,960 --> 01:28:12,320
correlated on the s second principal

2237
01:28:10,480 --> 01:28:13,840
component vector are these polar ones

2238
01:28:12,320 --> 01:28:15,280
and the ones that are anti-correlated

2239
01:28:13,840 --> 01:28:17,280
with that are apolar. That's kind of

2240
01:28:15,280 --> 01:28:19,600
cool. And then for the third principal

2241
01:28:17,280 --> 01:28:22,000
component, the positive residues of the

2242
01:28:19,600 --> 01:28:23,520
baits are anti-correlated with negative.

2243
01:28:22,000 --> 01:28:25,440
So this all makes sense. So, we also did

2244
01:28:23,520 --> 01:28:27,360
a Spearman rank correlation just to see

2245
01:28:25,440 --> 01:28:29,600
which which pairs fire together. It's

2246
01:28:27,360 --> 01:28:31,199
sort of redundant to the PCA. Um, but

2247
01:28:29,600 --> 01:28:33,040
just for sake of time, I'm just going to

2248
01:28:31,199 --> 01:28:35,199
sort of breeze through this and just say

2249
01:28:33,040 --> 01:28:37,520
that the um, as you would expect, the

2250
01:28:35,199 --> 01:28:40,080
hydrophobic baits sort of fire together.

2251
01:28:37,520 --> 01:28:41,440
Um, and the polar ones fire together and

2252
01:28:40,080 --> 01:28:42,880
the negative charged ones fire together

2253
01:28:41,440 --> 01:28:44,400
and the positively charged ones fire

2254
01:28:42,880 --> 01:28:46,400
together. That's all that really is

2255
01:28:44,400 --> 01:28:48,639
saying. Um, and then here's sort of

2256
01:28:46,400 --> 01:28:50,000
related to your question earlier. Do you

2257
01:28:48,639 --> 01:28:52,719
do you need all these features? If you

2258
01:28:50,000 --> 01:28:55,120
do PCA over the feature dimension and

2259
01:28:52,719 --> 01:28:57,280
you ask how many of these 5,128 features

2260
01:28:55,120 --> 01:29:00,080
are needed uh to explain all the

2261
01:28:57,280 --> 01:29:04,080
variance um you only need a little bit

2262
01:29:00,080 --> 01:29:06,159
over a thousand features um so you only

2263
01:29:04,080 --> 01:29:09,280
need a thousand components of of of the

2264
01:29:06,159 --> 01:29:11,040
PCA in order to explain about 90 95% of

2265
01:29:09,280 --> 01:29:12,480
the variance. I mean we use them all

2266
01:29:11,040 --> 01:29:13,920
because the logistic regression model is

2267
01:29:12,480 --> 01:29:15,440
so fast. We don't need to make it faster

2268
01:29:13,920 --> 01:29:17,600
by doing dimensionality reduction with

2269
01:29:15,440 --> 01:29:19,920
PCA. Um, but it does it goes to show

2270
01:29:17,600 --> 01:29:22,320
that probably those probably could have

2271
01:29:19,920 --> 01:29:23,679
just dropped out one of the pairs

2272
01:29:22,320 --> 01:29:26,000
because then that would have dropped out

2273
01:29:23,679 --> 01:29:28,000
a couple of thousand of features and you

2274
01:29:26,000 --> 01:29:29,840
don't need more than a thousand features

2275
01:29:28,000 --> 01:29:31,440
really. Um, but could be that you need

2276
01:29:29,840 --> 01:29:34,080
linear combinations of features. So

2277
01:29:31,440 --> 01:29:36,000
that's anyway. So um, so that was that.

2278
01:29:34,080 --> 01:29:38,320
Uh, what's really interesting for for

2279
01:29:36,000 --> 01:29:40,000
actually deploying AF2 bind and why it

2280
01:29:38,320 --> 01:29:44,000
might be different than a model like P2

2281
01:29:40,000 --> 01:29:46,560
rank is that we didn't need side chains.

2282
01:29:44,000 --> 01:29:49,760
If you delete side chain information

2283
01:29:46,560 --> 01:29:51,280
from the input template and have AF2ine

2284
01:29:49,760 --> 01:29:52,960
predict which residue. So it still knows

2285
01:29:51,280 --> 01:29:55,040
the sequence. It just doesn't know the

2286
01:29:52,960 --> 01:29:57,840
dihedral angles of the rotors in the

2287
01:29:55,040 --> 01:29:59,040
protein. So we just erase that. But it

2288
01:29:57,840 --> 01:30:01,920
knows the sequence and it knows the

2289
01:29:59,040 --> 01:30:04,480
backbone. And if you compare its its

2290
01:30:01,920 --> 01:30:07,520
performance so if you're on this side of

2291
01:30:04,480 --> 01:30:10,320
the line then the side chainless

2292
01:30:07,520 --> 01:30:12,080
template does better for AF2 bind for

2293
01:30:10,320 --> 01:30:14,159
the prediction of binding residues. If

2294
01:30:12,080 --> 01:30:15,760
you're on this side of the line, then um

2295
01:30:14,159 --> 01:30:18,239
input that has side chains does a little

2296
01:30:15,760 --> 01:30:21,280
bit better. And you see it's basically a

2297
01:30:18,239 --> 01:30:23,120
diagonal line. So um it's it's sort of a

2298
01:30:21,280 --> 01:30:25,199
wash whether you use side chains or not.

2299
01:30:23,120 --> 01:30:27,440
Some cases not using side chains is

2300
01:30:25,199 --> 01:30:30,239
better. And so that's kind of nice

2301
01:30:27,440 --> 01:30:33,679
because a lot of small molecule binding

2302
01:30:30,239 --> 01:30:36,639
uh sites in proteins um maybe they

2303
01:30:33,679 --> 01:30:38,080
change a rotimer or two on average when

2304
01:30:36,639 --> 01:30:40,080
they bind a lian. Sometimes they don't

2305
01:30:38,080 --> 01:30:41,920
change rotimer at all side chains at

2306
01:30:40,080 --> 01:30:44,400
all. Um but a lot of if you want to use

2307
01:30:41,920 --> 01:30:46,400
predicted structures you know and you

2308
01:30:44,400 --> 01:30:48,480
don't have a lian at all good luck

2309
01:30:46,400 --> 01:30:50,239
getting the right side chains for lian

2310
01:30:48,480 --> 01:30:52,080
binding right so if you have a model

2311
01:30:50,239 --> 01:30:53,840
that ignores side chains it still makes

2312
01:30:52,080 --> 01:30:56,080
good predictions this could be quite

2313
01:30:53,840 --> 01:30:58,560
advantageous so so that was really nice

2314
01:30:56,080 --> 01:31:01,360
so we um this is just a summary of what

2315
01:30:58,560 --> 01:31:03,360
it does uh these are the inputs and then

2316
01:31:01,360 --> 01:31:05,679
predictions for the outputs so we use no

2317
01:31:03,360 --> 01:31:07,840
side chain dihedrals as well and then we

2318
01:31:05,679 --> 01:31:09,040
deployed this at scale and I'm going to

2319
01:31:07,840 --> 01:31:10,960
get into a little bit of the results

2320
01:31:09,040 --> 01:31:13,920
none of this is I think really in the

2321
01:31:10,960 --> 01:31:15,760
bioarchchive paper. So we took the AF2

2322
01:31:13,920 --> 01:31:18,239
predicted human proteom structures which

2323
01:31:15,760 --> 01:31:22,800
is all single chain structures of

2324
01:31:18,239 --> 01:31:26,159
proteins um and uh and a lot but we took

2325
01:31:22,800 --> 01:31:28,239
those and we we ran AF2 bind on it. Um a

2326
01:31:26,159 --> 01:31:30,480
lot of human proteins are multi-chain a

2327
01:31:28,239 --> 01:31:33,280
lot of them are homoygr bind to other

2328
01:31:30,480 --> 01:31:34,639
things um and all this database is

2329
01:31:33,280 --> 01:31:37,280
single chain structures. So you could

2330
01:31:34,639 --> 01:31:39,440
you could imagine asking you know are

2331
01:31:37,280 --> 01:31:42,239
any of these structures relevant you

2332
01:31:39,440 --> 01:31:44,400
know to real proteins in in cells in

2333
01:31:42,239 --> 01:31:46,159
humans. Um and it turns out that a lot

2334
01:31:44,400 --> 01:31:47,679
of these single chain structures they

2335
01:31:46,159 --> 01:31:51,280
when you compare them to structures of

2336
01:31:47,679 --> 01:31:52,960
multi-chain proteins real structures the

2337
01:31:51,280 --> 01:31:55,920
predictive single chain structures look

2338
01:31:52,960 --> 01:31:58,159
like they look than when they're in the

2339
01:31:55,920 --> 01:31:59,600
multi-chain protein. And a lot of that

2340
01:31:58,159 --> 01:32:00,960
is because it's dealing with multiple

2341
01:31:59,600 --> 01:32:02,719
sequence alignments.

2342
01:32:00,960 --> 01:32:04,159
So there's a whole lot of subtleties of

2343
01:32:02,719 --> 01:32:06,080
why that might be what I don't have

2344
01:32:04,159 --> 01:32:08,239
really time to talk about right now. Um

2345
01:32:06,080 --> 01:32:09,280
but just take it take it for granted

2346
01:32:08,239 --> 01:32:10,560
right now that a lot of these single

2347
01:32:09,280 --> 01:32:12,320
chain structures actually resemble

2348
01:32:10,560 --> 01:32:14,880
multi-chain if there are multi-chain

2349
01:32:12,320 --> 01:32:17,280
proteins. Um and then in the human

2350
01:32:14,880 --> 01:32:19,280
proteome some of the some of the

2351
01:32:17,280 --> 01:32:21,840
proteins have low confidence from alpha.

2352
01:32:19,280 --> 01:32:23,679
We trim those off basically and we just

2353
01:32:21,840 --> 01:32:25,600
deal with the higher confidence bits of

2354
01:32:23,679 --> 01:32:28,480
the predictions. So that becomes the

2355
01:32:25,600 --> 01:32:29,840
input to AF2 bind. And uh then we just

2356
01:32:28,480 --> 01:32:31,199
we just say okay what are the binding

2357
01:32:29,840 --> 01:32:34,880
pockets that AF2 binds? What are the

2358
01:32:31,199 --> 01:32:36,480
binding sites um that AF2 bind finds. Um

2359
01:32:34,880 --> 01:32:38,080
and but first I want to say because

2360
01:32:36,480 --> 01:32:40,480
somebody was asking me this was asking

2361
01:32:38,080 --> 01:32:43,520
me this earlier is if you just stick any

2362
01:32:40,480 --> 01:32:45,280
really large protein into AF2 bind it

2363
01:32:43,520 --> 01:32:48,480
was only trained on proteins up to 500

2364
01:32:45,280 --> 01:32:50,560
amino acids. Um and the the pair

2365
01:32:48,480 --> 01:32:52,719
representation is there's a soft max

2366
01:32:50,560 --> 01:32:55,199
over that. the embeddings are normalized

2367
01:32:52,719 --> 01:32:56,960
and so the bigger the protein the lower

2368
01:32:55,199 --> 01:32:59,920
the magnitude of the embeddings

2369
01:32:56,960 --> 01:33:01,440
basically um but then this but AF2 bind

2370
01:32:59,920 --> 01:33:02,880
was trained for a certain magnitude of

2371
01:33:01,440 --> 01:33:05,760
the embeddings because the size of the

2372
01:33:02,880 --> 01:33:07,760
proteins was on average 300 amino acids

2373
01:33:05,760 --> 01:33:09,520
at most 500 amino acids but these

2374
01:33:07,760 --> 01:33:11,600
proteins can be thousands of amino acids

2375
01:33:09,520 --> 01:33:14,000
thousand or more and putting and if you

2376
01:33:11,600 --> 01:33:15,679
had a single threshold for the

2377
01:33:14,000 --> 01:33:17,040
probability of binding above which

2378
01:33:15,679 --> 01:33:19,199
you're saying this is a real binding

2379
01:33:17,040 --> 01:33:21,040
site below which you're saying this is

2380
01:33:19,199 --> 01:33:23,120
not a real binding site

2381
01:33:21,040 --> 01:33:25,199
And if if that threshold basically

2382
01:33:23,120 --> 01:33:26,800
depends on the size of the protein, it's

2383
01:33:25,199 --> 01:33:28,960
going to be harder to actually use this

2384
01:33:26,800 --> 01:33:30,320
and make real good predictions. And this

2385
01:33:28,960 --> 01:33:34,159
is an example. So if you take this

2386
01:33:30,320 --> 01:33:36,880
protein and you plug this into AF2

2387
01:33:34,159 --> 01:33:39,760
vanilla, we'll call it um the model as

2388
01:33:36,880 --> 01:33:41,679
we trained it, you see some weak blue

2389
01:33:39,760 --> 01:33:43,679
color here. And this is colored by the

2390
01:33:41,679 --> 01:33:46,000
probability of binding. So you can

2391
01:33:43,679 --> 01:33:47,040
almost say there's no binding sites, but

2392
01:33:46,000 --> 01:33:48,239
we know that there's binding sites in

2393
01:33:47,040 --> 01:33:50,000
here. Here are the binding sites

2394
01:33:48,239 --> 01:33:51,280
protein, right? And the question is how

2395
01:33:50,000 --> 01:33:53,040
can we recover this kind of thing

2396
01:33:51,280 --> 01:33:55,760
without you know how do we deal with

2397
01:33:53,040 --> 01:33:57,520
this dilution problem of pair embeddings

2398
01:33:55,760 --> 01:33:59,360
what's the so we thought about this a

2399
01:33:57,520 --> 01:34:00,880
lot and the the the best solution we

2400
01:33:59,360 --> 01:34:03,440
came up with actually is to split the

2401
01:34:00,880 --> 01:34:06,239
big proteins into domains and you can

2402
01:34:03,440 --> 01:34:08,880
use uh um you know already you know

2403
01:34:06,239 --> 01:34:10,800
computed domain labels from eod or you

2404
01:34:08,880 --> 01:34:12,719
could use a domain parser yourself. So

2405
01:34:10,800 --> 01:34:15,360
you split these into domains and then

2406
01:34:12,719 --> 01:34:17,280
you can do the um computations on on

2407
01:34:15,360 --> 01:34:19,199
different domains. So, so basically if

2408
01:34:17,280 --> 01:34:20,560
you have you know domains that are 300

2409
01:34:19,199 --> 01:34:22,719
residues or more is do the computation

2410
01:34:20,560 --> 01:34:23,840
on that and then any pair of contacting

2411
01:34:22,719 --> 01:34:25,120
domains

2412
01:34:23,840 --> 01:34:26,639
>> you can do the computation.

2413
01:34:25,120 --> 01:34:27,040
>> How would you decide the domain?

2414
01:34:26,639 --> 01:34:28,480
>> Sorry,

2415
01:34:27,040 --> 01:34:28,960
>> how would you decide which domain to

2416
01:34:28,480 --> 01:34:31,199
split?

2417
01:34:28,960 --> 01:34:32,880
>> How do you decide which domain? So um so

2418
01:34:31,199 --> 01:34:36,960
in this case because we did all human

2419
01:34:32,880 --> 01:34:38,639
proteins um the uh people who made um

2420
01:34:36,960 --> 01:34:41,040
ECOD which is the evolutionary

2421
01:34:38,639 --> 01:34:43,440
classification of domains actually have

2422
01:34:41,040 --> 01:34:45,280
already split all of the alphold 2

2423
01:34:43,440 --> 01:34:46,800
predicted structures into domains. So we

2424
01:34:45,280 --> 01:34:48,400
could just take a list and split them

2425
01:34:46,800 --> 01:34:49,679
that way. But if you if you didn't have

2426
01:34:48,400 --> 01:34:51,360
that list and you wanted your your

2427
01:34:49,679 --> 01:34:52,880
arbitrary protein to split it in

2428
01:34:51,360 --> 01:34:54,960
domains, you can use domain par. There's

2429
01:34:52,880 --> 01:34:57,360
a lot of domain parsers out there um

2430
01:34:54,960 --> 01:34:58,960
that are really fast and pretty accurate

2431
01:34:57,360 --> 01:35:00,560
um that are you know neural networks and

2432
01:34:58,960 --> 01:35:03,120
non-neural network based that you could

2433
01:35:00,560 --> 01:35:05,280
do. Um so this is the idea is that um we

2434
01:35:03,120 --> 01:35:07,520
we would then just do AF2 bind

2435
01:35:05,280 --> 01:35:09,600
predictions on these pairs and then we

2436
01:35:07,520 --> 01:35:11,280
would take the maximum probability for

2437
01:35:09,600 --> 01:35:14,960
any residue as the probability of

2438
01:35:11,280 --> 01:35:17,440
binding. We would zero out any um

2439
01:35:14,960 --> 01:35:19,199
residues that um when you look at them

2440
01:35:17,440 --> 01:35:21,280
in the full structure if they have no

2441
01:35:19,199 --> 01:35:23,760
solvent accessible surface area at all

2442
01:35:21,280 --> 01:35:25,440
and you remove the domain and now there

2443
01:35:23,760 --> 01:35:27,520
was a lot of solvent accessible surface

2444
01:35:25,440 --> 01:35:29,760
area it's unrealistic and AF2ine

2445
01:35:27,520 --> 01:35:31,920
predicts a binding site there well we

2446
01:35:29,760 --> 01:35:33,520
zero that out because in the in the full

2447
01:35:31,920 --> 01:35:35,600
in the full structure it would be

2448
01:35:33,520 --> 01:35:37,199
completely buried so if you take so

2449
01:35:35,600 --> 01:35:39,679
that's the one caveat maybe we could

2450
01:35:37,199 --> 01:35:41,679
miss some sites that way um but on in

2451
01:35:39,679 --> 01:35:43,440
practice I don't think we miss very

2452
01:35:41,679 --> 01:35:45,440
Um and so then we just take the maximum

2453
01:35:43,440 --> 01:35:47,679
binding probability across all for every

2454
01:35:45,440 --> 01:35:49,280
residue across these computations and

2455
01:35:47,679 --> 01:35:50,719
then we combine that into the one

2456
01:35:49,280 --> 01:35:53,040
structure. And so now you can see that

2457
01:35:50,719 --> 01:35:55,920
we do from that analysis we do recover

2458
01:35:53,040 --> 01:35:58,320
these true binding sites with high

2459
01:35:55,920 --> 01:36:00,400
binding probability. So this is what we

2460
01:35:58,320 --> 01:36:02,719
did to deploy AF2 bind across the whole

2461
01:36:00,400 --> 01:36:04,320
human proteome for structures that were

2462
01:36:02,719 --> 01:36:06,480
bigger than 500 amino acids. We would

2463
01:36:04,320 --> 01:36:09,360
split them up into into domains and then

2464
01:36:06,480 --> 01:36:11,040
do this. Um, okay. So, what does it

2465
01:36:09,360 --> 01:36:12,239
find? And what does it I think what does

2466
01:36:11,040 --> 01:36:13,440
it find? Well, what does it find more

2467
01:36:12,239 --> 01:36:15,760
importantly compared to what other

2468
01:36:13,440 --> 01:36:18,480
methods find already? Because you could

2469
01:36:15,760 --> 01:36:20,320
use P2 rank, you could use alpha fill to

2470
01:36:18,480 --> 01:36:21,840
look at the whole human proteome and

2471
01:36:20,320 --> 01:36:23,760
then you could use AF2 bind. And why

2472
01:36:21,840 --> 01:36:25,120
would we want to use AF2 bind? So,

2473
01:36:23,760 --> 01:36:26,960
ideally, you find something that the

2474
01:36:25,120 --> 01:36:29,520
other ones don't, right? And so, here's

2475
01:36:26,960 --> 01:36:31,760
a nice ven diagram of using those three

2476
01:36:29,520 --> 01:36:34,560
methods on the whole AF2 predicted human

2477
01:36:31,760 --> 01:36:36,880
proteome. Um and you can see that um

2478
01:36:34,560 --> 01:36:40,719
AF2bind has this are this is the number

2479
01:36:36,880 --> 01:36:43,360
of binding sites found in uh in in in

2480
01:36:40,719 --> 01:36:45,600
the human protein

2481
01:36:43,360 --> 01:36:47,840
sites that that P2 rank does not and

2482
01:36:45,600 --> 01:36:50,239
that alphaill does not and there's some

2483
01:36:47,840 --> 01:36:51,840
overlap between the two. Um so it's

2484
01:36:50,239 --> 01:36:53,679
finding very unique sites. The ones on

2485
01:36:51,840 --> 01:36:56,159
the right here I'm showing you are AF2

2486
01:36:53,679 --> 01:36:58,400
bindon sites that were not found by

2487
01:36:56,159 --> 01:36:59,679
alphaill. So the ones some some ones in

2488
01:36:58,400 --> 01:37:02,320
this blue region. They're not found by

2489
01:36:59,679 --> 01:37:04,000
alphaill, so by homology modeling, and

2490
01:37:02,320 --> 01:37:06,960
they're not found by P2 rank, which is

2491
01:37:04,000 --> 01:37:08,480
more of a classic classifier. Um, and

2492
01:37:06,960 --> 01:37:11,520
you could say, okay, are these binding

2493
01:37:08,480 --> 01:37:14,000
sites um relevant to drug discovery at

2494
01:37:11,520 --> 01:37:15,520
all? How druggable are they? Um, and so

2495
01:37:14,000 --> 01:37:17,280
you could take all the binding sites

2496
01:37:15,520 --> 01:37:20,239
that were predicted by, for example, P2

2497
01:37:17,280 --> 01:37:22,880
rank and AF2 bind, and you can classify

2498
01:37:20,239 --> 01:37:25,360
them in a another software, Schroinger's

2499
01:37:22,880 --> 01:37:28,560
sitemap software, which comes out with a

2500
01:37:25,360 --> 01:37:30,880
drugability score called Dcore. And um

2501
01:37:28,560 --> 01:37:32,159
and so drug ability

2502
01:37:30,880 --> 01:37:34,480
empirically it's been found if you have

2503
01:37:32,159 --> 01:37:37,280
a score above like 0.8 for drugability

2504
01:37:34,480 --> 01:37:39,520
maybe 082 or something um that for the

2505
01:37:37,280 --> 01:37:41,280
dcore in in Schroinger sitemap that it's

2506
01:37:39,520 --> 01:37:43,040
a pretty druggable pocket. Drugability

2507
01:37:41,280 --> 01:37:45,760
is basically how enclosed it is and how

2508
01:37:43,040 --> 01:37:48,000
hydrophobic it is. Um and so you can see

2509
01:37:45,760 --> 01:37:51,520
that on average the um the binding sites

2510
01:37:48,000 --> 01:37:54,239
from AF2 bind versus P2 rank are very

2511
01:37:51,520 --> 01:37:56,880
druggable above this threshold. And this

2512
01:37:54,239 --> 01:37:59,760
is um this is the morbid map database

2513
01:37:56,880 --> 01:38:02,080
here uh PT rank versus AF2 bind. So this

2514
01:37:59,760 --> 01:38:04,080
is actually about 4,000 5,000 proteins

2515
01:38:02,080 --> 01:38:06,400
in the human proteome that are known to

2516
01:38:04,080 --> 01:38:07,920
be involved in disease. And uh and so

2517
01:38:06,400 --> 01:38:09,600
you can see it's also finding sites in

2518
01:38:07,920 --> 01:38:12,320
that and it does find unique sites in

2519
01:38:09,600 --> 01:38:13,760
those proteins that P2 rank and alph

2520
01:38:12,320 --> 01:38:15,520
don't don't find which I'm not showing

2521
01:38:13,760 --> 01:38:17,520
here. So it's it's finding unique sites

2522
01:38:15,520 --> 01:38:18,880
which is really cool. If you look at the

2523
01:38:17,520 --> 01:38:21,440
protein families that are the most

2524
01:38:18,880 --> 01:38:24,080
druggable by AF2 bind these protein

2525
01:38:21,440 --> 01:38:25,520
families sort of make sense. You have um

2526
01:38:24,080 --> 01:38:29,440
I'm just I'm just pointing out a few

2527
01:38:25,520 --> 01:38:31,600
here. You've got um um these these uh

2528
01:38:29,440 --> 01:38:33,679
globin-l like proteins, these alenil

2529
01:38:31,600 --> 01:38:35,360
transfer proteins. GPCRs are up there.

2530
01:38:33,679 --> 01:38:37,520
I'm just not highlighting them with the

2531
01:38:35,360 --> 01:38:38,800
picture. Um tetraaspanins, a lot of

2532
01:38:37,520 --> 01:38:40,159
different proteins out there that you

2533
01:38:38,800 --> 01:38:42,480
would say, okay, yeah, I know these are

2534
01:38:40,159 --> 01:38:43,840
druggable. So, so it's making sense, but

2535
01:38:42,480 --> 01:38:46,320
then it finds proteins that you wouldn't

2536
01:38:43,840 --> 01:38:47,920
think about. Um which um I don't really

2537
01:38:46,320 --> 01:38:50,719
have time to highlight too much in this

2538
01:38:47,920 --> 01:38:53,440
talk. Uh the last thing I want to say

2539
01:38:50,719 --> 01:38:55,920
about the quality of the binding pockets

2540
01:38:53,440 --> 01:38:57,760
is um and I'm comparing them AF2 bind

2541
01:38:55,920 --> 01:39:00,080
and P2 rank. So AF2 bind is in blue and

2542
01:38:57,760 --> 01:39:02,560
P2 rank is in orange and these are all

2543
01:39:00,080 --> 01:39:04,800
the um features that that are output by

2544
01:39:02,560 --> 01:39:06,400
Schroinger sitemap. And I want to just

2545
01:39:04,800 --> 01:39:08,960
highlight the couple of features that

2546
01:39:06,400 --> 01:39:11,679
are different maybe strikingly different

2547
01:39:08,960 --> 01:39:13,760
uh between AF2bind versus P2 rank and

2548
01:39:11,679 --> 01:39:15,600
AF2bine has less on average a little bit

2549
01:39:13,760 --> 01:39:17,760
less enclosure of the pocket. So

2550
01:39:15,600 --> 01:39:19,040
shallower pockets, exposure of course

2551
01:39:17,760 --> 01:39:21,280
being higher and enclosure being a

2552
01:39:19,040 --> 01:39:22,960
little lower than sites in P2 rank. And

2553
01:39:21,280 --> 01:39:24,560
I want to point out a couple of, you

2554
01:39:22,960 --> 01:39:26,800
know, anecdotal examples of where that

2555
01:39:24,560 --> 01:39:28,239
might be advantageous that you have a

2556
01:39:26,800 --> 01:39:30,320
little bit shallower pockets that are

2557
01:39:28,239 --> 01:39:32,480
predicted by AF2 bind. Here's a few

2558
01:39:30,320 --> 01:39:33,600
examples is that these are missed by P2

2559
01:39:32,480 --> 01:39:35,040
rank there doesn't even predict

2560
01:39:33,600 --> 01:39:37,440
anything. There's P2 rank says there's

2561
01:39:35,040 --> 01:39:39,840
no site here. AF2 bind says yes, this is

2562
01:39:37,440 --> 01:39:41,440
a site. Here it is in purple. And this

2563
01:39:39,840 --> 01:39:42,960
is this is on the AF2 predicted

2564
01:39:41,440 --> 01:39:44,159
structures. But in these in this case,

2565
01:39:42,960 --> 01:39:47,440
there were actually crystal structures

2566
01:39:44,159 --> 01:39:49,199
of these proteins in the PDB. And so you

2567
01:39:47,440 --> 01:39:51,280
could look and say, well, okay, what

2568
01:39:49,199 --> 01:39:53,280
what lians are binding there? And in one

2569
01:39:51,280 --> 01:39:57,360
case, this is a part of tomeorase, I

2570
01:39:53,280 --> 01:40:00,080
think. And um and so it's binding to to

2571
01:39:57,360 --> 01:40:02,719
um to RNA. This is binding to DNA. This

2572
01:40:00,080 --> 01:40:06,080
is a transcription factor. Um, and then

2573
01:40:02,719 --> 01:40:11,040
this is a protein that binds to the um

2574
01:40:06,080 --> 01:40:13,199
oh uh the caps of of uh uh microtubules

2575
01:40:11,040 --> 01:40:14,560
actually. And so it's binding a peptide

2576
01:40:13,199 --> 01:40:16,080
up there in science. There's a few

2577
01:40:14,560 --> 01:40:18,080
residues of the peptide that are binding

2578
01:40:16,080 --> 01:40:20,400
into this pocket. And in this case,

2579
01:40:18,080 --> 01:40:22,960
there's RNA and DNA. You say, well, you

2580
01:40:20,400 --> 01:40:24,880
train this on small molecules. Why is it

2581
01:40:22,960 --> 01:40:28,480
predicting

2582
01:40:24,880 --> 01:40:31,679
peptide and RNA and DNA binding sites?

2583
01:40:28,480 --> 01:40:34,639
And my my answer to that, you know, my

2584
01:40:31,679 --> 01:40:38,400
philosophical answer to that is that

2585
01:40:34,639 --> 01:40:39,920
AF2bind is using AF2 embeddings

2586
01:40:38,400 --> 01:40:41,199
and then it's training a logistic

2587
01:40:39,920 --> 01:40:43,679
regression model to predict small

2588
01:40:41,199 --> 01:40:48,320
molecule binding pockets in so far as

2589
01:40:43,679 --> 01:40:52,320
those embeddings themselves from AF2

2590
01:40:48,320 --> 01:40:55,840
um uh in so far as they describe

2591
01:40:52,320 --> 01:40:58,800
small molecule or RNA or DNA or other

2592
01:40:55,840 --> 01:41:00,639
polymer binding then AF2 two bind is

2593
01:40:58,800 --> 01:41:02,000
going to predict that they're just the

2594
01:41:00,639 --> 01:41:03,920
same input AF2 bind. They're just going

2595
01:41:02,000 --> 01:41:07,119
to predict that that's small molecule

2596
01:41:03,920 --> 01:41:09,280
binding character if you will. And so

2597
01:41:07,119 --> 01:41:14,080
and so what what's really I think

2598
01:41:09,280 --> 01:41:15,440
tanalyzing about AF2 bind is you a

2599
01:41:14,080 --> 01:41:18,239
medicinal chemist might tell you that

2600
01:41:15,440 --> 01:41:20,400
these are not druggable sites. They bind

2601
01:41:18,239 --> 01:41:21,840
to DNA, they bind to RNA, they bind to

2602
01:41:20,400 --> 01:41:24,480
and we can't get a small molecule in

2603
01:41:21,840 --> 01:41:26,159
there and a lot of them aren't. But AF2

2604
01:41:24,480 --> 01:41:27,840
bind doesn't predict every DNA binding

2605
01:41:26,159 --> 01:41:30,239
site. It doesn't predict every RNA

2606
01:41:27,840 --> 01:41:32,000
binding site. It just is predicting

2607
01:41:30,239 --> 01:41:33,199
these ones. And and in my view, it's

2608
01:41:32,000 --> 01:41:35,440
because they they probably have some

2609
01:41:33,199 --> 01:41:38,159
small molecule binding character that it

2610
01:41:35,440 --> 01:41:39,440
might motivate somebody who's doing drug

2611
01:41:38,159 --> 01:41:41,040
discovery to say, I'm going to try to

2612
01:41:39,440 --> 01:41:43,760
find a small molecule that inhibits this

2613
01:41:41,040 --> 01:41:45,119
site. Um, and so it it did actually, if

2614
01:41:43,760 --> 01:41:47,440
you if you think about that for protein

2615
01:41:45,119 --> 01:41:48,880
protein interactions, for protein

2616
01:41:47,440 --> 01:41:50,400
protein interactions, these are also

2617
01:41:48,880 --> 01:41:52,400
considered somewhat undruggable because

2618
01:41:50,400 --> 01:41:54,480
they're very shallow interfaces. How do

2619
01:41:52,400 --> 01:41:56,880
you fit a small molecule there? These

2620
01:41:54,480 --> 01:42:00,960
are again sites that P2 rank wouldn't

2621
01:41:56,880 --> 01:42:03,600
say are um our binding pockets. And um

2622
01:42:00,960 --> 01:42:05,440
but nevertheless, here's here's some uh

2623
01:42:03,600 --> 01:42:07,760
proteins that are involved in protein

2624
01:42:05,440 --> 01:42:10,960
protein interfaces. I'm not showing the

2625
01:42:07,760 --> 01:42:12,639
protein partner here. Um but AF2bind in

2626
01:42:10,960 --> 01:42:14,800
purple predicts binding pockets for

2627
01:42:12,639 --> 01:42:16,639
these proteins. And there are crystal

2628
01:42:14,800 --> 01:42:17,600
structures. This is just using AF2

2629
01:42:16,639 --> 01:42:19,920
predicted structure, but there are

2630
01:42:17,600 --> 01:42:21,679
crystal structures of these proteins

2631
01:42:19,920 --> 01:42:24,560
where small molecule inhibitors have

2632
01:42:21,679 --> 01:42:26,000
been found that do inhibit the protein

2633
01:42:24,560 --> 01:42:27,840
protein interaction. So it's it's

2634
01:42:26,000 --> 01:42:30,560
actually finding that these are small

2635
01:42:27,840 --> 01:42:32,639
molecule bindable

2636
01:42:30,560 --> 01:42:34,400
uh protein protein binding interfaces.

2637
01:42:32,639 --> 01:42:36,480
It doesn't. So I have check mark check

2638
01:42:34,400 --> 01:42:40,639
mark and X here. It didn't work for this

2639
01:42:36,480 --> 01:42:43,920
one. This is a very shallow. This is 2

2640
01:42:40,639 --> 01:42:45,360
and this is a very shallow site. Um

2641
01:42:43,920 --> 01:42:46,960
there are actually instant this is a

2642
01:42:45,360 --> 01:42:50,000
microar inhibitor but there are some

2643
01:42:46,960 --> 01:42:51,760
nanomolar inhibitors there too. So it so

2644
01:42:50,000 --> 01:42:53,199
it missed that one. Okay it's not a

2645
01:42:51,760 --> 01:42:55,199
perfect thing but it's not it's not

2646
01:42:53,199 --> 01:42:56,960
really trained to do this. So the fact

2647
01:42:55,199 --> 01:42:59,679
that it does pick up two out of these

2648
01:42:56,960 --> 01:43:02,560
three I think is pretty tanalyzing and

2649
01:42:59,679 --> 01:43:03,840
one of the reasons why AF2 bind might be

2650
01:43:02,560 --> 01:43:06,320
giving a little bit more information

2651
01:43:03,840 --> 01:43:09,360
than a classic binding site predictor

2652
01:43:06,320 --> 01:43:11,600
model. Um I'm not sure how I think I

2653
01:43:09,360 --> 01:43:12,880
might be going way over time but um I

2654
01:43:11,600 --> 01:43:14,719
have a few more slides that just show

2655
01:43:12,880 --> 01:43:17,840
you a couple of tanalyzing features of

2656
01:43:14,719 --> 01:43:19,679
of AF2 binding and and one one slide is

2657
01:43:17,840 --> 01:43:23,840
this that if you look at big

2658
01:43:19,679 --> 01:43:25,520
confirmational changes between proteins.

2659
01:43:23,840 --> 01:43:27,760
So a lot of some proteins actually

2660
01:43:25,520 --> 01:43:30,000
undergo really big confirmational

2661
01:43:27,760 --> 01:43:31,280
changes when they bind a link. Is that

2662
01:43:30,000 --> 01:43:32,800
really important? I get this question

2663
01:43:31,280 --> 01:43:35,520
before. Is that really important? Could

2664
01:43:32,800 --> 01:43:37,600
AF2 mind pick that up? Um and in in

2665
01:43:35,520 --> 01:43:40,960
these cases it does. So I'm showing

2666
01:43:37,600 --> 01:43:42,560
three examples of proteins that calm

2667
01:43:40,960 --> 01:43:44,400
modulin multiples binding protein deny

2668
01:43:42,560 --> 01:43:46,159
kynise that have big confirmational

2669
01:43:44,400 --> 01:43:47,840
changes when they bind a small molecule

2670
01:43:46,159 --> 01:43:51,600
and in both cases the open and the

2671
01:43:47,840 --> 01:43:53,760
closed case um AF2bine is predicting the

2672
01:43:51,600 --> 01:43:55,199
binding pocket. It doesn't work for

2673
01:43:53,760 --> 01:43:57,840
everything. I mean these are open

2674
01:43:55,199 --> 01:44:00,159
binding pockets in both cases basically.

2675
01:43:57,840 --> 01:44:02,880
Um but if you have I'm g skip this for

2676
01:44:00,159 --> 01:44:04,639
now. If you have a binding pocket and

2677
01:44:02,880 --> 01:44:07,040
Ben showed this in his talk that's

2678
01:44:04,639 --> 01:44:10,800
completely cryptic and collapsed where

2679
01:44:07,040 --> 01:44:11,840
there's no pocket um AF2 bind does pred

2680
01:44:10,800 --> 01:44:13,520
I mean there's actually another small

2681
01:44:11,840 --> 01:44:15,520
molecule binding site here for

2682
01:44:13,520 --> 01:44:16,800
betalacttomies and it does predict that

2683
01:44:15,520 --> 01:44:19,520
but it doesn't predict this cryptic

2684
01:44:16,800 --> 01:44:20,800
pocket here which does open up when

2685
01:44:19,520 --> 01:44:21,920
there's a structure in the PDB where a

2686
01:44:20,800 --> 01:44:23,920
small molecule binds in between these

2687
01:44:21,920 --> 01:44:25,440
two helyses and so it predicts it it

2688
01:44:23,920 --> 01:44:27,760
predicts that pocket when you give it

2689
01:44:25,440 --> 01:44:29,440
the open structure when you give it the

2690
01:44:27,760 --> 01:44:31,760
the ligan bound structure it predicts

2691
01:44:29,440 --> 01:44:32,960
the pocket But if you didn't know that,

2692
01:44:31,760 --> 01:44:34,639
it wouldn't it wouldn't have told you

2693
01:44:32,960 --> 01:44:36,880
that because this is collapsed. So it

2694
01:44:34,639 --> 01:44:38,480
doesn't work in every case when the

2695
01:44:36,880 --> 01:44:40,880
structures are changing very much. But

2696
01:44:38,480 --> 01:44:42,080
it does work in in many cases. And I got

2697
01:44:40,880 --> 01:44:44,000
we got this question before about

2698
01:44:42,080 --> 01:44:46,239
alossteric sites. So here's the data

2699
01:44:44,000 --> 01:44:48,880
about alossteric sites again not in the

2700
01:44:46,239 --> 01:44:51,600
bioarchchive paper but will be in the um

2701
01:44:48,880 --> 01:44:55,360
the accepted paper that's coming out

2702
01:44:51,600 --> 01:44:58,000
soon. Uh is is we took there's about uh

2703
01:44:55,360 --> 01:45:00,000
50 to 60 proteins that are labeled as

2704
01:44:58,000 --> 01:45:03,040
alossteric site binders.

2705
01:45:00,000 --> 01:45:05,119
alossteric site. The PDB is dominated by

2706
01:45:03,040 --> 01:45:07,119
ligans that bind into so-cal orthosteric

2707
01:45:05,119 --> 01:45:09,679
sites, functional sites that are are

2708
01:45:07,119 --> 01:45:12,880
conserved and and shared between

2709
01:45:09,679 --> 01:45:14,719
proteins um of similar nature and

2710
01:45:12,880 --> 01:45:16,800
alossteric sites might be less conserved

2711
01:45:14,719 --> 01:45:20,480
and and those might be ones that people

2712
01:45:16,800 --> 01:45:24,480
want to drug um but those are only 1% of

2713
01:45:20,480 --> 01:45:27,280
the PDB has examples of alossteric sites

2714
01:45:24,480 --> 01:45:30,400
and 1% of the AF2 bind training set as

2715
01:45:27,280 --> 01:45:33,440
well. a small training set still 1% had

2716
01:45:30,400 --> 01:45:35,040
had alsteric sites and the question was

2717
01:45:33,440 --> 01:45:37,360
could AF2 bind actually predict these

2718
01:45:35,040 --> 01:45:39,119
alossteric sites um even though it was

2719
01:45:37,360 --> 01:45:40,480
barely trained on any of them I mean it

2720
01:45:39,119 --> 01:45:42,480
wasn't trained on any of them in that

2721
01:45:40,480 --> 01:45:45,119
we're testing out here um but it only

2722
01:45:42,480 --> 01:45:47,840
saw like one or two during training and

2723
01:45:45,119 --> 01:45:50,880
um and so here you can see that it

2724
01:45:47,840 --> 01:45:54,239
actually it can do that which is pretty

2725
01:45:50,880 --> 01:45:56,400
cool um that that AF2ine can do this uh

2726
01:45:54,239 --> 01:45:57,679
and so here it's showing that there's

2727
01:45:56,400 --> 01:45:59,040
some sites that are recover covered

2728
01:45:57,679 --> 01:46:01,199
versus some that are missed. It covers

2729
01:45:59,040 --> 01:46:04,080
about twothirds of the alisteric sites.

2730
01:46:01,199 --> 01:46:05,840
It doesn't really depend on the RMSD

2731
01:46:04,080 --> 01:46:08,080
uh between the crystal structure of the

2732
01:46:05,840 --> 01:46:10,000
alossteric site and the alphafold 2

2733
01:46:08,080 --> 01:46:11,760
predicted structure that we ran AF2 bind

2734
01:46:10,000 --> 01:46:13,760
on. We didn't run it on the on the

2735
01:46:11,760 --> 01:46:14,880
answer. We ran it on an AF2 prediction

2736
01:46:13,760 --> 01:46:16,800
and we asked can it predict the

2737
01:46:14,880 --> 01:46:18,719
alossteric site. Here's an example of

2738
01:46:16,800 --> 01:46:22,159
one of these where it's nowic inhibitor

2739
01:46:18,719 --> 01:46:25,600
is bound. Um and this is the AF2

2740
01:46:22,159 --> 01:46:27,360
predicted structure. But um but AF2 bind

2741
01:46:25,600 --> 01:46:29,920
predicts the alossteric site very purple

2742
01:46:27,360 --> 01:46:32,080
here. And if you overlay the the

2743
01:46:29,920 --> 01:46:34,880
structure with the AF2 bound prediction

2744
01:46:32,080 --> 01:46:36,800
or the AF2 um prediction of the

2745
01:46:34,880 --> 01:46:39,280
structure, you can see that there's a

2746
01:46:36,800 --> 01:46:41,679
big difference in the loops here. Um and

2747
01:46:39,280 --> 01:46:43,440
and in the AF2 predicted structure, the

2748
01:46:41,679 --> 01:46:45,280
loop would actually olude the ligan

2749
01:46:43,440 --> 01:46:47,840
binding. It would clash with it, but yet

2750
01:46:45,280 --> 01:46:49,119
it still predicts the lian binding site

2751
01:46:47,840 --> 01:46:50,880
is there. So that's good. So something

2752
01:46:49,119 --> 01:46:53,840
that P2 rank wouldn't predict. So P2

2753
01:46:50,880 --> 01:46:56,239
rank missed this, right? Um so that's

2754
01:46:53,840 --> 01:46:58,480
really great. We've talked about this

2755
01:46:56,239 --> 01:47:00,800
last slide. How could this synergize

2756
01:46:58,480 --> 01:47:03,199
with experiments?

2757
01:47:00,800 --> 01:47:06,239
So if we looked at chemopomics data from

2758
01:47:03,199 --> 01:47:09,679
Ben Crev's group and we said, okay, Ben

2759
01:47:06,239 --> 01:47:11,440
Cat and others have um done these

2760
01:47:09,679 --> 01:47:14,400
experiments where they have coalent

2761
01:47:11,440 --> 01:47:16,480
probes that go in in cells. They

2762
01:47:14,400 --> 01:47:19,440
permeate cells and then they bind to

2763
01:47:16,480 --> 01:47:21,280
cyines. If they bind to a a binding

2764
01:47:19,440 --> 01:47:23,280
pocket nearby then they can coalently

2765
01:47:21,280 --> 01:47:25,840
attach to a cysteine. Then you pop the

2766
01:47:23,280 --> 01:47:28,719
cells proteilize them and do mass spec

2767
01:47:25,840 --> 01:47:30,639
to read out which peptides get lied and

2768
01:47:28,719 --> 01:47:33,920
which don't. And so there's a bunch of

2769
01:47:30,639 --> 01:47:35,440
binding sites a bunch of cyines as that

2770
01:47:33,920 --> 01:47:38,400
are annotated that have been

2771
01:47:35,440 --> 01:47:41,199
experimentally quote ligan by

2772
01:47:38,400 --> 01:47:42,639
chemopomics experiments. Um and the

2773
01:47:41,199 --> 01:47:44,320
question is so those are basically

2774
01:47:42,639 --> 01:47:46,719
annotations of where binding pockets

2775
01:47:44,320 --> 01:47:49,520
might be right and and they may not be

2776
01:47:46,719 --> 01:47:51,760
obvious from AF2 predicted structures or

2777
01:47:49,520 --> 01:47:53,440
in crystal structures and so but

2778
01:47:51,760 --> 01:47:55,920
nevertheless we used AF2 predicted

2779
01:47:53,440 --> 01:47:58,880
structures with AF2 bind to see how many

2780
01:47:55,920 --> 01:48:01,119
of these in this case 200 lianted cyines

2781
01:47:58,880 --> 01:48:03,360
across the human proteome how much of

2782
01:48:01,119 --> 01:48:05,280
those are predicted by AF2bine is there

2783
01:48:03,360 --> 01:48:07,679
a pocket within 10 anstroms of that

2784
01:48:05,280 --> 01:48:09,600
cysteine that's predicted by AF2ine and

2785
01:48:07,679 --> 01:48:12,159
it turns out that AF2bine can predict

2786
01:48:09,600 --> 01:48:14,320
maybe a two

2787
01:48:12,159 --> 01:48:16,639
maybe about a

2788
01:48:14,320 --> 01:48:18,400
you know two-fifths of of the of the

2789
01:48:16,639 --> 01:48:20,480
liganted sites and then P2 rank could

2790
01:48:18,400 --> 01:48:22,000
predict another two- fifths um and

2791
01:48:20,480 --> 01:48:24,000
there's some overlap between them but

2792
01:48:22,000 --> 01:48:25,040
combined between P2 rank and AF2 bind

2793
01:48:24,000 --> 01:48:27,280
you can predict about half of the

2794
01:48:25,040 --> 01:48:29,679
liganted sites the other half why didn't

2795
01:48:27,280 --> 01:48:31,119
we predict them you know why would we

2796
01:48:29,679 --> 01:48:32,480
miss those why did all the methods miss

2797
01:48:31,119 --> 01:48:34,159
the other half and it could be for a

2798
01:48:32,480 --> 01:48:35,760
variety of reasons either the the site

2799
01:48:34,159 --> 01:48:38,800
itself is very cryptic and it's not

2800
01:48:35,760 --> 01:48:40,960
present in the AF2 predicted structure

2801
01:48:38,800 --> 01:48:43,520
or the site site might be a composite

2802
01:48:40,960 --> 01:48:45,679
site that involves other biomolelecules

2803
01:48:43,520 --> 01:48:47,280
that are not um represented in the

2804
01:48:45,679 --> 01:48:48,800
single chain prediction of the

2805
01:48:47,280 --> 01:48:51,040
structure. And that's something you

2806
01:48:48,800 --> 01:48:52,239
could address experimentally, right? You

2807
01:48:51,040 --> 01:48:53,440
could address the experiments by

2808
01:48:52,239 --> 01:48:55,520
overexpressing the protein and trying to

2809
01:48:53,440 --> 01:48:56,800
do this again. Um and so we're very

2810
01:48:55,520 --> 01:48:59,679
interested in trying to combine these

2811
01:48:56,800 --> 01:49:02,159
kinds of data sets with uh with with

2812
01:48:59,679 --> 01:49:04,719
CVAD and others uh to to create new

2813
01:49:02,159 --> 01:49:06,560
labels for especially for the cryptic um

2814
01:49:04,719 --> 01:49:09,600
binding site prediction. create new

2815
01:49:06,560 --> 01:49:12,000
labels for this site is likely going to

2816
01:49:09,600 --> 01:49:13,600
open up. It is a intrinsic cryptic site

2817
01:49:12,000 --> 01:49:15,440
or this site is a composite site with

2818
01:49:13,600 --> 01:49:17,679
this protein. We have new labels for

2819
01:49:15,440 --> 01:49:19,280
binding sites using using these models.

2820
01:49:17,679 --> 01:49:22,239
So then so these are the next steps.

2821
01:49:19,280 --> 01:49:23,199
This is the last slide is um is that

2822
01:49:22,239 --> 01:49:24,880
yeah we want to synergize with

2823
01:49:23,199 --> 01:49:26,880
chemopomics data to train the next

2824
01:49:24,880 --> 01:49:30,400
generation of models where the cryptic

2825
01:49:26,880 --> 01:49:33,199
sites um we want to um work with people

2826
01:49:30,400 --> 01:49:36,320
like Hillary to use these predictions to

2827
01:49:33,199 --> 01:49:38,480
um better predict pathogenic uh mutants

2828
01:49:36,320 --> 01:49:40,080
in human proteins. And then of course

2829
01:49:38,480 --> 01:49:43,040
the obvious thing might be to retrain

2830
01:49:40,080 --> 01:49:44,480
AF2 bind for something like AF3 bind

2831
01:49:43,040 --> 01:49:46,400
where you can actually use instead of a

2832
01:49:44,480 --> 01:49:49,360
basis set of 20 amino acids you could

2833
01:49:46,400 --> 01:49:52,239
use a basis set of a whole diverse

2834
01:49:49,360 --> 01:49:55,760
um a group of small molecule fragments

2835
01:49:52,239 --> 01:49:58,480
that might um might represent uh

2836
01:49:55,760 --> 01:50:00,320
nonnatural chemistry that amino acids

2837
01:49:58,480 --> 01:50:03,440
don't don't have like fluor like h

2838
01:50:00,320 --> 01:50:05,119
hallogens and um and and other kinds of

2839
01:50:03,440 --> 01:50:07,280
functional groups. Um so thank the

2840
01:50:05,119 --> 01:50:08,960
people did the work again summary is

2841
01:50:07,280 --> 01:50:11,280
that pre-trained neural networks can

2842
01:50:08,960 --> 01:50:13,600
actually accurately identify um small

2843
01:50:11,280 --> 01:50:15,119
molecule binding sites and proteins. um

2844
01:50:13,600 --> 01:50:18,480
we have to be really careful about how

2845
01:50:15,119 --> 01:50:21,360
we train and and test our our neural

2846
01:50:18,480 --> 01:50:25,280
nets. Um there are there's a GitHub to

2847
01:50:21,360 --> 01:50:27,119
use um AF2bind. Um there is a uh

2848
01:50:25,280 --> 01:50:29,600
interactive database that we need to

2849
01:50:27,119 --> 01:50:30,960
check to make sure it's still active um

2850
01:50:29,600 --> 01:50:32,639
on the right here for all the human

2851
01:50:30,960 --> 01:50:34,960
proteium results. And all of those

2852
01:50:32,639 --> 01:50:36,719
results will be coming out in a you know

2853
01:50:34,960 --> 01:50:38,000
basically a big CSV file soon with the

2854
01:50:36,719 --> 01:50:39,840
published paper if you're interested in

2855
01:50:38,000 --> 01:50:42,000
the human proteome prediction. I mean,

2856
01:50:39,840 --> 01:50:44,080
this is some people in my group. Um, Jod

2857
01:50:42,000 --> 01:50:45,840
and Anna did the work. And, uh, also

2858
01:50:44,080 --> 01:50:49,560
want to thank Ben for doing the primer

2859
01:50:45,840 --> 01:50:49,560
earlier. Thanks,

