1
00:00:07,520 --> 00:00:11,280
Hey everybody, I am delighted to be here

2
00:00:09,519 --> 00:00:13,519
with you today closing out the show for

3
00:00:11,280 --> 00:00:15,519
the day and sharing a little bit of our

4
00:00:13,519 --> 00:00:16,960
work in our newsroom and some of the

5
00:00:15,519 --> 00:00:18,480
trends and technologies within

6
00:00:16,960 --> 00:00:20,400
artificial intelligence specifically

7
00:00:18,480 --> 00:00:21,920
that we're tracking. My aim in the next

8
00:00:20,400 --> 00:00:23,439
30 minutes is just to give you a sense

9
00:00:21,920 --> 00:00:25,439
of some of the recent developments that

10
00:00:23,439 --> 00:00:30,240
we have covered and where we see this

11
00:00:25,439 --> 00:00:31,760
fastmoving space of AI headed from here.

12
00:00:30,240 --> 00:00:33,280
Uh if you don't know MIT Technology

13
00:00:31,760 --> 00:00:34,399
Review, I'll just briefly tell you a

14
00:00:33,280 --> 00:00:36,640
little bit about who we are and what

15
00:00:34,399 --> 00:00:39,840
we're all about. We are a media company

16
00:00:36,640 --> 00:00:41,920
that is owned by MIT based right here uh

17
00:00:39,840 --> 00:00:43,360
right off of campus. And we are

18
00:00:41,920 --> 00:00:45,200
editorially independent. So we don't

19
00:00:43,360 --> 00:00:46,960
write about MIT research. We cover the

20
00:00:45,200 --> 00:00:48,399
broader world of emerging technology.

21
00:00:46,960 --> 00:00:49,920
And our beats include artificial

22
00:00:48,399 --> 00:00:52,879
intelligence of course, but also

23
00:00:49,920 --> 00:00:55,520
computing, robotics, climate technology,

24
00:00:52,879 --> 00:00:57,120
biotech, all of the above.

25
00:00:55,520 --> 00:00:58,800
So, a few things I like to say about our

26
00:00:57,120 --> 00:01:01,680
approach to technology coverage in the

27
00:00:58,800 --> 00:01:03,920
year 2025. Uh, so we don't write about

28
00:01:01,680 --> 00:01:05,840
technology in a vacuum. We're always

29
00:01:03,920 --> 00:01:07,680
thinking about the economic and

30
00:01:05,840 --> 00:01:09,439
political and social context in which

31
00:01:07,680 --> 00:01:11,439
these technologies get developed and

32
00:01:09,439 --> 00:01:12,960
deployed because we recognize that that

33
00:01:11,439 --> 00:01:15,119
plays a big part in the effect that they

34
00:01:12,960 --> 00:01:17,040
have on our lives. We also don't write

35
00:01:15,119 --> 00:01:18,479
about technology for technologies sake.

36
00:01:17,040 --> 00:01:20,159
And that's just another way of saying we

37
00:01:18,479 --> 00:01:21,600
skip a lot of the small stuff. And we

38
00:01:20,159 --> 00:01:23,280
try to really focus our efforts and our

39
00:01:21,600 --> 00:01:25,040
resources in our newsroom on covering

40
00:01:23,280 --> 00:01:26,720
the highest impact technologies. So the

41
00:01:25,040 --> 00:01:28,720
ones that we really think have a big

42
00:01:26,720 --> 00:01:31,680
chance to reshape the way that we live

43
00:01:28,720 --> 00:01:33,040
and the way that we work.

44
00:01:31,680 --> 00:01:34,640
So you've all been sitting here for a

45
00:01:33,040 --> 00:01:36,320
little while and I wanted you to humor

46
00:01:34,640 --> 00:01:38,159
me and participate in this little

47
00:01:36,320 --> 00:01:39,520
activity uh before I get into what I

48
00:01:38,159 --> 00:01:41,200
have to share with you. This is

49
00:01:39,520 --> 00:01:43,119
something that our senior AI reporter

50
00:01:41,200 --> 00:01:44,799
James O'Donnell put together. He wanted

51
00:01:43,119 --> 00:01:46,479
to test whether our audience and our

52
00:01:44,799 --> 00:01:48,560
editors in our newsroom could

53
00:01:46,479 --> 00:01:50,720
distinguish between music created using

54
00:01:48,560 --> 00:01:53,200
artificial intelligence and generated by

55
00:01:50,720 --> 00:01:54,880
AI and music composed and performed by a

56
00:01:53,200 --> 00:01:56,159
human. So, I'm just going to play you

57
00:01:54,880 --> 00:01:57,759
two clips here and then I'm going to ask

58
00:01:56,159 --> 00:02:00,159
you all to vote. One of these was

59
00:01:57,759 --> 00:02:01,759
created by people, humans. One of them

60
00:02:00,159 --> 00:02:05,240
was created by AI. And we'll see if you

61
00:02:01,759 --> 00:02:05,240
can tell the difference.

62
00:02:07,439 --> 00:02:15,840
>> Quiet glow.

63
00:02:10,560 --> 00:02:18,160
I find myself alone. Tears go.

64
00:02:15,840 --> 00:02:21,520
Rains of sorrow

65
00:02:18,160 --> 00:02:27,040
from a love grown cold.

66
00:02:21,520 --> 00:02:30,760
Memories fading tales untold.

67
00:02:27,040 --> 00:02:30,760
Whispers in.

68
00:02:36,560 --> 00:02:40,519
>> And now for the rock song.

69
00:02:58,160 --> 00:03:01,640
Think about it.

70
00:03:10,879 --> 00:03:14,400
Okay, time to vote. So, one of these

71
00:03:12,640 --> 00:03:16,159
generated by AI, one of them composed

72
00:03:14,400 --> 00:03:18,319
and performed by humans. Who thinks the

73
00:03:16,159 --> 00:03:20,879
folk song was the one that AI created?

74
00:03:18,319 --> 00:03:22,159
Raise your hand.

75
00:03:20,879 --> 00:03:23,920
All right, it's a good chunk of people.

76
00:03:22,159 --> 00:03:26,080
Who thinks the rock song is the one that

77
00:03:23,920 --> 00:03:27,360
AI created?

78
00:03:26,080 --> 00:03:28,720
Okay, I think most of you got it right.

79
00:03:27,360 --> 00:03:30,879
It's a little bit of a split, but in

80
00:03:28,720 --> 00:03:32,879
fact, it was the first one. uh the folk

81
00:03:30,879 --> 00:03:35,519
song that was AI generated. He made the

82
00:03:32,879 --> 00:03:36,879
he made that in a tool called Udo and I

83
00:03:35,519 --> 00:03:38,000
think it'll be an interesting uh space

84
00:03:36,879 --> 00:03:40,159
to follow. Certainly we're going to see

85
00:03:38,000 --> 00:03:42,400
more generative AI making music I think

86
00:03:40,159 --> 00:03:44,319
in the future.

87
00:03:42,400 --> 00:03:45,440
But first the first uh trend and topic I

88
00:03:44,319 --> 00:03:46,799
wanted to talk with you all about is

89
00:03:45,440 --> 00:03:48,319
really generative video which is kind of

90
00:03:46,799 --> 00:03:50,879
the forefront of where where a lot of

91
00:03:48,319 --> 00:03:52,959
the uh best advances are happening in

92
00:03:50,879 --> 00:03:54,400
generative AI right now. We've seen for

93
00:03:52,959 --> 00:03:55,920
a couple of years these multimodal

94
00:03:54,400 --> 00:03:57,280
models getting better and better, models

95
00:03:55,920 --> 00:03:58,879
that you can input all kinds of

96
00:03:57,280 --> 00:04:00,799
different formats into, whether it's

97
00:03:58,879 --> 00:04:02,959
text or photos or videos, and they'll

98
00:04:00,799 --> 00:04:04,720
output in all of those formats as well.

99
00:04:02,959 --> 00:04:06,000
Uh, but lately, the generative video is

100
00:04:04,720 --> 00:04:07,200
getting really impressive, and I'll show

101
00:04:06,000 --> 00:04:10,599
you a couple clips of just what's

102
00:04:07,200 --> 00:04:10,599
possible today.

103
00:04:11,599 --> 00:04:16,639
This was made using Google's V3 model,

104
00:04:14,159 --> 00:04:18,959
and you can see that it you can uh add

105
00:04:16,639 --> 00:04:20,959
sound effects with this model. The next

106
00:04:18,959 --> 00:04:22,639
one's an action sequence produced by the

107
00:04:20,959 --> 00:04:25,800
same model. So, brace yourselves. It's a

108
00:04:22,639 --> 00:04:25,800
little louder.

109
00:04:32,080 --> 00:04:35,360
You can see how realistic that is. It

110
00:04:33,680 --> 00:04:37,440
took a long prompt to get that. You can

111
00:04:35,360 --> 00:04:38,639
see the level of detail described in

112
00:04:37,440 --> 00:04:40,639
this prompt, all the different things

113
00:04:38,639 --> 00:04:42,320
that they're specifying. But it doesn't

114
00:04:40,639 --> 00:04:43,840
have to be that complicated. The next

115
00:04:42,320 --> 00:04:45,759
clip I'm going to show you is almost

116
00:04:43,840 --> 00:04:49,479
just as realistic, and it was a prompt

117
00:04:45,759 --> 00:04:49,479
that just had two sentences.

118
00:04:56,160 --> 00:04:59,280
Then the next example I'm going to show

119
00:04:57,520 --> 00:05:00,720
you has a line of dialogue in it that

120
00:04:59,280 --> 00:05:04,160
was included in the prompt.

121
00:05:00,720 --> 00:05:07,280
>> This ocean, it's a force, a wild untamed

122
00:05:04,160 --> 00:05:09,680
might. And she commands your awe with

123
00:05:07,280 --> 00:05:11,120
every breaking light.

124
00:05:09,680 --> 00:05:12,639
>> So these tools are getting better and

125
00:05:11,120 --> 00:05:14,320
better. We've I've shown you a bunch of

126
00:05:12,639 --> 00:05:15,840
short clips so far, but it's also now

127
00:05:14,320 --> 00:05:17,520
possible to string a lot of these short

128
00:05:15,840 --> 00:05:19,120
clips together and make a short film

129
00:05:17,520 --> 00:05:23,720
with characters and scenes that are

130
00:05:19,120 --> 00:05:23,720
consistent and believable uh throughout.

131
00:05:27,680 --> 00:05:33,759
>> Wow, this is a real breakthrough.

132
00:05:31,199 --> 00:05:35,280
And of course, in the last couple of six

133
00:05:33,759 --> 00:05:36,800
weeks or so, one of the biggest

134
00:05:35,280 --> 00:05:39,120
announcements within generative video is

135
00:05:36,800 --> 00:05:40,960
the release of OpenAI's Sora app, which

136
00:05:39,120 --> 00:05:42,560
is a Tik Tok-like experience that has an

137
00:05:40,960 --> 00:05:44,479
endless scroll, but allows you to

138
00:05:42,560 --> 00:05:46,320
produce purely AI generated content. You

139
00:05:44,479 --> 00:05:48,320
can actually put celebrities, uh, people

140
00:05:46,320 --> 00:05:49,680
like Sam Alman into your videos if you

141
00:05:48,320 --> 00:05:51,520
want. You can put yourself into the

142
00:05:49,680 --> 00:05:53,919
videos in any kind of situation or

143
00:05:51,520 --> 00:05:56,320
scenario that you want. Um, so this is

144
00:05:53,919 --> 00:05:58,160
obviously an AI generated uh video of

145
00:05:56,320 --> 00:06:00,639
Sam Alman enjoying our 10 Breakthrough

146
00:05:58,160 --> 00:06:01,680
Technologies issue. So this is all

147
00:06:00,639 --> 00:06:03,120
pretty cool and there's some neat things

148
00:06:01,680 --> 00:06:04,400
we can do with it, but there's also some

149
00:06:03,120 --> 00:06:07,120
problems that come along with it, of

150
00:06:04,400 --> 00:06:09,120
course. One of them being AI slop. We're

151
00:06:07,120 --> 00:06:10,720
seeing tons of this out there. Uh, both

152
00:06:09,120 --> 00:06:12,720
with text that's just filling the

153
00:06:10,720 --> 00:06:14,560
internet and also a lot of weird images

154
00:06:12,720 --> 00:06:16,639
and videos. Some of them are just

155
00:06:14,560 --> 00:06:18,720
horrendously bad and awful. Um, others

156
00:06:16,639 --> 00:06:20,000
are being used to perpetuate hoaxes. Uh,

157
00:06:18,720 --> 00:06:21,440
people believing that they're they're

158
00:06:20,000 --> 00:06:22,880
real and they're not. So, this is

159
00:06:21,440 --> 00:06:24,319
unfortunately a problem that I really

160
00:06:22,880 --> 00:06:25,919
think is going to just be with us for a

161
00:06:24,319 --> 00:06:28,560
while. I don't see any kind of immediate

162
00:06:25,919 --> 00:06:30,240
solution to it.

163
00:06:28,560 --> 00:06:31,919
Another consequence of how good this

164
00:06:30,240 --> 00:06:33,440
technology is getting is that it's

165
00:06:31,919 --> 00:06:36,160
actually getting quite hard to tell who

166
00:06:33,440 --> 00:06:37,440
and what is real online these days. It's

167
00:06:36,160 --> 00:06:39,600
now possible to make really

168
00:06:37,440 --> 00:06:42,240
hyperrealistic digital avatars of

169
00:06:39,600 --> 00:06:45,440
ourselves or people that we know or to

170
00:06:42,240 --> 00:06:47,120
generate completely AI fabricated humans

171
00:06:45,440 --> 00:06:49,600
who look very realistic but don't

172
00:06:47,120 --> 00:06:50,639
actually exist in the real world. So to

173
00:06:49,600 --> 00:06:53,120
illustrate this, I'm going to show you

174
00:06:50,639 --> 00:06:55,600
this short clip of one of our reporters,

175
00:06:53,120 --> 00:06:57,199
Riionan, who used one of these tools

176
00:06:55,600 --> 00:06:59,440
produced by a company called Synthesia

177
00:06:57,199 --> 00:07:00,720
to make a digital avatar of herself. And

178
00:06:59,440 --> 00:07:02,160
I'll let you be the judge of how

179
00:07:00,720 --> 00:07:04,400
convincing this is.

180
00:07:02,160 --> 00:07:07,039
>> This is an avatar created with Express

181
00:07:04,400 --> 00:07:09,440
2. New technology elevates body

182
00:07:07,039 --> 00:07:11,520
movements to an entirely new dimension.

183
00:07:09,440 --> 00:07:13,280
This is great. Check out these

184
00:07:11,520 --> 00:07:14,960
movements.

185
00:07:13,280 --> 00:07:16,319
>> So, it's kind of cheesy. Uh, it's a

186
00:07:14,960 --> 00:07:18,240
little bit awkward, but if you know

187
00:07:16,319 --> 00:07:19,919
Rihanna, like I do, like it does look

188
00:07:18,240 --> 00:07:21,440
and sound like her, and it'll be

189
00:07:19,919 --> 00:07:23,680
interesting. This techn is also only

190
00:07:21,440 --> 00:07:25,599
going to get uh better. We also reported

191
00:07:23,680 --> 00:07:27,360
on a study by Stanford and Deep Mind

192
00:07:25,599 --> 00:07:28,880
that showed that an AI model could

193
00:07:27,360 --> 00:07:30,800
interview somebody for just two hours

194
00:07:28,880 --> 00:07:32,479
and then start to mimic aspects of their

195
00:07:30,800 --> 00:07:34,319
personality. So things like their values

196
00:07:32,479 --> 00:07:37,199
and their preferences, which could make

197
00:07:34,319 --> 00:07:39,599
these even more believable.

198
00:07:37,199 --> 00:07:41,199
One more test for you. Uh, so not not

199
00:07:39,599 --> 00:07:43,280
just videos, but even just straight

200
00:07:41,199 --> 00:07:45,199
images are really realistic these days.

201
00:07:43,280 --> 00:07:46,800
Uh this is from a site called

202
00:07:45,199 --> 00:07:48,560
whichfaces.com.

203
00:07:46,800 --> 00:07:50,080
Hopefully self-explanatory. It was put

204
00:07:48,560 --> 00:07:52,000
together by University of Washington

205
00:07:50,080 --> 00:07:53,440
researchers and they just give a ton of

206
00:07:52,000 --> 00:07:55,919
examples like this where one of the

207
00:07:53,440 --> 00:07:59,039
images is a real photo of a real actual

208
00:07:55,919 --> 00:08:00,800
human and the other image is AI

209
00:07:59,039 --> 00:08:02,639
generated. So I want to ask you all to

210
00:08:00,800 --> 00:08:05,360
vote again just one more time. So who

211
00:08:02,639 --> 00:08:07,599
thinks the person in the hat is the AI

212
00:08:05,360 --> 00:08:09,360
generated human here? Just raise your

213
00:08:07,599 --> 00:08:11,199
hand.

214
00:08:09,360 --> 00:08:14,720
And who thinks the person in the glasses

215
00:08:11,199 --> 00:08:16,639
is the AI generated human?

216
00:08:14,720 --> 00:08:17,840
That was a little bit more mixed. Um,

217
00:08:16,639 --> 00:08:19,840
but I think most of you got it wrong

218
00:08:17,840 --> 00:08:21,599
that time. Don't feel bad, though. I

219
00:08:19,840 --> 00:08:23,280
actually got this one wrong, too. Uh, if

220
00:08:21,599 --> 00:08:24,639
you like this game, there are a lot more

221
00:08:23,280 --> 00:08:26,960
examples on this site that you can play

222
00:08:24,639 --> 00:08:28,319
around with. So, what are we going to do

223
00:08:26,960 --> 00:08:29,360
about all this? I bet you've heard of

224
00:08:28,319 --> 00:08:31,199
some of the solutions that are being

225
00:08:29,360 --> 00:08:33,279
proposed. Things like watermarks that

226
00:08:31,199 --> 00:08:35,039
you embed in images or videos, either

227
00:08:33,279 --> 00:08:36,479
visibly or invisibly in the metadata

228
00:08:35,039 --> 00:08:38,560
that travels along with them across the

229
00:08:36,479 --> 00:08:40,000
internet. Also, there's uh work on

230
00:08:38,560 --> 00:08:42,399
developing what are called personhood

231
00:08:40,000 --> 00:08:44,640
credentials here at MIT actually uh with

232
00:08:42,399 --> 00:08:46,399
OpenAI and Microsoft. This would be like

233
00:08:44,640 --> 00:08:48,080
a token that's stored kind of on your

234
00:08:46,399 --> 00:08:49,600
phone, maybe like in the wallet app of

235
00:08:48,080 --> 00:08:51,680
your phone, and you might actually have

236
00:08:49,600 --> 00:08:53,680
to go to a a government office or an

237
00:08:51,680 --> 00:08:55,440
organization to get it installed on your

238
00:08:53,680 --> 00:08:57,519
phone, but then you'd be able to use it

239
00:08:55,440 --> 00:08:59,440
to cryptographically verify that you're

240
00:08:57,519 --> 00:09:01,519
in fact a real human anytime you wanted

241
00:08:59,440 --> 00:09:03,920
to do something uh online that required

242
00:09:01,519 --> 00:09:05,440
that.

243
00:09:03,920 --> 00:09:06,640
So, one of the areas that I think is

244
00:09:05,440 --> 00:09:08,000
most interesting to follow to kind of

245
00:09:06,640 --> 00:09:10,399
see what's coming up or what's most

246
00:09:08,000 --> 00:09:12,399
exciting uh with generative AI is the

247
00:09:10,399 --> 00:09:13,440
gaming world and gaming companies are

248
00:09:12,399 --> 00:09:15,120
kind of leaning full into this

249
00:09:13,440 --> 00:09:16,399
technology and developers are doing some

250
00:09:15,120 --> 00:09:17,760
really cool stuff with it. So, I wanted

251
00:09:16,399 --> 00:09:20,240
to spend a few moments talking about

252
00:09:17,760 --> 00:09:22,160
that. Um, for example, the gaming

253
00:09:20,240 --> 00:09:24,399
company Niantic is using some of the

254
00:09:22,160 --> 00:09:27,200
data collected from Pokemon Go to build

255
00:09:24,399 --> 00:09:29,120
a geospatial model of the world, uh,

256
00:09:27,200 --> 00:09:31,360
that actually obeys the the laws of

257
00:09:29,120 --> 00:09:32,560
physics that our real world relies on,

258
00:09:31,360 --> 00:09:34,640
which could help machines kind of

259
00:09:32,560 --> 00:09:36,000
understand how objects exist in the

260
00:09:34,640 --> 00:09:38,240
world and the rules by which they're

261
00:09:36,000 --> 00:09:40,320
governed.

262
00:09:38,240 --> 00:09:41,760
Roblox also had an open- source uh,

263
00:09:40,320 --> 00:09:43,600
project called Cube that it built. It's

264
00:09:41,760 --> 00:09:45,680
a 3D foundation model that would allow

265
00:09:43,600 --> 00:09:47,440
you to generate objects that can be

266
00:09:45,680 --> 00:09:49,200
moved around in the 3D space and viewed

267
00:09:47,440 --> 00:09:50,640
from any angle. And so this would allow

268
00:09:49,200 --> 00:09:53,200
somebody without any kind of game design

269
00:09:50,640 --> 00:09:56,080
or coding experience to start to develop

270
00:09:53,200 --> 00:09:57,519
digital worlds of their own.

271
00:09:56,080 --> 00:10:00,000
There's also been some really cool tests

272
00:09:57,519 --> 00:10:01,839
of like what happens if you put AI

273
00:10:00,000 --> 00:10:03,600
agents into a gaming environment and

274
00:10:01,839 --> 00:10:04,800
then just sort of let them go. Uh

275
00:10:03,600 --> 00:10:07,040
especially when they're trained using

276
00:10:04,800 --> 00:10:08,880
large language models. And this is an

277
00:10:07,040 --> 00:10:10,560
example from Minecraft. They let loose a

278
00:10:08,880 --> 00:10:12,320
bunch of AI agents in Minecraft to just

279
00:10:10,560 --> 00:10:13,760
see what they would do. And because they

280
00:10:12,320 --> 00:10:15,920
were using large language models that

281
00:10:13,760 --> 00:10:17,519
have ingested tons of data about humans

282
00:10:15,920 --> 00:10:20,000
behave, they started to do some really

283
00:10:17,519 --> 00:10:21,519
human things. So these agents actually

284
00:10:20,000 --> 00:10:24,160
started their own religion and they

285
00:10:21,519 --> 00:10:26,480
voted on tax reform laws. And so they

286
00:10:24,160 --> 00:10:28,240
were they didn't this behavior kind of

287
00:10:26,480 --> 00:10:30,000
spontaneously emerged. Of course, it's

288
00:10:28,240 --> 00:10:31,760
rooted in the patterns of the training

289
00:10:30,000 --> 00:10:32,959
data that they have ingested, but it's

290
00:10:31,760 --> 00:10:35,120
just kind of funny to like see these

291
00:10:32,959 --> 00:10:36,880
agents um coming up with these things

292
00:10:35,120 --> 00:10:39,200
kind of on their own as well based on

293
00:10:36,880 --> 00:10:41,920
what they know about us. Another example

294
00:10:39,200 --> 00:10:43,279
is this uh project putting into a

295
00:10:41,920 --> 00:10:45,279
environment called Smallville. There

296
00:10:43,279 --> 00:10:46,640
were 25 different agents in this

297
00:10:45,279 --> 00:10:48,959
environment. In this case, they were

298
00:10:46,640 --> 00:10:50,720
each given a job and like a personality

299
00:10:48,959 --> 00:10:53,120
and then just left and then just left

300
00:10:50,720 --> 00:10:54,640
for a few days. And they also did some

301
00:10:53,120 --> 00:10:56,000
funny stuff. So they a couple of them

302
00:10:54,640 --> 00:10:58,000
like organized and attended a

303
00:10:56,000 --> 00:11:00,079
Valentine's Day party. One of them asked

304
00:10:58,000 --> 00:11:01,600
another one out on a date. Uh another

305
00:11:00,079 --> 00:11:03,360
one decided to run for office and then

306
00:11:01,600 --> 00:11:05,839
there was like a side conversation about

307
00:11:03,360 --> 00:11:07,680
that. So it's you know kind of like a

308
00:11:05,839 --> 00:11:09,120
strange uh way to like look at ourselves

309
00:11:07,680 --> 00:11:11,200
through these agents and kind of the

310
00:11:09,120 --> 00:11:13,519
behavior that emerges uh when when they

311
00:11:11,200 --> 00:11:14,640
do these experiments.

312
00:11:13,519 --> 00:11:16,079
And then there's also some really

313
00:11:14,640 --> 00:11:18,399
interesting work being done around using

314
00:11:16,079 --> 00:11:20,399
generative AI to create games as you

315
00:11:18,399 --> 00:11:22,880
play them. This is a very rudimentary

316
00:11:20,399 --> 00:11:25,040
example, but it is a AI generated game

317
00:11:22,880 --> 00:11:27,200
example of Minecraft. So there was no

318
00:11:25,040 --> 00:11:29,279
code being written for this game. This

319
00:11:27,200 --> 00:11:30,720
is just a model trained on millions of

320
00:11:29,279 --> 00:11:32,800
hours of people playing Minecraft and

321
00:11:30,720 --> 00:11:34,880
it's being generated on the fly as you

322
00:11:32,800 --> 00:11:36,079
play it. And so this version isn't very

323
00:11:34,880 --> 00:11:37,600
good. You know, the graphics aren't

324
00:11:36,079 --> 00:11:39,279
great. It kind of breaks down after you

325
00:11:37,600 --> 00:11:41,040
play it for a few seconds. But you could

326
00:11:39,279 --> 00:11:42,560
imagine something like this eventually

327
00:11:41,040 --> 00:11:44,320
augmenting the abilities of human

328
00:11:42,560 --> 00:11:46,720
developers or maybe even allowing

329
00:11:44,320 --> 00:11:50,079
players to actually decide the course of

330
00:11:46,720 --> 00:11:51,920
the game as they play it.

331
00:11:50,079 --> 00:11:53,440
All right, next topic. Uh, AI and

332
00:11:51,920 --> 00:11:56,079
generative AI in particular is really

333
00:11:53,440 --> 00:11:57,600
changing the way that we search online.

334
00:11:56,079 --> 00:11:59,519
This is a really important development,

335
00:11:57,600 --> 00:12:02,000
I think, in the way that information is

336
00:11:59,519 --> 00:12:04,240
organized and consumed online. It's

337
00:12:02,000 --> 00:12:05,440
really the biggest change in decades to

338
00:12:04,240 --> 00:12:07,279
search in the internet, which is

339
00:12:05,440 --> 00:12:08,880
something that most of us do many times

340
00:12:07,279 --> 00:12:10,320
a day. And we're talking here, of

341
00:12:08,880 --> 00:12:12,480
course, about the infusion of large

342
00:12:10,320 --> 00:12:14,560
language models into search engines.

343
00:12:12,480 --> 00:12:15,920
things like uh Google's AI overviews,

344
00:12:14,560 --> 00:12:18,160
which I'm sure you're seeing pop up on

345
00:12:15,920 --> 00:12:20,160
all of your searches these days. Also

346
00:12:18,160 --> 00:12:23,120
things like Microsoft's efforts to

347
00:12:20,160 --> 00:12:24,880
infuse OpenAI's models into its uh Bing

348
00:12:23,120 --> 00:12:26,880
search engine. And also stuff like

349
00:12:24,880 --> 00:12:29,120
ChatGpt being able to go out and fetch

350
00:12:26,880 --> 00:12:31,200
data from the live web uh now when you

351
00:12:29,120 --> 00:12:33,040
type in a prompt. There's of course AI

352
00:12:31,200 --> 00:12:34,399
powered search on our devices too that's

353
00:12:33,040 --> 00:12:36,240
happening there as well. And also in

354
00:12:34,399 --> 00:12:38,639
apps uh like all of the Meta family

355
00:12:36,240 --> 00:12:40,320
apps, Facebook Messenger and Instagram

356
00:12:38,639 --> 00:12:41,839
uh etc.

357
00:12:40,320 --> 00:12:42,959
So, this is such a big shift that we

358
00:12:41,839 --> 00:12:44,560
actually put it on our list of 10

359
00:12:42,959 --> 00:12:46,959
breakthrough technologies this year for

360
00:12:44,560 --> 00:12:48,480
2025. And I think it raises some really

361
00:12:46,959 --> 00:12:50,639
interesting questions for all of you

362
00:12:48,480 --> 00:12:51,839
just about like your customers and your

363
00:12:50,639 --> 00:12:54,079
employees are going to be interacting

364
00:12:51,839 --> 00:12:55,440
with generative search all over the

365
00:12:54,079 --> 00:12:56,959
place. And so, what does it mean for the

366
00:12:55,440 --> 00:12:58,480
kinds of like the products that you're

367
00:12:56,959 --> 00:13:00,000
building and the information that you're

368
00:12:58,480 --> 00:13:02,399
sharing about your company and how

369
00:13:00,000 --> 00:13:03,519
you're getting that all out there?

370
00:13:02,399 --> 00:13:05,040
Of course, we're thinking about this in

371
00:13:03,519 --> 00:13:06,639
the media business a lot. How are

372
00:13:05,040 --> 00:13:08,480
publishers like us going to reach

373
00:13:06,639 --> 00:13:09,920
people? is all of the information and

374
00:13:08,480 --> 00:13:11,760
coverage that we do in the future going

375
00:13:09,920 --> 00:13:13,839
to be mediated through these kind of AI

376
00:13:11,760 --> 00:13:15,120
chat bots and what you know what can we

377
00:13:13,839 --> 00:13:16,959
do about it or what should we do about

378
00:13:15,120 --> 00:13:18,959
it? I've started to hear about companies

379
00:13:16,959 --> 00:13:21,839
that can like optimize what you're doing

380
00:13:18,959 --> 00:13:23,760
for supposedly to help you show up more

381
00:13:21,839 --> 00:13:25,279
in the large language models. I'm not

382
00:13:23,760 --> 00:13:26,880
convinced that's real yet. So if you've

383
00:13:25,279 --> 00:13:31,200
had any like good experiences or know

384
00:13:26,880 --> 00:13:33,200
more about that, I would love to hear.

385
00:13:31,200 --> 00:13:34,480
Uh next, this is a very personal topic.

386
00:13:33,200 --> 00:13:36,639
Perhaps some of you are already doing

387
00:13:34,480 --> 00:13:38,560
this or have tried it uh yourselves.

388
00:13:36,639 --> 00:13:39,760
People are turning to these AI chat bots

389
00:13:38,560 --> 00:13:42,240
increasingly for a form of

390
00:13:39,760 --> 00:13:44,720
companionship. In some case it's in

391
00:13:42,240 --> 00:13:46,480
cases it's basically around uh what's

392
00:13:44,720 --> 00:13:48,800
called as synthetic friendship. So it's

393
00:13:46,480 --> 00:13:50,320
a form of uh friendship but others the

394
00:13:48,800 --> 00:13:53,279
humans involved actually believe it to

395
00:13:50,320 --> 00:13:55,760
be a type of romantic relationship.

396
00:13:53,279 --> 00:13:57,120
So we uh have surveyed our subscribers

397
00:13:55,760 --> 00:13:59,760
who attended one of our monthly

398
00:13:57,120 --> 00:14:01,279
roundtables online events about this to

399
00:13:59,760 --> 00:14:03,440
see like how are you doing this like

400
00:14:01,279 --> 00:14:04,880
would you do this and the vast majority

401
00:14:03,440 --> 00:14:06,480
of them said that they either had tried

402
00:14:04,880 --> 00:14:08,079
this themselves using chat bots for

403
00:14:06,480 --> 00:14:10,720
companionship or were interested in

404
00:14:08,079 --> 00:14:11,920
doing it. So what kinds of companionship

405
00:14:10,720 --> 00:14:14,320
are we talking about here? There's a

406
00:14:11,920 --> 00:14:16,160
broad spectrum. Uh we had a actually the

407
00:14:14,320 --> 00:14:17,600
reporter you saw earlier the avatar of

408
00:14:16,160 --> 00:14:18,720
she did a nice story that was just

409
00:14:17,600 --> 00:14:20,320
talking with some people about how

410
00:14:18,720 --> 00:14:21,600
they're using chat bots for

411
00:14:20,320 --> 00:14:23,279
companionship, what they're getting out

412
00:14:21,600 --> 00:14:25,120
of it, how it's affecting their real

413
00:14:23,279 --> 00:14:26,480
world relationships. And so there's some

414
00:14:25,120 --> 00:14:28,240
nice examples here from that piece.

415
00:14:26,480 --> 00:14:30,160
There's a 52-year-old woman in Canada

416
00:14:28,240 --> 00:14:32,160
who was using it kind of as a way to

417
00:14:30,160 --> 00:14:33,839
like chat and vent about her stresses

418
00:14:32,160 --> 00:14:35,519
throughout the day related to work or

419
00:14:33,839 --> 00:14:36,880
problems going on in her life. It was

420
00:14:35,519 --> 00:14:38,160
like a type of journaling kind of for

421
00:14:36,880 --> 00:14:40,000
her but like with a more active

422
00:14:38,160 --> 00:14:42,480
conversation partner and she described

423
00:14:40,000 --> 00:14:45,120
it as being very cathartic. Then there

424
00:14:42,480 --> 00:14:46,480
was a a Dutch man who had married a

425
00:14:45,120 --> 00:14:48,720
Chinese woman and they were living

426
00:14:46,480 --> 00:14:50,160
together in Thailand. And he said that

427
00:14:48,720 --> 00:14:52,079
sometimes they just had these fights

428
00:14:50,160 --> 00:14:53,519
where like they could not see eye to eye

429
00:14:52,079 --> 00:14:54,800
and he was convinced that it was

430
00:14:53,519 --> 00:14:56,639
something to do with linguistic

431
00:14:54,800 --> 00:14:58,480
differences or just cultural differences

432
00:14:56,639 --> 00:15:00,959
in the way that they were raised. So he

433
00:14:58,480 --> 00:15:02,639
had taken to using anthropics claude as

434
00:15:00,959 --> 00:15:04,959
a sort of like third opinion in these

435
00:15:02,639 --> 00:15:06,399
arguments and asking it to help him get

436
00:15:04,959 --> 00:15:07,760
a different perspective on whatever they

437
00:15:06,399 --> 00:15:09,760
were fighting about. And he said it

438
00:15:07,760 --> 00:15:11,440
worked really well. Uh his advice if you

439
00:15:09,760 --> 00:15:12,959
want to try this at home is to tell

440
00:15:11,440 --> 00:15:15,120
Claude that you're asking about this

441
00:15:12,959 --> 00:15:16,399
situation for a friend because Claude

442
00:15:15,120 --> 00:15:17,920
always just like tends to agree with

443
00:15:16,399 --> 00:15:19,120
whoever is asking the question and

444
00:15:17,920 --> 00:15:21,360
that's not that helpful in that

445
00:15:19,120 --> 00:15:23,040
situation.

446
00:15:21,360 --> 00:15:24,480
There is a more sinister side of this of

447
00:15:23,040 --> 00:15:25,920
course like there are people that are

448
00:15:24,480 --> 00:15:28,399
starting to use these chat bots as a

449
00:15:25,920 --> 00:15:29,760
form of therapy um which we do not

450
00:15:28,399 --> 00:15:32,000
recommend and neither does the American

451
00:15:29,760 --> 00:15:33,440
Psychological Association. These chat

452
00:15:32,000 --> 00:15:35,120
bots none of them are approved for

453
00:15:33,440 --> 00:15:36,720
mental health treatment. They are not

454
00:15:35,120 --> 00:15:38,800
HIPPA compliant. So whatever you put

455
00:15:36,720 --> 00:15:40,880
into them may not stay there, may not

456
00:15:38,800 --> 00:15:42,399
stay private. Uh and there have been a

457
00:15:40,880 --> 00:15:44,160
few allegations of these chat bots

458
00:15:42,399 --> 00:15:46,480
encouraging suicidal thinking. We even

459
00:15:44,160 --> 00:15:49,440
reported on one that it suggested ways

460
00:15:46,480 --> 00:15:50,959
of carrying that out. I am hopeful that

461
00:15:49,440 --> 00:15:53,440
in the long run these chat bots might

462
00:15:50,959 --> 00:15:55,120
actually improve access to highquality

463
00:15:53,440 --> 00:15:57,519
therapy. There has been one clinical

464
00:15:55,120 --> 00:15:59,440
trial that we reported on in the spring

465
00:15:57,519 --> 00:16:01,600
uh reported on also in the the New

466
00:15:59,440 --> 00:16:04,160
England Journal of Medicine about a

467
00:16:01,600 --> 00:16:06,079
specially designed chatbot that was

468
00:16:04,160 --> 00:16:08,480
created by mental health professionals

469
00:16:06,079 --> 00:16:11,040
called Therabot. And it was shown in

470
00:16:08,480 --> 00:16:13,279
this trial to be as effective as uh

471
00:16:11,040 --> 00:16:14,800
psychotherapy in treating people for

472
00:16:13,279 --> 00:16:17,040
depression, anxiety, and those who are

473
00:16:14,800 --> 00:16:19,279
at risk of eating disorders. And it was

474
00:16:17,040 --> 00:16:21,040
equivalent to basically 16 hours of

475
00:16:19,279 --> 00:16:22,480
psychotherapy in terms of the results it

476
00:16:21,040 --> 00:16:24,160
got, but it delivered those in half the

477
00:16:22,480 --> 00:16:25,440
time. So, that still needs to make it

478
00:16:24,160 --> 00:16:27,199
its way all the way through clinical

479
00:16:25,440 --> 00:16:28,880
trials and be approved and such. But

480
00:16:27,199 --> 00:16:30,639
hopefully, uh, someday these will

481
00:16:28,880 --> 00:16:32,880
improve access to this important

482
00:16:30,639 --> 00:16:34,160
service.

483
00:16:32,880 --> 00:16:35,680
All right, next. We've heard a lot about

484
00:16:34,160 --> 00:16:37,040
agents recently. I bet you've heard a

485
00:16:35,680 --> 00:16:39,600
lot about these, too. It's a big buzzy

486
00:16:37,040 --> 00:16:40,720
topic in AI right now. I want this

487
00:16:39,600 --> 00:16:42,160
tomorrow. Like, I want this like

488
00:16:40,720 --> 00:16:43,519
yesterday. I would love to have one of

489
00:16:42,160 --> 00:16:45,519
these that can just go out and take care

490
00:16:43,519 --> 00:16:47,040
of a bunch of busy work for me. Um,

491
00:16:45,519 --> 00:16:48,480
there have been some promising

492
00:16:47,040 --> 00:16:49,920
developments over the last year. We've

493
00:16:48,480 --> 00:16:52,160
seen more of the underlying kind of

494
00:16:49,920 --> 00:16:53,920
fundamental protocols being developed.

495
00:16:52,160 --> 00:16:56,639
Anthropic had its model context

496
00:16:53,920 --> 00:16:58,639
protocols come out about a year ago and

497
00:16:56,639 --> 00:16:59,680
Google has its agentto aagent protocol.

498
00:16:58,639 --> 00:17:01,600
These are the kind of fundamental

499
00:16:59,680 --> 00:17:03,680
infrastructure layers that do need to

500
00:17:01,600 --> 00:17:05,679
happen in order to enable this agentic

501
00:17:03,680 --> 00:17:08,319
uh universe of the future. There's also

502
00:17:05,679 --> 00:17:10,400
a project here at the MIT media lab by

503
00:17:08,319 --> 00:17:12,240
Romesh Rascar, a professor there, that

504
00:17:10,400 --> 00:17:13,839
is aiming to build a it's called Nanda,

505
00:17:12,240 --> 00:17:15,919
and it's aiming to build a layer of

506
00:17:13,839 --> 00:17:18,160
search and authentication and other

507
00:17:15,919 --> 00:17:20,799
features on top of these protocols. Um,

508
00:17:18,160 --> 00:17:23,760
so that's all really exciting work. This

509
00:17:20,799 --> 00:17:26,319
is a demo of OpenAI's operator uh agent.

510
00:17:23,760 --> 00:17:27,679
So, there are some out there now in beta

511
00:17:26,319 --> 00:17:29,280
mode and and making their way out into

512
00:17:27,679 --> 00:17:31,520
the world. In this case, you can see it

513
00:17:29,280 --> 00:17:33,120
booking a campsite for somebody given

514
00:17:31,520 --> 00:17:35,919
the directions of of that person and

515
00:17:33,120 --> 00:17:37,039
what kind of campsite uh they wanted. I

516
00:17:35,919 --> 00:17:39,840
think it'll be interesting to think

517
00:17:37,039 --> 00:17:41,360
about how and when an AI needs to

518
00:17:39,840 --> 00:17:42,720
identify itself during one of these

519
00:17:41,360 --> 00:17:44,000
interactions when the person on the

520
00:17:42,720 --> 00:17:45,840
other end needs to know that it's

521
00:17:44,000 --> 00:17:48,400
interacting with an AI or your AI agent

522
00:17:45,840 --> 00:17:49,679
instead of actually you. I think it's

523
00:17:48,400 --> 00:17:51,520
still early days though for a lot of

524
00:17:49,679 --> 00:17:53,039
this agent stuff. uh it's still taking

525
00:17:51,520 --> 00:17:55,919
shape and I think there will be a number

526
00:17:53,039 --> 00:17:57,600
of of interoperability challenges that

527
00:17:55,919 --> 00:17:59,840
we have to overcome once these protocols

528
00:17:57,600 --> 00:18:01,200
are are more uh established and I also

529
00:17:59,840 --> 00:18:03,120
think there's a big question of trust

530
00:18:01,200 --> 00:18:04,799
with users where you know people will

531
00:18:03,120 --> 00:18:06,240
need to at some point hand over their

532
00:18:04,799 --> 00:18:08,160
social media credentials and their

533
00:18:06,240 --> 00:18:10,080
passwords and their credit cards to

534
00:18:08,160 --> 00:18:11,679
these agents and you know there will be

535
00:18:10,080 --> 00:18:13,520
some trust that uh people need to have

536
00:18:11,679 --> 00:18:15,840
in these systems uh to make them ready

537
00:18:13,520 --> 00:18:17,520
for that.

538
00:18:15,840 --> 00:18:18,720
There are a few areas though where AI

539
00:18:17,520 --> 00:18:20,240
and generative AI in particular is

540
00:18:18,720 --> 00:18:21,520
really already starting to help us get

541
00:18:20,240 --> 00:18:23,679
stuff done. I bet you've heard about a

542
00:18:21,520 --> 00:18:26,240
few of these. Uh coding is one of them.

543
00:18:23,679 --> 00:18:28,480
So there was a report by a study by MIT

544
00:18:26,240 --> 00:18:29,840
Sloan that came out last year looking at

545
00:18:28,480 --> 00:18:31,520
software engineers who were paired with

546
00:18:29,840 --> 00:18:33,600
a coding assistant. It was GitHub's

547
00:18:31,520 --> 00:18:36,160
co-pilot at three different companies,

548
00:18:33,600 --> 00:18:38,640
Microsoft, Accenture, and a Fortune 100

549
00:18:36,160 --> 00:18:40,640
electronics manufacturing firm. And

550
00:18:38,640 --> 00:18:42,480
that's that study found that those who

551
00:18:40,640 --> 00:18:43,760
were using the coding assistant were

552
00:18:42,480 --> 00:18:45,280
more productive. And that was

553
00:18:43,760 --> 00:18:47,600
particularly true among more junior

554
00:18:45,280 --> 00:18:49,600
employees and newer employees to these

555
00:18:47,600 --> 00:18:51,679
firms. Uh the caveat is that it was

556
00:18:49,600 --> 00:18:53,600
judging productivity by quantity of

557
00:18:51,679 --> 00:18:55,600
output, not necessarily the quality of

558
00:18:53,600 --> 00:18:57,280
code. So just keep that in mind. And

559
00:18:55,600 --> 00:18:58,400
then there's also vibe coding, which is

560
00:18:57,280 --> 00:19:01,120
something that you may have heard of.

561
00:18:58,400 --> 00:19:03,280
It's this other way of doing uh coding

562
00:19:01,120 --> 00:19:04,559
with uh an LLM where you're basically

563
00:19:03,280 --> 00:19:05,760
kind of just prompting it and you're not

564
00:19:04,559 --> 00:19:07,600
going into the code yourself. You're

565
00:19:05,760 --> 00:19:09,360
just going off of the vibes of whatever

566
00:19:07,600 --> 00:19:11,360
it gives you and kind of having this

567
00:19:09,360 --> 00:19:12,720
very low-key exchange. uh which is a

568
00:19:11,360 --> 00:19:14,480
really creative interesting way to use

569
00:19:12,720 --> 00:19:16,720
it and it can help people with no coding

570
00:19:14,480 --> 00:19:18,320
experience kind of start to make things

571
00:19:16,720 --> 00:19:19,600
um but it's probably not a best practice

572
00:19:18,320 --> 00:19:21,440
for most companies because you can

573
00:19:19,600 --> 00:19:22,799
introduce a lot of security um

574
00:19:21,440 --> 00:19:25,679
challenges that way and bugs in the

575
00:19:22,799 --> 00:19:26,880
code. And then biology is another area

576
00:19:25,679 --> 00:19:29,840
where we're starting to see some pretty

577
00:19:26,880 --> 00:19:31,679
miraculous um things happen. Google deep

578
00:19:29,840 --> 00:19:34,160
Google DeepMind's Alpha Fold of course

579
00:19:31,679 --> 00:19:35,919
last year won the Nobel Prize for its

580
00:19:34,160 --> 00:19:37,440
ability to predict protein folding.

581
00:19:35,919 --> 00:19:39,520
proteins are these building blocks of

582
00:19:37,440 --> 00:19:41,919
life and the way that they fold impacts

583
00:19:39,520 --> 00:19:44,080
their function and errors in that can

584
00:19:41,919 --> 00:19:45,600
cause disease. Um last year that team

585
00:19:44,080 --> 00:19:47,280
within the last year that team has also

586
00:19:45,600 --> 00:19:49,280
started to predict the behavior of DNA

587
00:19:47,280 --> 00:19:50,480
and RNA and there's a lot of hope in

588
00:19:49,280 --> 00:19:52,559
this space that this will help us

589
00:19:50,480 --> 00:19:55,679
discover new treatments and find new

590
00:19:52,559 --> 00:19:57,120
drugs and I hope it does too. Um, next

591
00:19:55,679 --> 00:19:58,640
we're seeing I just wanted to mention

592
00:19:57,120 --> 00:20:00,320
this. This is a newer area, but we are

593
00:19:58,640 --> 00:20:02,160
seeing uh some big names in the AI

594
00:20:00,320 --> 00:20:04,000
industry put a lot of uh effort and

595
00:20:02,160 --> 00:20:06,000
investment into what's called world

596
00:20:04,000 --> 00:20:09,360
models, which are these new kinds of

597
00:20:06,000 --> 00:20:12,320
models that are um able to generate very

598
00:20:09,360 --> 00:20:14,559
realistic 3D environments from a prompt

599
00:20:12,320 --> 00:20:16,240
uh that do obey the laws of our physical

600
00:20:14,559 --> 00:20:19,200
world and which an AI could then go off

601
00:20:16,240 --> 00:20:21,200
and train in and maybe learn the way our

602
00:20:19,200 --> 00:20:23,520
world works much like a human child

603
00:20:21,200 --> 00:20:25,280
works. the thinking goes um this could

604
00:20:23,520 --> 00:20:27,520
be useful for areas like robotics or

605
00:20:25,280 --> 00:20:29,600
self-driving cars. You can imagine uh

606
00:20:27,520 --> 00:20:32,159
Google's Genie3 is one example of this

607
00:20:29,600 --> 00:20:34,320
and FE Lee the Stanford professor uh who

608
00:20:32,159 --> 00:20:35,520
started her company World Labs is also

609
00:20:34,320 --> 00:20:36,799
working to develop these. I think you're

610
00:20:35,520 --> 00:20:39,840
going to hear a lot more about world

611
00:20:36,799 --> 00:20:41,120
models in the coming year.

612
00:20:39,840 --> 00:20:42,640
All right. So far, we've been talking

613
00:20:41,120 --> 00:20:44,320
mostly about these different models that

614
00:20:42,640 --> 00:20:45,919
are out there, some practical uses for

615
00:20:44,320 --> 00:20:47,120
them, but in these next few slides, I

616
00:20:45,919 --> 00:20:48,480
just wanted to kind of step back and

617
00:20:47,120 --> 00:20:50,240
talk about some of the bigger picture

618
00:20:48,480 --> 00:20:52,559
things we know about how these models

619
00:20:50,240 --> 00:20:54,880
work and some of the the issues around

620
00:20:52,559 --> 00:20:56,640
them. So, one is that hallucination is a

621
00:20:54,880 --> 00:20:59,440
feature, not a bug. It comes from the

622
00:20:56,640 --> 00:21:01,039
way that these models are built, and it

623
00:20:59,440 --> 00:21:03,840
it isn't a problem that's going to be

624
00:21:01,039 --> 00:21:05,440
able to uh probably be resolved. And to

625
00:21:03,840 --> 00:21:06,799
illustrate this, here's a meme that

626
00:21:05,440 --> 00:21:10,559
started making its way around after

627
00:21:06,799 --> 00:21:12,320
ChatGBT was released in late 2022. And

628
00:21:10,559 --> 00:21:14,480
it sort of explains a little bit about

629
00:21:12,320 --> 00:21:16,000
the architecture of how these models

630
00:21:14,480 --> 00:21:18,080
were created. Uh, ingesting large

631
00:21:16,000 --> 00:21:19,679
amounts of data, unstructured data from

632
00:21:18,080 --> 00:21:22,159
the internet. That's the unsupervised

633
00:21:19,679 --> 00:21:23,280
learning big ugly monster head there.

634
00:21:22,159 --> 00:21:25,039
And then you have supervised

635
00:21:23,280 --> 00:21:26,320
fine-tuning, which is the purple face

636
00:21:25,039 --> 00:21:28,000
that looks a little less scary, but

637
00:21:26,320 --> 00:21:29,840
still kind of dicey. And that's

638
00:21:28,000 --> 00:21:32,080
referring to the practice of going back

639
00:21:29,840 --> 00:21:34,400
in with much more cleaner kind of better

640
00:21:32,080 --> 00:21:35,840
labeled data sets in specific domains to

641
00:21:34,400 --> 00:21:37,360
help these models develop a little bit

642
00:21:35,840 --> 00:21:40,080
more expertise and improve their

643
00:21:37,360 --> 00:21:43,679
performance in a few key areas. And then

644
00:21:40,080 --> 00:21:45,600
you have the RLHF uh yellow smiley face

645
00:21:43,679 --> 00:21:47,840
which is uh stands for reinforcement

646
00:21:45,600 --> 00:21:49,440
learning from human feedback. And that's

647
00:21:47,840 --> 00:21:51,520
a practice that more of the big tech

648
00:21:49,440 --> 00:21:53,919
companies are using to basically go back

649
00:21:51,520 --> 00:21:56,799
in and manually reprogram and put

650
00:21:53,919 --> 00:21:58,480
patches on parts of the model that are

651
00:21:56,799 --> 00:22:00,640
returning harmful content or that are

652
00:21:58,480 --> 00:22:02,559
allowing users to jailbreak the model.

653
00:22:00,640 --> 00:22:04,000
Um, so you get the impression the artist

654
00:22:02,559 --> 00:22:05,600
was trying to convey that, you know,

655
00:22:04,000 --> 00:22:07,120
this is a lot of like polish being put

656
00:22:05,600 --> 00:22:10,400
on something that still has some pretty

657
00:22:07,120 --> 00:22:11,919
big fundamental issues here.

658
00:22:10,400 --> 00:22:13,280
These models obviously don't understand

659
00:22:11,919 --> 00:22:16,320
like what they're doing. They don't have

660
00:22:13,280 --> 00:22:17,760
a degree of comprehension. um they are

661
00:22:16,320 --> 00:22:19,280
essentially predicting the next word in

662
00:22:17,760 --> 00:22:21,039
a sentence or the next pixel in an image

663
00:22:19,280 --> 00:22:22,640
based on all the data that they've been

664
00:22:21,039 --> 00:22:23,760
uh trained on. And it can be hard to

665
00:22:22,640 --> 00:22:24,720
know when they're getting it right and

666
00:22:23,760 --> 00:22:26,080
when they're getting it wrong.

667
00:22:24,720 --> 00:22:27,760
Increasingly they're getting it more and

668
00:22:26,080 --> 00:22:30,000
more right, which is great because a lot

669
00:22:27,760 --> 00:22:31,360
of these fixes and fine-tuning. Um but

670
00:22:30,000 --> 00:22:33,039
it's still the case that sometimes you

671
00:22:31,360 --> 00:22:35,120
can put the same prompt into the same

672
00:22:33,039 --> 00:22:37,200
model and have, you know, a couple of

673
00:22:35,120 --> 00:22:39,360
tries at that uh over and over and it'll

674
00:22:37,200 --> 00:22:41,039
produce accurate answers uh part of the

675
00:22:39,360 --> 00:22:42,240
time and inaccurate answers other parts

676
00:22:41,039 --> 00:22:43,679
of the time, which is just good to keep

677
00:22:42,240 --> 00:22:45,840
in mind.

678
00:22:43,679 --> 00:22:47,679
Uh this is one just real world

679
00:22:45,840 --> 00:22:49,520
consequence relatively low stakes but

680
00:22:47,679 --> 00:22:51,200
still um you might have seen this over

681
00:22:49,520 --> 00:22:52,799
the summer. The Chicago Sun Times put

682
00:22:51,200 --> 00:22:54,159
out a summer reading list and

683
00:22:52,799 --> 00:22:56,000
unbeknownst to the publication the

684
00:22:54,159 --> 00:22:58,080
author had used generative AI to create

685
00:22:56,000 --> 00:23:00,400
it and so the model returned a list of

686
00:22:58,080 --> 00:23:02,480
real authors with books that they never

687
00:23:00,400 --> 00:23:04,320
wrote and which didn't actually exist.

688
00:23:02,480 --> 00:23:05,760
So just a reminder to just double check

689
00:23:04,320 --> 00:23:07,360
whatever you're getting out of the

690
00:23:05,760 --> 00:23:10,080
models and don't use them for anything

691
00:23:07,360 --> 00:23:12,159
like too consequential.

692
00:23:10,080 --> 00:23:13,760
All right, just two topics left. Um one

693
00:23:12,159 --> 00:23:15,679
you have probably heard that AI is

694
00:23:13,760 --> 00:23:17,760
getting power is very power hungry

695
00:23:15,679 --> 00:23:19,600
already and getting hungrier. There's a

696
00:23:17,760 --> 00:23:23,200
lot of uh discussion around this. You

697
00:23:19,600 --> 00:23:24,880
know AI requiring more uh computing and

698
00:23:23,200 --> 00:23:26,720
more electricity to run all the servers

699
00:23:24,880 --> 00:23:29,039
in these massive data centers that are

700
00:23:26,720 --> 00:23:31,440
being um built. There's a lot of concern

701
00:23:29,039 --> 00:23:33,360
as well about this demand for AI that's

702
00:23:31,440 --> 00:23:36,799
increasing exacerbating climate change

703
00:23:33,360 --> 00:23:38,480
and driving up uh carbon emissions.

704
00:23:36,799 --> 00:23:40,240
And so this is uh these are some charts

705
00:23:38,480 --> 00:23:41,919
from a report by the International

706
00:23:40,240 --> 00:23:43,679
Energy Agency that was released in the

707
00:23:41,919 --> 00:23:46,799
spring just illustrating what this

708
00:23:43,679 --> 00:23:48,080
increase in demand might look like. Um

709
00:23:46,799 --> 00:23:49,600
and you can see that most of the data

710
00:23:48,080 --> 00:23:50,799
centers in the world are in the US and

711
00:23:49,600 --> 00:23:53,360
China. So that's where the greatest

712
00:23:50,799 --> 00:23:55,440
projected demand is meant to be. And

713
00:23:53,360 --> 00:23:57,760
there is of course more demand coming

714
00:23:55,440 --> 00:23:59,200
from AI for energy and electricity, but

715
00:23:57,760 --> 00:24:01,120
that's also true for all kinds of other

716
00:23:59,200 --> 00:24:03,360
stuff like electric vehicles and air

717
00:24:01,120 --> 00:24:05,679
conditioning and heat pumps etc. Uh and

718
00:24:03,360 --> 00:24:06,880
so the real question is can we power an

719
00:24:05,679 --> 00:24:08,960
increasing share of these with

720
00:24:06,880 --> 00:24:10,400
renewables uh and nuclear instead of

721
00:24:08,960 --> 00:24:12,880
fossil fuels. And so you'll see on the

722
00:24:10,400 --> 00:24:14,559
other chart um most of the electricity

723
00:24:12,880 --> 00:24:17,120
provided to data centers today comes

724
00:24:14,559 --> 00:24:18,720
from fossil fuels, natural gas and coal.

725
00:24:17,120 --> 00:24:20,400
And that will be the case and these

726
00:24:18,720 --> 00:24:22,559
projections for the next five years. And

727
00:24:20,400 --> 00:24:24,480
then starting in 2030, you start to see

728
00:24:22,559 --> 00:24:26,960
renewables and nuclear uh coming more

729
00:24:24,480 --> 00:24:29,440
into the mix there.

730
00:24:26,960 --> 00:24:31,279
We did a big package on this issue uh in

731
00:24:29,440 --> 00:24:32,559
the spring called Tower Hungry. If

732
00:24:31,279 --> 00:24:34,320
you're interested, you can look that up.

733
00:24:32,559 --> 00:24:36,000
and learn more. It's not just an energy

734
00:24:34,320 --> 00:24:37,600
and electricity issue. It's also a water

735
00:24:36,000 --> 00:24:39,840
issue and we had a story about that.

736
00:24:37,600 --> 00:24:41,600
This is a data center um complex being

737
00:24:39,840 --> 00:24:44,400
built in Reno, Nevada, which is of

738
00:24:41,600 --> 00:24:46,880
course a desert.

739
00:24:44,400 --> 00:24:48,400
And uh there are other efforts being

740
00:24:46,880 --> 00:24:50,240
being made which we also have a story

741
00:24:48,400 --> 00:24:52,400
about in that package to make data

742
00:24:50,240 --> 00:24:54,640
centers more efficient. Um ways to

743
00:24:52,400 --> 00:24:56,240
manage their waste heat and also for

744
00:24:54,640 --> 00:24:57,760
chips to manage their temperature a

745
00:24:56,240 --> 00:24:58,799
little bit better. Um, so this is

746
00:24:57,760 --> 00:25:01,919
definitely a space that we're going to

747
00:24:58,799 --> 00:25:03,679
be following uh along with. When the

748
00:25:01,919 --> 00:25:05,120
team that produced that package first

749
00:25:03,679 --> 00:25:06,559
started working on their story, they

750
00:25:05,120 --> 00:25:08,880
really couldn't get any information from

751
00:25:06,559 --> 00:25:10,320
Google or OpenAI about how much energy

752
00:25:08,880 --> 00:25:13,360
these large language models were using

753
00:25:10,320 --> 00:25:15,760
or how much a individual prompt uh would

754
00:25:13,360 --> 00:25:17,679
consume. But eventually uh after we

755
00:25:15,760 --> 00:25:18,880
published Google and open did release a

756
00:25:17,679 --> 00:25:20,880
little bit more information and Google

757
00:25:18,880 --> 00:25:22,880
has said that the median prompt so in

758
00:25:20,880 --> 00:25:25,200
the middle of the distribution uh that

759
00:25:22,880 --> 00:25:27,200
they handle produces the same or

760
00:25:25,200 --> 00:25:28,720
requires the same amount of power to as

761
00:25:27,200 --> 00:25:31,200
it would to run a mic a standard

762
00:25:28,720 --> 00:25:33,200
microwave for uh one second and about

763
00:25:31,200 --> 00:25:34,720
five drops of water. Uh if you're

764
00:25:33,200 --> 00:25:36,400
concerned about your personal AI use,

765
00:25:34,720 --> 00:25:38,559
our analysis did also show that

766
00:25:36,400 --> 00:25:41,200
generating videos is a lot more energy

767
00:25:38,559 --> 00:25:42,960
intensive than images or text. So maybe

768
00:25:41,200 --> 00:25:45,600
just go easy on the go easy on the

769
00:25:42,960 --> 00:25:46,960
videos if that's important to you. Um,

770
00:25:45,600 --> 00:25:48,720
next final note, I just wanted to end on

771
00:25:46,960 --> 00:25:50,400
a bit of a philosophical one, which is

772
00:25:48,720 --> 00:25:51,760
that nobody really knows how these large

773
00:25:50,400 --> 00:25:53,440
language models work, which is sort of

774
00:25:51,760 --> 00:25:55,120
just fascinating to think about. Not

775
00:25:53,440 --> 00:25:56,159
even the the researchers and the tech

776
00:25:55,120 --> 00:25:57,919
companies that are building them and

777
00:25:56,159 --> 00:25:59,840
putting them out. We know that they are,

778
00:25:57,919 --> 00:26:01,360
you know, built uh using deep learning

779
00:25:59,840 --> 00:26:03,120
techniques, built on neural networks

780
00:26:01,360 --> 00:26:05,039
that are meant to mimic the architecture

781
00:26:03,120 --> 00:26:06,880
of our brains. But as far as how they

782
00:26:05,039 --> 00:26:08,960
learn, what they learn, and how they

783
00:26:06,880 --> 00:26:11,200
take inputs and produce outputs, it's

784
00:26:08,960 --> 00:26:12,799
all still a bit of a mystery. There are

785
00:26:11,200 --> 00:26:14,159
some cool new research techniques being

786
00:26:12,799 --> 00:26:16,000
developed to help us get a better

787
00:26:14,159 --> 00:26:17,520
glimpse inside that black box, though.

788
00:26:16,000 --> 00:26:19,520
One of them is called a mechanistic

789
00:26:17,520 --> 00:26:21,679
interpretability. Um, it's a way of

790
00:26:19,520 --> 00:26:24,159
trying to analyze the circuits within

791
00:26:21,679 --> 00:26:26,400
the uh models and try to trace kind of

792
00:26:24,159 --> 00:26:28,880
the inputs to the outputs and understand

793
00:26:26,400 --> 00:26:31,360
more about that process.

794
00:26:28,880 --> 00:26:33,039
Just a few fun examples here. uh Claude,

795
00:26:31,360 --> 00:26:34,880
which is anthropic's large language

796
00:26:33,039 --> 00:26:36,720
model. Uh researchers there took a look

797
00:26:34,880 --> 00:26:39,039
at that using some of these techniques

798
00:26:36,720 --> 00:26:40,799
and they noticed that um there were

799
00:26:39,039 --> 00:26:43,200
certain parts of the model that were

800
00:26:40,799 --> 00:26:45,279
associated with celebrities, certain

801
00:26:43,200 --> 00:26:48,320
celebrities, so like Michael Jordan or

802
00:26:45,279 --> 00:26:49,679
certain concepts like potatoes uh or

803
00:26:48,320 --> 00:26:51,840
even abstract concepts like

804
00:26:49,679 --> 00:26:53,520
disobedience. And they found a section

805
00:26:51,840 --> 00:26:55,440
of the model that was seemed to be like

806
00:26:53,520 --> 00:26:57,760
associated with the Golden Gate Bridge

807
00:26:55,440 --> 00:26:59,440
because um every time it was like shown

808
00:26:57,760 --> 00:27:00,880
images of the bridge or asked a question

809
00:26:59,440 --> 00:27:03,600
about the bridge, that part of the model

810
00:27:00,880 --> 00:27:05,120
like lit up so to speak. Um and so then

811
00:27:03,600 --> 00:27:07,120
when they like amped up that part of the

812
00:27:05,120 --> 00:27:08,880
model, Claude started like putting the

813
00:27:07,120 --> 00:27:10,799
Golden Gate Bridge in every answer that

814
00:27:08,880 --> 00:27:12,400
it produced about anything. So even if

815
00:27:10,799 --> 00:27:14,080
you ask it for like a recipe, it would

816
00:27:12,400 --> 00:27:16,240
like put the Golden Gate Bridge in the

817
00:27:14,080 --> 00:27:18,240
recipe. And at one point it even thought

818
00:27:16,240 --> 00:27:21,120
it was the Golden Gate Bridge. So that

819
00:27:18,240 --> 00:27:22,960
was just really like interesting. um

820
00:27:21,120 --> 00:27:24,880
experiment. This is another one also by

821
00:27:22,960 --> 00:27:27,200
the anthropic team. So they asked Claude

822
00:27:24,880 --> 00:27:28,640
to solve a simple math problem and it

823
00:27:27,200 --> 00:27:30,159
got the right answer and they said, "How

824
00:27:28,640 --> 00:27:31,520
did you solve that problem?" And it told

825
00:27:30,159 --> 00:27:33,440
them like a very straightforward

826
00:27:31,520 --> 00:27:36,400
explanation like you would read in any

827
00:27:33,440 --> 00:27:38,240
textbook. But then they used their their

828
00:27:36,400 --> 00:27:39,679
techniques uh circuit tracing techniques

829
00:27:38,240 --> 00:27:41,600
of showing kind of what the model was

830
00:27:39,679 --> 00:27:43,840
actually doing, not just taking its word

831
00:27:41,600 --> 00:27:46,159
for it. And it had this wild process

832
00:27:43,840 --> 00:27:48,080
that was not intuitive at all to how we

833
00:27:46,159 --> 00:27:49,679
would solve a math problem. So you can

834
00:27:48,080 --> 00:27:51,600
see it took the two numbers that they

835
00:27:49,679 --> 00:27:53,360
asked it to add and it sort of like

836
00:27:51,600 --> 00:27:54,880
estimated some numbers around those two

837
00:27:53,360 --> 00:27:57,360
numbers and then it looked at the final

838
00:27:54,880 --> 00:27:58,720
digits of each number and it said, "Oh,

839
00:27:57,360 --> 00:28:00,240
this answer is going to have to end in

840
00:27:58,720 --> 00:28:01,840
the number five." And then it sort of

841
00:28:00,240 --> 00:28:03,039
mashed those ideas together and it

842
00:28:01,840 --> 00:28:05,039
happened to come up with the right

843
00:28:03,039 --> 00:28:06,480
answer. But it's not at all the way that

844
00:28:05,039 --> 00:28:07,840
it said it was solving the problem. And

845
00:28:06,480 --> 00:28:09,600
it's just like fascinating to think that

846
00:28:07,840 --> 00:28:11,120
this is what's going on uh inside these.

847
00:28:09,600 --> 00:28:12,880
And I hope we'll learn more about what

848
00:28:11,120 --> 00:28:15,200
these models look like in the years to

849
00:28:12,880 --> 00:28:16,799
come. And then finally, last I just

850
00:28:15,200 --> 00:28:19,440
wanted to mention that um there's

851
00:28:16,799 --> 00:28:22,080
increasingly efforts uh to make AI that

852
00:28:19,440 --> 00:28:24,159
is better at making making itself

853
00:28:22,080 --> 00:28:26,080
better. So self-improving artificial

854
00:28:24,159 --> 00:28:27,600
intelligence. Uh Meta's super

855
00:28:26,080 --> 00:28:29,679
intelligence team is working on this

856
00:28:27,600 --> 00:28:31,520
trying to make uh AI that can develop

857
00:28:29,679 --> 00:28:33,279
its own training data or produce its own

858
00:28:31,520 --> 00:28:35,200
algorithms that will make it uh more

859
00:28:33,279 --> 00:28:36,720
efficient. Um, and there's other other

860
00:28:35,200 --> 00:28:39,919
efforts around this as well, including

861
00:28:36,720 --> 00:28:41,360
at uh Google uh with their alpha evolve

862
00:28:39,919 --> 00:28:42,480
program, which is taking large language

863
00:28:41,360 --> 00:28:44,559
models and using what's called

864
00:28:42,480 --> 00:28:47,360
evolutionary algorithms meant to be able

865
00:28:44,559 --> 00:28:49,600
to improve themselves over time. So, we

866
00:28:47,360 --> 00:28:52,080
might see accelerated progress in this

867
00:28:49,600 --> 00:28:53,520
field. Uh, from this, that's all I have

868
00:28:52,080 --> 00:28:54,720
for you. That was my whirlwind tour

869
00:28:53,520 --> 00:28:56,159
through the future of artificial

870
00:28:54,720 --> 00:28:57,520
intelligence. I don't have a crystal

871
00:28:56,159 --> 00:28:59,120
crystal ball. These are all educated

872
00:28:57,520 --> 00:29:00,399
guesses. That's what we do in our

873
00:28:59,120 --> 00:29:02,799
newsroom. Sometimes we're right.

874
00:29:00,399 --> 00:29:04,720
oftentimes, you know, things evolve in

875
00:29:02,799 --> 00:29:06,159
unexpected ways. Um, if you want to

876
00:29:04,720 --> 00:29:08,000
follow along with our coverage on these

877
00:29:06,159 --> 00:29:09,840
topics, there's a QR code here. You can

878
00:29:08,000 --> 00:29:11,200
scan and sign up for our morning

879
00:29:09,840 --> 00:29:13,039
newsletter. It's called the download. It

880
00:29:11,200 --> 00:29:14,240
comes out at 8 a.m. Eastern every day.

881
00:29:13,039 --> 00:29:16,640
It's free to sign up for. And there's

882
00:29:14,240 --> 00:29:18,480
also a subscription offer here as well.

883
00:29:16,640 --> 00:29:21,120
Um, if we have time for questions, I'm

884
00:29:18,480 --> 00:29:22,559
happy to take a few. And also, um, if

885
00:29:21,120 --> 00:29:24,640
anybody has any like thing they would

886
00:29:22,559 --> 00:29:26,240
love to talk to me about later that they

887
00:29:24,640 --> 00:29:27,600
would love to see more in our magazine,

888
00:29:26,240 --> 00:29:30,480
I'm always interested in hearing, uh,

889
00:29:27,600 --> 00:29:32,080
ideas for that as well.

890
00:29:30,480 --> 00:29:34,000
Um yes. Okay. So this is a great

891
00:29:32,080 --> 00:29:36,480
question. So we also have this other

892
00:29:34,000 --> 00:29:39,360
list called innovators under 35 which I

893
00:29:36,480 --> 00:29:41,600
think this is referring to. Um you know

894
00:29:39,360 --> 00:29:43,679
this year actually

895
00:29:41,600 --> 00:29:44,799
this year so we have done this in uh

896
00:29:43,679 --> 00:29:47,039
we've been doing this list the

897
00:29:44,799 --> 00:29:51,679
breakthrough technologies list for this

898
00:29:47,039 --> 00:29:53,919
will be the 25th year 26th year. And at

899
00:29:51,679 --> 00:29:56,240
the 20-year mark, one of our editors,

900
00:29:53,919 --> 00:29:57,679
David, who was on stage earlier, uh he

901
00:29:56,240 --> 00:29:59,679
went back and looked at the very first

902
00:29:57,679 --> 00:30:01,120
list of 10 breakthrough technologies, at

903
00:29:59,679 --> 00:30:02,720
what was on that list and like did, you

904
00:30:01,120 --> 00:30:04,240
know, evaluated our record. And there

905
00:30:02,720 --> 00:30:06,320
were a few things on there like natural

906
00:30:04,240 --> 00:30:08,000
language processing, which is still very

907
00:30:06,320 --> 00:30:09,919
relevant and kind of underpins the large

908
00:30:08,000 --> 00:30:11,840
language models we have today. Uh brain

909
00:30:09,919 --> 00:30:13,679
computer interfaces, which you know have

910
00:30:11,840 --> 00:30:14,960
been a little bit slower um to develop,

911
00:30:13,679 --> 00:30:16,960
but that we're still talking about

912
00:30:14,960 --> 00:30:18,320
today, which is pretty interesting. And

913
00:30:16,960 --> 00:30:20,640
uh this year I'm actually participating

914
00:30:18,320 --> 00:30:22,000
with a MIT class that is doing that same

915
00:30:20,640 --> 00:30:24,159
exercise. They're looking into our

916
00:30:22,000 --> 00:30:25,440
archives and um we're having a

917
00:30:24,159 --> 00:30:27,279
conversation kind of about what they're

918
00:30:25,440 --> 00:30:28,399
finding and which technologies haven't

919
00:30:27,279 --> 00:30:30,960
worked out because there are definitely

920
00:30:28,399 --> 00:30:33,840
some flops in there and um talking

921
00:30:30,960 --> 00:30:35,840
through kind of what uh what what caused

922
00:30:33,840 --> 00:30:37,279
them to not work out and what might have

923
00:30:35,840 --> 00:30:39,039
might have been changed to make them

924
00:30:37,279 --> 00:30:40,880
have a better chance. Um so I'm always

925
00:30:39,039 --> 00:30:41,919
interested in uh in that as well now

926
00:30:40,880 --> 00:30:43,520
that especially we have this long

927
00:30:41,919 --> 00:30:44,799
history of it. I think that's all we

928
00:30:43,520 --> 00:30:46,000
have time for, but thank you all so much

929
00:30:44,799 --> 00:30:49,480
for your attention this late in the day.

930
00:30:46,000 --> 00:30:49,480
I appreciate it.

