1
00:00:04,359 --> 00:00:11,799
next speaker is Dr Lian hary uh he is a

2
00:00:08,719 --> 00:00:15,480
research scientist at MIT Quantum honic

3
00:00:11,799 --> 00:00:18,560
and AI group and a postdoc fellow at

4
00:00:15,480 --> 00:00:22,600
Professor DK ingland laboratory and also

5
00:00:18,560 --> 00:00:25,039
he is a senior scientist at NT research

6
00:00:22,600 --> 00:00:28,519
physics and informatics

7
00:00:25,039 --> 00:00:31,320
love his talk is about the research of

8
00:00:28,519 --> 00:00:33,800
honic computing to solve the problem of

9
00:00:31,320 --> 00:00:37,200
high demand of computation energy for

10
00:00:33,800 --> 00:00:37,200
data center

11
00:00:44,879 --> 00:00:49,719
so all right so thanks Micky for that

12
00:00:47,559 --> 00:00:51,239
introduction and today I'm going to be

13
00:00:49,719 --> 00:00:54,079
talking about Optical neural networks

14
00:00:51,239 --> 00:00:55,359
and Computing with light so a little

15
00:00:54,079 --> 00:00:57,960
background of myself before I get

16
00:00:55,359 --> 00:01:00,160
started uh I actually come into this

17
00:00:57,960 --> 00:01:02,480
field of applied engineering from

18
00:01:00,160 --> 00:01:04,320
theoretical physics background so I was

19
00:01:02,480 --> 00:01:06,439
at the other Institute in technology

20
00:01:04,320 --> 00:01:08,240
Caltech where I studied theoretical

21
00:01:06,439 --> 00:01:12,159
physics relativity Theory and black

22
00:01:08,240 --> 00:01:14,040
holes then I went to Stanford where I uh

23
00:01:12,159 --> 00:01:16,280
got to know this professor named Yoshi

24
00:01:14,040 --> 00:01:18,840
Yamamoto that I worked with in Japan for

25
00:01:16,280 --> 00:01:20,640
one year on these uh uh Computing

26
00:01:18,840 --> 00:01:21,960
machines called coherent icing machines

27
00:01:20,640 --> 00:01:24,079
and that's research that eventually

28
00:01:21,960 --> 00:01:27,280
brought me back to MIT and this

29
00:01:24,079 --> 00:01:29,600
interesting work in Optical Computing so

30
00:01:27,280 --> 00:01:32,799
co coherent icing machines the original

31
00:01:29,600 --> 00:01:36,079
vision was merge the classical in the

32
00:01:32,799 --> 00:01:38,600
quantum and into and create a machine

33
00:01:36,079 --> 00:01:41,079
that can solve hard optimization

34
00:01:38,600 --> 00:01:42,439
problems so this is a kind of a it's a

35
00:01:41,079 --> 00:01:44,560
what we call an optical parametric

36
00:01:42,439 --> 00:01:47,560
oscillator you can think of this as one

37
00:01:44,560 --> 00:01:49,119
part laser and one part bit it's a laser

38
00:01:47,560 --> 00:01:52,079
that oscillates in one of two phase

39
00:01:49,119 --> 00:01:55,280
States and that uh that phase State

40
00:01:52,079 --> 00:01:56,840
encodes the bit and it has some Quant

41
00:01:55,280 --> 00:01:58,600
interesting Quantum properties when the

42
00:01:56,840 --> 00:02:00,079
opo is below threshold this gives you

43
00:01:58,600 --> 00:02:02,399
squeezing which is a very use full

44
00:02:00,079 --> 00:02:03,880
Quantum resource um but it also has

45
00:02:02,399 --> 00:02:05,520
Classical properties if you go above

46
00:02:03,880 --> 00:02:07,680
threshold it's just a coherent state so

47
00:02:05,520 --> 00:02:10,440
that's classical and we realized you

48
00:02:07,680 --> 00:02:12,239
could solve use these uh use these

49
00:02:10,440 --> 00:02:14,080
parametric oscillators to solve what's

50
00:02:12,239 --> 00:02:16,040
called an icing problem which is a hard

51
00:02:14,080 --> 00:02:18,959
optimization problem as part of this NB

52
00:02:16,040 --> 00:02:21,680
hard uh class but the thing that I that

53
00:02:18,959 --> 00:02:24,800
really inspired me was uh just the fact

54
00:02:21,680 --> 00:02:27,800
that Optics provides a a very strong

55
00:02:24,800 --> 00:02:29,239
resource in in the efficiency of these

56
00:02:27,800 --> 00:02:31,599
icing machines so we did this

57
00:02:29,239 --> 00:02:34,080
benchmarking study this was my my work

58
00:02:31,599 --> 00:02:36,040
when I was here in Japan uh comparing

59
00:02:34,080 --> 00:02:38,080
this machine to another kind of quantum

60
00:02:36,040 --> 00:02:39,879
computer or the d-wave quantum maner and

61
00:02:38,080 --> 00:02:41,800
that was really Hut a couple years ago

62
00:02:39,879 --> 00:02:44,080
and we found that the optical machine

63
00:02:41,800 --> 00:02:45,959
does a I mean orders of magnitude better

64
00:02:44,080 --> 00:02:47,840
at a large number of these icing

65
00:02:45,959 --> 00:02:50,159
problems especially the ones with dense

66
00:02:47,840 --> 00:02:53,040
connectivity that you can't easily embed

67
00:02:50,159 --> 00:02:54,879
into the sparse d-waves hardware and so

68
00:02:53,040 --> 00:02:57,360
this immediately told us that there's

69
00:02:54,879 --> 00:03:01,200
something really special about the use

70
00:02:57,360 --> 00:03:02,720
for Optics in a computation algorithms

71
00:03:01,200 --> 00:03:05,440
and I guess at the same time there was a

72
00:03:02,720 --> 00:03:07,000
lot of uh work that I was not aware of

73
00:03:05,440 --> 00:03:09,200
which is related to Optical neural

74
00:03:07,000 --> 00:03:11,560
networks which really got started back

75
00:03:09,200 --> 00:03:14,519
in the 80s and the 70s uh but there was

76
00:03:11,560 --> 00:03:17,159
a Resurgence of that uh in the late

77
00:03:14,519 --> 00:03:18,840
2010s and when I saw that work I

78
00:03:17,159 --> 00:03:20,159
immediately saw oh that makes a lot of

79
00:03:18,840 --> 00:03:21,799
sense we knew that Optics was going to

80
00:03:20,159 --> 00:03:24,000
be very useful for computing and so

81
00:03:21,799 --> 00:03:26,080
that's going to be really the the story

82
00:03:24,000 --> 00:03:28,519
of this talk I'm going to start with a

83
00:03:26,080 --> 00:03:30,599
more of a background about why we need

84
00:03:28,519 --> 00:03:32,200
Optical Computing that I'm going to talk

85
00:03:30,599 --> 00:03:34,720
about three different approaches for our

86
00:03:32,200 --> 00:03:36,959
research groups into using optics for

87
00:03:34,720 --> 00:03:39,360
computing so the background is really

88
00:03:36,959 --> 00:03:40,879
motivated by how deep learning works and

89
00:03:39,360 --> 00:03:43,280
what the limits to deep learning are

90
00:03:40,879 --> 00:03:46,159
going to be on the hardware side uh so

91
00:03:43,280 --> 00:03:48,400
deep learning is uh based on this

92
00:03:46,159 --> 00:03:50,640
fundamental block called the perceptron

93
00:03:48,400 --> 00:03:51,599
this is an old idea I think this goes

94
00:03:50,640 --> 00:03:54,480
back to

95
00:03:51,599 --> 00:03:56,680
1958 and the first perceptron was a

96
00:03:54,480 --> 00:03:59,000
physical device that was uh built at

97
00:03:56,680 --> 00:04:02,000
Cornell University and basically a

98
00:03:59,000 --> 00:04:03,799
perceptron it sums up a bunch of inputs

99
00:04:02,000 --> 00:04:06,599
which are appropriately weighted and

100
00:04:03,799 --> 00:04:09,400
then it thresholds that output and that

101
00:04:06,599 --> 00:04:12,239
uh can do a pretty decent job at a lot

102
00:04:09,400 --> 00:04:14,879
of simple learning tasks such as a digit

103
00:04:12,239 --> 00:04:17,400
classification uh but at the same time

104
00:04:14,879 --> 00:04:19,959
uh there there is a linearity in this

105
00:04:17,400 --> 00:04:22,720
perceptron relation and that leads to

106
00:04:19,959 --> 00:04:25,440
the perceptron not doing well at many

107
00:04:22,720 --> 00:04:28,080
very trivial tasks so a great example is

108
00:04:25,440 --> 00:04:29,720
here you have three different uh

109
00:04:28,080 --> 00:04:31,880
classification tasks I want to sep

110
00:04:29,720 --> 00:04:33,400
separate the red dots from the blue dots

111
00:04:31,880 --> 00:04:35,520
and the perceptron thanks to the

112
00:04:33,400 --> 00:04:37,320
linearity of this function here it can't

113
00:04:35,520 --> 00:04:38,600
do these two on the right it can only do

114
00:04:37,320 --> 00:04:40,880
the one on the left the so-called

115
00:04:38,600 --> 00:04:43,919
linearly separable uh

116
00:04:40,880 --> 00:04:45,720
problem and so that is a it's called the

117
00:04:43,919 --> 00:04:47,600
exor problem because the simplest

118
00:04:45,720 --> 00:04:50,039
problem that the perceptron cannot solve

119
00:04:47,600 --> 00:04:54,680
is just the exor operation that's just a

120
00:04:50,039 --> 00:04:57,360
gate and this is it was one of the big

121
00:04:54,680 --> 00:04:59,840
uh big challenges to actually using

122
00:04:57,360 --> 00:05:01,880
neural networks when these were initi

123
00:04:59,840 --> 00:05:04,240
initially introduced the solution to

124
00:05:01,880 --> 00:05:05,919
that was just to realize that you need

125
00:05:04,240 --> 00:05:07,280
to have neurons in between your inputs

126
00:05:05,919 --> 00:05:09,120
and outputs so this is called a deep

127
00:05:07,280 --> 00:05:11,560
neuron Network and if each of those

128
00:05:09,120 --> 00:05:13,600
neurons implements a perceptron then it

129
00:05:11,560 --> 00:05:15,759
turns out that this has a lot more

130
00:05:13,600 --> 00:05:19,039
expressibility than just the perceptron

131
00:05:15,759 --> 00:05:20,919
alone and this is the basis for uh mod

132
00:05:19,039 --> 00:05:22,880
for deep learning now now actually when

133
00:05:20,919 --> 00:05:25,759
you implement these on in computer

134
00:05:22,880 --> 00:05:27,319
hardware you're not just randomly uh

135
00:05:25,759 --> 00:05:30,120
arranging the perceptrons like on the

136
00:05:27,319 --> 00:05:33,520
right left you arrange them into regular

137
00:05:30,120 --> 00:05:36,039
grids of arrays of data and that uh that

138
00:05:33,520 --> 00:05:38,479
gives you a much much faster performance

139
00:05:36,039 --> 00:05:40,759
in computers uh and when you do that you

140
00:05:38,479 --> 00:05:42,840
realize that deep learning or running

141
00:05:40,759 --> 00:05:45,960
these neural networks is really just two

142
00:05:42,840 --> 00:05:48,400
things uh that are interleaved one is

143
00:05:45,960 --> 00:05:50,360
linear Matrix Vector multiplication and

144
00:05:48,400 --> 00:05:52,479
the other is a nonlinear function and we

145
00:05:50,360 --> 00:05:53,840
just call that an activation function

146
00:05:52,479 --> 00:05:55,360
and if you want to write code to do this

147
00:05:53,840 --> 00:05:59,680
the simplest code would just be that on

148
00:05:55,360 --> 00:06:02,880
the right so this idea combined with

149
00:05:59,680 --> 00:06:05,639
with a couple of additional tools one is

150
00:06:02,880 --> 00:06:07,639
gradient-based optimization or training

151
00:06:05,639 --> 00:06:10,520
and additional types of layers such as

152
00:06:07,639 --> 00:06:13,039
convolutional layers led to the first

153
00:06:10,520 --> 00:06:15,120
successful and useful implementation of

154
00:06:13,039 --> 00:06:17,599
these perceptrons which would be the

155
00:06:15,120 --> 00:06:19,880
convolutional neural networks for uh

156
00:06:17,599 --> 00:06:22,160
digit or character classification so

157
00:06:19,880 --> 00:06:25,840
this was work that was done by Yan laon

158
00:06:22,160 --> 00:06:28,759
back when he was at Bell labs and it was

159
00:06:25,840 --> 00:06:31,120
very successful at this task but uh it

160
00:06:28,759 --> 00:06:34,000
didn't the Tech the technique didn't

161
00:06:31,120 --> 00:06:35,840
really catch catch on and the reason was

162
00:06:34,000 --> 00:06:37,319
that there were many other machine

163
00:06:35,840 --> 00:06:40,240
learning techniques that were a lot

164
00:06:37,319 --> 00:06:42,080
easier to use uh more familiar to people

165
00:06:40,240 --> 00:06:43,039
that did the same thing with almost the

166
00:06:42,080 --> 00:06:44,960
same

167
00:06:43,039 --> 00:06:47,720
accuracy and so that's where the field

168
00:06:44,960 --> 00:06:49,919
stood for quite a while and it was only

169
00:06:47,720 --> 00:06:52,840
with the Advent of much more complicated

170
00:06:49,919 --> 00:06:54,360
data sets where you needed these deep

171
00:06:52,840 --> 00:06:56,639
neural networks to really solve the

172
00:06:54,360 --> 00:07:00,919
problems so there is this imag net data

173
00:06:56,639 --> 00:07:03,000
set this was uh formulated in

174
00:07:00,919 --> 00:07:05,560
and people people tried the classic

175
00:07:03,000 --> 00:07:07,280
machine learning techniques to classify

176
00:07:05,560 --> 00:07:09,440
the images in this data set it's a

177
00:07:07,280 --> 00:07:12,560
thousand images not just let's say 30

178
00:07:09,440 --> 00:07:16,520
characters and it's millions of

179
00:07:12,560 --> 00:07:18,280
images and nothing worked well uh until

180
00:07:16,520 --> 00:07:19,960
uh these guys from Jeff hinton's group

181
00:07:18,280 --> 00:07:22,759
came along and created what's called

182
00:07:19,960 --> 00:07:25,400
alexnet so Alex net is very similar to

183
00:07:22,759 --> 00:07:27,919
Lynette except that it has much bigger

184
00:07:25,400 --> 00:07:29,720
layers and I guess the answer is a

185
00:07:27,919 --> 00:07:33,800
couple days out of date but that's when

186
00:07:29,720 --> 00:07:35,639
I first gave this talk um so it uh this

187
00:07:33,800 --> 00:07:38,440
really blew all all the competition out

188
00:07:35,639 --> 00:07:40,879
of the water and after the ne in the

189
00:07:38,440 --> 00:07:42,680
next couple of years uh this these

190
00:07:40,879 --> 00:07:44,639
neural networks were doing as well as

191
00:07:42,680 --> 00:07:47,879
humans at this very hard machine

192
00:07:44,639 --> 00:07:50,280
learning uh Benchmark problem um and so

193
00:07:47,879 --> 00:07:52,319
what's the difference between uh Lynette

194
00:07:50,280 --> 00:07:53,879
or Yan laon's initial experiment and

195
00:07:52,319 --> 00:07:56,199
Alex net there are a couple of

196
00:07:53,879 --> 00:07:59,840
theoretical changes but the big thing is

197
00:07:56,199 --> 00:08:01,479
just that Alex net is a lot bigger and

198
00:07:59,840 --> 00:08:03,520
you can quantify that by looking at the

199
00:08:01,479 --> 00:08:04,800
number of weights or the number of Max

200
00:08:03,520 --> 00:08:06,360
at a neural network those are things

201
00:08:04,800 --> 00:08:08,560
that are very easy to compute just

202
00:08:06,360 --> 00:08:10,560
looking at the uh the layer Dimensions

203
00:08:08,560 --> 00:08:14,039
which you can get out of any paper and

204
00:08:10,560 --> 00:08:17,159
you find that in the in lyette it's

205
00:08:14,039 --> 00:08:20,560
about 400 KX and 50 Kobes whereas in

206
00:08:17,159 --> 00:08:23,720
Alex net it's closer to 600 megax or 60

207
00:08:20,560 --> 00:08:25,759
meytes so that's a factor of 1,000

208
00:08:23,720 --> 00:08:27,960
increase in the amount of Max and the

209
00:08:25,759 --> 00:08:30,599
amount of Weights going from one to the

210
00:08:27,960 --> 00:08:32,440
other and it was that factor of a

211
00:08:30,599 --> 00:08:34,080
thousand that you needed in Hardware

212
00:08:32,440 --> 00:08:36,839
capabilities that really made deep

213
00:08:34,080 --> 00:08:39,399
learning take off and so that was an

214
00:08:36,839 --> 00:08:41,080
inflection point in the growth of uh the

215
00:08:39,399 --> 00:08:44,800
exponential growth of machine learning

216
00:08:41,080 --> 00:08:46,680
models that people saw in around 2012

217
00:08:44,800 --> 00:08:48,399
and after that point these models

218
00:08:46,680 --> 00:08:51,080
started growing exponentially but with a

219
00:08:48,399 --> 00:08:54,040
much faster uh time constant where I

220
00:08:51,080 --> 00:08:56,240
think at the peak of this um the model

221
00:08:54,040 --> 00:08:59,000
sizes were doubling every three months

222
00:08:56,240 --> 00:09:00,440
or so uh which really let makes you

223
00:08:59,000 --> 00:09:02,800
realize this is going to tax the

224
00:09:00,440 --> 00:09:05,079
hardware very quickly and so if you look

225
00:09:02,800 --> 00:09:07,200
at modern deep learning Hardware there

226
00:09:05,079 --> 00:09:09,600
are three things that are very critical

227
00:09:07,200 --> 00:09:12,560
one is that all of this Hardware is

228
00:09:09,600 --> 00:09:14,720
optimized to to optimally compute these

229
00:09:12,560 --> 00:09:15,800
Matrix Matrix multiplications which is

230
00:09:14,720 --> 00:09:18,600
the linear

231
00:09:15,800 --> 00:09:20,640
algebra second is that they're all bottl

232
00:09:18,600 --> 00:09:22,600
knucked by energy consumption and most

233
00:09:20,640 --> 00:09:24,560
of that energy consumption comes from

234
00:09:22,600 --> 00:09:27,120
these electronic interconnects to send

235
00:09:24,560 --> 00:09:30,200
data from a point A to point B on your

236
00:09:27,120 --> 00:09:35,399
chip now uh there are many ways this is

237
00:09:30,200 --> 00:09:37,920
done uh if the hardware to do efficient

238
00:09:35,399 --> 00:09:40,360
linear algebra is not old is not new

239
00:09:37,920 --> 00:09:42,360
it's quite old and uh there's this

240
00:09:40,360 --> 00:09:44,079
concept called a systolic array which is

241
00:09:42,360 --> 00:09:47,680
one of the most efficient ways to do

242
00:09:44,079 --> 00:09:50,120
linear algebra u in digital electronics

243
00:09:47,680 --> 00:09:52,760
and this is something that has people

244
00:09:50,120 --> 00:09:55,560
had been developing in Academia but uh

245
00:09:52,760 --> 00:09:57,320
Now with uh with this demand of deep

246
00:09:55,560 --> 00:10:00,120
learning it's become a commercial

247
00:09:57,320 --> 00:10:02,320
product and so the Google TPU and gpus

248
00:10:00,120 --> 00:10:05,320
nowadays are all really good examples of

249
00:10:02,320 --> 00:10:07,079
systolic arrays uh so this was a product

250
00:10:05,320 --> 00:10:09,640
that was first introduced in

251
00:10:07,079 --> 00:10:12,440
2016 and you can still use it on the

252
00:10:09,640 --> 00:10:16,600
cloud um it's also used to train all of

253
00:10:12,440 --> 00:10:18,200
Google's machine learning models and uh

254
00:10:16,600 --> 00:10:20,360
the TPU has gone through several

255
00:10:18,200 --> 00:10:22,680
iterations and I think that comparing

256
00:10:20,360 --> 00:10:25,480
those iterations is also a good way of

257
00:10:22,680 --> 00:10:28,880
uh illustrating the limitations we see

258
00:10:25,480 --> 00:10:30,640
to electronic solutions to to this

259
00:10:28,880 --> 00:10:33,399
machine learning learning problem and

260
00:10:30,640 --> 00:10:36,480
how mors law probably has a limit has a

261
00:10:33,399 --> 00:10:39,360
limited uh gives limited gains in the

262
00:10:36,480 --> 00:10:41,560
future so if you look at uh these these

263
00:10:39,360 --> 00:10:44,560
this technical chart here is just the

264
00:10:41,560 --> 00:10:46,920
energy cost of different operations uh

265
00:10:44,560 --> 00:10:49,040
at the 45 millimeter node or nanometer

266
00:10:46,920 --> 00:10:51,360
node and so that was the first TPU and

267
00:10:49,040 --> 00:10:53,040
the 7even nanometer node and the

268
00:10:51,360 --> 00:10:55,839
Improvement should be a factor of 45

269
00:10:53,040 --> 00:10:57,360
over7 that's about 6.5 but what they

270
00:10:55,839 --> 00:11:00,560
actually find is that the Improvement

271
00:10:57,360 --> 00:11:04,079
for most of these Ops is about 3 four so

272
00:11:00,560 --> 00:11:06,600
there's a limited uh there's a a a

273
00:11:04,079 --> 00:11:08,480
limited gains for a a large number of

274
00:11:06,600 --> 00:11:10,680
the important kinds of Ops that you want

275
00:11:08,480 --> 00:11:13,839
to do the especially the max but also

276
00:11:10,680 --> 00:11:16,120
the reading from memor going from this

277
00:11:13,839 --> 00:11:17,720
older process node to 7 nanometers and

278
00:11:16,120 --> 00:11:19,639
of course that has continued to three

279
00:11:17,720 --> 00:11:22,959
nanometers and 2 nanometers and however

280
00:11:19,639 --> 00:11:25,800
many nanometers we expect in the future

281
00:11:22,959 --> 00:11:28,639
and even if you had even if you didn't

282
00:11:25,800 --> 00:11:30,800
have that limitation there are other

283
00:11:28,639 --> 00:11:32,360
fundamental sounds that are digital

284
00:11:30,800 --> 00:11:34,360
electronics because we've had so much

285
00:11:32,360 --> 00:11:37,000
improvement in the last couple decades

286
00:11:34,360 --> 00:11:39,440
are coming close to reaching so

287
00:11:37,000 --> 00:11:41,200
fundamentally the lowest energy to

288
00:11:39,440 --> 00:11:44,160
perform a gate is called the land hour

289
00:11:41,200 --> 00:11:46,120
limit and that's KT log 2 um but there's

290
00:11:44,160 --> 00:11:47,760
also a limit called the reliability

291
00:11:46,120 --> 00:11:50,040
reliable Computing limit and that's

292
00:11:47,760 --> 00:11:51,760
about the land hour limit times the log

293
00:11:50,040 --> 00:11:54,200
of your error probability which is

294
00:11:51,760 --> 00:11:58,279
around uh you would like it to be around

295
00:11:54,200 --> 00:12:00,320
10 Theus 20 or lower and there are uh

296
00:11:58,279 --> 00:12:02,600
there are trends that that uh people

297
00:12:00,320 --> 00:12:05,040
have calculated that suggests that we

298
00:12:02,600 --> 00:12:06,920
may be approaching this reliable

299
00:12:05,040 --> 00:12:11,639
Computing bound with in the in the next

300
00:12:06,920 --> 00:12:13,800
decade or two um and so that really

301
00:12:11,639 --> 00:12:15,240
highlights the need for another Paradigm

302
00:12:13,800 --> 00:12:17,760
for computing if we really want to

303
00:12:15,240 --> 00:12:20,040
continue this exponential growth in

304
00:12:17,760 --> 00:12:22,560
model sizes and this is potentially

305
00:12:20,040 --> 00:12:23,880
where Optics comes into play now to

306
00:12:22,560 --> 00:12:25,240
understand where Optics can give us an

307
00:12:23,880 --> 00:12:26,639
advantage which we should look at the

308
00:12:25,240 --> 00:12:29,279
differences between Optics and

309
00:12:26,639 --> 00:12:30,600
electronics um they're the same in many

310
00:12:29,279 --> 00:12:32,480
ways they have the same fundamental

311
00:12:30,600 --> 00:12:34,639
force and we're using the same particle

312
00:12:32,480 --> 00:12:36,040
to communicate data or the photon but

313
00:12:34,639 --> 00:12:39,240
there's this big difference in terms of

314
00:12:36,040 --> 00:12:42,000
the in terms of the uh carrier frequency

315
00:12:39,240 --> 00:12:45,399
of electronic excitations versus Optical

316
00:12:42,000 --> 00:12:47,680
ones a factor of 10,000 to 100,000 and

317
00:12:45,399 --> 00:12:51,199
that manifests in a lot of different

318
00:12:47,680 --> 00:12:53,399
ways in in terms of the photon energy as

319
00:12:51,199 --> 00:12:55,600
well as its wavelength in terms of its

320
00:12:53,399 --> 00:12:57,839
interaction with matter the types of

321
00:12:55,600 --> 00:13:00,279
sources and detectors that you would use

322
00:12:57,839 --> 00:13:01,279
and the methods in which you wire data

323
00:13:00,279 --> 00:13:03,880
from A to

324
00:13:01,279 --> 00:13:05,600
B and this leads to a lot of potential

325
00:13:03,880 --> 00:13:07,560
advantages for using Optics over

326
00:13:05,600 --> 00:13:10,639
Electronics both for communications and

327
00:13:07,560 --> 00:13:13,399
for computing the related to data rates

328
00:13:10,639 --> 00:13:17,040
loss Distortion cross talk the number of

329
00:13:13,399 --> 00:13:18,839
modes um linearity and also potentially

330
00:13:17,040 --> 00:13:21,199
but I won't talk about it as much here

331
00:13:18,839 --> 00:13:22,279
but Quantum advantages and Computing at

332
00:13:21,199 --> 00:13:25,160
the speed of

333
00:13:22,279 --> 00:13:28,120
light so data rates is kind of a given

334
00:13:25,160 --> 00:13:31,240
uh fundamentally this is because the the

335
00:13:28,120 --> 00:13:34,440
optical signals are being transmitted at

336
00:13:31,240 --> 00:13:37,320
this carrier frequency which is 200 300

337
00:13:34,440 --> 00:13:38,720
terahertz or so and that's a orders of

338
00:13:37,320 --> 00:13:42,279
magnitude higher than the carrier

339
00:13:38,720 --> 00:13:43,680
frequencies of electronic excitations

340
00:13:42,279 --> 00:13:46,560
and there are a lot of other practical

341
00:13:43,680 --> 00:13:48,240
things that limit Optical data rates but

342
00:13:46,560 --> 00:13:50,639
uh those have been engineered for a long

343
00:13:48,240 --> 00:13:54,839
time to the point that we're very close

344
00:13:50,639 --> 00:13:57,160
to uh that fundamental limit uh over a

345
00:13:54,839 --> 00:13:59,440
single optical fiber you can transmit 50

346
00:13:57,160 --> 00:14:01,839
to 200 terabits per second of data a lot

347
00:13:59,440 --> 00:14:05,880
of these records get broken every every

348
00:14:01,839 --> 00:14:07,440
year by Japanese companies like NDT or

349
00:14:05,880 --> 00:14:10,839
institutes like

350
00:14:07,440 --> 00:14:14,240
aist and uh a good way to visualize this

351
00:14:10,839 --> 00:14:17,440
is that uh nowadays in your high-end

352
00:14:14,240 --> 00:14:18,959
gpus you have a processor and then

353
00:14:17,440 --> 00:14:20,440
there's this high bandwidth memory that

354
00:14:18,959 --> 00:14:21,880
goes on top of that and there are

355
00:14:20,440 --> 00:14:25,079
thousands of connections between your

356
00:14:21,880 --> 00:14:28,360
processor and your hbm and you can get

357
00:14:25,079 --> 00:14:30,160
the same amount of uh data throughput

358
00:14:28,360 --> 00:14:33,880
through the core an optical fiber so

359
00:14:30,160 --> 00:14:37,480
that's just roughly a 10 Micron uh size

360
00:14:33,880 --> 00:14:39,399
um region as as on all of those

361
00:14:37,480 --> 00:14:41,199
connections going from the processor to

362
00:14:39,399 --> 00:14:43,000
the hbm but the fiber can carry it

363
00:14:41,199 --> 00:14:45,079
hundreds of kilometers so that's one

364
00:14:43,000 --> 00:14:47,079
advantage uh to you should compare this

365
00:14:45,079 --> 00:14:48,600
to how data is transmitted on chip

366
00:14:47,079 --> 00:14:50,320
though because there's a fundamental

367
00:14:48,600 --> 00:14:52,920
limit to how you can transmit data on

368
00:14:50,320 --> 00:14:55,279
chip that's related to RC circuitry so

369
00:14:52,920 --> 00:14:58,639
the resistance of a wire depends on its

370
00:14:55,279 --> 00:15:00,560
length and the capacitance also depends

371
00:14:58,639 --> 00:15:02,399
on its length length and if you

372
00:15:00,560 --> 00:15:04,279
calculate the RC time constant you'll

373
00:15:02,399 --> 00:15:06,000
find that that depends on some material

374
00:15:04,279 --> 00:15:08,399
constants you can't really change those

375
00:15:06,000 --> 00:15:14,279
by very much uh times the aspect ratio

376
00:15:08,399 --> 00:15:15,959
of your wires and this leads to uh this

377
00:15:14,279 --> 00:15:17,480
leads to if you want to ask yourself how

378
00:15:15,959 --> 00:15:20,600
much data can you squeeze through a

379
00:15:17,480 --> 00:15:23,279
micron siiz wire uh a limit that depends

380
00:15:20,600 --> 00:15:24,759
very sharply on the length of your wire

381
00:15:23,279 --> 00:15:27,000
whereas in photonics it's really

382
00:15:24,759 --> 00:15:28,680
independent and with state-of-the-art

383
00:15:27,000 --> 00:15:30,440
technology it can be orders of magnitude

384
00:15:28,680 --> 00:15:32,600
higher

385
00:15:30,440 --> 00:15:35,120
now in addition electronic signals are

386
00:15:32,600 --> 00:15:36,959
very sensitive to uh loss Distortion

387
00:15:35,120 --> 00:15:38,800
interference and cross talk and many of

388
00:15:36,959 --> 00:15:41,240
these problems just go away with

389
00:15:38,800 --> 00:15:42,600
photonics this is true because

390
00:15:41,240 --> 00:15:45,240
electronic signals usually have

391
00:15:42,600 --> 00:15:46,959
frequency dependent frequency dependent

392
00:15:45,240 --> 00:15:49,360
loss when they go down wave guides but

393
00:15:46,959 --> 00:15:52,360
in fonics you're more limited by uh

394
00:15:49,360 --> 00:15:54,920
group velocity dispersion which which uh

395
00:15:52,360 --> 00:15:57,880
only only is relevant on much longer

396
00:15:54,920 --> 00:15:59,920
length scales and cross talk can be much

397
00:15:57,880 --> 00:16:02,600
lower because

398
00:15:59,920 --> 00:16:04,759
the modes attenuate exponentially in the

399
00:16:02,600 --> 00:16:07,079
evanescent regions outside of the wave

400
00:16:04,759 --> 00:16:09,639
guide and I think I'll skip over that

401
00:16:07,079 --> 00:16:11,880
for L of time but another important

402
00:16:09,639 --> 00:16:13,880
question is not just communicating but

403
00:16:11,880 --> 00:16:15,639
how do you compute with Optics and how

404
00:16:13,880 --> 00:16:17,519
can you make Optical Computing more

405
00:16:15,639 --> 00:16:19,360
efficient U and this is something where

406
00:16:17,519 --> 00:16:21,319
we're not just naively trying to replace

407
00:16:19,360 --> 00:16:23,279
what everything what what an electronic

408
00:16:21,319 --> 00:16:25,279
processor does uh many things like

409
00:16:23,279 --> 00:16:28,360
nonlinearity and memory are much better

410
00:16:25,279 --> 00:16:30,920
done in electronics versus in Optics but

411
00:16:28,360 --> 00:16:32,959
this linear algebra which I mentioned at

412
00:16:30,920 --> 00:16:34,560
the very beginning that's a key part of

413
00:16:32,959 --> 00:16:36,199
deep learning and it's actually the

414
00:16:34,560 --> 00:16:38,920
bottleneck

415
00:16:36,199 --> 00:16:41,120
in that's something that theoretically

416
00:16:38,920 --> 00:16:43,800
can be very easily mapped to Optics and

417
00:16:41,120 --> 00:16:45,839
if you map it to Optics it becomes free

418
00:16:43,800 --> 00:16:47,920
because Optical propagation is just

419
00:16:45,839 --> 00:16:51,000
based on Maxwell's equations which is

420
00:16:47,920 --> 00:16:53,399
passive and free and so if you can make

421
00:16:51,000 --> 00:16:55,240
this mapping between linear algebra and

422
00:16:53,399 --> 00:16:57,880
Optics then potentially you can get a

423
00:16:55,240 --> 00:17:00,240
big Advantage uh to running this part of

424
00:16:57,880 --> 00:17:03,360
your neural network in l

425
00:17:00,240 --> 00:17:06,559
and so there are uh there are many ways

426
00:17:03,360 --> 00:17:09,240
to uh understand how this would work uh

427
00:17:06,559 --> 00:17:10,439
so in a matrix Vector multiplication

428
00:17:09,240 --> 00:17:12,799
you're going to have fan out you're

429
00:17:10,439 --> 00:17:15,000
going to have uh multiplication and

430
00:17:12,799 --> 00:17:17,079
you're going to have summation and each

431
00:17:15,000 --> 00:17:20,520
one of these operations can be thought

432
00:17:17,079 --> 00:17:22,039
of as being implemented in Optics um and

433
00:17:20,520 --> 00:17:23,360
when you implement these in Optics you

434
00:17:22,039 --> 00:17:26,160
get something that's called Optical

435
00:17:23,360 --> 00:17:29,039
parallelism that's an idea that for

436
00:17:26,160 --> 00:17:30,120
every active conversion step between

437
00:17:29,039 --> 00:17:34,280
ronic and

438
00:17:30,120 --> 00:17:36,520
optic you you are effectively doing many

439
00:17:34,280 --> 00:17:38,320
operations if you have fan out an

440
00:17:36,520 --> 00:17:39,960
integration in your circuit also if your

441
00:17:38,320 --> 00:17:42,200
circuit has depth it's basically the

442
00:17:39,960 --> 00:17:43,760
same thing and if you look at the amount

443
00:17:42,200 --> 00:17:46,400
of parallelism you have in a matrix

444
00:17:43,760 --> 00:17:49,640
Vector multiplication well you if it's

445
00:17:46,400 --> 00:17:54,400
an M byn Matrix then you have a fan out

446
00:17:49,640 --> 00:17:58,840
of N and you have a fan in of M

447
00:17:54,400 --> 00:18:00,919
and you can just uh calculate the energy

448
00:17:58,840 --> 00:18:03,679
per operation here and you see that it

449
00:18:00,919 --> 00:18:07,520
depends inversely on both M and N which

450
00:18:03,679 --> 00:18:09,720
are the sizes of your matrices now for

451
00:18:07,520 --> 00:18:11,840
the trend in machine learning has bur

452
00:18:09,720 --> 00:18:13,559
towards larger and larger matrices

453
00:18:11,840 --> 00:18:14,960
especially in these large generative

454
00:18:13,559 --> 00:18:19,320
models like

455
00:18:14,960 --> 00:18:21,480
llms and so that really suggests that

456
00:18:19,320 --> 00:18:25,400
the since the advantage of Optics will

457
00:18:21,480 --> 00:18:27,840
go go with M and N that really suggests

458
00:18:25,400 --> 00:18:29,919
uh that the advantage should should be

459
00:18:27,840 --> 00:18:32,640
quite substantial

460
00:18:29,919 --> 00:18:34,960
and so motivated by that our group has a

461
00:18:32,640 --> 00:18:36,200
large number of we're studying a large

462
00:18:34,960 --> 00:18:38,080
number of different Optical

463
00:18:36,200 --> 00:18:40,240
architectures to try to get this

464
00:18:38,080 --> 00:18:42,640
Advantage for computing powered by

465
00:18:40,240 --> 00:18:45,799
photonics uh the first uh this was what

466
00:18:42,640 --> 00:18:47,760
I worked on when I first joined MIT is

467
00:18:45,799 --> 00:18:50,360
based on coherent detection which we

468
00:18:47,760 --> 00:18:52,360
also implemented with with the what are

469
00:18:50,360 --> 00:18:54,320
called vixel arrays uh we also have some

470
00:18:52,360 --> 00:18:56,600
freespace Optical schemes based on

471
00:18:54,320 --> 00:18:58,159
programmable fanout and then we have uh

472
00:18:56,600 --> 00:19:00,360
some more traditional approaches based

473
00:18:58,159 --> 00:19:01,600
on beam splitter meshes and some

474
00:19:00,360 --> 00:19:03,240
approaches based on wavelength

475
00:19:01,600 --> 00:19:06,039
multiplexing and I'll try to see how

476
00:19:03,240 --> 00:19:08,000
much I can get through um starting with

477
00:19:06,039 --> 00:19:09,840
I think the most standard architecture

478
00:19:08,000 --> 00:19:12,200
which is the beam splitter mesh or

479
00:19:09,840 --> 00:19:15,000
multiport interferometer so sorry about

480
00:19:12,200 --> 00:19:18,600
the uh pixelated image here but a

481
00:19:15,000 --> 00:19:22,120
multiport interferometer it's a it's

482
00:19:18,600 --> 00:19:24,200
a it's a a programmable Matrix Vector

483
00:19:22,120 --> 00:19:27,159
multiplier that works through Optical

484
00:19:24,200 --> 00:19:29,039
interference so the idea is that if your

485
00:19:27,159 --> 00:19:32,440
vector is encoded in different different

486
00:19:29,039 --> 00:19:34,000
modes then a mode mixer is equivalent to

487
00:19:32,440 --> 00:19:35,960
passively performing Matrix Vector

488
00:19:34,000 --> 00:19:38,360
multiplication so think of things like a

489
00:19:35,960 --> 00:19:41,440
directional coupler a beam splitter an

490
00:19:38,360 --> 00:19:43,600
MMI all these things perform a fixed

491
00:19:41,440 --> 00:19:45,559
Matrix Vector multiplication but what we

492
00:19:43,600 --> 00:19:47,640
want to do here is do a programmable

493
00:19:45,559 --> 00:19:50,799
Matrix Vector multiplication so we can

494
00:19:47,640 --> 00:19:53,600
Implement all those matrices in in a a

495
00:19:50,799 --> 00:19:56,080
deep neural network in this Hardware how

496
00:19:53,600 --> 00:19:58,280
do you do that well if your Matrix has a

497
00:19:56,080 --> 00:19:59,520
size of two there's a known way to do

498
00:19:58,280 --> 00:20:01,679
this and it's called a MOX Ander

499
00:19:59,520 --> 00:20:03,720
interferometer MOX Ander interferometer

500
00:20:01,679 --> 00:20:06,240
has two beam Splitters and uh usually

501
00:20:03,720 --> 00:20:08,760
one phase shifter but here you need two

502
00:20:06,240 --> 00:20:11,200
and you can show that this implements a

503
00:20:08,760 --> 00:20:12,640
matrix of the following form which uh

504
00:20:11,200 --> 00:20:14,880
coupled with some with two phase

505
00:20:12,640 --> 00:20:18,000
shifters on the output this gives you a

506
00:20:14,880 --> 00:20:20,200
a universal 2x two Matrix that respects

507
00:20:18,000 --> 00:20:22,440
the respects energy conservation so it's

508
00:20:20,200 --> 00:20:24,960
a unitary Matrix so this can Implement

509
00:20:22,440 --> 00:20:27,360
any 2x2 matrix that respects energy

510
00:20:24,960 --> 00:20:29,000
conservation now the real in the real

511
00:20:27,360 --> 00:20:31,840
world our neural networks need more than

512
00:20:29,000 --> 00:20:34,200
two neurons and so you need to find a

513
00:20:31,840 --> 00:20:36,919
way to generalize this 2 by two to an n

514
00:20:34,200 --> 00:20:40,240
byn and there are ways to do that uh so

515
00:20:36,919 --> 00:20:42,280
this first way was devised by wreck

516
00:20:40,240 --> 00:20:45,320
Zinger and some other people back in the

517
00:20:42,280 --> 00:20:48,320
90s and this is just to tile this MOX

518
00:20:45,320 --> 00:20:51,640
Ender into this triangular mesh and you

519
00:20:48,320 --> 00:20:54,320
can show that any in this case 4x4

520
00:20:51,640 --> 00:20:57,400
Matrix can be realized as a product of 2

521
00:20:54,320 --> 00:20:59,559
x two matrices uh if they're arranged in

522
00:20:57,400 --> 00:21:01,720
this particular way and the reason it

523
00:20:59,559 --> 00:21:03,559
needs to be arranged in this way is that

524
00:21:01,720 --> 00:21:06,280
you can do some math rewrite this

525
00:21:03,559 --> 00:21:07,720
equation as this and then you can show

526
00:21:06,280 --> 00:21:10,480
that this is equivalent to a

527
00:21:07,720 --> 00:21:12,480
diagonalization of this Matrix U so

528
00:21:10,480 --> 00:21:14,279
those of you who have taken linear

529
00:21:12,480 --> 00:21:16,600
algebra might know about the QR

530
00:21:14,279 --> 00:21:19,000
decomposition of matrices this is just a

531
00:21:16,600 --> 00:21:21,440
QR decomposition but our Matrix starts

532
00:21:19,000 --> 00:21:24,440
as unitary so R is

533
00:21:21,440 --> 00:21:26,640
one and there this also gives you a

534
00:21:24,440 --> 00:21:28,919
prescription to find the phases the

535
00:21:26,640 --> 00:21:31,919
thetas and F you need in order to imp

536
00:21:28,919 --> 00:21:33,640
this desired Matrix

537
00:21:31,919 --> 00:21:36,679
U

538
00:21:33,640 --> 00:21:39,279
and that uh that was the first uh first

539
00:21:36,679 --> 00:21:41,520
approach now this has some limitations U

540
00:21:39,279 --> 00:21:43,480
the the first one that everyone mentions

541
00:21:41,520 --> 00:21:45,760
is well this is a triangle and your

542
00:21:43,480 --> 00:21:47,240
chips are always square or rectangular

543
00:21:45,760 --> 00:21:49,760
and so you're wasting a lot of space

544
00:21:47,240 --> 00:21:53,039
doing this uh but actually that problem

545
00:21:49,760 --> 00:21:54,720
has been remedied so in 2016 this

546
00:21:53,039 --> 00:21:56,480
postdoc William Clemens came up with

547
00:21:54,720 --> 00:21:58,919
this architecture that just splits the

548
00:21:56,480 --> 00:22:01,600
rect triangle in half and then it mer es

549
00:21:58,919 --> 00:22:04,159
those two triangles along this the

550
00:22:01,600 --> 00:22:06,880
hypotenuse and that gives you Al that's

551
00:22:04,159 --> 00:22:09,000
also un Universal it's also very easy to

552
00:22:06,880 --> 00:22:10,640
program um so these things

553
00:22:09,000 --> 00:22:12,320
experimentally they were first used for

554
00:22:10,640 --> 00:22:14,440
Quantum photonics and only later were

555
00:22:12,320 --> 00:22:17,279
they you uh deployed to classical

556
00:22:14,440 --> 00:22:18,679
photonics um so in Quantum photonics

557
00:22:17,279 --> 00:22:19,880
there are a lot of these operations that

558
00:22:18,679 --> 00:22:22,559
you have to

559
00:22:19,880 --> 00:22:25,159
implement a lot of these Primitives for

560
00:22:22,559 --> 00:22:28,520
to make a Quantum Optical computer these

561
00:22:25,159 --> 00:22:30,520
include things like Fusion Gates the km

562
00:22:28,520 --> 00:22:32,760
scheme has a particular interference

563
00:22:30,520 --> 00:22:35,200
pattern and then if you want to do a

564
00:22:32,760 --> 00:22:37,760
non-universal do non-universal Computing

565
00:22:35,200 --> 00:22:40,840
like bz on sampling you still need a a

566
00:22:37,760 --> 00:22:42,960
way of mixing modes and you want to mix

567
00:22:40,840 --> 00:22:45,600
the modes in a manner that's fully

568
00:22:42,960 --> 00:22:48,120
programmable U so this was originally

569
00:22:45,600 --> 00:22:50,720
developed for uh Quantum photonics and

570
00:22:48,120 --> 00:22:52,960
it it spun off a number of startups uh

571
00:22:50,720 --> 00:22:55,760
so these two startups I know in

572
00:22:52,960 --> 00:22:57,400
particular squantum and Xanadu uh their

573
00:22:55,760 --> 00:22:59,600
technology is very critically dependent

574
00:22:57,400 --> 00:23:00,919
on being able to Implement these uh

575
00:22:59,600 --> 00:23:05,559
programmable

576
00:23:00,919 --> 00:23:08,400
unitaries but at MIT we realized in 2017

577
00:23:05,559 --> 00:23:10,679
or a couple years before that um that

578
00:23:08,400 --> 00:23:13,760
you can also use this for classical

579
00:23:10,679 --> 00:23:15,880
computation uh in particular we use this

580
00:23:13,760 --> 00:23:18,720
silicon batonic programmable beam

581
00:23:15,880 --> 00:23:20,960
splitter mesh uh based on Thermo optic

582
00:23:18,720 --> 00:23:22,520
phase shifters up here and we realize

583
00:23:20,960 --> 00:23:25,200
that this thing since it can do

584
00:23:22,520 --> 00:23:28,480
programmable Matrix Vector products uh

585
00:23:25,200 --> 00:23:30,880
that you could run a proof of concept uh

586
00:23:28,480 --> 00:23:33,600
deep learning models on this so we we

587
00:23:30,880 --> 00:23:35,760
did a proof of concept demo of a vowel

588
00:23:33,600 --> 00:23:38,240
classification with this and that was

589
00:23:35,760 --> 00:23:40,760
something that uh it spun off two

590
00:23:38,240 --> 00:23:43,360
companies uh one was called light matter

591
00:23:40,760 --> 00:23:46,159
it's still headquartered in Boston and

592
00:23:43,360 --> 00:23:47,880
the bay area as well as lightelligence

593
00:23:46,159 --> 00:23:50,200
that's partly in Boston but now mostly

594
00:23:47,880 --> 00:23:52,080
in China and these companies were

595
00:23:50,200 --> 00:23:54,520
originally founded with the intent of

596
00:23:52,080 --> 00:23:56,559
taking those measures and making Optical

597
00:23:54,520 --> 00:23:58,240
Computing units now that didn't happen

598
00:23:56,559 --> 00:24:00,159
but largely because they discovered that

599
00:23:58,240 --> 00:24:02,679
in in the near- term there were was more

600
00:24:00,159 --> 00:24:04,840
customer demand for interconnects uh not

601
00:24:02,679 --> 00:24:06,720
so much that the meshes wouldn't work

602
00:24:04,840 --> 00:24:09,200
there are reasons that the meshes might

603
00:24:06,720 --> 00:24:11,559
be hard in practice one is the need for

604
00:24:09,200 --> 00:24:13,559
a nonlinearity in the Chip And so this

605
00:24:11,559 --> 00:24:16,279
is something that we've been working on

606
00:24:13,559 --> 00:24:18,840
uh even after they they spawn off and we

607
00:24:16,279 --> 00:24:21,159
realized that this problem has a

608
00:24:18,840 --> 00:24:23,880
solution is which is that you can create

609
00:24:21,159 --> 00:24:25,720
a nonlinearity on a chip that has uh no

610
00:24:23,880 --> 00:24:28,960
nonlinear Optics through a combination

611
00:24:25,720 --> 00:24:30,679
of photo detection and modulation so you

612
00:24:28,960 --> 00:24:32,480
take the you take your light you use

613
00:24:30,679 --> 00:24:33,960
your light to drive a photo detector and

614
00:24:32,480 --> 00:24:35,720
then it becomes a current and then that

615
00:24:33,960 --> 00:24:38,600
current goes into a modulator and then

616
00:24:35,720 --> 00:24:41,679
the modulator modulates the light and

617
00:24:38,600 --> 00:24:43,399
that ends up giving you a nonlinear uh a

618
00:24:41,679 --> 00:24:45,080
nonlinear mapping between the input

619
00:24:43,399 --> 00:24:47,520
light and the output light and not only

620
00:24:45,080 --> 00:24:49,080
is it nonlinear but it's programmable uh

621
00:24:47,520 --> 00:24:51,600
so you can get all these different kinds

622
00:24:49,080 --> 00:24:54,559
of nonlinearities just by tuning the set

623
00:24:51,600 --> 00:24:58,440
point of this ring

624
00:24:54,559 --> 00:25:01,120
modulator and so that coupled with a uh

625
00:24:58,440 --> 00:25:04,640
novel Inu Training Method allowed us to

626
00:25:01,120 --> 00:25:08,760
demonstrate for the first time U uh a

627
00:25:04,640 --> 00:25:11,360
single chip um single chip photonic

628
00:25:08,760 --> 00:25:13,640
coherent deep learning with a latency

629
00:25:11,360 --> 00:25:15,760
that is limited not by the nonlinearity

630
00:25:13,640 --> 00:25:18,120
and not by any readout but only by the

631
00:25:15,760 --> 00:25:20,320
speed of light for the yeah the speed of

632
00:25:18,120 --> 00:25:22,080
the light that propagates through this

633
00:25:20,320 --> 00:25:25,720
chip and

634
00:25:22,080 --> 00:25:28,840
also where is the yeah and this idea of

635
00:25:25,720 --> 00:25:30,360
training it's something that uh many

636
00:25:28,840 --> 00:25:32,360
other groups research groups are also

637
00:25:30,360 --> 00:25:35,320
studying uh so there's a lot of interest

638
00:25:32,360 --> 00:25:37,120
in uh doing back back propagation in

639
00:25:35,320 --> 00:25:39,159
fonics and it turns out that back

640
00:25:37,120 --> 00:25:42,640
propagation theoretically is very

641
00:25:39,159 --> 00:25:45,080
similar to what they call uh the adjoint

642
00:25:42,640 --> 00:25:47,600
method and so there have been uh there

643
00:25:45,080 --> 00:25:49,240
was this Theory proposal back in 2018

644
00:25:47,600 --> 00:25:52,039
about how you could physically realize

645
00:25:49,240 --> 00:25:54,880
the adjoint method u in batonic hardware

646
00:25:52,039 --> 00:25:56,799
and that's simp since been realized in

647
00:25:54,880 --> 00:25:58,799
2023 now there there are a lot of

648
00:25:56,799 --> 00:26:01,360
limitations to these meas that have to

649
00:25:58,799 --> 00:26:04,279
do with uh Hardware Hardware

650
00:26:01,360 --> 00:26:05,559
imperfections um I think I'll I think

651
00:26:04,279 --> 00:26:07,840
I'll skip through this for a lack of

652
00:26:05,559 --> 00:26:11,559
time but basically the slides I'm going

653
00:26:07,840 --> 00:26:13,279
through are are about how uh there are a

654
00:26:11,559 --> 00:26:16,559
lot of challenges in physically

655
00:26:13,279 --> 00:26:18,720
realizing these these large MOX Zander

656
00:26:16,559 --> 00:26:21,559
meshes when your Hardware is fabricated

657
00:26:18,720 --> 00:26:23,799
and it's not quite uh what what your

658
00:26:21,559 --> 00:26:25,679
theoretical model predicts it should be

659
00:26:23,799 --> 00:26:27,240
and there are self-configuration and

660
00:26:25,679 --> 00:26:29,520
error correction methods that you can

661
00:26:27,240 --> 00:26:30,840
use um so that you can still use the

662
00:26:29,520 --> 00:26:33,240
hardware to implement the desired

663
00:26:30,840 --> 00:26:35,039
matrices even in spite of these these

664
00:26:33,240 --> 00:26:37,000
imperfections but I wanted to talk about

665
00:26:35,039 --> 00:26:38,720
two other approaches to Optical

666
00:26:37,000 --> 00:26:41,120
Computing uh which are probably a little

667
00:26:38,720 --> 00:26:43,320
bit more scalable and one is delocalized

668
00:26:41,120 --> 00:26:46,240
Computing uh and so why would you want

669
00:26:43,320 --> 00:26:50,760
delocalized Computing uh so there are

670
00:26:46,240 --> 00:26:53,799
two Trends in the in the world right now

671
00:26:50,760 --> 00:26:55,200
uh one is one is Edge Computing where we

672
00:26:53,799 --> 00:26:56,960
have all these devices that are

673
00:26:55,200 --> 00:26:58,360
generating data and we don't know what

674
00:26:56,960 --> 00:27:00,000
to do with all that data we can't can't

675
00:26:58,360 --> 00:27:01,679
send it all to the cloud for processing

676
00:27:00,000 --> 00:27:03,320
that would overload the cloud and often

677
00:27:01,679 --> 00:27:04,760
times there's uh there are privacy

678
00:27:03,320 --> 00:27:08,440
reasons that you wouldn't want to do

679
00:27:04,760 --> 00:27:11,360
that and then also uh for training a lot

680
00:27:08,440 --> 00:27:14,200
of large machine learning models you run

681
00:27:11,360 --> 00:27:16,520
into memory bottlenecks where you can't

682
00:27:14,200 --> 00:27:18,520
store all of the weight memory and all

683
00:27:16,520 --> 00:27:20,840
of the activation memory especially all

684
00:27:18,520 --> 00:27:23,760
the intermediate layers that you have to

685
00:27:20,840 --> 00:27:25,760
store temporarily stash the activations

686
00:27:23,760 --> 00:27:27,960
for to do back propagation you can't

687
00:27:25,760 --> 00:27:31,600
store all of that in a single GPU so you

688
00:27:27,960 --> 00:27:33,399
have to delize your your your training

689
00:27:31,600 --> 00:27:36,279
algorithms usually through a combination

690
00:27:33,399 --> 00:27:38,559
of weight broadcasting and then uh

691
00:27:36,279 --> 00:27:41,880
weight update reduce

692
00:27:38,559 --> 00:27:44,799
steps and and so some kind of an

693
00:27:41,880 --> 00:27:46,919
algorithm or some kind of a photonic

694
00:27:44,799 --> 00:27:48,799
architecture uh that is suitable for

695
00:27:46,919 --> 00:27:50,640
delocalized computing might be very

696
00:27:48,799 --> 00:27:53,279
useful to the community and this is

697
00:27:50,640 --> 00:27:55,720
something that uh we realized in late

698
00:27:53,279 --> 00:27:57,559
2020 early 21 and we came up with this

699
00:27:55,720 --> 00:27:59,279
architecture called ncast that I think

700
00:27:57,559 --> 00:28:00,960
goes along way towards doing that the

701
00:27:59,279 --> 00:28:01,760
main ideas are reported in this paper

702
00:28:00,960 --> 00:28:05,159
down

703
00:28:01,760 --> 00:28:07,000
here and this is motivated by a concept

704
00:28:05,159 --> 00:28:09,519
called Magic State Computing something

705
00:28:07,000 --> 00:28:12,960
that comes from the quantum literature

706
00:28:09,519 --> 00:28:14,799
and it's this idea that uh you can in

707
00:28:12,960 --> 00:28:16,720
certain cases you can create this

708
00:28:14,799 --> 00:28:18,960
physical state called a magic State and

709
00:28:16,720 --> 00:28:21,919
once you have that magic State you can

710
00:28:18,960 --> 00:28:24,240
send that magic state to a user and the

711
00:28:21,919 --> 00:28:27,519
user can combine the magic state with

712
00:28:24,240 --> 00:28:30,320
some data to process uh a given

713
00:28:27,519 --> 00:28:31,880
algorithm and give you a result and I

714
00:28:30,320 --> 00:28:33,399
guess the magic part about this is that

715
00:28:31,880 --> 00:28:36,039
the magic State lets you process the

716
00:28:33,399 --> 00:28:37,720
data with much less in much less time

717
00:28:36,039 --> 00:28:41,159
and with much less energy than if you

718
00:28:37,720 --> 00:28:44,399
had to process it by yourself now that

719
00:28:41,159 --> 00:28:46,039
uh that idea it has a very rigorous uh

720
00:28:44,399 --> 00:28:47,720
definition in quantum mechanics it has

721
00:28:46,039 --> 00:28:50,440
to do with performing certain Quantum

722
00:28:47,720 --> 00:28:51,720
Gates like T Gates non- Clifford Gates

723
00:28:50,440 --> 00:28:53,760
that are hard especially when they're

724
00:28:51,720 --> 00:28:55,440
error corrected uh you can do that by

725
00:28:53,760 --> 00:28:58,720
first generating a magic state for your

726
00:28:55,440 --> 00:29:00,799
Tate and then once you have that state

727
00:28:58,720 --> 00:29:03,279
doing the Tate is just a a set of

728
00:29:00,799 --> 00:29:05,399
Clifford Gates which are easy now this

729
00:29:03,279 --> 00:29:07,120
doesn't obviate the need to generate the

730
00:29:05,399 --> 00:29:09,399
magic State that's still hard but once

731
00:29:07,120 --> 00:29:12,159
you have the state then it becomes easy

732
00:29:09,399 --> 00:29:14,440
uh netcast is a is a sort of bringing

733
00:29:12,159 --> 00:29:16,600
this concept to the classical domain for

734
00:29:14,440 --> 00:29:18,080
machine learning so in netcast you have

735
00:29:16,600 --> 00:29:20,320
a server this is a thing that's

736
00:29:18,080 --> 00:29:22,120
generating your magic States in a Time

737
00:29:20,320 --> 00:29:23,679
frequency basis and then you have a

738
00:29:22,120 --> 00:29:25,720
client and that's the thing that's going

739
00:29:23,679 --> 00:29:28,399
to use your magic States and these can

740
00:29:25,720 --> 00:29:29,880
be separated over an optical link and it

741
00:29:28,399 --> 00:29:32,320
can be tens of

742
00:29:29,880 --> 00:29:35,320
kilometers this the magic state has this

743
00:29:32,320 --> 00:29:37,720
time frequency basis um and this is

744
00:29:35,320 --> 00:29:39,360
basically a matrix it's one of the many

745
00:29:37,720 --> 00:29:42,000
matrices that you're going to send from

746
00:29:39,360 --> 00:29:44,279
the server to the client to perform the

747
00:29:42,000 --> 00:29:49,279
uh to run the neural network and the

748
00:29:44,279 --> 00:29:51,200
client drives a modulator uh and then um

749
00:29:49,279 --> 00:29:54,120
and then uses some Dem multiplexing

750
00:29:51,200 --> 00:29:56,600
Optics and that computes the vector y =

751
00:29:54,120 --> 00:29:58,679
w * X so that's the ACT that's the

752
00:29:56,600 --> 00:30:00,240
waiting section of your neural network

753
00:29:58,679 --> 00:30:02,640
and then you go on to the next layer you

754
00:30:00,240 --> 00:30:05,679
wait that one and and perform the

755
00:30:02,640 --> 00:30:07,320
nonlinearities in the client and so

756
00:30:05,679 --> 00:30:10,760
forth

757
00:30:07,320 --> 00:30:12,559
so we demonstrated this um again we used

758
00:30:10,760 --> 00:30:14,799
a silicon photonic technology to

759
00:30:12,559 --> 00:30:18,000
demonstrate this this was a uh 48

760
00:30:14,799 --> 00:30:20,960
Channel MOX Zender uh modulator array we

761
00:30:18,000 --> 00:30:22,880
drove this with a laser array with 16

762
00:30:20,960 --> 00:30:25,399
different wavelengths and that generated

763
00:30:22,880 --> 00:30:26,960
our photonic magic State and once we

764
00:30:25,399 --> 00:30:28,880
have that state we wanted to show that

765
00:30:26,960 --> 00:30:32,159
you can really do de localized

766
00:30:28,880 --> 00:30:34,960
computation so we decided to separate

767
00:30:32,159 --> 00:30:37,159
the server and the client um over a

768
00:30:34,960 --> 00:30:39,760
deployed optical fiber link connecting

769
00:30:37,159 --> 00:30:43,360
MIT down here in downtown Cambridge to

770
00:30:39,760 --> 00:30:45,600
Lincoln lab over here in uh Lexington

771
00:30:43,360 --> 00:30:47,760
Massachusetts I don't qu I don't quite

772
00:30:45,600 --> 00:30:48,919
have a map of Tokyo to superimpose on

773
00:30:47,760 --> 00:30:51,200
here but I think that's something like

774
00:30:48,919 --> 00:30:53,519
Tokyo to Kawasaki or

775
00:30:51,200 --> 00:30:56,960
Yokohama uh the fiber takes a bit of a

776
00:30:53,519 --> 00:30:58,799
longer route so it's 86 it's 43 km but

777
00:30:56,960 --> 00:31:00,279
we're going there and back again so that

778
00:30:58,799 --> 00:31:03,200
we can have both the server and the

779
00:31:00,279 --> 00:31:04,679
client at MIT in different rooms so that

780
00:31:03,200 --> 00:31:06,000
the grad student doesn't have to go back

781
00:31:04,679 --> 00:31:09,080
and forth to do the

782
00:31:06,000 --> 00:31:10,600
experiment so this is a a good example

783
00:31:09,080 --> 00:31:12,880
of the kind of data that you'll get so

784
00:31:10,600 --> 00:31:14,679
you get some weights they get mixed with

785
00:31:12,880 --> 00:31:17,880
your activations like this this number

786
00:31:14,679 --> 00:31:21,880
three U you they get multiplied then you

787
00:31:17,880 --> 00:31:23,360
sum up the uh the multiplied data and

788
00:31:21,880 --> 00:31:25,000
yeah and you record the output and this

789
00:31:23,360 --> 00:31:27,200
is this is what you call a calibration

790
00:31:25,000 --> 00:31:28,679
curve so this should just be a line uh

791
00:31:27,200 --> 00:31:30,600
this is desired data and this is the

792
00:31:28,679 --> 00:31:32,960
measure data and you get an we get an

793
00:31:30,600 --> 00:31:35,559
error of about 05% so that's about eight

794
00:31:32,960 --> 00:31:38,279
bits of precision which is good uh8 bits

795
00:31:35,559 --> 00:31:40,559
is what most people need to do a a large

796
00:31:38,279 --> 00:31:42,440
number of neuron netw Works in

797
00:31:40,559 --> 00:31:44,039
practice and you can also look at the

798
00:31:42,440 --> 00:31:46,000
energy consumption required the optical

799
00:31:44,039 --> 00:31:48,120
energy consumption is limited by

800
00:31:46,000 --> 00:31:51,880
detector noise and it ends up being on

801
00:31:48,120 --> 00:31:53,480
the order of 100,000 photons or so and

802
00:31:51,880 --> 00:31:56,080
if you use a Quantum loed detector it

803
00:31:53,480 --> 00:31:57,480
can be even down to a single Photon uh

804
00:31:56,080 --> 00:31:59,720
and we have some work about how you

805
00:31:57,480 --> 00:32:01,440
could uh do this with coherent detectors

806
00:31:59,720 --> 00:32:03,960
which would be Quantum limited this was

807
00:32:01,440 --> 00:32:07,039
a cryogenic demo but if it's coherent it

808
00:32:03,960 --> 00:32:10,600
would be U room

809
00:32:07,039 --> 00:32:13,399
temperature okay so this uh it's this is

810
00:32:10,600 --> 00:32:16,960
uh it's also very interesting because of

811
00:32:13,399 --> 00:32:18,799
in information security uh reasons so

812
00:32:16,960 --> 00:32:20,320
this is kind of an architecture with

813
00:32:18,799 --> 00:32:22,240
this architecture you're never sharing

814
00:32:20,320 --> 00:32:24,000
the activation data with the server so

815
00:32:22,240 --> 00:32:25,440
there's privacy in that sense but you'd

816
00:32:24,000 --> 00:32:28,480
also like to have privacy in the sense

817
00:32:25,440 --> 00:32:30,799
that the server never fully rals its

818
00:32:28,480 --> 00:32:32,279
model to the user either and we found

819
00:32:30,799 --> 00:32:34,360
that there are ways to do this with

820
00:32:32,279 --> 00:32:36,279
modifications to the algorithm where you

821
00:32:34,360 --> 00:32:38,200
can get efficient uh deep learning

822
00:32:36,279 --> 00:32:40,240
inference even when you're leaking only

823
00:32:38,200 --> 00:32:42,120
a very fraction small fraction of the

824
00:32:40,240 --> 00:32:44,519
weight data and a very small fraction of

825
00:32:42,120 --> 00:32:49,240
the activation data um and this can be

826
00:32:44,519 --> 00:32:51,120
proved using theorems related to Quantum

827
00:32:49,240 --> 00:32:52,559
communication so this final thing that

828
00:32:51,120 --> 00:32:55,720
I'd like to talk about in the last three

829
00:32:52,559 --> 00:32:58,080
minutes I have is uh photonics for uh

830
00:32:55,720 --> 00:33:00,799
dense dense or highly scalable computing

831
00:32:58,080 --> 00:33:03,480
based on coherent detection and so this

832
00:33:00,799 --> 00:33:04,679
is based on a question that is a as a

833
00:33:03,480 --> 00:33:06,960
new architecture it's based on a

834
00:33:04,679 --> 00:33:09,399
question which is if I have two Optical

835
00:33:06,960 --> 00:33:12,200
waves X and each each of these Optical

836
00:33:09,399 --> 00:33:13,279
signals encodes a number X and Y and

837
00:33:12,200 --> 00:33:14,679
what's the most efficient way to

838
00:33:13,279 --> 00:33:17,080
multiply those

839
00:33:14,679 --> 00:33:18,600
signals you can't do it just by sending

840
00:33:17,080 --> 00:33:20,240
the signals through each other I mean

841
00:33:18,600 --> 00:33:24,000
Optics they just go straight through

842
00:33:20,240 --> 00:33:25,360
each other uh interference is a linear

843
00:33:24,000 --> 00:33:27,919
and so that will never produce a

844
00:33:25,360 --> 00:33:28,720
nonlinear result uh you can do it

845
00:33:27,919 --> 00:33:30,440
through something that's called

846
00:33:28,720 --> 00:33:33,480
nonlinear Optics some frequency

847
00:33:30,440 --> 00:33:36,159
generation uh but that's is very very

848
00:33:33,480 --> 00:33:38,000
very challenging it's a whole field and

849
00:33:36,159 --> 00:33:40,320
it usually doesn't work at low

850
00:33:38,000 --> 00:33:43,120
energy and the solution to this is

851
00:33:40,320 --> 00:33:45,840
actually it won the Nobel Prize and it

852
00:33:43,120 --> 00:33:48,600
won the Nobel Prize back in 2020

853
00:33:45,840 --> 00:33:50,679
1921 and it's this concept called the

854
00:33:48,600 --> 00:33:54,279
photoelectric effect which is if you

855
00:33:50,679 --> 00:33:55,760
have a light incident on uh a metal or

856
00:33:54,279 --> 00:33:58,440
in our case a photod detector it

857
00:33:55,760 --> 00:34:01,159
produces a a stream of electrons and the

858
00:33:58,440 --> 00:34:02,880
number of electrons it's related to the

859
00:34:01,159 --> 00:34:04,519
intensity of the field but the intensity

860
00:34:02,880 --> 00:34:06,039
of the field is your electric field

861
00:34:04,519 --> 00:34:09,159
squared or E

862
00:34:06,039 --> 00:34:11,320
squared so that's a nonlinearity and you

863
00:34:09,159 --> 00:34:12,760
can combine that nonlinearity with just

864
00:34:11,320 --> 00:34:15,760
interference and you get something that

865
00:34:12,760 --> 00:34:17,399
multiplies two numbers on a photocurrent

866
00:34:15,760 --> 00:34:19,359
and this is something that we're all

867
00:34:17,399 --> 00:34:21,359
familiar with in coherent Optics is

868
00:34:19,359 --> 00:34:23,639
called a homine detector they have the

869
00:34:21,359 --> 00:34:25,240
same thing in radio frequency uh you can

870
00:34:23,639 --> 00:34:27,480
do it to multiply two numbers you can

871
00:34:25,240 --> 00:34:29,359
also do it to multiply two vectors and

872
00:34:27,480 --> 00:34:31,520
if you have many such detectors you can

873
00:34:29,359 --> 00:34:32,720
multiply a matrix on a vector and so

874
00:34:31,520 --> 00:34:37,639
this was the architecture that we

875
00:34:32,720 --> 00:34:41,599
proposed back in 2019 um and in 2022 we

876
00:34:37,639 --> 00:34:43,079
also realized this one um to generate

877
00:34:41,599 --> 00:34:45,480
the optical signals you needed for the

878
00:34:43,079 --> 00:34:49,079
weights we used an array of vixel so

879
00:34:45,480 --> 00:34:52,119
these are uh these are microscale lasers

880
00:34:49,079 --> 00:34:53,879
we used a fan a a phase mask to Fan out

881
00:34:52,119 --> 00:34:56,839
the data appropriately and then we had a

882
00:34:53,879 --> 00:34:58,760
detector array um this is just a p

883
00:34:56,839 --> 00:35:00,599
picture of the pixels pixels can be very

884
00:34:58,760 --> 00:35:02,720
fast they can modulate very fast and

885
00:35:00,599 --> 00:35:04,839
they can be very compact uh so those are

886
00:35:02,720 --> 00:35:08,320
the two things we need and this is just

887
00:35:04,839 --> 00:35:10,400
an example of the fan out experiment and

888
00:35:08,320 --> 00:35:11,760
uh an example of the data we didn't get

889
00:35:10,400 --> 00:35:15,720
quite as good accuracy it was on the

890
00:35:11,760 --> 00:35:18,680
order of 2% not 0.5% um but this was I

891
00:35:15,720 --> 00:35:21,320
think there's room for improving that

892
00:35:18,680 --> 00:35:24,160
and I just kind of wanted since I'm out

893
00:35:21,320 --> 00:35:27,000
of time I just wanted to uh highlight

894
00:35:24,160 --> 00:35:28,240
how the this approach based on coherent

895
00:35:27,000 --> 00:35:30,000
detection is something that we're

896
00:35:28,240 --> 00:35:32,680
looking towards spinning off a startup

897
00:35:30,000 --> 00:35:34,680
on uh so our hope is that the

898
00:35:32,680 --> 00:35:36,800
theoretical performance of the Prototype

899
00:35:34,680 --> 00:35:40,079
that we developed in this paper already

900
00:35:36,800 --> 00:35:41,640
sits uh well above what's possible with

901
00:35:40,079 --> 00:35:43,280
state-of-the-art electronics and our

902
00:35:41,640 --> 00:35:45,200
hope is that building on that prototype

903
00:35:43,280 --> 00:35:48,000
and making this theoretical performance

904
00:35:45,200 --> 00:35:50,280
and actual performance um that we can

905
00:35:48,000 --> 00:35:51,359
make this into a useful product and it's

906
00:35:50,280 --> 00:35:52,960
also something where I think that

907
00:35:51,359 --> 00:35:54,839
there's a space in the market for this

908
00:35:52,960 --> 00:35:57,640
kind of an approach uh there were a lot

909
00:35:54,839 --> 00:35:59,280
of different uh uh startups that start

910
00:35:57,640 --> 00:36:01,240
started out doing optical Computing that

911
00:35:59,280 --> 00:36:02,920
went into Optical interconnects and so

912
00:36:01,240 --> 00:36:05,400
now somebody has to carry on the idea of

913
00:36:02,920 --> 00:36:07,640
optical Computing and so with that I'd

914
00:36:05,400 --> 00:36:10,000
uh like to conclude and if you're

915
00:36:07,640 --> 00:36:11,400
interested more uh we do have these uh

916
00:36:10,000 --> 00:36:12,920
papers and I'd really like to

917
00:36:11,400 --> 00:36:14,839
acknowledge all the great people who

918
00:36:12,920 --> 00:36:16,850
collaborated with us on this work thank

919
00:36:14,839 --> 00:36:21,520
you very

920
00:36:16,850 --> 00:36:24,160
[Applause]

921
00:36:21,520 --> 00:36:26,960
much all right so the first one about

922
00:36:24,160 --> 00:36:29,440
the topic is is it possible to create

923
00:36:26,960 --> 00:36:32,119
General Purp a general purpose computer

924
00:36:29,440 --> 00:36:34,599
with photonic systems only are there any

925
00:36:32,119 --> 00:36:36,839
obstacles to Achi achieving universality

926
00:36:34,599 --> 00:36:39,800
or tur

927
00:36:36,839 --> 00:36:41,960
completeness so possible it's certainly

928
00:36:39,800 --> 00:36:44,800
possible the question is will that

929
00:36:41,960 --> 00:36:48,319
computer be efficient and the answer is

930
00:36:44,800 --> 00:36:50,880
probably not uh so the closest to to

931
00:36:48,319 --> 00:36:52,480
this direction that I've seen is there

932
00:36:50,880 --> 00:36:55,079
there are a lot of this work on

933
00:36:52,480 --> 00:36:57,720
efficient photonic transistors and

934
00:36:55,079 --> 00:37:00,720
transistors are very important primitive

935
00:36:57,720 --> 00:37:02,240
uh to any kind of universal Computing um

936
00:37:00,720 --> 00:37:04,640
there was some work out of

937
00:37:02,240 --> 00:37:07,240
ntbr uh that demonstrated transistors

938
00:37:04,640 --> 00:37:10,200
out of fto dual scale at around a

939
00:37:07,240 --> 00:37:11,440
gigahertz speed so that's good uh the

940
00:37:10,200 --> 00:37:16,000
transistors

941
00:37:11,440 --> 00:37:17,240
were tens of microns in size so you're

942
00:37:16,000 --> 00:37:20,640
you're immediately going to have

943
00:37:17,240 --> 00:37:22,960
something that's much larger than uh a

944
00:37:20,640 --> 00:37:24,000
standard chip if you implement many of

945
00:37:22,960 --> 00:37:26,400
these

946
00:37:24,000 --> 00:37:28,960
transistors and I don't know if there's

947
00:37:26,400 --> 00:37:30,440
a fundamental way to fix that

948
00:37:28,960 --> 00:37:32,319
fundamentally Optics is going to be

949
00:37:30,440 --> 00:37:34,280
limited by the wavelength of light so

950
00:37:32,319 --> 00:37:38,040
that's going to be microns and

951
00:37:34,280 --> 00:37:43,079
transistors can go down to tens of

952
00:37:38,040 --> 00:37:46,119
nanometers but I but I think that uh you

953
00:37:43,079 --> 00:37:48,920
don't need full universality to make use

954
00:37:46,119 --> 00:37:50,720
of make use of optics for computation I

955
00:37:48,920 --> 00:37:52,960
think that the best way to use Optics is

956
00:37:50,720 --> 00:37:54,599
to use optics for what it's good at and

957
00:37:52,960 --> 00:37:57,599
then do the rest of the stuff in

958
00:37:54,599 --> 00:37:57,599
electronics

959
00:37:59,680 --> 00:38:03,079
okay so once it reaches the scaleup

960
00:38:01,599 --> 00:38:04,760
point how do you envision the

961
00:38:03,079 --> 00:38:06,800
integration between photonic Computing

962
00:38:04,760 --> 00:38:08,440
systems and electronic ones since the

963
00:38:06,800 --> 00:38:10,520
input lag can be an issue due to the

964
00:38:08,440 --> 00:38:12,079
orders of magnitude Gap and so yeah this

965
00:38:10,520 --> 00:38:13,920
is related to the first question which

966
00:38:12,079 --> 00:38:15,599
is if you make if you have a hybrid

967
00:38:13,920 --> 00:38:17,760
computer with both Optical and

968
00:38:15,599 --> 00:38:20,319
electronic you have to do conversion

969
00:38:17,760 --> 00:38:23,319
between the two uh so you'll need you'll

970
00:38:20,319 --> 00:38:24,800
need Dax and adcs to go from Optical to

971
00:38:23,319 --> 00:38:29,000
go from analog to digital and then

972
00:38:24,800 --> 00:38:32,480
you'll need modulators and uh detectors

973
00:38:29,000 --> 00:38:36,000
and so I think that a key point to

974
00:38:32,480 --> 00:38:38,240
mention here is that the the number of

975
00:38:36,000 --> 00:38:40,240
Ops you perform in a matrix Vector

976
00:38:38,240 --> 00:38:42,280
product in the optical domain is much

977
00:38:40,240 --> 00:38:44,920
much larger than the amount of data that

978
00:38:42,280 --> 00:38:47,240
comes in and out uh so if we're talking

979
00:38:44,920 --> 00:38:49,280
about a thousand by, matrix

980
00:38:47,240 --> 00:38:51,040
multiplication you send in a vector of

981
00:38:49,280 --> 00:38:53,040
size a thousand and you read out a

982
00:38:51,040 --> 00:38:55,839
vector of a SI a thousand the number of

983
00:38:53,040 --> 00:38:58,200
Ops is about a million and so you can

984
00:38:55,839 --> 00:39:00,960
still get very good throughputs even

985
00:38:58,200 --> 00:39:02,839
with uh uh even if you're electronically

986
00:39:00,960 --> 00:39:05,040
bottlenecked uh because you're doing a

987
00:39:02,839 --> 00:39:07,000
million for the cost of a thousand but

988
00:39:05,040 --> 00:39:08,599
in order to get those advantages you

989
00:39:07,000 --> 00:39:11,079
need really need to scale something up

990
00:39:08,599 --> 00:39:13,680
where you can where you can process

991
00:39:11,079 --> 00:39:15,720
those matrices of size a

992
00:39:13,680 --> 00:39:19,000
thousand so let's

993
00:39:15,720 --> 00:39:22,560
see okay so this question is uh when do

994
00:39:19,000 --> 00:39:25,160
you expect a photonic Quantum Computing

995
00:39:22,560 --> 00:39:26,760
uh system to be available for an

996
00:39:25,160 --> 00:39:29,040
industrial use so there are two things

997
00:39:26,760 --> 00:39:30,640
there one is when do you expect a

998
00:39:29,040 --> 00:39:33,079
photonic Quantum Computing system to be

999
00:39:30,640 --> 00:39:37,520
available and second when will it be

1000
00:39:33,079 --> 00:39:39,640
industrially relevant and these these

1001
00:39:37,520 --> 00:39:42,480
startups already have prototypes out

1002
00:39:39,640 --> 00:39:45,640
there for example zanadu has a a

1003
00:39:42,480 --> 00:39:48,319
particular system involving uh U

1004
00:39:45,640 --> 00:39:50,839
involving delay lines and interference

1005
00:39:48,319 --> 00:39:52,880
and I think at this point I would say

1006
00:39:50,839 --> 00:39:54,760
those are primarily science experiments

1007
00:39:52,880 --> 00:39:58,680
that they're trying to engineer very

1008
00:39:54,760 --> 00:40:02,640
well when it will become a universal

1009
00:39:58,680 --> 00:40:05,760
processor there are a bunch of uh open

1010
00:40:02,640 --> 00:40:07,319
questions um kind of fundamental open

1011
00:40:05,760 --> 00:40:10,079
questions and technologies that need to

1012
00:40:07,319 --> 00:40:13,079
be developed before it becomes a pro be

1013
00:40:10,079 --> 00:40:15,079
becomes a universal processor um if that

1014
00:40:13,079 --> 00:40:17,800
happens within 5 to 10 years I think

1015
00:40:15,079 --> 00:40:21,560
that that would be great but I I can't

1016
00:40:17,800 --> 00:40:24,800
guarantee that and for an industrial

1017
00:40:21,560 --> 00:40:26,720
use this is the applications question

1018
00:40:24,800 --> 00:40:30,000
and it's one that I think no one has a

1019
00:40:26,720 --> 00:40:32,520
good answer to to there are there are

1020
00:40:30,000 --> 00:40:34,560
very specific there are very few

1021
00:40:32,520 --> 00:40:36,119
specific applications where we know

1022
00:40:34,560 --> 00:40:37,640
quantum computers have an exponential

1023
00:40:36,119 --> 00:40:42,640
speed up and some of these are really

1024
00:40:37,640 --> 00:40:46,240
useful like uh U like a RSA decryption

1025
00:40:42,640 --> 00:40:49,240
or uh quantum chemistry simulation uh

1026
00:40:46,240 --> 00:40:51,440
some of them are not very have no use uh

1027
00:40:49,240 --> 00:40:53,839
like very specific toy problems that you

1028
00:40:51,440 --> 00:40:56,359
put in a paper and then there are a

1029
00:40:53,839 --> 00:40:59,079
bunch of problems that might have a

1030
00:40:56,359 --> 00:41:02,720
speed up uh especially problems related

1031
00:40:59,079 --> 00:41:05,280
to um optimization and so I think more

1032
00:41:02,720 --> 00:41:09,240
research needs to be done uh to see if

1033
00:41:05,280 --> 00:41:09,240
there will be industrially useful

1034
00:41:09,960 --> 00:41:15,280
algorithms and that's the last question

1035
00:41:12,079 --> 00:41:15,280
thank you

