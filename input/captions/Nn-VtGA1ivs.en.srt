1
00:00:00,080 --> 00:00:05,640
hi everyone uh my name is Hara Hara

2
00:00:02,320 --> 00:00:08,040
podima I'm um an assistant professor of

3
00:00:05,640 --> 00:00:10,800
operations research as statistics at

4
00:00:08,040 --> 00:00:14,360
Sloan I'm fairly new this is my uh

5
00:00:10,800 --> 00:00:18,000
second year being here um my background

6
00:00:14,360 --> 00:00:20,359
is from theoretical computer science um

7
00:00:18,000 --> 00:00:23,080
I did my PhD at Harvard then I did a

8
00:00:20,359 --> 00:00:25,680
post hook at Berkeley uh and now I'm

9
00:00:23,080 --> 00:00:29,640
super happy to be among the faculty here

10
00:00:25,680 --> 00:00:31,679
uh at SLO so my research focuses on

11
00:00:29,640 --> 00:00:34,520
everything that has to do with the human

12
00:00:31,679 --> 00:00:38,120
facing algorithmic decision making and

13
00:00:34,520 --> 00:00:40,520
specifically about how human behavior um

14
00:00:38,120 --> 00:00:43,760
affects the results of algorithms that

15
00:00:40,520 --> 00:00:46,960
we use to make um real life decisions

16
00:00:43,760 --> 00:00:49,640
for people to give you some uh better

17
00:00:46,960 --> 00:00:53,039
idea of the pipeline of problems that I

18
00:00:49,640 --> 00:00:54,440
consider um so all of us know that you

19
00:00:53,039 --> 00:00:56,760
know there are many algorithmic

20
00:00:54,440 --> 00:01:00,120
decision-making systems nowadays that

21
00:00:56,760 --> 00:01:02,680
can classify us and um give give

22
00:01:00,120 --> 00:01:05,960
decisions give us like loan approvals or

23
00:01:02,680 --> 00:01:08,680
whatnot so the pipeline of uh the

24
00:01:05,960 --> 00:01:12,600
interaction there is as follows uh

25
00:01:08,680 --> 00:01:14,400
humans submit uh applications that that

26
00:01:12,600 --> 00:01:16,119
contain all of their features for

27
00:01:14,400 --> 00:01:18,560
example their features include their

28
00:01:16,119 --> 00:01:20,840
demographics their credit history if we

29
00:01:18,560 --> 00:01:24,200
talk about the loan approval

30
00:01:20,840 --> 00:01:28,799
situation um any loans that they already

31
00:01:24,200 --> 00:01:30,759
have and so on so forth and um the

32
00:01:28,799 --> 00:01:33,040
output of this inter action is a

33
00:01:30,759 --> 00:01:35,280
decision so this decision is whether the

34
00:01:33,040 --> 00:01:37,880
person was approved for a loan whether

35
00:01:35,280 --> 00:01:40,720
the person that submitted an application

36
00:01:37,880 --> 00:01:43,280
will be invited for a on-site interview

37
00:01:40,720 --> 00:01:45,200
or even whether uh the person will be

38
00:01:43,280 --> 00:01:48,040
given some benefits or some

39
00:01:45,200 --> 00:01:49,719
recommendations according to um what

40
00:01:48,040 --> 00:01:51,200
their preferences or what their

41
00:01:49,719 --> 00:01:53,880
consumption profiles were in the

42
00:01:51,200 --> 00:01:56,039
previous rounds now because of how

43
00:01:53,880 --> 00:01:58,799
consequential these decisions are for

44
00:01:56,039 --> 00:02:01,200
people's everyday lives um we have

45
00:01:58,799 --> 00:02:04,240
started seeing that um the way that

46
00:02:01,200 --> 00:02:08,000
algorithms make decisions for humans

47
00:02:04,240 --> 00:02:10,800
directly impacts in the downstream way

48
00:02:08,000 --> 00:02:13,400
society and because people have realized

49
00:02:10,800 --> 00:02:15,519
how consequential these decisions are

50
00:02:13,400 --> 00:02:18,480
they have started being documented to

51
00:02:15,519 --> 00:02:21,160
quote unquote misbehave when it comes to

52
00:02:18,480 --> 00:02:22,519
how they submit their applications

53
00:02:21,160 --> 00:02:25,239
through all these algorithmic

54
00:02:22,519 --> 00:02:28,000
decision-making systems um some of the

55
00:02:25,239 --> 00:02:31,080
ways uh of misbehaving that my research

56
00:02:28,000 --> 00:02:34,080
is considering uh is what happens when

57
00:02:31,080 --> 00:02:36,280
agents are myop agents or humans we call

58
00:02:34,080 --> 00:02:38,440
them agents interchangeably are

59
00:02:36,280 --> 00:02:40,800
myopically strategic uh what does it

60
00:02:38,440 --> 00:02:43,280
mean to be myopically strategic it means

61
00:02:40,800 --> 00:02:46,280
that when you are applying let's say for

62
00:02:43,280 --> 00:02:48,760
a loan and you send your application to

63
00:02:46,280 --> 00:02:51,120
the system you're trying to uh

64
00:02:48,760 --> 00:02:53,319
strategize with the features that you

65
00:02:51,120 --> 00:02:55,120
submit in order to maximize the

66
00:02:53,319 --> 00:02:58,319
probability that you will be given the

67
00:02:55,120 --> 00:03:01,480
loan so uh a very concrete example of

68
00:02:58,319 --> 00:03:04,360
strategize that actually many websites

69
00:03:01,480 --> 00:03:07,560
uh advocate for and suggest that you do

70
00:03:04,360 --> 00:03:09,599
um is that before you apply for a loan

71
00:03:07,560 --> 00:03:11,680
you have to uh increase the number of

72
00:03:09,599 --> 00:03:13,319
credit cards that you have so that uh

73
00:03:11,680 --> 00:03:16,799
you are shown to the system as more

74
00:03:13,319 --> 00:03:19,159
credit worthy other types of misbehavior

75
00:03:16,799 --> 00:03:22,159
uh include individuals that are

76
00:03:19,159 --> 00:03:24,840
sometimes completely unpredictable or uh

77
00:03:22,159 --> 00:03:25,640
make take actions that can be described

78
00:03:24,840 --> 00:03:28,560
as

79
00:03:25,640 --> 00:03:32,120
irrational um but also sometimes people

80
00:03:28,560 --> 00:03:35,159
are um like not just myopic so they try

81
00:03:32,120 --> 00:03:39,080
to strategize uh thinking almost like 10

82
00:03:35,159 --> 00:03:42,040
steps ahead in the future now um of

83
00:03:39,080 --> 00:03:45,120
course there are many ways that the

84
00:03:42,040 --> 00:03:47,680
algorithms themselves can stop

85
00:03:45,120 --> 00:03:49,799
strategize but another component that my

86
00:03:47,680 --> 00:03:52,480
research considers is what are the

87
00:03:49,799 --> 00:03:55,280
inputs from policy and regulation that

88
00:03:52,480 --> 00:03:57,720
can help uh algorithms make good

89
00:03:55,280 --> 00:03:59,920
decisions in spite of the fact that

90
00:03:57,720 --> 00:04:02,200
individuals are misbehaving when they

91
00:03:59,920 --> 00:04:04,360
supplying their data as input to these

92
00:04:02,200 --> 00:04:06,480
algorithms so the three pillars of my

93
00:04:04,360 --> 00:04:09,239
research are the following pillar number

94
00:04:06,480 --> 00:04:11,599
one how are the decisions that

95
00:04:09,239 --> 00:04:15,000
algorithmic decision-making systems are

96
00:04:11,599 --> 00:04:17,919
affected when uh the users the agents

97
00:04:15,000 --> 00:04:21,600
are misbehaving pillar number

98
00:04:17,919 --> 00:04:23,520
two how can we quantify the downstream

99
00:04:21,600 --> 00:04:27,000
effects of

100
00:04:23,520 --> 00:04:29,479
strategizes uh to society as a whole uh

101
00:04:27,000 --> 00:04:30,680
pillar number three what is the correct

102
00:04:29,479 --> 00:04:32,759
approach approach that we should be

103
00:04:30,680 --> 00:04:35,039
taking when it comes to policy and

104
00:04:32,759 --> 00:04:37,199
regulation when we know that uh

105
00:04:35,039 --> 00:04:40,080
algorithmic decision making systems are

106
00:04:37,199 --> 00:04:42,199
going to be interacting with humans that

107
00:04:40,080 --> 00:04:44,960
have you know different incentives that

108
00:04:42,199 --> 00:04:47,720
what uh systems may have thought about

109
00:04:44,960 --> 00:04:50,240
already um I want to highlight a very

110
00:04:47,720 --> 00:04:52,400
quick uh research project uh that was

111
00:04:50,240 --> 00:04:55,520
very fun to do about the disperate

112
00:04:52,400 --> 00:04:57,800
effects of strategic recommendation so

113
00:04:55,520 --> 00:05:00,520
I'm sure all of you know here uh that

114
00:04:57,800 --> 00:05:02,680
recommendation systems nowadays exist

115
00:05:00,520 --> 00:05:05,240
and create a certain type of feedback

116
00:05:02,680 --> 00:05:08,039
loop what is this feedback loop on the

117
00:05:05,240 --> 00:05:09,800
one hand we have the user who goes on

118
00:05:08,039 --> 00:05:11,639
the recommendation system and for the

119
00:05:09,800 --> 00:05:15,360
purposes of this example let's just

120
00:05:11,639 --> 00:05:17,320
consider Tik Tok uh but again like these

121
00:05:15,360 --> 00:05:18,919
results generalized to any type of

122
00:05:17,320 --> 00:05:22,639
recommendation system you can have in

123
00:05:18,919 --> 00:05:24,840
mind so let's assume that our user uh is

124
00:05:22,639 --> 00:05:27,080
interacting with the recommendation

125
00:05:24,840 --> 00:05:31,080
system of uh Tik

126
00:05:27,080 --> 00:05:33,560
Tok now the user consumes content uh

127
00:05:31,080 --> 00:05:36,440
based on the recommendations that the

128
00:05:33,560 --> 00:05:39,479
recommendation system or Tik Tok itself

129
00:05:36,440 --> 00:05:42,240
gives to the user so the system gives a

130
00:05:39,479 --> 00:05:45,160
slate uh of options to the user and the

131
00:05:42,240 --> 00:05:48,720
user can decide what to consume and for

132
00:05:45,160 --> 00:05:51,720
how long to consume it but note that the

133
00:05:48,720 --> 00:05:54,800
Slate of recommendations that Tik Tok

134
00:05:51,720 --> 00:05:58,639
chooses to present to the user crucially

135
00:05:54,800 --> 00:06:00,479
depends on what the user consumed uh in

136
00:05:58,639 --> 00:06:03,160
the previous round

137
00:06:00,479 --> 00:06:05,759
so um this is uh this is a very

138
00:06:03,160 --> 00:06:08,479
well-known phenomenon in um

139
00:06:05,759 --> 00:06:11,639
recommendation systems and it's the

140
00:06:08,479 --> 00:06:14,479
whole purpose behind personalization of

141
00:06:11,639 --> 00:06:17,000
these recommendation systems uh and

142
00:06:14,479 --> 00:06:19,080
because of the fact that every one of us

143
00:06:17,000 --> 00:06:21,440
can personalize the experience that we

144
00:06:19,080 --> 00:06:25,199
have in recommendation systems you may

145
00:06:21,440 --> 00:06:27,400
hear in popular press that uh curation

146
00:06:25,199 --> 00:06:29,599
nowadays uh has become a form of

147
00:06:27,400 --> 00:06:32,160
Creation in the content that people

148
00:06:29,599 --> 00:06:34,759
consuming recommendation

149
00:06:32,160 --> 00:06:36,639
systems uh and of course you don't have

150
00:06:34,759 --> 00:06:39,240
to think about the recommendation system

151
00:06:36,639 --> 00:06:41,800
as just Tik Tok um your favorite

152
00:06:39,240 --> 00:06:44,759
recommendation systems Netflix YouTube

153
00:06:41,800 --> 00:06:46,080
Instagram Spotify they all satisfy the

154
00:06:44,759 --> 00:06:48,960
same core

155
00:06:46,080 --> 00:06:52,440
property so what are some main questions

156
00:06:48,960 --> 00:06:55,360
uh that I'm interested in in this area

157
00:06:52,440 --> 00:06:58,720
um question number one is are people

158
00:06:55,360 --> 00:07:01,400
that are not researchers uh at MIT aware

159
00:06:58,720 --> 00:07:04,000
of these feedback Loop um in some sense

160
00:07:01,400 --> 00:07:06,879
I want to understand whether Society

161
00:07:04,000 --> 00:07:09,680
knows that you know whatever they

162
00:07:06,879 --> 00:07:12,000
consume is based on recommendations that

163
00:07:09,680 --> 00:07:13,319
they themselves are based on past

164
00:07:12,000 --> 00:07:15,800
patterns of

165
00:07:13,319 --> 00:07:18,599
consumption uh and sub question number

166
00:07:15,800 --> 00:07:20,960
two is if they are aware of this

167
00:07:18,599 --> 00:07:22,800
feedback loop how do they act in

168
00:07:20,960 --> 00:07:25,919
response and I want to give you an

169
00:07:22,800 --> 00:07:29,800
example here of uh something from my

170
00:07:25,919 --> 00:07:32,960
personal experience so I have a maintain

171
00:07:29,800 --> 00:07:34,800
two Instagram accounts uh for one of my

172
00:07:32,960 --> 00:07:37,960
Instagram accounts which is my personal

173
00:07:34,800 --> 00:07:41,199
Instagram account I have all my personal

174
00:07:37,960 --> 00:07:43,759
interests except for dogs then I have an

175
00:07:41,199 --> 00:07:47,400
Instagram account specifically for my

176
00:07:43,759 --> 00:07:50,080
dog where I only consume dog related

177
00:07:47,400 --> 00:07:52,400
content um so I was wondering when I

178
00:07:50,080 --> 00:07:54,800
started this project is this something

179
00:07:52,400 --> 00:07:57,120
that other people do are they

180
00:07:54,800 --> 00:07:59,520
specifically trying to strategize with

181
00:07:57,120 --> 00:08:02,520
what they consume in order to steer the

182
00:07:59,520 --> 00:08:05,560
system one way or the other and then if

183
00:08:02,520 --> 00:08:08,080
they do are there any harms to users if

184
00:08:05,560 --> 00:08:11,440
the recommendation system does not adapt

185
00:08:08,080 --> 00:08:13,479
to this type of curation or strategize

186
00:08:11,440 --> 00:08:15,520
and are there any

187
00:08:13,479 --> 00:08:17,639
interventions so some of the things that

188
00:08:15,520 --> 00:08:20,240
we proved in this work well first of all

189
00:08:17,639 --> 00:08:23,280
before uh starting to prove concrete

190
00:08:20,240 --> 00:08:26,199
results uh my research uh thinks a lot

191
00:08:23,280 --> 00:08:29,560
about the problem from of um

192
00:08:26,199 --> 00:08:31,840
identifying like what are the realistic

193
00:08:29,560 --> 00:08:34,800
human behaviors that we encounter in

194
00:08:31,840 --> 00:08:37,479
practice and the way that I uh identify

195
00:08:34,800 --> 00:08:40,240
that in my work is I run uh surveys on

196
00:08:37,479 --> 00:08:42,760
user consumption patterns so we surveyed

197
00:08:40,240 --> 00:08:45,399
a population of 200 individuals that

198
00:08:42,760 --> 00:08:48,200
have no relationship with a university

199
00:08:45,399 --> 00:08:50,920
um that basically they told us how they

200
00:08:48,200 --> 00:08:53,839
are interacting with uh platforms like

201
00:08:50,920 --> 00:08:56,760
Tik Tok then my resarch introduced a

202
00:08:53,839 --> 00:08:59,200
theoretical model uh about recommending

203
00:08:56,760 --> 00:09:02,240
uh content to different types of users

204
00:08:59,200 --> 00:09:04,839
and we show that if you have a

205
00:09:02,240 --> 00:09:07,600
population a total population that is

206
00:09:04,839 --> 00:09:09,680
split into two big chunks a majorities

207
00:09:07,600 --> 00:09:11,880
of population and the minorities of

208
00:09:09,680 --> 00:09:14,640
population if the system the

209
00:09:11,880 --> 00:09:16,360
recommendation platform does not adapt

210
00:09:14,640 --> 00:09:19,760
to the type of

211
00:09:16,360 --> 00:09:22,200
strategize that the users are doing it

212
00:09:19,760 --> 00:09:25,360
may end up being the case that the

213
00:09:22,200 --> 00:09:28,360
minority population becomes even more

214
00:09:25,360 --> 00:09:30,600
secluded uh from the general uh

215
00:09:28,360 --> 00:09:34,480
subpopulation the mainstream content

216
00:09:30,600 --> 00:09:36,519
that the uh majority population consumes

217
00:09:34,480 --> 00:09:38,079
this means that another explanation for

218
00:09:36,519 --> 00:09:40,680
filter bubbles another another

219
00:09:38,079 --> 00:09:44,200
explanation for polarization can be the

220
00:09:40,680 --> 00:09:46,600
fact that the um the system does not

221
00:09:44,200 --> 00:09:50,160
understand that users are strategizing

222
00:09:46,600 --> 00:09:52,959
in order to uh curate the content um in

223
00:09:50,160 --> 00:09:55,880
a more aggressive uh active way compared

224
00:09:52,959 --> 00:09:58,480
to what we thought before now I have uh

225
00:09:55,880 --> 00:10:00,640
roughly 3 to four minutes I had two

226
00:09:58,480 --> 00:10:04,560
sample research projects to discuss with

227
00:10:00,640 --> 00:10:07,760
you um that I have done uh with MBA and

228
00:10:04,560 --> 00:10:10,720
uh Ras one is about uh one is more

229
00:10:07,760 --> 00:10:13,040
traditional operations research uh land

230
00:10:10,720 --> 00:10:14,680
where we tried to improve the operations

231
00:10:13,040 --> 00:10:18,360
of uh breaking ground which is a

232
00:10:14,680 --> 00:10:22,200
nonprofit in New York City that helps uh

233
00:10:18,360 --> 00:10:24,760
match uh unhoused individuals to um

234
00:10:22,200 --> 00:10:27,240
housing uh to some like temporary and

235
00:10:24,760 --> 00:10:30,360
then hopefully permanent housing but for

236
00:10:27,240 --> 00:10:33,000
the purposes of uh

237
00:10:30,360 --> 00:10:36,680
being concise and being on time I want

238
00:10:33,000 --> 00:10:39,760
to highlight the uh current uh project

239
00:10:36,680 --> 00:10:41,839
that uh me and Alex shakya who is one of

240
00:10:39,760 --> 00:10:45,000
the uh panelists that is going to talk

241
00:10:41,839 --> 00:10:48,560
to you later today are doing with uh two

242
00:10:45,000 --> 00:10:50,600
MBA and uh student arrays uh at a very

243
00:10:48,560 --> 00:10:54,079
high level we are trying to build a

244
00:10:50,600 --> 00:10:57,320
canvas GPT um this is like an automated

245
00:10:54,079 --> 00:11:01,040
uh assistant or an automated bot for all

246
00:10:57,320 --> 00:11:05,760
uh MIT loan classes so let me just parse

247
00:11:01,040 --> 00:11:08,639
uh what that means um so MIT Sloan uses

248
00:11:05,760 --> 00:11:10,920
canvas which is a website uh for this is

249
00:11:08,639 --> 00:11:12,639
a learning management system that helps

250
00:11:10,920 --> 00:11:16,760
us manage class

251
00:11:12,639 --> 00:11:19,639
information now the problem here is that

252
00:11:16,760 --> 00:11:21,560
um all of the classes have a lot of

253
00:11:19,639 --> 00:11:24,240
information that is really really hard

254
00:11:21,560 --> 00:11:27,240
to navigate at times just to give you a

255
00:11:24,240 --> 00:11:29,880
very uh quick example here I hope that

256
00:11:27,240 --> 00:11:33,079
you can see my screen but this is like a

257
00:11:29,880 --> 00:11:36,760
sample of uh the information that a

258
00:11:33,079 --> 00:11:38,839
student in our MBA program gets um when

259
00:11:36,760 --> 00:11:41,399
they take my class which is one of the

260
00:11:38,839 --> 00:11:44,240
core classes data models and decisions

261
00:11:41,399 --> 00:11:46,600
so this is um a lot of information that

262
00:11:44,240 --> 00:11:49,560
the students need to remember and pay

263
00:11:46,600 --> 00:11:53,079
attention to at all times so our project

264
00:11:49,560 --> 00:11:55,800
is dat is to build a custom GPT a custom

265
00:11:53,079 --> 00:11:57,760
um assistant to answer questions about

266
00:11:55,800 --> 00:12:00,680
class material that is uploaded on

267
00:11:57,760 --> 00:12:03,920
canvas uh the question questions can be

268
00:12:00,680 --> 00:12:08,240
um what time is Professor hara's office

269
00:12:03,920 --> 00:12:10,560
hours uh what did we learn in class 13

270
00:12:08,240 --> 00:12:12,760
uh when did I when did we learn about

271
00:12:10,560 --> 00:12:15,880
linear regression and how can I find the

272
00:12:12,760 --> 00:12:18,519
specific content so it has to do with um

273
00:12:15,880 --> 00:12:20,720
mostly administrative questions and

274
00:12:18,519 --> 00:12:22,440
questions that pertain to um you know

275
00:12:20,720 --> 00:12:26,480
the syllabus of the

276
00:12:22,440 --> 00:12:27,920
class and uh at a second at a second

277
00:12:26,480 --> 00:12:30,199
phase which is not the thing that we're

278
00:12:27,920 --> 00:12:32,480
currently focused on at the second phase

279
00:12:30,199 --> 00:12:37,160
we would like to build uh an automated

280
00:12:32,480 --> 00:12:40,320
tutor that can help students uh learn so

281
00:12:37,160 --> 00:12:43,760
what would be a tutor uh in our example

282
00:12:40,320 --> 00:12:46,959
well a tutor would be some like pot that

283
00:12:43,760 --> 00:12:49,079
explains to the students um how linear

284
00:12:46,959 --> 00:12:50,760
aggression actually works given the

285
00:12:49,079 --> 00:12:54,199
material that we have been taught in

286
00:12:50,760 --> 00:12:56,120
class uh and also can hopefully uh help

287
00:12:54,199 --> 00:12:59,120
uh professors create homework for their

288
00:12:56,120 --> 00:12:59,120
classes

