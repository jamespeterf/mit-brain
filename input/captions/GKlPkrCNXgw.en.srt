1
00:00:04,839 --> 00:00:12,160
Our next speaker is professor Marcus

2
00:00:08,720 --> 00:00:15,280
Bureau. Many of you have worked with him

3
00:00:12,160 --> 00:00:18,800
as you can imagine. Uh professor Bureau

4
00:00:15,280 --> 00:00:20,720
is a professor of engineering in both

5
00:00:18,800 --> 00:00:23,680
the civil and environmental engineering

6
00:00:20,720 --> 00:00:28,279
and mechanical engineering. Today he's

7
00:00:23,680 --> 00:00:31,679
going to discuss some new AI

8
00:00:28,279 --> 00:00:34,760
architecture driving the breakthroughs

9
00:00:31,679 --> 00:00:37,120
both in science and in real life

10
00:00:34,760 --> 00:00:40,000
applications. Let's welcome Professor

11
00:00:37,120 --> 00:00:42,000
Bork. Yes. Um well, thank you so much.

12
00:00:40,000 --> 00:00:44,640
Um really great event and nice to see so

13
00:00:42,000 --> 00:00:46,160
many of you. Um yeah so I'll be talking

14
00:00:44,640 --> 00:00:48,480
really about how we built AI for

15
00:00:46,160 --> 00:00:50,640
scientific applications and you know

16
00:00:48,480 --> 00:00:52,800
where we are today and building AI that

17
00:00:50,640 --> 00:00:54,719
can think and you know make predictions

18
00:00:52,800 --> 00:00:56,800
that are not obvious uh maybe discover

19
00:00:54,719 --> 00:00:58,160
new things. So where we're today I mean

20
00:00:56,800 --> 00:01:00,000
we've seen a lot probably in the

21
00:00:58,160 --> 00:01:02,000
presentations um the last day or so

22
00:01:00,000 --> 00:01:04,320
about machine learning uh we can train

23
00:01:02,000 --> 00:01:07,439
complex functions we can even generate

24
00:01:04,320 --> 00:01:09,600
new data um you know code sequences and

25
00:01:07,439 --> 00:01:10,960
things like this but the the real

26
00:01:09,600 --> 00:01:12,720
challenge really is how do we really

27
00:01:10,960 --> 00:01:14,799
create something absolutely new things

28
00:01:12,720 --> 00:01:16,240
we haven't had before things that aren't

29
00:01:14,799 --> 00:01:17,600
in the training data and that's what the

30
00:01:16,240 --> 00:01:20,479
presentation will be about you know we

31
00:01:17,600 --> 00:01:23,520
refer to this as um broadly reasoning

32
00:01:20,479 --> 00:01:25,759
models um agentic AI AI that can take

33
00:01:23,520 --> 00:01:27,600
actions um with the world and interact

34
00:01:25,759 --> 00:01:29,360
with the world and make um you know

35
00:01:27,600 --> 00:01:31,840
decisions that hopefully bring us bring

36
00:01:29,360 --> 00:01:34,159
us forward. So that's what the about and

37
00:01:31,840 --> 00:01:37,280
I my research deals as you've seen with

38
00:01:34,159 --> 00:01:39,119
materials. I I study materials um these

39
00:01:37,280 --> 00:01:40,880
are really interesting systems. So I

40
00:01:39,119 --> 00:01:42,880
mean how many of you work on materials?

41
00:01:40,880 --> 00:01:44,079
Not everyone. Some. Yes. Great. Some.

42
00:01:42,880 --> 00:01:45,920
But you all of course know about

43
00:01:44,079 --> 00:01:47,840
materials. So you can look at anything

44
00:01:45,920 --> 00:01:50,240
fabric in your clothing. And if you take

45
00:01:47,840 --> 00:01:52,399
a microscope, it it will ultimately look

46
00:01:50,240 --> 00:01:53,840
like an atom, a molecule. And you can

47
00:01:52,399 --> 00:01:55,280
imagine it's a very challenging problem.

48
00:01:53,840 --> 00:01:56,880
Economically very important because

49
00:01:55,280 --> 00:01:58,000
everything's made from materials. You

50
00:01:56,880 --> 00:01:59,680
know, even if you're not making

51
00:01:58,000 --> 00:02:01,840
materials, you're relying on materials

52
00:01:59,680 --> 00:02:03,920
obviously. Um and the challenge a lot of

53
00:02:01,840 --> 00:02:06,320
times is how do we design these systems?

54
00:02:03,920 --> 00:02:08,640
How do we understand them? And there's a

55
00:02:06,320 --> 00:02:11,520
lot of relationships across scales. Now

56
00:02:08,640 --> 00:02:13,360
for I would say um 50 years 70 years

57
00:02:11,520 --> 00:02:15,440
since the 1950s really since we had

58
00:02:13,360 --> 00:02:17,680
bigger computers what physicists

59
00:02:15,440 --> 00:02:19,200
scientists chemists did we broke down

60
00:02:17,680 --> 00:02:21,680
the problem in different scales you know

61
00:02:19,200 --> 00:02:23,440
we said okay we have a a a chemical

62
00:02:21,680 --> 00:02:25,920
scale we have a molecular scale a

63
00:02:23,440 --> 00:02:27,840
continuum scale for engineers and we

64
00:02:25,920 --> 00:02:29,360
kind of try to bootstrap our way out of

65
00:02:27,840 --> 00:02:31,520
these systems. Now what the problem with

66
00:02:29,360 --> 00:02:33,200
this is that first of all the the

67
00:02:31,520 --> 00:02:35,519
complexity is very large. The other

68
00:02:33,200 --> 00:02:38,000
thing is that many of these problems

69
00:02:35,519 --> 00:02:39,599
across scales cannot be captured. So if

70
00:02:38,000 --> 00:02:42,000
you're in the protein business or

71
00:02:39,599 --> 00:02:43,760
chemistry business, if you're ignoring

72
00:02:42,000 --> 00:02:46,000
some of the details in your sequence

73
00:02:43,760 --> 00:02:47,519
that matters for a disease, you're not

74
00:02:46,000 --> 00:02:49,280
going to capture how the organism, the

75
00:02:47,519 --> 00:02:52,000
human will respond to it in drug design

76
00:02:49,280 --> 00:02:53,680
and things like this. Um so this idea of

77
00:02:52,000 --> 00:02:55,680
sort of coarse graining and simplifying

78
00:02:53,680 --> 00:02:56,959
things really doesn't work. And in fact,

79
00:02:55,680 --> 00:02:59,920
um that's sort of the road we've been

80
00:02:56,959 --> 00:03:01,920
hitting for a couple of decades now. AI

81
00:02:59,920 --> 00:03:05,200
has potential for us to change this

82
00:03:01,920 --> 00:03:06,640
game. change the equations here. And um

83
00:03:05,200 --> 00:03:08,319
something that I think is happening as

84
00:03:06,640 --> 00:03:10,800
you've all seen you know we have um

85
00:03:08,319 --> 00:03:12,640
better models even though you know these

86
00:03:10,800 --> 00:03:14,080
models are they look kind of impressive

87
00:03:12,640 --> 00:03:16,080
but most of these models are still

88
00:03:14,080 --> 00:03:17,920
pretty simplistic and primitive actually

89
00:03:16,080 --> 00:03:19,440
you know and especially you know even

90
00:03:17,920 --> 00:03:21,920
though of course you know today's jet

91
00:03:19,440 --> 00:03:23,360
GPT is here frontier models that

92
00:03:21,920 --> 00:03:25,360
unreleased yet are all the way up there

93
00:03:23,360 --> 00:03:27,599
but still a lot of these models aren't

94
00:03:25,360 --> 00:03:29,280
really able to produce new things and

95
00:03:27,599 --> 00:03:31,040
you can see this if you interacting with

96
00:03:29,280 --> 00:03:33,200
these models right and you ask them

97
00:03:31,040 --> 00:03:34,720
something totally new they really just

98
00:03:33,200 --> 00:03:36,799
interpret a lot of times between what

99
00:03:34,720 --> 00:03:39,280
they've seen in the data and this is

100
00:03:36,799 --> 00:03:41,680
shown here right so if you um look at AI

101
00:03:39,280 --> 00:03:43,760
today machine learning you're really

102
00:03:41,680 --> 00:03:45,440
limited to the kinds of things the model

103
00:03:43,760 --> 00:03:46,959
has seen maybe you can interpolate and

104
00:03:45,440 --> 00:03:48,560
combine things here and there but you

105
00:03:46,959 --> 00:03:51,120
you really cannot go to those white

106
00:03:48,560 --> 00:03:52,400
spaces where you have never seen any

107
00:03:51,120 --> 00:03:54,720
data and that's of course what we want

108
00:03:52,400 --> 00:03:57,200
to do as scientists we want to engineer

109
00:03:54,720 --> 00:03:59,519
discover things and create insights and

110
00:03:57,200 --> 00:04:01,439
knowledge of of of systems we have never

111
00:03:59,519 --> 00:04:04,480
studied um and that's what we're trying

112
00:04:01,439 --> 00:04:06,879
to build um and so this you know can be

113
00:04:04,480 --> 00:04:09,200
done and I think we've made a lot of

114
00:04:06,879 --> 00:04:10,720
progress in the last couple years uh to

115
00:04:09,200 --> 00:04:13,040
do this by by really thinking about how

116
00:04:10,720 --> 00:04:15,680
do we actually train AI how do we teach

117
00:04:13,040 --> 00:04:18,799
these models and in conventional

118
00:04:15,680 --> 00:04:20,479
learning you you teach models examples

119
00:04:18,799 --> 00:04:22,320
and they remember these examples and

120
00:04:20,479 --> 00:04:25,600
they produce sort of new combinations of

121
00:04:22,320 --> 00:04:27,680
examples in in in the solutions what we

122
00:04:25,600 --> 00:04:29,360
got to do though to have models really

123
00:04:27,680 --> 00:04:31,040
understand like scientists do like

124
00:04:29,360 --> 00:04:32,720
engineers do we have to sort of go one

125
00:04:31,040 --> 00:04:33,759
level deeper and understand principles

126
00:04:32,720 --> 00:04:35,440
and this is what's shown on the right

127
00:04:33,759 --> 00:04:36,400
hand side. So for many years in my group

128
00:04:35,440 --> 00:04:38,199
we've worked on something called

129
00:04:36,400 --> 00:04:40,320
category theory. It's a a branch of

130
00:04:38,199 --> 00:04:41,919
mathematics that really deals with

131
00:04:40,320 --> 00:04:44,400
relationships. So that looks at the

132
00:04:41,919 --> 00:04:46,560
world not as a collection of facts but

133
00:04:44,400 --> 00:04:48,240
of as relationships and relationships

134
00:04:46,560 --> 00:04:49,840
between relationships and abstractions.

135
00:04:48,240 --> 00:04:52,040
And this is how we understand the world.

136
00:04:49,840 --> 00:04:54,320
So if you're asked to design something

137
00:04:52,040 --> 00:04:55,600
new, you're not just copying something,

138
00:04:54,320 --> 00:04:57,199
you're thinking about how this might

139
00:04:55,600 --> 00:04:59,440
relate to the problem you're trying to

140
00:04:57,199 --> 00:05:01,440
solve, how you combine new ideas. And so

141
00:04:59,440 --> 00:05:02,880
you do compositional reasoning um really

142
00:05:01,440 --> 00:05:05,199
breaking it down and synthesizing

143
00:05:02,880 --> 00:05:07,039
something new. So this is in a nutshell

144
00:05:05,199 --> 00:05:08,639
what we're trying to build and teach AI

145
00:05:07,039 --> 00:05:10,479
models to do. Now the question is how do

146
00:05:08,639 --> 00:05:12,320
we do it and why? Well here's an obvious

147
00:05:10,479 --> 00:05:13,840
example right? So current AI models

148
00:05:12,320 --> 00:05:15,680
really look like sort of thing on the

149
00:05:13,840 --> 00:05:17,440
left. So they remember you know they

150
00:05:15,680 --> 00:05:18,720
look at an apple falling and they say

151
00:05:17,440 --> 00:05:20,320
yeah it falls pretty fast. They look at

152
00:05:18,720 --> 00:05:22,639
a feather and might say yeah it falls

153
00:05:20,320 --> 00:05:24,240
pretty slow and it might look at a a

154
00:05:22,639 --> 00:05:26,639
different problem like you know dropping

155
00:05:24,240 --> 00:05:28,639
a glass and it shatters. Okay, well the

156
00:05:26,639 --> 00:05:31,520
AI can remember this and it can give you

157
00:05:28,639 --> 00:05:32,800
answers. But we know in physics, you

158
00:05:31,520 --> 00:05:34,560
know, there's actually all of these

159
00:05:32,800 --> 00:05:36,639
problems, there's an underlying theory

160
00:05:34,560 --> 00:05:37,840
which is Newton's laws in this case. And

161
00:05:36,639 --> 00:05:39,919
you can actually describe all of these

162
00:05:37,840 --> 00:05:41,840
phenomena and many more by knowing this

163
00:05:39,919 --> 00:05:43,759
fundamental theory, right? And so now

164
00:05:41,840 --> 00:05:45,120
you have a true understanding and you

165
00:05:43,759 --> 00:05:46,880
can make predictions and this is what

166
00:05:45,120 --> 00:05:48,759
really the limitations of AI are today.

167
00:05:46,880 --> 00:05:52,000
You can't make these kind of fundamental

168
00:05:48,759 --> 00:05:53,120
predictions. Um, so in category theory

169
00:05:52,000 --> 00:05:54,560
and this is how this looks like. So

170
00:05:53,120 --> 00:05:58,080
we've done this work about 105 years

171
00:05:54,560 --> 00:06:00,160
ago. It's mathematics on piece of paper

172
00:05:58,080 --> 00:06:02,639
of a system you understand and you can

173
00:06:00,160 --> 00:06:05,199
kind of write down how things relate. So

174
00:06:02,639 --> 00:06:07,280
on the left you have spider silk protein

175
00:06:05,199 --> 00:06:08,960
and on the right you have music and we

176
00:06:07,280 --> 00:06:10,800
can build theories to relate these two

177
00:06:08,960 --> 00:06:12,560
concepts. That's pretty exciting but

178
00:06:10,800 --> 00:06:15,039
it's manually made. You know we made it

179
00:06:12,560 --> 00:06:17,280
took months and years to do. Uh it's not

180
00:06:15,039 --> 00:06:20,400
scalable. So what we really got to do or

181
00:06:17,280 --> 00:06:22,319
we'd like to do is to build systems that

182
00:06:20,400 --> 00:06:24,000
can make these discoveries on their own

183
00:06:22,319 --> 00:06:27,120
from observations. And this is where

184
00:06:24,000 --> 00:06:29,280
agentic reasoning AI comes in. So we are

185
00:06:27,120 --> 00:06:30,880
basically teaching these models to look

186
00:06:29,280 --> 00:06:32,120
at information which is sort of

187
00:06:30,880 --> 00:06:34,240
measurements of the world and

188
00:06:32,120 --> 00:06:36,080
observations and make connections

189
00:06:34,240 --> 00:06:38,240
between them at a very basic level. Sort

190
00:06:36,080 --> 00:06:39,680
of understand how things are related. Um

191
00:06:38,240 --> 00:06:40,880
and then hopefully we'll make actionable

192
00:06:39,680 --> 00:06:43,600
outcomes. So like if you're in

193
00:06:40,880 --> 00:06:45,680
healthcare we might you know help AI can

194
00:06:43,600 --> 00:06:47,840
help us with administration automation

195
00:06:45,680 --> 00:06:49,840
to drug discovery can discover proteins

196
00:06:47,840 --> 00:06:52,639
and molecules and things like this. Um

197
00:06:49,840 --> 00:06:54,160
and so I I'll talk about this process in

198
00:06:52,639 --> 00:06:55,759
quite some depth and then we'll show

199
00:06:54,160 --> 00:06:58,720
examples and sort of talk about some of

200
00:06:55,759 --> 00:07:01,440
the applications of course but um the

201
00:06:58,720 --> 00:07:02,560
beauty of AI today with even the models

202
00:07:01,440 --> 00:07:05,280
that we've seen in the previous

203
00:07:02,560 --> 00:07:06,560
presentation we have is multimodality

204
00:07:05,280 --> 00:07:08,080
and that's really important for science

205
00:07:06,560 --> 00:07:10,319
and engineering and tech many other

206
00:07:08,080 --> 00:07:11,599
areas too um because if you think about

207
00:07:10,319 --> 00:07:13,599
machine learning maybe in your

208
00:07:11,599 --> 00:07:15,440
businesses in your companies a lot of

209
00:07:13,599 --> 00:07:17,520
times you need perfect data right you

210
00:07:15,440 --> 00:07:19,919
need a table basically you have labeled

211
00:07:17,520 --> 00:07:21,440
data here's a molecule a molecule be

212
00:07:19,919 --> 00:07:22,720
these are the properties. The world

213
00:07:21,440 --> 00:07:24,880
doesn't look like this, right? It's

214
00:07:22,720 --> 00:07:26,960
incomplete. It's imperfect. You might

215
00:07:24,880 --> 00:07:28,720
have handwritten notes. You might have a

216
00:07:26,960 --> 00:07:31,039
video. You might have interviewed a

217
00:07:28,720 --> 00:07:33,120
machinist who worked on a machine for 30

218
00:07:31,039 --> 00:07:34,720
years and they figured out how to how to

219
00:07:33,120 --> 00:07:36,080
do this really well. So, you have all

220
00:07:34,720 --> 00:07:37,039
these different modalities and what we

221
00:07:36,080 --> 00:07:39,039
really want to do, you want to integrate

222
00:07:37,039 --> 00:07:41,680
all of these modalities, digitize them,

223
00:07:39,039 --> 00:07:44,240
tokenize them, and make sense of them

224
00:07:41,680 --> 00:07:46,240
and build abstractions. And the the key

225
00:07:44,240 --> 00:07:48,160
idea, the takeaway from this talk really

226
00:07:46,240 --> 00:07:49,520
is going to be in order to us to do

227
00:07:48,160 --> 00:07:52,160
this, we got to build something like

228
00:07:49,520 --> 00:07:54,080
Newton's laws. Um obviously not Newton's

229
00:07:52,160 --> 00:07:56,960
laws again might be boring, right? But

230
00:07:54,080 --> 00:07:58,560
other fundamental principles um that you

231
00:07:56,960 --> 00:08:00,720
know that are underlying relationships.

232
00:07:58,560 --> 00:08:02,960
And so now you're able to connect

233
00:08:00,720 --> 00:08:05,039
principles from biology to art,

234
00:08:02,960 --> 00:08:06,400
creativity to industrial processes and

235
00:08:05,039 --> 00:08:09,039
many more. Right? So that's sort of the

236
00:08:06,400 --> 00:08:10,560
promise of this. We're building this

237
00:08:09,039 --> 00:08:13,440
essentially by thinking really deeply

238
00:08:10,560 --> 00:08:15,039
about how models work and not just AI

239
00:08:13,440 --> 00:08:17,199
models or machine learning, but any

240
00:08:15,039 --> 00:08:19,840
model, right? So a finite element model

241
00:08:17,199 --> 00:08:22,240
in your company or fluid dynamics model

242
00:08:19,840 --> 00:08:24,479
um or a climate prediction is going to

243
00:08:22,240 --> 00:08:26,879
be one like on the left. It has no

244
00:08:24,479 --> 00:08:28,800
awareness of its own prediction. You

245
00:08:26,879 --> 00:08:30,720
your company humans assess the

246
00:08:28,800 --> 00:08:32,640
prediction, right? And and you you

247
00:08:30,720 --> 00:08:34,959
decide it's a good prediction, bad, new

248
00:08:32,640 --> 00:08:37,279
boundary condition, new new measurement.

249
00:08:34,959 --> 00:08:40,080
So what we really going to do is to have

250
00:08:37,279 --> 00:08:41,599
models assess their own predictions to

251
00:08:40,080 --> 00:08:43,440
be have to have some awareness in

252
00:08:41,599 --> 00:08:46,000
quotation mark okay awareness in a sense

253
00:08:43,440 --> 00:08:47,519
that they can assess what they do um

254
00:08:46,000 --> 00:08:49,040
they can reason over this they can

255
00:08:47,519 --> 00:08:51,760
correct their own mistakes they can

256
00:08:49,040 --> 00:08:53,680
correct logic they can make new ideas

257
00:08:51,760 --> 00:08:55,760
and new suggestions and sort of iterate

258
00:08:53,680 --> 00:08:57,040
through and this idea you know actually

259
00:08:55,760 --> 00:09:01,680
it's been around for a long time you

260
00:08:57,040 --> 00:09:03,200
know in the uh 60s uh included MIT um

261
00:09:01,680 --> 00:09:04,959
people have worked on what they called

262
00:09:03,200 --> 00:09:08,080
at the time artificial intelligence of

263
00:09:04,959 --> 00:09:10,240
course and these systems were um built

264
00:09:08,080 --> 00:09:12,800
sort of hard-coded logic right so they

265
00:09:10,240 --> 00:09:14,480
try to mimic how intelligence works and

266
00:09:12,800 --> 00:09:16,000
that didn't actually work because they

267
00:09:14,480 --> 00:09:18,080
became very brittle you know you needed

268
00:09:16,000 --> 00:09:20,399
to know everything these systems would

269
00:09:18,080 --> 00:09:22,160
do and that's not how the world works we

270
00:09:20,399 --> 00:09:24,320
have to have autonomous assembly of

271
00:09:22,160 --> 00:09:26,480
relationships and understanding and so

272
00:09:24,320 --> 00:09:28,720
this really took you know another 60

273
00:09:26,480 --> 00:09:31,200
years or so until we really had agentic

274
00:09:28,720 --> 00:09:33,519
models that cannot rely that do not rely

275
00:09:31,200 --> 00:09:34,560
on preconceived notions of how these

276
00:09:33,519 --> 00:09:36,080
individual ual components of

277
00:09:34,560 --> 00:09:38,080
intelligence interact but they can self

278
00:09:36,080 --> 00:09:41,200
assemble right and so I'll talk about

279
00:09:38,080 --> 00:09:43,279
this in in more depth of course because

280
00:09:41,200 --> 00:09:45,519
self assembly is fundamental to biology

281
00:09:43,279 --> 00:09:47,920
you know we study biological materials

282
00:09:45,519 --> 00:09:50,399
and any protein in your body is self

283
00:09:47,920 --> 00:09:52,080
assembled right it it it's not told to

284
00:09:50,399 --> 00:09:54,640
assemble in a certain way it has a

285
00:09:52,080 --> 00:09:57,680
memory an ability to do this naturally

286
00:09:54,640 --> 00:09:59,279
and it's a concerted action that happens

287
00:09:57,680 --> 00:10:01,519
an emergence of properties and so this

288
00:09:59,279 --> 00:10:03,120
is what we're beginning to see with AI

289
00:10:01,519 --> 00:10:04,399
and that's really the direction my lab

290
00:10:03,120 --> 00:10:05,920
has taken

291
00:10:04,399 --> 00:10:07,440
um pretty pretty intensely in the last

292
00:10:05,920 --> 00:10:09,839
couple years and I'm giving you sort of

293
00:10:07,440 --> 00:10:12,399
a story of how we got to this. Okay, so

294
00:10:09,839 --> 00:10:14,880
here's an example. Again, for many

295
00:10:12,399 --> 00:10:16,640
years, machine learning, AI, you ask the

296
00:10:14,880 --> 00:10:18,959
question, you get an answer. Most

297
00:10:16,640 --> 00:10:21,360
answers were wrong. And even if they're

298
00:10:18,959 --> 00:10:22,560
correct or wrong, the model had no idea,

299
00:10:21,360 --> 00:10:24,000
right? I mean, you would ask a

300
00:10:22,560 --> 00:10:26,079
regression model, make a prediction

301
00:10:24,000 --> 00:10:27,440
about this amazing new molecule, and you

302
00:10:26,079 --> 00:10:30,800
had a kind of, you know, is it really

303
00:10:27,440 --> 00:10:32,480
true? We don't know. So, um, we built a

304
00:10:30,800 --> 00:10:33,920
few years ago when GPD4 came out, and

305
00:10:32,480 --> 00:10:35,760
this was, I think, the first model that

306
00:10:33,920 --> 00:10:38,320
sort of had capabilities of of thinking

307
00:10:35,760 --> 00:10:41,279
about a problem more deeply. uh we put a

308
00:10:38,320 --> 00:10:42,880
bunch of GPD4 models together and made

309
00:10:41,279 --> 00:10:44,000
them work as a group. Right? So this was

310
00:10:42,880 --> 00:10:46,000
sort of the innovation, you know, we

311
00:10:44,000 --> 00:10:47,920
instead of having one model that you

312
00:10:46,000 --> 00:10:50,000
talk to, we had a bunch of AIs talk to

313
00:10:47,920 --> 00:10:51,600
each other. And what the model did, we

314
00:10:50,000 --> 00:10:53,839
asked the question about creating what

315
00:10:51,600 --> 00:10:56,320
we call force field, which is a kind of

316
00:10:53,839 --> 00:10:59,240
description of the relationship between

317
00:10:56,320 --> 00:11:01,680
uh geometry and energy in physical

318
00:10:59,240 --> 00:11:02,959
chemistry. And again, any other AI model

319
00:11:01,680 --> 00:11:04,320
like graph neural network would have

320
00:11:02,959 --> 00:11:06,800
just made a prediction and it would have

321
00:11:04,320 --> 00:11:08,480
probably been wrong, right? This model

322
00:11:06,800 --> 00:11:09,920
understood that it cannot make this

323
00:11:08,480 --> 00:11:12,560
prediction because it never seen this

324
00:11:09,920 --> 00:11:15,519
data. So what it did instead wrote code,

325
00:11:12,560 --> 00:11:17,839
it executed the code and it produced the

326
00:11:15,519 --> 00:11:19,200
data by running a DFT simulation which

327
00:11:17,839 --> 00:11:20,880
is the first principles quantum

328
00:11:19,200 --> 00:11:23,600
simulation. And so this pretty exciting.

329
00:11:20,880 --> 00:11:25,760
So now once it had the data from quantum

330
00:11:23,600 --> 00:11:27,519
mechanics, it did a regression problem

331
00:11:25,760 --> 00:11:29,360
and fitted this curve, right? And that's

332
00:11:27,519 --> 00:11:32,000
the beauty, you know, is that the model

333
00:11:29,360 --> 00:11:33,760
really had an ability to make its own

334
00:11:32,000 --> 00:11:34,880
data and solve the problem. And so this

335
00:11:33,760 --> 00:11:36,440
was kind of like the moment where we

336
00:11:34,880 --> 00:11:38,720
really thought this is really cool

337
00:11:36,440 --> 00:11:40,160
because you know we want to solve

338
00:11:38,720 --> 00:11:41,680
problems and design and understanding

339
00:11:40,160 --> 00:11:44,079
and now we have a tool we can actually

340
00:11:41,680 --> 00:11:46,959
build towards this end. So I'll s so to

341
00:11:44,079 --> 00:11:49,360
go through um a couple of examples um to

342
00:11:46,959 --> 00:11:51,120
to highlight this and obviously there

343
00:11:49,360 --> 00:11:53,040
you know too many I think the time is

344
00:11:51,120 --> 00:11:56,720
ticking here so I'm going to go quickly

345
00:11:53,040 --> 00:11:59,360
but so there's biology assembly you know

346
00:11:56,720 --> 00:12:00,800
collective motions emergence in ants

347
00:11:59,360 --> 00:12:02,880
they're building bridges they don't know

348
00:12:00,800 --> 00:12:06,079
about bridges they're building them um

349
00:12:02,880 --> 00:12:08,320
bees can build honeycoms and proteins

350
00:12:06,079 --> 00:12:09,920
can be built with AI now is a similar

351
00:12:08,320 --> 00:12:12,000
idea so you can kind of take a bunch of

352
00:12:09,920 --> 00:12:13,839
these individual AIs like like a chat

353
00:12:12,000 --> 00:12:15,600
GPT if you wish simplistically. It's not

354
00:12:13,839 --> 00:12:16,959
actually like this, but and you put a

355
00:12:15,600 --> 00:12:18,079
bunch of them together, not just four or

356
00:12:16,959 --> 00:12:20,000
five like in the previous example, but

357
00:12:18,079 --> 00:12:22,639
hundreds, thousands of them, and they

358
00:12:20,000 --> 00:12:24,000
work together on a problem. Um, and this

359
00:12:22,639 --> 00:12:26,160
is, of course, something we see in

360
00:12:24,000 --> 00:12:27,600
biology all the time, right? We have

361
00:12:26,160 --> 00:12:30,800
building blocks that make larger

362
00:12:27,600 --> 00:12:32,480
structures. We see this in swarming,

363
00:12:30,800 --> 00:12:34,880
right? So, not just in ants and

364
00:12:32,480 --> 00:12:36,240
honeybees, but in many other phenomena.

365
00:12:34,880 --> 00:12:37,680
And so, we're building these systems.

366
00:12:36,240 --> 00:12:39,680
We're building sort of two components

367
00:12:37,680 --> 00:12:42,160
here. One is we spend time in our lab

368
00:12:39,680 --> 00:12:43,720
building these integrated AI systems

369
00:12:42,160 --> 00:12:45,680
that can sort of think and organize

370
00:12:43,720 --> 00:12:47,920
themselves. But we also need to build

371
00:12:45,680 --> 00:12:49,440
individual models that solve specific

372
00:12:47,920 --> 00:12:50,959
tasks. Like for example, we might want

373
00:12:49,440 --> 00:12:52,720
to build an AI model that can design

374
00:12:50,959 --> 00:12:54,079
proteins. This is really an expert into

375
00:12:52,720 --> 00:12:56,399
protein design, right? Because a general

376
00:12:54,079 --> 00:12:58,639
purpose AI today at least can't quite do

377
00:12:56,399 --> 00:13:00,480
this yet. So you have to train AIs to

378
00:12:58,639 --> 00:13:03,040
become very good in specific tasks. We

379
00:13:00,480 --> 00:13:05,279
can make AIs that can make graphs that

380
00:13:03,040 --> 00:13:06,959
can understand forces and mechanics and

381
00:13:05,279 --> 00:13:09,839
chemistry. So these are special purpose

382
00:13:06,959 --> 00:13:11,120
tools which we're building and then

383
00:13:09,839 --> 00:13:12,959
we're putting them together in groups

384
00:13:11,120 --> 00:13:14,399
right so now we're building a team and

385
00:13:12,959 --> 00:13:16,560
so we kind of have okay so we have a

386
00:13:14,399 --> 00:13:19,279
bunch of different components there we

387
00:13:16,560 --> 00:13:21,440
have uh general purpose AI that can plan

388
00:13:19,279 --> 00:13:23,279
we have AIs that are specialists and we

389
00:13:21,440 --> 00:13:26,000
have even physics simulators and so

390
00:13:23,279 --> 00:13:27,680
that's really the key right so now um

391
00:13:26,000 --> 00:13:29,040
when a prediction is made it can

392
00:13:27,680 --> 00:13:30,480
actually assess the prediction by

393
00:13:29,040 --> 00:13:33,200
running maybe a molecular dynamics

394
00:13:30,480 --> 00:13:34,480
simulation or DFT and if all fails as

395
00:13:33,200 --> 00:13:36,639
you've seen in the initial example it

396
00:13:34,480 --> 00:13:38,399
can even write its own code to run its

397
00:13:36,639 --> 00:13:40,320
own simulations. So this is kind of like

398
00:13:38,399 --> 00:13:42,320
where the world lies. It's incredibly

399
00:13:40,320 --> 00:13:43,519
exciting. We can solve problems we've

400
00:13:42,320 --> 00:13:46,160
never imagined being solving like

401
00:13:43,519 --> 00:13:48,079
protein design and their physics

402
00:13:46,160 --> 00:13:50,320
verified because the model isn't just

403
00:13:48,079 --> 00:13:52,720
predicting. It assesses its prediction

404
00:13:50,320 --> 00:13:54,560
and improves the prediction. We can do

405
00:13:52,720 --> 00:13:56,480
alloy design. So I'm just going to flash

406
00:13:54,560 --> 00:13:59,120
through a bunch of examples here. Again,

407
00:13:56,480 --> 00:14:00,480
proteins, alloys. Um we can make

408
00:13:59,120 --> 00:14:03,639
scientific discoveries. We've built

409
00:14:00,480 --> 00:14:06,160
models that can actually form hypotheses

410
00:14:03,639 --> 00:14:07,760
themselves. and solve them. So instead

411
00:14:06,160 --> 00:14:09,760
of just starting with something you come

412
00:14:07,760 --> 00:14:12,079
up with a problem, the model can

413
00:14:09,760 --> 00:14:14,000
actually come up with its own ideas or

414
00:14:12,079 --> 00:14:15,040
what it might want to study. And we can

415
00:14:14,000 --> 00:14:16,480
do this through what we call graph

416
00:14:15,040 --> 00:14:19,120
reasoning. So we can build very large

417
00:14:16,480 --> 00:14:21,680
representations of knowledge from papers

418
00:14:19,120 --> 00:14:24,000
and books and build graphs that describe

419
00:14:21,680 --> 00:14:26,320
the ontologies of how knowledge is

420
00:14:24,000 --> 00:14:28,639
organized essentially. And this can be

421
00:14:26,320 --> 00:14:30,639
fed again into a solution uh agent as

422
00:14:28,639 --> 00:14:32,079
well. And so now we can do both

423
00:14:30,639 --> 00:14:33,360
formulate the problem and solve the

424
00:14:32,079 --> 00:14:35,199
problem. So this is sort of the how the

425
00:14:33,360 --> 00:14:36,720
future looks like. It's very exciting.

426
00:14:35,199 --> 00:14:38,720
Um, this is the swarm example of

427
00:14:36,720 --> 00:14:41,600
designing proteins using hundreds of AI

428
00:14:38,720 --> 00:14:43,600
agents that each works together on a

429
00:14:41,600 --> 00:14:45,279
particular part of the protein. They

430
00:14:43,600 --> 00:14:47,120
don't actually know about each other.

431
00:14:45,279 --> 00:14:48,800
So, we're formulating this as a problem

432
00:14:47,120 --> 00:14:51,480
from reinforcement learning where we're

433
00:14:48,800 --> 00:14:54,240
basically just telling the models each

434
00:14:51,480 --> 00:14:56,240
agent what its actions does for the

435
00:14:54,240 --> 00:14:58,880
objective. And the model figures out on

436
00:14:56,240 --> 00:15:00,480
its own each model how it can best act

437
00:14:58,880 --> 00:15:03,360
to make a better outcome. So, this is

438
00:15:00,480 --> 00:15:06,079
really cool, right? So um kind of kind

439
00:15:03,360 --> 00:15:08,000
of really really fun to think about and

440
00:15:06,079 --> 00:15:09,279
you know this all again is sort of in

441
00:15:08,000 --> 00:15:10,959
this picture here right so we're looking

442
00:15:09,279 --> 00:15:12,959
at relational modeling we're looking at

443
00:15:10,959 --> 00:15:15,360
models they can think we're looking at

444
00:15:12,959 --> 00:15:17,920
isomorphic mapping so we're forcing

445
00:15:15,360 --> 00:15:21,959
models to kind of reflect their own

446
00:15:17,920 --> 00:15:25,120
strategies into these shared spaces in

447
00:15:21,959 --> 00:15:26,560
which knowledge can be abstracted from

448
00:15:25,120 --> 00:15:27,680
which we can then make much more general

449
00:15:26,560 --> 00:15:30,399
solutions. And we're doing this

450
00:15:27,680 --> 00:15:34,320
essentially by forcing models to put

451
00:15:30,399 --> 00:15:36,240
their thoughts into graphs and symbols.

452
00:15:34,320 --> 00:15:37,920
And so those of you who've been around

453
00:15:36,240 --> 00:15:40,000
for a long time, you know that this was

454
00:15:37,920 --> 00:15:42,800
one of the early ideas in the 1960s

455
00:15:40,000 --> 00:15:45,440
actually. Symbolic AI again at that time

456
00:15:42,800 --> 00:15:48,480
the symbolic AI was pre-programmed by

457
00:15:45,440 --> 00:15:51,040
humans. Here the AI learns symbolic

458
00:15:48,480 --> 00:15:53,360
abstractions by itself. And it actually

459
00:15:51,040 --> 00:15:54,800
is is sort of does this using what we

460
00:15:53,360 --> 00:15:57,360
call reinforcement learning. So instead

461
00:15:54,800 --> 00:15:59,199
of teaching the AI the outcome, we're

462
00:15:57,360 --> 00:16:00,800
actually letting the AI figure out the

463
00:15:59,199 --> 00:16:03,120
right abstractions to get the best

464
00:16:00,800 --> 00:16:04,880
possible answer. And we can let the AI

465
00:16:03,120 --> 00:16:07,199
kind of do this for many, many days.

466
00:16:04,880 --> 00:16:10,560
Here's an example where the AI has been

467
00:16:07,199 --> 00:16:13,680
thinking for five or seven days. And it

468
00:16:10,560 --> 00:16:16,639
can create a very deep reflection on the

469
00:16:13,680 --> 00:16:18,720
world. So again, this is an AI in this

470
00:16:16,639 --> 00:16:20,560
case talking to another AI where the

471
00:16:18,720 --> 00:16:22,480
reasoning steps are used as a way to

472
00:16:20,560 --> 00:16:24,000
critique itself, right? And so the AI

473
00:16:22,480 --> 00:16:25,839
can kind of bootstrap itself. So this is

474
00:16:24,000 --> 00:16:27,279
really kind of where the field is going

475
00:16:25,839 --> 00:16:30,480
and you can really teach models of

476
00:16:27,279 --> 00:16:32,320
course to um you know become even better

477
00:16:30,480 --> 00:16:34,079
at innovating even better at making

478
00:16:32,320 --> 00:16:36,320
connections and here's sort of how this

479
00:16:34,079 --> 00:16:38,800
graph looks like. So this is how the the

480
00:16:36,320 --> 00:16:40,639
world model built by the AI itself by

481
00:16:38,800 --> 00:16:41,920
basically asking itself questions and

482
00:16:40,639 --> 00:16:43,600
reflecting on it and building more

483
00:16:41,920 --> 00:16:47,600
abstractions over and over for many many

484
00:16:43,600 --> 00:16:50,000
many days and um at the end you know we

485
00:16:47,600 --> 00:16:52,639
create we see these graphs which

486
00:16:50,000 --> 00:16:55,759
actually and it's surprising uh never

487
00:16:52,639 --> 00:16:58,399
end never saturate so the AI never runs

488
00:16:55,759 --> 00:17:00,560
out of ideas in fact there's always a

489
00:16:58,399 --> 00:17:02,480
reservoir of semantic entropy meaning

490
00:17:00,560 --> 00:17:04,319
capacity to discover new relationships

491
00:17:02,480 --> 00:17:06,000
which are then realized

492
00:17:04,319 --> 00:17:08,720
through structural connections. As the

493
00:17:06,000 --> 00:17:10,400
AI sees these opportunities, it

494
00:17:08,720 --> 00:17:11,760
logically draws connections between

495
00:17:10,400 --> 00:17:13,600
these ideas. And so this is sort of how

496
00:17:11,760 --> 00:17:15,120
innovation works as well. And also sand

497
00:17:13,600 --> 00:17:18,000
piles of course and criticality in

498
00:17:15,120 --> 00:17:19,280
physics is a related concept here. Um so

499
00:17:18,000 --> 00:17:20,799
this slide actually I think you saw it

500
00:17:19,280 --> 00:17:22,559
in the previous presentation. So

501
00:17:20,799 --> 00:17:24,799
underlying all of this of course is how

502
00:17:22,559 --> 00:17:28,400
long can AIs work. So they can work for

503
00:17:24,799 --> 00:17:30,720
days some but generally AI systems in

504
00:17:28,400 --> 00:17:32,720
agendic frameworks can work for about an

505
00:17:30,720 --> 00:17:34,240
hour or two reliably. And you can see

506
00:17:32,720 --> 00:17:35,600
this here and this is a scaling law. I

507
00:17:34,240 --> 00:17:38,720
think it's going to be really really

508
00:17:35,600 --> 00:17:40,960
important for this field as these AIs

509
00:17:38,720 --> 00:17:44,080
can work for longer not just for an hour

510
00:17:40,960 --> 00:17:45,200
day you know two three days 5 days maybe

511
00:17:44,080 --> 00:17:48,000
um they're going to become very very

512
00:17:45,200 --> 00:17:49,679
powerful. Um how do we do this? We built

513
00:17:48,000 --> 00:17:51,799
lots of technology to make this happen.

514
00:17:49,679 --> 00:17:53,760
We built models. We built inference

515
00:17:51,799 --> 00:17:55,600
engines. Everything open source.

516
00:17:53,760 --> 00:17:57,520
Everything at MIT we do is open source.

517
00:17:55,600 --> 00:18:00,000
So you can access this on GitHub and and

518
00:17:57,520 --> 00:18:01,840
hugging face. Um, we've built inference

519
00:18:00,000 --> 00:18:04,080
engines for very large models that we

520
00:18:01,840 --> 00:18:06,640
need to serve these large AI systems, of

521
00:18:04,080 --> 00:18:08,640
course, on a local computer. Um, we need

522
00:18:06,640 --> 00:18:09,840
access to data. One of the things we can

523
00:18:08,640 --> 00:18:12,240
do at MIT, and you might have heard

524
00:18:09,840 --> 00:18:13,760
about MIT Nano, um, is an amazing

525
00:18:12,240 --> 00:18:15,600
facility where we can actually automate

526
00:18:13,760 --> 00:18:17,039
some of those processes in the future.

527
00:18:15,600 --> 00:18:20,240
So, this is an exciting opportunity here

528
00:18:17,039 --> 00:18:21,840
at MIT. You need of course have an idea.

529
00:18:20,240 --> 00:18:23,360
Of course, you can do a simulation, but

530
00:18:21,840 --> 00:18:24,799
sometimes you need to go to the lab at

531
00:18:23,360 --> 00:18:26,400
the end, right? And test some of the,

532
00:18:24,799 --> 00:18:28,160
you know, collect new data. So this is

533
00:18:26,400 --> 00:18:29,840
really how we think we think the web

534
00:18:28,160 --> 00:18:32,640
look like. It's going to change the

535
00:18:29,840 --> 00:18:34,799
world. I think you know we um we're not

536
00:18:32,640 --> 00:18:36,400
using these AI systems in this way today

537
00:18:34,799 --> 00:18:39,600
obviously. So this is I think a study

538
00:18:36,400 --> 00:18:42,000
entropic did um mostly done for simple

539
00:18:39,600 --> 00:18:44,160
office tasks today. Still makes our

540
00:18:42,000 --> 00:18:48,160
lives easier but there's a lot more room

541
00:18:44,160 --> 00:18:50,080
of course to grow. Um the expertise you

542
00:18:48,160 --> 00:18:52,720
need of your staff is going to change

543
00:18:50,080 --> 00:18:54,160
right? So, you know, in the future, um,

544
00:18:52,720 --> 00:18:56,160
you know, the role of an engineer is

545
00:18:54,160 --> 00:19:00,160
going to change. And I like to remind

546
00:18:56,160 --> 00:19:02,320
us, if you think back, um, sort of in

547
00:19:00,160 --> 00:19:04,559
the 1980s and 90s, so I grew up in the

548
00:19:02,320 --> 00:19:06,240
'9s with computers, uh, you know, but if

549
00:19:04,559 --> 00:19:08,080
you look back to the 80s when PCs came

550
00:19:06,240 --> 00:19:10,320
out, and I've been watching a lot of

551
00:19:08,080 --> 00:19:11,840
these, um, sort of historical accounts

552
00:19:10,320 --> 00:19:13,520
at that time. If you listen, for

553
00:19:11,840 --> 00:19:15,120
example, uh, computer chronicles, I

554
00:19:13,520 --> 00:19:16,720
don't know how many of you know that. I

555
00:19:15,120 --> 00:19:18,240
did not know it obviously, but I came

556
00:19:16,720 --> 00:19:20,160
across it. I was too young. I was like

557
00:19:18,240 --> 00:19:21,760
six years old at the time. You can you

558
00:19:20,160 --> 00:19:25,679
can watch it on YouTube. And they

559
00:19:21,760 --> 00:19:27,919
discuss PCs like C64, Apple, you know,

560
00:19:25,679 --> 00:19:29,280
MS DOS, Windows, and they were kind of

561
00:19:27,919 --> 00:19:31,280
wondering what can these things do?

562
00:19:29,280 --> 00:19:32,640
They're very brittle. And they were

563
00:19:31,280 --> 00:19:34,080
advertising them kind of like they had

564
00:19:32,640 --> 00:19:37,200
the guy from Microsoft come in and show

565
00:19:34,080 --> 00:19:39,039
the new machine. And it seemed so um

566
00:19:37,200 --> 00:19:40,880
pedestrian if you look back at this

567
00:19:39,039 --> 00:19:42,880
time, but fascinating. And I think we're

568
00:19:40,880 --> 00:19:44,480
in the same point at AI today. These

569
00:19:42,880 --> 00:19:46,480
things are very brittle. Lots of

570
00:19:44,480 --> 00:19:47,919
questions. Um and we kind of look sort

571
00:19:46,480 --> 00:19:49,679
of ahead a couple years and we're going

572
00:19:47,919 --> 00:19:50,720
to see some really amazing developments.

573
00:19:49,679 --> 00:19:52,240
Um if you're interested in learning

574
00:19:50,720 --> 00:19:54,320
more, I mean I teach classes in

575
00:19:52,240 --> 00:19:57,520
professional education, one in June here

576
00:19:54,320 --> 00:19:59,280
at MIT on campus and one in July live

577
00:19:57,520 --> 00:20:00,880
virtual. So some of you or your staff

578
00:19:59,280 --> 00:20:02,240
want to come. This is obviously much

579
00:20:00,880 --> 00:20:05,600
more in-depth discussion than what I can

580
00:20:02,240 --> 00:20:07,039
do in 20 minutes here. Um yes but so

581
00:20:05,600 --> 00:20:08,559
with that yeah thank you so much. I mean

582
00:20:07,039 --> 00:20:10,000
this you know I like to think about this

583
00:20:08,559 --> 00:20:12,240
as thinking machines. I've been doing

584
00:20:10,000 --> 00:20:13,600
modeling simulation my entire life. In

585
00:20:12,240 --> 00:20:15,039
fact, actually almost my entire life

586
00:20:13,600 --> 00:20:17,360
because when as a kid I was programming

587
00:20:15,039 --> 00:20:19,440
computers to simulate the world and and

588
00:20:17,360 --> 00:20:20,720
it's sort of to me a dream come true

589
00:20:19,440 --> 00:20:22,240
that I have computers can actually

590
00:20:20,720 --> 00:20:24,640
think. They can really think and build

591
00:20:22,240 --> 00:20:26,880
their own models um write their own

592
00:20:24,640 --> 00:20:28,159
code, collect their own data and um

593
00:20:26,880 --> 00:20:29,440
yeah, we're using it for some really

594
00:20:28,159 --> 00:20:31,520
cool applications. I didn't talk about

595
00:20:29,440 --> 00:20:32,799
all the fancy stuff we can do in the

596
00:20:31,520 --> 00:20:35,120
application side. I wanted to kind of

597
00:20:32,799 --> 00:20:36,480
convey some of the technology uh and

598
00:20:35,120 --> 00:20:38,400
possibilities, but I'm happy to talk

599
00:20:36,480 --> 00:20:40,320
about any of the applications with IoP.

600
00:20:38,400 --> 00:20:41,760
I talked to a lot of companies and many

601
00:20:40,320 --> 00:20:42,960
others around this. So if you're

602
00:20:41,760 --> 00:20:44,320
interested in learning about specific

603
00:20:42,960 --> 00:20:45,919
applications and materials and what we

604
00:20:44,320 --> 00:20:48,919
can do with it, happy to discuss. Thank

605
00:20:45,919 --> 00:20:48,919
you.

606
00:20:52,480 --> 00:20:57,679
So you can read your question first.

607
00:20:55,919 --> 00:20:59,280
So do I have to read question? So how

608
00:20:57,679 --> 00:21:02,679
difficult is it to implement high level

609
00:20:59,280 --> 00:21:04,720
material characteristics like conduct

610
00:21:02,679 --> 00:21:06,480
conductivities on all mechanical

611
00:21:04,720 --> 00:21:10,559
properties so they can be predicted by

612
00:21:06,480 --> 00:21:11,720
model? Well, so you know, you you need

613
00:21:10,559 --> 00:21:14,400
um some

614
00:21:11,720 --> 00:21:16,960
observations and if you were to go, you

615
00:21:14,400 --> 00:21:19,520
know, with an AI model that has no idea

616
00:21:16,960 --> 00:21:22,080
about any, you know, any kind of

617
00:21:19,520 --> 00:21:23,679
mechanical properties, you traditionally

618
00:21:22,080 --> 00:21:25,600
do like a regression model, you need a

619
00:21:23,679 --> 00:21:27,360
whole ton of data. Now, if you're

620
00:21:25,600 --> 00:21:29,120
starting with these agentic AIs, they

621
00:21:27,360 --> 00:21:30,799
have some understanding usually about

622
00:21:29,120 --> 00:21:32,080
the problem at hand. So, they know

623
00:21:30,799 --> 00:21:35,919
something about mechanics, they know

624
00:21:32,080 --> 00:21:38,159
something about uh connectivity. So the

625
00:21:35,919 --> 00:21:40,000
way we're approaching this is by using

626
00:21:38,159 --> 00:21:42,000
models that we call pre-trained. They

627
00:21:40,000 --> 00:21:43,480
understand something about the world and

628
00:21:42,000 --> 00:21:45,919
then they learn. They can learn from

629
00:21:43,480 --> 00:21:48,520
papers, sub literature. They can learn

630
00:21:45,919 --> 00:21:51,120
from simulations. They can learn from

631
00:21:48,520 --> 00:21:54,640
experimentation. And often times we need

632
00:21:51,120 --> 00:21:56,840
very little data and or because these

633
00:21:54,640 --> 00:21:59,679
models are trained using reinforcement

634
00:21:56,840 --> 00:22:01,360
processes, we can also teach them to

635
00:21:59,679 --> 00:22:03,120
make better predictions by letting them

636
00:22:01,360 --> 00:22:04,320
basically fail. And that's the point,

637
00:22:03,120 --> 00:22:06,720
right? We want the models to make

638
00:22:04,320 --> 00:22:08,320
prediction, do reasoning and make a

639
00:22:06,720 --> 00:22:09,919
mistake and then the models will have to

640
00:22:08,320 --> 00:22:11,840
figure out a pathway to make better

641
00:22:09,919 --> 00:22:12,960
predictions. So all of this is and I

642
00:22:11,840 --> 00:22:15,760
actually I think I went through this

643
00:22:12,960 --> 00:22:18,400
very quickly but is that we are not

644
00:22:15,760 --> 00:22:21,600
we're really trying very hard to make

645
00:22:18,400 --> 00:22:23,360
models not remember the answer. We're

646
00:22:21,600 --> 00:22:25,840
we're forcing we're encouraging the

647
00:22:23,360 --> 00:22:28,400
model very strongly to understand and

648
00:22:25,840 --> 00:22:29,679
learn the path to get the answer. Right?

649
00:22:28,400 --> 00:22:31,919
We're doing this through symbolic

650
00:22:29,679 --> 00:22:34,080
abstraction. So models in the reasoning

651
00:22:31,919 --> 00:22:35,840
steps are not allowed to just remember

652
00:22:34,080 --> 00:22:37,840
the answer. They can if they really if

653
00:22:35,840 --> 00:22:39,200
it's trivial, but we're making it really

654
00:22:37,840 --> 00:22:42,480
hard for them. So we're penalizing

655
00:22:39,200 --> 00:22:45,360
penalizing them for this. And so um you

656
00:22:42,480 --> 00:22:47,120
know in a way yeah it's like you you

657
00:22:45,360 --> 00:22:48,320
want to find these hidden laws these

658
00:22:47,120 --> 00:22:50,080
hidden Newton's laws doesn't have to be

659
00:22:48,320 --> 00:22:52,400
quite at the level of Newton's laws but

660
00:22:50,080 --> 00:22:54,559
you know these these relationships that

661
00:22:52,400 --> 00:22:56,159
are foundational to describing your

662
00:22:54,559 --> 00:22:57,840
properties and so you can get away with

663
00:22:56,159 --> 00:22:59,679
reasonably little data. So that's a real

664
00:22:57,840 --> 00:23:02,159
game changer. So to traditional machine

665
00:22:59,679 --> 00:23:05,039
learning of course. Yeah. Um another

666
00:23:02,159 --> 00:23:08,240
question um how would you validate?

667
00:23:05,039 --> 00:23:10,080
Yeah. So you know you validate it in in

668
00:23:08,240 --> 00:23:12,320
you can give different reward functions

669
00:23:10,080 --> 00:23:14,880
right. So one could be let's say you're

670
00:23:12,320 --> 00:23:16,880
designing a protein. I mean it could be

671
00:23:14,880 --> 00:23:19,919
the patient response right and that

672
00:23:16,880 --> 00:23:21,600
could be your your final outcome. But

673
00:23:19,919 --> 00:23:24,720
usually you have intermediate steps like

674
00:23:21,600 --> 00:23:26,480
for example you want activity you want

675
00:23:24,720 --> 00:23:28,360
stability and you want to get something

676
00:23:26,480 --> 00:23:30,960
like you want to get proteins that are

677
00:23:28,360 --> 00:23:32,880
stable and that are active and

678
00:23:30,960 --> 00:23:34,720
ultimately the reward will be does it

679
00:23:32,880 --> 00:23:35,919
actually work in a patient. So I would

680
00:23:34,720 --> 00:23:37,679
say you know there's different levels

681
00:23:35,919 --> 00:23:39,280
and the beauty of these agentic models

682
00:23:37,679 --> 00:23:41,200
is exactly that they can do this. You're

683
00:23:39,280 --> 00:23:43,600
not limited to one thing. You're not

684
00:23:41,200 --> 00:23:47,120
training models to be have one outcome

685
00:23:43,600 --> 00:23:49,880
that you know in fact um you know what

686
00:23:47,120 --> 00:23:52,400
we find in in these agendic systems is

687
00:23:49,880 --> 00:23:55,760
that they will define their own

688
00:23:52,400 --> 00:23:58,320
intermediate steps. So they will say I

689
00:23:55,760 --> 00:24:00,799
have to design a protein. Okay, I I

690
00:23:58,320 --> 00:24:02,480
don't know the answer to this problem.

691
00:24:00,799 --> 00:24:04,320
Let me find a path to get to that

692
00:24:02,480 --> 00:24:06,240
answer. And they will actually define

693
00:24:04,320 --> 00:24:08,799
their own merit. So they might make a

694
00:24:06,240 --> 00:24:11,520
plot and they might analyze the plot. I

695
00:24:08,799 --> 00:24:14,080
didn't really get to this either, but um

696
00:24:11,520 --> 00:24:16,720
you know the sort of the the design of

697
00:24:14,080 --> 00:24:18,159
what validation looks like could be part

698
00:24:16,720 --> 00:24:21,039
of what the agents can do on their own.

699
00:24:18,159 --> 00:24:23,279
Now that being said, you're not out of a

700
00:24:21,039 --> 00:24:24,320
job yet. No, I'm kidding. Um especially

701
00:24:23,279 --> 00:24:26,720
if you're here, you're in the right

702
00:24:24,320 --> 00:24:28,799
place. And but you you we we can

703
00:24:26,720 --> 00:24:30,799
supervise this process, of course. And

704
00:24:28,799 --> 00:24:32,559
you know, there are there are you know

705
00:24:30,799 --> 00:24:35,039
human interventions we can have. You

706
00:24:32,559 --> 00:24:36,559
know, we can say, okay, we um here's an

707
00:24:35,039 --> 00:24:38,559
intermediate result. How does it look

708
00:24:36,559 --> 00:24:41,760
like? And we do this all the time. And

709
00:24:38,559 --> 00:24:43,039
then you say, okay, I I like it, but um

710
00:24:41,760 --> 00:24:44,320
you know, I don't like this. I I like

711
00:24:43,039 --> 00:24:46,400
the general direction, but I don't like

712
00:24:44,320 --> 00:24:48,559
this chemistry because it's actually

713
00:24:46,400 --> 00:24:50,080
toxic, right? And so we can give this

714
00:24:48,559 --> 00:24:51,440
feedback to the model, of course. So you

715
00:24:50,080 --> 00:24:55,120
are one of the team members, if you wish

716
00:24:51,440 --> 00:24:57,120
in the agendic AI system and um we're

717
00:24:55,120 --> 00:24:59,760
building these in of course um because

718
00:24:57,120 --> 00:25:01,520
there's a lot of human domain expertise

719
00:24:59,760 --> 00:25:03,919
of course that we don't want to um you

720
00:25:01,520 --> 00:25:05,520
know uh get get rid of. Yeah. Thank you

721
00:25:03,919 --> 00:25:06,470
very much. Thank you so much. Thank you

722
00:25:05,520 --> 00:25:12,069
so much.

723
00:25:06,470 --> 00:25:12,069
[Applause]

