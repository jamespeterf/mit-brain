1
00:00:04,560 --> 00:00:12,480
the future of work. That is the topic

2
00:00:09,040 --> 00:00:15,679
for the second part of this seminar.

3
00:00:12,480 --> 00:00:18,800
Before introducing you to our next

4
00:00:15,679 --> 00:00:21,199
speaker that comes from MIT, let me tell

5
00:00:18,800 --> 00:00:24,400
you that his research is focused on how

6
00:00:21,199 --> 00:00:28,240
workers, companies, and regions are

7
00:00:24,400 --> 00:00:30,640
adapting to technological change.

8
00:00:28,240 --> 00:00:35,040
what happens with change or with the

9
00:00:30,640 --> 00:00:38,879
capacity to adapt to it to changes. He's

10
00:00:35,040 --> 00:00:42,879
currently leading a work group on

11
00:00:38,879 --> 00:00:46,399
generative AI. He's also authoring a

12
00:00:42,879 --> 00:00:49,120
work on competitiveness of US

13
00:00:46,399 --> 00:00:52,239
manufacturing. Part of his work has been

14
00:00:49,120 --> 00:00:54,239
developed in Google INC and today he

15
00:00:52,239 --> 00:00:56,160
co-directs the initiative work of the

16
00:00:54,239 --> 00:00:59,600
future in the industrial performance

17
00:00:56,160 --> 00:01:05,199
center of MIT.

18
00:00:59,600 --> 00:01:09,560
Ben Armstrong, executive director of MIT

19
00:01:05,199 --> 00:01:09,560
industrial performance center.

20
00:01:25,840 --> 00:01:32,880
scholars of England and they became very

21
00:01:30,159 --> 00:01:35,759
interested in

22
00:01:32,880 --> 00:01:38,079
artificial intelligence and robots on

23
00:01:35,759 --> 00:01:40,799
the future of the workforce. So what

24
00:01:38,079 --> 00:01:42,720
they did was they went to some of the

25
00:01:40,799 --> 00:01:44,560
leading experts in machine learning and

26
00:01:42,720 --> 00:01:46,640
artificial intelligence and they

27
00:01:44,560 --> 00:01:49,840
presented them with a long list and this

28
00:01:46,640 --> 00:01:52,479
was a list of jobs. Jobs like home

29
00:01:49,840 --> 00:01:54,320
healthc care aid or doctor and lawyer

30
00:01:52,479 --> 00:01:56,799
and then they had a list of skills and

31
00:01:54,320 --> 00:01:59,439
tasks associated with each job. And what

32
00:01:56,799 --> 00:02:01,439
these two scholars from Oxford asked was

33
00:01:59,439 --> 00:02:04,799
for could these machine learning experts

34
00:02:01,439 --> 00:02:07,759
predict which jobs would be automated by

35
00:02:04,799 --> 00:02:10,319
2030? which jobs would still be here in

36
00:02:07,759 --> 00:02:12,879
just a few decades and which would not.

37
00:02:10,319 --> 00:02:15,440
And after collecting all this data and

38
00:02:12,879 --> 00:02:17,680
listening to the survey, what they found

39
00:02:15,440 --> 00:02:20,319
was these machine learning experts

40
00:02:17,680 --> 00:02:22,560
thought that about half of all jobs

41
00:02:20,319 --> 00:02:24,879
would be automated by 2030. So they

42
00:02:22,560 --> 00:02:26,160
wrote this up in a paper. The paper

43
00:02:24,879 --> 00:02:29,120
estimated with these you know

44
00:02:26,160 --> 00:02:31,200
complicated methodologies about 47% of

45
00:02:29,120 --> 00:02:33,599
the labor force in North America and

46
00:02:31,200 --> 00:02:35,519
Europe would be vulnerable to automation

47
00:02:33,599 --> 00:02:38,720
in the next two decades. And as you can

48
00:02:35,519 --> 00:02:41,120
imagine, this study released a wave of

49
00:02:38,720 --> 00:02:43,519
anxiety among people. Newspapers,

50
00:02:41,120 --> 00:02:46,160
magazines picked it up and talked about

51
00:02:43,519 --> 00:02:48,720
a robot apocalypse where all these jobs

52
00:02:46,160 --> 00:02:50,400
would be destroyed. And of course, in in

53
00:02:48,720 --> 00:02:52,480
retrospect, you know, we're pretty close

54
00:02:50,400 --> 00:02:54,319
to 2030 by now. And a few months ago,

55
00:02:52,480 --> 00:02:56,480
there was a paper that looked back at

56
00:02:54,319 --> 00:02:58,800
this study and evaluated all the jobs

57
00:02:56,480 --> 00:03:00,959
that this uh these researchers said

58
00:02:58,800 --> 00:03:03,599
would be automated. And what they found

59
00:03:00,959 --> 00:03:05,599
was rather than being destroyed, many of

60
00:03:03,599 --> 00:03:07,440
these jobs that machine learning experts

61
00:03:05,599 --> 00:03:10,159
thought were would be fully automated

62
00:03:07,440 --> 00:03:12,480
had actually grown in number since this

63
00:03:10,159 --> 00:03:15,519
study was released. So I think one of

64
00:03:12,480 --> 00:03:17,760
the big lessons of this study, but also

65
00:03:15,519 --> 00:03:20,400
predictions of the future and the impact

66
00:03:17,760 --> 00:03:22,159
of technologies like AI more broadly is

67
00:03:20,400 --> 00:03:23,840
don't listen to researchers when they

68
00:03:22,159 --> 00:03:25,120
try to predict the future. Don't listen

69
00:03:23,840 --> 00:03:27,200
to people like me. So what I'm going to

70
00:03:25,120 --> 00:03:29,519
do today isn't to try to predict the

71
00:03:27,200 --> 00:03:31,920
future of work uh in any way, but

72
00:03:29,519 --> 00:03:34,640
instead try to draw on lessons from the

73
00:03:31,920 --> 00:03:37,120
present data that we have today and also

74
00:03:34,640 --> 00:03:39,440
lessons of the past to try to understand

75
00:03:37,120 --> 00:03:41,040
how we can prepare for an uncertain

76
00:03:39,440 --> 00:03:44,480
future, one that we really can't

77
00:03:41,040 --> 00:03:46,400
predict. And I want to start with an

78
00:03:44,480 --> 00:03:47,760
image really from the present and an

79
00:03:46,400 --> 00:03:50,720
image that many of you are probably

80
00:03:47,760 --> 00:03:53,040
familiar with. This is a a control room,

81
00:03:50,720 --> 00:03:55,120
a supervisory control environment, a

82
00:03:53,040 --> 00:03:58,159
really an automated environment where a

83
00:03:55,120 --> 00:04:00,560
human worker is operating uh and

84
00:03:58,159 --> 00:04:02,319
overseeing a mine in a very different

85
00:04:00,560 --> 00:04:04,799
way than they might have a generation

86
00:04:02,319 --> 00:04:06,879
ago. And one of the big questions I

87
00:04:04,799 --> 00:04:09,599
think for for the mining industry is

88
00:04:06,879 --> 00:04:12,080
whether the work in 20 years looks a lot

89
00:04:09,599 --> 00:04:14,480
more like this or if it looks something

90
00:04:12,080 --> 00:04:16,639
like this and how what's going to be the

91
00:04:14,480 --> 00:04:19,199
balance between these different types of

92
00:04:16,639 --> 00:04:22,240
jobs. And I think there's maybe an

93
00:04:19,199 --> 00:04:24,560
assumption that almost all operator jobs

94
00:04:22,240 --> 00:04:27,360
in the future might look like this. That

95
00:04:24,560 --> 00:04:30,080
there's this unstoppable path toward

96
00:04:27,360 --> 00:04:32,080
automation and a more control room type

97
00:04:30,080 --> 00:04:34,479
environment. And what I'm here to argue

98
00:04:32,080 --> 00:04:37,280
today is that the evidence does not

99
00:04:34,479 --> 00:04:39,759
favor this scenario over the long term.

100
00:04:37,280 --> 00:04:41,680
It doesn't favor a completely automated

101
00:04:39,759 --> 00:04:44,560
environment in mining or in other

102
00:04:41,680 --> 00:04:46,880
industries like manufacturing. Instead,

103
00:04:44,560 --> 00:04:48,240
what the future, you know, could be, and

104
00:04:46,880 --> 00:04:51,120
I think what it's likely to be based on

105
00:04:48,240 --> 00:04:53,199
past evidence is a combination. It's

106
00:04:51,120 --> 00:04:54,320
going to be part supervisory control. I

107
00:04:53,199 --> 00:04:57,040
think there will be a lot more control

108
00:04:54,320 --> 00:04:59,600
room jobs. It'll be part manual jobs,

109
00:04:57,040 --> 00:05:02,000
jobs that are require someone close to a

110
00:04:59,600 --> 00:05:04,560
process, and then some jobs that are a

111
00:05:02,000 --> 00:05:07,199
hybrid of the two that require a really

112
00:05:04,560 --> 00:05:09,600
close connection between humans and

113
00:05:07,199 --> 00:05:12,240
machines, between oversight and in the

114
00:05:09,600 --> 00:05:14,479
field experience. And I think the

115
00:05:12,240 --> 00:05:16,880
challenge then for for the companies

116
00:05:14,479 --> 00:05:18,960
here today isn't necessarily to drive as

117
00:05:16,880 --> 00:05:20,960
fast as possible toward automation

118
00:05:18,960 --> 00:05:22,639
toward making that control room type

119
00:05:20,960 --> 00:05:25,280
environment more and more present. The

120
00:05:22,639 --> 00:05:27,600
challenge instead is to understand in

121
00:05:25,280 --> 00:05:29,759
what context automation really makes

122
00:05:27,600 --> 00:05:31,840
sense and can be very valuable and in

123
00:05:29,759 --> 00:05:33,520
what context you need someone really

124
00:05:31,840 --> 00:05:35,199
close to the field and that knowledge is

125
00:05:33,520 --> 00:05:37,039
essential. So I'm going to try to talk

126
00:05:35,199 --> 00:05:38,960
you through some examples, many of which

127
00:05:37,039 --> 00:05:40,800
are from other industries, of how you

128
00:05:38,960 --> 00:05:43,120
can make that judgment call of when

129
00:05:40,800 --> 00:05:45,280
technology really has value and when it

130
00:05:43,120 --> 00:05:48,400
doesn't. Okay. So I want to get started

131
00:05:45,280 --> 00:05:50,880
with three real puzzles. And I call them

132
00:05:48,400 --> 00:05:53,440
puzzles because for a long time the

133
00:05:50,880 --> 00:05:55,440
assumption was in research that more

134
00:05:53,440 --> 00:05:57,520
technology is better. That the more

135
00:05:55,440 --> 00:05:59,759
technology you give to workers, the more

136
00:05:57,520 --> 00:06:01,440
productive they'll be. and technology

137
00:05:59,759 --> 00:06:03,759
adoption was going to be really a

138
00:06:01,440 --> 00:06:05,440
frictionless path toward higher

139
00:06:03,759 --> 00:06:07,120
performance. And what we've learned

140
00:06:05,440 --> 00:06:08,880
really over the last 40 years is that

141
00:06:07,120 --> 00:06:10,400
this isn't the case. It's the case in

142
00:06:08,880 --> 00:06:11,440
some scenarios but not in others. And I

143
00:06:10,400 --> 00:06:13,919
want to share with you a little bit

144
00:06:11,440 --> 00:06:17,039
about why that is. And and the first is

145
00:06:13,919 --> 00:06:19,520
really that what we found despite the

146
00:06:17,039 --> 00:06:22,080
anxiety and and concern about automation

147
00:06:19,520 --> 00:06:23,919
destroying jobs is that in many cases

148
00:06:22,080 --> 00:06:26,000
when firms integrate automation

149
00:06:23,919 --> 00:06:27,759
successfully, they do become more

150
00:06:26,000 --> 00:06:30,960
productive and they become more

151
00:06:27,759 --> 00:06:32,639
profitable at the firm level. So in

152
00:06:30,960 --> 00:06:34,639
doing that, they end up growing and they

153
00:06:32,639 --> 00:06:36,319
hire more people, not fewer. And that's

154
00:06:34,639 --> 00:06:39,919
what some of the revisions and and

155
00:06:36,319 --> 00:06:41,840
reactions to this 2017 paper show. But

156
00:06:39,919 --> 00:06:43,680
given this evidence, you would expect

157
00:06:41,840 --> 00:06:45,199
because the adoption of robots and

158
00:06:43,680 --> 00:06:47,199
technologies like AI and being

159
00:06:45,199 --> 00:06:48,880
profitable, you'd expect that every

160
00:06:47,199 --> 00:06:50,960
company would be adopting these

161
00:06:48,880 --> 00:06:52,240
technologies in all sorts of ways. But

162
00:06:50,960 --> 00:06:54,800
that's not the case. Actually,

163
00:06:52,240 --> 00:06:57,199
automation adoption has been quite

164
00:06:54,800 --> 00:06:59,520
stagnant um over the last 10 years

165
00:06:57,199 --> 00:07:01,280
across a variety of environments. So

166
00:06:59,520 --> 00:07:03,039
here is just a plot. This is of

167
00:07:01,280 --> 00:07:04,639
industrial robots. I'll show that this

168
00:07:03,039 --> 00:07:06,720
isn't just the case for robots, it's for

169
00:07:04,639 --> 00:07:08,479
other technologies as well. that in a

170
00:07:06,720 --> 00:07:11,280
number of advanced industrial c

171
00:07:08,479 --> 00:07:13,039
countries since 2015

172
00:07:11,280 --> 00:07:15,039
the adoption of these technologies has

173
00:07:13,039 --> 00:07:17,440
been flat or declining. The one

174
00:07:15,039 --> 00:07:20,160
exceptional case is China and that's

175
00:07:17,440 --> 00:07:21,280
aided u mostly by government heavy

176
00:07:20,160 --> 00:07:22,560
government subsidies and I'm going to

177
00:07:21,280 --> 00:07:24,080
share with you in a bit that these

178
00:07:22,560 --> 00:07:26,479
government subsidies don't necessarily

179
00:07:24,080 --> 00:07:28,960
translate into productive use of of

180
00:07:26,479 --> 00:07:31,520
robots even in in the Chinese market.

181
00:07:28,960 --> 00:07:34,720
Second, it's not just robots. This is

182
00:07:31,520 --> 00:07:36,560
data from the US. And in the US context,

183
00:07:34,720 --> 00:07:38,800
if you break down any of these advanced

184
00:07:36,560 --> 00:07:41,360
technologies, we have robots down here,

185
00:07:38,800 --> 00:07:43,039
which it's about 8 to 12% of all US

186
00:07:41,360 --> 00:07:44,720
manufacturing firms have any robots at

187
00:07:43,039 --> 00:07:46,800
all. But even advanced computing

188
00:07:44,720 --> 00:07:48,720
technologies and specialized software, a

189
00:07:46,800 --> 00:07:50,720
minority of companies has actually

190
00:07:48,720 --> 00:07:53,199
invested in these technologies. So

191
00:07:50,720 --> 00:07:55,280
there's this real there's this real uh

192
00:07:53,199 --> 00:07:57,120
bizarre puzzle that these technologies

193
00:07:55,280 --> 00:07:58,960
seem to have benefits for the firms that

194
00:07:57,120 --> 00:08:00,240
adopt them, but not all firms are doing

195
00:07:58,960 --> 00:08:04,240
so. And we need to really understand

196
00:08:00,240 --> 00:08:06,479
why. Okay. Second,

197
00:08:04,240 --> 00:08:08,080
when we have this conversation about AI

198
00:08:06,479 --> 00:08:10,560
that I'm sure many of you have have

199
00:08:08,080 --> 00:08:12,639
heard, there are predictions that these

200
00:08:10,560 --> 00:08:14,879
new technologies are going to transform

201
00:08:12,639 --> 00:08:18,240
the productivity of the economy and lead

202
00:08:14,879 --> 00:08:20,080
us to a whole new era of prosperity. One

203
00:08:18,240 --> 00:08:21,520
where work might not even be required.

204
00:08:20,080 --> 00:08:23,919
These are the the technological

205
00:08:21,520 --> 00:08:26,000
optimists make this case. And this there

206
00:08:23,919 --> 00:08:28,639
was a similar case for this during the

207
00:08:26,000 --> 00:08:30,720
early era of computers and the internet.

208
00:08:28,639 --> 00:08:33,440
And what the the proponents of these

209
00:08:30,720 --> 00:08:36,159
technologies said then was look at how

210
00:08:33,440 --> 00:08:38,479
much faster a computer can do a task

211
00:08:36,159 --> 00:08:40,560
than a fax machine or someone doing it

212
00:08:38,479 --> 00:08:42,880
by hand. They predicted huge leaps

213
00:08:40,560 --> 00:08:45,519
forward in productivity. And over time

214
00:08:42,880 --> 00:08:47,440
what they found was that in the 90s and

215
00:08:45,519 --> 00:08:49,519
2000s when the computers really became

216
00:08:47,440 --> 00:08:51,279
saturated in the business market that

217
00:08:49,519 --> 00:08:53,279
those productivity gains never really

218
00:08:51,279 --> 00:08:55,279
showed up. They didn't come. And what

219
00:08:53,279 --> 00:08:57,680
economists ended up calling this was the

220
00:08:55,279 --> 00:09:00,240
productivity paradox. They said that

221
00:08:57,680 --> 00:09:02,000
they saw computers everywhere except in

222
00:09:00,240 --> 00:09:04,000
the productivity numbers. And I think

223
00:09:02,000 --> 00:09:06,959
this dilemma is is captured nicely by

224
00:09:04,000 --> 00:09:08,800
this cartoon where it says economists

225
00:09:06,959 --> 00:09:10,160
are wondering where the you know where

226
00:09:08,800 --> 00:09:11,920
the productivity gains are from

227
00:09:10,160 --> 00:09:13,680
computers. And then it shows of course a

228
00:09:11,920 --> 00:09:16,320
bunch of workers slacking off on their

229
00:09:13,680 --> 00:09:17,920
computers. The implication being that

230
00:09:16,320 --> 00:09:20,000
sure computers allow you to do some

231
00:09:17,920 --> 00:09:21,920
things faster but they also lead you to

232
00:09:20,000 --> 00:09:24,320
do all sorts of other tasks that might

233
00:09:21,920 --> 00:09:25,760
not be adding value in your job. So just

234
00:09:24,320 --> 00:09:28,000
to give you a sense of the the numbers

235
00:09:25,760 --> 00:09:31,440
here. So here's just productivity growth

236
00:09:28,000 --> 00:09:33,920
numbers 1970 to204 in the kind of the

237
00:09:31,440 --> 00:09:36,320
computer era lower than the precomputer

238
00:09:33,920 --> 00:09:38,080
era and kind of back in this early uh

239
00:09:36,320 --> 00:09:39,120
second industrial revolution era. So we

240
00:09:38,080 --> 00:09:40,399
didn't see the big boost in

241
00:09:39,120 --> 00:09:42,240
productivity. Of course there are a lot

242
00:09:40,399 --> 00:09:44,000
of factors that affect these numbers but

243
00:09:42,240 --> 00:09:45,920
if we would have seen this real phase

244
00:09:44,000 --> 00:09:48,000
shift in technology we would expect

245
00:09:45,920 --> 00:09:49,920
productivity growth to go up big time

246
00:09:48,000 --> 00:09:52,560
and we didn't. So this is a question to

247
00:09:49,920 --> 00:09:54,000
keep raising about generative AI and the

248
00:09:52,560 --> 00:09:55,279
new technologies like is this time

249
00:09:54,000 --> 00:09:58,080
really different or are we going to be

250
00:09:55,279 --> 00:10:00,640
stuck in a similar productivity paradox.

251
00:09:58,080 --> 00:10:02,160
The third puzzle and the one that I find

252
00:10:00,640 --> 00:10:06,240
most interesting and is most relevant to

253
00:10:02,160 --> 00:10:08,240
my work is that sometimes humans are

254
00:10:06,240 --> 00:10:10,480
actually better than machines at doing a

255
00:10:08,240 --> 00:10:12,160
task. Sometimes a machine is better than

256
00:10:10,480 --> 00:10:13,839
humans and then sometimes the human and

257
00:10:12,160 --> 00:10:15,920
the machine together are the best at

258
00:10:13,839 --> 00:10:18,240
doing the task. But the challenge is we

259
00:10:15,920 --> 00:10:19,680
don't know to predict when each case

260
00:10:18,240 --> 00:10:22,560
will be true. So let me give you two

261
00:10:19,680 --> 00:10:26,079
examples. There's been a series of

262
00:10:22,560 --> 00:10:28,480
studies uh of radiologists. So um

263
00:10:26,079 --> 00:10:30,640
doctors whose job it is to study scans

264
00:10:28,480 --> 00:10:32,399
and try to detect anomalies. And of

265
00:10:30,640 --> 00:10:34,480
course this is also a great task for

266
00:10:32,399 --> 00:10:36,800
machine learning algorithms. So the

267
00:10:34,480 --> 00:10:40,079
studies have compared doctor's

268
00:10:36,800 --> 00:10:43,120
performance in identifying uh cancer in

269
00:10:40,079 --> 00:10:45,440
radiology scans to AI performance in

270
00:10:43,120 --> 00:10:48,480
detecting cancer in radiology scans. And

271
00:10:45,440 --> 00:10:52,560
what they found is that when you just

272
00:10:48,480 --> 00:10:53,680
have the doctor or just have the um radi

273
00:10:52,560 --> 00:10:55,839
you just have the doctor or you just

274
00:10:53,680 --> 00:10:57,839
have the AI, they both do pretty well

275
00:10:55,839 --> 00:11:01,760
and they actually have a pretty low rate

276
00:10:57,839 --> 00:11:03,519
of finding false positives essentially

277
00:11:01,760 --> 00:11:05,040
identifying cancer where it's not there.

278
00:11:03,519 --> 00:11:06,640
And that's false positives are a problem

279
00:11:05,040 --> 00:11:08,480
because they can really lead to high

280
00:11:06,640 --> 00:11:10,800
costs in the system of of further

281
00:11:08,480 --> 00:11:12,720
procedures. But when you have the AI and

282
00:11:10,800 --> 00:11:14,560
the doctor together, the assumption

283
00:11:12,720 --> 00:11:17,120
would be that more expertise is better,

284
00:11:14,560 --> 00:11:19,120
right? Not really. You see that the the

285
00:11:17,120 --> 00:11:21,360
rate of false positives actually goes up

286
00:11:19,120 --> 00:11:23,519
quite a lot with the doctor and the AI

287
00:11:21,360 --> 00:11:26,079
together, even though the doctor and the

288
00:11:23,519 --> 00:11:27,519
AI together can find the cancer when

289
00:11:26,079 --> 00:11:29,200
it's there and has a lower risk of

290
00:11:27,519 --> 00:11:31,279
negatives. So, there's this really

291
00:11:29,200 --> 00:11:32,959
interesting dilemma of the AI and the

292
00:11:31,279 --> 00:11:34,720
human are better at some things and not

293
00:11:32,959 --> 00:11:37,839
at others. And we don't really know the

294
00:11:34,720 --> 00:11:39,680
conditions under which that's true. Um

295
00:11:37,839 --> 00:11:42,320
the second thing we there was a recent

296
00:11:39,680 --> 00:11:44,959
study from a colleague of of ours at MIT

297
00:11:42,320 --> 00:11:47,839
um as well as others on uh consultants

298
00:11:44,959 --> 00:11:50,000
and they gave consultants two tasks. One

299
00:11:47,839 --> 00:11:52,800
was to develop a marketing campaign for

300
00:11:50,000 --> 00:11:55,120
a shoe and the other was to analyze

301
00:11:52,800 --> 00:11:57,360
market data and advise a company which

302
00:11:55,120 --> 00:11:58,640
new industry to enter into. And they

303
00:11:57,360 --> 00:12:00,880
thought that if you give these

304
00:11:58,640 --> 00:12:03,040
consultants AI tools, they'd be better

305
00:12:00,880 --> 00:12:05,040
at these tasks. They both provide the

306
00:12:03,040 --> 00:12:07,040
right answer more of the time and they'd

307
00:12:05,040 --> 00:12:08,800
also do the task more quickly than they

308
00:12:07,040 --> 00:12:10,880
could have done it on their own. So they

309
00:12:08,800 --> 00:12:13,440
tested this and what they found to their

310
00:12:10,880 --> 00:12:15,519
surprise was that in creating the

311
00:12:13,440 --> 00:12:17,519
marketing campaign for the shoe, the

312
00:12:15,519 --> 00:12:19,680
consultants with AI were far better,

313
00:12:17,519 --> 00:12:22,240
faster, and their marketing plans were

314
00:12:19,680 --> 00:12:23,760
considered higher quality by experts.

315
00:12:22,240 --> 00:12:26,160
But when you asked them to do the market

316
00:12:23,760 --> 00:12:28,800
analysis, those with AI actually came up

317
00:12:26,160 --> 00:12:30,720
with the wrong answer more frequently.

318
00:12:28,800 --> 00:12:32,160
and and they wondered why could they

319
00:12:30,720 --> 00:12:33,920
have gotten this wrong. So they went

320
00:12:32,160 --> 00:12:35,680
back and they looked at how the

321
00:12:33,920 --> 00:12:37,760
consultants were using the AI and it

322
00:12:35,680 --> 00:12:39,760
turned out the consultants were asking a

323
00:12:37,760 --> 00:12:41,680
lot of the right questions but the AI

324
00:12:39,760 --> 00:12:43,600
was giving them misleading answers and

325
00:12:41,680 --> 00:12:45,760
it was doing it so persuasively that

326
00:12:43,600 --> 00:12:47,680
even these highly educated consultants

327
00:12:45,760 --> 00:12:50,160
bought in and and ended up following the

328
00:12:47,680 --> 00:12:52,160
AI into the wrong answer. So again the

329
00:12:50,160 --> 00:12:54,000
question here is how do you exercise

330
00:12:52,160 --> 00:12:57,600
discretion? How do you find out which

331
00:12:54,000 --> 00:13:00,000
are the tasks where the the experts the

332
00:12:57,600 --> 00:13:02,880
human experts can benefit from new

333
00:13:00,000 --> 00:13:04,800
technology and which are the tasks where

334
00:13:02,880 --> 00:13:06,480
the these scholars call it outside the

335
00:13:04,800 --> 00:13:08,160
frontier where the technology isn't a

336
00:13:06,480 --> 00:13:12,079
benefit and might actually lead you

337
00:13:08,160 --> 00:13:14,000
astray. So this leads me to a few cases

338
00:13:12,079 --> 00:13:16,959
that I think can be instructive for this

339
00:13:14,000 --> 00:13:18,880
audience of of failures and automation.

340
00:13:16,959 --> 00:13:21,279
And I think failures and automation are

341
00:13:18,880 --> 00:13:23,279
are important because they can help us

342
00:13:21,279 --> 00:13:25,040
identify, you know, what went wrong and

343
00:13:23,279 --> 00:13:27,519
to try to kind of we can avoid stepping

344
00:13:25,040 --> 00:13:30,000
in those those holes in in the future.

345
00:13:27,519 --> 00:13:32,880
So one of the reasons why automation

346
00:13:30,000 --> 00:13:35,360
fails is that people have a tough time

347
00:13:32,880 --> 00:13:37,360
calibrating risk. And you can imagine

348
00:13:35,360 --> 00:13:39,120
some failures of automation aren't

349
00:13:37,360 --> 00:13:40,959
really a problem of the technology.

350
00:13:39,120 --> 00:13:42,959
They're a problem of an organization

351
00:13:40,959 --> 00:13:46,000
unwilling to take the risk to use the

352
00:13:42,959 --> 00:13:48,399
technology. And here are two scholars

353
00:13:46,000 --> 00:13:50,560
who became famous in the 1970s, Danny

354
00:13:48,399 --> 00:13:52,240
Conan and Amos Tverki. Um they were at

355
00:13:50,560 --> 00:13:54,480
the University of California, Berkeley.

356
00:13:52,240 --> 00:13:57,040
They were psychologists who recognized

357
00:13:54,480 --> 00:13:58,560
that humans pretty consistently make

358
00:13:57,040 --> 00:14:00,880
what economists would think of as

359
00:13:58,560 --> 00:14:02,320
irrational decisions. That even if they

360
00:14:00,880 --> 00:14:04,399
could gain a certain amount of money,

361
00:14:02,320 --> 00:14:06,639
they would choose not to gain that money

362
00:14:04,399 --> 00:14:08,320
uh in favor of some alternative. And

363
00:14:06,639 --> 00:14:09,920
they realized that the consistency of

364
00:14:08,320 --> 00:14:12,160
the behavior was something they called

365
00:14:09,920 --> 00:14:14,560
loss aversion. that humans would rather

366
00:14:12,160 --> 00:14:16,000
avoid losing money versus taking the

367
00:14:14,560 --> 00:14:18,880
chance to gain money. So you could see

368
00:14:16,000 --> 00:14:21,120
here is that even with a small loss,

369
00:14:18,880 --> 00:14:23,120
humans see this as a big negative,

370
00:14:21,120 --> 00:14:25,360
whereas it would take a much bigger gain

371
00:14:23,120 --> 00:14:27,440
to get to the same positive. So this is

372
00:14:25,360 --> 00:14:30,320
kind of abstract. Let's put it in in

373
00:14:27,440 --> 00:14:32,480
more concrete terms. So, when you were a

374
00:14:30,320 --> 00:14:34,720
if you were asked if you had an 80%

375
00:14:32,480 --> 00:14:37,440
chance of winning $4,000, let's call it

376
00:14:34,720 --> 00:14:40,720
US dollars versus a 100% guaranteed

377
00:14:37,440 --> 00:14:45,160
chance of $3,000, how many would choose

378
00:14:40,720 --> 00:14:45,160
the 80% chance of $4,000?

379
00:14:45,839 --> 00:14:52,079
Barely anyone, right? How about the 100%

380
00:14:48,880 --> 00:14:53,600
guarantee of 3,000?

381
00:14:52,079 --> 00:14:55,760
Okay, many just wouldn't take the money

382
00:14:53,600 --> 00:14:58,000
at all, but okay. So the the the

383
00:14:55,760 --> 00:15:00,399
guarantee of 3,000 wins out and that's

384
00:14:58,000 --> 00:15:02,079
the case in the study that that Terski

385
00:15:00,399 --> 00:15:06,000
and Conoran did too. So it was only

386
00:15:02,079 --> 00:15:08,959
about 10 to 20% of all people took the

387
00:15:06,000 --> 00:15:11,199
80% chance. But economists and our best

388
00:15:08,959 --> 00:15:12,959
economic models would predict that a

389
00:15:11,199 --> 00:15:14,720
100% of people would take that chance

390
00:15:12,959 --> 00:15:16,079
because in a repeated game over time

391
00:15:14,720 --> 00:15:17,839
you're going to win more than the you're

392
00:15:16,079 --> 00:15:20,000
going to make more than the 3,000.

393
00:15:17,839 --> 00:15:22,240
Right? So this is showing a flaw in how

394
00:15:20,000 --> 00:15:24,320
we evaluated risk as individuals. And

395
00:15:22,240 --> 00:15:25,760
this actually translates into into

396
00:15:24,320 --> 00:15:27,680
organizations as well that many

397
00:15:25,760 --> 00:15:29,600
organizations particularly ones that are

398
00:15:27,680 --> 00:15:31,760
dominant in industry end up being

399
00:15:29,600 --> 00:15:34,560
riskaverse preserving what they have

400
00:15:31,760 --> 00:15:36,639
versus trying for more. And you might

401
00:15:34,560 --> 00:15:39,760
think okay 1970s maybe this has changed

402
00:15:36,639 --> 00:15:41,519
over time. In 2020 there were scholars

403
00:15:39,760 --> 00:15:43,279
um that that tried to replicate this

404
00:15:41,519 --> 00:15:45,519
study around the world to see if there

405
00:15:43,279 --> 00:15:47,040
were patterns um they could find. And

406
00:15:45,519 --> 00:15:48,639
what they found was really a lot of

407
00:15:47,040 --> 00:15:50,079
consistent findings. But what I wanted

408
00:15:48,639 --> 00:15:51,680
to share with, you know, this audience

409
00:15:50,079 --> 00:15:54,240
in particular, Chile was kind of an

410
00:15:51,680 --> 00:15:55,519
outlier down here. So what I would

411
00:15:54,240 --> 00:15:57,120
expect from this audience was more

412
00:15:55,519 --> 00:15:59,199
people willing to take the risk actually

413
00:15:57,120 --> 00:16:02,160
because Chileans it seems are more

414
00:15:59,199 --> 00:16:04,000
riskaverse than people in other places,

415
00:16:02,160 --> 00:16:06,000
but we can set that aside maybe for for

416
00:16:04,000 --> 00:16:07,440
Q&A. Um, okay. So so one of the

417
00:16:06,000 --> 00:16:08,560
challenges with automation is that

418
00:16:07,440 --> 00:16:10,480
people might not take the risk.

419
00:16:08,560 --> 00:16:12,880
Organizations might not take the risk to

420
00:16:10,480 --> 00:16:14,880
invest in new technologies. A second

421
00:16:12,880 --> 00:16:17,279
challenge is that even among the

422
00:16:14,880 --> 00:16:20,560
risk-taking organizations, the ones that

423
00:16:17,279 --> 00:16:22,000
try to automate many of their activities

424
00:16:20,560 --> 00:16:25,680
that that that automation might

425
00:16:22,000 --> 00:16:29,360
backfire. So one of the reasons why I I

426
00:16:25,680 --> 00:16:32,240
am anticipating that moving entirely in

427
00:16:29,360 --> 00:16:33,839
a supervisory control direction,

428
00:16:32,240 --> 00:16:35,839
essentially all operators in control

429
00:16:33,839 --> 00:16:38,320
rooms is that this aversion of this has

430
00:16:35,839 --> 00:16:39,519
been tried over and over again in

431
00:16:38,320 --> 00:16:42,320
industrial environments in

432
00:16:39,519 --> 00:16:44,079
manufacturing. Um the the term of art is

433
00:16:42,320 --> 00:16:46,560
lights out automation. This was coined

434
00:16:44,079 --> 00:16:48,800
in the early 1980s by General Motors and

435
00:16:46,560 --> 00:16:50,560
the General Motors CEO at the time said

436
00:16:48,800 --> 00:16:52,560
that he wanted the factory to be so

437
00:16:50,560 --> 00:16:53,839
automated to have so many robots that

438
00:16:52,560 --> 00:16:55,920
you wouldn't even need to turn on the

439
00:16:53,839 --> 00:16:58,079
lights. That would be a waste of money.

440
00:16:55,920 --> 00:17:00,720
And he tried to do this in 1982. He

441
00:16:58,079 --> 00:17:03,279
called it the factory of the future. And

442
00:17:00,720 --> 00:17:05,039
the factory did terribly. The robots

443
00:17:03,279 --> 00:17:06,799
couldn't actually figure out which car

444
00:17:05,039 --> 00:17:08,720
was which, which bumper would go with

445
00:17:06,799 --> 00:17:10,240
which body. they would be painting each

446
00:17:08,720 --> 00:17:12,240
other instead of painting the cars. It

447
00:17:10,240 --> 00:17:13,360
was a total disaster. So the factory of

448
00:17:12,240 --> 00:17:16,559
the future essentially became a

449
00:17:13,360 --> 00:17:19,679
laughingstock until Elon Musk comes

450
00:17:16,559 --> 00:17:21,120
around in 2019 2020. He says this lights

451
00:17:19,679 --> 00:17:23,439
out automation idea. It's really

452
00:17:21,120 --> 00:17:26,240
terrific. And he tries the same idea

453
00:17:23,439 --> 00:17:28,160
really with the Model 3 for Tesla

454
00:17:26,240 --> 00:17:29,520
backfires again. Even though there have

455
00:17:28,160 --> 00:17:31,520
been great advances in robotics

456
00:17:29,520 --> 00:17:34,400
technology, the implementation of that

457
00:17:31,520 --> 00:17:36,559
technology was really flawed and and uh

458
00:17:34,400 --> 00:17:38,640
Tesla ends up assembling Model 3s in a

459
00:17:36,559 --> 00:17:41,520
tent outside the factory about as manual

460
00:17:38,640 --> 00:17:44,000
as can be. Okay, this plays out over and

461
00:17:41,520 --> 00:17:46,080
over again in different industries. And

462
00:17:44,000 --> 00:17:48,720
what I think is important is that the

463
00:17:46,080 --> 00:17:50,960
the lesson from each of these firms

464
00:17:48,720 --> 00:17:52,559
wasn't that the technology was incapable

465
00:17:50,960 --> 00:17:54,960
of automation. In many cases, the

466
00:17:52,559 --> 00:17:57,120
technology was available. it was that

467
00:17:54,960 --> 00:17:59,600
they hadn't really aligned the human

468
00:17:57,120 --> 00:18:01,840
skills and they hadn't designed jobs to

469
00:17:59,600 --> 00:18:05,039
manage automated systems. So you could

470
00:18:01,840 --> 00:18:07,520
see the post-mortem from uh Mr. Musk up

471
00:18:05,039 --> 00:18:09,200
top that humans are are underrated. uh

472
00:18:07,520 --> 00:18:11,360
various Chinese companies that have

473
00:18:09,200 --> 00:18:12,960
tried to use automation uh and a lot of

474
00:18:11,360 --> 00:18:16,160
those robots that you saw in the red

475
00:18:12,960 --> 00:18:18,080
line for assembling electronic devices.

476
00:18:16,160 --> 00:18:19,679
They've had, you know, when scholars go

477
00:18:18,080 --> 00:18:21,520
there to study how they've done it, they

478
00:18:19,679 --> 00:18:23,440
see robots kind of hiding in corners

479
00:18:21,520 --> 00:18:25,200
that have been they've been unable to

480
00:18:23,440 --> 00:18:27,280
use and and they say, you know, the

481
00:18:25,200 --> 00:18:29,360
human body is is magic. And then

482
00:18:27,280 --> 00:18:31,360
finally, what they realize is that in

483
00:18:29,360 --> 00:18:33,600
many cases, if you develop an automated

484
00:18:31,360 --> 00:18:35,760
system, it's inflexible. Much of the

485
00:18:33,600 --> 00:18:37,360
technology needs to be fixed. And what

486
00:18:35,760 --> 00:18:39,520
you need in order to innovate and

487
00:18:37,360 --> 00:18:40,960
continue to identify improvements is you

488
00:18:39,520 --> 00:18:43,440
need human intervention and you need

489
00:18:40,960 --> 00:18:45,280
flexibility. So this idea of lights out,

490
00:18:43,440 --> 00:18:47,919
one of the uh German manufacturing

491
00:18:45,280 --> 00:18:49,919
executives that we interviewed said um

492
00:18:47,919 --> 00:18:51,440
that he he wasn't even the goal

493
00:18:49,919 --> 00:18:52,880
shouldn't be a lights out factory

494
00:18:51,440 --> 00:18:56,480
because that was antithetical to

495
00:18:52,880 --> 00:18:58,080
innovation. So what I think it's really

496
00:18:56,480 --> 00:18:59,440
important to understand, you know, one

497
00:18:58,080 --> 00:19:01,520
reason why automation fails isn't

498
00:18:59,440 --> 00:19:03,679
because the technolog is incapable of

499
00:19:01,520 --> 00:19:05,360
performing a task. In this case, this

500
00:19:03,679 --> 00:19:06,960
was a a tool company that wanted to make

501
00:19:05,360 --> 00:19:08,880
wrenches in an automated way in the

502
00:19:06,960 --> 00:19:11,360
United States. And what they evaluated

503
00:19:08,880 --> 00:19:13,360
was that they had done the math right.

504
00:19:11,360 --> 00:19:15,200
They had tried to they knew that they

505
00:19:13,360 --> 00:19:17,360
could actually produce the technology

506
00:19:15,200 --> 00:19:19,360
was capable of producing wrenches at a

507
00:19:17,360 --> 00:19:21,440
cost-effective way in the United States

508
00:19:19,360 --> 00:19:23,200
with high wage labor. But they hadn't

509
00:19:21,440 --> 00:19:25,200
really invested in the training and they

510
00:19:23,200 --> 00:19:27,440
hadn't invested in redesigning the jobs

511
00:19:25,200 --> 00:19:29,360
to to suit an automated factory.

512
00:19:27,440 --> 00:19:30,880
Instead, what they were doing was the

513
00:19:29,360 --> 00:19:33,280
equivalent of asking someone who had

514
00:19:30,880 --> 00:19:35,520
spent their career driving, you know,

515
00:19:33,280 --> 00:19:37,200
being a manual operator in a mine and

516
00:19:35,520 --> 00:19:39,200
then asking them to go operate in a

517
00:19:37,200 --> 00:19:40,799
control room. And that really didn't

518
00:19:39,200 --> 00:19:43,679
work in their situation. They needed to

519
00:19:40,799 --> 00:19:45,440
rethink the skills that were required to

520
00:19:43,679 --> 00:19:48,000
really make an automated system function

521
00:19:45,440 --> 00:19:49,360
and to scale it up. All right. And this

522
00:19:48,000 --> 00:19:50,480
is yeah, I'm I'm going to skip this and

523
00:19:49,360 --> 00:19:52,640
we go back to it, but that was really

524
00:19:50,480 --> 00:19:54,880
just a plot of of, you know, how the

525
00:19:52,640 --> 00:19:57,360
jobs are mismatched between automated

526
00:19:54,880 --> 00:19:59,360
systems and more manual systems. So now

527
00:19:57,360 --> 00:20:00,880
I imagine what you're wondering is if if

528
00:19:59,360 --> 00:20:02,799
we hear about the stories of failure,

529
00:20:00,880 --> 00:20:06,720
what differentiates those stories of

530
00:20:02,799 --> 00:20:08,640
failure from the stories of success. And

531
00:20:06,720 --> 00:20:10,080
first it comes down to people that

532
00:20:08,640 --> 00:20:12,400
somehow there are certain people,

533
00:20:10,080 --> 00:20:14,160
certain workers that seem more open and

534
00:20:12,400 --> 00:20:16,880
willing to champion new technologies

535
00:20:14,160 --> 00:20:18,720
than others. And a group of us at MIT,

536
00:20:16,880 --> 00:20:19,919
my colleagues and I did a study. We

537
00:20:18,720 --> 00:20:21,600
wanted to understand what are the

538
00:20:19,919 --> 00:20:23,280
characteristics of these workers who are

539
00:20:21,600 --> 00:20:24,480
champions of automation who are willing

540
00:20:23,280 --> 00:20:27,679
to go work in that control room

541
00:20:24,480 --> 00:20:30,320
environment and fill it figure it out.

542
00:20:27,679 --> 00:20:32,640
So we surveyed about 9,000 workers in

543
00:20:30,320 --> 00:20:34,159
nine different countries and asked them

544
00:20:32,640 --> 00:20:35,280
questions about their jobs, questions

545
00:20:34,159 --> 00:20:37,440
about their perceptions of new

546
00:20:35,280 --> 00:20:39,600
technology, how they thought automation

547
00:20:37,440 --> 00:20:41,280
might affect their work. And what we

548
00:20:39,600 --> 00:20:43,200
found which was really surprising to us

549
00:20:41,280 --> 00:20:46,080
in the first instance was that many of

550
00:20:43,200 --> 00:20:48,720
these workers more workers were actually

551
00:20:46,080 --> 00:20:50,240
positive optimistic about the impact of

552
00:20:48,720 --> 00:20:52,080
automation on their jobs than were

553
00:20:50,240 --> 00:20:53,840
pessimistic. Particularly we asked

554
00:20:52,080 --> 00:20:55,840
workers about the impact of automation

555
00:20:53,840 --> 00:20:58,400
on their safety and comfort at work on

556
00:20:55,840 --> 00:21:00,480
their upward mobility. And it was it was

557
00:20:58,400 --> 00:21:01,520
there's a big gap actually much more

558
00:21:00,480 --> 00:21:03,360
workers who were optimistic than

559
00:21:01,520 --> 00:21:05,280
pessimistic. When it came to job

560
00:21:03,360 --> 00:21:06,880
security, workers were were still a

561
00:21:05,280 --> 00:21:08,559
little more wary, but there were more

562
00:21:06,880 --> 00:21:10,559
workers optimistic about their job

563
00:21:08,559 --> 00:21:12,240
security than pessimistic. And this

564
00:21:10,559 --> 00:21:14,320
frankly is because a lot of the workers

565
00:21:12,240 --> 00:21:15,760
we surveyed were generally happy at

566
00:21:14,320 --> 00:21:18,000
work. We had pretty high job

567
00:21:15,760 --> 00:21:19,520
satisfaction across these countries. And

568
00:21:18,000 --> 00:21:21,360
I think it's easy to forget that that's

569
00:21:19,520 --> 00:21:23,120
true in in most environments that job

570
00:21:21,360 --> 00:21:25,840
satisfaction is pretty high. In the

571
00:21:23,120 --> 00:21:27,840
United States, it's over 60%.

572
00:21:25,840 --> 00:21:29,120
So what did we learn? What's the

573
00:21:27,840 --> 00:21:30,960
difference between the group that was

574
00:21:29,120 --> 00:21:32,880
optimistic about automation, willing to

575
00:21:30,960 --> 00:21:34,480
champion it versus the group that was

576
00:21:32,880 --> 00:21:37,440
pessimistic. The first thing we wanted

577
00:21:34,480 --> 00:21:39,600
to know was does this vary across place.

578
00:21:37,440 --> 00:21:42,400
And what we found was yes, it does. And

579
00:21:39,600 --> 00:21:44,000
in in European environments, um there

580
00:21:42,400 --> 00:21:45,679
was actually much more optimism about

581
00:21:44,000 --> 00:21:47,520
the role of technology in shaping their

582
00:21:45,679 --> 00:21:48,720
work than in the United States for

583
00:21:47,520 --> 00:21:51,520
example where we were the most

584
00:21:48,720 --> 00:21:53,440
pessimistic about the adoption of new

585
00:21:51,520 --> 00:21:55,280
technologies and its impact on work. I

586
00:21:53,440 --> 00:21:57,120
would imagine it's based on some other

587
00:21:55,280 --> 00:21:59,120
some other data that if we surveyed

588
00:21:57,120 --> 00:22:00,559
Chileans, they would be much look much

589
00:21:59,120 --> 00:22:02,880
more like the Europeans than like the

590
00:22:00,559 --> 00:22:04,960
Americans in this in this survey. But

591
00:22:02,880 --> 00:22:06,720
beyond just the the geography, we asked

592
00:22:04,960 --> 00:22:09,120
workers based on their tasks how they

593
00:22:06,720 --> 00:22:11,760
thought about automation. And our

594
00:22:09,120 --> 00:22:13,679
expectation was that workers who do a

595
00:22:11,760 --> 00:22:15,520
lot of routine tasks, things over and

596
00:22:13,679 --> 00:22:17,679
over again, would feel vulnerable to

597
00:22:15,520 --> 00:22:19,200
automation and would resist it. Whereas

598
00:22:17,679 --> 00:22:20,480
workers who did a lot of different

599
00:22:19,200 --> 00:22:21,840
things during the day, they wouldn't

600
00:22:20,480 --> 00:22:23,120
feel vulnerable to automation. they

601
00:22:21,840 --> 00:22:24,880
might be more interested in new

602
00:22:23,120 --> 00:22:25,919
technologies. But what we found was

603
00:22:24,880 --> 00:22:28,400
actually something a little bit

604
00:22:25,919 --> 00:22:29,919
different. We found that and I'm going

605
00:22:28,400 --> 00:22:31,840
to focus for this audience on the

606
00:22:29,919 --> 00:22:34,559
physical industries. So think think

607
00:22:31,840 --> 00:22:37,200
mining, think um agriculture,

608
00:22:34,559 --> 00:22:39,280
manufacturing, logistics,

609
00:22:37,200 --> 00:22:41,840
that we when we looked at workers with

610
00:22:39,280 --> 00:22:44,080
high level of routine tasks and low

611
00:22:41,840 --> 00:22:47,280
problem solving, so workers really wrote

612
00:22:44,080 --> 00:22:49,280
jobs, they were indeed very pessimistic

613
00:22:47,280 --> 00:22:52,400
about automation and its impact on their

614
00:22:49,280 --> 00:22:54,640
work. But if we looked at workers with

615
00:22:52,400 --> 00:22:55,679
the same level of routine tasks, so

616
00:22:54,640 --> 00:22:57,360
people who were doing a lot of the same

617
00:22:55,679 --> 00:22:59,200
thing over and over again, but still had

618
00:22:57,360 --> 00:23:00,880
to do some problem solving, when the

619
00:22:59,200 --> 00:23:03,039
workers had to do problem solving, their

620
00:23:00,880 --> 00:23:05,360
perceptions totally flipped. They were

621
00:23:03,039 --> 00:23:07,200
the most optimistic about automation,

622
00:23:05,360 --> 00:23:08,799
its impact on their work. So I think the

623
00:23:07,200 --> 00:23:10,880
implication here and what I think we can

624
00:23:08,799 --> 00:23:13,600
learn from this data is that if you

625
00:23:10,880 --> 00:23:15,440
design jobs and this goes back to the

626
00:23:13,600 --> 00:23:17,919
the control room environment. If you

627
00:23:15,440 --> 00:23:19,840
design jobs to focus on high levels of

628
00:23:17,919 --> 00:23:22,240
problem solving, workers in those

629
00:23:19,840 --> 00:23:23,760
environments might be more open to

630
00:23:22,240 --> 00:23:25,760
adopting new technologies and figuring

631
00:23:23,760 --> 00:23:27,600
out how they can make it work. And our

632
00:23:25,760 --> 00:23:29,760
explanation for this is if you have a

633
00:23:27,600 --> 00:23:31,200
high level of problem solving, you feel

634
00:23:29,760 --> 00:23:32,799
confident that you're still going to be

635
00:23:31,200 --> 00:23:34,000
able to have discretion at work and

636
00:23:32,799 --> 00:23:36,000
you're still going to be able to have

637
00:23:34,000 --> 00:23:38,080
kind of a purpose and grow in your job.

638
00:23:36,000 --> 00:23:40,320
And the way you can use automation is to

639
00:23:38,080 --> 00:23:41,760
get rid of those pesky routine tasks.

640
00:23:40,320 --> 00:23:43,440
That technology really can be helpful

641
00:23:41,760 --> 00:23:46,640
for you in that way without being a

642
00:23:43,440 --> 00:23:48,400
threat. Whereas workers here, technology

643
00:23:46,640 --> 00:23:49,840
might might be helpful, but it might

644
00:23:48,400 --> 00:23:51,360
also be a threat because you don't have

645
00:23:49,840 --> 00:23:54,559
the problem solving that might help you

646
00:23:51,360 --> 00:23:56,720
grow in your job. Okay.

647
00:23:54,559 --> 00:23:58,559
All right. So, so I talked about, you

648
00:23:56,720 --> 00:24:01,200
know, the role of people in in helping

649
00:23:58,559 --> 00:24:03,520
make automation work and the role of

650
00:24:01,200 --> 00:24:05,440
redesigning jobs. So, we can't just take

651
00:24:03,520 --> 00:24:07,280
a job that existed in a manual

652
00:24:05,440 --> 00:24:10,760
environment and a person with those

653
00:24:07,280 --> 00:24:10,760
skills and necessity

654
00:24:11,679 --> 00:24:14,720
intensive environment. That does not

655
00:24:13,279 --> 00:24:16,559
seem to work based on our historical

656
00:24:14,720 --> 00:24:19,200
evidence. So, what does work? What are

657
00:24:16,559 --> 00:24:21,039
the skills compositions that we see um

658
00:24:19,200 --> 00:24:23,120
successful? So this was a paper that

659
00:24:21,039 --> 00:24:25,039
came out of what might seem like a very

660
00:24:23,120 --> 00:24:27,039
different industry, but I think it's

661
00:24:25,039 --> 00:24:29,200
relevant here. And it's it's hospital

662
00:24:27,039 --> 00:24:31,600
systems. So over the last 10 to 15

663
00:24:29,200 --> 00:24:33,679
years, hospital systems have invested

664
00:24:31,600 --> 00:24:36,159
pretty significantly in automating the

665
00:24:33,679 --> 00:24:37,840
routine tasks of administration through

666
00:24:36,159 --> 00:24:39,760
technologies, a family of technologies

667
00:24:37,840 --> 00:24:41,520
called robotic process automation. Now

668
00:24:39,760 --> 00:24:43,440
there's some more AI involved in these

669
00:24:41,520 --> 00:24:45,200
in these technologies. And what we found

670
00:24:43,440 --> 00:24:47,120
in comparing different hospital systems

671
00:24:45,200 --> 00:24:50,480
and their approaches to using these

672
00:24:47,120 --> 00:24:52,080
tools was that a lot of their their uh a

673
00:24:50,480 --> 00:24:54,960
lot of these deployments of new

674
00:24:52,080 --> 00:24:57,440
technology focused on specific use cases

675
00:24:54,960 --> 00:24:59,440
or focused on specific technologies but

676
00:24:57,440 --> 00:25:01,679
they had difficulty scaling up across

677
00:24:59,440 --> 00:25:03,039
the system. So in the mining context it

678
00:25:01,679 --> 00:25:05,039
might be like you could provide a

679
00:25:03,039 --> 00:25:07,200
control room environment for one type of

680
00:25:05,039 --> 00:25:08,720
task or one type of operator but you

681
00:25:07,200 --> 00:25:10,400
couldn't necessarily look at the whole

682
00:25:08,720 --> 00:25:12,559
system in an integrated way. And we

683
00:25:10,400 --> 00:25:15,039
wondered why that was the case. And we

684
00:25:12,559 --> 00:25:17,279
realized that when the hospital system

685
00:25:15,039 --> 00:25:18,880
designed jobs to work in an automated

686
00:25:17,279 --> 00:25:20,960
environment, they they made one of two

687
00:25:18,880 --> 00:25:23,120
decisions. They either really focused on

688
00:25:20,960 --> 00:25:25,440
people with process knowledge, so people

689
00:25:23,120 --> 00:25:27,679
who know knew what it looked like to do

690
00:25:25,440 --> 00:25:29,279
that task well, or it focused on

691
00:25:27,679 --> 00:25:30,640
investing people with engineering

692
00:25:29,279 --> 00:25:33,039
knowledge, with a lot of technical

693
00:25:30,640 --> 00:25:35,279
skills. And those people really knew

694
00:25:33,039 --> 00:25:38,080
what the technology was capable of. But

695
00:25:35,279 --> 00:25:40,080
rarely did they really build a kind of a

696
00:25:38,080 --> 00:25:42,000
multid-disciplinary team that had both

697
00:25:40,080 --> 00:25:44,159
process knowledge and technical

698
00:25:42,000 --> 00:25:45,840
knowledge that could develop the kind of

699
00:25:44,159 --> 00:25:47,840
flexible automation that scaled up more

700
00:25:45,840 --> 00:25:49,200
successfully. And this was this was very

701
00:25:47,840 --> 00:25:50,640
difficult because you could think of

702
00:25:49,200 --> 00:25:52,000
these the process knowledge and the

703
00:25:50,640 --> 00:25:53,760
technical knowledge is two very

704
00:25:52,000 --> 00:25:55,520
different languages and it takes a lot

705
00:25:53,760 --> 00:25:57,440
of time and it actually the people who

706
00:25:55,520 --> 00:25:59,760
know both of those languages can demand

707
00:25:57,440 --> 00:26:02,240
pretty high wages. So the challenge here

708
00:25:59,760 --> 00:26:04,320
isn't necessarily to train the unicorn

709
00:26:02,240 --> 00:26:06,159
type person who can do everything, but

710
00:26:04,320 --> 00:26:07,840
instead to build kind of channels of

711
00:26:06,159 --> 00:26:11,440
translation across those two different

712
00:26:07,840 --> 00:26:14,159
types of workers. And and I I want to

713
00:26:11,440 --> 00:26:17,440
emphasize two real I think important

714
00:26:14,159 --> 00:26:18,960
skills for um these teams that I'm

715
00:26:17,440 --> 00:26:20,880
talking about these multi-disiplinary

716
00:26:18,960 --> 00:26:24,559
teams that can scale up automated

717
00:26:20,880 --> 00:26:26,640
systems. The first set of skills is in

718
00:26:24,559 --> 00:26:29,440
experimentation. And why experimentation

719
00:26:26,640 --> 00:26:30,640
is important is what we talk about when

720
00:26:29,440 --> 00:26:32,559
we're talking about deploying new

721
00:26:30,640 --> 00:26:35,200
technology is a pretty dramatic change

722
00:26:32,559 --> 00:26:36,640
to a system. And when you're changing a

723
00:26:35,200 --> 00:26:38,000
system, you need to be able to

724
00:26:36,640 --> 00:26:40,159
understand what's working and what's

725
00:26:38,000 --> 00:26:41,840
not. And our best way of understanding

726
00:26:40,159 --> 00:26:44,159
the impact of different changes that we

727
00:26:41,840 --> 00:26:46,240
make is experimentation. So the the

728
00:26:44,159 --> 00:26:48,799
skills associated with experimentation

729
00:26:46,240 --> 00:26:51,039
are not just kind of how do you um do

730
00:26:48,799 --> 00:26:52,400
the statistics to understand is there a

731
00:26:51,039 --> 00:26:54,080
significant improvement in your

732
00:26:52,400 --> 00:26:55,760
performance. It's really about

733
00:26:54,080 --> 00:26:57,679
understanding what question you want to

734
00:26:55,760 --> 00:26:59,440
answer in the first place. What are the

735
00:26:57,679 --> 00:27:01,600
metrics that you want to improve upon?

736
00:26:59,440 --> 00:27:03,600
And that requires process knowledge.

737
00:27:01,600 --> 00:27:05,440
Whereas the actual running of the

738
00:27:03,600 --> 00:27:07,200
experiment, the collection of data might

739
00:27:05,440 --> 00:27:08,880
require technical knowledge. So this is

740
00:27:07,200 --> 00:27:10,880
is the first kind of skill that requires

741
00:27:08,880 --> 00:27:13,679
a bridging of these two types of

742
00:27:10,880 --> 00:27:15,360
knowledge. And the second is

743
00:27:13,679 --> 00:27:16,720
interpretation. And I'll come back to

744
00:27:15,360 --> 00:27:20,799
this in a moment with with generative

745
00:27:16,720 --> 00:27:23,200
AI. But one of the I think the misguided

746
00:27:20,799 --> 00:27:26,320
uh arguments around generative AI today

747
00:27:23,200 --> 00:27:28,000
is that you need to be skilled in the AI

748
00:27:26,320 --> 00:27:29,840
itself. You need to know what's going on

749
00:27:28,000 --> 00:27:31,520
under the hood. You need to have pretty

750
00:27:29,840 --> 00:27:33,440
deep technical skills to be able to use

751
00:27:31,520 --> 00:27:35,760
these technologies well. And what I

752
00:27:33,440 --> 00:27:37,760
think that argument ignores is that one

753
00:27:35,760 --> 00:27:39,919
of the key skills of being able to

754
00:27:37,760 --> 00:27:41,520
interact with new technologies is to

755
00:27:39,919 --> 00:27:44,159
interpret whether the data the

756
00:27:41,520 --> 00:27:46,159
technology is providing is useful or

757
00:27:44,159 --> 00:27:47,840
relevant to the key challenges that

758
00:27:46,159 --> 00:27:50,400
you're trying to meet. So if you use

759
00:27:47,840 --> 00:27:52,880
generative AI to try to identify a

760
00:27:50,400 --> 00:27:54,480
better approach to a mining task, you

761
00:27:52,880 --> 00:27:56,480
need to know quite a lot about mining to

762
00:27:54,480 --> 00:27:58,480
understand if the generative AI is full

763
00:27:56,480 --> 00:28:02,559
of it or if it's providing something

764
00:27:58,480 --> 00:28:05,039
really uh useful and insightful. Okay.

765
00:28:02,559 --> 00:28:07,120
So, finally, I want to say a little bit

766
00:28:05,039 --> 00:28:08,640
about this this current wave of

767
00:28:07,120 --> 00:28:10,480
technologies that's captured a lot of

768
00:28:08,640 --> 00:28:12,000
people's interest and attention, what we

769
00:28:10,480 --> 00:28:15,440
know about it currently and what we

770
00:28:12,000 --> 00:28:17,039
still uh need to learn. So, first I

771
00:28:15,440 --> 00:28:19,279
talked a bit about robots and and

772
00:28:17,039 --> 00:28:20,640
robotic process automation and the

773
00:28:19,279 --> 00:28:22,480
characteristics of those technologies

774
00:28:20,640 --> 00:28:24,159
are quite a bit different from the

775
00:28:22,480 --> 00:28:26,399
current uh large language model

776
00:28:24,159 --> 00:28:28,000
technologies. So if we think about two

777
00:28:26,399 --> 00:28:29,760
two features of a technology, its

778
00:28:28,000 --> 00:28:32,159
reliability, its ability to do the same

779
00:28:29,760 --> 00:28:34,720
task over and over again versus its

780
00:28:32,159 --> 00:28:36,880
flexibility, it's it's the quickness and

781
00:28:34,720 --> 00:28:39,120
dexterity with which it can move across

782
00:28:36,880 --> 00:28:40,480
tasks. We have robots are very good at

783
00:28:39,120 --> 00:28:42,480
doing the same thing over and over

784
00:28:40,480 --> 00:28:43,919
again. And for a long time, kind of

785
00:28:42,480 --> 00:28:45,360
before we applied some of these machine

786
00:28:43,919 --> 00:28:47,600
learning tools, they weren't very good

787
00:28:45,360 --> 00:28:49,520
at flexibility. So the role of of human

788
00:28:47,600 --> 00:28:52,799
workers, the role of human machine

789
00:28:49,520 --> 00:28:55,039
interaction was to add flexibility to to

790
00:28:52,799 --> 00:28:57,600
robots that were quite reliable. But

791
00:28:55,039 --> 00:28:59,279
with LMS, and many of you probably know

792
00:28:57,600 --> 00:29:00,480
this from interacting with them, they

793
00:28:59,279 --> 00:29:01,760
can answer all sorts of questions.

794
00:29:00,480 --> 00:29:03,039
They'll answer almost any question you

795
00:29:01,760 --> 00:29:04,960
give them. They'll give it you different

796
00:29:03,039 --> 00:29:07,200
form factors back. They'll answer in

797
00:29:04,960 --> 00:29:09,360
different languages. Um, but what the

798
00:29:07,200 --> 00:29:11,360
challenge is is to get them to answer in

799
00:29:09,360 --> 00:29:13,440
the same way, in a reliable way. And

800
00:29:11,360 --> 00:29:15,840
oftentimes the answers might be

801
00:29:13,440 --> 00:29:17,360
different and they're not predictable in

802
00:29:15,840 --> 00:29:20,640
how they're different. So this is one of

803
00:29:17,360 --> 00:29:22,559
the the big challenges. Now

804
00:29:20,640 --> 00:29:24,720
you know one of the early questions that

805
00:29:22,559 --> 00:29:26,240
we had when we started studying this um

806
00:29:24,720 --> 00:29:28,159
kind of the role of generative AI and

807
00:29:26,240 --> 00:29:29,919
reshaping work. We we gathered a bunch

808
00:29:28,159 --> 00:29:31,760
of companies and we tried to understand

809
00:29:29,919 --> 00:29:33,039
what experiments they were performing

810
00:29:31,760 --> 00:29:35,039
with generative AI. Where were they

811
00:29:33,039 --> 00:29:36,080
using it? And we heard a few consistent

812
00:29:35,039 --> 00:29:37,760
things. We heard that they were using it

813
00:29:36,080 --> 00:29:39,120
in customer service. We heard that their

814
00:29:37,760 --> 00:29:42,000
software engineers were using these

815
00:29:39,120 --> 00:29:44,640
tools. And then as we were asking our 50

816
00:29:42,000 --> 00:29:46,960
or so companies about their use cases,

817
00:29:44,640 --> 00:29:50,159
Enthropic, the company that uh makes the

818
00:29:46,960 --> 00:29:52,480
Claude uh chatbot, they came out with

819
00:29:50,159 --> 00:29:54,720
this really uh interesting study where

820
00:29:52,480 --> 00:29:57,039
they took all of the conversations with

821
00:29:54,720 --> 00:30:00,240
Claude from their non-commercial uh

822
00:29:57,039 --> 00:30:02,480
users and they said they tried to link

823
00:30:00,240 --> 00:30:04,000
these conversations to different tasks

824
00:30:02,480 --> 00:30:06,960
and then they linked those tasks to

825
00:30:04,000 --> 00:30:09,760
jobs. And what they tried to learn was

826
00:30:06,960 --> 00:30:12,000
what what job what occupation does the

827
00:30:09,760 --> 00:30:13,679
person most likely have who is having

828
00:30:12,000 --> 00:30:15,360
this conversation you know what's the

829
00:30:13,679 --> 00:30:17,520
nature of this conversation and what

830
00:30:15,360 --> 00:30:18,880
they found apologies for the the size of

831
00:30:17,520 --> 00:30:21,520
this but I can share these slides if

832
00:30:18,880 --> 00:30:24,159
you're interested was that the high

833
00:30:21,520 --> 00:30:26,159
concentration of the the chats with claw

834
00:30:24,159 --> 00:30:29,360
the use of LLMs were in just a few

835
00:30:26,159 --> 00:30:30,799
occupations they were in the computer

836
00:30:29,360 --> 00:30:32,720
and mathematical occupations you can

837
00:30:30,799 --> 00:30:34,559
think of that as just as coding so over

838
00:30:32,720 --> 00:30:36,960
a third of all claude conversations were

839
00:30:34,559 --> 00:30:38,159
coding related, about half were either

840
00:30:36,960 --> 00:30:40,159
coding or research related

841
00:30:38,159 --> 00:30:41,679
conversations, and then you get up to

842
00:30:40,159 --> 00:30:45,440
twothirds if you add in kind of

843
00:30:41,679 --> 00:30:47,039
marketing and writing tasks. So a a

844
00:30:45,440 --> 00:30:49,679
segment of the workforce that's only

845
00:30:47,039 --> 00:30:51,679
about 10 to 15% of workers was having

846
00:30:49,679 --> 00:30:54,240
about twothirds of the generative AI

847
00:30:51,679 --> 00:30:56,320
conversations. So as of now, what I want

848
00:30:54,240 --> 00:30:58,159
to emphasize is that the the use of

849
00:30:56,320 --> 00:30:59,840
these technology tools has been very

850
00:30:58,159 --> 00:31:01,520
concentrated in a pretty small segment

851
00:30:59,840 --> 00:31:04,960
of the workforce and a pretty small

852
00:31:01,520 --> 00:31:06,720
segment of firms. Now

853
00:31:04,960 --> 00:31:07,760
the other thing that I I I think caveat

854
00:31:06,720 --> 00:31:11,360
I want to throw out about this

855
00:31:07,760 --> 00:31:12,559
technology is that a lot of advocates of

856
00:31:11,360 --> 00:31:15,360
generative AI, a lot of the generative

857
00:31:12,559 --> 00:31:17,520
AI companies are talking about the

858
00:31:15,360 --> 00:31:20,159
continuous improvement of these models.

859
00:31:17,520 --> 00:31:23,120
They often share how the latest and

860
00:31:20,159 --> 00:31:24,640
greatest model can do uh kind of solve a

861
00:31:23,120 --> 00:31:26,000
scientific problem that the previous

862
00:31:24,640 --> 00:31:27,679
models couldn't. They could do more and

863
00:31:26,000 --> 00:31:29,200
more complex tasks. They talk about

864
00:31:27,679 --> 00:31:30,880
reasoning.

865
00:31:29,200 --> 00:31:32,880
But what they don't talk about is that

866
00:31:30,880 --> 00:31:34,480
as these models have improved in some

867
00:31:32,880 --> 00:31:37,919
dimensions, they've actually gotten

868
00:31:34,480 --> 00:31:40,720
worse in in others. So this is OpenAI's

869
00:31:37,919 --> 00:31:43,200
uh publicly available release about its

870
00:31:40,720 --> 00:31:44,960
its reasoning models. And what what it

871
00:31:43,200 --> 00:31:46,799
shows is both that what these reasoning

872
00:31:44,960 --> 00:31:49,600
models can do that's new, which is which

873
00:31:46,799 --> 00:31:52,159
is exciting. And it also shows that the

874
00:31:49,600 --> 00:31:54,159
accuracy rates and the the accuracy

875
00:31:52,159 --> 00:31:56,080
rates of these models have gone down and

876
00:31:54,159 --> 00:31:58,720
the hallucination rates of these models

877
00:31:56,080 --> 00:32:00,799
have gone up. So what we see is a real

878
00:31:58,720 --> 00:32:02,960
trade-off that as these models become

879
00:32:00,799 --> 00:32:05,360
better in certain domains, they become

880
00:32:02,960 --> 00:32:06,960
worse in others. So I think when you

881
00:32:05,360 --> 00:32:08,320
might hear from their advocates, you

882
00:32:06,960 --> 00:32:09,360
know, this is the worst AI we're ever

883
00:32:08,320 --> 00:32:10,640
going to have, these models are going to

884
00:32:09,360 --> 00:32:12,559
get better and better. Hallucinations

885
00:32:10,640 --> 00:32:14,080
are going to get lower and lower. I

886
00:32:12,559 --> 00:32:16,559
think we really need to scrutinize that

887
00:32:14,080 --> 00:32:18,720
and look at the data and ask why uh

888
00:32:16,559 --> 00:32:22,159
hallucination rates continue to go up

889
00:32:18,720 --> 00:32:24,000
even as these models improve. And AI,

890
00:32:22,159 --> 00:32:25,519
open AI has been asked why this is the

891
00:32:24,000 --> 00:32:27,200
case. and they don't have an explanation

892
00:32:25,519 --> 00:32:28,799
partly because they can't always

893
00:32:27,200 --> 00:32:31,440
understand why their models are giving

894
00:32:28,799 --> 00:32:34,960
the answers that they do. All right.

895
00:32:31,440 --> 00:32:36,880
Okay. So now I I want to you know end

896
00:32:34,960 --> 00:32:38,559
with a few examples that I think are are

897
00:32:36,880 --> 00:32:40,480
really promising from these technologies

898
00:32:38,559 --> 00:32:42,880
and and I think might be relevant to the

899
00:32:40,480 --> 00:32:45,840
industries that that you represent. One

900
00:32:42,880 --> 00:32:48,000
is that a lot of the early use cases of

901
00:32:45,840 --> 00:32:50,080
these technologies have emphasized the

902
00:32:48,000 --> 00:32:51,120
ability to do certain tasks faster than

903
00:32:50,080 --> 00:32:54,960
you might have been able to do them

904
00:32:51,120 --> 00:32:57,120
before. So you can now uh you know write

905
00:32:54,960 --> 00:32:59,120
u you know write code you can you could

906
00:32:57,120 --> 00:33:00,960
program u you can write a program faster

907
00:32:59,120 --> 00:33:02,480
than you could previously or you can

908
00:33:00,960 --> 00:33:04,880
maybe write a memo faster than you could

909
00:33:02,480 --> 00:33:07,600
previously with just a few prompts. But

910
00:33:04,880 --> 00:33:09,279
what I think is important is that speed

911
00:33:07,600 --> 00:33:11,600
is not always the right measure. So this

912
00:33:09,279 --> 00:33:15,200
is a study of of doctors making uh

913
00:33:11,600 --> 00:33:17,519
diagnoses with the benefit of chat GPT.

914
00:33:15,200 --> 00:33:20,720
And what it found was that doctors who

915
00:33:17,519 --> 00:33:23,039
had access to chat GPT actually made

916
00:33:20,720 --> 00:33:25,279
their diagnosis slower than doctors who

917
00:33:23,039 --> 00:33:27,279
made their diagnosis just based on their

918
00:33:25,279 --> 00:33:29,279
memory and their training. But the

919
00:33:27,279 --> 00:33:31,760
doctors who were slower also were mo

920
00:33:29,279 --> 00:33:34,000
more likely to give a thorough and an

921
00:33:31,760 --> 00:33:35,440
accurate diagnosis. So what we care

922
00:33:34,000 --> 00:33:38,080
about the metric we care about with

923
00:33:35,440 --> 00:33:39,440
doctors isn't are you the fastest, it's

924
00:33:38,080 --> 00:33:41,600
really are you going to come up with the

925
00:33:39,440 --> 00:33:43,760
right answer the highest share of the

926
00:33:41,600 --> 00:33:45,840
time. So, I do think that we, you know,

927
00:33:43,760 --> 00:33:47,200
that users of these technologies always

928
00:33:45,840 --> 00:33:49,200
need to think, what are you using them

929
00:33:47,200 --> 00:33:50,640
for? Are you using them just because

930
00:33:49,200 --> 00:33:52,000
they're exciting and they're this newest

931
00:33:50,640 --> 00:33:53,840
tool and, you know, I have a new hammer

932
00:33:52,000 --> 00:33:55,600
and I'm going to go look for nails, or

933
00:33:53,840 --> 00:33:57,760
are you using them to really get better

934
00:33:55,600 --> 00:33:59,440
at the core purpose of your work? And in

935
00:33:57,760 --> 00:34:03,360
this case, the core purpose was to

936
00:33:59,440 --> 00:34:06,880
become more accurate. Second, I want to

937
00:34:03,360 --> 00:34:08,639
emphasize that the the nature of how

938
00:34:06,880 --> 00:34:10,639
people are doing their work is changing

939
00:34:08,639 --> 00:34:13,520
when they have access to these tools. So

940
00:34:10,639 --> 00:34:15,839
this comes from a survey asking people

941
00:34:13,520 --> 00:34:17,119
about the different kind of tasks and

942
00:34:15,839 --> 00:34:19,040
how they're doing them when they when

943
00:34:17,119 --> 00:34:22,000
they start using generative AI tools.

944
00:34:19,040 --> 00:34:23,599
And what it shows is that people are are

945
00:34:22,000 --> 00:34:25,200
accessing really different parts of

946
00:34:23,599 --> 00:34:26,639
their brain when they're using

947
00:34:25,200 --> 00:34:28,480
generative AI than if they were doing

948
00:34:26,639 --> 00:34:30,560
these tasks on their own. And I think

949
00:34:28,480 --> 00:34:32,720
this is kind of intuitive and obvious,

950
00:34:30,560 --> 00:34:36,240
but it's it's important that now that we

951
00:34:32,720 --> 00:34:38,159
have some more detailed data on how um

952
00:34:36,240 --> 00:34:40,240
the access to these tools are different

953
00:34:38,159 --> 00:34:41,760
than even access to a search engine like

954
00:34:40,240 --> 00:34:43,679
Google with how our brain is

955
00:34:41,760 --> 00:34:45,679
functioning. So the most recent study of

956
00:34:43,679 --> 00:34:48,960
this compares three groups. People doing

957
00:34:45,679 --> 00:34:50,960
a research task on their own by memory,

958
00:34:48,960 --> 00:34:52,560
people doing it with access to Google

959
00:34:50,960 --> 00:34:55,599
search and then people doing it with

960
00:34:52,560 --> 00:34:57,440
access to a chat GPT like tool. And then

961
00:34:55,599 --> 00:34:59,920
they they looked they did a scan of

962
00:34:57,440 --> 00:35:01,839
these people's brains as they were doing

963
00:34:59,920 --> 00:35:04,560
the work. And what they found is that

964
00:35:01,839 --> 00:35:06,240
that the the people uh who were doing

965
00:35:04,560 --> 00:35:08,400
the research task on their own there

966
00:35:06,240 --> 00:35:10,079
were all sorts of kind of neurons firing

967
00:35:08,400 --> 00:35:11,839
with their in their brains all sorts of

968
00:35:10,079 --> 00:35:13,839
neural connections but the people who

969
00:35:11,839 --> 00:35:15,359
were doing the work with chat GPT had

970
00:35:13,839 --> 00:35:17,440
very little brain activity at all. They

971
00:35:15,359 --> 00:35:19,119
did the task quite quickly. Um but they

972
00:35:17,440 --> 00:35:20,720
they weren't actually activating their

973
00:35:19,119 --> 00:35:23,119
brain. And as a result, when the

974
00:35:20,720 --> 00:35:24,880
researchers asked about the task they

975
00:35:23,119 --> 00:35:27,280
had just done afterward, the people who

976
00:35:24,880 --> 00:35:28,880
had done the task with chat GPT couldn't

977
00:35:27,280 --> 00:35:30,160
really recall what they had just done.

978
00:35:28,880 --> 00:35:31,599
They didn't really remember it. It

979
00:35:30,160 --> 00:35:33,280
didn't sink in compared to the people

980
00:35:31,599 --> 00:35:35,599
who had done it on their own. So again,

981
00:35:33,280 --> 00:35:36,960
there there might be productivity uh tra

982
00:35:35,599 --> 00:35:38,400
productivity benefits, there might be

983
00:35:36,960 --> 00:35:40,480
quality benefits, but there are

984
00:35:38,400 --> 00:35:42,000
trade-offs necessarily for workers and

985
00:35:40,480 --> 00:35:43,760
how they're developing their skills that

986
00:35:42,000 --> 00:35:45,760
I think we need to be concerned about. I

987
00:35:43,760 --> 00:35:47,520
think we need to guard against. And one

988
00:35:45,760 --> 00:35:50,800
way to think about it that that I like

989
00:35:47,520 --> 00:35:53,040
is really thinking of these tools uh you

990
00:35:50,800 --> 00:35:55,119
know like a new type of food. So we have

991
00:35:53,040 --> 00:35:57,359
processed food this is really processed

992
00:35:55,119 --> 00:36:00,480
information and just as processed food

993
00:35:57,359 --> 00:36:02,240
can be healthy or unhealthy right some

994
00:36:00,480 --> 00:36:03,520
processed food might taste really good

995
00:36:02,240 --> 00:36:05,599
in the short term but actually have

996
00:36:03,520 --> 00:36:07,440
negative health effects in the long term

997
00:36:05,599 --> 00:36:09,119
but other processed food could be a

998
00:36:07,440 --> 00:36:11,520
cheaper way of getting really essential

999
00:36:09,119 --> 00:36:13,280
nutrition like vitamins. Right? So the

1000
00:36:11,520 --> 00:36:15,040
question is how does this how do we use

1001
00:36:13,280 --> 00:36:16,880
these tools to get us the essential

1002
00:36:15,040 --> 00:36:18,720
nutrition that helps us in the long term

1003
00:36:16,880 --> 00:36:21,599
and helps us grow as people and as

1004
00:36:18,720 --> 00:36:24,079
workers versus using this tool in a way

1005
00:36:21,599 --> 00:36:28,160
that leads to skill atrophy and and kind

1006
00:36:24,079 --> 00:36:31,119
of cognitive decay. So

1007
00:36:28,160 --> 00:36:32,320
I'm going to you know end here with a

1008
00:36:31,119 --> 00:36:34,800
comparison that I think is really

1009
00:36:32,320 --> 00:36:37,440
important. A lot of our data on how

1010
00:36:34,800 --> 00:36:40,400
humans can work best with machines comes

1011
00:36:37,440 --> 00:36:42,720
from early evidence from uh airline

1012
00:36:40,400 --> 00:36:44,960
pilots and and even military pilots kind

1013
00:36:42,720 --> 00:36:46,720
of the the cockpit environment that

1014
00:36:44,960 --> 00:36:48,880
became more and more automated over

1015
00:36:46,720 --> 00:36:50,160
time. So how do you design an automated

1016
00:36:48,880 --> 00:36:52,480
system? How do you design a control

1017
00:36:50,160 --> 00:36:56,320
room? A lot of that research comes out

1018
00:36:52,480 --> 00:36:57,599
of the study of um of aerospace. And I

1019
00:36:56,320 --> 00:36:59,760
think a lot of the kind of the core

1020
00:36:57,599 --> 00:37:01,200
lessons from about 50 years ago still

1021
00:36:59,760 --> 00:37:03,119
really apply and how we should think

1022
00:37:01,200 --> 00:37:06,320
about the interaction between humans and

1023
00:37:03,119 --> 00:37:08,079
machines. And the first is that you can

1024
00:37:06,320 --> 00:37:10,720
automate too much. And when you automate

1025
00:37:08,079 --> 00:37:13,680
too much, what that does is it reduces

1026
00:37:10,720 --> 00:37:16,000
the situational awareness of the person

1027
00:37:13,680 --> 00:37:18,160
such that they become worse at their job

1028
00:37:16,000 --> 00:37:19,760
when you really need them to be expert.

1029
00:37:18,160 --> 00:37:22,079
So when something goes wrong in the

1030
00:37:19,760 --> 00:37:23,680
mind, you need the operator to have

1031
00:37:22,079 --> 00:37:25,119
excellent skills and a complete

1032
00:37:23,680 --> 00:37:27,520
awareness of what's going on in the

1033
00:37:25,119 --> 00:37:29,440
system and you you can't automate so

1034
00:37:27,520 --> 00:37:30,800
much that their awareness goes down and

1035
00:37:29,440 --> 00:37:32,640
they don't have access to the right data

1036
00:37:30,800 --> 00:37:34,000
that they can interpret. So maintaining

1037
00:37:32,640 --> 00:37:36,240
situational awareness is really

1038
00:37:34,000 --> 00:37:38,079
important even if you have to sacrifice

1039
00:37:36,240 --> 00:37:39,760
some automation and and the example of

1040
00:37:38,079 --> 00:37:41,440
of cockpits for example, if you've been

1041
00:37:39,760 --> 00:37:43,200
in a cockpit recently, you know, they

1042
00:37:41,440 --> 00:37:44,640
don't have a bunch of, you know, tablet

1043
00:37:43,200 --> 00:37:46,240
screens that are perfectly automated.

1044
00:37:44,640 --> 00:37:48,079
the plane really could fly itself but

1045
00:37:46,240 --> 00:37:50,000
they've dialed back in the automation

1046
00:37:48,079 --> 00:37:52,560
particularly because they want the pilot

1047
00:37:50,000 --> 00:37:55,680
to be locked in and very skilled when

1048
00:37:52,560 --> 00:37:57,280
they need the pilot. The second is that

1049
00:37:55,680 --> 00:37:59,040
the you know good human machine

1050
00:37:57,280 --> 00:38:00,960
interaction really requires a balance of

1051
00:37:59,040 --> 00:38:03,119
the cognitive load. You don't want to

1052
00:38:00,960 --> 00:38:04,560
overload the human with information such

1053
00:38:03,119 --> 00:38:07,280
that they can't process it and they kind

1054
00:38:04,560 --> 00:38:09,839
of throw their hands up. But you also

1055
00:38:07,280 --> 00:38:11,680
don't want the the human to be on

1056
00:38:09,839 --> 00:38:13,520
essentially autopilot and just clicking

1057
00:38:11,680 --> 00:38:16,160
yes or clicking no and in response to

1058
00:38:13,520 --> 00:38:18,560
certain tasks that humans human workers

1059
00:38:16,160 --> 00:38:20,480
who are engaged in that middle range

1060
00:38:18,560 --> 00:38:22,400
find their jobs more fulfilling and are

1061
00:38:20,480 --> 00:38:24,000
also more able to you know maintain the

1062
00:38:22,400 --> 00:38:26,000
situational awareness that's required in

1063
00:38:24,000 --> 00:38:27,920
an emergency. And then finally, and and

1064
00:38:26,000 --> 00:38:30,320
I I touched on this that you really need

1065
00:38:27,920 --> 00:38:32,480
to guard against skill atrophy that in

1066
00:38:30,320 --> 00:38:33,920
order to use technology successfully,

1067
00:38:32,480 --> 00:38:35,200
the the organizations that we've seen

1068
00:38:33,920 --> 00:38:36,880
scale up their automation are really

1069
00:38:35,200 --> 00:38:38,960
learning organizations, they continue to

1070
00:38:36,880 --> 00:38:40,720
invest in the skills and capabilities of

1071
00:38:38,960 --> 00:38:43,599
their workers such that they can avoid

1072
00:38:40,720 --> 00:38:46,240
the potential trade-offs of these tools.

1073
00:38:43,599 --> 00:38:48,560
And finally, I want to, you know, if I

1074
00:38:46,240 --> 00:38:50,320
leave you with one point today, I I want

1075
00:38:48,560 --> 00:38:52,640
to, you know, make this, I think,

1076
00:38:50,320 --> 00:38:54,480
counterintuitive argument that the

1077
00:38:52,640 --> 00:38:56,079
success for an organization using

1078
00:38:54,480 --> 00:38:57,920
technology

1079
00:38:56,079 --> 00:38:59,520
is it has something to do with the

1080
00:38:57,920 --> 00:39:00,880
technology. Yes. But you know, your

1081
00:38:59,520 --> 00:39:02,960
organization and your competitor

1082
00:39:00,880 --> 00:39:05,200
organizations in many ways, they have

1083
00:39:02,960 --> 00:39:06,800
access to the same technology tools,

1084
00:39:05,200 --> 00:39:08,720
right? The a lot of the technologies

1085
00:39:06,800 --> 00:39:10,640
that organizations are adopting are

1086
00:39:08,720 --> 00:39:12,320
commodities and your comparative

1087
00:39:10,640 --> 00:39:14,079
advantage isn't with the technology

1088
00:39:12,320 --> 00:39:15,599
necessarily. it's with your people and

1089
00:39:14,079 --> 00:39:17,119
how your people can adapt the

1090
00:39:15,599 --> 00:39:18,800
technology, the skills they have to use

1091
00:39:17,119 --> 00:39:21,280
it effectively and the skills they have

1092
00:39:18,800 --> 00:39:23,200
really to translate it into your domain.

1093
00:39:21,280 --> 00:39:26,400
So that that's really critically

1094
00:39:23,200 --> 00:39:28,079
important here. And and and finally, you

1095
00:39:26,400 --> 00:39:30,720
know, the the image that we started with

1096
00:39:28,079 --> 00:39:32,960
with one person in a control room, we

1097
00:39:30,720 --> 00:39:36,560
know that that's not the best scenario

1098
00:39:32,960 --> 00:39:38,720
in part because one person alone rarely

1099
00:39:36,560 --> 00:39:40,960
can be as innovative and and successful

1100
00:39:38,720 --> 00:39:42,480
as many people working together. And and

1101
00:39:40,960 --> 00:39:44,880
this is the kind of the evidence of of

1102
00:39:42,480 --> 00:39:46,720
knowledge spillovers that we have from

1103
00:39:44,880 --> 00:39:48,160
all different types of industries, all

1104
00:39:46,720 --> 00:39:49,920
different types of environments that

1105
00:39:48,160 --> 00:39:52,000
when you get a lot of talented people

1106
00:39:49,920 --> 00:39:53,599
with complimentary expertise together,

1107
00:39:52,000 --> 00:39:55,760
we don't really know why it happens, but

1108
00:39:53,599 --> 00:39:57,599
great creative things tend to happen. So

1109
00:39:55,760 --> 00:39:59,280
even in an automated environment, having

1110
00:39:57,599 --> 00:40:01,200
people close together exchanging

1111
00:39:59,280 --> 00:40:03,920
information, learning from each other

1112
00:40:01,200 --> 00:40:05,599
really remains critical and and I I hope

1113
00:40:03,920 --> 00:40:07,119
will remain critical well into the

1114
00:40:05,599 --> 00:40:10,839
future. So thank you so much and I would

1115
00:40:07,119 --> 00:40:10,839
be glad to take your questions.

1116
00:40:14,480 --> 00:40:23,359
Thank you Ben for your presentation. We

1117
00:40:18,480 --> 00:40:25,599
have some questions for you now. Please

1118
00:40:23,359 --> 00:40:26,400
put this for the questions.

1119
00:40:25,599 --> 00:40:28,640
>> I think so.

1120
00:40:26,400 --> 00:40:30,240
>> Okay. It's okay.

1121
00:40:28,640 --> 00:40:31,440
>> Yeah.

1122
00:40:30,240 --> 00:40:34,440
>> Are you okay? Yeah.

1123
00:40:31,440 --> 00:40:34,440
>> Yeah.

1124
00:40:34,800 --> 00:40:40,560
We're going to hear questions now.

1125
00:40:43,760 --> 00:40:49,839
Ben Armstrong,

1126
00:40:46,720 --> 00:40:53,599
executive director of MIT. Okay, we have

1127
00:40:49,839 --> 00:40:56,000
some questions for you now, Ben.

1128
00:40:53,599 --> 00:40:59,920
The question is,

1129
00:40:56,000 --> 00:41:02,720
how can we move forward

1130
00:40:59,920 --> 00:41:04,400
in knowing the skills that will be

1131
00:41:02,720 --> 00:41:08,920
required

1132
00:41:04,400 --> 00:41:08,920
in an uncertain world?

1133
00:41:09,440 --> 00:41:12,440
So

1134
00:41:12,480 --> 00:41:17,920
again with the caveat that uh I have

1135
00:41:15,920 --> 00:41:19,760
very little capability in predicting the

1136
00:41:17,920 --> 00:41:22,800
future but I I instead would like to

1137
00:41:19,760 --> 00:41:25,040
think of preparing for the future. I

1138
00:41:22,800 --> 00:41:28,160
agree that there's that we have an

1139
00:41:25,040 --> 00:41:30,319
uncertain world in terms of what are

1140
00:41:28,160 --> 00:41:33,119
going to be the dominant technologies of

1141
00:41:30,319 --> 00:41:35,040
10 to 20 to 30 years from now. What will

1142
00:41:33,119 --> 00:41:36,319
be the dominant designs of those

1143
00:41:35,040 --> 00:41:37,359
technologies? you know, we have large

1144
00:41:36,319 --> 00:41:38,880
language models, but we don't really

1145
00:41:37,359 --> 00:41:40,160
know how we're going to be using them in

1146
00:41:38,880 --> 00:41:41,839
the long term. I think a lot of the ways

1147
00:41:40,160 --> 00:41:43,839
that we use computers and the internet

1148
00:41:41,839 --> 00:41:46,000
today, we couldn't have anticipated when

1149
00:41:43,839 --> 00:41:48,400
when they were first introduced. But

1150
00:41:46,000 --> 00:41:51,119
with that uncertainty, we also know that

1151
00:41:48,400 --> 00:41:53,440
there have been certain kind of skills

1152
00:41:51,119 --> 00:41:54,640
and important practices of organizations

1153
00:41:53,440 --> 00:41:58,160
that have been quite consistent

1154
00:41:54,640 --> 00:41:59,280
throughout time. And one of those um has

1155
00:41:58,160 --> 00:42:01,040
to do with creativity and

1156
00:41:59,280 --> 00:42:02,960
entrepreneurship. So if you look at

1157
00:42:01,040 --> 00:42:04,400
entrepreneurial practices around

1158
00:42:02,960 --> 00:42:07,119
experimentation and continuous

1159
00:42:04,400 --> 00:42:08,800
improvement going back to the 1800s, a

1160
00:42:07,119 --> 00:42:09,920
lot of the entrepreneurs and and how

1161
00:42:08,800 --> 00:42:12,319
they approached building their

1162
00:42:09,920 --> 00:42:13,680
businesses um in the late 1800s was

1163
00:42:12,319 --> 00:42:15,760
actually it's actually quite similar to

1164
00:42:13,680 --> 00:42:17,520
how tech entrepreneurs did in the 2000s

1165
00:42:15,760 --> 00:42:19,520
and how hard tech entrepreneurs like

1166
00:42:17,520 --> 00:42:21,280
some of the folks you've seen today um

1167
00:42:19,520 --> 00:42:23,520
are approaching their businesses. So

1168
00:42:21,280 --> 00:42:25,599
some of those types of uh skills and

1169
00:42:23,520 --> 00:42:27,280
approaches to growing a business and and

1170
00:42:25,599 --> 00:42:29,440
experimentation I think is the key skill

1171
00:42:27,280 --> 00:42:31,040
I have mentioned today are they seem

1172
00:42:29,440 --> 00:42:33,599
timeless at least they've been timeless

1173
00:42:31,040 --> 00:42:36,079
uh for the last 100 plus years and and

1174
00:42:33,599 --> 00:42:37,920
that kind of that reasoning from history

1175
00:42:36,079 --> 00:42:39,440
is how I think we could be best prepared

1176
00:42:37,920 --> 00:42:43,160
for an uncertain future.

1177
00:42:39,440 --> 00:42:43,160
>> Thank you Rodrigo.

1178
00:42:43,920 --> 00:42:51,040
The second question for you, Ben.

1179
00:42:47,839 --> 00:42:53,119
After visiting our sites, what do you

1180
00:42:51,040 --> 00:42:55,520
think about our sites? How do we

1181
00:42:53,119 --> 00:42:58,520
progress with more technology to be more

1182
00:42:55,520 --> 00:42:58,520
productive?

1183
00:42:58,880 --> 00:43:03,680
>> So, it was a it was a great pleasure to

1184
00:43:00,720 --> 00:43:06,160
visit um your site yesterday. I I told

1185
00:43:03,680 --> 00:43:07,839
my my wife and kids that it felt a bit

1186
00:43:06,160 --> 00:43:10,400
like being on Mars where there was a

1187
00:43:07,839 --> 00:43:13,040
sense of kind of exploration and it was

1188
00:43:10,400 --> 00:43:16,079
uh like nothing I had seen before. Um

1189
00:43:13,040 --> 00:43:18,319
and it was very exciting to see. Now

1190
00:43:16,079 --> 00:43:20,079
with that said, you know, when we were

1191
00:43:18,319 --> 00:43:21,440
watching the autonomous vehicles, and I

1192
00:43:20,079 --> 00:43:24,079
think it was different yesterday than it

1193
00:43:21,440 --> 00:43:26,640
normally is, um I saw a lot of the

1194
00:43:24,079 --> 00:43:30,720
similar kind of um similar challenges

1195
00:43:26,640 --> 00:43:33,839
with autonomy or or similar um maybe uh

1196
00:43:30,720 --> 00:43:35,599
a similar design to how we see

1197
00:43:33,839 --> 00:43:38,079
autonomous mobile robots deployed in

1198
00:43:35,599 --> 00:43:40,000
warehouses where sometimes they they get

1199
00:43:38,079 --> 00:43:41,839
on the same route and they're less

1200
00:43:40,000 --> 00:43:45,280
flexible than they could be. So, one of

1201
00:43:41,839 --> 00:43:47,280
the questions I have is how do you get

1202
00:43:45,280 --> 00:43:49,359
from in a mining environment, which I

1203
00:43:47,280 --> 00:43:51,119
know very little about mining, um, but I

1204
00:43:49,359 --> 00:43:52,960
I I know a little bit about automation,

1205
00:43:51,119 --> 00:43:54,880
how would you take a deterministic

1206
00:43:52,960 --> 00:43:57,119
automated system and try to make it more

1207
00:43:54,880 --> 00:43:59,680
flexible? And one way you do that is by

1208
00:43:57,119 --> 00:44:02,319
adding kind of new data and new skills

1209
00:43:59,680 --> 00:44:03,440
into that system and and kind of you

1210
00:44:02,319 --> 00:44:05,280
could do it with software, not

1211
00:44:03,440 --> 00:44:06,400
necessarily with with uh with new

1212
00:44:05,280 --> 00:44:08,240
hardware, which I know that that

1213
00:44:06,400 --> 00:44:10,640
hardware is very expensive and very

1214
00:44:08,240 --> 00:44:12,240
capable um, as it stands. I don't know

1215
00:44:10,640 --> 00:44:14,640
if that's a satisfying question, but I

1216
00:44:12,240 --> 00:44:15,920
I'm I I think I'm still processing and

1217
00:44:14,640 --> 00:44:17,280
learning from the visit, and I I look

1218
00:44:15,920 --> 00:44:19,359
forward to talking to some of the

1219
00:44:17,280 --> 00:44:21,440
colleagues here today to to put more

1220
00:44:19,359 --> 00:44:24,560
context around what we saw.

1221
00:44:21,440 --> 00:44:26,480
>> Ben Armstrong,

1222
00:44:24,560 --> 00:44:29,440
thank you for your answers. Thank you

1223
00:44:26,480 --> 00:44:32,440
for your words.

1224
00:44:29,440 --> 00:44:32,440
Armstrong

