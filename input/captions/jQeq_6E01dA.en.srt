1
00:00:01,460 --> 00:00:17,199
[Music]

2
00:00:15,440 --> 00:00:18,960
The floor is all yours. Thanks. Thank

3
00:00:17,199 --> 00:00:22,800
you.

4
00:00:18,960 --> 00:00:25,680
Well, welcome back. Um, our next uh

5
00:00:22,800 --> 00:00:27,920
presenter is Bruce Lawler. He's managing

6
00:00:25,680 --> 00:00:31,199
director of MIT's machine intelligence

7
00:00:27,920 --> 00:00:33,120
for manufacturing operations. Today he

8
00:00:31,199 --> 00:00:36,000
will explore with us practical and

9
00:00:33,120 --> 00:00:38,800
successful applications of analytics and

10
00:00:36,000 --> 00:00:42,239
AI and manufacturing across small,

11
00:00:38,800 --> 00:00:44,719
medium and large enterprises in the US.

12
00:00:42,239 --> 00:00:47,039
He will show us where to invest to start

13
00:00:44,719 --> 00:00:49,680
and sustain a digital transformation

14
00:00:47,039 --> 00:00:53,520
journey drawing on research and best

15
00:00:49,680 --> 00:00:56,239
practices from MIT and McKenzie. He'll

16
00:00:53,520 --> 00:00:59,280
also give us insight into impactful uses

17
00:00:56,239 --> 00:01:01,160
of generative AI currently shaping the

18
00:00:59,280 --> 00:01:04,560
future that will drive future

19
00:01:01,160 --> 00:01:06,200
investment. Please welcome Bruce Lawler.

20
00:01:04,560 --> 00:01:24,620
[Applause]

21
00:01:06,200 --> 00:01:24,620
[Music]

22
00:01:24,650 --> 00:01:31,240
[Applause]

23
00:01:29,840 --> 00:01:34,240
Good morning

24
00:01:31,240 --> 00:01:36,960
everyone. Thank you. So I'm going to

25
00:01:34,240 --> 00:01:38,960
show you the MIT fire hose. So when you

26
00:01:36,960 --> 00:01:41,119
come to MIT they tell you it's like

27
00:01:38,960 --> 00:01:43,040
learning uh learning there is like

28
00:01:41,119 --> 00:01:44,560
drinking from a fire hose. So I'm going

29
00:01:43,040 --> 00:01:46,720
to go through a bunch of slides pretty

30
00:01:44,560 --> 00:01:48,720
quickly. feel free to take pictures, but

31
00:01:46,720 --> 00:01:50,640
I just want to let you know that each

32
00:01:48,720 --> 00:01:52,159
slide's only going to be up there for a

33
00:01:50,640 --> 00:01:53,799
very short amount of time if there's one

34
00:01:52,159 --> 00:01:58,560
that you

35
00:01:53,799 --> 00:02:00,399
like. So, uh, first of all, MIT MIMO or

36
00:01:58,560 --> 00:02:03,200
machine intelligence and manufacturing

37
00:02:00,399 --> 00:02:07,200
operations. We started this in in 2018

38
00:02:03,200 --> 00:02:09,440
before AI was cool. Um and our focus is

39
00:02:07,200 --> 00:02:11,520
really how do we bring AI into

40
00:02:09,440 --> 00:02:14,640
manufacturing and operations and we have

41
00:02:11,520 --> 00:02:17,840
three major uh pillars research,

42
00:02:14,640 --> 00:02:19,360
education and collaboration. Uh so today

43
00:02:17,840 --> 00:02:21,920
I'll share some of our research.

44
00:02:19,360 --> 00:02:23,520
Obviously this is uh this seminar is

45
00:02:21,920 --> 00:02:25,360
about education and obviously

46
00:02:23,520 --> 00:02:30,360
collaboration. So I'm happy to talk with

47
00:02:25,360 --> 00:02:30,360
all of you uh at at our breaks.

48
00:02:30,400 --> 00:02:34,959
So today first I'm going to give you a

49
00:02:32,319 --> 00:02:38,560
brief history and a current status of of

50
00:02:34,959 --> 00:02:40,800
AI and and more in the consumer uh space

51
00:02:38,560 --> 00:02:43,360
uh because it's moving so fast and and

52
00:02:40,800 --> 00:02:45,920
what I'm trying to show you there is how

53
00:02:43,360 --> 00:02:48,720
quickly uh capabilities are increasing

54
00:02:45,920 --> 00:02:51,680
and cost is decreasing as kind of a

55
00:02:48,720 --> 00:02:54,319
forcing function for for what's going to

56
00:02:51,680 --> 00:02:57,280
then move as a wave into manufacturing

57
00:02:54,319 --> 00:03:00,080
in the next five years. Um we recently

58
00:02:57,280 --> 00:03:02,080
published in Harvard Business Review a a

59
00:03:00,080 --> 00:03:04,239
article called what companies succeeding

60
00:03:02,080 --> 00:03:06,560
in AI do different Differently. So we'll

61
00:03:04,239 --> 00:03:09,040
talk a little bit about uh those results

62
00:03:06,560 --> 00:03:10,720
and uh which of those things you can

63
00:03:09,040 --> 00:03:12,959
implement

64
00:03:10,720 --> 00:03:14,480
um and specifically where they invest.

65
00:03:12,959 --> 00:03:16,480
So for those of you who thought I was

66
00:03:14,480 --> 00:03:18,800
going to tell you the next Nvidia to

67
00:03:16,480 --> 00:03:22,000
invest in, no, I'm telling you how to

68
00:03:18,800 --> 00:03:24,159
invest in your own businesses uh to uh

69
00:03:22,000 --> 00:03:26,560
to make them more productive. and then

70
00:03:24,159 --> 00:03:29,599
we'll we'll go through a bunch of uh use

71
00:03:26,560 --> 00:03:31,519
cases primarily and a few case studies

72
00:03:29,599 --> 00:03:33,440
uh that we can uh that you can learn

73
00:03:31,519 --> 00:03:36,319
from.

74
00:03:33,440 --> 00:03:38,760
So first of all uh history of AI it it's

75
00:03:36,319 --> 00:03:42,000
had a long history

76
00:03:38,760 --> 00:03:44,560
um started back in the the late 50s

77
00:03:42,000 --> 00:03:46,879
early 60s there's been a couple of

78
00:03:44,560 --> 00:03:49,760
winters here uh this is kind of a

79
00:03:46,879 --> 00:03:53,680
symbolic phase uh this is more of an

80
00:03:49,760 --> 00:03:56,799
expert system kind of phase and so I put

81
00:03:53,680 --> 00:03:58,400
some kind of uh some milestones in there

82
00:03:56,799 --> 00:04:00,400
so the neural network the thing that

83
00:03:58,400 --> 00:04:02,000
really powers the technically the

84
00:04:00,400 --> 00:04:05,439
perceptron

85
00:04:02,000 --> 00:04:07,360
uh was invented uh in 1944. So the

86
00:04:05,439 --> 00:04:10,319
concept's actually pretty old. It really

87
00:04:07,360 --> 00:04:13,200
took until now to have the compute power

88
00:04:10,319 --> 00:04:15,799
uh data and uh more advanced algorithms

89
00:04:13,200 --> 00:04:19,519
to to power it.

90
00:04:15,799 --> 00:04:22,240
Um many of you remember Deep Blue. Uh

91
00:04:19,519 --> 00:04:24,160
and obviously this was a moment that I

92
00:04:22,240 --> 00:04:26,400
think most of you probably didn't

93
00:04:24,160 --> 00:04:28,800
realize uh transformers were invented by

94
00:04:26,400 --> 00:04:31,040
at Google. Uh but then the introduction

95
00:04:28,800 --> 00:04:33,680
of chat GPT is kind of what's kicked off

96
00:04:31,040 --> 00:04:36,400
this current phase and things have moved

97
00:04:33,680 --> 00:04:38,160
pretty quickly since then. Um we've got

98
00:04:36,400 --> 00:04:39,840
a lot of capabilities. I'll show you a

99
00:04:38,160 --> 00:04:41,360
couple examples. These multimodalm

100
00:04:39,840 --> 00:04:43,720
models are some that I think are

101
00:04:41,360 --> 00:04:46,440
extremely powerful.

102
00:04:43,720 --> 00:04:48,560
Um especially in manufacturing and

103
00:04:46,440 --> 00:04:52,639
operations. Uh what we see is we're

104
00:04:48,560 --> 00:04:55,680
getting a lot of choices in 2024. Um we

105
00:04:52,639 --> 00:04:58,479
also saw openweight models. they existed

106
00:04:55,680 --> 00:05:00,960
previously from meta uh deepseek many of

107
00:04:58,479 --> 00:05:03,040
you have heard of uh so that is really

108
00:05:00,960 --> 00:05:06,479
driving down price and increasing

109
00:05:03,040 --> 00:05:08,960
capability but also openness open weight

110
00:05:06,479 --> 00:05:11,600
models uh the sharing and this is unlike

111
00:05:08,960 --> 00:05:13,360
any most other technologies that have

112
00:05:11,600 --> 00:05:15,520
been created is the amount of sharing

113
00:05:13,360 --> 00:05:18,160
and availability uh globally for these

114
00:05:15,520 --> 00:05:20,160
tools and then in 2025 we've seen some

115
00:05:18,160 --> 00:05:22,400
really interesting things uh with

116
00:05:20,160 --> 00:05:25,199
reasoning models uh we've already talked

117
00:05:22,400 --> 00:05:28,080
a little bit about agentic AI AI. Uh,

118
00:05:25,199 --> 00:05:31,120
and then for those of you who have seen

119
00:05:28,080 --> 00:05:33,360
some of the computer use capabilities,

120
00:05:31,120 --> 00:05:36,160
uh, certainly from Enthropic and others,

121
00:05:33,360 --> 00:05:38,320
you can now install it on your machine.

122
00:05:36,160 --> 00:05:40,080
It can read files, it can execute

123
00:05:38,320 --> 00:05:42,160
commands on your computer. So,

124
00:05:40,080 --> 00:05:45,280
literally, uh, can take over the screen

125
00:05:42,160 --> 00:05:47,120
on your computer. So, um, you know,

126
00:05:45,280 --> 00:05:50,560
these capabilities are all coming very

127
00:05:47,120 --> 00:05:52,400
very quickly in in in the ability to

128
00:05:50,560 --> 00:05:54,000
automate work. And it it's very

129
00:05:52,400 --> 00:05:56,080
different from robotic process

130
00:05:54,000 --> 00:05:59,360
automation that's come before it. So

131
00:05:56,080 --> 00:06:01,880
just a couple of examples. Um I don't

132
00:05:59,360 --> 00:06:06,160
how many people have played with chat

133
00:06:01,880 --> 00:06:07,840
GPT Dolly. Right. So uh I I'm going to

134
00:06:06,160 --> 00:06:09,680
give this a prompt. I'm say change the

135
00:06:07,840 --> 00:06:11,639
flowers to an orange color. What do you

136
00:06:09,680 --> 00:06:14,120
think's going to

137
00:06:11,639 --> 00:06:16,560
happen? It's going to make those orange,

138
00:06:14,120 --> 00:06:18,400
right? No, it actually puts orange

139
00:06:16,560 --> 00:06:20,479
flowers in. So these are the kinds of

140
00:06:18,400 --> 00:06:22,720
real world training things that these

141
00:06:20,479 --> 00:06:24,080
models are capable of. And this is

142
00:06:22,720 --> 00:06:27,360
really for those of you who've been kind

143
00:06:24,080 --> 00:06:30,240
of sleeping under a rock. Uh this one um

144
00:06:27,360 --> 00:06:32,319
you know I think is is just a a couple

145
00:06:30,240 --> 00:06:34,880
of examples of how these models are

146
00:06:32,319 --> 00:06:37,360
learning physics. So you normally see

147
00:06:34,880 --> 00:06:40,240
you think that was a generated image.

148
00:06:37,360 --> 00:06:42,800
Now these images also we can train them

149
00:06:40,240 --> 00:06:45,039
with uh physics and Nvidia's done a ton

150
00:06:42,800 --> 00:06:47,199
of work there where uh we don't have to

151
00:06:45,039 --> 00:06:48,800
do finite element analysis anymore. We

152
00:06:47,199 --> 00:06:51,680
can actually just train a neural network

153
00:06:48,800 --> 00:06:54,000
with physics uh and it can do these kind

154
00:06:51,680 --> 00:06:55,680
of things and obviously a little ad

155
00:06:54,000 --> 00:06:58,720
there

156
00:06:55,680 --> 00:07:01,280
um from from Amazon to give you an

157
00:06:58,720 --> 00:07:04,319
example of of how pervasive these things

158
00:07:01,280 --> 00:07:07,440
have been. Um for those of you who don't

159
00:07:04,319 --> 00:07:11,360
know that these models can code um they

160
00:07:07,440 --> 00:07:13,919
can. So Andrew Carpath uh was famous two

161
00:07:11,360 --> 00:07:16,960
years ago for saying English is now the

162
00:07:13,919 --> 00:07:19,199
most powerful computing language. Uh so

163
00:07:16,960 --> 00:07:21,440
what this is and I apologize for the bad

164
00:07:19,199 --> 00:07:24,319
grammar here. I didn't write that. Uh so

165
00:07:21,440 --> 00:07:27,919
it's essentially asking a prompt to

166
00:07:24,319 --> 00:07:29,560
build an app. Um it's now writing

167
00:07:27,919 --> 00:07:32,479
JavaScript.

168
00:07:29,560 --> 00:07:35,280
Uh and uh the nice thing about the way

169
00:07:32,479 --> 00:07:37,440
uh Enthropic has interviewed their or

170
00:07:35,280 --> 00:07:39,520
developed their product is it can now

171
00:07:37,440 --> 00:07:43,360
actually run in the same browser. So

172
00:07:39,520 --> 00:07:46,639
it's kicked off um that JavaScript and

173
00:07:43,360 --> 00:07:50,720
it's a simple app. It just paste your um

174
00:07:46,639 --> 00:07:54,639
your text in there and uh depending on

175
00:07:50,720 --> 00:07:56,240
the speed at which you want to talk um

176
00:07:54,639 --> 00:07:58,879
you know it will tell you how long your

177
00:07:56,240 --> 00:08:01,280
talk is going to take.

178
00:07:58,879 --> 00:08:05,120
Nothing earthshattering about that app

179
00:08:01,280 --> 00:08:06,639
other than it wrote the code itself. It

180
00:08:05,120 --> 00:08:09,680
it didn't have to compile it, but it's

181
00:08:06,639 --> 00:08:11,520
running it here in the browser and uh it

182
00:08:09,680 --> 00:08:14,240
picked those four levels. I didn't tell

183
00:08:11,520 --> 00:08:17,199
it four pick four levels. I didn't tell

184
00:08:14,240 --> 00:08:19,360
it what uh speed and words per minute.

185
00:08:17,199 --> 00:08:20,639
It figured all of that out for me. And

186
00:08:19,360 --> 00:08:23,120
so you start to think about the

187
00:08:20,639 --> 00:08:24,800
implications of that coming into

188
00:08:23,120 --> 00:08:27,039
manufacturing.

189
00:08:24,800 --> 00:08:29,360
uh and and essentially we'll talk a lot

190
00:08:27,039 --> 00:08:32,240
about the front office today the types

191
00:08:29,360 --> 00:08:34,719
of tasks people do

192
00:08:32,240 --> 00:08:36,120
um how many have heard with re heard of

193
00:08:34,719 --> 00:08:39,760
reasoning

194
00:08:36,120 --> 00:08:44,080
models how many have used them right

195
00:08:39,760 --> 00:08:46,720
okay good so here's an example of um a

196
00:08:44,080 --> 00:08:52,000
reasoning model once again this is just

197
00:08:46,720 --> 00:08:55,600
using um sonnet 3.7 here um so this is

198
00:08:52,000 --> 00:08:57,760
the text that's given to the model and

199
00:08:55,600 --> 00:08:59,920
and what reasoning models are is they

200
00:08:57,760 --> 00:09:02,440
take large language models and they call

201
00:08:59,920 --> 00:09:05,279
them repeatedly

202
00:09:02,440 --> 00:09:07,040
um to ask it different questions and

203
00:09:05,279 --> 00:09:08,880
essentially reason through things and it

204
00:09:07,040 --> 00:09:10,120
takes the output of one answer and uses

205
00:09:08,880 --> 00:09:12,720
it to drive

206
00:09:10,120 --> 00:09:15,440
another. Um many of you also probably

207
00:09:12,720 --> 00:09:18,240
heard that um large language models

208
00:09:15,440 --> 00:09:20,720
can't do math and they cannot but they

209
00:09:18,240 --> 00:09:22,640
can generate the code that does math. So

210
00:09:20,720 --> 00:09:24,959
now it's going to go through it's going

211
00:09:22,640 --> 00:09:27,920
to run the calculations that I asked it

212
00:09:24,959 --> 00:09:30,880
to run. Um, it's going to do the

213
00:09:27,920 --> 00:09:33,279
analysis that I asked it to do and

214
00:09:30,880 --> 00:09:34,959
essentially in 24 seconds it's going to

215
00:09:33,279 --> 00:09:37,360
do what probably would have taken a

216
00:09:34,959 --> 00:09:39,120
human and a half a day or a day to do.

217
00:09:37,360 --> 00:09:41,680
Now, you need to think of it as an

218
00:09:39,120 --> 00:09:44,880
intern. It's not necessarily going to be

219
00:09:41,680 --> 00:09:46,560
right. You should check it. Um, but the

220
00:09:44,880 --> 00:09:48,480
point is these things are happening

221
00:09:46,560 --> 00:09:51,120
faster and faster. So, as you're looking

222
00:09:48,480 --> 00:09:55,680
at developing uh production schedules,

223
00:09:51,120 --> 00:09:57,680
quality manuals, uh routing sheets, um

224
00:09:55,680 --> 00:10:00,480
things like that that you use every day

225
00:09:57,680 --> 00:10:03,920
in manufacturing, think about these kind

226
00:10:00,480 --> 00:10:06,000
of tools, not necessarily today, but

227
00:10:03,920 --> 00:10:10,080
very, very soon being able to help with

228
00:10:06,000 --> 00:10:12,240
those kinds of of tasks.

229
00:10:10,080 --> 00:10:15,519
The other big thing that's important

230
00:10:12,240 --> 00:10:19,600
because cost is super important is

231
00:10:15,519 --> 00:10:21,920
really the decline in cost. So, um, for

232
00:10:19,600 --> 00:10:25,839
equivalent LLM performance, we're seeing

233
00:10:21,920 --> 00:10:28,959
about a 10x decline per year in cost. By

234
00:10:25,839 --> 00:10:30,480
comparison, Morris law is about 2x. So,

235
00:10:28,959 --> 00:10:33,120
that's a huge difference when you

236
00:10:30,480 --> 00:10:34,959
compound that year over year over year.

237
00:10:33,120 --> 00:10:39,079
And so, this was done by Andre and

238
00:10:34,959 --> 00:10:42,959
Horowitz. that, you know, in 2022 GPT uh

239
00:10:39,079 --> 00:10:46,920
cost uh $60

240
00:10:42,959 --> 00:10:49,279
um for hitting a benchmark of

241
00:10:46,920 --> 00:10:51,560
MMLU42. It's just a human performance

242
00:10:49,279 --> 00:10:54,720
benchmark, but they set that as the

243
00:10:51,560 --> 00:10:57,120
constant. Um you know, by 2024,

244
00:10:54,720 --> 00:10:59,200
essentially that cost was zero in many

245
00:10:57,120 --> 00:11:03,279
ways driven by these openweight models.

246
00:10:59,200 --> 00:11:05,040
So uh if you look at we've uh

247
00:11:03,279 --> 00:11:07,200
uh you know we've had a thousandx

248
00:11:05,040 --> 00:11:10,079
decrease in three years for comparable

249
00:11:07,200 --> 00:11:13,360
performance. Now what people are asking

250
00:11:10,079 --> 00:11:15,120
it to do has gone up and up and up. Uh

251
00:11:13,360 --> 00:11:17,600
like these reasoning models you know

252
00:11:15,120 --> 00:11:20,279
that model ran for 24 seconds that I

253
00:11:17,600 --> 00:11:22,959
just showed you. So it's

254
00:11:20,279 --> 00:11:24,880
um the you know is everything the

255
00:11:22,959 --> 00:11:27,279
requirements keep going up but the cost

256
00:11:24,880 --> 00:11:29,040
is going down. And to put that in human

257
00:11:27,279 --> 00:11:31,680
terms, if I were to stand up here and

258
00:11:29,040 --> 00:11:33,839
talk for 10 hours a day for a year, you

259
00:11:31,680 --> 00:11:36,800
could summarize everything that I said

260
00:11:33,839 --> 00:11:38,560
for $2, which probably everyone would

261
00:11:36,800 --> 00:11:41,279
say that's really worth it rather than

262
00:11:38,560 --> 00:11:43,680
listening to you for a year, 10 hours a

263
00:11:41,279 --> 00:11:43,680
day.

264
00:11:44,920 --> 00:11:51,839
So now, if I'm Corona, I want to make

265
00:11:48,480 --> 00:11:53,200
ads for all over the world. I could

266
00:11:51,839 --> 00:11:54,880
obviously send people to all these

267
00:11:53,200 --> 00:11:57,640
different beaches. That would be nice, a

268
00:11:54,880 --> 00:12:00,399
nice job to have, but pretty

269
00:11:57,640 --> 00:12:03,600
expensive. And I could do all those beer

270
00:12:00,399 --> 00:12:06,480
ads. And this is not surprising in in

271
00:12:03,600 --> 00:12:08,920
today's age. What do you think it costs

272
00:12:06,480 --> 00:12:11,920
to generate all those different

273
00:12:08,920 --> 00:12:11,920
images?

274
00:12:14,440 --> 00:12:20,959
Anyone, a US cent, euro cent, whatever,

275
00:12:18,240 --> 00:12:23,760
roughly the same these days. So I think

276
00:12:20,959 --> 00:12:25,920
it's it's that price performance that

277
00:12:23,760 --> 00:12:27,519
people don't fully grasp and the change

278
00:12:25,920 --> 00:12:28,760
of that and how you could bring that

279
00:12:27,519 --> 00:12:31,760
into your

280
00:12:28,760 --> 00:12:31,760
operations.

281
00:12:31,959 --> 00:12:36,720
So all right I'm going to talk a little

282
00:12:34,320 --> 00:12:38,519
bit now about the research that that we

283
00:12:36,720 --> 00:12:42,240
do.

284
00:12:38,519 --> 00:12:44,160
Um so the the problem that that I think

285
00:12:42,240 --> 00:12:46,160
exists I don't worry about worker

286
00:12:44,160 --> 00:12:49,279
displacement. What I worry about is not

287
00:12:46,160 --> 00:12:51,360
having enough workers. So in the US our

288
00:12:49,279 --> 00:12:55,600
uh manufacturing unemployment rates very

289
00:12:51,360 --> 00:12:58,160
low. We have about as of last year June

290
00:12:55,600 --> 00:13:00,800
we had about uh you know 600,000

291
00:12:58,160 --> 00:13:03,839
unfilled jobs. We think in the next uh

292
00:13:00,800 --> 00:13:07,120
four years five years you know there'll

293
00:13:03,839 --> 00:13:09,360
be 2 million jobs uh that are unfilled

294
00:13:07,120 --> 00:13:12,000
and other people have mentioned that you

295
00:13:09,360 --> 00:13:14,560
know we're losing about 11,000 Americans

296
00:13:12,000 --> 00:13:16,959
a day to retirement. I'm guessing you

297
00:13:14,560 --> 00:13:19,040
have kind of a similar problem and it's

298
00:13:16,959 --> 00:13:23,240
uh whether we call it a skills gap or an

299
00:13:19,040 --> 00:13:26,880
expertise gap as Ben alluded to

300
00:13:23,240 --> 00:13:29,200
um you know there's uh a lot of people

301
00:13:26,880 --> 00:13:32,160
that are retiring out of the workforce

302
00:13:29,200 --> 00:13:34,000
that know how things are done and so

303
00:13:32,160 --> 00:13:36,480
I'll talk about some use cases later

304
00:13:34,000 --> 00:13:38,399
about how we can uh capture that

305
00:13:36,480 --> 00:13:41,120
knowledge from those people before they

306
00:13:38,399 --> 00:13:43,360
leave but also take the instit the

307
00:13:41,120 --> 00:13:46,920
knowledge that's embedded in the these

308
00:13:43,360 --> 00:13:50,720
systems today and apply that uh to our

309
00:13:46,920 --> 00:13:52,320
problems. And obviously rising wages, we

310
00:13:50,720 --> 00:13:54,000
want to pay people more. We want to be

311
00:13:52,320 --> 00:13:57,440
more productive so we can pay people

312
00:13:54,000 --> 00:14:00,480
more. Um but obviously that drives up

313
00:13:57,440 --> 00:14:02,639
the cost of our products. So uh in

314
00:14:00,480 --> 00:14:05,040
partnership with McKenzie, we run a

315
00:14:02,639 --> 00:14:07,519
study every two years and it's

316
00:14:05,040 --> 00:14:11,680
specifically to look at the adoption of

317
00:14:07,519 --> 00:14:14,720
AI in manufacturing and operations.

318
00:14:11,680 --> 00:14:17,360
And I'm not going to uh go into great

319
00:14:14,720 --> 00:14:19,440
detail because today because in the

320
00:14:17,360 --> 00:14:20,800
workshop tomorrow we will go into more

321
00:14:19,440 --> 00:14:23,320
detail. How many people will be

322
00:14:20,800 --> 00:14:28,399
attending tomorrow's

323
00:14:23,320 --> 00:14:29,920
workshop? Okay. So um I'll I may I don't

324
00:14:28,399 --> 00:14:31,600
have all the slides in there but I may

325
00:14:29,920 --> 00:14:33,040
voice over a few things since there

326
00:14:31,600 --> 00:14:35,360
aren't going to be that many people

327
00:14:33,040 --> 00:14:37,199
tomorrow. So uh we look at about a

328
00:14:35,360 --> 00:14:40,240
hundred companies across all these

329
00:14:37,199 --> 00:14:43,120
different segments. Um it does skew to

330
00:14:40,240 --> 00:14:45,320
larger corporations as you can see over

331
00:14:43,120 --> 00:14:48,480
here and you don't really see that on

332
00:14:45,320 --> 00:14:50,399
here. So you know the much larger

333
00:14:48,480 --> 00:14:53,360
corporations do tend to do more with

334
00:14:50,399 --> 00:14:57,040
with AI and machine learning. Uh these

335
00:14:53,360 --> 00:15:00,399
are the types of of companies

336
00:14:57,040 --> 00:15:03,839
uh that we talk with and we we try to

337
00:15:00,399 --> 00:15:06,160
look um we look at different K per KPIs

338
00:15:03,839 --> 00:15:09,440
key performance indicators. We look

339
00:15:06,160 --> 00:15:12,320
across multiple use cases, multiple

340
00:15:09,440 --> 00:15:15,760
business outcomes. Uh and then we look

341
00:15:12,320 --> 00:15:18,760
at nine different enabler categories. Uh

342
00:15:15,760 --> 00:15:24,160
we've upped the number of KPIs to our

343
00:15:18,760 --> 00:15:26,000
2025 study. Um so we've increased uh the

344
00:15:24,160 --> 00:15:29,360
size of the study a little bit because

345
00:15:26,000 --> 00:15:31,600
we needed to add Gen AI. Um we have

346
00:15:29,360 --> 00:15:34,320
published several times in Harvard

347
00:15:31,600 --> 00:15:36,560
Business Review. Uh, I'll talk today

348
00:15:34,320 --> 00:15:38,519
about that rightmost article, what

349
00:15:36,560 --> 00:15:40,880
companies succeeding in AI do

350
00:15:38,519 --> 00:15:42,920
differently. Uh, we do have another

351
00:15:40,880 --> 00:15:45,600
article coming out next month on

352
00:15:42,920 --> 00:15:48,800
mckenzie.com that will uh be more

353
00:15:45,600 --> 00:15:52,759
detailed. So, um, feel free to look

354
00:15:48,800 --> 00:15:54,360
those up. Uh, you can find the links at

355
00:15:52,759 --> 00:15:58,240
mimo.mmit.edu.

356
00:15:54,360 --> 00:16:01,680
Um so let's talk first about about what

357
00:15:58,240 --> 00:16:04,800
uh we did uh in in the study as as we

358
00:16:01,680 --> 00:16:06,600
look at it. And it's the only study that

359
00:16:04,800 --> 00:16:09,360
I've seen that

360
00:16:06,600 --> 00:16:10,680
links the enablers which is essentially

361
00:16:09,360 --> 00:16:14,639
the money you

362
00:16:10,680 --> 00:16:18,000
spend to the outcomes. So the results.

363
00:16:14,639 --> 00:16:21,199
So if you do X, what's the correlation

364
00:16:18,000 --> 00:16:23,199
uh to the outcomes? Uh so and what we're

365
00:16:21,199 --> 00:16:26,880
trying to do is give people a road map.

366
00:16:23,199 --> 00:16:28,959
How do I how do I know when I invest in

367
00:16:26,880 --> 00:16:31,680
certain things that there might be a

368
00:16:28,959 --> 00:16:36,320
certain outcome from them? And then we

369
00:16:31,680 --> 00:16:39,839
uh we clustered them in uh to leaders

370
00:16:36,320 --> 00:16:42,720
uh leaders, planners, uh executors and

371
00:16:39,839 --> 00:16:44,040
emerging companies over here. And

372
00:16:42,720 --> 00:16:47,839
certainly in

373
00:16:44,040 --> 00:16:50,000
2021, uh, the everything tended to skew

374
00:16:47,839 --> 00:16:52,639
a little higher, which means you're

375
00:16:50,000 --> 00:16:55,279
spending more money. And the enablers,

376
00:16:52,639 --> 00:16:57,440
I'm sorry, are things like training, the

377
00:16:55,279 --> 00:16:59,759
people you hire, the data you collect,

378
00:16:57,440 --> 00:17:02,639
the things that you you spend money on

379
00:16:59,759 --> 00:17:05,120
that make you more capable. Um, so

380
00:17:02,639 --> 00:17:10,079
people spend a little bit more money. Uh

381
00:17:05,120 --> 00:17:14,480
what we saw then in 2023 and this was

382
00:17:10,079 --> 00:17:16,079
after chat GPT came out um and it helped

383
00:17:14,480 --> 00:17:18,559
raise awareness it didn't really move

384
00:17:16,079 --> 00:17:21,760
these particular numbers that much is

385
00:17:18,559 --> 00:17:24,079
things shifted over to the right so

386
00:17:21,760 --> 00:17:28,520
better results so shorter payback

387
00:17:24,079 --> 00:17:31,919
periods better KPI performance

388
00:17:28,520 --> 00:17:34,240
um and we also saw

389
00:17:31,919 --> 00:17:35,919
um the emergence of people down in this

390
00:17:34,240 --> 00:17:38,559
space. So, I'll go back one. You see

391
00:17:35,919 --> 00:17:41,840
there's not a lot of people here. Oops,

392
00:17:38,559 --> 00:17:44,960
back too many. Um, and then you see

393
00:17:41,840 --> 00:17:48,160
people emerging here. And they're able

394
00:17:44,960 --> 00:17:52,400
to get results by spending less. And

395
00:17:48,160 --> 00:17:52,400
we'll talk in a minute about why that

396
00:17:53,000 --> 00:17:58,720
is. So, you say, well, why why do I need

397
00:17:56,880 --> 00:18:00,640
to spend? You notice leaders in the

398
00:17:58,720 --> 00:18:02,320
upper right, they spend the most. Why do

399
00:18:00,640 --> 00:18:06,000
I even care? Why do I want to be a

400
00:18:02,320 --> 00:18:08,080
leader? uh the reason to be a leader is

401
00:18:06,000 --> 00:18:11,679
because they get better performance. So

402
00:18:08,080 --> 00:18:15,640
in 2021 they had about three times the

403
00:18:11,679 --> 00:18:19,559
KPI performance. So KPIs are things like

404
00:18:15,640 --> 00:18:23,720
efficiency over here uh cost

405
00:18:19,559 --> 00:18:27,280
um revenue, customer responsiveness,

406
00:18:23,720 --> 00:18:31,200
um and so cost is you know things like

407
00:18:27,280 --> 00:18:35,360
scrap rate etc. And so they get better

408
00:18:31,200 --> 00:18:37,280
performance by 2023. Uh we saw a 4x

409
00:18:35,360 --> 00:18:40,080
increase of the leaders over everyone

410
00:18:37,280 --> 00:18:42,360
else. So uh the best are getting better

411
00:18:40,080 --> 00:18:45,039
and pulling ahead.

412
00:18:42,360 --> 00:18:46,640
Um so that's in some sense you could say

413
00:18:45,039 --> 00:18:48,880
I'm not a leader. That's the bad news.

414
00:18:46,640 --> 00:18:51,360
We'll get to the good news in a second.

415
00:18:48,880 --> 00:18:53,360
But there is a reason to invest that you

416
00:18:51,360 --> 00:18:55,480
do get the kind of efficiency gains you

417
00:18:53,360 --> 00:18:57,799
were hoping to

418
00:18:55,480 --> 00:19:01,200
get.

419
00:18:57,799 --> 00:19:04,480
Um, and how do Now I'm going to go into

420
00:19:01,200 --> 00:19:07,600
like how do leaders get there? Um, one

421
00:19:04,480 --> 00:19:11,120
of the big changes was what this chart

422
00:19:07,600 --> 00:19:14,880
is showing you is leaders in light blue

423
00:19:11,120 --> 00:19:17,240
and the bottom 50% in in black. But who

424
00:19:14,880 --> 00:19:20,640
owned AI? So in

425
00:19:17,240 --> 00:19:24,240
2021, it was, you know, kind of down at

426
00:19:20,640 --> 00:19:28,960
a VP or sometimes director level. So it

427
00:19:24,240 --> 00:19:30,840
was primarily held here um is where a

428
00:19:28,960 --> 00:19:33,640
lot of the energy was in these two

429
00:19:30,840 --> 00:19:37,120
levels but then by

430
00:19:33,640 --> 00:19:40,320
2023 it had moved up to a board level or

431
00:19:37,120 --> 00:19:43,240
CEO level priority. So I think that that

432
00:19:40,320 --> 00:19:47,200
that was a big move that we

433
00:19:43,240 --> 00:19:49,280
saw and you know the implications are

434
00:19:47,200 --> 00:19:51,200
are really kind of large and so I'll

435
00:19:49,280 --> 00:19:56,080
talk a little bit about why a CEO

436
00:19:51,200 --> 00:19:58,880
matters right so um two uh pretty famous

437
00:19:56,080 --> 00:20:00,880
CEOs I I know General Electric isn't

438
00:19:58,880 --> 00:20:03,679
doing very well right now but back in

439
00:20:00,880 --> 00:20:07,600
Thomas's Edison's day it was quite a

440
00:20:03,679 --> 00:20:09,440
successful company um how did how did he

441
00:20:07,600 --> 00:20:12,400
get there right he was meticulous does

442
00:20:09,440 --> 00:20:14,480
it with his lab notebooks. Um they

443
00:20:12,400 --> 00:20:16,320
documented and measured everything and

444
00:20:14,480 --> 00:20:18,960
he was able to invent things like the

445
00:20:16,320 --> 00:20:22,400
electric light bulb, the phongraph and

446
00:20:18,960 --> 00:20:25,520
moving pictures. So uh pretty successful

447
00:20:22,400 --> 00:20:27,840
with datadriven uh technology. Henry

448
00:20:25,520 --> 00:20:29,360
Ford is famous for his time motion

449
00:20:27,840 --> 00:20:31,039
studies. Most people think it's the

450
00:20:29,360 --> 00:20:33,919
assembly line which is obviously part of

451
00:20:31,039 --> 00:20:35,919
it. But he also then totally optimized

452
00:20:33,919 --> 00:20:38,159
that process through a lot of data

453
00:20:35,919 --> 00:20:41,039
collection and was able to reduce uh

454
00:20:38,159 --> 00:20:43,080
production times and costs and they

455
00:20:41,039 --> 00:20:45,120
really had a culture of datadriven

456
00:20:43,080 --> 00:20:46,799
decision-m making. We talk about

457
00:20:45,120 --> 00:20:49,360
digitization

458
00:20:46,799 --> 00:20:52,400
uh as being important and and it is for

459
00:20:49,360 --> 00:20:54,720
the efficiency of collecting data, but

460
00:20:52,400 --> 00:20:57,039
you still need a mentality and a culture

461
00:20:54,720 --> 00:20:58,880
of datadriven decision-making and these

462
00:20:57,039 --> 00:21:01,760
gentlemen had it even though everything

463
00:20:58,880 --> 00:21:05,520
was done by hand and with paper. I'll

464
00:21:01,760 --> 00:21:07,520
throw out two other CEOs that uh you've

465
00:21:05,520 --> 00:21:10,320
heard of. You probably recognize Jeff

466
00:21:07,520 --> 00:21:11,840
Bezos from his picture. Um they really

467
00:21:10,320 --> 00:21:13,760
measure everything. I've had the

468
00:21:11,840 --> 00:21:17,159
pleasure of working very closely with

469
00:21:13,760 --> 00:21:20,480
Amazon executives and

470
00:21:17,159 --> 00:21:23,200
um everything gets measured and you you

471
00:21:20,480 --> 00:21:26,720
try to predict as much as you possibly

472
00:21:23,200 --> 00:21:29,200
can and he's been able to do that uh and

473
00:21:26,720 --> 00:21:31,679
optimize logistics and supply chain but

474
00:21:29,200 --> 00:21:33,679
then also customer preference right you

475
00:21:31,679 --> 00:21:35,240
for those of you who use Amazon you know

476
00:21:33,679 --> 00:21:37,520
you get lots of

477
00:21:35,240 --> 00:21:40,200
recommendations uh the other person on

478
00:21:37,520 --> 00:21:44,400
the the right you may love or hate uh

479
00:21:40,200 --> 00:21:47,480
Starbucks. Um, and uh, Howard Schultz

480
00:21:44,400 --> 00:21:50,320
really has used data to mine consumer

481
00:21:47,480 --> 00:21:55,120
preference and determine on which corner

482
00:21:50,320 --> 00:21:55,120
we're going to put a Starbucks. Um,

483
00:21:56,360 --> 00:22:04,240
so and uh, the one thing that they did

484
00:21:59,520 --> 00:22:06,880
is um, uh, Bezos and Schultz, they now

485
00:22:04,240 --> 00:22:08,320
have automated AI, automated analytics.

486
00:22:06,880 --> 00:22:10,400
So that's the big difference, but you

487
00:22:08,320 --> 00:22:12,080
still need that datadriven culture. And

488
00:22:10,400 --> 00:22:14,320
so we've seen that in the data from the

489
00:22:12,080 --> 00:22:17,440
McKenzie study. And I just wanted to

490
00:22:14,320 --> 00:22:21,400
give you some examples. And now when you

491
00:22:17,440 --> 00:22:25,120
bring that back um what you

492
00:22:21,400 --> 00:22:29,440
see is uh the the impact of some of

493
00:22:25,120 --> 00:22:32,720
those changes uh specifically uh in kind

494
00:22:29,440 --> 00:22:35,120
of culture

495
00:22:32,720 --> 00:22:37,840
um because uh you know what happens

496
00:22:35,120 --> 00:22:41,360
obviously is CEOs help to drive culture

497
00:22:37,840 --> 00:22:43,400
but what what this chart shows is where

498
00:22:41,360 --> 00:22:47,120
leaders were thinking about problems in

499
00:22:43,400 --> 00:22:48,440
2021 here and here and then where they

500
00:22:47,120 --> 00:22:52,080
move to by

501
00:22:48,440 --> 00:22:54,559
2023. So what we saw is people didn't

502
00:22:52,080 --> 00:22:56,720
think they had the skills in 2021. They

503
00:22:54,559 --> 00:22:58,960
think they do now have them or it's at

504
00:22:56,720 --> 00:23:00,880
least less of an issue and culture is

505
00:22:58,960 --> 00:23:04,080
less of an issue because remember by now

506
00:23:00,880 --> 00:23:06,400
people have chat GPT that they can use

507
00:23:04,080 --> 00:23:09,840
and they're realizing oh I really don't

508
00:23:06,400 --> 00:23:13,520
have the data that I need. Um I now need

509
00:23:09,840 --> 00:23:16,159
to do these ROI calculations. I'm I I'm

510
00:23:13,520 --> 00:23:17,919
not really prepared to do them and now I

511
00:23:16,159 --> 00:23:19,760
don't really have all the time and

512
00:23:17,919 --> 00:23:24,000
resources. So, it isn't so much about

513
00:23:19,760 --> 00:23:26,400
the the lack of expertise on AI. Um it's

514
00:23:24,000 --> 00:23:30,080
really more about did I collect the data

515
00:23:26,400 --> 00:23:33,679
and can I uh calculate the returns? And

516
00:23:30,080 --> 00:23:36,480
that's this unclear ROI is where having

517
00:23:33,679 --> 00:23:38,480
CEO level support is super important

518
00:23:36,480 --> 00:23:40,480
because if you've ever been caught down

519
00:23:38,480 --> 00:23:44,080
inside an organization trying to do an

520
00:23:40,480 --> 00:23:46,159
ROI calculation and convince finance and

521
00:23:44,080 --> 00:23:47,440
the rest of the organization

522
00:23:46,159 --> 00:23:51,039
uh that you should be able to do

523
00:23:47,440 --> 00:23:54,159
something uh you realize that

524
00:23:51,039 --> 00:23:56,600
um having a a CEO level mandate is is

525
00:23:54,159 --> 00:24:00,720
very important.

526
00:23:56,600 --> 00:24:05,159
So um we talked a little bit about data.

527
00:24:00,720 --> 00:24:09,039
I wanted to quantify what that means.

528
00:24:05,159 --> 00:24:12,240
So in our leader category over 90% of

529
00:24:09,039 --> 00:24:14,400
them recorded data on 10% of or more of

530
00:24:12,240 --> 00:24:16,720
their equipment. So they do record data.

531
00:24:14,400 --> 00:24:20,559
They do store data. And Thomas talked a

532
00:24:16,720 --> 00:24:22,640
little bit about the u the effect of

533
00:24:20,559 --> 00:24:26,880
that because what happens is you end up

534
00:24:22,640 --> 00:24:29,679
storing data on more than one device

535
00:24:26,880 --> 00:24:32,240
uh and more devices in a row. And when

536
00:24:29,679 --> 00:24:34,400
you store things on more than one device

537
00:24:32,240 --> 00:24:36,400
in a row, you can actually automate

538
00:24:34,400 --> 00:24:39,799
chains of things together very much like

539
00:24:36,400 --> 00:24:42,799
you would automate a factory uh with

540
00:24:39,799 --> 00:24:45,279
multiple um

541
00:24:42,799 --> 00:24:47,600
uh robots if you were doing that. So you

542
00:24:45,279 --> 00:24:49,279
can do the same thing with data. You get

543
00:24:47,600 --> 00:24:51,520
better at collecting data. You get

544
00:24:49,279 --> 00:24:53,760
better at storing data. You get better

545
00:24:51,520 --> 00:24:55,520
at processing data. So we do find that

546
00:24:53,760 --> 00:24:57,919
there are economies of scale and

547
00:24:55,520 --> 00:25:00,799
certainly storing them in the cloud uh

548
00:24:57,919 --> 00:25:00,799
is is super

549
00:25:01,400 --> 00:25:05,279
valuable. Uh there's some other

550
00:25:03,360 --> 00:25:09,600
attributes of data. I know this chart's

551
00:25:05,279 --> 00:25:09,600
a little uh hard to read. Uh

552
00:25:09,720 --> 00:25:16,279
but what we do find is that the leaders

553
00:25:13,039 --> 00:25:18,640
uh share data with their

554
00:25:16,279 --> 00:25:21,760
suppliers. Uh they acquire data from

555
00:25:18,640 --> 00:25:24,480
their suppliers and they share uh data

556
00:25:21,760 --> 00:25:27,559
uh with their customers. Uh they give

557
00:25:24,480 --> 00:25:31,760
their employees access to data. We see

558
00:25:27,559 --> 00:25:34,000
um you know almost 100% of leaders or

559
00:25:31,760 --> 00:25:35,720
actually 100% do give frontline access

560
00:25:34,000 --> 00:25:38,960
to data.

561
00:25:35,720 --> 00:25:42,159
um whereas uh you know the the bottom

562
00:25:38,960 --> 00:25:45,240
50% really don't do that as nearly as

563
00:25:42,159 --> 00:25:48,559
much and I you know I talked about the

564
00:25:45,240 --> 00:25:51,360
the one of the investments in data

565
00:25:48,559 --> 00:25:54,240
collection but uh leaders have a program

566
00:25:51,360 --> 00:25:56,880
for determining which data matters and

567
00:25:54,240 --> 00:25:58,960
deciding to collect it and to store it

568
00:25:56,880 --> 00:26:00,640
and they typically uh store it on the

569
00:25:58,960 --> 00:26:03,840
cloud and they store it on the cloud so

570
00:26:00,640 --> 00:26:05,520
they can get more access to tools and

571
00:26:03,840 --> 00:26:08,320
very importantly combin combine

572
00:26:05,520 --> 00:26:10,000
different data sets because it's uh very

573
00:26:08,320 --> 00:26:12,480
important to have what's called a label

574
00:26:10,000 --> 00:26:14,960
data set. I know whether a part is good

575
00:26:12,480 --> 00:26:17,279
or bad and then I can compare it to all

576
00:26:14,960 --> 00:26:19,200
the process variables

577
00:26:17,279 --> 00:26:21,120
uh that were existed that made good

578
00:26:19,200 --> 00:26:25,240
parts or bad parts and now I can start

579
00:26:21,120 --> 00:26:27,799
to determine what will make good or bad

580
00:26:25,240 --> 00:26:30,480
parts.

581
00:26:27,799 --> 00:26:33,960
Um partnering is very important. I'll

582
00:26:30,480 --> 00:26:37,679
give some use cases on that.

583
00:26:33,960 --> 00:26:40,559
Um what we've seen is uh the the types

584
00:26:37,679 --> 00:26:43,600
of partners here are academia startups

585
00:26:40,559 --> 00:26:45,840
consulting vendor and industry partners

586
00:26:43,600 --> 00:26:48,320
uh and the industry the vendors would be

587
00:26:45,840 --> 00:26:51,919
like SAP or Zemens the people you're

588
00:26:48,320 --> 00:26:56,400
already using and so what we've seen is

589
00:26:51,919 --> 00:26:59,600
uh because these vendors are uh you know

590
00:26:56,400 --> 00:27:03,520
having better solutions uh they're re

591
00:26:59,600 --> 00:27:06,000
companies are have now using startups in

592
00:27:03,520 --> 00:27:08,960
academia less. So that they're still

593
00:27:06,000 --> 00:27:10,960
using them at a kind of a 50% rate, but

594
00:27:08,960 --> 00:27:13,440
they don't need to rely on them as much

595
00:27:10,960 --> 00:27:16,279
because there's more sources. In 2021,

596
00:27:13,440 --> 00:27:19,960
there just weren't other sources

597
00:27:16,279 --> 00:27:23,200
of of people to partner with. And and

598
00:27:19,960 --> 00:27:25,840
that's really a key recommendation is

599
00:27:23,200 --> 00:27:28,720
don't try to do this yourself. Uh it's

600
00:27:25,840 --> 00:27:30,960
if if it was complicated in 2021, it's

601
00:27:28,720 --> 00:27:32,880
only gotten more so. Uh but the good

602
00:27:30,960 --> 00:27:34,520
news is there's a lot more uh solution

603
00:27:32,880 --> 00:27:38,480
choices available to

604
00:27:34,520 --> 00:27:41,679
you. Uh training. So we that's going to

605
00:27:38,480 --> 00:27:45,840
be a pretty continual theme training on

606
00:27:41,679 --> 00:27:47,200
not AI but making decisions with data.

607
00:27:45,840 --> 00:27:50,000
You don't need to train your whole

608
00:27:47,200 --> 00:27:52,559
workforce on AI. What you need to train

609
00:27:50,000 --> 00:27:56,320
them to is is make decisions with data.

610
00:27:52,559 --> 00:27:58,480
And what we're looking here is uh the

611
00:27:56,320 --> 00:28:01,840
really a big disparity in training

612
00:27:58,480 --> 00:28:03,520
between leaders in the bottom uh 50%.

613
00:28:01,840 --> 00:28:06,159
And you know obviously they train their

614
00:28:03,520 --> 00:28:07,960
engineers but they train everyone from

615
00:28:06,159 --> 00:28:11,840
line workers

616
00:28:07,960 --> 00:28:11,840
um up to division

617
00:28:15,240 --> 00:28:23,840
leaders. Okay. Um, I will give you now

618
00:28:19,679 --> 00:28:27,520
some uh a few case studies that uh

619
00:28:23,840 --> 00:28:30,200
relate um excuse me that cover some of

620
00:28:27,520 --> 00:28:33,200
these examples

621
00:28:30,200 --> 00:28:36,520
and I'll I probably have more than I

622
00:28:33,200 --> 00:28:39,600
have time uh to go through.

623
00:28:36,520 --> 00:28:44,360
So, how much time do I have and I want

624
00:28:39,600 --> 00:28:44,360
to kind of gauge this.

625
00:28:47,799 --> 00:28:54,799
Okay. So, we can kind of go through um

626
00:28:51,279 --> 00:28:58,320
as many or as as few as you want. Uh

627
00:28:54,799 --> 00:29:00,799
some of these uh case studies come um

628
00:28:58,320 --> 00:29:03,159
there's a program at MIT that I'm a part

629
00:29:00,799 --> 00:29:06,159
of called the leaders for global

630
00:29:03,159 --> 00:29:08,720
operations. And um a lot of these case

631
00:29:06,159 --> 00:29:11,520
studies are drawn from these sixmonth

632
00:29:08,720 --> 00:29:14,440
research projects that uh the students

633
00:29:11,520 --> 00:29:19,360
do. So they go on site at these member

634
00:29:14,440 --> 00:29:23,600
companies and um they do theirh thesis

635
00:29:19,360 --> 00:29:25,600
uh work there, their thesis research and

636
00:29:23,600 --> 00:29:27,760
um these students are getting both a

637
00:29:25,600 --> 00:29:30,880
masters in engineering and MBA. So they

638
00:29:27,760 --> 00:29:34,000
combine the business aspects uh with the

639
00:29:30,880 --> 00:29:38,640
technical aspects.

640
00:29:34,000 --> 00:29:40,159
So uh one example here um is is not from

641
00:29:38,640 --> 00:29:42,320
that particular data set but it's a

642
00:29:40,159 --> 00:29:47,440
company called North American Stainless.

643
00:29:42,320 --> 00:29:49,440
Uh they sit uh in Kentucky uh which is

644
00:29:47,440 --> 00:29:51,679
no one here's from Kentucky but it's not

645
00:29:49,440 --> 00:29:54,080
known as a one of the more

646
00:29:51,679 --> 00:29:56,679
forwardthinking uh states much like

647
00:29:54,080 --> 00:30:00,559
South Carolina was

648
00:29:56,679 --> 00:30:02,840
um you know before BMW came there. Uh

649
00:30:00,559 --> 00:30:06,000
but this particular factory is pretty

650
00:30:02,840 --> 00:30:09,039
advanced. They make uh 2 billion pounds

651
00:30:06,000 --> 00:30:12,399
of stainless steel a year. So very high

652
00:30:09,039 --> 00:30:15,679
volume. They have 1,400 employees. How

653
00:30:12,399 --> 00:30:16,520
many sensors to do you think exist in

654
00:30:15,679 --> 00:30:18,840
this

655
00:30:16,520 --> 00:30:20,960
factory? It's stainless

656
00:30:18,840 --> 00:30:23,000
steel. You know, it can't be that

657
00:30:20,960 --> 00:30:26,080
complex,

658
00:30:23,000 --> 00:30:29,200
right? Guess anyone? How many data

659
00:30:26,080 --> 00:30:30,880
sensors do they have? Okay. 30,000 is

660
00:30:29,200 --> 00:30:32,559
the answer since no one will guess which

661
00:30:30,880 --> 00:30:34,880
is probably way more than what other

662
00:30:32,559 --> 00:30:37,039
people thought. What percentage of

663
00:30:34,880 --> 00:30:40,640
people here do you think can write a SQL

664
00:30:37,039 --> 00:30:43,039
query? So do basic have frontline access

665
00:30:40,640 --> 00:30:45,399
and then can actually manipulate it with

666
00:30:43,039 --> 00:30:49,760
you know relatively old tool

667
00:30:45,399 --> 00:30:52,320
SQL. Turns out 5% so 70 of their people

668
00:30:49,760 --> 00:30:54,399
can actually get in write queries on

669
00:30:52,320 --> 00:30:56,080
their data. So for a factory that you

670
00:30:54,399 --> 00:30:58,600
think is not that advanced, they're

671
00:30:56,080 --> 00:31:02,960
actually quite advanced.

672
00:30:58,600 --> 00:31:05,279
um you know they have a problem here um

673
00:31:02,960 --> 00:31:08,799
with these 30,000 sensors. How do they

674
00:31:05,279 --> 00:31:11,279
determine uh what's they know when they

675
00:31:08,799 --> 00:31:13,840
start to get bad steel but that's

676
00:31:11,279 --> 00:31:15,679
usually too late because the you know

677
00:31:13,840 --> 00:31:18,399
the temperatures take a long time to

678
00:31:15,679 --> 00:31:20,600
ramp etc. So what they've done is

679
00:31:18,399 --> 00:31:23,039
they've uh in this case partnered with

680
00:31:20,600 --> 00:31:26,080
Falconry to look at two years of

681
00:31:23,039 --> 00:31:29,440
historical data on these 30,000 sensors

682
00:31:26,080 --> 00:31:32,640
and learn what is normal behavior, not

683
00:31:29,440 --> 00:31:35,039
normal behavior. And it automatically is

684
00:31:32,640 --> 00:31:37,360
able to tell them the yellow bars are

685
00:31:35,039 --> 00:31:40,720
things that are anomalous behavior. And

686
00:31:37,360 --> 00:31:43,120
so they're it doesn't try necessarily to

687
00:31:40,720 --> 00:31:44,480
solve the problem or say for sure

688
00:31:43,120 --> 00:31:46,559
there's a problem. It just tells them

689
00:31:44,480 --> 00:31:50,320
when things are bad. And uh you know, I

690
00:31:46,559 --> 00:31:51,760
love the VP quote, which is um when I

691
00:31:50,320 --> 00:31:53,440
have a problem, I want you to blow away

692
00:31:51,760 --> 00:31:55,200
all the hay and just leave me with a

693
00:31:53,440 --> 00:31:57,840
stack of needles. I want to know where

694
00:31:55,200 --> 00:31:58,919
to look. Which of the five sensors out

695
00:31:57,840 --> 00:32:01,919
of the

696
00:31:58,919 --> 00:32:04,559
30,000 are different that I should spend

697
00:32:01,919 --> 00:32:07,240
time on? So, it's a great use of this

698
00:32:04,559 --> 00:32:10,240
technically would be machine learning.

699
00:32:07,240 --> 00:32:12,480
Um here's another example of of

700
00:32:10,240 --> 00:32:16,720
partnering. Uh this is one that we

701
00:32:12,480 --> 00:32:18,039
worked on at MIMO. So, Analog Devices

702
00:32:16,720 --> 00:32:20,080
makes

703
00:32:18,039 --> 00:32:21,799
semiconductors uh in the US. They're

704
00:32:20,080 --> 00:32:24,360
actually one of the the largest in in

705
00:32:21,799 --> 00:32:28,000
Massachusetts. They have 24,000

706
00:32:24,360 --> 00:32:30,080
employees. Um and the problem in

707
00:32:28,000 --> 00:32:33,039
semiconductor manufacturing, especially

708
00:32:30,080 --> 00:32:35,120
high mix, low volume, is it can take

709
00:32:33,039 --> 00:32:36,559
months to create all the different

710
00:32:35,120 --> 00:32:38,559
layers because they make so many

711
00:32:36,559 --> 00:32:41,120
different products.

712
00:32:38,559 --> 00:32:43,919
And in this particular case, um, you

713
00:32:41,120 --> 00:32:46,080
know, after three months, they were able

714
00:32:43,919 --> 00:32:48,080
to electrically test this product to see

715
00:32:46,080 --> 00:32:51,679
if it was correct. Well, they ended up

716
00:32:48,080 --> 00:32:55,200
with millions of dollars in scrap. Um,

717
00:32:51,679 --> 00:32:57,360
and the so they came to us and said,

718
00:32:55,200 --> 00:33:00,720
"Can you can you help us with this?" And

719
00:32:57,360 --> 00:33:04,000
and we did. Um, the problem they have is

720
00:33:00,720 --> 00:33:06,960
they have almost no bad parts. Their

721
00:33:04,000 --> 00:33:09,039
yields are so high. um and their

722
00:33:06,960 --> 00:33:11,760
processes are so well controlled that it

723
00:33:09,039 --> 00:33:14,080
very rarely happens. So uh we had a

724
00:33:11,760 --> 00:33:17,279
problem it's called class imbalance. We

725
00:33:14,080 --> 00:33:18,880
have no failures basically and we have

726
00:33:17,279 --> 00:33:20,480
to tell them when things are going

727
00:33:18,880 --> 00:33:23,200
wrong. And so that was a really

728
00:33:20,480 --> 00:33:25,279
interesting uh project. uh the first two

729
00:33:23,200 --> 00:33:27,200
years were uh spent largely

730
00:33:25,279 --> 00:33:30,799
understanding the problem and proving

731
00:33:27,200 --> 00:33:33,919
that we could solve it on one recipe in

732
00:33:30,799 --> 00:33:36,080
one uh one machine and then it was

733
00:33:33,919 --> 00:33:39,679
extended further to another two years to

734
00:33:36,080 --> 00:33:43,360
say now do this on more machines with

735
00:33:39,679 --> 00:33:44,960
more recipes so more types of chips

736
00:33:43,360 --> 00:33:47,600
uh we talked about training so this is

737
00:33:44,960 --> 00:33:50,559
another case study Merna Academy not all

738
00:33:47,600 --> 00:33:53,760
of you obviously can afford what Merna

739
00:33:50,559 --> 00:33:55,039
uh does very popular um vaccine company.

740
00:33:53,760 --> 00:33:57,120
That's what they're known for as a

741
00:33:55,039 --> 00:33:59,080
vaccine company. They're really an mRNA

742
00:33:57,120 --> 00:34:02,000
company, but they're an AI first

743
00:33:59,080 --> 00:34:04,080
biioarma company. And because they said

744
00:34:02,000 --> 00:34:06,240
we're AI first, we're going to create

745
00:34:04,080 --> 00:34:09,359
our own academy. The important thing

746
00:34:06,240 --> 00:34:12,079
here is they trained everyone in the

747
00:34:09,359 --> 00:34:14,320
same vocabulary and the same level so

748
00:34:12,079 --> 00:34:17,040
that when they talked about analytical

749
00:34:14,320 --> 00:34:20,159
problems, they all were coming at it uh

750
00:34:17,040 --> 00:34:24,560
with the same uh kind of baseline

751
00:34:20,159 --> 00:34:25,560
knowledge and the same approach. Um so

752
00:34:24,560 --> 00:34:28,639
that's

753
00:34:25,560 --> 00:34:30,159
um and you don't have to form your own

754
00:34:28,639 --> 00:34:32,079
academy to do that. you can work with

755
00:34:30,159 --> 00:34:35,119
your local universities to just make

756
00:34:32,079 --> 00:34:36,720
sure everyone's has a tiered kind of uh

757
00:34:35,119 --> 00:34:39,359
education. We have a program called

758
00:34:36,720 --> 00:34:41,520
yellow belt in the US which I'm not sure

759
00:34:39,359 --> 00:34:44,879
how many how popular it is over here or

760
00:34:41,520 --> 00:34:47,240
black belt, yellow belt, green belt um

761
00:34:44,879 --> 00:34:50,800
that's

762
00:34:47,240 --> 00:34:52,440
u that you can also take advantage of.

763
00:34:50,800 --> 00:34:56,040
So let me

764
00:34:52,440 --> 00:34:59,359
um talk about a couple of uh other case

765
00:34:56,040 --> 00:35:02,359
studies or excuse me use cases. So one

766
00:34:59,359 --> 00:35:07,240
of the most popular

767
00:35:02,359 --> 00:35:09,920
um uh case stud or uh use cases is

768
00:35:07,240 --> 00:35:11,680
forecasting. So forecasting can be used

769
00:35:09,920 --> 00:35:13,440
to increase sales. It can be used to

770
00:35:11,680 --> 00:35:17,440
reduce inventory or it can be used to

771
00:35:13,440 --> 00:35:20,920
inform investments. Um I'll give you an

772
00:35:17,440 --> 00:35:23,680
example here of of Zara. Uh

773
00:35:20,920 --> 00:35:25,680
Zara needs to predict sizes small,

774
00:35:23,680 --> 00:35:29,040
mediums, and largest when I make a

775
00:35:25,680 --> 00:35:32,040
product. And they actually were able to

776
00:35:29,040 --> 00:35:34,800
uh look at the text descriptions of

777
00:35:32,040 --> 00:35:37,480
products and then relate that to

778
00:35:34,800 --> 00:35:43,040
historical uh sales for those text

779
00:35:37,480 --> 00:35:45,200
descriptions and uh predict future um uh

780
00:35:43,040 --> 00:35:46,720
mixes so that they don't lose sales and

781
00:35:45,200 --> 00:35:47,880
they don't have a lot of things that go

782
00:35:46,720 --> 00:35:50,960
to the discount

783
00:35:47,880 --> 00:35:56,280
rack. We I'm just kind of I'm mindful of

784
00:35:50,960 --> 00:35:56,280
time. I can keep going for hours. Um

785
00:35:59,599 --> 00:36:04,560
Okay. So, I'll um I'll just This is a

786
00:36:02,880 --> 00:36:07,880
visual inspection. We talked a little

787
00:36:04,560 --> 00:36:10,079
bit about that. Uh this one

788
00:36:07,880 --> 00:36:13,599
is and they're just doing visual

789
00:36:10,079 --> 00:36:17,040
inspection on uh on semiconductor or uh

790
00:36:13,599 --> 00:36:18,960
printed circuit boards. Uh this is a

791
00:36:17,040 --> 00:36:22,400
change over time in a mill. I wanted to

792
00:36:18,960 --> 00:36:23,160
give someone a really a heavy industrial

793
00:36:22,400 --> 00:36:27,599
uh

794
00:36:23,160 --> 00:36:30,560
problem. And uh the this is a stainless

795
00:36:27,599 --> 00:36:34,400
steel mill and in a stainless steel mill

796
00:36:30,560 --> 00:36:36,480
uh it's very important that it be um

797
00:36:34,400 --> 00:36:39,960
super smooth. These are the the finishes

798
00:36:36,480 --> 00:36:42,800
that go on your sinks and on your u

799
00:36:39,960 --> 00:36:45,200
refrigerators. And so this mill uh

800
00:36:42,800 --> 00:36:47,160
actually has to be super flat. And so

801
00:36:45,200 --> 00:36:49,119
they have to get it perfectly

802
00:36:47,160 --> 00:36:50,880
calibrated. Uh and they're actually

803
00:36:49,119 --> 00:36:54,320
using a visual inspection to see what

804
00:36:50,880 --> 00:36:56,640
the people are doing and also um to see

805
00:36:54,320 --> 00:36:58,079
if this door is open or closed because

806
00:36:56,640 --> 00:36:59,680
it turns out there's so much vibration

807
00:36:58,079 --> 00:37:02,520
in the middle they can't get a sensor to

808
00:36:59,680 --> 00:37:08,160
accurately predict it.

809
00:37:02,520 --> 00:37:10,079
Um I do want to talk about this one here

810
00:37:08,160 --> 00:37:15,680
uh because this is probably the most

811
00:37:10,079 --> 00:37:18,960
prevalent uh use case. Um, so Panasonic

812
00:37:15,680 --> 00:37:23,200
make batteries for Tesla. They make 5.5

813
00:37:18,960 --> 00:37:25,440
million cylindrical cells a day. Um,

814
00:37:23,200 --> 00:37:28,280
they got hundreds of uh cell winding

815
00:37:25,440 --> 00:37:31,200
machines. They have 350

816
00:37:28,280 --> 00:37:33,280
technicians and they trained a mo they

817
00:37:31,200 --> 00:37:35,240
have a historical database of a million

818
00:37:33,280 --> 00:37:38,800
trouble tickets.

819
00:37:35,240 --> 00:37:42,240
Um, so what they did here is they

820
00:37:38,800 --> 00:37:44,720
trained an LLM on those million trouble

821
00:37:42,240 --> 00:37:47,280
tickets to help those 350 maintenance

822
00:37:44,720 --> 00:37:49,520
technicians uh do a better job of

823
00:37:47,280 --> 00:37:52,160
keeping these machines up. I'm running a

824
00:37:49,520 --> 00:37:54,560
symposium on May the 6th. It's

825
00:37:52,160 --> 00:37:57,800
interesting. I've got Nissan, Toyota,

826
00:37:54,560 --> 00:38:00,800
Kamasu, and Panasonic all coming to talk

827
00:37:57,800 --> 00:38:02,400
about similar applications of LLMs,

828
00:38:00,800 --> 00:38:05,839
whether it's to keep an assembly line

829
00:38:02,400 --> 00:38:08,599
running to help you in Kamasu's case

830
00:38:05,839 --> 00:38:12,640
assemble these big complex uh earth

831
00:38:08,599 --> 00:38:14,320
movers. Um, and so Toyota and Nissan are

832
00:38:12,640 --> 00:38:16,160
both keeping the assembly line running

833
00:38:14,320 --> 00:38:18,640
and they're keeping battery machines

834
00:38:16,160 --> 00:38:20,800
running. So, this is probably one of the

835
00:38:18,640 --> 00:38:23,240
most prevalent use cases I'm starting to

836
00:38:20,800 --> 00:38:25,640
see right now.

837
00:38:23,240 --> 00:38:28,240
Um and

838
00:38:25,640 --> 00:38:32,720
I'll this is another really uh

839
00:38:28,240 --> 00:38:35,800
interesting gen AI uh visual inspection

840
00:38:32,720 --> 00:38:38,240
uh some uh drug discovery uh kinds of

841
00:38:35,800 --> 00:38:40,720
problems. Okay. So where to invest the

842
00:38:38,240 --> 00:38:44,280
the summary slide train your workforce

843
00:38:40,720 --> 00:38:47,760
in datadriven decision making

844
00:38:44,280 --> 00:38:50,079
um drive it through the organization use

845
00:38:47,760 --> 00:38:53,839
lean use a worker centric approach. I'm

846
00:38:50,079 --> 00:38:56,079
a big fan of digital lean. Um, choose

847
00:38:53,839 --> 00:38:57,920
impactful projects that have ROI. If

848
00:38:56,079 --> 00:39:00,720
they don't have ROI, they're going to

849
00:38:57,920 --> 00:39:02,880
fizzle out. Uh, typically things that

850
00:39:00,720 --> 00:39:05,040
generate revenue are more impactful than

851
00:39:02,880 --> 00:39:09,040
things that save cost. So, if you can uh

852
00:39:05,040 --> 00:39:10,599
find a revenue uh generating product and

853
00:39:09,040 --> 00:39:13,280
collect

854
00:39:10,599 --> 00:39:15,440
data, partner, as I mentioned, I've

855
00:39:13,280 --> 00:39:17,200
given you some examples. And I think

856
00:39:15,440 --> 00:39:20,880
there's a huge opportunity in

857
00:39:17,200 --> 00:39:22,640
unstructured data. um which is what LLMs

858
00:39:20,880 --> 00:39:25,040
are. Historically, we've done machine

859
00:39:22,640 --> 00:39:27,599
learning on structured data. Uh I showed

860
00:39:25,040 --> 00:39:30,079
you one example of starting to combine

861
00:39:27,599 --> 00:39:33,040
structured and unstructured data, but I

862
00:39:30,079 --> 00:39:33,040
think that's another big

863
00:39:33,800 --> 00:39:39,520
area. Um

864
00:39:36,359 --> 00:39:42,480
so, hit that QR code. Love to have you

865
00:39:39,520 --> 00:39:47,920
travel to Cambridge May the 6th. Uh but

866
00:39:42,480 --> 00:39:50,960
also our 2025 study is out. So, um you

867
00:39:47,920 --> 00:39:54,720
can find that on our web page here. Um

868
00:39:50,960 --> 00:39:56,880
or you can uh ask to uh be sent the

869
00:39:54,720 --> 00:39:59,440
study. It's frankly just easier to go to

870
00:39:56,880 --> 00:40:02,079
the web page and click on the link and

871
00:39:59,440 --> 00:40:04,480
start to fill it out. Uh and then for

872
00:40:02,079 --> 00:40:08,000
those of you in the workshop, uh hit

873
00:40:04,480 --> 00:40:12,119
that QR code and you can download uh the

874
00:40:08,000 --> 00:40:12,119
the materials for tomorrow.

875
00:40:17,520 --> 00:40:23,320
Okay, I think we have an opportunity for

876
00:40:21,200 --> 00:40:26,640
a few

877
00:40:23,320 --> 00:40:26,640
questions. Let's

878
00:40:27,480 --> 00:40:34,160
see. How do you see AI impacting front

879
00:40:31,040 --> 00:40:36,400
of house tasks versus back of house

880
00:40:34,160 --> 00:40:39,200
task? Right. So if you if you go back to

881
00:40:36,400 --> 00:40:43,440
to World War II,

882
00:40:39,200 --> 00:40:45,839
um 40% of product cost was direct labor,

883
00:40:43,440 --> 00:40:47,599
people in the factory doing things. As

884
00:40:45,839 --> 00:40:50,960
Ben will show you, that cost has come

885
00:40:47,599 --> 00:40:52,800
down to roughly 15%. If you looked in

886
00:40:50,960 --> 00:40:55,200
the same time period, the front of

887
00:40:52,800 --> 00:40:57,520
office tasks like engineering, design,

888
00:40:55,200 --> 00:40:59,280
planning, scheduling, procurement, all

889
00:40:57,520 --> 00:41:02,800
the things the people that sit at desks

890
00:40:59,280 --> 00:41:07,280
do has remained relatively flat at uh 12

891
00:41:02,800 --> 00:41:09,440
to 15%. And when you look at what LLMs

892
00:41:07,280 --> 00:41:11,599
are capable of doing and generative AI

893
00:41:09,440 --> 00:41:13,760
is capable of doing, that's the area

894
00:41:11,599 --> 00:41:15,920
where we see the most opportunity for

895
00:41:13,760 --> 00:41:18,800
impact are those how do I get more

896
00:41:15,920 --> 00:41:20,720
efficient in all those mental tasks, not

897
00:41:18,800 --> 00:41:23,200
the physical task. I'm not saying don't

898
00:41:20,720 --> 00:41:25,680
do the physical tasks. uh but I think

899
00:41:23,200 --> 00:41:28,000
there's a huge opportunity in this this

900
00:41:25,680 --> 00:41:30,000
knowledge work and unfortunately if you

901
00:41:28,000 --> 00:41:33,119
look within a factory you see a lot of

902
00:41:30,000 --> 00:41:35,200
that sits on someone's PC or uh you know

903
00:41:33,119 --> 00:41:37,040
my biggest friend and worst enemy is

904
00:41:35,200 --> 00:41:38,560
Microsoft Excel because that's where

905
00:41:37,040 --> 00:41:41,280
everything sits but at least there's

906
00:41:38,560 --> 00:41:43,599
data there uh sits in email servers how

907
00:41:41,280 --> 00:41:45,599
do we get all of that into one

908
00:41:43,599 --> 00:41:48,079
integrated data set where we can start

909
00:41:45,599 --> 00:41:51,599
to do things so uh we think there's a

910
00:41:48,079 --> 00:41:54,640
lot of a lot of opportunity there

911
00:41:51,599 --> 00:41:56,880
Okay, maybe just one more uh question.

912
00:41:54,640 --> 00:41:59,040
How do you balance short-term ROI

913
00:41:56,880 --> 00:42:01,359
expectations with long-term digital

914
00:41:59,040 --> 00:42:04,839
transformation goals when implementing

915
00:42:01,359 --> 00:42:04,839
AI solutions?

916
00:42:05,240 --> 00:42:10,000
Um, so the, you know, the challenge is

917
00:42:08,319 --> 00:42:12,800
if you're trying to implement an AI

918
00:42:10,000 --> 00:42:15,040
solution and you don't have Wi-Fi in

919
00:42:12,800 --> 00:42:16,920
your factory and you don't have sensors

920
00:42:15,040 --> 00:42:18,960
on your

921
00:42:16,920 --> 00:42:20,880
machines, it's going to be really hard

922
00:42:18,960 --> 00:42:24,160
to make that full business case. there's

923
00:42:20,880 --> 00:42:26,480
too much risk. Um, and I don't even

924
00:42:24,160 --> 00:42:28,800
really talk about digital transformation

925
00:42:26,480 --> 00:42:30,079
as much as datadriven decision-m if

926
00:42:28,800 --> 00:42:32,800
you're going to make decisions with

927
00:42:30,079 --> 00:42:35,680
data. You will have put the sensors in

928
00:42:32,800 --> 00:42:38,240
to collect the data. The transformation

929
00:42:35,680 --> 00:42:40,880
will come over time. But I think it's

930
00:42:38,240 --> 00:42:43,839
it's the ROI of looking at a line and

931
00:42:40,880 --> 00:42:45,359
saying geez if we could track uh you

932
00:42:43,839 --> 00:42:47,839
know the performance. If we could

933
00:42:45,359 --> 00:42:51,040
automate statistical process control on

934
00:42:47,839 --> 00:42:53,599
this line um then that would be worth it

935
00:42:51,040 --> 00:42:56,640
to us. So you got to work your work your

936
00:42:53,599 --> 00:42:58,480
way up not try to say I want to do AI.

937
00:42:56,640 --> 00:43:03,200
You you've got to go through all the

938
00:42:58,480 --> 00:43:06,319
steps. Well thank you very much Bruce.

939
00:43:03,200 --> 00:43:06,319
Thank you.

