1
00:00:01,199 --> 00:00:04,720
Okay. Yeah, thanks for having me. Thanks

2
00:00:02,560 --> 00:00:06,400
for being here. So, I'm going to talk a

3
00:00:04,720 --> 00:00:08,320
little bit today about some of the work

4
00:00:06,400 --> 00:00:10,960
that we've been doing on trying to

5
00:00:08,320 --> 00:00:13,360
understand how different agents in some

6
00:00:10,960 --> 00:00:14,880
sort of multi-acctor system broadly

7
00:00:13,360 --> 00:00:16,800
construed have access to different

8
00:00:14,880 --> 00:00:18,160
pieces of information and what impacts

9
00:00:16,800 --> 00:00:20,000
that that's going to have on the overall

10
00:00:18,160 --> 00:00:20,880
outcomes of that system. And so, the

11
00:00:20,000 --> 00:00:23,119
main question that we're going to be

12
00:00:20,880 --> 00:00:24,760
trying to ask and answer today is what

13
00:00:23,119 --> 00:00:29,119
is the role that heterogeneous

14
00:00:24,760 --> 00:00:30,640
information play in in AI systems? And

15
00:00:29,119 --> 00:00:33,360
there's a couple ways we can think about

16
00:00:30,640 --> 00:00:36,880
this. Um I'll talk about one in

17
00:00:33,360 --> 00:00:39,520
particular today. One perspective on the

18
00:00:36,880 --> 00:00:41,600
sort of multi- or information

19
00:00:39,520 --> 00:00:43,920
heterogeneity and AI systems comes from

20
00:00:41,600 --> 00:00:47,280
human AI collaboration. And often the

21
00:00:43,920 --> 00:00:50,320
goal there is you say well my uh my

22
00:00:47,280 --> 00:00:52,800
model my AI system knows something. The

23
00:00:50,320 --> 00:00:54,399
people I have know something else. And

24
00:00:52,800 --> 00:00:56,320
hopefully they can make better decisions

25
00:00:54,399 --> 00:00:58,160
together. And so my goal is to say can I

26
00:00:56,320 --> 00:00:59,280
combine these information sets into some

27
00:00:58,160 --> 00:01:00,960
productive way and get something that is

28
00:00:59,280 --> 00:01:01,840
better than either in isolation. Okay,

29
00:01:00,960 --> 00:01:03,039
that's not what I'm going to talk about

30
00:01:01,840 --> 00:01:04,400
today, but we have some interesting work

31
00:01:03,039 --> 00:01:06,320
on that if you're if you want to talk to

32
00:01:04,400 --> 00:01:08,240
me more about it. A different

33
00:01:06,320 --> 00:01:10,799
perspective comes from what happens when

34
00:01:08,240 --> 00:01:13,040
I have lots of different AI models

35
00:01:10,799 --> 00:01:14,320
acting out in the world somewhere. So I

36
00:01:13,040 --> 00:01:15,600
have lots of little robots. They're

37
00:01:14,320 --> 00:01:17,280
maybe going around and doing things.

38
00:01:15,600 --> 00:01:18,560
They're producing stuff. They're setting

39
00:01:17,280 --> 00:01:20,880
prices. They're making decisions.

40
00:01:18,560 --> 00:01:22,000
Whatever it might be. And I might think

41
00:01:20,880 --> 00:01:23,920
that I'm getting a lot of benefits

42
00:01:22,000 --> 00:01:25,759
because I have lots of different robots

43
00:01:23,920 --> 00:01:27,600
running around doing things, but it

44
00:01:25,759 --> 00:01:29,280
might end up being the case that they

45
00:01:27,600 --> 00:01:30,799
all have access to the same underlying

46
00:01:29,280 --> 00:01:32,079
set of information and that's going to

47
00:01:30,799 --> 00:01:34,320
correlate their behavior in

48
00:01:32,079 --> 00:01:35,520
unpredictable and perhaps problematic

49
00:01:34,320 --> 00:01:37,439
ways. And so this is the thing that

50
00:01:35,520 --> 00:01:38,799
we're going to be focusing on today.

51
00:01:37,439 --> 00:01:40,640
Now, one thing that you might hope for

52
00:01:38,799 --> 00:01:43,040
is in this world, maybe it would be

53
00:01:40,640 --> 00:01:44,720
actually be better if I had a different

54
00:01:43,040 --> 00:01:46,399
little model come. Maybe this guy's, you

55
00:01:44,720 --> 00:01:48,479
know, not very good, maybe it's not very

56
00:01:46,399 --> 00:01:50,320
powerful, but it has access to a sort of

57
00:01:48,479 --> 00:01:52,159
a different information set relative to

58
00:01:50,320 --> 00:01:53,759
everybody else. Okay? And so in a world

59
00:01:52,159 --> 00:01:55,360
where everybody's behaving the same way,

60
00:01:53,759 --> 00:01:57,119
it might make sense for me to try to

61
00:01:55,360 --> 00:01:58,399
diversify in some way. And so we're

62
00:01:57,119 --> 00:02:00,479
going to look at settings where that

63
00:01:58,399 --> 00:02:02,759
might be the case and try to think about

64
00:02:00,479 --> 00:02:06,000
what is the right way to analyze them.

65
00:02:02,759 --> 00:02:08,239
Okay. So I'll talk about a few different

66
00:02:06,000 --> 00:02:11,520
papers today sort of jointly together.

67
00:02:08,239 --> 00:02:13,680
This all started with work that uh we we

68
00:02:11,520 --> 00:02:14,879
started thinking about in 2021. uh

69
00:02:13,680 --> 00:02:16,879
around what happens when we get

70
00:02:14,879 --> 00:02:18,720
monocultures of algorithmic systems and

71
00:02:16,879 --> 00:02:20,640
how should we think about that. Okay, so

72
00:02:18,720 --> 00:02:22,000
what is a monoculture? Well, the

73
00:02:20,640 --> 00:02:24,480
traditional agricultural notion of

74
00:02:22,000 --> 00:02:26,160
monoculture says, you know, if we bu

75
00:02:24,480 --> 00:02:27,280
plant lots of different crops, those are

76
00:02:26,160 --> 00:02:28,400
going to be resilient to different

77
00:02:27,280 --> 00:02:30,480
things. They're going to fail in

78
00:02:28,400 --> 00:02:32,560
different ways. And so hopefully that is

79
00:02:30,480 --> 00:02:34,319
going to be a resilient overall system.

80
00:02:32,560 --> 00:02:36,480
And I'll contrast that with a

81
00:02:34,319 --> 00:02:38,879
monocultural system in which I only

82
00:02:36,480 --> 00:02:41,200
plant the same crop over and over again.

83
00:02:38,879 --> 00:02:42,959
The problem with this is, you know, even

84
00:02:41,200 --> 00:02:45,040
though this is a pretty hardy plant, it

85
00:02:42,959 --> 00:02:46,480
survives with probability 0.9. If

86
00:02:45,040 --> 00:02:48,000
there's some virus that's going to come

87
00:02:46,480 --> 00:02:49,360
out and target this particular plant,

88
00:02:48,000 --> 00:02:50,640
all of my crops are going to die and I'm

89
00:02:49,360 --> 00:02:52,000
in trouble. It would have been better if

90
00:02:50,640 --> 00:02:53,360
id planted a bunch of different things.

91
00:02:52,000 --> 00:02:54,560
Even if some of them were weak, some of

92
00:02:53,360 --> 00:02:56,080
them were going to fail under different

93
00:02:54,560 --> 00:02:58,000
conditions simply because the

94
00:02:56,080 --> 00:02:59,200
probability of catastrophic failure is

95
00:02:58,000 --> 00:03:00,879
lower, right? They're not all going to

96
00:02:59,200 --> 00:03:02,720
fail simultaneously. And that is good

97
00:03:00,879 --> 00:03:04,159
for my system, even though any

98
00:03:02,720 --> 00:03:05,840
individual plant might actually not be

99
00:03:04,159 --> 00:03:08,080
as good as the best plant I could think

100
00:03:05,840 --> 00:03:09,840
of. Okay? So this is a common problem

101
00:03:08,080 --> 00:03:11,760
that people think about in agriculture.

102
00:03:09,840 --> 00:03:13,840
It turns out the analogy actually

103
00:03:11,760 --> 00:03:15,040
extends really nicely to uh computer

104
00:03:13,840 --> 00:03:17,120
security. So people think about

105
00:03:15,040 --> 00:03:18,640
monocultural issues in computer systems.

106
00:03:17,120 --> 00:03:21,200
So if you think we're all running the

107
00:03:18,640 --> 00:03:23,440
same operating system, it's we all have

108
00:03:21,200 --> 00:03:24,879
vulnerabilities to the same bugs. And so

109
00:03:23,440 --> 00:03:26,239
therefore, if someone designs a virus

110
00:03:24,879 --> 00:03:27,760
that targets my Mac, it's going to

111
00:03:26,239 --> 00:03:29,440
target all of your Macs and we're all in

112
00:03:27,760 --> 00:03:30,879
trouble. But maybe if some people were

113
00:03:29,440 --> 00:03:32,640
using Windows, maybe if some people were

114
00:03:30,879 --> 00:03:34,080
using different Linux distributions,

115
00:03:32,640 --> 00:03:36,319
whatever it might be, we would have a

116
00:03:34,080 --> 00:03:37,920
more resilient overall system. Uh we

117
00:03:36,319 --> 00:03:40,720
actually saw a good example of this.

118
00:03:37,920 --> 00:03:42,879
This is a sort of a tangent, but I was

119
00:03:40,720 --> 00:03:45,200
I'd gone to the the urgent care because

120
00:03:42,879 --> 00:03:47,120
I thought I had broken a bone uh a few

121
00:03:45,200 --> 00:03:49,680
months ago. And so I went in and I got

122
00:03:47,120 --> 00:03:51,120
an X-ray taken and the I come out to the

123
00:03:49,680 --> 00:03:52,920
doctor and she's like, "Well, actually,

124
00:03:51,120 --> 00:03:55,200
we can't do anything with this X-ray

125
00:03:52,920 --> 00:03:57,360
because all the Windows machines are

126
00:03:55,200 --> 00:03:58,799
down. We can't send this to our lab

127
00:03:57,360 --> 00:04:00,640
where they would normally analyze it.

128
00:03:58,799 --> 00:04:01,840
So, you know, wait a few days and we'll

129
00:04:00,640 --> 00:04:02,959
try to look at it and we'll call you if

130
00:04:01,840 --> 00:04:04,879
something bad happens." And never called

131
00:04:02,959 --> 00:04:07,200
me, so hopefully that means I was fine.

132
00:04:04,879 --> 00:04:08,720
But this is a system where all of you

133
00:04:07,200 --> 00:04:10,319
may remember this. This is a few months

134
00:04:08,720 --> 00:04:11,840
ago or maybe was it a few months ago?

135
00:04:10,319 --> 00:04:13,519
Yeah. Everybody's Windows machines

136
00:04:11,840 --> 00:04:14,879
crashed. Nothing was working and all

137
00:04:13,519 --> 00:04:17,600
these legacy systems that were built on

138
00:04:14,879 --> 00:04:19,040
it were therefore vulnerable. Okay. So

139
00:04:17,600 --> 00:04:21,040
that's an example of how we might see

140
00:04:19,040 --> 00:04:22,880
monoculture in computer systems. More

141
00:04:21,040 --> 00:04:25,520
recently, actually there's a Gary

142
00:04:22,880 --> 00:04:30,320
Gendler who's a former I think treasury

143
00:04:25,520 --> 00:04:33,280
secretary um SEC sorry SEC. Yeah. He had

144
00:04:30,320 --> 00:04:36,000
a lot of stuff to say about monoculture

145
00:04:33,280 --> 00:04:37,759
in uh AI systems in finance. So if

146
00:04:36,000 --> 00:04:39,440
people are you know you have a trading

147
00:04:37,759 --> 00:04:40,800
algorithm that's going to make decisions

148
00:04:39,440 --> 00:04:42,639
on your behalf based on the information

149
00:04:40,800 --> 00:04:44,720
it has available to it and if everybody

150
00:04:42,639 --> 00:04:46,240
is running that same algorithm if

151
00:04:44,720 --> 00:04:48,800
there's some mistake some weird

152
00:04:46,240 --> 00:04:50,479
condition leads to unexpected behavior

153
00:04:48,800 --> 00:04:51,840
that could trigger some sort of

154
00:04:50,479 --> 00:04:53,840
catastrophic failure throughout the

155
00:04:51,840 --> 00:04:55,680
financial system just because there's

156
00:04:53,840 --> 00:04:57,520
some sort of weird failure that mode

157
00:04:55,680 --> 00:04:59,040
that all these things are simultaneously

158
00:04:57,520 --> 00:05:00,639
uh running into. And these are problems

159
00:04:59,040 --> 00:05:02,080
that you don't have with human traders

160
00:05:00,639 --> 00:05:03,199
necessarily because yes, they're going

161
00:05:02,080 --> 00:05:04,479
to fail, they're going to make mistakes,

162
00:05:03,199 --> 00:05:05,840
but the whole point of the system is

163
00:05:04,479 --> 00:05:07,039
that people make different mistakes from

164
00:05:05,840 --> 00:05:08,800
each other, right? There's independence

165
00:05:07,039 --> 00:05:11,360
across actors and that actually creates

166
00:05:08,800 --> 00:05:13,360
resilience in the system. Okay, so how

167
00:05:11,360 --> 00:05:15,759
should we think about monoculture when

168
00:05:13,360 --> 00:05:18,800
it comes to algorithms and AI decision-

169
00:05:15,759 --> 00:05:20,240
making uh prediction and so on? Uh when

170
00:05:18,800 --> 00:05:22,080
is it a bad thing? What sort of risks

171
00:05:20,240 --> 00:05:24,720
are we encountering by increasingly

172
00:05:22,080 --> 00:05:26,000
adopting AI models? and what are the

173
00:05:24,720 --> 00:05:27,919
forces that are going to push back

174
00:05:26,000 --> 00:05:31,039
against monoculture in the the world

175
00:05:27,919 --> 00:05:32,880
that we have today. Okay, so I'll take

176
00:05:31,039 --> 00:05:34,160
you through a few examples. So one is

177
00:05:32,880 --> 00:05:36,400
this initial setting that we started

178
00:05:34,160 --> 00:05:37,919
thinking about a few years ago which is

179
00:05:36,400 --> 00:05:39,360
think about a decision-making setting.

180
00:05:37,919 --> 00:05:42,400
So we were thinking about in the context

181
00:05:39,360 --> 00:05:44,479
of uh like resume screening algorithms

182
00:05:42,400 --> 00:05:45,680
sort of these uh models that are

183
00:05:44,479 --> 00:05:48,560
designed to make predictive decisions

184
00:05:45,680 --> 00:05:50,960
about people's potential for employment

185
00:05:48,560 --> 00:05:52,639
and firms in this world often face a

186
00:05:50,960 --> 00:05:54,000
choice. Sometimes they, you know, in in

187
00:05:52,639 --> 00:05:55,520
reality they use a combination of these

188
00:05:54,000 --> 00:05:57,280
things, but for for simplicity to say

189
00:05:55,520 --> 00:05:59,520
you face a choice. You could hire a

190
00:05:57,280 --> 00:06:01,680
bunch of people to screen rumÃ©s or you

191
00:05:59,520 --> 00:06:04,560
could buy some model that somebody sells

192
00:06:01,680 --> 00:06:06,000
that screens rumÃ©s for you. Okay? And so

193
00:06:04,560 --> 00:06:08,720
there's lots of these third parties that

194
00:06:06,000 --> 00:06:10,400
now offer uh machine learned models that

195
00:06:08,720 --> 00:06:11,840
say give me a resume or give me some

196
00:06:10,400 --> 00:06:12,960
information about a candidate and I will

197
00:06:11,840 --> 00:06:14,400
predict how good of a candidate they're

198
00:06:12,960 --> 00:06:16,960
going to be. Okay? So we could choose to

199
00:06:14,400 --> 00:06:19,199
adopt that those models instead.

200
00:06:16,960 --> 00:06:20,560
Forget about the difference in how fast

201
00:06:19,199 --> 00:06:21,759
these things operate or how much they

202
00:06:20,560 --> 00:06:23,280
cost or whatever. I'm gonna ignore that

203
00:06:21,759 --> 00:06:24,560
for now and just think about what is the

204
00:06:23,280 --> 00:06:26,080
information they have access to, how

205
00:06:24,560 --> 00:06:28,319
much correlation is there going to be in

206
00:06:26,080 --> 00:06:31,759
their decisions and what sorts of risks

207
00:06:28,319 --> 00:06:33,199
will that introduce. Okay? And so a

208
00:06:31,759 --> 00:06:36,319
caricature of what this might look like

209
00:06:33,199 --> 00:06:38,319
is in a non-algorithmic world, all of my

210
00:06:36,319 --> 00:06:39,520
human HR managers have different

211
00:06:38,319 --> 00:06:41,360
information sets. They have different

212
00:06:39,520 --> 00:06:43,600
biases. They have they make different

213
00:06:41,360 --> 00:06:45,759
mistakes. And so what that means is

214
00:06:43,600 --> 00:06:47,199
they're unlikely to fail all at the same

215
00:06:45,759 --> 00:06:48,960
time, right? Another way of saying this

216
00:06:47,199 --> 00:06:50,080
is if I go apply to a bunch of jobs and

217
00:06:48,960 --> 00:06:51,440
there's a bunch of humans sitting on the

218
00:06:50,080 --> 00:06:52,960
other side of that, some of them might

219
00:06:51,440 --> 00:06:54,360
like my resume, some of them might not,

220
00:06:52,960 --> 00:06:57,600
but at least I'll have opportunities

221
00:06:54,360 --> 00:06:59,440
somewhere. Okay? Now, as we start to use

222
00:06:57,600 --> 00:07:01,199
more and more algorithms in this space,

223
00:06:59,440 --> 00:07:02,880
maybe it's okay if some people are using

224
00:07:01,199 --> 00:07:04,240
uh human screeners and some people are

225
00:07:02,880 --> 00:07:05,759
making decisions algorithmically because

226
00:07:04,240 --> 00:07:07,520
I still have some opportunity there,

227
00:07:05,759 --> 00:07:09,039
right? I might find a human who likes my

228
00:07:07,520 --> 00:07:10,960
resume or maybe this particular model

229
00:07:09,039 --> 00:07:12,880
likes my resume. But you start to

230
00:07:10,960 --> 00:07:15,039
encounter problems when everybody is

231
00:07:12,880 --> 00:07:16,240
reliant on the same algorithms, right?

232
00:07:15,039 --> 00:07:17,599
If everybody's just using the same

233
00:07:16,240 --> 00:07:18,880
resume screener, then if I get rejected

234
00:07:17,599 --> 00:07:20,400
from one job, I'm going to get rejected

235
00:07:18,880 --> 00:07:23,120
from all jobs. A different example of

236
00:07:20,400 --> 00:07:24,720
this is if my credit score is low, if

237
00:07:23,120 --> 00:07:26,000
this bank rejects me for a loan, then

238
00:07:24,720 --> 00:07:27,440
every other bank is also going to reject

239
00:07:26,000 --> 00:07:28,560
me for a loan, right? And you could say,

240
00:07:27,440 --> 00:07:29,520
well, that's the whole point of a credit

241
00:07:28,560 --> 00:07:30,880
score. It's supposed to measure

242
00:07:29,520 --> 00:07:33,120
creditworthiness. But obviously, there

243
00:07:30,880 --> 00:07:34,479
are idiosyncratic errors that it makes,

244
00:07:33,120 --> 00:07:36,000
right? Some people just have thin credit

245
00:07:34,479 --> 00:07:37,280
files. They are truly creditworthy. They

246
00:07:36,000 --> 00:07:40,880
have low credit scores and they're going

247
00:07:37,280 --> 00:07:43,120
to be systematically denied opportunity.

248
00:07:40,880 --> 00:07:44,800
Okay. Now, so you can see how this might

249
00:07:43,120 --> 00:07:46,880
be bad for individuals. The interesting

250
00:07:44,800 --> 00:07:49,199
thing that we found when we we sort of

251
00:07:46,880 --> 00:07:50,639
modeled this out uh more formally is

252
00:07:49,199 --> 00:07:52,880
that this can actually be bad for the

253
00:07:50,639 --> 00:07:54,639
decision makers themselves and yet it is

254
00:07:52,880 --> 00:07:56,319
still in their best interest to switch

255
00:07:54,639 --> 00:07:58,000
over to using algorithms. Okay. And the

256
00:07:56,319 --> 00:08:00,319
intuition I'll give you for this is as

257
00:07:58,000 --> 00:08:02,479
follows. For me as an individual, if I'm

258
00:08:00,319 --> 00:08:04,720
a firm deciding what to do, it might

259
00:08:02,479 --> 00:08:06,400
make me better off individually to start

260
00:08:04,720 --> 00:08:08,400
using this algorithm. Right? it is sort

261
00:08:06,400 --> 00:08:11,360
of marginally better than my the the

262
00:08:08,400 --> 00:08:12,800
humans that I'm using instead. But when

263
00:08:11,360 --> 00:08:14,080
I start using this algorithm, it's

264
00:08:12,800 --> 00:08:15,599
actually going to make everybody else a

265
00:08:14,080 --> 00:08:17,360
little bit worse off. All the people who

266
00:08:15,599 --> 00:08:18,639
are also using the same algorithm as me,

267
00:08:17,360 --> 00:08:20,400
I'm going to have an externality on

268
00:08:18,639 --> 00:08:22,000
them. So my predictions get better,

269
00:08:20,400 --> 00:08:23,680
theirs get slightly worse, right?

270
00:08:22,000 --> 00:08:25,840
Because we are converging on the same

271
00:08:23,680 --> 00:08:27,440
information set. And so if we all make

272
00:08:25,840 --> 00:08:28,639
this same decision, I'm going to get

273
00:08:27,440 --> 00:08:31,120
slightly better, everyone else is going

274
00:08:28,639 --> 00:08:32,560
to get slightly worse. On average or

275
00:08:31,120 --> 00:08:34,880
across the entire system, everybody

276
00:08:32,560 --> 00:08:36,120
still ends up slightly worse off. Okay?

277
00:08:34,880 --> 00:08:38,080
And so this is a somewhat

278
00:08:36,120 --> 00:08:39,760
counterintuitive result. It's sort of

279
00:08:38,080 --> 00:08:42,399
hard to wrap your head around this, but

280
00:08:39,760 --> 00:08:44,720
for me it was a a clear indication that

281
00:08:42,399 --> 00:08:46,399
there's some value in information

282
00:08:44,720 --> 00:08:48,080
diversity, right? That creating these

283
00:08:46,399 --> 00:08:50,240
monocultures is actually a negative for

284
00:08:48,080 --> 00:08:51,920
society and it is still a path that we

285
00:08:50,240 --> 00:08:53,200
might go down even if everybody's

286
00:08:51,920 --> 00:08:55,720
behaving rationally. And that seemed

287
00:08:53,200 --> 00:08:59,480
like a sort of a strange place to

288
00:08:55,720 --> 00:09:02,320
be. Okay. So the key observation here is

289
00:08:59,480 --> 00:09:03,920
that the negative impacts of monoculture

290
00:09:02,320 --> 00:09:05,839
come from these externalities that when

291
00:09:03,920 --> 00:09:07,360
I make a decision or I choose to adopt

292
00:09:05,839 --> 00:09:09,040
some some algorithm, it's going to

293
00:09:07,360 --> 00:09:10,160
affect everybody else who was also using

294
00:09:09,040 --> 00:09:12,240
that same algorithm. It's going to make

295
00:09:10,160 --> 00:09:13,680
them slightly worse off because I'm

296
00:09:12,240 --> 00:09:15,360
doing the same thing as them. And as

297
00:09:13,680 --> 00:09:17,760
long as that is in in in many of these

298
00:09:15,360 --> 00:09:20,320
cases, that ends up being a bad thing.

299
00:09:17,760 --> 00:09:22,240
Okay? So these links between people or

300
00:09:20,320 --> 00:09:24,000
the the interactions between people is

301
00:09:22,240 --> 00:09:25,440
really going to dictate what are the bad

302
00:09:24,000 --> 00:09:26,640
things that happen when we all start

303
00:09:25,440 --> 00:09:28,560
doing the same thing. we all start

304
00:09:26,640 --> 00:09:30,000
behaving in the same way. And so there's

305
00:09:28,560 --> 00:09:31,519
some key questions that in the next five

306
00:09:30,000 --> 00:09:34,640
minutes and 29 seconds I'm going to try

307
00:09:31,519 --> 00:09:36,240
to to explore here. When does us

308
00:09:34,640 --> 00:09:37,600
behaving similarly lead to these

309
00:09:36,240 --> 00:09:39,040
negative externalities? When is it bad

310
00:09:37,600 --> 00:09:40,640
to have similar behaviors as your

311
00:09:39,040 --> 00:09:42,640
competitors or your cooperators,

312
00:09:40,640 --> 00:09:44,880
whatever it might be? What are the

313
00:09:42,640 --> 00:09:46,480
implications for this? And where in

314
00:09:44,880 --> 00:09:49,760
which settings should we expect to see

315
00:09:46,480 --> 00:09:51,839
more or less monoculture? Okay, so I'll

316
00:09:49,760 --> 00:09:54,000
give you two very quick examples that

317
00:09:51,839 --> 00:09:55,680
we've been studying throughout this uh

318
00:09:54,000 --> 00:09:57,839
this last couple years. One is on

319
00:09:55,680 --> 00:09:59,360
algorithmic pricing. So how when people

320
00:09:57,839 --> 00:10:00,880
are using algorithms to set prices, what

321
00:09:59,360 --> 00:10:02,640
are the implications there? And the

322
00:10:00,880 --> 00:10:04,480
second is on content generation. So when

323
00:10:02,640 --> 00:10:05,600
we're using AI to generate some content

324
00:10:04,480 --> 00:10:07,640
again, what are the impacts of

325
00:10:05,600 --> 00:10:10,240
monoculture

326
00:10:07,640 --> 00:10:11,920
there? Okay. So the the example I'll

327
00:10:10,240 --> 00:10:14,560
give you to to sort of motivate this

328
00:10:11,920 --> 00:10:17,040
notion of algorithmic pricing is you're

329
00:10:14,560 --> 00:10:19,120
increasingly seeing people try to

330
00:10:17,040 --> 00:10:21,600
predict willingness to pay and use that

331
00:10:19,120 --> 00:10:22,880
as a factor to set prices. Okay? So, for

332
00:10:21,600 --> 00:10:24,480
instance, you see this a lot on like

333
00:10:22,880 --> 00:10:26,079
travel booking sites, like depending on

334
00:10:24,480 --> 00:10:27,360
your location, depending on the device

335
00:10:26,079 --> 00:10:29,040
you're using and that kind of thing,

336
00:10:27,360 --> 00:10:31,360
they'll set show you different prices

337
00:10:29,040 --> 00:10:32,560
because they're trying to effectively

338
00:10:31,360 --> 00:10:33,440
estimate your willingness to pay and

339
00:10:32,560 --> 00:10:35,040
then charge you as much as you're

340
00:10:33,440 --> 00:10:37,600
willing to pay. And there's two

341
00:10:35,040 --> 00:10:39,519
competing forces here. One is that firms

342
00:10:37,600 --> 00:10:41,760
want prices to be high, right? High

343
00:10:39,519 --> 00:10:43,600
prices mean more profits. But the second

344
00:10:41,760 --> 00:10:46,560
force is firms also want to undercut

345
00:10:43,600 --> 00:10:48,800
each other, right? If you know, Lyft is

346
00:10:46,560 --> 00:10:51,200
tra charging $10 and Uber is charging

347
00:10:48,800 --> 00:10:53,440
$9, then I'm gonna go to Uber and that's

348
00:10:51,200 --> 00:10:55,440
probably good for them, right? And so

349
00:10:53,440 --> 00:10:58,000
the question is, what is the impact of

350
00:10:55,440 --> 00:11:00,240
monoculture on these competing forces?

351
00:10:58,000 --> 00:11:01,360
So in this example, uh, Lyft gets to

352
00:11:00,240 --> 00:11:03,200
decide when they're going to give me a

353
00:11:01,360 --> 00:11:04,959
10% discount. Uber gets to decide who

354
00:11:03,200 --> 00:11:06,480
they're going to give a 10% discount to.

355
00:11:04,959 --> 00:11:07,920
People are sort of varying degrees of

356
00:11:06,480 --> 00:11:09,760
price sensitive, and they're going to

357
00:11:07,920 --> 00:11:11,600
make decisions accordingly. And the

358
00:11:09,760 --> 00:11:13,120
question I'm going to ask is, as their

359
00:11:11,600 --> 00:11:14,720
strategies for offering these discounts

360
00:11:13,120 --> 00:11:16,399
become more correlated, what does that

361
00:11:14,720 --> 00:11:18,560
do to the market? Is that good for me as

362
00:11:16,399 --> 00:11:20,240
a consumer? Is it bad for me? Uh,

363
00:11:18,560 --> 00:11:21,800
spoiler, it's going to be bad for me,

364
00:11:20,240 --> 00:11:25,360
but for interesting

365
00:11:21,800 --> 00:11:28,079
reasons. Okay. And so the the punchline

366
00:11:25,360 --> 00:11:30,320
is firms may actually prefer to use the

367
00:11:28,079 --> 00:11:31,920
same pricing algorithms as each other

368
00:11:30,320 --> 00:11:33,839
because that allows them to effectively

369
00:11:31,920 --> 00:11:34,880
collude on keeping prices high. Right?

370
00:11:33,839 --> 00:11:36,000
If I know that you're not going to

371
00:11:34,880 --> 00:11:37,519
undercut me because I know that we're

372
00:11:36,000 --> 00:11:40,480
using the same algorithm, we can both

373
00:11:37,519 --> 00:11:42,240
raise our prices. Okay. And so this

374
00:11:40,480 --> 00:11:43,440
shows that firms in in this sort of

375
00:11:42,240 --> 00:11:44,880
pricing setting actually have an

376
00:11:43,440 --> 00:11:47,760
incentive to correlate their predictions

377
00:11:44,880 --> 00:11:50,320
as much as they can. Right? Now this

378
00:11:47,760 --> 00:11:51,839
should worry you a little bit. This is a

379
00:11:50,320 --> 00:11:53,519
setting where you know if they were

380
00:11:51,839 --> 00:11:55,519
using different pricing algorithms I as

381
00:11:53,519 --> 00:11:57,360
a consumer could choose my favorite

382
00:11:55,519 --> 00:11:58,640
price. I could be a sort of shop around

383
00:11:57,360 --> 00:12:00,880
and choose the lowest price and

384
00:11:58,640 --> 00:12:02,240
therefore I win as a consumer. And their

385
00:12:00,880 --> 00:12:04,399
goal is to say actually can they just

386
00:12:02,240 --> 00:12:06,160
use the same pricing algorithm? And if

387
00:12:04,399 --> 00:12:08,760
so I don't have any choices anymore and

388
00:12:06,160 --> 00:12:12,480
they can raise the floor on that price.

389
00:12:08,760 --> 00:12:14,399
Okay. Now the this is sort of comes from

390
00:12:12,480 --> 00:12:15,680
uh from from fairly standard economic

391
00:12:14,399 --> 00:12:17,480
models. The interesting things for us

392
00:12:15,680 --> 00:12:20,240
here is the the legal implications

393
00:12:17,480 --> 00:12:21,680
actually. So I I mentioned the word

394
00:12:20,240 --> 00:12:23,519
collusion before. You might think of

395
00:12:21,680 --> 00:12:25,440
collusion in pricing as sort of vaguely

396
00:12:23,519 --> 00:12:27,279
familiar as something that antitrust law

397
00:12:25,440 --> 00:12:29,519
is designed to prevent us from doing.

398
00:12:27,279 --> 00:12:32,160
Right? Antitrust law says that uh two

399
00:12:29,519 --> 00:12:34,880
firms can't illegally collude to can't

400
00:12:32,160 --> 00:12:36,800
uh to fix prices or to set prices to be

401
00:12:34,880 --> 00:12:39,600
high.

402
00:12:36,800 --> 00:12:41,839
But it doesn't really appear to apply to

403
00:12:39,600 --> 00:12:44,480
this form of algorithmic collusion.

404
00:12:41,839 --> 00:12:46,639
Right? If if firms make rational choices

405
00:12:44,480 --> 00:12:47,920
that still, you know, lead me and my

406
00:12:46,639 --> 00:12:49,279
competitor to adopting the same

407
00:12:47,920 --> 00:12:50,880
algorithm, those might still be

408
00:12:49,279 --> 00:12:52,320
individually rational choices. And as a

409
00:12:50,880 --> 00:12:54,000
result, antitrust law actually doesn't

410
00:12:52,320 --> 00:12:55,600
have a lot of teeth there, but it can

411
00:12:54,000 --> 00:12:57,200
still result in these same collusive

412
00:12:55,600 --> 00:12:59,120
outcomes that are as if they were able

413
00:12:57,200 --> 00:13:01,360
to collude and keep prices high. And so

414
00:12:59,120 --> 00:13:02,480
this represents a frontier of antitrust

415
00:13:01,360 --> 00:13:04,399
law that the government is actually

416
00:13:02,480 --> 00:13:06,399
still grappling with today. And when I

417
00:13:04,399 --> 00:13:09,040
say today, I mean like in the last five

418
00:13:06,399 --> 00:13:11,279
days, um the state of New Jersey joined

419
00:13:09,040 --> 00:13:14,160
like 10 other states uh in filing a

420
00:13:11,279 --> 00:13:16,639
lawsuit against Real Page, which is a

421
00:13:14,160 --> 00:13:18,160
provider of software for rent pricing.

422
00:13:16,639 --> 00:13:19,760
Okay, so Real Page goes out to a bunch

423
00:13:18,160 --> 00:13:21,360
of apartment complexes and says, "We're

424
00:13:19,760 --> 00:13:23,279
going to provide you some software. You

425
00:13:21,360 --> 00:13:25,279
can use this to price your apartments."

426
00:13:23,279 --> 00:13:26,639
And so instead of a world where every

427
00:13:25,279 --> 00:13:28,399
apartment is going to be doing their own

428
00:13:26,639 --> 00:13:30,000
independent pricing and I as a consumer

429
00:13:28,399 --> 00:13:31,360
can come in and say here's the price

430
00:13:30,000 --> 00:13:33,519
that I want for the apartment that I

431
00:13:31,360 --> 00:13:35,279
want, you now see everybody sort of

432
00:13:33,519 --> 00:13:36,560
hiding behind the same algorithm, right?

433
00:13:35,279 --> 00:13:38,320
And saying we're all going to use the

434
00:13:36,560 --> 00:13:39,839
same algorithm to set prices. And as a

435
00:13:38,320 --> 00:13:41,680
result, we might be able to use those to

436
00:13:39,839 --> 00:13:43,760
illegally or potentially illegally set

437
00:13:41,680 --> 00:13:45,440
prices uh to be uh above their

438
00:13:43,760 --> 00:13:48,000
competitive rate. And so this is what's

439
00:13:45,440 --> 00:13:50,560
getting litigated right now uh between

440
00:13:48,000 --> 00:13:52,800
the states and real page.

441
00:13:50,560 --> 00:13:54,959
Okay, I will very quickly give you the

442
00:13:52,800 --> 00:13:56,480
the second example of what happens when

443
00:13:54,959 --> 00:13:59,399
we what what should we expect to see

444
00:13:56,480 --> 00:14:01,600
when it comes to monoculture in content

445
00:13:59,399 --> 00:14:03,120
generation. The stylized fact that I

446
00:14:01,600 --> 00:14:04,639
found particularly intriguing recently

447
00:14:03,120 --> 00:14:06,440
is you can find a bunch of these studies

448
00:14:04,639 --> 00:14:08,639
that effectively find the the the

449
00:14:06,440 --> 00:14:10,480
following. As people start to use

450
00:14:08,639 --> 00:14:11,920
generative AI more and more in their

451
00:14:10,480 --> 00:14:13,480
processes, you start to see a

452
00:14:11,920 --> 00:14:15,600
convergence of the stuff that they

453
00:14:13,480 --> 00:14:17,440
produce. Okay? So you see this in

454
00:14:15,600 --> 00:14:19,360
creative ideation and writing and art

455
00:14:17,440 --> 00:14:21,040
production and so on. The the picture

456
00:14:19,360 --> 00:14:22,959
version of this is if I ask people to

457
00:14:21,040 --> 00:14:24,880
come up with a bunch of creative ideas

458
00:14:22,959 --> 00:14:28,079
in a condition without AI, they come up

459
00:14:24,880 --> 00:14:29,760
with let's say two ideas each. If I give

460
00:14:28,079 --> 00:14:31,519
them access to AI, maybe they come up

461
00:14:29,760 --> 00:14:33,120
with three ideas each or more detailed

462
00:14:31,519 --> 00:14:36,000
ideas or whatever. And I might look at

463
00:14:33,120 --> 00:14:37,680
this and say, great, AI has helped. But

464
00:14:36,000 --> 00:14:39,120
then you actually analyze those those

465
00:14:37,680 --> 00:14:40,320
ideas and they all kind of turn out to

466
00:14:39,120 --> 00:14:41,920
be the same. So even though each

467
00:14:40,320 --> 00:14:43,680
individual seems to be producing more,

468
00:14:41,920 --> 00:14:44,839
the set of things produced has has

469
00:14:43,680 --> 00:14:48,079
gotten

470
00:14:44,839 --> 00:14:50,800
smaller. Okay. So what how should I

471
00:14:48,079 --> 00:14:52,720
think about this? There's there seems to

472
00:14:50,800 --> 00:14:54,399
be at least from my perspective a force

473
00:14:52,720 --> 00:14:55,760
pushing back against this which is if

474
00:14:54,399 --> 00:14:57,839
we're all trying to come up with you

475
00:14:55,760 --> 00:14:59,279
know creative business ideas or whatever

476
00:14:57,839 --> 00:15:00,880
it is not just in my interest to come up

477
00:14:59,279 --> 00:15:02,639
with good ideas it's in my interest to

478
00:15:00,880 --> 00:15:05,600
come up with good ideas that nobody else

479
00:15:02,639 --> 00:15:08,079
has had right I am worse off if we all

480
00:15:05,600 --> 00:15:09,600
pick the same uh the same ideas or

481
00:15:08,079 --> 00:15:11,199
whatever it might be and so shouldn't

482
00:15:09,600 --> 00:15:13,440
that be a force pushing back against it

483
00:15:11,199 --> 00:15:16,320
shouldn't I be trying to diversify my

484
00:15:13,440 --> 00:15:17,760
creativity in some way okay so these are

485
00:15:16,320 --> 00:15:19,279
uh going back to these externalities

486
00:15:17,760 --> 00:15:21,199
that we mentioned this This is a setting

487
00:15:19,279 --> 00:15:22,800
where us producing the same thing

488
00:15:21,199 --> 00:15:24,160
creates negative externalities for us.

489
00:15:22,800 --> 00:15:26,320
It makes me worse off if I do the same

490
00:15:24,160 --> 00:15:27,519
thing as you. So how does that propagate

491
00:15:26,320 --> 00:15:29,600
back to the way that we're going to use

492
00:15:27,519 --> 00:15:32,079
AI? Without going too much into the

493
00:15:29,600 --> 00:15:34,800
details, at a very high level, what we

494
00:15:32,079 --> 00:15:36,800
find is that uh more competition

495
00:15:34,800 --> 00:15:38,720
incentivizes more diverse production. So

496
00:15:36,800 --> 00:15:40,639
the stronger our competition is, the

497
00:15:38,720 --> 00:15:42,480
more that my behavior impacts yours and

498
00:15:40,639 --> 00:15:45,040
vice versa, the more incentives we have

499
00:15:42,480 --> 00:15:47,519
to diversify what we're doing.

500
00:15:45,040 --> 00:15:49,360
We also find that the quality of an AI

501
00:15:47,519 --> 00:15:50,560
model is therefore multi-dimensional.

502
00:15:49,360 --> 00:15:52,480
Okay, in a world where I'm not

503
00:15:50,560 --> 00:15:54,880
competing, maybe one model is is

504
00:15:52,480 --> 00:15:56,800
particularly good. But in a world where

505
00:15:54,880 --> 00:15:58,399
we're competing with each other, I might

506
00:15:56,800 --> 00:16:00,000
actually just want to sample from a wide

507
00:15:58,399 --> 00:16:01,519
range of good answers. I might not be

508
00:16:00,000 --> 00:16:02,880
interested in the one great answer

509
00:16:01,519 --> 00:16:05,120
because everybody has that one great

510
00:16:02,880 --> 00:16:06,480
answer. And so some models are good at

511
00:16:05,120 --> 00:16:08,160
giving you that one great answer. Some

512
00:16:06,480 --> 00:16:09,680
of them are better at sort of exploring

513
00:16:08,160 --> 00:16:11,519
out into the tail. And it's actually not

514
00:16:09,680 --> 00:16:13,759
clear which one uh just given a set of

515
00:16:11,519 --> 00:16:15,279
models, which one is going to be better.

516
00:16:13,759 --> 00:16:16,880
And so what that means is that the

517
00:16:15,279 --> 00:16:19,519
market for AI tools is going to

518
00:16:16,880 --> 00:16:21,120
incentivize information diversity in a

519
00:16:19,519 --> 00:16:22,959
sense you get some benefits out of

520
00:16:21,120 --> 00:16:24,399
filling a gap left by other tools.

521
00:16:22,959 --> 00:16:26,240
Right? You see this sometimes where you

522
00:16:24,399 --> 00:16:27,600
know I have a coding question I go to my

523
00:16:26,240 --> 00:16:28,800
favorite model it gives me the wrong

524
00:16:27,600 --> 00:16:30,240
answer. So I'm going to go find a

525
00:16:28,800 --> 00:16:31,519
different model and see if that try if

526
00:16:30,240 --> 00:16:32,720
that fixes my bug and if that doesn't

527
00:16:31,519 --> 00:16:34,480
work I'll go on to the next one to see

528
00:16:32,720 --> 00:16:36,320
if that works. Right? It is helpful for

529
00:16:34,480 --> 00:16:37,920
me that those three models are doing

530
00:16:36,320 --> 00:16:39,839
different things. If it's the same model

531
00:16:37,920 --> 00:16:40,800
over and over again it's going to get my

532
00:16:39,839 --> 00:16:42,399
you know it's not going to be able to

533
00:16:40,800 --> 00:16:43,600
fix my bug repeatedly. that's not

534
00:16:42,399 --> 00:16:45,759
particularly useful for me as a

535
00:16:43,600 --> 00:16:47,680
consumer. And so what I'm hoping is that

536
00:16:45,759 --> 00:16:49,040
we end up not with a market that looks

537
00:16:47,680 --> 00:16:50,959
like this where everybody's using the

538
00:16:49,040 --> 00:16:52,959
same tool, but actually a market like

539
00:16:50,959 --> 00:16:56,639
this where people just have very

540
00:16:52,959 --> 00:16:58,600
different tools uh that sort of span the

541
00:16:56,639 --> 00:17:01,199
spectrum of things that people might

542
00:16:58,600 --> 00:17:02,959
want. Okay? And so I'll leave you with

543
00:17:01,199 --> 00:17:05,120
this. This is sort of I didn't get to

544
00:17:02,959 --> 00:17:06,799
talk about this uh due to for time

545
00:17:05,120 --> 00:17:08,319
reasons, but there's sort of two nice

546
00:17:06,799 --> 00:17:10,400
ways I like to think about information

547
00:17:08,319 --> 00:17:11,439
diversity playing out in AI systems. One

548
00:17:10,400 --> 00:17:13,600
is trying to leverage the

549
00:17:11,439 --> 00:17:15,039
complimentarity between humans and AI,

550
00:17:13,600 --> 00:17:17,120
right? So what do humans know? What do

551
00:17:15,039 --> 00:17:18,799
AI models what are AI models good at?

552
00:17:17,120 --> 00:17:20,959
How do we put those together? And the

553
00:17:18,799 --> 00:17:23,439
second is how should we think about a

554
00:17:20,959 --> 00:17:25,360
diverse ecosystem of tools that have

555
00:17:23,439 --> 00:17:26,880
different strengths and weaknesses and

556
00:17:25,360 --> 00:17:29,039
allow us to select the thing that works

557
00:17:26,880 --> 00:17:30,480
best for us as opposed to a monoculture

558
00:17:29,039 --> 00:17:32,080
where everybody's just using the same

559
00:17:30,480 --> 00:17:33,440
product. So if you want to know more

560
00:17:32,080 --> 00:17:35,120
about that, there's some QR codes at the

561
00:17:33,440 --> 00:17:36,559
top to look at the papers. These are my

562
00:17:35,120 --> 00:17:38,280
wonderful collaborators and thanks so

563
00:17:36,559 --> 00:17:41,160
much for your

564
00:17:38,280 --> 00:17:45,640
time.

565
00:17:41,160 --> 00:17:45,640
Awesome questions for Manish.

566
00:17:53,280 --> 00:17:58,880
So the um the idea is that the the

567
00:17:55,919 --> 00:18:01,039
monoculture is just one model. Everybody

568
00:17:58,880 --> 00:18:05,039
is sharing that. What if the world we

569
00:18:01,039 --> 00:18:06,880
are in is like by or dur uh culture. So

570
00:18:05,039 --> 00:18:08,400
there are two parts. You're either this

571
00:18:06,880 --> 00:18:10,080
way or that way. You hate each other.

572
00:18:08,400 --> 00:18:11,600
you don't believe in each other then

573
00:18:10,080 --> 00:18:13,440
what's going to be happening that way

574
00:18:11,600 --> 00:18:15,160
with AI

575
00:18:13,440 --> 00:18:18,080
good okay so

576
00:18:15,160 --> 00:18:20,480
there's maybe one so one way to to sort

577
00:18:18,080 --> 00:18:22,240
of interpret what you're saying so

578
00:18:20,480 --> 00:18:23,520
there's some polarization perhaps in the

579
00:18:22,240 --> 00:18:25,360
world some people believe this some

580
00:18:23,520 --> 00:18:27,360
people believe that and there's

581
00:18:25,360 --> 00:18:29,440
polarized sets of demands of what people

582
00:18:27,360 --> 00:18:31,200
want so I might want a model that aligns

583
00:18:29,440 --> 00:18:32,720
with my beliefs somebody with different

584
00:18:31,200 --> 00:18:34,480
uh different beliefs might want a model

585
00:18:32,720 --> 00:18:37,799
that aligns with theirs I think you're

586
00:18:34,480 --> 00:18:40,960
starting to see that a little bit right

587
00:18:37,799 --> 00:18:42,480
So, it's a combination of things. When

588
00:18:40,960 --> 00:18:44,559
we build these models, especially the

589
00:18:42,480 --> 00:18:46,799
sort of big pre-trained models, they're

590
00:18:44,559 --> 00:18:48,640
often pre-trained on whatever data we

591
00:18:46,799 --> 00:18:50,080
could find. And as a result, that

592
00:18:48,640 --> 00:18:51,360
pre-training process ends up being

593
00:18:50,080 --> 00:18:52,880
pretty similar for a lot of these

594
00:18:51,360 --> 00:18:54,480
models, right? There's only so much data

595
00:18:52,880 --> 00:18:55,600
on the internet. If we all scoop up that

596
00:18:54,480 --> 00:18:57,520
same data, we're going to end up with

597
00:18:55,600 --> 00:18:59,360
things that look very similar. But on

598
00:18:57,520 --> 00:19:01,039
top of that, we have more and more of an

599
00:18:59,360 --> 00:19:02,559
ability to steer models in a direction

600
00:19:01,039 --> 00:19:03,919
that we want them to go. So, if I want

601
00:19:02,559 --> 00:19:04,880
this to be very good at coding, I'm

602
00:19:03,919 --> 00:19:06,480
going to make it very good at coding.

603
00:19:04,880 --> 00:19:08,240
and I'll get a bunch of data that uh

604
00:19:06,480 --> 00:19:10,080
sort of makes it good in that direction.

605
00:19:08,240 --> 00:19:11,440
If I want it to speak with a particular

606
00:19:10,080 --> 00:19:13,440
tone, I'm going to collect data of that

607
00:19:11,440 --> 00:19:15,600
form. So that gives a designer actually

608
00:19:13,440 --> 00:19:16,880
a lot of degrees of freedom to say I'm

609
00:19:15,600 --> 00:19:18,000
going to specialize the model in this

610
00:19:16,880 --> 00:19:19,520
direction and maybe it's going to get

611
00:19:18,000 --> 00:19:20,640
worse at some things, but it's going to

612
00:19:19,520 --> 00:19:23,919
get really good at the thing that I want

613
00:19:20,640 --> 00:19:25,840
it to do. Now, is it a good thing for

614
00:19:23,919 --> 00:19:27,919
society if we start to polarize where

615
00:19:25,840 --> 00:19:29,440
people can sort of find the echo chamber

616
00:19:27,919 --> 00:19:32,160
model that reflects their own beliefs

617
00:19:29,440 --> 00:19:34,000
and reflects that back to them? No, I

618
00:19:32,160 --> 00:19:36,480
don't think so. It's not clear to me.

619
00:19:34,000 --> 00:19:38,080
And I think this just is a reflection of

620
00:19:36,480 --> 00:19:40,960
a lot of the problems that you see with

621
00:19:38,080 --> 00:19:42,480
polarized media broadly speaking even

622
00:19:40,960 --> 00:19:44,640
before AI, right? So you you know

623
00:19:42,480 --> 00:19:46,559
polarization was a a problem that people

624
00:19:44,640 --> 00:19:48,480
started to talk about in social media in

625
00:19:46,559 --> 00:19:50,640
like the 2010s long before we had

626
00:19:48,480 --> 00:19:52,559
generative AI. I think some of that is

627
00:19:50,640 --> 00:19:54,480
feeding back into these models. Right

628
00:19:52,559 --> 00:19:56,600
now actually the the fact that it is so

629
00:19:54,480 --> 00:19:59,520
hard and expensive to train them means

630
00:19:56,600 --> 00:20:01,360
that you don't actually see that much

631
00:19:59,520 --> 00:20:04,000
sort of split and polarization between

632
00:20:01,360 --> 00:20:05,520
them yet. But there's a chance that as

633
00:20:04,000 --> 00:20:06,960
they become cheaper to to train and

634
00:20:05,520 --> 00:20:09,039
people can really self- select the model

635
00:20:06,960 --> 00:20:10,720
that agrees with their beliefs the most,

636
00:20:09,039 --> 00:20:12,240
you'll actually see more polarization

637
00:20:10,720 --> 00:20:13,919
than you do in social media simply

638
00:20:12,240 --> 00:20:15,760
because media require has a sort of

639
00:20:13,919 --> 00:20:17,440
aworked effect. Right? So in order for a

640
00:20:15,760 --> 00:20:18,720
social media platform to be useful,

641
00:20:17,440 --> 00:20:20,640
there needs to be enough other people

642
00:20:18,720 --> 00:20:21,840
using it otherwise it's not a useful

643
00:20:20,640 --> 00:20:23,360
media platform. But that's not

644
00:20:21,840 --> 00:20:24,799
necessarily true with AI models, right?

645
00:20:23,360 --> 00:20:25,919
If I'm just interacting with this model

646
00:20:24,799 --> 00:20:27,520
over and over again, I don't need

647
00:20:25,919 --> 00:20:29,200
anybody else to share my beliefs. I

648
00:20:27,520 --> 00:20:30,960
don't need a, you know, platform, a

649
00:20:29,200 --> 00:20:32,880
cohesive group of people who all feel

650
00:20:30,960 --> 00:20:34,080
the same way. I just need, you know, a

651
00:20:32,880 --> 00:20:35,600
small number of people who are willing

652
00:20:34,080 --> 00:20:36,880
to train the model to behave that way.

653
00:20:35,600 --> 00:20:39,320
So, I think there's actually a potential

654
00:20:36,880 --> 00:20:41,600
for it to get worse than it is on social

655
00:20:39,320 --> 00:20:42,880
media, but I I don't really have a good

656
00:20:41,600 --> 00:20:45,880
prediction as to whether that's going to

657
00:20:42,880 --> 00:20:45,880
happen.

658
00:20:46,400 --> 00:20:50,159
Um, thanks for the interesting talk. Um,

659
00:20:48,640 --> 00:20:53,200
I'm I'm wondering more on the content

660
00:20:50,159 --> 00:20:56,320
generation sort of side of things. Um I

661
00:20:53,200 --> 00:20:58,320
guess one argument could be that um you

662
00:20:56,320 --> 00:21:00,640
know even if you have the same AI being

663
00:20:58,320 --> 00:21:03,120
used by everyone everyone is using it

664
00:21:00,640 --> 00:21:05,039
differently right so as long as you have

665
00:21:03,120 --> 00:21:07,440
sufficient variation in the inputs to

666
00:21:05,039 --> 00:21:09,200
the AI even if it's the same AI the

667
00:21:07,440 --> 00:21:10,480
outputs can be still dramatically

668
00:21:09,200 --> 00:21:13,440
different

669
00:21:10,480 --> 00:21:16,960
um so so in given that how different

670
00:21:13,440 --> 00:21:18,480
would this be say to like just using a

671
00:21:16,960 --> 00:21:20,000
search engine which would also you know

672
00:21:18,480 --> 00:21:23,280
sort of give similar results to people

673
00:21:20,000 --> 00:21:25,280
if you input the same things it um like

674
00:21:23,280 --> 00:21:28,159
where is the where's the key difference

675
00:21:25,280 --> 00:21:30,720
here between yeah so this is a great

676
00:21:28,159 --> 00:21:32,400
question the

677
00:21:30,720 --> 00:21:33,840
so a lot of the variation that you're

678
00:21:32,400 --> 00:21:36,000
going to get if people are interacting

679
00:21:33,840 --> 00:21:38,159
with different ways are variation in

680
00:21:36,000 --> 00:21:39,760
sort of the dimensions in which people

681
00:21:38,159 --> 00:21:41,200
know that they want differences so if I

682
00:21:39,760 --> 00:21:42,799
ask a different question from you we're

683
00:21:41,200 --> 00:21:44,559
obviously going to get different answers

684
00:21:42,799 --> 00:21:46,240
and we have to sort of communicate the

685
00:21:44,559 --> 00:21:47,520
fact that we want diff that we are

686
00:21:46,240 --> 00:21:50,159
asking questions different questions in

687
00:21:47,520 --> 00:21:51,919
the first place I'm more sort of

688
00:21:50,159 --> 00:21:54,400
interested in the subconscious choices

689
00:21:51,919 --> 00:21:55,919
that get made. So one way of viewing

690
00:21:54,400 --> 00:21:58,000
what these models are doing is sort of

691
00:21:55,919 --> 00:21:59,840
decompression of the inputs, right? I

692
00:21:58,000 --> 00:22:01,200
provide some very concise question or

693
00:21:59,840 --> 00:22:03,520
something like that and it blows it up

694
00:22:01,200 --> 00:22:04,799
into a very long response. In order to

695
00:22:03,520 --> 00:22:06,559
do that, it has to make a bunch of

696
00:22:04,799 --> 00:22:08,320
choices on my behalf, right? I didn't

697
00:22:06,559 --> 00:22:09,840
specify, you know, what I wanted the

698
00:22:08,320 --> 00:22:10,960
variables to be named when I asked it to

699
00:22:09,840 --> 00:22:12,400
solve a math problem. And maybe I don't

700
00:22:10,960 --> 00:22:14,960
really care that much about that. I

701
00:22:12,400 --> 00:22:16,640
didn't specify, you know, the style or

702
00:22:14,960 --> 00:22:18,320
the tone that I wanted it to use when it

703
00:22:16,640 --> 00:22:20,159
responded to me. It just sort of made a

704
00:22:18,320 --> 00:22:22,080
decision on my behalf there, right? And

705
00:22:20,159 --> 00:22:23,760
so it's less the higher order bits that

706
00:22:22,080 --> 00:22:25,600
I'm worried about like the actual intent

707
00:22:23,760 --> 00:22:27,360
of the query that I'm producing. It's

708
00:22:25,600 --> 00:22:29,120
more the lower order bits of the things

709
00:22:27,360 --> 00:22:30,960
that get left unsaid, right? The things

710
00:22:29,120 --> 00:22:32,960
that I'm not specifying and it's doing

711
00:22:30,960 --> 00:22:35,039
this lossy decompression. It's going to

712
00:22:32,960 --> 00:22:37,120
infer a bunch of stuff on my behalf. I

713
00:22:35,039 --> 00:22:39,039
think it matters to some extent whether

714
00:22:37,120 --> 00:22:40,400
it uh whether there's heterogeneity

715
00:22:39,039 --> 00:22:43,280
there, right? Like I say, you know, make

716
00:22:40,400 --> 00:22:44,960
me a clip art that I can use in my uh my

717
00:22:43,280 --> 00:22:46,960
talk here and it's going to do that in a

718
00:22:44,960 --> 00:22:48,320
certain style. It would be nice if

719
00:22:46,960 --> 00:22:49,919
different models actually did that in

720
00:22:48,320 --> 00:22:51,440
different styles. So there's some more

721
00:22:49,919 --> 00:22:54,799
sort of information diversity in the

722
00:22:51,440 --> 00:22:56,559
world. That's the kind of sort of

723
00:22:54,799 --> 00:22:58,400
diversity that I would see there and

724
00:22:56,559 --> 00:22:59,919
that's even conditioned on us you know

725
00:22:58,400 --> 00:23:03,360
communicating our intent with the right

726
00:22:59,919 --> 00:23:05,039
questions. Okay great.

727
00:23:03,360 --> 00:23:06,559
Uh thank you for this presentation. It

728
00:23:05,039 --> 00:23:08,559
was very interesting. I would like to

729
00:23:06,559 --> 00:23:10,799
ask one question about an example you

730
00:23:08,559 --> 00:23:12,799
made about the rental units. So if there

731
00:23:10,799 --> 00:23:15,360
is an AI model which can basically set

732
00:23:12,799 --> 00:23:17,840
the price for a unit. How would you get

733
00:23:15,360 --> 00:23:19,360
diversification in this area since most

734
00:23:17,840 --> 00:23:21,360
of the owners would just use a model

735
00:23:19,360 --> 00:23:22,880
which would maximize their profit? How

736
00:23:21,360 --> 00:23:25,280
would you tackle this issue? Oh,

737
00:23:22,880 --> 00:23:26,799
absolutely. I I don't think the unit

738
00:23:25,280 --> 00:23:28,080
owners or the the building owners

739
00:23:26,799 --> 00:23:29,440
actually have strong incentives to

740
00:23:28,080 --> 00:23:30,799
diversify there and that's what the law

741
00:23:29,440 --> 00:23:32,880
is supposed to do basically. So the

742
00:23:30,799 --> 00:23:34,640
question is is the law is antitrust law

743
00:23:32,880 --> 00:23:36,720
sufficient force to push back against

744
00:23:34,640 --> 00:23:38,480
that and I think the current answer is

745
00:23:36,720 --> 00:23:41,360
maybe and it depends on how these cases

746
00:23:38,480 --> 00:23:43,520
go right. So if you know the courts find

747
00:23:41,360 --> 00:23:45,679
that actually all these states suing

748
00:23:43,520 --> 00:23:47,039
real page have no grounds and you know

749
00:23:45,679 --> 00:23:48,720
real page is just doing something that

750
00:23:47,039 --> 00:23:51,120
is sort of economically rational for

751
00:23:48,720 --> 00:23:53,440
them and not illegal then yeah antitrust

752
00:23:51,120 --> 00:23:54,559
law does not have the force uh required

753
00:23:53,440 --> 00:23:56,480
and at that point I don't really know

754
00:23:54,559 --> 00:23:57,400
what the answer is. I think then you end

755
00:23:56,480 --> 00:24:00,480
up

756
00:23:57,400 --> 00:24:03,280
with incentives to create a monoculture

757
00:24:00,480 --> 00:24:06,559
and no legal recourse for people who

758
00:24:03,280 --> 00:24:09,760
think that's sort of bad for consumers.

759
00:24:06,559 --> 00:24:14,200
I'm afraid it's 1210. So, uh, lunchtime.

760
00:24:09,760 --> 00:24:14,200
Thank you so much to Manish.

