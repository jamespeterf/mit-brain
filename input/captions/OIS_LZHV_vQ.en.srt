1
00:00:05,680 --> 00:00:12,360
So I'm Marco Turkete. I'm a MIT PhD

2
00:00:09,519 --> 00:00:16,000
alumna and co-founder of

3
00:00:12,360 --> 00:00:18,960
EVA. Uh so uh while it's pretty clear

4
00:00:16,000 --> 00:00:23,519
that AI is revolutionizing many

5
00:00:18,960 --> 00:00:25,680
industries, uh the um the uh the actual

6
00:00:23,519 --> 00:00:29,279
bottleneck right now is actually the

7
00:00:25,680 --> 00:00:31,920
availability of compute. In fact, if you

8
00:00:29,279 --> 00:00:36,320
look at this uh this graph, you can see

9
00:00:31,920 --> 00:00:38,640
that um uh to train the next generation

10
00:00:36,320 --> 00:00:42,480
state-of-the-art model requires every

11
00:00:38,640 --> 00:00:45,680
year an improvement in uh uh 4x in the

12
00:00:42,480 --> 00:00:48,800
uh in the computation demand. Instead,

13
00:00:45,680 --> 00:00:52,239
uh at the same time, the GPUs are

14
00:00:48,800 --> 00:00:54,559
improving only by 30% year-over-year,

15
00:00:52,239 --> 00:00:58,000
which is uh impressive, but not enough

16
00:00:54,559 --> 00:01:01,120
to meet the demand. And moreover, as you

17
00:00:58,000 --> 00:01:04,000
can see, uh to train this next

18
00:01:01,120 --> 00:01:07,680
state-of-the-art model, uh it's already

19
00:01:04,000 --> 00:01:09,280
requiring uh more than $200 million. And

20
00:01:07,680 --> 00:01:13,840
this is just the tip of the iceberg

21
00:01:09,280 --> 00:01:16,159
because actually the uh compute cost uh

22
00:01:13,840 --> 00:01:18,799
that is required to develop such a model

23
00:01:16,159 --> 00:01:22,880
is actually already in the five

24
00:01:18,799 --> 00:01:25,840
billions. Uh so uh what are we doing at

25
00:01:22,880 --> 00:01:28,560
to solve this uh this issue? So at we

26
00:01:25,840 --> 00:01:32,799
are developing the next generation of uh

27
00:01:28,560 --> 00:01:36,320
uh clusters which uh uh are going to um

28
00:01:32,799 --> 00:01:39,200
are going to give an improvement of 72x

29
00:01:36,320 --> 00:01:42,000
uh in terms of throughput per dollar uh

30
00:01:39,200 --> 00:01:45,520
uh with respect to uh Nvidia uh

31
00:01:42,000 --> 00:01:47,920
blackwell. So if you look at for example

32
00:01:45,520 --> 00:01:52,079
uh how this translate in the training of

33
00:01:47,920 --> 00:01:54,320
a um um like a model such as llama 3.1

34
00:01:52,079 --> 00:01:57,600
you can see that uh with this kind of

35
00:01:54,320 --> 00:02:02,320
cluster the improvement in uh time to

36
00:01:57,600 --> 00:02:05,119
train is u uh from u um uh 80 goes from

37
00:02:02,320 --> 00:02:08,160
80 days to less than two days and the

38
00:02:05,119 --> 00:02:12,080
cost to train this model goes from $47

39
00:02:08,160 --> 00:02:16,480
million to alpha million. So how are we

40
00:02:12,080 --> 00:02:18,720
able to uh to do that? So uh the way we

41
00:02:16,480 --> 00:02:20,720
are uh we are approaching this problem

42
00:02:18,720 --> 00:02:23,200
is that we are building actually the

43
00:02:20,720 --> 00:02:26,879
deepest AI technology stack ever. In

44
00:02:23,200 --> 00:02:30,560
fact, EVA is built upon a breakthrough

45
00:02:26,879 --> 00:02:33,519
in semiconductor technology uh that is

46
00:02:30,560 --> 00:02:37,040
down at the level of the transistors and

47
00:02:33,519 --> 00:02:40,640
at the same time we are uh um we are

48
00:02:37,040 --> 00:02:42,720
doing an approach uh that is uh uh that

49
00:02:40,640 --> 00:02:45,519
comprises like uh uh hardware and

50
00:02:42,720 --> 00:02:50,400
software code development such that we

51
00:02:45,519 --> 00:02:54,080
are able to uh uh be um compatible with

52
00:02:50,400 --> 00:02:56,720
uh legacy libraries and uh and also we

53
00:02:54,080 --> 00:03:00,920
are going to be able therefore to have a

54
00:02:56,720 --> 00:03:04,319
turnkey uh deployment of our

55
00:03:00,920 --> 00:03:08,000
hardware which brings me to my uh last

56
00:03:04,319 --> 00:03:11,440
point. So uh which is we are uh uh uh

57
00:03:08,000 --> 00:03:13,920
want to invite you in a uh partnership

58
00:03:11,440 --> 00:03:16,879
uh with us a collaboration. In fact, we

59
00:03:13,920 --> 00:03:20,319
would like to invite all enterprises

60
00:03:16,879 --> 00:03:25,519
that requires a lot of uh uh compute uh

61
00:03:20,319 --> 00:03:28,800
to um uh to uh adopt to uh on board on

62
00:03:25,519 --> 00:03:30,879
our digital twin platform. Uh the reason

63
00:03:28,800 --> 00:03:34,640
why we're doing this is because we would

64
00:03:30,879 --> 00:03:39,120
like to um uh our future customer to be

65
00:03:34,640 --> 00:03:41,840
able to uh uh run their tasks uh on uh

66
00:03:39,120 --> 00:03:44,560
on our new architecture seamlessly. Uh

67
00:03:41,840 --> 00:03:48,560
this uh will allow you essentially when

68
00:03:44,560 --> 00:03:52,959
we uh deploy our uh um our clusters in

69
00:03:48,560 --> 00:03:56,879
Q1 of 2027 uh to transfer uh

70
00:03:52,959 --> 00:04:00,159
effortlessly uh all your uh uh AI cloud

71
00:03:56,879 --> 00:04:02,879
compute uh to our hardware and and and

72
00:04:00,159 --> 00:04:05,360
so enjoy all the uh improvement in terms

73
00:04:02,879 --> 00:04:09,599
of performance and cost that I uh talked

74
00:04:05,360 --> 00:04:12,159
before. And um um and uh therefore uh uh

75
00:04:09,599 --> 00:04:13,840
if you are uh uh from one of the

76
00:04:12,159 --> 00:04:15,519
industries such as the one that I'm

77
00:04:13,840 --> 00:04:20,120
listing on the right hand side of the

78
00:04:15,519 --> 00:04:24,720
slide uh so from uh generative AI or uh

79
00:04:20,120 --> 00:04:27,520
BFSI uh biioarma or defense and so on.

80
00:04:24,720 --> 00:04:30,840
Uh uh I would like to uh invite you to

81
00:04:27,520 --> 00:04:35,400
join us in building the uh future of AI

82
00:04:30,840 --> 00:04:35,400
together. Thank you very much.

83
00:04:36,400 --> 00:04:39,880
Thank you, Marco.

