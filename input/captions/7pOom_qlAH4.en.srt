1
00:00:00,746 --> 00:00:02,765
(air whooshing)

2
00:00:02,765 --> 00:00:05,437
(text buzzing)

3
00:00:05,437 --> 00:00:06,780
- "What are the current limitations

4
00:00:06,780 --> 00:00:08,703
of LLMs in financial contexts

5
00:00:08,703 --> 00:00:11,670
and how might future advancements in areas

6
00:00:11,670 --> 00:00:14,970
like causal reasoning and
knowledge representation

7
00:00:14,970 --> 00:00:17,340
unlock new possibilities?"

8
00:00:17,340 --> 00:00:20,010
Well, that's a really
interesting question.

9
00:00:20,010 --> 00:00:21,870
There are a number of limitations

10
00:00:21,870 --> 00:00:24,450
that I think many people know about.

11
00:00:24,450 --> 00:00:28,805
LLMs are not that good at numbers per se.

12
00:00:28,805 --> 00:00:31,680
They're very good with language,

13
00:00:31,680 --> 00:00:32,970
but for whatever reason,

14
00:00:32,970 --> 00:00:35,550
when you start asking numerical questions,

15
00:00:35,550 --> 00:00:37,620
they easily get tripped up.

16
00:00:37,620 --> 00:00:40,143
A friend of mine recently
just gave me an example.

17
00:00:41,130 --> 00:00:45,932
Try asking LLM to name all of the capitals

18
00:00:45,932 --> 00:00:50,932
of European countries and
rank them by population.

19
00:00:52,198 --> 00:00:55,510
That seems like a pretty
straightforward question,

20
00:00:55,510 --> 00:00:58,230
but you'd be surprised how difficult it is

21
00:00:58,230 --> 00:01:01,770
for typical LLMs to get that correct.

22
00:01:01,770 --> 00:01:03,630
And there are all sorts of conjectures

23
00:01:03,630 --> 00:01:07,320
as to why that might be the
case, but we don't really know.

24
00:01:07,320 --> 00:01:10,770
And so at this point, we
need to start coming up

25
00:01:10,770 --> 00:01:14,880
with better ways of informing LLMs

26
00:01:14,880 --> 00:01:16,950
with respect to these kinds
of numerical questions

27
00:01:16,950 --> 00:01:18,418
and to be able to integrate them

28
00:01:18,418 --> 00:01:21,360
into their text-based abilities.

29
00:01:21,360 --> 00:01:24,120
That's a key aspect of financial context

30
00:01:24,120 --> 00:01:25,260
because everything that we do

31
00:01:25,260 --> 00:01:28,980
in finance is both numbers and language.

32
00:01:28,980 --> 00:01:30,720
In terms of causal reasoning,

33
00:01:30,720 --> 00:01:34,050
now this is getting on the
border of the theoretical

34
00:01:34,050 --> 00:01:36,840
and beyond it the science fiction.

35
00:01:36,840 --> 00:01:40,140
The question that I sometimes
get asked by my students is,

36
00:01:40,140 --> 00:01:42,918
do LMS really understand?

37
00:01:42,918 --> 00:01:44,130
And you know,

38
00:01:44,130 --> 00:01:46,050
I have to answer the
question with the question,

39
00:01:46,050 --> 00:01:48,663
which is, well, what do
you mean by understand?

40
00:01:49,830 --> 00:01:54,830
And that's a very difficult
concept to deconstruct

41
00:01:55,200 --> 00:01:58,470
because we ourselves don't understand

42
00:01:58,470 --> 00:02:00,300
what we mean by understand.

43
00:02:00,300 --> 00:02:01,650
But lemme give you an idea

44
00:02:01,650 --> 00:02:04,983
of how I'm beginning to
understand understanding.

45
00:02:05,970 --> 00:02:07,080
The way that you test

46
00:02:07,080 --> 00:02:09,240
whether somebody really
understands something is very much

47
00:02:09,240 --> 00:02:11,160
like how we test our PhD candidates

48
00:02:11,160 --> 00:02:12,750
when we give 'em the oral exam.

49
00:02:12,750 --> 00:02:15,090
And I'll tell you how I give my oral exam.

50
00:02:15,090 --> 00:02:16,803
I'll start with an easy question,

51
00:02:17,730 --> 00:02:19,680
and if the student

52
00:02:19,680 --> 00:02:22,350
that's taking the exam
answers it correctly,

53
00:02:22,350 --> 00:02:25,410
I'll move on, and I'll
give 'em a harder version.

54
00:02:25,410 --> 00:02:26,790
And if they answer that correctly,

55
00:02:26,790 --> 00:02:30,090
I'll give them yet another
version that's even harder.

56
00:02:30,090 --> 00:02:32,979
And I'll keep increasing
the level of difficulty

57
00:02:32,979 --> 00:02:35,667
and changing the context of the question

58
00:02:35,667 --> 00:02:40,140
to really probe the limits
of that student's knowledge.

59
00:02:40,140 --> 00:02:43,830
Because what I wanna understand
is how much they understand,

60
00:02:43,830 --> 00:02:46,770
how deep is their understanding.

61
00:02:46,770 --> 00:02:49,620
So at some point, I'll get to the point

62
00:02:49,620 --> 00:02:52,425
where a student will get it wrong,

63
00:02:52,425 --> 00:02:56,100
and that tells me that they
don't really understand

64
00:02:56,100 --> 00:02:58,530
the particular question that I'm asking.

65
00:02:58,530 --> 00:03:00,930
And I'll ask them one
more question after that,

66
00:03:00,930 --> 00:03:02,400
that is deeper still,

67
00:03:02,400 --> 00:03:05,610
just to see whether or not it
was just a careless mistake

68
00:03:05,610 --> 00:03:07,080
or whether or not it's really true

69
00:03:07,080 --> 00:03:09,060
that now we're in deeper waters

70
00:03:09,060 --> 00:03:11,133
that they really haven't ever tread.

71
00:03:12,510 --> 00:03:16,710
And if they continue to
falter, I will have determined

72
00:03:16,710 --> 00:03:19,143
that that's their level of understanding.

73
00:03:19,980 --> 00:03:23,220
So let's apply that approach
to a large language model.

74
00:03:23,220 --> 00:03:25,920
We ask it a question,
it provides an answer,

75
00:03:25,920 --> 00:03:27,240
and the answer seems correct.

76
00:03:27,240 --> 00:03:30,840
We'll provide a harder version,
and that seems correct.

77
00:03:30,840 --> 00:03:33,360
At some point, we will reach a level

78
00:03:33,360 --> 00:03:38,360
where the LLM will not be
able to answer correctly.

79
00:03:38,406 --> 00:03:41,760
And so you can think of it as going down

80
00:03:41,760 --> 00:03:44,820
below the level of knowledge to the point

81
00:03:44,820 --> 00:03:49,200
where there's no more
answers to be gotten.

82
00:03:49,200 --> 00:03:54,180
That kind of depth of knowledge
is a form of understanding.

83
00:03:54,180 --> 00:03:57,720
Right now, I would say that
LLMs are quite shallow.

84
00:03:57,720 --> 00:04:00,360
They can answer in complete sentences.

85
00:04:00,360 --> 00:04:03,300
Most of those sentences make sense.

86
00:04:03,300 --> 00:04:05,400
And if you ask them questions,

87
00:04:05,400 --> 00:04:08,610
they can usually provide
reasonably correct answers.

88
00:04:08,610 --> 00:04:11,340
If you ask them to write a piece of code

89
00:04:11,340 --> 00:04:13,530
that engages in certain calculations,

90
00:04:13,530 --> 00:04:15,750
they can usually do that correctly,

91
00:04:15,750 --> 00:04:17,640
perhaps with some modification.

92
00:04:17,640 --> 00:04:19,080
But that tells me that their level

93
00:04:19,080 --> 00:04:20,733
of understanding is very shallow.

94
00:04:21,840 --> 00:04:24,450
The next generation of LLMs

95
00:04:24,450 --> 00:04:26,970
ought to be able to answer questions

96
00:04:26,970 --> 00:04:30,333
at 2, 3, 4 levels of depth.

97
00:04:30,333 --> 00:04:33,600
And there will come a point in time

98
00:04:33,600 --> 00:04:36,930
where they will exceed our capacity

99
00:04:36,930 --> 00:04:39,090
to answer those questions.

100
00:04:39,090 --> 00:04:39,930
And at that point,

101
00:04:39,930 --> 00:04:44,010
I believe we will then
say they understand,

102
00:04:44,010 --> 00:04:48,120
and then causal reasoning
will be possible.

103
00:04:48,120 --> 00:04:52,112
So it's coming, but in the world of LLMs,

104
00:04:52,112 --> 00:04:55,080
it's gonna take a while
before they get to the point

105
00:04:55,080 --> 00:04:58,800
where we consider it to be
truly causal understanding.

106
00:04:58,800 --> 00:05:02,647
We've got a question from one
of our social media followers.

107
00:05:02,647 --> 00:05:04,080
"Applications of machine learning

108
00:05:04,080 --> 00:05:05,880
to forecast in clinical trial outcomes

109
00:05:05,880 --> 00:05:07,620
and improving investment performance

110
00:05:07,620 --> 00:05:09,600
in the biopharma industry."

111
00:05:09,600 --> 00:05:10,950
Thank you for that.

112
00:05:10,950 --> 00:05:13,080
That's referring to some research

113
00:05:13,080 --> 00:05:14,820
that I've done over the last few years

114
00:05:14,820 --> 00:05:17,100
with a number of my collaborators

115
00:05:17,100 --> 00:05:19,050
applying machine learning techniques

116
00:05:19,050 --> 00:05:21,960
to predicting the outcome
of clinical trials.

117
00:05:21,960 --> 00:05:23,010
I got interested in that

118
00:05:23,010 --> 00:05:25,920
because a number of friends
and family were dealing

119
00:05:25,920 --> 00:05:28,290
with various kinds of
cancer a few years ago,

120
00:05:28,290 --> 00:05:32,940
and I realized that actually
money plays a huge role

121
00:05:32,940 --> 00:05:34,170
in drug development.

122
00:05:34,170 --> 00:05:36,390
It turns out that money can be scarce,

123
00:05:36,390 --> 00:05:39,660
particularly at the very
earliest stages of drug discovery

124
00:05:39,660 --> 00:05:42,060
because that's when the
risk is the highest.

125
00:05:42,060 --> 00:05:45,570
And so the so-called valley
of death between, you know,

126
00:05:45,570 --> 00:05:49,178
early stage research and the
first in human clinical trials,

127
00:05:49,178 --> 00:05:51,960
that's the area that I
started thinking about

128
00:05:51,960 --> 00:05:54,720
how do we get more money
to come into that space

129
00:05:54,720 --> 00:05:57,360
to help get over this valley of death?

130
00:05:57,360 --> 00:06:00,720
And the answer of course
is you need investors,

131
00:06:00,720 --> 00:06:02,340
but how do you attract investors?

132
00:06:02,340 --> 00:06:05,700
The way you attract
them is by demonstrating

133
00:06:05,700 --> 00:06:08,340
that there are some
attractive opportunities

134
00:06:08,340 --> 00:06:10,590
to make money with their capital.

135
00:06:10,590 --> 00:06:12,300
And in order to do that,

136
00:06:12,300 --> 00:06:15,510
you need to have some kind
of statistical framework

137
00:06:15,510 --> 00:06:17,310
for thinking about risk and reward.

138
00:06:17,310 --> 00:06:19,320
Over the last few years, my collaborators

139
00:06:19,320 --> 00:06:21,390
and I have applied machine
learning techniques,

140
00:06:21,390 --> 00:06:23,520
and now we're using large language models

141
00:06:23,520 --> 00:06:26,760
to try to read all of
the body of information

142
00:06:26,760 --> 00:06:28,719
about medical clinical trials

143
00:06:28,719 --> 00:06:32,910
and what kinds of features tend
to make them more successful

144
00:06:32,910 --> 00:06:35,460
and other features that
make them less successful.

145
00:06:35,460 --> 00:06:38,400
And use those features to predict outcomes

146
00:06:38,400 --> 00:06:40,410
so that we can tell investors,

147
00:06:40,410 --> 00:06:43,710
under what circumstances
are you likely to do better

148
00:06:43,710 --> 00:06:47,280
or worse, we can actually
provide them with a framework

149
00:06:47,280 --> 00:06:49,230
for thinking about how to manage risk.

150
00:06:49,230 --> 00:06:50,550
And if we do that,

151
00:06:50,550 --> 00:06:52,860
we assume that they will
then be more willing

152
00:06:52,860 --> 00:06:56,310
to deploy their capital as
they have in other industries

153
00:06:56,310 --> 00:06:58,380
where these tools have
been made available.

154
00:06:58,380 --> 00:06:59,213
So stay tuned,

155
00:06:59,213 --> 00:07:01,184
and hopefully we'll be able
to tell you more about that

156
00:07:01,184 --> 00:07:02,700
over the course of the next few years

157
00:07:02,700 --> 00:07:05,640
as we apply these models
in practical circumstances.

158
00:07:05,640 --> 00:07:07,687
Okay, another social media post.

159
00:07:07,687 --> 00:07:10,620
"What's the dark side of AI,
and what's the bright side,

160
00:07:10,620 --> 00:07:12,900
and which is more likely to occur?"

161
00:07:12,900 --> 00:07:16,320
Well, that's the age old
question of good versus evil,

162
00:07:16,320 --> 00:07:18,690
and happy to talk about both.

163
00:07:18,690 --> 00:07:21,990
I think I already mentioned a
bit about the dark side of AI.

164
00:07:21,990 --> 00:07:26,975
Powerful tools can be
used for adverse purposes.

165
00:07:26,975 --> 00:07:30,480
Things like cheating on your tax returns,

166
00:07:30,480 --> 00:07:32,010
coming up with fake news

167
00:07:32,010 --> 00:07:35,580
that will fool a certain
part of the population

168
00:07:35,580 --> 00:07:38,460
for financial or political gain.

169
00:07:38,460 --> 00:07:40,050
Those are the kinds of things

170
00:07:40,050 --> 00:07:42,750
that not only can AI be used for,

171
00:07:42,750 --> 00:07:45,840
but I suspect that has
already been used for.

172
00:07:45,840 --> 00:07:47,730
And so I think that we really need

173
00:07:47,730 --> 00:07:51,060
to consider ways of guarding against that.

174
00:07:51,060 --> 00:07:55,890
We need antidotes for fake
news and for manipulation.

175
00:07:55,890 --> 00:08:00,093
That really is not what we
intend with these amazing tools.

176
00:08:00,960 --> 00:08:05,310
On the positive side, I think
there is tremendous potential

177
00:08:05,310 --> 00:08:08,250
for AI to transform society.

178
00:08:08,250 --> 00:08:10,380
I'll give you one very,
very simple example

179
00:08:10,380 --> 00:08:11,703
that's already being done.

180
00:08:12,780 --> 00:08:15,480
If you take patient medical records

181
00:08:15,480 --> 00:08:18,570
and look at the various
different kinds of behaviors

182
00:08:18,570 --> 00:08:21,090
that people engage in

183
00:08:21,090 --> 00:08:23,940
and the diseases that they suffer from,

184
00:08:23,940 --> 00:08:27,660
we now are able to come up
with all sorts of hypotheses

185
00:08:27,660 --> 00:08:29,610
for certain kinds of things

186
00:08:29,610 --> 00:08:32,520
that we can do to improve our health.

187
00:08:32,520 --> 00:08:35,550
Now you already know some of
those things, like exercise,

188
00:08:35,550 --> 00:08:40,170
have a proper balanced diet,
sleep well, so on and so forth.

189
00:08:40,170 --> 00:08:42,000
But there are lots of nuances

190
00:08:42,000 --> 00:08:44,520
to those broad-based pieces of advice

191
00:08:44,520 --> 00:08:47,760
that have to be tailored just to you.

192
00:08:47,760 --> 00:08:50,640
And so this is where
LLMs can really shine.

193
00:08:50,640 --> 00:08:53,670
They can take your medical
records, they take the sum total

194
00:08:53,670 --> 00:08:56,010
of all of the knowledge
that's in the medical system

195
00:08:56,010 --> 00:08:58,710
and ultimately tailor that advice

196
00:08:58,710 --> 00:09:01,650
to be suited to your
particular health goals.

197
00:09:01,650 --> 00:09:03,330
Now we're not there yet because obviously

198
00:09:03,330 --> 00:09:05,700
the hallucination problem
can be very, very dangerous

199
00:09:05,700 --> 00:09:07,320
in the medical context,

200
00:09:07,320 --> 00:09:09,600
but I don't think we're that far away,

201
00:09:09,600 --> 00:09:11,370
particularly given the rapid pace

202
00:09:11,370 --> 00:09:13,620
at which we're making progress.

203
00:09:13,620 --> 00:09:15,667
Okay, another social media post.

204
00:09:15,667 --> 00:09:19,110
"How can non-engineers
get involved in AI?"

205
00:09:19,110 --> 00:09:20,820
That's a wonderful question.

206
00:09:20,820 --> 00:09:23,640
One of the reasons that I
was so excited by the advent

207
00:09:23,640 --> 00:09:26,160
of broad-based large language models,

208
00:09:26,160 --> 00:09:28,530
and you've already played
with many of them, I'm sure,

209
00:09:28,530 --> 00:09:32,220
large language models have democratized AI

210
00:09:32,220 --> 00:09:36,990
so that you don't have to be
an engineer to make use of AI.

211
00:09:36,990 --> 00:09:39,630
That wasn't true a couple of years ago.

212
00:09:39,630 --> 00:09:41,610
If you wanted to use machine learning,

213
00:09:41,610 --> 00:09:43,920
you needed to have programming experience.

214
00:09:43,920 --> 00:09:46,200
There were very few generic,

215
00:09:46,200 --> 00:09:48,960
non-technical machine
learning engines, if any,

216
00:09:48,960 --> 00:09:50,010
that would allow people

217
00:09:50,010 --> 00:09:52,170
who didn't understand the intricacies

218
00:09:52,170 --> 00:09:55,890
of these various different
random forest models and so on

219
00:09:55,890 --> 00:09:59,550
to be able to use them in any
way that would be beneficial.

220
00:09:59,550 --> 00:10:01,140
With large language models,

221
00:10:01,140 --> 00:10:03,840
we can actually speak in plain English.

222
00:10:03,840 --> 00:10:06,750
And moreover, there are
a number of non-engineers

223
00:10:06,750 --> 00:10:10,650
that I've met who have
become extraordinarily good

224
00:10:10,650 --> 00:10:14,970
at formulating just the right
prompt in order to be able

225
00:10:14,970 --> 00:10:19,200
to get their desired outcome
from these different LLMs.

226
00:10:19,200 --> 00:10:22,410
So I think the answer is absolutely

227
00:10:22,410 --> 00:10:26,490
non-engineers can become
extremely adept at AI.

228
00:10:26,490 --> 00:10:29,256
And the way to do it is to use it,

229
00:10:29,256 --> 00:10:32,490
start playing with your favorite LLM

230
00:10:32,490 --> 00:10:36,210
and just talk to it, ask
it a bunch of questions.

231
00:10:36,210 --> 00:10:37,043
In a way,

232
00:10:37,043 --> 00:10:39,419
that's kind of how we
develop relationships

233
00:10:39,419 --> 00:10:43,410
right now with our human friends.

234
00:10:43,410 --> 00:10:46,980
We begin by asking them questions
and gauging their answers.

235
00:10:46,980 --> 00:10:48,480
And based upon what we hear,

236
00:10:48,480 --> 00:10:50,940
we either decide to become
better friends with them

237
00:10:50,940 --> 00:10:54,123
or decide stay away, we're not interested.

238
00:10:54,960 --> 00:10:59,160
With LLMs, we need to talk with
them in order to understand

239
00:10:59,160 --> 00:11:01,320
how they operate, understand
where they're coming from,

240
00:11:01,320 --> 00:11:03,660
understand what they can do for us.

241
00:11:03,660 --> 00:11:08,190
And once we develop that
kind of, dare I say rapport,

242
00:11:08,190 --> 00:11:12,330
we will then be adept at
being able to prompt them

243
00:11:12,330 --> 00:11:14,010
in the most efficient manner.

244
00:11:14,010 --> 00:11:16,230
You do not have to have
an engineering degree

245
00:11:16,230 --> 00:11:17,130
to be able to do that.

246
00:11:17,130 --> 00:11:20,520
Anybody watching this video can do this,

247
00:11:20,520 --> 00:11:23,853
and I would urge you all to try
doing it as soon as you can.

248
00:11:25,140 --> 00:11:27,247
Another social media post,

249
00:11:27,247 --> 00:11:29,910
"Are cryptocurrencies
a temporary phenomenon,

250
00:11:29,910 --> 00:11:31,890
or are they here to stay?"

251
00:11:31,890 --> 00:11:34,792
Yes, they are here to stay,

252
00:11:34,792 --> 00:11:37,500
but no, the cryptocurrency

253
00:11:37,500 --> 00:11:40,953
that your trading may not be here forever.

254
00:11:42,030 --> 00:11:45,720
The idea of cryptocurrency
is extremely powerful,

255
00:11:45,720 --> 00:11:48,450
and it provides enormous benefits

256
00:11:48,450 --> 00:11:53,010
for financial transactions of
a variety of different sorts.

257
00:11:53,010 --> 00:11:56,850
At the same time, it also
has a number of flaws

258
00:11:56,850 --> 00:11:59,910
that can be exploited by bad actors

259
00:11:59,910 --> 00:12:02,850
and the improper usage of crypto.

260
00:12:02,850 --> 00:12:05,190
A lot's gonna change over the
course of the next few years.

261
00:12:05,190 --> 00:12:06,540
In fact, one of the biggest changes

262
00:12:06,540 --> 00:12:08,730
that I think we all
should all watch out for

263
00:12:08,730 --> 00:12:12,900
is when a very large and
stable sovereign entity decides

264
00:12:12,900 --> 00:12:15,420
to issue crypto of its own.

265
00:12:15,420 --> 00:12:18,030
For example, imagine the
United States decides

266
00:12:18,030 --> 00:12:22,500
to issue Fed Coin and that
becomes the cryptocurrency

267
00:12:22,500 --> 00:12:25,260
of preference for the entire world.

268
00:12:25,260 --> 00:12:26,490
If that happens,

269
00:12:26,490 --> 00:12:30,240
then the existing cryptocurrencies
may not do as well.

270
00:12:30,240 --> 00:12:31,073
Without a doubt,

271
00:12:31,073 --> 00:12:33,960
some of them will become
worthless overnight.

272
00:12:33,960 --> 00:12:35,490
And if that's your currency

273
00:12:35,490 --> 00:12:36,930
that you're holding your wealth in,

274
00:12:36,930 --> 00:12:38,910
that's not good news for you.

275
00:12:38,910 --> 00:12:41,850
In a way, this happened in the 1800s,

276
00:12:41,850 --> 00:12:46,410
because as the west was
being developed in the US,

277
00:12:46,410 --> 00:12:51,210
a number of banks arose to
finance that kind of development,

278
00:12:51,210 --> 00:12:54,750
and these banks would
print their own currency

279
00:12:54,750 --> 00:12:57,750
to be able to hand out to
workers that wanted to get paid.

280
00:12:57,750 --> 00:12:59,670
And of course, these are like bonds

281
00:12:59,670 --> 00:13:01,650
that the banks were issuing.

282
00:13:01,650 --> 00:13:03,540
The problem is that every once in a while

283
00:13:03,540 --> 00:13:06,210
these pieces of paper
would become worthless

284
00:13:06,210 --> 00:13:07,800
because the bank would go outta business,

285
00:13:07,800 --> 00:13:09,932
they would extend too far,

286
00:13:09,932 --> 00:13:12,540
and they couldn't basically make good

287
00:13:12,540 --> 00:13:14,820
on all of these pieces of paper.

288
00:13:14,820 --> 00:13:16,260
Now, these pieces of paper

289
00:13:16,260 --> 00:13:18,510
often had pictures of wildcats on them

290
00:13:18,510 --> 00:13:20,100
because that's what,

291
00:13:20,100 --> 00:13:23,580
that's the animal that dominated
the population of the west

292
00:13:23,580 --> 00:13:24,930
as it was being developed.

293
00:13:24,930 --> 00:13:27,990
And so this phenomenon
called wildcat banking

294
00:13:27,990 --> 00:13:31,080
became quite prominent in the mid-1800s.

295
00:13:31,080 --> 00:13:34,980
And at some point, the number
of failures became so bad

296
00:13:34,980 --> 00:13:37,020
that the US government stepped in

297
00:13:37,020 --> 00:13:40,950
and basically shut down this
whole wildcat banking system

298
00:13:40,950 --> 00:13:43,290
by creating the Federal Reserve

299
00:13:43,290 --> 00:13:47,162
and providing a much broader base of laws

300
00:13:47,162 --> 00:13:49,260
and the US currency

301
00:13:49,260 --> 00:13:52,560
that supplanted all of
these wildcat currencies.

302
00:13:52,560 --> 00:13:54,570
So pretty much overnight,

303
00:13:54,570 --> 00:13:57,960
as the US began to issue its own currency,

304
00:13:57,960 --> 00:14:02,960
that made these particular
smaller currencies worthless.

305
00:14:03,060 --> 00:14:06,930
If a sovereign entity were
to issue a cryptocurrency

306
00:14:06,930 --> 00:14:09,866
and that ended up taking the place

307
00:14:09,866 --> 00:14:13,590
of a number of existing
currencies, you need to worry about

308
00:14:13,590 --> 00:14:16,110
whether or not your currency is the one

309
00:14:16,110 --> 00:14:17,730
that's gonna become worthless.

310
00:14:17,730 --> 00:14:21,870
So that's one of the challenges
of investing in crypto.

311
00:14:21,870 --> 00:14:22,703
But without a doubt,

312
00:14:22,703 --> 00:14:26,760
the technology that is used
in crypto is here to stay.

313
00:14:26,760 --> 00:14:29,370
And I believe that
cryptocurrencies themselves

314
00:14:29,370 --> 00:14:31,860
as an asset class are here to stay.

315
00:14:31,860 --> 00:14:33,030
But you need to think about

316
00:14:33,030 --> 00:14:35,700
how to manage that risk very carefully.

317
00:14:35,700 --> 00:14:37,440
Can't believe it, but
this is the last question

318
00:14:37,440 --> 00:14:39,547
from our social media friend.

319
00:14:39,547 --> 00:14:43,110
"How can we deal with large
scale market manipulation

320
00:14:43,110 --> 00:14:46,590
now that LMS are so cheap
to deploy at large scale?"

321
00:14:46,590 --> 00:14:48,450
Wow, what a question to end on.

322
00:14:48,450 --> 00:14:50,490
So it turns out that one of the things

323
00:14:50,490 --> 00:14:54,990
that large language models
could be very capable of

324
00:14:54,990 --> 00:14:59,250
is providing fraudsters
and other bad actors

325
00:14:59,250 --> 00:15:01,950
with the appropriate technology

326
00:15:01,950 --> 00:15:04,980
for disrupting financial stability.

327
00:15:04,980 --> 00:15:06,720
Now, this may seem farfetched,

328
00:15:06,720 --> 00:15:09,720
but I bet you if you ask
the large language model

329
00:15:09,720 --> 00:15:11,670
how to build an atomic bomb,

330
00:15:11,670 --> 00:15:14,490
you'd probably get a
pretty reasonable answer.

331
00:15:14,490 --> 00:15:19,200
And so it actually scares
me that now the knowledge

332
00:15:19,200 --> 00:15:22,440
of the entire financial
literature is available

333
00:15:22,440 --> 00:15:24,780
to bad actors who can
use large language models

334
00:15:24,780 --> 00:15:28,080
to summarize the ways
that they can involve

335
00:15:28,080 --> 00:15:30,420
their various different nefarious plots

336
00:15:30,420 --> 00:15:32,520
within the financial system.

337
00:15:32,520 --> 00:15:36,420
The regulators also have access
to large language models,

338
00:15:36,420 --> 00:15:39,330
and I believe that they are
also using the tools of AI

339
00:15:39,330 --> 00:15:42,840
to figure out how to detect
these kinds of schemes

340
00:15:42,840 --> 00:15:46,200
while they're being hatched
and before they occur.

341
00:15:46,200 --> 00:15:49,620
But as I said earlier, it is an arms race,

342
00:15:49,620 --> 00:15:50,790
and this is one of the reasons

343
00:15:50,790 --> 00:15:53,640
why we at CSAIL are making these videos.

344
00:15:53,640 --> 00:15:56,310
We want all of you to start thinking about

345
00:15:56,310 --> 00:16:00,750
how to use these tools to
make society a better place.

346
00:16:00,750 --> 00:16:03,840
There's no doubt that
there's tremendous value

347
00:16:03,840 --> 00:16:06,000
in these new technologies,

348
00:16:06,000 --> 00:16:08,580
but there's also some potential dangers

349
00:16:08,580 --> 00:16:10,410
that we have to worry about.

350
00:16:10,410 --> 00:16:12,690
And the only way that we're
gonna get better at it

351
00:16:12,690 --> 00:16:16,027
is if more and better people are involved.

352
00:16:16,027 --> 00:16:18,150
Well, that's it for the questions.

353
00:16:18,150 --> 00:16:20,220
Can't believe that we're all done,

354
00:16:20,220 --> 00:16:23,250
but thank you so much for
taking the time to submit them.

355
00:16:23,250 --> 00:16:24,360
It was fascinating,

356
00:16:24,360 --> 00:16:26,850
and I look forward to seeing
you in the next video.

357
00:16:26,850 --> 00:16:27,683
Bye-Bye.

