1
00:00:05,920 --> 00:00:10,960
We chatted briefly before this and I I I

2
00:00:09,040 --> 00:00:12,400
went through all the different areas of

3
00:00:10,960 --> 00:00:14,480
questions that I was going to ask

4
00:00:12,400 --> 00:00:16,080
everybody while we're up here and I

5
00:00:14,480 --> 00:00:18,000
decided that I would pull the rug out

6
00:00:16,080 --> 00:00:21,439
from under everybody and and and change

7
00:00:18,000 --> 00:00:23,519
and create a new question. Uh, one of

8
00:00:21,439 --> 00:00:25,519
the things I I noticed in in sitting

9
00:00:23,519 --> 00:00:29,039
through the presentations, which were

10
00:00:25,519 --> 00:00:30,960
fantastic, by the way, is that uh

11
00:00:29,039 --> 00:00:34,640
something I'd like to tease out a little

12
00:00:30,960 --> 00:00:37,520
bit more is some of the business use

13
00:00:34,640 --> 00:00:40,160
cases. And and and

14
00:00:37,520 --> 00:00:42,879
when you talk to people at MIT about AI

15
00:00:40,160 --> 00:00:45,680
or almost anything is they they like to

16
00:00:42,879 --> 00:00:47,280
talk about not doing things that are the

17
00:00:45,680 --> 00:00:49,440
obvious. And the obvious thing to do

18
00:00:47,280 --> 00:00:52,640
with AI is well, we could maybe reduce

19
00:00:49,440 --> 00:00:54,239
the number of copywriters we can have.

20
00:00:52,640 --> 00:00:55,680
But the big payoff is when you could

21
00:00:54,239 --> 00:00:58,320
start thinking about your business

22
00:00:55,680 --> 00:01:00,079
differently and transforming it. So, I'm

23
00:00:58,320 --> 00:01:02,559
going to throw out a use case and then

24
00:01:00,079 --> 00:01:04,559
maybe uh there'll be a few others that

25
00:01:02,559 --> 00:01:07,840
you could you could put in there. But

26
00:01:04,559 --> 00:01:12,320
take the example of if I want to book a

27
00:01:07,840 --> 00:01:15,200
trip to London, I'll go on to a travel

28
00:01:12,320 --> 00:01:17,920
site, an aggregator, and I'll search for

29
00:01:15,200 --> 00:01:20,000
my flight, my hotel for London. I'll

30
00:01:17,920 --> 00:01:24,159
book that. Maybe I'll schedule Uber to

31
00:01:20,000 --> 00:01:27,759
pick me up. And when I do that, the

32
00:01:24,159 --> 00:01:29,600
travel site gets paid by a commission

33
00:01:27,759 --> 00:01:31,759
theoretically on the flight that I book

34
00:01:29,600 --> 00:01:33,759
or on the hotel that I book. They'll be

35
00:01:31,759 --> 00:01:35,600
showing me an ad while I'm doing that

36
00:01:33,759 --> 00:01:37,600
and they'll get paid for that ad. And

37
00:01:35,600 --> 00:01:40,000
then when I go to a completely unrelated

38
00:01:37,600 --> 00:01:42,640
site after I book my trip to London,

39
00:01:40,000 --> 00:01:44,720
they'll be offering me uh tickets to a

40
00:01:42,640 --> 00:01:46,320
play in London for some reason. and it

41
00:01:44,720 --> 00:01:49,360
knows that I'm going to London because

42
00:01:46,320 --> 00:01:52,159
they've sold that information to to to

43
00:01:49,360 --> 00:01:54,560
other entities.

44
00:01:52,159 --> 00:01:56,720
Now, when that interface for booking

45
00:01:54,560 --> 00:01:59,280
that trip becomes

46
00:01:56,720 --> 00:02:01,360
pick your LLM, claude, chat, GBT, and I

47
00:01:59,280 --> 00:02:03,920
just tell my agent, book me a trip to

48
00:02:01,360 --> 00:02:05,600
London based on my preferences where I

49
00:02:03,920 --> 00:02:07,520
live and make all the arrangements for

50
00:02:05,600 --> 00:02:10,800
me. And it then it does that on my

51
00:02:07,520 --> 00:02:13,680
behalf. Uh, I'm maybe no longer getting

52
00:02:10,800 --> 00:02:15,920
ads that uh the the booker is being paid

53
00:02:13,680 --> 00:02:18,400
for. I'm not even going to any other

54
00:02:15,920 --> 00:02:20,080
website potentially. Maybe you don't

55
00:02:18,400 --> 00:02:21,920
need an aggregator at that point because

56
00:02:20,080 --> 00:02:25,120
the agent just goes out and finds me the

57
00:02:21,920 --> 00:02:27,280
best flight based on my preferences.

58
00:02:25,120 --> 00:02:30,319
And and that to me it starts to change

59
00:02:27,280 --> 00:02:32,800
the entire revenue model of the web and

60
00:02:30,319 --> 00:02:34,080
becomes something that is more of just

61
00:02:32,800 --> 00:02:35,840
doing things a little bit more

62
00:02:34,080 --> 00:02:38,080
efficiently, a little bit better to

63
00:02:35,840 --> 00:02:40,720
being things that well this this

64
00:02:38,080 --> 00:02:42,879
actually throws the checkerboard up in

65
00:02:40,720 --> 00:02:44,720
the air and and and what how does that

66
00:02:42,879 --> 00:02:46,720
land after that? So what what do you

67
00:02:44,720 --> 00:02:48,800
think about those things and and based

68
00:02:46,720 --> 00:02:52,480
on your perspectives where you see do

69
00:02:48,800 --> 00:02:55,200
you see other sort of big nobody here is

70
00:02:52,480 --> 00:02:57,360
from Sloan but you know that sort of

71
00:02:55,200 --> 00:02:59,680
business perspective of things changing

72
00:02:57,360 --> 00:03:03,120
based on an agentic web or the future

73
00:02:59,680 --> 00:03:04,319
agentic web

74
00:03:03,120 --> 00:03:07,840
you have a

75
00:03:04,319 --> 00:03:10,319
>> yeah um you know for context uh our our

76
00:03:07,840 --> 00:03:12,640
team at Leela AI we are creating

77
00:03:10,319 --> 00:03:13,680
multitudes of agents that are more

78
00:03:12,640 --> 00:03:15,519
visual

79
00:03:13,680 --> 00:03:18,800
And so they're able to see the real

80
00:03:15,519 --> 00:03:22,640
world and use real time on what they're

81
00:03:18,800 --> 00:03:24,400
seeing to uh decisions, take actions on

82
00:03:22,640 --> 00:03:29,040
behalf of our customers and stuff like

83
00:03:24,400 --> 00:03:31,200
that. And so um when we initially went

84
00:03:29,040 --> 00:03:32,720
and showed this to uh manufacturers and

85
00:03:31,200 --> 00:03:34,560
industrial customers, they said, "Oh,

86
00:03:32,720 --> 00:03:36,319
this is great. I I don't have to hire

87
00:03:34,560 --> 00:03:38,400
five of new people. I can just use your

88
00:03:36,319 --> 00:03:40,560
agents and I don't have to hire people."

89
00:03:38,400 --> 00:03:42,799
Um, and that may be partially true, but

90
00:03:40,560 --> 00:03:45,040
what we discovered after talking to them

91
00:03:42,799 --> 00:03:46,560
more deeply is that at least in the

92
00:03:45,040 --> 00:03:49,200
industrial manufacturing world, they're

93
00:03:46,560 --> 00:03:51,280
highly understaffed. So what instead of

94
00:03:49,200 --> 00:03:53,440
saying we can replace people or we can

95
00:03:51,280 --> 00:03:55,040
let some people go, it it becomes like

96
00:03:53,440 --> 00:03:56,560
we really should have had five people,

97
00:03:55,040 --> 00:03:58,799
we only have one person in that job. So

98
00:03:56,560 --> 00:04:01,760
if we can help this person become the

99
00:03:58,799 --> 00:04:04,080
equivalent of of four more people uh and

100
00:04:01,760 --> 00:04:06,720
augment them and amplify them with these

101
00:04:04,080 --> 00:04:09,120
agents that will actually make us a much

102
00:04:06,720 --> 00:04:12,080
better organization and have lots of

103
00:04:09,120 --> 00:04:14,319
other positive outcomes. So for our

104
00:04:12,080 --> 00:04:15,680
customers not not a travel agent on the

105
00:04:14,319 --> 00:04:18,720
web thing. We're looking at more like

106
00:04:15,680 --> 00:04:20,639
how do you uh make a better outcome for

107
00:04:18,720 --> 00:04:21,840
manufacturers, industrial customers.

108
00:04:20,639 --> 00:04:25,440
It's all about amplification,

109
00:04:21,840 --> 00:04:27,040
augmentation. The agents become uh sort

110
00:04:25,440 --> 00:04:28,880
of like supporters of the existing

111
00:04:27,040 --> 00:04:30,720
people who have expertise. I don't think

112
00:04:28,880 --> 00:04:32,639
the agents that we have today are have

113
00:04:30,720 --> 00:04:35,520
enough expertise to really operate

114
00:04:32,639 --> 00:04:36,960
autonomously in in fully autonomously in

115
00:04:35,520 --> 00:04:38,720
in industrial applications. It's a

116
00:04:36,960 --> 00:04:41,520
little bit scary to say, oh, this agent

117
00:04:38,720 --> 00:04:42,960
just shut down our conveyor belt. That

118
00:04:41,520 --> 00:04:44,479
might be the right thing to do, but it

119
00:04:42,960 --> 00:04:45,680
might also be have been a false alarm

120
00:04:44,479 --> 00:04:47,199
and then we're losing a million dollars

121
00:04:45,680 --> 00:04:48,560
an hour because that conveyor belt is

122
00:04:47,199 --> 00:04:50,800
shut down. So that we I don't think

123
00:04:48,560 --> 00:04:52,479
we're ready yet to let them loose in the

124
00:04:50,800 --> 00:04:54,880
manufacturing world where we work, but

125
00:04:52,479 --> 00:04:56,800
they can amplify people and that has

126
00:04:54,880 --> 00:04:58,800
enormous benefits that we've seen with

127
00:04:56,800 --> 00:05:02,080
our customers.

128
00:04:58,800 --> 00:05:04,800
>> So yes, so I I want to add to that that

129
00:05:02,080 --> 00:05:07,199
there are two I mean the two very

130
00:05:04,800 --> 00:05:09,600
separate applications and I think very

131
00:05:07,199 --> 00:05:11,280
separate business model. One is consumer

132
00:05:09,600 --> 00:05:13,759
which is what you were talking about and

133
00:05:11,280 --> 00:05:16,000
one is business. We IBM model similar to

134
00:05:13,759 --> 00:05:17,680
you. We just work with business luckily.

135
00:05:16,000 --> 00:05:19,520
Uh because I think there the business

136
00:05:17,680 --> 00:05:22,080
model is pretty clear. You go to

137
00:05:19,520 --> 00:05:23,520
companies you you know you improve their

138
00:05:22,080 --> 00:05:25,120
you know the operations you know they

139
00:05:23,520 --> 00:05:27,600
save money they pay you right. It's it's

140
00:05:25,120 --> 00:05:28,880
very simple for consumer. Luckily I

141
00:05:27,600 --> 00:05:30,479
don't work for Google so I don't need to

142
00:05:28,880 --> 00:05:31,919
think about it. But but yeah like you

143
00:05:30,479 --> 00:05:34,960
said this is going to be disruptive for

144
00:05:31,919 --> 00:05:37,600
for for that you know part of the the

145
00:05:34,960 --> 00:05:38,320
business. And I I'm not sure that I have

146
00:05:37,600 --> 00:05:38,720
good answers.

147
00:05:38,320 --> 00:05:41,199
>> Okay.

148
00:05:38,720 --> 00:05:42,960
>> Prauma, you you work with a lot of

149
00:05:41,199 --> 00:05:45,360
different companies that are

150
00:05:42,960 --> 00:05:48,080
collaborating with you on project NANDA.

151
00:05:45,360 --> 00:05:50,720
Uh what how are they looking at this?

152
00:05:48,080 --> 00:05:53,919
>> Yeah. Uh I mean great points here and

153
00:05:50,720 --> 00:05:57,120
agree with uh all of that. Uh from like

154
00:05:53,919 --> 00:05:59,280
a more academic perspective, one of the

155
00:05:57,120 --> 00:06:01,680
uh maybe let's say a paradigm shift that

156
00:05:59,280 --> 00:06:05,280
will happen with agents, distributed

157
00:06:01,680 --> 00:06:08,160
agents is as follows. So as I'm sure all

158
00:06:05,280 --> 00:06:11,199
of us agree today data is key right

159
00:06:08,160 --> 00:06:12,800
training any AI model any large model is

160
00:06:11,199 --> 00:06:15,520
really dependent on how much data you

161
00:06:12,800 --> 00:06:19,280
have access to but also this is a

162
00:06:15,520 --> 00:06:21,600
blocker because uh uh most of these big

163
00:06:19,280 --> 00:06:25,280
models today are trained on largely

164
00:06:21,600 --> 00:06:27,440
public data. Uh uh now the paradox here

165
00:06:25,280 --> 00:06:29,840
is uh not all data not all data of

166
00:06:27,440 --> 00:06:32,000
humanity is public and in fact the more

167
00:06:29,840 --> 00:06:34,319
valuable the data the less likely it is

168
00:06:32,000 --> 00:06:35,680
to be public. It'll be siloed behind

169
00:06:34,319 --> 00:06:38,160
enterprise walls. It'll be in a

170
00:06:35,680 --> 00:06:41,440
hospital. So there's a lot of critical

171
00:06:38,160 --> 00:06:44,479
knowledge, let's say, of our collective

172
00:06:41,440 --> 00:06:47,440
society that is still not reflected in

173
00:06:44,479 --> 00:06:49,680
these amazingly capable models. So one

174
00:06:47,440 --> 00:06:54,160
of the most exciting things that uh sort

175
00:06:49,680 --> 00:06:57,759
of uh we think about a lot is how uh

176
00:06:54,160 --> 00:07:00,319
agentic AI or networkked agentic AI will

177
00:06:57,759 --> 00:07:03,440
shift the transaction

178
00:07:00,319 --> 00:07:06,080
uh unit from data to intelligence. So

179
00:07:03,440 --> 00:07:08,479
here's what I mean by that, right? Uh

180
00:07:06,080 --> 00:07:10,479
you can have these data sources that are

181
00:07:08,479 --> 00:07:12,400
siloed that are that are private. You

182
00:07:10,479 --> 00:07:14,319
can have an agent on top of it. And this

183
00:07:12,400 --> 00:07:17,840
agent need not communicate with other

184
00:07:14,319 --> 00:07:19,919
agents the data entities, the raw data

185
00:07:17,840 --> 00:07:21,759
itself which may be protected for

186
00:07:19,919 --> 00:07:23,599
privacy and many other reasons, but it

187
00:07:21,759 --> 00:07:26,479
can communicate in insights from the

188
00:07:23,599 --> 00:07:28,800
data, broader statistics. And so you can

189
00:07:26,479 --> 00:07:30,880
leverage a lot of this siloed data

190
00:07:28,800 --> 00:07:33,919
without actually having to transact this

191
00:07:30,880 --> 00:07:35,759
data. And this may be like a new avenue

192
00:07:33,919 --> 00:07:38,479
to think about, you know, revenue

193
00:07:35,759 --> 00:07:41,199
strategies or new ways to optimize

194
00:07:38,479 --> 00:07:42,800
across organizational silos that does

195
00:07:41,199 --> 00:07:43,680
not exist today.

196
00:07:42,800 --> 00:07:45,599
>> If that makes sense.

197
00:07:43,680 --> 00:07:48,240
>> Interesting.

198
00:07:45,599 --> 00:07:51,520
I I I think shifting gears a little bit

199
00:07:48,240 --> 00:07:52,720
and and what you touched on uh Cyrus, uh

200
00:07:51,520 --> 00:07:54,879
let's talk a little bit about

201
00:07:52,720 --> 00:07:56,400
accountability and risk. So you're

202
00:07:54,879 --> 00:07:58,560
you're saying that in an industrial

203
00:07:56,400 --> 00:08:01,280
setting and I I

204
00:07:58,560 --> 00:08:04,240
monetary loss by shutting down a a a

205
00:08:01,280 --> 00:08:06,879
factory could be pretty significant, but

206
00:08:04,240 --> 00:08:08,400
also somebody gets hurt is another one.

207
00:08:06,879 --> 00:08:10,080
A lot of the applications you're dealing

208
00:08:08,400 --> 00:08:10,560
with I think are safety related.

209
00:08:10,080 --> 00:08:14,160
>> Yeah.

210
00:08:10,560 --> 00:08:17,599
>> So what what happens when something

211
00:08:14,160 --> 00:08:19,360
unfortunate happens there is uh when

212
00:08:17,599 --> 00:08:21,919
somebody says it wasn't my decision, it

213
00:08:19,360 --> 00:08:24,879
was it was the AI agent that did it. How

214
00:08:21,919 --> 00:08:28,400
are we handling accountability risk? Is

215
00:08:24,879 --> 00:08:30,240
is is there a a plan for that when when

216
00:08:28,400 --> 00:08:32,640
we do get to it?

217
00:08:30,240 --> 00:08:35,519
>> Uh that's a really great question and I

218
00:08:32,640 --> 00:08:38,399
would say that there's a lot of

219
00:08:35,519 --> 00:08:41,599
variability out there right now and how

220
00:08:38,399 --> 00:08:43,279
prepared a lot of organizations are to

221
00:08:41,599 --> 00:08:44,959
deal with the point when there is a

222
00:08:43,279 --> 00:08:46,800
decision that is an autonomous decision

223
00:08:44,959 --> 00:08:49,360
that was taken with either minimal or

224
00:08:46,800 --> 00:08:50,560
not no human oversight. No humans in the

225
00:08:49,360 --> 00:08:54,000
loop, right? That's where we're heading

226
00:08:50,560 --> 00:08:56,320
towards in many uh environments. And if

227
00:08:54,000 --> 00:08:57,760
those errors, if those decisions are are

228
00:08:56,320 --> 00:09:00,959
not correct, if they're mistaken

229
00:08:57,760 --> 00:09:02,480
decisions, who's to blame? Um old, you

230
00:09:00,959 --> 00:09:04,800
know, I forget who said this, but you

231
00:09:02,480 --> 00:09:06,080
know, you you can you can't blame a

232
00:09:04,800 --> 00:09:07,279
computer program. You can only blame the

233
00:09:06,080 --> 00:09:08,320
person who wrote the computer program.

234
00:09:07,279 --> 00:09:10,160
>> It's Winston Churchill.

235
00:09:08,320 --> 00:09:11,680
>> It was Winston Churchill. Yeah, that's

236
00:09:10,160 --> 00:09:15,680
coming up a lot lately, and I forgot who

237
00:09:11,680 --> 00:09:17,279
said it. Uh but in any case, it's a

238
00:09:15,680 --> 00:09:19,279
little hard now because as Dan said,

239
00:09:17,279 --> 00:09:21,040
it's stocastic, right? So, if you roll

240
00:09:19,279 --> 00:09:23,839
the dice and the LM spits out something

241
00:09:21,040 --> 00:09:26,080
that's correct, great. Everyone gets a

242
00:09:23,839 --> 00:09:28,160
happy credit to the to the people who

243
00:09:26,080 --> 00:09:30,080
are operating this LLM. But if the LM

244
00:09:28,160 --> 00:09:31,760
comes back with a decision that's wrong,

245
00:09:30,080 --> 00:09:33,200
then oh, it's the people who wrote the

246
00:09:31,760 --> 00:09:35,200
software that runs. That doesn't make

247
00:09:33,200 --> 00:09:36,399
much sense. So, I think we need to all

248
00:09:35,200 --> 00:09:38,240
think deeply. I don't have an answer

249
00:09:36,399 --> 00:09:40,800
yet, but we don't have to think deeper

250
00:09:38,240 --> 00:09:42,240
about the ethics and morals of using

251
00:09:40,800 --> 00:09:46,000
these kinds of very very powerful

252
00:09:42,240 --> 00:09:47,680
systems to uh make unchecked decisions

253
00:09:46,000 --> 00:09:49,519
that are mission critical decisions.

254
00:09:47,680 --> 00:09:51,279
Now, a decision to let's say book this

255
00:09:49,519 --> 00:09:52,640
flight or that flight, sure, I I spent

256
00:09:51,279 --> 00:09:54,480
200 bucks more than I should have.

257
00:09:52,640 --> 00:09:57,120
That's in the end not mission critical.

258
00:09:54,480 --> 00:09:59,360
As you pointed out, if uh there's a

259
00:09:57,120 --> 00:10:02,160
situation where people get injured, um

260
00:09:59,360 --> 00:10:03,760
that's sort of another level, maybe

261
00:10:02,160 --> 00:10:08,080
multiple levels higher mission

262
00:10:03,760 --> 00:10:10,800
criticality. And so looking at the basis

263
00:10:08,080 --> 00:10:12,399
of a system and asking is this decision

264
00:10:10,800 --> 00:10:14,000
correct or not? And can it be explained?

265
00:10:12,399 --> 00:10:16,160
And that's where I think we have to

266
00:10:14,000 --> 00:10:18,079
really dig deep. Can the systems we're

267
00:10:16,160 --> 00:10:20,480
using explain why they made that

268
00:10:18,079 --> 00:10:21,920
decision? And if they can explain, can

269
00:10:20,480 --> 00:10:23,760
we verify that the reason that the

270
00:10:21,920 --> 00:10:25,519
system is given for making decision is

271
00:10:23,760 --> 00:10:27,760
the actual way that it came to that

272
00:10:25,519 --> 00:10:28,959
decision? Because we see a lot with LLMs

273
00:10:27,760 --> 00:10:30,720
is they're sick of fans. They're great

274
00:10:28,959 --> 00:10:32,399
at saying, "You're so smart. Here's the

275
00:10:30,720 --> 00:10:33,680
answer to your question. It was a great

276
00:10:32,399 --> 00:10:34,959
question. Here's the answer. It's a

277
00:10:33,680 --> 00:10:36,800
correct answer." Right? And then you

278
00:10:34,959 --> 00:10:38,160
take that because it's such a positive

279
00:10:36,800 --> 00:10:39,600
tone of voice in the way that it's

280
00:10:38,160 --> 00:10:41,279
talking to you. You believe that it's

281
00:10:39,600 --> 00:10:42,480
the right answer. And then you can even

282
00:10:41,279 --> 00:10:43,360
ask, "How did you do that? How did you

283
00:10:42,480 --> 00:10:44,560
answer? How did you figure that out?"

284
00:10:43,360 --> 00:10:46,399
says, "Oh, I did this and this and

285
00:10:44,560 --> 00:10:47,760
this." And then if you to bother to look

286
00:10:46,399 --> 00:10:49,360
at the reasons that it gave for its

287
00:10:47,760 --> 00:10:50,480
decision, you dig deep and say, "Are

288
00:10:49,360 --> 00:10:51,680
these a real thing? Does this make any

289
00:10:50,480 --> 00:10:53,200
sense? Is the evidence the real

290
00:10:51,680 --> 00:10:54,880
evidence? Oh, it made up a website. Oh,

291
00:10:53,200 --> 00:10:56,800
it made up an academic paper. Oh, it

292
00:10:54,880 --> 00:10:58,800
made up a number that wasn't even in any

293
00:10:56,800 --> 00:11:00,720
web page." Right? Then you realize, wow,

294
00:10:58,800 --> 00:11:01,920
this this reason is a is is not I don't

295
00:11:00,720 --> 00:11:03,920
call them hallucinations. I'll call them

296
00:11:01,920 --> 00:11:06,800
confabulations. These reasons are not

297
00:11:03,920 --> 00:11:08,959
right. So unless you can really trust

298
00:11:06,800 --> 00:11:10,240
not only the answer but the the the

299
00:11:08,959 --> 00:11:12,000
explanation of the way that it got to

300
00:11:10,240 --> 00:11:13,680
the answer and maybe you can introspect

301
00:11:12,000 --> 00:11:16,800
and open up the black box and look at

302
00:11:13,680 --> 00:11:18,320
the what's going inside the LLM to

303
00:11:16,800 --> 00:11:19,600
understand what's really happening not

304
00:11:18,320 --> 00:11:21,360
what it says is happening what's really

305
00:11:19,600 --> 00:11:23,040
happening inside unless you can do that

306
00:11:21,360 --> 00:11:24,399
I don't think we should let them have

307
00:11:23,040 --> 00:11:26,160
missionritical decision-making power

308
00:11:24,399 --> 00:11:28,240
without human intervention so I have a

309
00:11:26,160 --> 00:11:30,560
sort of maybe a controversial stance

310
00:11:28,240 --> 00:11:32,959
here but I think we need to find better

311
00:11:30,560 --> 00:11:34,480
options than large language models to

312
00:11:32,959 --> 00:11:36,640
help us I think we're we're sort of hit

313
00:11:34,480 --> 00:11:38,399
a wall with LLMs and we need other ways

314
00:11:36,640 --> 00:11:41,600
of making decisions. Some we're working

315
00:11:38,399 --> 00:11:43,519
on at Leela AI, but that are not uh

316
00:11:41,600 --> 00:11:44,000
monolithic the way that the LLMs are

317
00:11:43,519 --> 00:11:46,480
today.

318
00:11:44,000 --> 00:11:48,079
>> So ways

319
00:11:46,480 --> 00:11:51,200
that do the same thing, but are more

320
00:11:48,079 --> 00:11:54,320
transparent, more explainable,

321
00:11:51,200 --> 00:11:56,480
>> do the same thing differently and and

322
00:11:54,320 --> 00:11:58,399
have to be explainable, transparent and

323
00:11:56,480 --> 00:11:59,760
and and actually verifiable that the the

324
00:11:58,399 --> 00:12:01,680
explanations are true. They have you

325
00:11:59,760 --> 00:12:04,079
have to be able to verify those quickly.

326
00:12:01,680 --> 00:12:04,880
May maybe a whole new startup idea in

327
00:12:04,079 --> 00:12:06,959
there somewhere.

328
00:12:04,880 --> 00:12:09,360
>> Yeah.

329
00:12:06,959 --> 00:12:10,959
>> Um good. Any any anything to add?

330
00:12:09,360 --> 00:12:13,519
>> Yeah. Yeah. A couple of things to add to

331
00:12:10,959 --> 00:12:15,279
that. So So first of all, um I think

332
00:12:13,519 --> 00:12:16,720
there's two aspects. One one is the tech

333
00:12:15,279 --> 00:12:18,639
what what can we do more from the

334
00:12:16,720 --> 00:12:21,519
technical point of view? Explanability

335
00:12:18,639 --> 00:12:22,959
is of course really important. Um I I

336
00:12:21,519 --> 00:12:24,639
think there is a limit to how much we'll

337
00:12:22,959 --> 00:12:26,079
be able to explain inside the box. I

338
00:12:24,639 --> 00:12:28,160
think we can do more than we do now, but

339
00:12:26,079 --> 00:12:31,440
there's a limit. Um they've got billions

340
00:12:28,160 --> 00:12:33,040
of parameters. Um

341
00:12:31,440 --> 00:12:35,040
the other thing is I mentioned it in my

342
00:12:33,040 --> 00:12:37,600
talk uncertainty quantification. It's

343
00:12:35,040 --> 00:12:40,560
really important if we can get the LM to

344
00:12:37,600 --> 00:12:42,880
report reliably what it what's its own

345
00:12:40,560 --> 00:12:44,720
certainty about its answer. Now you

346
00:12:42,880 --> 00:12:47,200
would say why would if I don't trans the

347
00:12:44,720 --> 00:12:49,440
answer why would I trust

348
00:12:47,200 --> 00:12:50,639
the model reporting. So there's actually

349
00:12:49,440 --> 00:12:53,279
ways to do it and we have work that came

350
00:12:50,639 --> 00:12:54,720
from the MIT IBM lab on first you need

351
00:12:53,279 --> 00:12:55,760
to calibrate the model. So calibrate

352
00:12:54,720 --> 00:12:58,000
means that when the model says

353
00:12:55,760 --> 00:12:59,680
uncertainty, it's it's really true in

354
00:12:58,000 --> 00:13:01,360
the sense that if I have a model that

355
00:12:59,680 --> 00:13:03,360
predicts what's what if it's going to be

356
00:13:01,360 --> 00:13:06,000
raining or not today. If the model says

357
00:13:03,360 --> 00:13:08,399
that it's 70% certain, it means it will

358
00:13:06,000 --> 00:13:10,160
be correct 70% of the time and wrong 30%

359
00:13:08,399 --> 00:13:12,639
of the time. So it'll re so it'll say

360
00:13:10,160 --> 00:13:15,279
how much is going to be wrong. Um that

361
00:13:12,639 --> 00:13:17,839
will I think that will add a lot of uh

362
00:13:15,279 --> 00:13:19,279
trustworthiness and also we could do

363
00:13:17,839 --> 00:13:21,600
something about it. If the model is not

364
00:13:19,279 --> 00:13:23,760
certain enough then okay we put that

365
00:13:21,600 --> 00:13:26,000
aside. That's one one aspect. The other

366
00:13:23,760 --> 00:13:28,560
aspect is more from I guess legal than

367
00:13:26,000 --> 00:13:29,600
the law makers and standardization. I

368
00:13:28,560 --> 00:13:33,279
think that's really important. There is

369
00:13:29,600 --> 00:13:36,160
actually a standard. It's ISO 42,0001.

370
00:13:33,279 --> 00:13:39,120
Um it was created I think two years ago.

371
00:13:36,160 --> 00:13:42,160
Um and it's for AI management systems.

372
00:13:39,120 --> 00:13:43,839
Um I know that anthropic was first to

373
00:13:42,160 --> 00:13:45,760
get it in Microsoft and just recently

374
00:13:43,839 --> 00:13:47,600
from the open community IBM we got it

375
00:13:45,760 --> 00:13:49,120
just just recently. So so yeah we need

376
00:13:47,600 --> 00:13:50,800
we need standards and we need lawmakers

377
00:13:49,120 --> 00:13:55,680
to say well what can we do and what can

378
00:13:50,800 --> 00:13:57,920
we not do. Interesting. I I guess um

379
00:13:55,680 --> 00:14:00,560
picking on you again a little bit, I was

380
00:13:57,920 --> 00:14:04,320
intrigued uh with your generative

381
00:14:00,560 --> 00:14:07,519
programming model and um could you

382
00:14:04,320 --> 00:14:08,959
elaborate a little bit more on uh

383
00:14:07,519 --> 00:14:11,440
basically talk about the problem of

384
00:14:08,959 --> 00:14:13,839
maybe maybe call it prompt spaghetti and

385
00:14:11,440 --> 00:14:15,519
then how that addresses it.

386
00:14:13,839 --> 00:14:17,199
>> Yeah. So so I think I I I mentioned in

387
00:14:15,519 --> 00:14:18,880
the talk and we we can you know can have

388
00:14:17,199 --> 00:14:21,120
a discussion on that. So, so, so the

389
00:14:18,880 --> 00:14:23,279
yeah the prompt spaghetti comes from uh

390
00:14:21,120 --> 00:14:25,839
from that trial and error process uh of

391
00:14:23,279 --> 00:14:27,680
creating uh these agents. uh if it's a

392
00:14:25,839 --> 00:14:30,320
simple agent then then it's easy it

393
00:14:27,680 --> 00:14:32,639
doesn't become spaghetti but but once uh

394
00:14:30,320 --> 00:14:34,720
you try and encode a very complex uh

395
00:14:32,639 --> 00:14:36,399
workflow and there are many constraints

396
00:14:34,720 --> 00:14:37,600
and what you see because of that trial

397
00:14:36,399 --> 00:14:39,040
and error or maybe you're working with

398
00:14:37,600 --> 00:14:41,040
another LLM to help you write that

399
00:14:39,040 --> 00:14:43,920
prompt it becomes very long and it's

400
00:14:41,040 --> 00:14:45,519
also there's no standard way of the

401
00:14:43,920 --> 00:14:46,800
order of things and how so so you look

402
00:14:45,519 --> 00:14:48,079
at these prompts and these are like I'm

403
00:14:46,800 --> 00:14:49,440
looking at prompts that people even from

404
00:14:48,079 --> 00:14:50,560
IBM consulting so we're getting some of

405
00:14:49,440 --> 00:14:52,639
these prompts and we're seeing what they

406
00:14:50,560 --> 00:14:54,240
what they're doing and so so you see

407
00:14:52,639 --> 00:14:55,519
you'll have like an instruction and then

408
00:14:54,240 --> 00:14:57,519
you have a piece of data and then all of

409
00:14:55,519 --> 00:14:59,440
a sudden you have kind of an incont

410
00:14:57,519 --> 00:15:00,560
incontext example and then another

411
00:14:59,440 --> 00:15:02,160
instruction but then there's another

412
00:15:00,560 --> 00:15:05,600
incontext example for the previous

413
00:15:02,160 --> 00:15:08,720
instruction there's no order and no uh

414
00:15:05,600 --> 00:15:10,560
um so really that's that's where uh the

415
00:15:08,720 --> 00:15:12,560
prawn spaghetti is coming from and and I

416
00:15:10,560 --> 00:15:14,560
think the answer is like I that was my

417
00:15:12,560 --> 00:15:16,160
mantra in the talk is let's think about

418
00:15:14,560 --> 00:15:17,760
it like computer scientists or like

419
00:15:16,160 --> 00:15:18,959
software engineers and and say okay what

420
00:15:17,760 --> 00:15:21,440
are we trying to achieve what is the

421
00:15:18,959 --> 00:15:23,120
control flow and then let's follow that

422
00:15:21,440 --> 00:15:25,519
control flow mainly in code and wherever

423
00:15:23,120 --> 00:15:27,839
we need to call an LN

424
00:15:25,519 --> 00:15:29,360
So is the hopeful sign for people with a

425
00:15:27,839 --> 00:15:31,360
background in software engineering that

426
00:15:29,360 --> 00:15:31,920
all that experience is not going to go

427
00:15:31,360 --> 00:15:33,519
to waste?

428
00:15:31,920 --> 00:15:34,880
>> Yeah. So so I have my own incentive

429
00:15:33,519 --> 00:15:36,160
because my son is now in college

430
00:15:34,880 --> 00:15:37,600
studying computer science and I want him

431
00:15:36,160 --> 00:15:39,199
to be employed in a couple of years. Um

432
00:15:37,600 --> 00:15:39,920
but but yeah, I think there's hope for

433
00:15:39,199 --> 00:15:41,120
programmers.

434
00:15:39,920 --> 00:15:43,440
>> Yeah, that that's always nice when

435
00:15:41,120 --> 00:15:47,600
you're paying for college.

436
00:15:43,440 --> 00:15:50,480
Uh so uh another aspect that I'd like to

437
00:15:47,600 --> 00:15:52,320
touch on and and I think we we covered

438
00:15:50,480 --> 00:15:56,079
it a little bit but what about

439
00:15:52,320 --> 00:15:58,320
jurisdictions and and what about policy

440
00:15:56,079 --> 00:16:00,560
if you're talking about decentralized

441
00:15:58,320 --> 00:16:04,320
system famous systems famously don't

442
00:16:00,560 --> 00:16:06,800
really respect international boundaries

443
00:16:04,320 --> 00:16:10,639
ordinarily it's that they they sort of

444
00:16:06,800 --> 00:16:13,839
go global. So how do you see that moving

445
00:16:10,639 --> 00:16:16,240
forward in terms of maybe US regulations

446
00:16:13,839 --> 00:16:18,320
being consistent with EU regulations in

447
00:16:16,240 --> 00:16:23,040
other parts of the world?

448
00:16:18,320 --> 00:16:24,639
>> Yeah, I think uh certainly uh there will

449
00:16:23,040 --> 00:16:26,959
be a future where we have to start

450
00:16:24,639 --> 00:16:29,199
thinking about uh some very specific

451
00:16:26,959 --> 00:16:31,199
policies pertaining to agentic

452
00:16:29,199 --> 00:16:33,519
interaction and orchestration as I

453
00:16:31,199 --> 00:16:35,839
touched upon earlier. I think right now

454
00:16:33,519 --> 00:16:38,000
of course these don't exist broadly

455
00:16:35,839 --> 00:16:39,519
because today we are currently solving

456
00:16:38,000 --> 00:16:41,680
smaller problems but still important

457
00:16:39,519 --> 00:16:44,079
problems uh which are still policy

458
00:16:41,680 --> 00:16:46,160
problems. How do organizations allow

459
00:16:44,079 --> 00:16:49,199
agents to talk within an enterprise or

460
00:16:46,160 --> 00:16:51,440
between enterprises? And so uh a lot of

461
00:16:49,199 --> 00:16:54,160
the critical aspects to think about is

462
00:16:51,440 --> 00:16:55,680
of course guardrailing ensuring that uh

463
00:16:54,160 --> 00:16:58,399
safety critical information is not

464
00:16:55,680 --> 00:17:00,320
regurgitated. I think comes back to uh

465
00:16:58,399 --> 00:17:01,920
also what Cyrus and Dan were mentioning

466
00:17:00,320 --> 00:17:04,160
which is explanability and

467
00:17:01,920 --> 00:17:08,000
interpretability having that kind of

468
00:17:04,160 --> 00:17:10,559
trust in agents and uh I think this may

469
00:17:08,000 --> 00:17:14,160
also be part of policy for example maybe

470
00:17:10,559 --> 00:17:17,120
uh uh there for certain mission critical

471
00:17:14,160 --> 00:17:19,919
as Cyrus mentioned uh aspects there may

472
00:17:17,120 --> 00:17:23,120
be a necessity to have uh uncertainty

473
00:17:19,919 --> 00:17:25,679
quantification or trust or verifiability

474
00:17:23,120 --> 00:17:27,760
before these sort of autonomous blackbox

475
00:17:25,679 --> 00:17:29,520
entities are deployed in those settings.

476
00:17:27,760 --> 00:17:33,039
There may also be data sovereignty

477
00:17:29,520 --> 00:17:37,679
aspects uh famously in EU right uh for

478
00:17:33,039 --> 00:17:40,080
example uh questions such as uh where

479
00:17:37,679 --> 00:17:43,120
does an agent or an LLM actually

480
00:17:40,080 --> 00:17:44,559
actually reside because uh an LLM is a

481
00:17:43,120 --> 00:17:46,559
reflection of the data that is trained

482
00:17:44,559 --> 00:17:49,200
on. So where does this model actually

483
00:17:46,559 --> 00:17:52,000
reside if it's being served across the

484
00:17:49,200 --> 00:17:54,080
world? Uh what does resolution what does

485
00:17:52,000 --> 00:17:56,160
serving look like for these agents

486
00:17:54,080 --> 00:17:58,480
across the web? uh I think these will

487
00:17:56,160 --> 00:18:00,160
all be uh questions that need to be

488
00:17:58,480 --> 00:18:03,679
carefully thought about and addressed as

489
00:18:00,160 --> 00:18:07,039
part of policy but uh uh maybe we are

490
00:18:03,679 --> 00:18:09,200
building towards it right now and uh uh

491
00:18:07,039 --> 00:18:10,400
I think we first need to collectively

492
00:18:09,200 --> 00:18:12,640
agree upon what are the important

493
00:18:10,400 --> 00:18:17,000
questions to ask and then policy will

494
00:18:12,640 --> 00:18:17,000
determine how they need to be answered

495
00:18:17,919 --> 00:18:24,080
>> on the issue of privacy and and I know

496
00:18:21,280 --> 00:18:26,640
Nandanda deals a lot of trying to

497
00:18:24,080 --> 00:18:29,120
maintain privacy which is a is fighting

498
00:18:26,640 --> 00:18:30,559
the good fight. Uh one of the things

499
00:18:29,120 --> 00:18:32,960
that makes me a little bit uncomfortable

500
00:18:30,559 --> 00:18:36,559
is that when you start talking about

501
00:18:32,960 --> 00:18:38,880
privacy and private enterprise that

502
00:18:36,559 --> 00:18:42,320
there always seems to be a financial

503
00:18:38,880 --> 00:18:45,440
imperative to not respect that privacy

504
00:18:42,320 --> 00:18:48,160
uh to exploit the data that you have. Uh

505
00:18:45,440 --> 00:18:50,320
how do you see that working out uh in

506
00:18:48,160 --> 00:18:52,880
terms of incentives?

507
00:18:50,320 --> 00:18:54,320
Does you or any of our other other

508
00:18:52,880 --> 00:18:58,000
panelists have something to say about

509
00:18:54,320 --> 00:18:59,840
that? Is is there a way to address it?

510
00:18:58,000 --> 00:19:01,840
>> I I can get started and then hand over

511
00:18:59,840 --> 00:19:06,400
to the others as well. But yeah, I think

512
00:19:01,840 --> 00:19:09,280
uh again uh to me uh this again goes

513
00:19:06,400 --> 00:19:11,200
back to the argument whether to sort of

514
00:19:09,280 --> 00:19:13,120
reap the benefits of these large AI

515
00:19:11,200 --> 00:19:15,440
models. Do we need all the data to be

516
00:19:13,120 --> 00:19:16,240
centralized or is it okay if the data is

517
00:19:15,440 --> 00:19:19,200
distributed?

518
00:19:16,240 --> 00:19:21,840
>> Right? And with sort of this idea of an

519
00:19:19,200 --> 00:19:24,240
agent web and agents transacting as I

520
00:19:21,840 --> 00:19:26,400
touched upon earlier in intelligence and

521
00:19:24,240 --> 00:19:28,160
knowledge rather than in data, this is a

522
00:19:26,400 --> 00:19:30,559
positive sign towards privacy because I

523
00:19:28,160 --> 00:19:34,000
can hold sovereignty over my data but

524
00:19:30,559 --> 00:19:36,160
still uh maybe for example ask questions

525
00:19:34,000 --> 00:19:38,720
about something to do with my health

526
00:19:36,160 --> 00:19:42,400
without exposing all my health records.

527
00:19:38,720 --> 00:19:44,160
uh uh aspects like these uh could have a

528
00:19:42,400 --> 00:19:46,320
like sort of this idea of an agentic web

529
00:19:44,160 --> 00:19:49,039
could have positive impact on personal

530
00:19:46,320 --> 00:19:51,520
privacy. That being said that there are

531
00:19:49,039 --> 00:19:54,960
still a lot of questions to be answered

532
00:19:51,520 --> 00:19:56,960
uh with respect to uh controllability of

533
00:19:54,960 --> 00:19:59,520
agents like if I have my agent

534
00:19:56,960 --> 00:20:01,919
autonomously interacting with some other

535
00:19:59,520 --> 00:20:04,160
agents maybe this agent can be prompt

536
00:20:01,919 --> 00:20:06,559
injected to reveal a lot of my private

537
00:20:04,160 --> 00:20:09,919
information. So how do I prevent that

538
00:20:06,559 --> 00:20:11,440
from happening? uh how do I safely

539
00:20:09,919 --> 00:20:14,240
orchestrate these agents how do I

540
00:20:11,440 --> 00:20:16,080
discover trustworthy agents so coming

541
00:20:14,240 --> 00:20:17,679
back to these I think a lot of these

542
00:20:16,080 --> 00:20:21,039
problems that we've talked about trust

543
00:20:17,679 --> 00:20:23,440
reputation privacy uh guardrails a lot

544
00:20:21,039 --> 00:20:26,880
of these are very closely intertwined uh

545
00:20:23,440 --> 00:20:28,240
but to I feel like uh future is uh

546
00:20:26,880 --> 00:20:29,679
promising

547
00:20:28,240 --> 00:20:33,520
>> otherwise you wouldn't be working on

548
00:20:29,679 --> 00:20:36,159
project nanda uh well that is um

549
00:20:33,520 --> 00:20:37,760
>> in in preparing for this panel we were

550
00:20:36,159 --> 00:20:39,280
talking beforehand that I spent spent a

551
00:20:37,760 --> 00:20:42,000
bit of time creating two different

552
00:20:39,280 --> 00:20:44,320
personas on chat GPT and I told them

553
00:20:42,000 --> 00:20:47,600
both I want you to be highly competent

554
00:20:44,320 --> 00:20:50,240
computer scientists. One is a big

555
00:20:47,600 --> 00:20:53,039
proponent of decentralized AI project

556
00:20:50,240 --> 00:20:54,799
NANDA and another one is skeptical and

557
00:20:53,039 --> 00:20:58,400
it the interesting thing about it is

558
00:20:54,799 --> 00:21:00,960
that both of those personas gave pretty

559
00:20:58,400 --> 00:21:02,640
compelling arguments in terms of what

560
00:21:00,960 --> 00:21:05,120
are the problems and we talked about

561
00:21:02,640 --> 00:21:08,000
that in terms of interoperability. How

562
00:21:05,120 --> 00:21:09,919
do I get heterogeneous agents to talk to

563
00:21:08,000 --> 00:21:11,919
each other? I can't even get a program

564
00:21:09,919 --> 00:21:14,240
my own company wrote to talk to talk to

565
00:21:11,919 --> 00:21:17,280
each other. Uh the the power

566
00:21:14,240 --> 00:21:20,159
requirements, resource requirements, and

567
00:21:17,280 --> 00:21:21,840
of course as you can come up know very

568
00:21:20,159 --> 00:21:22,559
well that there are answers to all of

569
00:21:21,840 --> 00:21:26,799
those things.

570
00:21:22,559 --> 00:21:28,720
>> Yes. I is is the is the knowing that

571
00:21:26,799 --> 00:21:31,039
nobody has a crystal ball, do you see

572
00:21:28,720 --> 00:21:35,120
the future as being something a little

573
00:21:31,039 --> 00:21:38,000
bit messy or are you obviously you're

574
00:21:35,120 --> 00:21:40,320
optimistic in general, but h how do you

575
00:21:38,000 --> 00:21:42,480
see things playing out? And then if I

576
00:21:40,320 --> 00:21:44,880
could put everybody on the spot, do you

577
00:21:42,480 --> 00:21:47,280
what do you see? Is it when do we have

578
00:21:44,880 --> 00:21:50,320
an agentic web? Is it five years, 10

579
00:21:47,280 --> 00:21:54,320
years, or is it is it always going to be

580
00:21:50,320 --> 00:21:54,320
something something hybrid?

581
00:21:55,039 --> 00:21:56,080
She has start.

582
00:21:55,600 --> 00:22:00,799
>> Go ahead.

583
00:21:56,080 --> 00:22:03,200
>> Okay, perfect. Uh I think

584
00:22:00,799 --> 00:22:05,919
uh Okay, so how do I see the near-term

585
00:22:03,200 --> 00:22:08,320
future? I think it'll be uh nebulous. Uh

586
00:22:05,919 --> 00:22:10,240
but not in a bad way. I think it's

587
00:22:08,320 --> 00:22:12,159
because all of us are collectively sort

588
00:22:10,240 --> 00:22:14,320
of uh reconciling with this new

589
00:22:12,159 --> 00:22:17,360
technology, figuring out how it works,

590
00:22:14,320 --> 00:22:19,679
what are its limitations, uh figuring

591
00:22:17,360 --> 00:22:22,720
out policy protocols, interoperability.

592
00:22:19,679 --> 00:22:26,320
So I think it'll be a bit nebulous as we

593
00:22:22,720 --> 00:22:29,760
collectively arrive at a steady state

594
00:22:26,320 --> 00:22:33,360
some uh protocols that we all agree upon

595
00:22:29,760 --> 00:22:36,320
architectures that make sense uh broadly

596
00:22:33,360 --> 00:22:39,360
uh but I don't think that's like uh at

597
00:22:36,320 --> 00:22:41,120
least agents broadly existing across

598
00:22:39,360 --> 00:22:43,760
enterprise and for consumer use cases

599
00:22:41,120 --> 00:22:45,840
and uh interacting across organizational

600
00:22:43,760 --> 00:22:48,240
cyers I don't think that's too far out

601
00:22:45,840 --> 00:22:50,000
maybe a few years out uh in the future

602
00:22:48,240 --> 00:22:52,720
just looking at how fast this technology

603
00:22:50,000 --> 00:22:54,960
has developed right again chat GPT came

604
00:22:52,720 --> 00:22:57,440
out in November 2022

605
00:22:54,960 --> 00:22:59,039
uh that's like 3 years ago I remember I

606
00:22:57,440 --> 00:23:01,919
was a PhD student at that point of time

607
00:22:59,039 --> 00:23:04,880
and a lot of my peers were sort of

608
00:23:01,919 --> 00:23:06,640
thinking oh like is our thesis gone now

609
00:23:04,880 --> 00:23:08,640
because this thing can really do

610
00:23:06,640 --> 00:23:11,280
everything that we can do but much much

611
00:23:08,640 --> 00:23:13,200
faster so uh this paradigm shifting

612
00:23:11,280 --> 00:23:15,600
moment is only 3 years ago from there

613
00:23:13,200 --> 00:23:18,159
we've gone to like these much better

614
00:23:15,600 --> 00:23:20,720
reasoning models to agentic models that

615
00:23:18,159 --> 00:23:22,480
can actually do things. Uh now we are

616
00:23:20,720 --> 00:23:24,159
talking about guardrails and trust and

617
00:23:22,480 --> 00:23:26,159
accountability and talking about

618
00:23:24,159 --> 00:23:28,080
interoperability. So I don't think this

619
00:23:26,159 --> 00:23:31,520
is too farfetched. I think this is in

620
00:23:28,080 --> 00:23:35,520
near term. Uh uh I don't think it's a

621
00:23:31,520 --> 00:23:37,760
given uh that like this uh utopian

622
00:23:35,520 --> 00:23:40,720
asentic web is what we'll see in the

623
00:23:37,760 --> 00:23:42,400
future. It could be like the app economy

624
00:23:40,720 --> 00:23:46,000
or it could be like cloud providers.

625
00:23:42,400 --> 00:23:48,320
could be in like uh a few uh providers

626
00:23:46,000 --> 00:23:51,280
as opposed to like uh broadly open like

627
00:23:48,320 --> 00:23:54,320
the web. Uh we think it should be open.

628
00:23:51,280 --> 00:23:56,799
Uh I know there will be uh uh differing

629
00:23:54,320 --> 00:23:58,000
perspectives with pros and cons and I

630
00:23:56,799 --> 00:23:59,679
think we'll arrive at some sort of

631
00:23:58,000 --> 00:24:01,840
hybrid in the future.

632
00:23:59,679 --> 00:24:02,640
>> Great. Um I'll put in my two bits.

633
00:24:01,840 --> 00:24:04,480
Thanks for that. It was a really

634
00:24:02,640 --> 00:24:06,960
interesting uh answer. I I would say

635
00:24:04,480 --> 00:24:10,159
that today we're seeing our customers

636
00:24:06,960 --> 00:24:12,000
ask us to allow some other agents that

637
00:24:10,159 --> 00:24:13,760
they're working with in other parts of

638
00:24:12,000 --> 00:24:16,559
their organizations to speak to our

639
00:24:13,760 --> 00:24:19,200
agents. So this request is already here.

640
00:24:16,559 --> 00:24:22,159
It's not in the future, it's today. Um

641
00:24:19,200 --> 00:24:24,480
how can our agents talk to your agents?

642
00:24:22,159 --> 00:24:25,919
So we and then there's going to be some

643
00:24:24,480 --> 00:24:28,880
simplistic ways and it's going to get

644
00:24:25,919 --> 00:24:31,039
better and better for sure. So I I think

645
00:24:28,880 --> 00:24:32,799
uh it's not too early to start figuring

646
00:24:31,039 --> 00:24:34,400
this stuff out. In fact, it may be ready

647
00:24:32,799 --> 00:24:37,679
for a little bit too late. And then the

648
00:24:34,400 --> 00:24:40,720
other thing I'd say is that I do think

649
00:24:37,679 --> 00:24:42,080
that autonomy, you know, there's a lot

650
00:24:40,720 --> 00:24:43,760
of automation stuff that's been

651
00:24:42,080 --> 00:24:45,600
happening. There's this thing called

652
00:24:43,760 --> 00:24:48,320
robotic process automation, which was a

653
00:24:45,600 --> 00:24:50,320
big deal uh in the software world for a

654
00:24:48,320 --> 00:24:52,080
while, and that's sort of that that

655
00:24:50,320 --> 00:24:54,559
trend is pushing more and more things to

656
00:24:52,080 --> 00:24:55,919
get automated. Um, so that's not going

657
00:24:54,559 --> 00:24:58,559
away. The pressure for automation

658
00:24:55,919 --> 00:25:00,880
continues and can will continue to grow.

659
00:24:58,559 --> 00:25:03,039
And so I I see everything that's

660
00:25:00,880 --> 00:25:04,880
automatable, safely automatable, will

661
00:25:03,039 --> 00:25:07,440
get automated over the next five years.

662
00:25:04,880 --> 00:25:10,000
It feels that way to me. Um and and then

663
00:25:07,440 --> 00:25:11,520
after that, how do we make it safe for

664
00:25:10,000 --> 00:25:13,039
the rest of the stuff to get that's you

665
00:25:11,520 --> 00:25:14,880
know as we continue to automate more and

666
00:25:13,039 --> 00:25:19,600
more and that's where I'm a little bit

667
00:25:14,880 --> 00:25:22,400
concerned that um we are overly uh

668
00:25:19,600 --> 00:25:25,919
invested in some some some very popular

669
00:25:22,400 --> 00:25:28,159
approaches right now that may not turn

670
00:25:25,919 --> 00:25:30,159
out to be the right ones for the high

671
00:25:28,159 --> 00:25:32,720
trust high mission criticality use

672
00:25:30,159 --> 00:25:35,919
cases. So that that's just I I I want to

673
00:25:32,720 --> 00:25:38,640
say that if we need to have a plurality

674
00:25:35,919 --> 00:25:41,200
plurality of approaches and right now we

675
00:25:38,640 --> 00:25:42,960
seem to have sort of a monoculture uh in

676
00:25:41,200 --> 00:25:45,039
in in the way that we're representing

677
00:25:42,960 --> 00:25:46,559
knowledge and the way that we're asking

678
00:25:45,039 --> 00:25:48,960
to build these systems that can help us

679
00:25:46,559 --> 00:25:50,320
make decisions. Um there's there's

680
00:25:48,960 --> 00:25:51,919
there's some there there many kinds of

681
00:25:50,320 --> 00:25:53,360
agents not all of them have to be based

682
00:25:51,919 --> 00:25:53,600
on LLM. That's that's what I'm trying to

683
00:25:53,360 --> 00:25:54,960
get.

684
00:25:53,600 --> 00:25:57,360
>> That's what I was getting at is or

685
00:25:54,960 --> 00:25:59,600
thinking about is that I I sense a

686
00:25:57,360 --> 00:26:02,400
controversial opinion there. or is is

687
00:25:59,600 --> 00:26:03,600
there if do you want to expand on that?

688
00:26:02,400 --> 00:26:05,600
>> Sure. Um

689
00:26:03,600 --> 00:26:05,919
>> I mean if you think LLM suck just just

690
00:26:05,600 --> 00:26:07,200
tell us.

691
00:26:05,919 --> 00:26:11,279
>> I don't think they suck. They're good at

692
00:26:07,200 --> 00:26:12,640
what they do but uh every time someone

693
00:26:11,279 --> 00:26:13,840
bumps into a thing where it's like oh

694
00:26:12,640 --> 00:26:15,600
this LLM keeps on giving me the wrong

695
00:26:13,840 --> 00:26:17,120
answer. Oh, it's only right 50% of the

696
00:26:15,600 --> 00:26:19,200
time. I'm just going to sort of ignore

697
00:26:17,120 --> 00:26:20,720
that and move on. How common is that

698
00:26:19,200 --> 00:26:23,279
situation? I think it's way more common

699
00:26:20,720 --> 00:26:25,120
that people are uh putting on. And I do

700
00:26:23,279 --> 00:26:26,320
think that the solutions they're coming

701
00:26:25,120 --> 00:26:28,960
up with a bit more like putting

702
00:26:26,320 --> 00:26:30,720
band-aids when there's something sort of

703
00:26:28,960 --> 00:26:31,919
wrong at the core that's never there's

704
00:26:30,720 --> 00:26:34,159
never there's never going to be this

705
00:26:31,919 --> 00:26:37,760
kind of really humanlike reasoning and

706
00:26:34,159 --> 00:26:39,120
planning and uh sort of common sense

707
00:26:37,760 --> 00:26:40,480
understanding capability. I don't think

708
00:26:39,120 --> 00:26:41,600
it's going to come out of LMS. So I

709
00:26:40,480 --> 00:26:42,799
think we need to invest in other

710
00:26:41,600 --> 00:26:45,760
approaches and there are people doing

711
00:26:42,799 --> 00:26:47,760
that. Uh but uh we need to invest more

712
00:26:45,760 --> 00:26:49,039
in those things than to keep on dumping

713
00:26:47,760 --> 00:26:50,320
billions into build building data

714
00:26:49,039 --> 00:26:52,000
centers for LM. That's that's that's

715
00:26:50,320 --> 00:26:53,120
that's what I'm going to say. I was

716
00:26:52,000 --> 00:26:56,000
thinking maybe if we can get some

717
00:26:53,120 --> 00:26:56,240
funding for AI startups. U then that'll

718
00:26:56,000 --> 00:26:57,520
happen.

719
00:26:56,240 --> 00:26:57,760
>> That that that's a good way to do it.

720
00:26:57,520 --> 00:27:00,720
Yeah,

721
00:26:57,760 --> 00:27:02,400
>> I'm making a joke there. So

722
00:27:00,720 --> 00:27:06,240
>> it's about the the main things that are

723
00:27:02,400 --> 00:27:09,240
getting funding these days. Um

724
00:27:06,240 --> 00:27:09,240
so

725
00:27:09,600 --> 00:27:16,080
let's talk about open versus controlled

726
00:27:13,360 --> 00:27:20,960
and uh

727
00:27:16,080 --> 00:27:22,880
do you see open standards like NANDA? Um

728
00:27:20,960 --> 00:27:25,520
how do you see that playing with more

729
00:27:22,880 --> 00:27:30,240
the the the closed ecosystems that may

730
00:27:25,520 --> 00:27:33,600
be required or enforced by say corporate

731
00:27:30,240 --> 00:27:35,919
uh entities? you do you see there that

732
00:27:33,600 --> 00:27:38,799
conflict or you see any uh way way to

733
00:27:35,919 --> 00:27:42,320
solve the compatibility?

734
00:27:38,799 --> 00:27:44,400
uh so I think the good thing is a lot of

735
00:27:42,320 --> 00:27:46,799
the innovation and a lot of it is coming

736
00:27:44,400 --> 00:27:49,440
from industry in this case uh both at

737
00:27:46,799 --> 00:27:52,559
the protocol and uh infrastructure

738
00:27:49,440 --> 00:27:54,399
aspect a lot of this is almost all of it

739
00:27:52,559 --> 00:27:58,640
is open source if you think of it think

740
00:27:54,399 --> 00:28:00,799
of MCP or A2A all of this is largely

741
00:27:58,640 --> 00:28:02,240
open source which is great uh I think

742
00:28:00,799 --> 00:28:04,480
these things need to be open source they

743
00:28:02,240 --> 00:28:06,240
need to be collaboratively developed uh

744
00:28:04,480 --> 00:28:09,279
that being said there are very

745
00:28:06,240 --> 00:28:11,360
legitimate use cases and requirements

746
00:28:09,279 --> 00:28:15,360
where things need to be private

747
00:28:11,360 --> 00:28:19,200
protected sort of uh firewalled uh

748
00:28:15,360 --> 00:28:22,320
access to critical agents uh cannot be

749
00:28:19,200 --> 00:28:24,159
like you know open to everyone uh and I

750
00:28:22,320 --> 00:28:26,000
think that's perfectly fine uh that's

751
00:28:24,159 --> 00:28:28,720
how the current web also operates not

752
00:28:26,000 --> 00:28:33,120
everything is open to everyone and this

753
00:28:28,720 --> 00:28:36,320
uh uh should be accommodated uh so uh

754
00:28:33,120 --> 00:28:38,720
for example when I was talking about our

755
00:28:36,320 --> 00:28:42,399
uh thinking about interoperability and

756
00:28:38,720 --> 00:28:44,480
registries. Uh the reason why we are not

757
00:28:42,399 --> 00:28:46,080
thinking about building the one registry

758
00:28:44,480 --> 00:28:48,080
for AI agents but more thinking about

759
00:28:46,080 --> 00:28:50,880
building the switchboard is exactly this

760
00:28:48,080 --> 00:28:53,919
because uh practically speaking it is

761
00:28:50,880 --> 00:28:55,679
very legitimate for industry players to

762
00:28:53,919 --> 00:28:58,080
have access to their own agent

763
00:28:55,679 --> 00:29:00,320
registries and have control over who

764
00:28:58,080 --> 00:29:02,000
accesses them versus who does not. But

765
00:29:00,320 --> 00:29:04,480
when someone wants to go outside the

766
00:29:02,000 --> 00:29:06,159
silo as Cyrus mentioned uh when someone

767
00:29:04,480 --> 00:29:07,440
wants to interoperate with other agents

768
00:29:06,159 --> 00:29:09,279
there should be a way for that to be

769
00:29:07,440 --> 00:29:11,600
possible. So I think it has to be this

770
00:29:09,279 --> 00:29:14,320
sort of pragmatic approach uh rather

771
00:29:11,600 --> 00:29:16,159
than uh gunning for everything has to be

772
00:29:14,320 --> 00:29:19,240
open. Uh so I think that's that's the

773
00:29:16,159 --> 00:29:19,240
way forward.

774
00:29:19,279 --> 00:29:26,480
Um Cyrus uh just to return back uh

775
00:29:22,960 --> 00:29:29,600
before it it seems if I could paraphrase

776
00:29:26,480 --> 00:29:31,679
what you said is that it at least at

777
00:29:29,600 --> 00:29:34,640
Leela the with your customers the

778
00:29:31,679 --> 00:29:36,159
agentic web is happening now and it's

779
00:29:34,640 --> 00:29:39,520
happening because people are paying you

780
00:29:36,159 --> 00:29:40,880
to do it which

781
00:29:39,520 --> 00:29:42,640
it from a business point of view it's

782
00:29:40,880 --> 00:29:44,799
great to have a market signal that that

783
00:29:42,640 --> 00:29:48,080
that this is happening and it's it

784
00:29:44,799 --> 00:29:49,440
bodess well for the future of agentic

785
00:29:48,080 --> 00:29:54,000
systems.

786
00:29:49,440 --> 00:29:56,320
in in in general, do you see other than

787
00:29:54,000 --> 00:29:59,440
what we've mentioned here already, do

788
00:29:56,320 --> 00:30:01,679
you see any killer apps or industries

789
00:29:59,440 --> 00:30:04,320
where this will have the most return or

790
00:30:01,679 --> 00:30:06,559
or things happening sooner? Is there any

791
00:30:04,320 --> 00:30:09,600
projections that that forecast that you

792
00:30:06,559 --> 00:30:11,120
can make in those or may maybe from IBM

793
00:30:09,600 --> 00:30:13,039
and and your customers that you're

794
00:30:11,120 --> 00:30:15,039
dealing with? Are there are there things

795
00:30:13,039 --> 00:30:16,640
that you see happening sooner rather

796
00:30:15,039 --> 00:30:18,159
than later?

797
00:30:16,640 --> 00:30:19,919
Well, I think I spoke a lot, so I think

798
00:30:18,159 --> 00:30:24,159
Dan should go first and I'll go after.

799
00:30:19,919 --> 00:30:25,760
>> Yeah, I'm Yeah, I'm not seeing it yet.

800
00:30:24,159 --> 00:30:27,840
So, I can't just say, "Oh, this industry

801
00:30:25,760 --> 00:30:30,320
is like I Yeah, I can see this

802
00:30:27,840 --> 00:30:32,640
happening." I mean, I'll go back to also

803
00:30:30,320 --> 00:30:35,679
your previous questions about and I

804
00:30:32,640 --> 00:30:38,320
agree with Puma about things will be

805
00:30:35,679 --> 00:30:39,919
hybrid and in some you know sections

806
00:30:38,320 --> 00:30:42,720
there will be a web and people will be

807
00:30:39,919 --> 00:30:44,720
more sharing it and other others not. Um

808
00:30:42,720 --> 00:30:47,120
I I think the big blocker is going back

809
00:30:44,720 --> 00:30:49,279
two questions before is the privacy

810
00:30:47,120 --> 00:30:50,320
issue, data privacy. Uh there's a whole

811
00:30:49,279 --> 00:30:52,799
field in machine learning. It's called

812
00:30:50,320 --> 00:30:55,039
federated learning. Um it's been doing

813
00:30:52,799 --> 00:30:57,520
great academically. It almost had no

814
00:30:55,039 --> 00:30:59,520
impact on business because at the end of

815
00:30:57,520 --> 00:31:01,520
the of the day, you know, it was

816
00:30:59,520 --> 00:31:03,919
supposed to be the kind of the story we

817
00:31:01,520 --> 00:31:05,360
always tell healthcare and hospitals

818
00:31:03,919 --> 00:31:06,960
will train their own models and they'll

819
00:31:05,360 --> 00:31:11,679
share the models, but they'll keep the

820
00:31:06,960 --> 00:31:14,480
data. Never happened. Um I think this

821
00:31:11,679 --> 00:31:16,159
will still not happen. Um especially in

822
00:31:14,480 --> 00:31:18,559
those industries, healthcare, banking,

823
00:31:16,159 --> 00:31:21,279
things like that. Um but there could be

824
00:31:18,559 --> 00:31:23,760
others where you know organization will

825
00:31:21,279 --> 00:31:26,000
be willing to be more sharing but I'm

826
00:31:23,760 --> 00:31:27,360
not like I can't point what would that

827
00:31:26,000 --> 00:31:28,640
be.

828
00:31:27,360 --> 00:31:30,480
>> Okay.

829
00:31:28,640 --> 00:31:33,360
>> Yeah. And to the your point about you

830
00:31:30,480 --> 00:31:35,679
know what are the other big wins for

831
00:31:33,360 --> 00:31:38,480
having uh agentic systems out in the

832
00:31:35,679 --> 00:31:40,640
wild and being used by our customers I

833
00:31:38,480 --> 00:31:42,320
can't speak for lots of other but in the

834
00:31:40,640 --> 00:31:43,519
indust other other other verticals but

835
00:31:42,320 --> 00:31:46,080
in the industrial vertical and the

836
00:31:43,519 --> 00:31:49,360
manufacturing vertical um even within

837
00:31:46,080 --> 00:31:51,120
there there's multiple uh use cases that

838
00:31:49,360 --> 00:31:52,480
have proven at least in our experience

839
00:31:51,120 --> 00:31:55,120
to be extremely beneficial for

840
00:31:52,480 --> 00:31:59,440
customers. Uh we spoke about a little

841
00:31:55,120 --> 00:32:01,600
bit about let's say um safety and so you

842
00:31:59,440 --> 00:32:03,600
know it's not like an agent's going to

843
00:32:01,600 --> 00:32:06,720
turn off the machine but we see our

844
00:32:03,600 --> 00:32:09,039
agents doing things like um noticing

845
00:32:06,720 --> 00:32:10,320
when there's uh people not following the

846
00:32:09,039 --> 00:32:12,640
rules right there's some safety rules

847
00:32:10,320 --> 00:32:15,039
that have to h happen in an environment.

848
00:32:12,640 --> 00:32:16,880
So if our sensors see someone's not

849
00:32:15,039 --> 00:32:19,120
following those rules you have to make a

850
00:32:16,880 --> 00:32:21,120
very contextual decision on whether this

851
00:32:19,120 --> 00:32:23,679
is something that needs to be noted or

852
00:32:21,120 --> 00:32:25,279
not. So those kinds of decisions our

853
00:32:23,679 --> 00:32:27,679
agents are making and that's turning out

854
00:32:25,279 --> 00:32:29,760
to be extremely valuable. And so when

855
00:32:27,679 --> 00:32:31,279
you ask any CEO of any manufacturer or

856
00:32:29,760 --> 00:32:33,279
industrial, what's your number one

857
00:32:31,279 --> 00:32:35,679
concern? I don't want anybody getting

858
00:32:33,279 --> 00:32:36,880
hurt or injured on my watch, right?

859
00:32:35,679 --> 00:32:39,039
That's actually often the number one

860
00:32:36,880 --> 00:32:40,720
concern that we hear. So agents that can

861
00:32:39,039 --> 00:32:42,080
deliver on that are going to be highly

862
00:32:40,720 --> 00:32:45,200
desirable and will deliver lots of

863
00:32:42,080 --> 00:32:46,559
value. Um especially because if you

864
00:32:45,200 --> 00:32:48,640
think about safety, you have to the more

865
00:32:46,559 --> 00:32:50,559
eyes you have on someplace, the safer it

866
00:32:48,640 --> 00:32:52,559
is usually, right? So if you have visual

867
00:32:50,559 --> 00:32:54,720
agents that can be everywhere that your

868
00:32:52,559 --> 00:32:56,559
people can't be, that blankets it with

869
00:32:54,720 --> 00:32:58,960
these uh many agents that are looking

870
00:32:56,559 --> 00:33:00,799
for certain signals and and and that

871
00:32:58,960 --> 00:33:03,279
creates safety. So that's a big one. The

872
00:33:00,799 --> 00:33:06,000
other ones I think are uh agents that

873
00:33:03,279 --> 00:33:07,440
are sort of like industrial engineers,

874
00:33:06,000 --> 00:33:09,360
uh artificial industrial engineers,

875
00:33:07,440 --> 00:33:10,799
artificial manufacturing engineers. So

876
00:33:09,360 --> 00:33:12,320
they're looking at what they're seeing

877
00:33:10,799 --> 00:33:13,840
and saying is is there a better way to

878
00:33:12,320 --> 00:33:16,240
do this? Is there a more efficient, more

879
00:33:13,840 --> 00:33:17,360
productive way of spending our time? And

880
00:33:16,240 --> 00:33:19,440
so they can come back with

881
00:33:17,360 --> 00:33:22,159
recommendations saying, "Hey, I noticed

882
00:33:19,440 --> 00:33:24,159
that if you move this around and lay

883
00:33:22,159 --> 00:33:27,120
change the layout, you could have people

884
00:33:24,159 --> 00:33:28,240
walking 40% less per day and still

885
00:33:27,120 --> 00:33:30,559
getting their work done without having

886
00:33:28,240 --> 00:33:32,799
to walk so far. So that that kind of

887
00:33:30,559 --> 00:33:34,240
stuff, those agents are are I think

888
00:33:32,799 --> 00:33:35,840
going to be extreme. They've they've

889
00:33:34,240 --> 00:33:37,200
already shown their value and the

890
00:33:35,840 --> 00:33:39,840
customers want more of that. And the

891
00:33:37,200 --> 00:33:41,360
last one is quality. So uh there's

892
00:33:39,840 --> 00:33:43,360
there's a right way to do things that

893
00:33:41,360 --> 00:33:45,440
can create good products. There's some

894
00:33:43,360 --> 00:33:47,519
things that people do or even robots can

895
00:33:45,440 --> 00:33:49,840
do that can create defects and create

896
00:33:47,519 --> 00:33:51,039
quality problems and having agents on

897
00:33:49,840 --> 00:33:53,200
the lookout for that and supporting

898
00:33:51,039 --> 00:33:54,640
people and and uh giving them support

899
00:33:53,200 --> 00:33:56,240
when they're maybe about to do something

900
00:33:54,640 --> 00:33:58,000
that would cause a quality problem.

901
00:33:56,240 --> 00:33:59,200
Those are things that are also really

902
00:33:58,000 --> 00:34:01,200
really big. So those are the three

903
00:33:59,200 --> 00:34:03,440
things that I've noticed that I think

904
00:34:01,200 --> 00:34:05,679
agents if they're distributed and

905
00:34:03,440 --> 00:34:07,919
working all over an organization can can

906
00:34:05,679 --> 00:34:10,560
really deliver a lot of value.

907
00:34:07,919 --> 00:34:13,119
>> Perfect. I I think in the the last

908
00:34:10,560 --> 00:34:15,359
minute or two we'll just uh maybe open

909
00:34:13,119 --> 00:34:17,359
it up for the audience. Are there any

910
00:34:15,359 --> 00:34:19,679
burning questions that anyone has at

911
00:34:17,359 --> 00:34:22,679
this point and uh wants to put it to the

912
00:34:19,679 --> 00:34:22,679
panel?

913
00:34:23,760 --> 00:34:26,720
>> Oh,

914
00:34:24,960 --> 00:34:28,560
>> not really a burning question. I'm more

915
00:34:26,720 --> 00:34:30,960
curious on what you what you mentioned

916
00:34:28,560 --> 00:34:33,839
size. You said something about have

917
00:34:30,960 --> 00:34:36,639
reached that peak in terms of contextual

918
00:34:33,839 --> 00:34:39,119
you know reasoning or actually coming up

919
00:34:36,639 --> 00:34:41,040
with an answer. You you also mentioned a

920
00:34:39,119 --> 00:34:43,280
very thing which got me very curious.

921
00:34:41,040 --> 00:34:45,119
You said something about

922
00:34:43,280 --> 00:34:46,480
having other ways or other different

923
00:34:45,119 --> 00:34:48,000
type of agents. People are already

924
00:34:46,480 --> 00:34:49,679
working on it. I was more curious on

925
00:34:48,000 --> 00:34:53,040
what you meant by that.

926
00:34:49,679 --> 00:34:56,879
>> Yeah, sure. There's active uh work being

927
00:34:53,040 --> 00:34:59,359
done on um

928
00:34:56,879 --> 00:35:00,480
systems that can approach the same kinds

929
00:34:59,359 --> 00:35:04,079
of problems that people are trying to

930
00:35:00,480 --> 00:35:06,560
solve with language models but using uh

931
00:35:04,079 --> 00:35:08,720
hybrids that include symbolic

932
00:35:06,560 --> 00:35:10,320
representations and symbolic processing.

933
00:35:08,720 --> 00:35:13,119
Uh you can call these neurosymbolic

934
00:35:10,320 --> 00:35:15,680
models. And so these models are hybrids.

935
00:35:13,119 --> 00:35:18,720
They're not stochastic and purely based

936
00:35:15,680 --> 00:35:20,320
on tokens in tokens out but more uh

937
00:35:18,720 --> 00:35:22,960
there's there's actually a word world

938
00:35:20,320 --> 00:35:24,720
model inside of it that can have

939
00:35:22,960 --> 00:35:28,560
different uristics and different kinds

940
00:35:24,720 --> 00:35:30,240
of um learning uh pro processes and also

941
00:35:28,560 --> 00:35:32,160
different kinds of inferial processes

942
00:35:30,240 --> 00:35:34,560
that are not doing the same kind of

943
00:35:32,160 --> 00:35:36,079
inference that LM do and they do develop

944
00:35:34,560 --> 00:35:38,400
internal ontologies sort of automated

945
00:35:36,079 --> 00:35:40,000
ontology building automated uh graph

946
00:35:38,400 --> 00:35:42,160
building automated knowledge network

947
00:35:40,000 --> 00:35:43,280
building so you still get the

948
00:35:42,160 --> 00:35:46,839
scalability

949
00:35:43,280 --> 00:35:46,839
But you als

950
00:35:57,520 --> 00:36:02,960
Facebook's meta's emphasis on uh allin

951
00:36:01,200 --> 00:36:05,760
on language models was never going to

952
00:36:02,960 --> 00:36:08,160
achieve the uh type of intelligence that

953
00:36:05,760 --> 00:36:10,320
Yan Lun who's been working in AI for 40

954
00:36:08,160 --> 00:36:12,880
years or 30 years maybe but anyway he he

955
00:36:10,320 --> 00:36:14,720
he sees a dead end and So he left

956
00:36:12,880 --> 00:36:15,839
Facebook and Meta to found his own

957
00:36:14,720 --> 00:36:18,400
company and do things in a different

958
00:36:15,839 --> 00:36:20,640
way. So keep an eye on him and us and

959
00:36:18,400 --> 00:36:22,560
others. There'll be other ideas. There's

960
00:36:20,640 --> 00:36:24,720
many ways to get there. It's not there's

961
00:36:22,560 --> 00:36:26,160
not only one way and I think the other

962
00:36:24,720 --> 00:36:28,320
ways have a more likelihood of

963
00:36:26,160 --> 00:36:30,960
succeeding than the current scaling of

964
00:36:28,320 --> 00:36:32,720
LMS because as somebody said maybe you

965
00:36:30,960 --> 00:36:34,240
said that you know there's there's no

966
00:36:32,720 --> 00:36:36,560
more public data to find. So if you're

967
00:36:34,240 --> 00:36:38,079
hoping to get better results from adding

968
00:36:36,560 --> 00:36:39,680
another trillion words or another 100red

969
00:36:38,079 --> 00:36:41,599
trillion words there are no more public

970
00:36:39,680 --> 00:36:42,800
sources of that data to train on. So

971
00:36:41,599 --> 00:36:44,640
then you say, well, okay, maybe our

972
00:36:42,800 --> 00:36:46,480
algorithms can get better, but the

973
00:36:44,640 --> 00:36:48,560
algorithms are incremental, right? Is

974
00:36:46,480 --> 00:36:50,640
GPT5

975
00:36:48,560 --> 00:36:52,720
order of magnitude better than GPT4? And

976
00:36:50,640 --> 00:36:54,720
will GTP6 be? No, they're not. They're

977
00:36:52,720 --> 00:36:57,520
incremental improvements, but they're

978
00:36:54,720 --> 00:36:59,920
not that next order of magnitude. So I I

979
00:36:57,520 --> 00:37:01,760
see the the wall being clear. And uh I'm

980
00:36:59,920 --> 00:37:03,920
not alone in that uh in that in that

981
00:37:01,760 --> 00:37:04,480
feeling right now.

982
00:37:03,920 --> 00:37:07,599
>> Okay.

983
00:37:04,480 --> 00:37:10,240
>> Not on this stage.

984
00:37:07,599 --> 00:37:13,520
Well, I I think I just want to thank Dan

985
00:37:10,240 --> 00:37:15,200
Cyrus Praduna uh and uh the whole panel

986
00:37:13,520 --> 00:37:16,560
and also the audience for your

987
00:37:15,200 --> 00:37:19,520
participation, but we're going to wrap

988
00:37:16,560 --> 00:37:21,440
things up now and make sure you get out

989
00:37:19,520 --> 00:37:23,839
and get your bag lunch before all the

990
00:37:21,440 --> 00:37:27,720
good sandwiches are gone. So, uh let's

991
00:37:23,839 --> 00:37:27,720
let's hear it for our panel.

