1
00:00:01,360 --> 00:00:05,920
I heard that you guys talked too much

2
00:00:03,520 --> 00:00:08,480
before I got here and so we're running

3
00:00:05,920 --> 00:00:09,840
late and so I will not talk too much. I

4
00:00:08,480 --> 00:00:11,200
will keep us on time which means you

5
00:00:09,840 --> 00:00:12,519
have to pay attention because I talk

6
00:00:11,200 --> 00:00:15,280
fast.

7
00:00:12,519 --> 00:00:16,720
So how do you think nonclinical

8
00:00:15,280 --> 00:00:18,800
information shapes clinical

9
00:00:16,720 --> 00:00:20,640
decisionmaking in an age of large

10
00:00:18,800 --> 00:00:22,640
language models? It's an open question.

11
00:00:20,640 --> 00:00:24,880
Large language models are actually being

12
00:00:22,640 --> 00:00:26,640
used in every Boston area hospital right

13
00:00:24,880 --> 00:00:29,439
now to make decisions about patient

14
00:00:26,640 --> 00:00:32,239
care. Every Boston area hospital that uh

15
00:00:29,439 --> 00:00:34,800
you've been to or will go to is using

16
00:00:32,239 --> 00:00:36,960
large language models to create patient

17
00:00:34,800 --> 00:00:38,800
note summaries and to answer questions

18
00:00:36,960 --> 00:00:40,640
and to interact between patients and

19
00:00:38,800 --> 00:00:42,800
providers. So if you get an email from

20
00:00:40,640 --> 00:00:46,000
your provider, it was probably drafted

21
00:00:42,800 --> 00:00:47,440
by a large language model. Okay, so

22
00:00:46,000 --> 00:00:50,559
that's a reality. This is what's

23
00:00:47,440 --> 00:00:53,320
happening. The question is, how does

24
00:00:50,559 --> 00:00:56,320
changing the non-clinical

25
00:00:53,320 --> 00:00:59,760
information in clinical text affect

26
00:00:56,320 --> 00:01:02,239
large language model reasoning? Okay, so

27
00:00:59,760 --> 00:01:04,320
let's figure out how we're going to uh

28
00:01:02,239 --> 00:01:06,640
do this experiment. The first thing that

29
00:01:04,320 --> 00:01:08,159
we have is original clinical data, and

30
00:01:06,640 --> 00:01:10,400
we'll go through some examples of what

31
00:01:08,159 --> 00:01:13,280
those look like. We're going to perturb

32
00:01:10,400 --> 00:01:16,159
the data in some simple ways using some

33
00:01:13,280 --> 00:01:17,640
plausible uh differences that come from

34
00:01:16,159 --> 00:01:20,119
social

35
00:01:17,640 --> 00:01:22,560
sciences. The categories of

36
00:01:20,119 --> 00:01:25,200
pertabbations match things that people

37
00:01:22,560 --> 00:01:27,280
in health and social sciences have noted

38
00:01:25,200 --> 00:01:29,920
correspond to patient groups of

39
00:01:27,280 --> 00:01:32,400
interest. So for example, we know that

40
00:01:29,920 --> 00:01:34,560
female patients tend to get less care

41
00:01:32,400 --> 00:01:36,799
and people if you just swap the gender

42
00:01:34,560 --> 00:01:38,479
of a patient, doctors tend to prescribe,

43
00:01:36,799 --> 00:01:40,400
for example, fewer painkillers and fewer

44
00:01:38,479 --> 00:01:42,880
interventions to them. And so we want to

45
00:01:40,400 --> 00:01:44,799
see if that carries through. There have

46
00:01:42,880 --> 00:01:46,960
been a lot of studies done on patients

47
00:01:44,799 --> 00:01:48,880
who have more more dramatic uh

48
00:01:46,960 --> 00:01:50,799
disposition or maybe more health

49
00:01:48,880 --> 00:01:53,680
anxiety. Doctors tend to interact with

50
00:01:50,799 --> 00:01:55,920
them differently. And finally, we have

51
00:01:53,680 --> 00:01:57,680
uh modifications that mimic people who

52
00:01:55,920 --> 00:02:00,079
have less technological aptitude or

53
00:01:57,680 --> 00:02:01,280
English proficiency. Okay? And so the

54
00:02:00,079 --> 00:02:03,680
way that we're going to actually

55
00:02:01,280 --> 00:02:05,200
perturve the notes or the questions or

56
00:02:03,680 --> 00:02:07,759
the summarizations, whatever we're

57
00:02:05,200 --> 00:02:10,319
using, is we're going to swap or remove

58
00:02:07,759 --> 00:02:12,400
gender to look at markers of gender.

59
00:02:10,319 --> 00:02:14,800
We're going to add uncertain language or

60
00:02:12,400 --> 00:02:16,800
colorful language to mimic implicit

61
00:02:14,800 --> 00:02:18,480
changes to the style of language. The

62
00:02:16,800 --> 00:02:20,400
content doesn't change, but the style

63
00:02:18,480 --> 00:02:23,920
does. And we're going to look at

64
00:02:20,400 --> 00:02:25,800
realistic semantic or structural changes

65
00:02:23,920 --> 00:02:28,239
to mimic people who have maybe less

66
00:02:25,800 --> 00:02:30,319
technological aptitude or maybe don't

67
00:02:28,239 --> 00:02:31,920
have English proficiency. Okay, these

68
00:02:30,319 --> 00:02:34,400
are simple pertibations that you can

69
00:02:31,920 --> 00:02:36,840
make. They do not change the actual

70
00:02:34,400 --> 00:02:39,280
clinical content of a question for

71
00:02:36,840 --> 00:02:43,360
example, but they do change sort of the

72
00:02:39,280 --> 00:02:46,800
style. All right, we are not going to

73
00:02:43,360 --> 00:02:49,599
study any genderspecific

74
00:02:46,800 --> 00:02:51,599
uh clinical conditions because we are

75
00:02:49,599 --> 00:02:54,080
looking at gendered pertibbations. And

76
00:02:51,599 --> 00:02:55,920
so before we do any of this uh this

77
00:02:54,080 --> 00:02:58,480
pertabbation, we actually remove all

78
00:02:55,920 --> 00:03:00,720
cases of gendered conditions, mentions

79
00:02:58,480 --> 00:03:03,040
of gender relevant organs or mentions of

80
00:03:00,720 --> 00:03:05,040
cases that uh have things like menration

81
00:03:03,040 --> 00:03:07,120
or pregnancy. Okay? Because the focus is

82
00:03:05,040 --> 00:03:08,800
to look at what happens when you flip

83
00:03:07,120 --> 00:03:09,920
gender and it shouldn't matter for the

84
00:03:08,800 --> 00:03:13,120
case. So we're not going to look at

85
00:03:09,920 --> 00:03:15,200
gendered conditions. Okay. All right.

86
00:03:13,120 --> 00:03:18,400
The data sets we're using are ask a do

87
00:03:15,200 --> 00:03:19,920
and on Q&A. These are standard clinical

88
00:03:18,400 --> 00:03:23,280
uh data sets that are used in the

89
00:03:19,920 --> 00:03:24,959
machine learning community. Um ask a do

90
00:03:23,280 --> 00:03:28,080
is Reddit posts about you know health

91
00:03:24,959 --> 00:03:29,599
questions and ailments. Um we reduce the

92
00:03:28,080 --> 00:03:31,760
subset so we're not looking at gendered

93
00:03:29,599 --> 00:03:32,879
questions. Same thing with an Q&A. We

94
00:03:31,760 --> 00:03:34,720
don't want to look at breast cancer or

95
00:03:32,879 --> 00:03:36,959
prostate cancer for example. So we take

96
00:03:34,720 --> 00:03:39,599
a subset of these questions. For all of

97
00:03:36,959 --> 00:03:41,200
these questions, we have a question and

98
00:03:39,599 --> 00:03:42,640
then we can ask a large language model

99
00:03:41,200 --> 00:03:45,440
to answer it. And then we also have a

100
00:03:42,640 --> 00:03:48,080
ground truth answer. Ask a doc looks

101
00:03:45,440 --> 00:03:49,840
like this. And maybe you've gone on

102
00:03:48,080 --> 00:03:52,159
Reddit before and and use these sort of

103
00:03:49,840 --> 00:03:53,680
posts. It's people sort of talking about

104
00:03:52,159 --> 00:03:55,599
something that they're concerned about

105
00:03:53,680 --> 00:03:57,599
and they're asking a doctor to respond

106
00:03:55,599 --> 00:03:59,599
to it. Right? So my question is how long

107
00:03:57,599 --> 00:04:01,040
should this gross thing be happening?

108
00:03:59,599 --> 00:04:02,560
How much can possibly be in there?

109
00:04:01,040 --> 00:04:03,879
Right? This is the style of question

110
00:04:02,560 --> 00:04:06,680
that's

111
00:04:03,879 --> 00:04:08,879
there. A Q&A is slightly more

112
00:04:06,680 --> 00:04:11,439
structured, right? And so it gives a

113
00:04:08,879 --> 00:04:13,120
sort of um vignette of a patient's

114
00:04:11,439 --> 00:04:14,480
history and then you have a message from

115
00:04:13,120 --> 00:04:17,040
a patient. I've been feeling more

116
00:04:14,480 --> 00:04:18,880
fatigued than usual for the past week.

117
00:04:17,040 --> 00:04:21,120
Having trouble completing my tasks. Is

118
00:04:18,880 --> 00:04:23,199
this normal? Should I be concerned? All

119
00:04:21,120 --> 00:04:25,759
right. So in both of these cases, we

120
00:04:23,199 --> 00:04:28,600
have some clinical text. A question is

121
00:04:25,759 --> 00:04:32,000
being asked. We're going to perturb the

122
00:04:28,600 --> 00:04:33,680
non-clinical part of the the text and

123
00:04:32,000 --> 00:04:36,639
we're going to see whether the large

124
00:04:33,680 --> 00:04:39,120
language model changes its response to

125
00:04:36,639 --> 00:04:42,400
your clinical question when we change

126
00:04:39,120 --> 00:04:44,080
non-clinical things. Okay, we use four

127
00:04:42,400 --> 00:04:45,360
different large language models that are

128
00:04:44,080 --> 00:04:47,360
popular in the machine learning

129
00:04:45,360 --> 00:04:48,960
community um because we wanted to look

130
00:04:47,360 --> 00:04:50,880
at a range of open and closed source

131
00:04:48,960 --> 00:04:52,400
models, large and small models, right?

132
00:04:50,880 --> 00:04:55,600
To make sure that that's not what would

133
00:04:52,400 --> 00:04:58,080
make a difference. Each time uh we have

134
00:04:55,600 --> 00:05:01,360
a question and its perturbed uh sort of

135
00:04:58,080 --> 00:05:03,520
cousin, we sample the LLM response three

136
00:05:01,360 --> 00:05:06,400
times for each model and then we're

137
00:05:03,520 --> 00:05:08,240
going to evaluate differences. Okay,

138
00:05:06,400 --> 00:05:10,160
does this setup sort of make sense? We

139
00:05:08,240 --> 00:05:12,479
have clinical things changing the

140
00:05:10,160 --> 00:05:14,759
nonclinical content measure whether the

141
00:05:12,479 --> 00:05:18,000
LLM changes its response and it

142
00:05:14,759 --> 00:05:20,400
shouldn't, right? We all agree here that

143
00:05:18,000 --> 00:05:22,240
if we don't change the clinical content,

144
00:05:20,400 --> 00:05:24,560
then hopefully the clinical

145
00:05:22,240 --> 00:05:26,240
recommendation shouldn't change. How are

146
00:05:24,560 --> 00:05:27,680
we going to measure differences? We're

147
00:05:26,240 --> 00:05:29,840
going to measure three things that are

148
00:05:27,680 --> 00:05:32,320
important in healthcare contexts. We're

149
00:05:29,840 --> 00:05:34,240
going to ask the large language model,

150
00:05:32,320 --> 00:05:37,039
do you think this patient should just

151
00:05:34,240 --> 00:05:39,440
manage their condition at home? Do you

152
00:05:37,039 --> 00:05:41,280
recommend that they get a visit? Right?

153
00:05:39,440 --> 00:05:43,520
Or do you recommend that they be given a

154
00:05:41,280 --> 00:05:45,520
resource? Right? And this is because

155
00:05:43,520 --> 00:05:47,199
this is often how large language models

156
00:05:45,520 --> 00:05:48,720
are being used in clinical setting.

157
00:05:47,199 --> 00:05:50,759
We're using them to triage who gets

158
00:05:48,720 --> 00:05:53,759
different resources.

159
00:05:50,759 --> 00:05:56,240
Okay. So this is an example of how we

160
00:05:53,759 --> 00:05:57,919
would annotate using a large language

161
00:05:56,240 --> 00:05:59,759
model a recommendation. You have a

162
00:05:57,919 --> 00:06:01,600
response and we can see here that this

163
00:05:59,759 --> 00:06:03,680
large language model thinks yes this

164
00:06:01,600 --> 00:06:04,960
person should get a visit. Yes, they

165
00:06:03,680 --> 00:06:08,080
should get a resource. No, they

166
00:06:04,960 --> 00:06:10,960
shouldn't manage it at home. Okay,

167
00:06:08,080 --> 00:06:14,240
nothing too fancy here.

168
00:06:10,960 --> 00:06:16,880
We're going to measure how bad uh the

169
00:06:14,240 --> 00:06:18,240
the model differences are in um a

170
00:06:16,880 --> 00:06:20,080
specific way. First, we're going to

171
00:06:18,240 --> 00:06:21,680
measure did something change at all.

172
00:06:20,080 --> 00:06:23,759
Let's say the question is I'm a

173
00:06:21,680 --> 00:06:25,600
35year-old male and a stray puppy's

174
00:06:23,759 --> 00:06:27,199
teeth grazed my leg. Should I get a

175
00:06:25,600 --> 00:06:30,639
treatment for rabies? This was a real

176
00:06:27,199 --> 00:06:32,960
question. Um if I just perturb and say,

177
00:06:30,639 --> 00:06:35,680
uh now I'm a 35-year-old female. So,

178
00:06:32,960 --> 00:06:37,360
just change M to F. And the model's

179
00:06:35,680 --> 00:06:39,440
original answer was no, don't

180
00:06:37,360 --> 00:06:42,080
self-manage. But now it says yes,

181
00:06:39,440 --> 00:06:44,240
self-manage. That was a change. It went

182
00:06:42,080 --> 00:06:46,960
from no to yes. And so I would record

183
00:06:44,240 --> 00:06:48,400
this as a treatment shift. But if it

184
00:06:46,960 --> 00:06:49,680
originally said yes and I change it to

185
00:06:48,400 --> 00:06:51,360
no, I would still record it as a

186
00:06:49,680 --> 00:06:54,080
treatment shift. So the second thing

187
00:06:51,360 --> 00:06:57,039
we're going to look at is whether care

188
00:06:54,080 --> 00:07:00,080
allocation is reduced. So whether I go

189
00:06:57,039 --> 00:07:02,080
from saying no, don't self-manage this,

190
00:07:00,080 --> 00:07:03,680
you get a resource, you get a visit, you

191
00:07:02,080 --> 00:07:06,319
get a lab test to saying yeah, yeah,

192
00:07:03,680 --> 00:07:08,160
you're fine, manage it at home. Okay. So

193
00:07:06,319 --> 00:07:10,960
the second thing is going to be when we

194
00:07:08,160 --> 00:07:13,360
have a difference a treatment shift do I

195
00:07:10,960 --> 00:07:16,560
reduce care that's the second metric the

196
00:07:13,360 --> 00:07:19,120
third metric is is that reduction of

197
00:07:16,560 --> 00:07:22,560
care something that clinicians think is

198
00:07:19,120 --> 00:07:25,039
an error okay and so we have a tiered

199
00:07:22,560 --> 00:07:28,560
set of metrics did something shift in

200
00:07:25,039 --> 00:07:30,560
your recommendation of those shifts were

201
00:07:28,560 --> 00:07:32,560
they reductions in care of the

202
00:07:30,560 --> 00:07:34,960
reductions of care which ones do

203
00:07:32,560 --> 00:07:36,639
clinicians think were wrong that is a

204
00:07:34,960 --> 00:07:39,000
clinical error it's not up to judgment

205
00:07:36,639 --> 00:07:41,759
like this shouldn't have happened.

206
00:07:39,000 --> 00:07:45,039
Okay, so first let's look in the

207
00:07:41,759 --> 00:07:47,759
aggregate. Do LLM change? And the answer

208
00:07:45,039 --> 00:07:49,840
is yes, they do. Under all of these

209
00:07:47,759 --> 00:07:51,440
pertibations, gender swapping, gender

210
00:07:49,840 --> 00:07:53,560
removal, uncertain and colorful

211
00:07:51,440 --> 00:07:56,240
language, exclamations, lower and

212
00:07:53,560 --> 00:08:00,360
uppercase. Uh there is a reasonable

213
00:07:56,240 --> 00:08:03,919
amount of change. Okay. Uh is care

214
00:08:00,360 --> 00:08:06,919
reduced in most cases? Yes. In most

215
00:08:03,919 --> 00:08:09,520
cases when you uh modify a

216
00:08:06,919 --> 00:08:11,000
note care tends to be reduced. People

217
00:08:09,520 --> 00:08:13,840
get fewer

218
00:08:11,000 --> 00:08:17,720
resources. Are these reductions in care

219
00:08:13,840 --> 00:08:21,759
resources medical errors? In most cases,

220
00:08:17,720 --> 00:08:24,720
yes. Okay, that's bad. Let's take a

221
00:08:21,759 --> 00:08:27,680
closer look. Is it just the models? No.

222
00:08:24,720 --> 00:08:30,240
Models actually flip across classes and

223
00:08:27,680 --> 00:08:31,759
metrics. And so here you can see for all

224
00:08:30,240 --> 00:08:34,240
of the different uh large language

225
00:08:31,759 --> 00:08:36,000
models that we evaluated, there isn't a

226
00:08:34,240 --> 00:08:37,200
consistent trend in like oh it's the

227
00:08:36,000 --> 00:08:38,719
large models that are doing really

228
00:08:37,200 --> 00:08:40,479
poorly or the small models that are

229
00:08:38,719 --> 00:08:43,200
doing really well. They're all

230
00:08:40,479 --> 00:08:45,519
consistently having these pertabbations

231
00:08:43,200 --> 00:08:47,720
uh change their behavior and this is

232
00:08:45,519 --> 00:08:50,080
across the manage visit and resource

233
00:08:47,720 --> 00:08:53,440
metrics. If we look at what's happening

234
00:08:50,080 --> 00:08:56,480
a little carefully, flips are reducing

235
00:08:53,440 --> 00:08:59,200
care more in female patients. Right? So

236
00:08:56,480 --> 00:09:02,320
this is when when uh a model chooses to

237
00:08:59,200 --> 00:09:04,959
reduce care. This is the difference

238
00:09:02,320 --> 00:09:07,839
between care allocated uh between male

239
00:09:04,959 --> 00:09:10,480
and uh female questions. And you can see

240
00:09:07,839 --> 00:09:12,320
if it's blue that means that the male

241
00:09:10,480 --> 00:09:14,320
questions were uh something was taken

242
00:09:12,320 --> 00:09:15,839
from them. If it's yellow that means

243
00:09:14,320 --> 00:09:18,160
that something was taken from female

244
00:09:15,839 --> 00:09:20,720
questions. And so in general what we're

245
00:09:18,160 --> 00:09:22,800
seeing is that the flips when there was

246
00:09:20,720 --> 00:09:25,040
a difference in the model the difference

247
00:09:22,800 --> 00:09:27,440
specifically reduces care in female

248
00:09:25,040 --> 00:09:29,600
patients. But is this reduction in care

249
00:09:27,440 --> 00:09:31,519
an error? Maybe these are things that

250
00:09:29,600 --> 00:09:34,399
are again, you know, not clinically an

251
00:09:31,519 --> 00:09:36,880
error. Well, they are. So, of the ones

252
00:09:34,399 --> 00:09:38,800
that uh have care reduction, this is the

253
00:09:36,880 --> 00:09:41,279
reduced care error rate. These are

254
00:09:38,800 --> 00:09:44,959
clinically incorrect reductions in uh

255
00:09:41,279 --> 00:09:46,839
care allocation. Okay. So, uh the last

256
00:09:44,959 --> 00:09:50,640
thing that we wanted to check

257
00:09:46,839 --> 00:09:52,959
is what is it even mean to be uh you

258
00:09:50,640 --> 00:09:54,480
know sort of a female question, right?

259
00:09:52,959 --> 00:09:56,800
because this is, you know, an original

260
00:09:54,480 --> 00:09:59,040
poster is saying that I am a 35-year-old

261
00:09:56,800 --> 00:10:01,360
male or female. That doesn't make the

262
00:09:59,040 --> 00:10:02,880
question female, right? It's just a

263
00:10:01,360 --> 00:10:06,399
question. You happen to be a female

264
00:10:02,880 --> 00:10:09,200
asking it. Um, and so we removed all

265
00:10:06,399 --> 00:10:11,519
mentions of gender in all of the queries

266
00:10:09,200 --> 00:10:13,680
and none of them are gender specific uh

267
00:10:11,519 --> 00:10:15,800
to begin with. And then we asked the

268
00:10:13,680 --> 00:10:19,519
model to infer the

269
00:10:15,800 --> 00:10:23,920
gender of the uh of the question and

270
00:10:19,519 --> 00:10:27,120
then wanted to see whether um the models

271
00:10:23,920 --> 00:10:29,040
uh the model would reduce care more for

272
00:10:27,120 --> 00:10:30,399
patients who were actually female

273
00:10:29,040 --> 00:10:32,399
questions from patients who are actually

274
00:10:30,399 --> 00:10:35,839
female in the original posting or for

275
00:10:32,399 --> 00:10:37,240
people it thought were female. So uh put

276
00:10:35,839 --> 00:10:40,720
another

277
00:10:37,240 --> 00:10:42,800
way, option one is if we remove all

278
00:10:40,720 --> 00:10:45,200
mentions of who is female, gender could

279
00:10:42,800 --> 00:10:46,959
still be sticky and so the model could

280
00:10:45,200 --> 00:10:50,880
still choose to reduce care for

281
00:10:46,959 --> 00:10:53,519
originally female text. Option two is

282
00:10:50,880 --> 00:10:55,839
that gender is fluid in the model's

283
00:10:53,519 --> 00:10:58,279
mind. Whoever it thinks is female,

284
00:10:55,839 --> 00:11:00,120
that's who it's going to reduce the care

285
00:10:58,279 --> 00:11:03,279
for.

286
00:11:00,120 --> 00:11:06,000
Okay. Do you who thinks option one will

287
00:11:03,279 --> 00:11:09,160
happen?

288
00:11:06,000 --> 00:11:13,120
Nobody. Who thinks option two will

289
00:11:09,160 --> 00:11:15,440
happen? You are so smart. Yeah. The

290
00:11:13,120 --> 00:11:18,000
model reduces care for who it thinks is

291
00:11:15,440 --> 00:11:20,399
female, not who is actually female. And

292
00:11:18,000 --> 00:11:23,120
when you explicitly mention female in

293
00:11:20,399 --> 00:11:25,760
the record, it knows. But when we remove

294
00:11:23,120 --> 00:11:27,600
all mentions of gender and we ask it to

295
00:11:25,760 --> 00:11:30,000
think about who is female, the people

296
00:11:27,600 --> 00:11:32,720
who it's reducing care for flips and it

297
00:11:30,000 --> 00:11:34,720
reduces the care in an erroneous way.

298
00:11:32,720 --> 00:11:36,880
Right? may not just reducing but in a

299
00:11:34,720 --> 00:11:38,600
way that clinicians say is incorrect for

300
00:11:36,880 --> 00:11:41,320
those it thinks are

301
00:11:38,600 --> 00:11:45,680
female. Good job ladies. Very

302
00:11:41,320 --> 00:11:48,399
smart. Um this is part of a larger set

303
00:11:45,680 --> 00:11:51,040
of experiments that we are uh conducting

304
00:11:48,399 --> 00:11:53,200
trying to understand how we have models

305
00:11:51,040 --> 00:11:55,279
that will be wrong sometimes and are

306
00:11:53,200 --> 00:11:57,440
likely to be wrong more often in groups

307
00:11:55,279 --> 00:12:00,079
that we often see mistakes in by human

308
00:11:57,440 --> 00:12:02,480
doctors and care providers such as women

309
00:12:00,079 --> 00:12:04,079
and minorities. Um some of the things

310
00:12:02,480 --> 00:12:06,639
that we've learned in other work to

311
00:12:04,079 --> 00:12:09,360
contextualize this contribution are that

312
00:12:06,639 --> 00:12:12,880
number one the losses that we see in

313
00:12:09,360 --> 00:12:14,399
medical contexts from bad AI are much

314
00:12:12,880 --> 00:12:16,639
worse than the gains we see from good

315
00:12:14,399 --> 00:12:19,120
AI. So this is a really nice paper this

316
00:12:16,639 --> 00:12:20,920
is uh not my paper it's uh from 2020 in

317
00:12:19,120 --> 00:12:24,000
nature medicine they did

318
00:12:20,920 --> 00:12:26,240
dermatology assisted by AI. This is

319
00:12:24,000 --> 00:12:28,240
hidden in figure 2 subfigure J but it's

320
00:12:26,240 --> 00:12:30,160
the best figure in the whole paper. What

321
00:12:28,240 --> 00:12:31,240
you can see here is that the benefit

322
00:12:30,160 --> 00:12:34,320
that you

323
00:12:31,240 --> 00:12:36,959
get across different expertise levels

324
00:12:34,320 --> 00:12:38,959
from having a good AI algorithm is good,

325
00:12:36,959 --> 00:12:42,079
right? It's statistically sign it's over

326
00:12:38,959 --> 00:12:44,279
zero. It's maybe good. And the loss from

327
00:12:42,079 --> 00:12:46,320
bad AI is much

328
00:12:44,279 --> 00:12:48,399
worse. Okay? So, if we're going to

329
00:12:46,320 --> 00:12:50,399
deploy AI in a clinical setting, which

330
00:12:48,399 --> 00:12:52,120
we already have, it better be pretty

331
00:12:50,399 --> 00:12:55,200
good, which it

332
00:12:52,120 --> 00:12:57,519
isn't. Humans are really susceptible to

333
00:12:55,200 --> 00:13:00,680
incorrect advice. This is a paper I

334
00:12:57,519 --> 00:13:04,399
wrote and here you can see that even

335
00:13:00,680 --> 00:13:07,440
radiologists so every um line there is

336
00:13:04,399 --> 00:13:09,360
an individual radiologist that we uh had

337
00:13:07,440 --> 00:13:11,600
participate in this study. So for

338
00:13:09,360 --> 00:13:15,160
example radiologist is this a pointer?

339
00:13:11,600 --> 00:13:19,839
Yes. Radiologist number

340
00:13:15,160 --> 00:13:21,680
60 got 75% of their cases right. But

341
00:13:19,839 --> 00:13:23,279
you'll notice that their bar is colored

342
00:13:21,680 --> 00:13:26,240
all blue. That means that they only got

343
00:13:23,279 --> 00:13:29,440
a case right when the AI that was giving

344
00:13:26,240 --> 00:13:31,519
them uh advice was correct. So some

345
00:13:29,440 --> 00:13:34,320
doctors over here got a 100% of their

346
00:13:31,519 --> 00:13:36,079
cases right. The two that were incorrect

347
00:13:34,320 --> 00:13:38,399
recommendations and the ones that were

348
00:13:36,079 --> 00:13:40,480
correct recommendations. So some doctors

349
00:13:38,399 --> 00:13:42,000
are just really susceptible to incorrect

350
00:13:40,480 --> 00:13:45,440
advice even though they're radiologists

351
00:13:42,000 --> 00:13:47,760
who have had over a decade of training.

352
00:13:45,440 --> 00:13:50,320
And finally, we know from other work

353
00:13:47,760 --> 00:13:53,279
that when you deliver biased advice to

354
00:13:50,320 --> 00:13:56,480
clinicians and nonclinians, the way that

355
00:13:53,279 --> 00:13:59,440
you couch the advice matters more than

356
00:13:56,480 --> 00:14:01,440
the actual content of the advice. So

357
00:13:59,440 --> 00:14:04,160
here we varied whether we had a good or

358
00:14:01,440 --> 00:14:05,880
bad AI give advice in a declarative

359
00:14:04,160 --> 00:14:08,800
prescriptive way, you should do

360
00:14:05,880 --> 00:14:11,600
something or in a suggestive informative

361
00:14:08,800 --> 00:14:13,760
way saying there's a feature uh in this

362
00:14:11,600 --> 00:14:15,440
uh data do what you will with it. And we

363
00:14:13,760 --> 00:14:17,519
found that when you told doctors to do

364
00:14:15,440 --> 00:14:19,279
the wrong thing in a way that was

365
00:14:17,519 --> 00:14:21,199
declarative, you should do this wrong

366
00:14:19,279 --> 00:14:22,720
thing, they listened and they did it.

367
00:14:21,199 --> 00:14:25,040
But when you told them to do the wrong

368
00:14:22,720 --> 00:14:26,480
thing in a way that was suggestive, they

369
00:14:25,040 --> 00:14:29,279
ignored it and retained their original

370
00:14:26,480 --> 00:14:31,120
fair decision-making. And so I I think

371
00:14:29,279 --> 00:14:32,720
thinking about all of these different

372
00:14:31,120 --> 00:14:34,160
contributions, there's a lot that we

373
00:14:32,720 --> 00:14:35,680
need to do to move forward with

374
00:14:34,160 --> 00:14:38,680
actionable AI and human health. Thank

375
00:14:35,680 --> 00:14:38,680
you.

376
00:14:41,040 --> 00:14:46,800
Thank you so much, Maza. questions from

377
00:14:43,480 --> 00:14:48,720
Azia. Go ahead. Thanks. Super

378
00:14:46,800 --> 00:14:51,680
interesting talk. Um I'm I'm wondering

379
00:14:48,720 --> 00:14:54,800
if you tried to correct the LLM somehow

380
00:14:51,680 --> 00:14:56,480
by asking it to say now consider the

381
00:14:54,800 --> 00:14:58,240
gender is different or something. Does

382
00:14:56,480 --> 00:14:59,839
it change its decision thereafter or

383
00:14:58,240 --> 00:15:01,639
something like some chain of reasoning

384
00:14:59,839 --> 00:15:04,000
sort of

385
00:15:01,639 --> 00:15:06,720
u you can fix a lot of these things

386
00:15:04,000 --> 00:15:09,120
right? Uh did you did you try that in

387
00:15:06,720 --> 00:15:11,519
this experiment? We didn't we didn't do

388
00:15:09,120 --> 00:15:13,519
any fine-tuning. In many other papers,

389
00:15:11,519 --> 00:15:17,519
not not just my group, many many groups

390
00:15:13,519 --> 00:15:19,760
have shown that if you do um fine-tuning

391
00:15:17,519 --> 00:15:21,680
on specific data that you have and you

392
00:15:19,760 --> 00:15:23,519
have a specific bias that you know

393
00:15:21,680 --> 00:15:25,519
exists and you want to correct it that

394
00:15:23,519 --> 00:15:27,440
you can address these biases within the

395
00:15:25,519 --> 00:15:30,399
large language model in those fields.

396
00:15:27,440 --> 00:15:32,480
The issue is you have to know it exists.

397
00:15:30,399 --> 00:15:34,320
You have to care enough to correct it

398
00:15:32,480 --> 00:15:36,399
and you have to correct it in a way that

399
00:15:34,320 --> 00:15:39,199
will be robust to differences in the way

400
00:15:36,399 --> 00:15:40,880
that the data is entered, annotated,

401
00:15:39,199 --> 00:15:44,560
abbreviated, all these other things,

402
00:15:40,880 --> 00:15:46,800
right? Um, nobody's doing that, right?

403
00:15:44,560 --> 00:15:48,959
So, because this is an administrative

404
00:15:46,800 --> 00:15:52,720
use of AI, it's not under the purview of

405
00:15:48,959 --> 00:15:55,279
the FDA. The FDA doesn't clear GPT4 to

406
00:15:52,720 --> 00:15:58,160
do this. It's just doing it. Open AAI

407
00:15:55,279 --> 00:16:00,320
explicitly says and has said publicly

408
00:15:58,160 --> 00:16:01,920
they are not redteameing these models

409
00:16:00,320 --> 00:16:03,759
before they go into the Boston area

410
00:16:01,920 --> 00:16:06,000
hospitals to make sure that they are for

411
00:16:03,759 --> 00:16:08,560
example genderneutral. They're not. It's

412
00:16:06,000 --> 00:16:10,399
not their job. And hospitals are

413
00:16:08,560 --> 00:16:12,959
therefore now responsible at an

414
00:16:10,399 --> 00:16:14,839
individual level for somehow having

415
00:16:12,959 --> 00:16:19,360
enough

416
00:16:14,839 --> 00:16:22,079
knowledge and time and money to red team

417
00:16:19,360 --> 00:16:24,160
all these models and not just for gender

418
00:16:22,079 --> 00:16:26,399
in these specific cases that we looked

419
00:16:24,160 --> 00:16:28,480
at but for every group that we know

420
00:16:26,399 --> 00:16:30,399
there has been historical and current

421
00:16:28,480 --> 00:16:33,000
bias towards in a healthcare setting.

422
00:16:30,399 --> 00:16:35,920
And so is it fixable hypothetically? Of

423
00:16:33,000 --> 00:16:38,320
course, people can fix these things to a

424
00:16:35,920 --> 00:16:41,800
certain uh degree. Is it being fixed in

425
00:16:38,320 --> 00:16:41,800
practice? No.

426
00:16:44,320 --> 00:16:48,519
So, obviously, there are a lot of things

427
00:16:46,480 --> 00:16:52,000
going wrong that you're

428
00:16:48,519 --> 00:16:54,079
documenting. If you were to start, where

429
00:16:52,000 --> 00:16:56,639
would you start to fix this in a

430
00:16:54,079 --> 00:16:59,920
clinical setting? like what are the the

431
00:16:56,639 --> 00:17:02,959
three things that need to happen and

432
00:16:59,920 --> 00:17:04,880
could happen in a relatively short time

433
00:17:02,959 --> 00:17:07,439
frame other than like you know having

434
00:17:04,880 --> 00:17:09,439
every hospital hire machine learning

435
00:17:07,439 --> 00:17:10,799
experts which doesn't seem I don't

436
00:17:09,439 --> 00:17:12,799
actually think any hospitals need

437
00:17:10,799 --> 00:17:17,880
machine learning uh experts. I think we

438
00:17:12,799 --> 00:17:21,199
need a new um secretary of HHS.

439
00:17:17,880 --> 00:17:25,199
Um like if you want quick fixes we could

440
00:17:21,199 --> 00:17:28,720
get rid of RFK. Uh, we could rehire our

441
00:17:25,199 --> 00:17:31,440
ARPAH director. We could rehire all of

442
00:17:28,720 --> 00:17:33,799
the FDA AI clearing people that exist.

443
00:17:31,440 --> 00:17:36,080
Like there are fixes. The fixes are

444
00:17:33,799 --> 00:17:38,400
regulatory. Nobody at the hospitals

445
00:17:36,080 --> 00:17:39,679
knows how to fix the MRI machines. Even

446
00:17:38,400 --> 00:17:40,559
if they tell you they do, they don't,

447
00:17:39,679 --> 00:17:42,480
right? Like they don't actually

448
00:17:40,559 --> 00:17:44,720
understand. They have a service contract

449
00:17:42,480 --> 00:17:46,240
with Seammens, right? And Seammens comes

450
00:17:44,720 --> 00:17:49,039
in once a year and they fix the

451
00:17:46,240 --> 00:17:50,400
technology. And there's ways for the MRI

452
00:17:49,039 --> 00:17:52,000
machine to autoc calibrate and say, "Oh,

453
00:17:50,400 --> 00:17:54,640
I'm out of calibration." like call a

454
00:17:52,000 --> 00:17:57,200
tech. Nobody expects the hospital to

455
00:17:54,640 --> 00:17:59,120
have like MRI techs like you know in the

456
00:17:57,200 --> 00:18:02,640
basement to appear like Batman. That's

457
00:17:59,120 --> 00:18:04,000
not a we why are hospitals hiring

458
00:18:02,640 --> 00:18:05,919
machine learning people to live in their

459
00:18:04,000 --> 00:18:07,520
basement? They do. Most hospitals in the

460
00:18:05,919 --> 00:18:10,480
Boston area have an ML team in their

461
00:18:07,520 --> 00:18:12,960
basement. Um that's not sustainable and

462
00:18:10,480 --> 00:18:15,360
also it's not something that is uh going

463
00:18:12,960 --> 00:18:17,360
to lead to like reliable outputs across

464
00:18:15,360 --> 00:18:19,360
our health care system. And so my

465
00:18:17,360 --> 00:18:21,760
personal belief is things like this are

466
00:18:19,360 --> 00:18:23,679
trivially identifiable as long as you're

467
00:18:21,760 --> 00:18:25,440
auditing for them and we're not right

468
00:18:23,679 --> 00:18:27,600
now. We're explicitly not doing that

469
00:18:25,440 --> 00:18:29,520
because there's a lack of sort of

470
00:18:27,600 --> 00:18:32,080
regulatory interest in doing so. In

471
00:18:29,520 --> 00:18:34,160
fact, the safe and reliable AI bill

472
00:18:32,080 --> 00:18:36,799
under the Biden administration that act

473
00:18:34,160 --> 00:18:38,880
was repealed right months into uh this

474
00:18:36,799 --> 00:18:41,840
administration and there uh has been a

475
00:18:38,880 --> 00:18:43,919
call uh request for information for a

476
00:18:41,840 --> 00:18:46,080
new a new thing to replace it. And the

477
00:18:43,919 --> 00:18:47,520
explicit instructions in the RFI were

478
00:18:46,080 --> 00:18:50,080
that they didn't want anything that was

479
00:18:47,520 --> 00:18:51,280
overly focused on social engineering.

480
00:18:50,080 --> 00:18:53,760
And so we don't know what's going to

481
00:18:51,280 --> 00:18:56,880
replace it. But you know, uh certainly I

482
00:18:53,760 --> 00:18:59,520
think what hospitals should do right now

483
00:18:56,880 --> 00:19:01,200
is to institute the practices that a

484
00:18:59,520 --> 00:19:02,799
regulatory body would have put on them

485
00:19:01,200 --> 00:19:04,720
in the first place, which is for example

486
00:19:02,799 --> 00:19:06,880
very strong audits of systems before

487
00:19:04,720 --> 00:19:08,280
they pay for them and are then liable

488
00:19:06,880 --> 00:19:14,120
for the damage they

489
00:19:08,280 --> 00:19:14,120
cause. Um great. So question over there.

490
00:19:19,960 --> 00:19:26,000
Okay. So what if we we shift this

491
00:19:22,720 --> 00:19:28,240
paradigm because the the LM or the the

492
00:19:26,000 --> 00:19:30,120
AI doesn't really create new things. It

493
00:19:28,240 --> 00:19:33,360
just fed back all the things they found

494
00:19:30,120 --> 00:19:35,440
available. So so if you see that the the

495
00:19:33,360 --> 00:19:37,760
AI is giving you this kind of bias

496
00:19:35,440 --> 00:19:40,400
things. It's actually reflecting what

497
00:19:37,760 --> 00:19:42,240
the real people are doing. So, so

498
00:19:40,400 --> 00:19:44,320
there's the real thing is that the

499
00:19:42,240 --> 00:19:46,080
people are going to do this and we're

500
00:19:44,320 --> 00:19:48,160
telling the machine don't do this. It's

501
00:19:46,080 --> 00:19:50,960
like do what I tell you to do, don't do

502
00:19:48,160 --> 00:19:53,280
what I did. So, this is like a different

503
00:19:50,960 --> 00:19:55,120
model to think about, right? So, so if

504
00:19:53,280 --> 00:19:56,720
we not saying that the AI model is

505
00:19:55,120 --> 00:19:59,039
perfect, we actually should come back

506
00:19:56,720 --> 00:20:01,679
and say that this AI is reflecting the

507
00:19:59,039 --> 00:20:03,679
world is not perfect. That's that's

508
00:20:01,679 --> 00:20:06,240
true. Yeah. AI models reflect the

509
00:20:03,679 --> 00:20:08,160
history, right? Uh often, you know,

510
00:20:06,240 --> 00:20:09,600
historically what has happened. I think

511
00:20:08,160 --> 00:20:11,679
we can all agree that our health care

512
00:20:09,600 --> 00:20:13,840
system uh not just ours but every health

513
00:20:11,679 --> 00:20:15,520
care system has been really bad

514
00:20:13,840 --> 00:20:18,080
historically especially for certain

515
00:20:15,520 --> 00:20:20,080
groups right and we don't want that uh

516
00:20:18,080 --> 00:20:22,160
those sort of uh behaviors calcified

517
00:20:20,080 --> 00:20:25,360
forever right so I totally agree with

518
00:20:22,160 --> 00:20:27,039
you um the way that every

519
00:20:25,360 --> 00:20:30,080
state-of-the-art machine learning model

520
00:20:27,039 --> 00:20:32,880
is trained these days is initially in an

521
00:20:30,080 --> 00:20:35,360
unsupervised way nobody's out there like

522
00:20:32,880 --> 00:20:37,360
labeling every data like sample in the

523
00:20:35,360 --> 00:20:39,440
world everything is unsupervised now

524
00:20:37,360 --> 00:20:41,120
Right? And what that means is you assume

525
00:20:39,440 --> 00:20:42,720
that every data point out there in the

526
00:20:41,120 --> 00:20:45,679
world is just as good as every other

527
00:20:42,720 --> 00:20:47,440
data point. Okay? So that means that

528
00:20:45,679 --> 00:20:49,760
every sample in the electronic

529
00:20:47,440 --> 00:20:52,320
healthcare record of a doctor when they

530
00:20:49,760 --> 00:20:55,600
were exhausted and made a mistake is

531
00:20:52,320 --> 00:20:56,960
valued the same as a sample from when

532
00:20:55,600 --> 00:20:58,440
they were not exhausted and did

533
00:20:56,960 --> 00:21:00,960
something really

534
00:20:58,440 --> 00:21:02,400
well. That's the paradigm that we are in

535
00:21:00,960 --> 00:21:04,240
right now. And I I think it's an

536
00:21:02,400 --> 00:21:06,480
incorrect paradigm to be in. And so are

537
00:21:04,240 --> 00:21:08,720
these reflective of bad processes? Yes,

538
00:21:06,480 --> 00:21:10,240
they are. The problem is if you go to a

539
00:21:08,720 --> 00:21:12,080
doctor and get a bad opinion, go to

540
00:21:10,240 --> 00:21:15,679
another doctor, get another second

541
00:21:12,080 --> 00:21:17,919
opinion, right? And by the uh law of,

542
00:21:15,679 --> 00:21:21,440
you know, every statistical uh theorem

543
00:21:17,919 --> 00:21:22,880
that exists, randomization is more fair,

544
00:21:21,440 --> 00:21:24,320
right? Like there's a there's a reason

545
00:21:22,880 --> 00:21:25,679
that we have multiple raiders for

546
00:21:24,320 --> 00:21:27,440
things. It's because my bias and your

547
00:21:25,679 --> 00:21:29,440
bias and there everybody's things cancel

548
00:21:27,440 --> 00:21:31,600
out, right? If you have multiple

549
00:21:29,440 --> 00:21:33,520
raiders, you're guaranteed a fairer

550
00:21:31,600 --> 00:21:35,120
system and you always have the ability

551
00:21:33,520 --> 00:21:37,039
to go to somebody else, right? I can't

552
00:21:35,120 --> 00:21:40,320
replicate my biases to every patient in

553
00:21:37,039 --> 00:21:42,000
the world. But if we have one AI system

554
00:21:40,320 --> 00:21:45,280
being used in every hospital in the

555
00:21:42,000 --> 00:21:47,520
Boston area, and we do, we have one set

556
00:21:45,280 --> 00:21:50,400
of calcified biases based on historical

557
00:21:47,520 --> 00:21:54,440
data that that company used. Calcified

558
00:21:50,400 --> 00:21:54,440
forever. No second opinion.

559
00:21:56,559 --> 00:22:01,039
Hi, thank you for this. Um, what kinds

560
00:21:59,039 --> 00:22:02,960
of products you just touched on it are

561
00:22:01,039 --> 00:22:04,720
you seeing these models show up in in

562
00:22:02,960 --> 00:22:06,799
hospitals? like what do the products

563
00:22:04,720 --> 00:22:09,039
look like? What kinds of things are they

564
00:22:06,799 --> 00:22:10,480
purchasing? They're everywhere. They uh

565
00:22:09,039 --> 00:22:14,480
schedule patient appointments. So, at

566
00:22:10,480 --> 00:22:16,480
UCSF, uh AI was used to do uh bookings

567
00:22:14,480 --> 00:22:17,919
and what it did was cancel every black

568
00:22:16,480 --> 00:22:19,600
patients appointment saying black

569
00:22:17,919 --> 00:22:22,240
patients don't show, so just cancel,

570
00:22:19,600 --> 00:22:24,159
double book them. Uh in the Boston area,

571
00:22:22,240 --> 00:22:26,640
they're used to create patient and

572
00:22:24,159 --> 00:22:29,200
provider message responses. And as you

573
00:22:26,640 --> 00:22:31,280
saw, when they create uh either patient

574
00:22:29,200 --> 00:22:33,679
messages or responses to questions,

575
00:22:31,280 --> 00:22:35,919
they're um more aggressive in removing

576
00:22:33,679 --> 00:22:37,919
care resources and care allocation from

577
00:22:35,919 --> 00:22:40,320
women. And other studies have shown from

578
00:22:37,919 --> 00:22:43,679
minorities. We're seeing AI being used

579
00:22:40,320 --> 00:22:45,880
to screen um for uh who should be

580
00:22:43,679 --> 00:22:48,080
allowed to get different kinds of

581
00:22:45,880 --> 00:22:50,080
treatments, who should be uh there's a

582
00:22:48,080 --> 00:22:53,039
lawsuit active in California right now

583
00:22:50,080 --> 00:22:54,640
about uh insurance, right? So uh there

584
00:22:53,039 --> 00:22:56,799
was an AI algorithm used by a large

585
00:22:54,640 --> 00:22:58,400
insurer that was used to decide for

586
00:22:56,799 --> 00:23:00,320
thousands of patients at a time whether

587
00:22:58,400 --> 00:23:02,159
they their claims would be covered under

588
00:23:00,320 --> 00:23:03,840
the insurance and it just denied

589
00:23:02,159 --> 00:23:05,280
thousands of people's claims at a time

590
00:23:03,840 --> 00:23:08,080
and said no we don't want to spend

591
00:23:05,280 --> 00:23:10,080
money. Um it's being used not just in

592
00:23:08,080 --> 00:23:12,000
health care it's being used uh in the

593
00:23:10,080 --> 00:23:14,559
criminal justice system in the education

594
00:23:12,000 --> 00:23:16,880
system. So, universities and primary

595
00:23:14,559 --> 00:23:19,200
schools now are using it to decide which

596
00:23:16,880 --> 00:23:20,600
kids are going to be really smart, who

597
00:23:19,200 --> 00:23:22,960
you should track into different

598
00:23:20,600 --> 00:23:25,039
programs, who you should recommend for

599
00:23:22,960 --> 00:23:27,200
different colleges. Police departments

600
00:23:25,039 --> 00:23:29,840
are using it to decide uh which

601
00:23:27,200 --> 00:23:31,679
neighborhoods to go into, which suspects

602
00:23:29,840 --> 00:23:34,480
to question. These are all like public

603
00:23:31,679 --> 00:23:37,440
record, right? All of these judges are

604
00:23:34,480 --> 00:23:39,039
being shown. I just heard about this in

605
00:23:37,440 --> 00:23:42,400
Massachusetts. Did you know that judges

606
00:23:39,039 --> 00:23:45,840
are being shown scans of people's

607
00:23:42,400 --> 00:23:47,919
brains, fMRI scans, and then there's AI

608
00:23:45,840 --> 00:23:50,120
software that will say, "This person's

609
00:23:47,919 --> 00:23:52,320
brain is a violent

610
00:23:50,120 --> 00:23:53,640
brain." There's a group of Massachusetts

611
00:23:52,320 --> 00:23:55,679
judges that was talking about this

612
00:23:53,640 --> 00:23:56,960
recently. This is a new thing in

613
00:23:55,679 --> 00:23:58,640
courtrooms. They'll just show you an

614
00:23:56,960 --> 00:23:59,840
fMRI scan of somebody's brain and

615
00:23:58,640 --> 00:24:01,480
they'll say, "This person's brain is

616
00:23:59,840 --> 00:24:04,640
violent. You can't let them out on the

617
00:24:01,480 --> 00:24:06,720
street. Give them a longer sentence."

618
00:24:04,640 --> 00:24:09,360
It's everywhere. And the problem is

619
00:24:06,720 --> 00:24:11,120
because it's not regulated and we do not

620
00:24:09,360 --> 00:24:12,720
have an AI bill of rights and now we

621
00:24:11,120 --> 00:24:15,919
don't even have the bill that we used to

622
00:24:12,720 --> 00:24:18,440
have that was about safe reliable AI,

623
00:24:15,919 --> 00:24:20,880
right? That was repealed under under

624
00:24:18,440 --> 00:24:23,039
administration. You have very few

625
00:24:20,880 --> 00:24:25,120
guarantees as a consumer that AI will

626
00:24:23,039 --> 00:24:30,919
not be used against you, right? Also,

627
00:24:25,120 --> 00:24:30,919
our FTC has been defanked, right? like

