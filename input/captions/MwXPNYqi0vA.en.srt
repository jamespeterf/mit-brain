1
00:00:00,120 --> 00:00:05,759
thank you very much for having me it's

2
00:00:02,360 --> 00:00:08,080
uh great to be here with you uh today

3
00:00:05,759 --> 00:00:09,639
just out of curiosity how many of you

4
00:00:08,080 --> 00:00:12,000
are interested in the business

5
00:00:09,639 --> 00:00:15,080
applications of

6
00:00:12,000 --> 00:00:17,640
AI and how many of you are interested in

7
00:00:15,080 --> 00:00:22,000
more technical aspects of

8
00:00:17,640 --> 00:00:23,320
AI perfect so now I know how to tune my

9
00:00:22,000 --> 00:00:26,519
uh

10
00:00:23,320 --> 00:00:29,439
presentations so um I want to talk to

11
00:00:26,519 --> 00:00:31,640
you about a new paradigm in AI that I

12
00:00:29,439 --> 00:00:33,960
call it perspective with where Ai and I

13
00:00:31,640 --> 00:00:37,559
want to tell you how I came up with that

14
00:00:33,960 --> 00:00:40,000
with my team I spent a lot of my career

15
00:00:37,559 --> 00:00:42,360
especially during my Graduate Studies to

16
00:00:40,000 --> 00:00:44,960
build these next Generation ubiquitous

17
00:00:42,360 --> 00:00:47,600
Computing applications you know from

18
00:00:44,960 --> 00:00:49,760
coffee machines that could sense the

19
00:00:47,600 --> 00:00:53,079
stress of the colleagues in an office

20
00:00:49,760 --> 00:00:56,760
and adjusted the espresso intensity to

21
00:00:53,079 --> 00:00:58,800
about 10 years ago to a sensory Paradigm

22
00:00:56,760 --> 00:01:01,640
that generated a comic strip about

23
00:00:58,800 --> 00:01:04,199
people's daily life this is way before

24
00:01:01,640 --> 00:01:07,759
uh the Advent of the language models

25
00:01:04,199 --> 00:01:11,119
figuring out tabletops augmented reality

26
00:01:07,759 --> 00:01:14,200
and many applications like that but what

27
00:01:11,119 --> 00:01:16,040
I became interested in was that well the

28
00:01:14,200 --> 00:01:18,320
Paradigm in which you build these

29
00:01:16,040 --> 00:01:20,759
applications are very mechanical there

30
00:01:18,320 --> 00:01:22,759
is no design pattern there's no process

31
00:01:20,759 --> 00:01:26,280
everyone came up with a new way of

32
00:01:22,759 --> 00:01:28,640
Designing these and they cannot scale at

33
00:01:26,280 --> 00:01:31,439
the same time we were watching in our

34
00:01:28,640 --> 00:01:33,399
team that things were moving from human

35
00:01:31,439 --> 00:01:35,880
computer interaction which is about the

36
00:01:33,399 --> 00:01:39,640
interaction of a person versus a cold

37
00:01:35,880 --> 00:01:42,280
silicon to AI systems that can really go

38
00:01:39,640 --> 00:01:43,920
into the background and enabling us to

39
00:01:42,280 --> 00:01:46,520
communicate with each other more

40
00:01:43,920 --> 00:01:49,479
effectively so that really became the

41
00:01:46,520 --> 00:01:51,360
focus area of our research group and

42
00:01:49,479 --> 00:01:54,799
some of the things that we spawn off

43
00:01:51,360 --> 00:01:58,439
later on as technology so what I want to

44
00:01:54,799 --> 00:02:01,280
talk to you about today is that how AI

45
00:01:58,439 --> 00:02:03,479
is helping us to move Beyond

46
00:02:01,280 --> 00:02:05,799
personalization I'm sure you all heard

47
00:02:03,479 --> 00:02:08,039
about this term putting a lot of data in

48
00:02:05,799 --> 00:02:10,319
one location process it and push things

49
00:02:08,039 --> 00:02:13,800
to people and maybe receive some

50
00:02:10,319 --> 00:02:16,640
analytics and data back into a model

51
00:02:13,800 --> 00:02:19,640
that a user can cocreate with the

52
00:02:16,640 --> 00:02:23,080
machine and really develop things

53
00:02:19,640 --> 00:02:25,840
collectively in a in a symbiogenetic way

54
00:02:23,080 --> 00:02:28,080
and also information can have

55
00:02:25,840 --> 00:02:31,160
perspectives one thing that is important

56
00:02:28,080 --> 00:02:33,160
to me when I'm at home as a father is

57
00:02:31,160 --> 00:02:36,360
different than when I'm at school

58
00:02:33,160 --> 00:02:37,879
teaching so how can you use AI to

59
00:02:36,360 --> 00:02:41,000
understand those

60
00:02:37,879 --> 00:02:44,080
perspectives so the goal of our group

61
00:02:41,000 --> 00:02:47,599
instead of building applications is to

62
00:02:44,080 --> 00:02:51,200
really enable and Empower others to have

63
00:02:47,599 --> 00:02:54,480
access to easy to ous intuitive tools to

64
00:02:51,200 --> 00:02:58,640
bring their creativity and build things

65
00:02:54,480 --> 00:03:00,599
themselves not everyone can be Patty ma

66
00:02:58,640 --> 00:03:03,040
it's very difficult because you really

67
00:03:00,599 --> 00:03:06,080
need to have a very cross-disciplinary

68
00:03:03,040 --> 00:03:08,920
view on things but what if we get to a

69
00:03:06,080 --> 00:03:10,840
point that we provide tools similar to

70
00:03:08,920 --> 00:03:12,920
what we did in the early internet days

71
00:03:10,840 --> 00:03:15,239
that first you had to code your HTML

72
00:03:12,920 --> 00:03:17,360
pages and you had to be an engineer to a

73
00:03:15,239 --> 00:03:19,080
model that you can now WordPress say oh

74
00:03:17,360 --> 00:03:20,959
I want a wedding website and a birthday

75
00:03:19,080 --> 00:03:23,360
website and put everything together

76
00:03:20,959 --> 00:03:26,360
using two-dimensional content what if we

77
00:03:23,360 --> 00:03:28,200
can do that with AI for everyone that's

78
00:03:26,360 --> 00:03:30,720
what I want to talk to you about

79
00:03:28,200 --> 00:03:33,599
today while we were working with our

80
00:03:30,720 --> 00:03:35,680
group we were basically realizing things

81
00:03:33,599 --> 00:03:37,959
were changing and things were not

82
00:03:35,680 --> 00:03:40,720
necessarily changing on the interface

83
00:03:37,959 --> 00:03:44,519
layer but things were changing when it

84
00:03:40,720 --> 00:03:46,319
comes to our relationship with data my

85
00:03:44,519 --> 00:03:48,640
dad in the 70s used to work for a

86
00:03:46,319 --> 00:03:50,319
company called Control Data Corporation

87
00:03:48,640 --> 00:03:53,000
how many of you heard about that company

88
00:03:50,319 --> 00:03:55,799
it was a Mainframe company it's very

89
00:03:53,000 --> 00:03:58,000
interesting that the term Control Data

90
00:03:55,799 --> 00:04:01,120
is becoming more and more relevant these

91
00:03:58,000 --> 00:04:03,680
days we are seeing that data is becoming

92
00:04:01,120 --> 00:04:07,079
more decentralized coming from Cobalt

93
00:04:03,680 --> 00:04:10,000
systems CRM systems file systems object

94
00:04:07,079 --> 00:04:12,239
systems and our literacy is growing so

95
00:04:10,000 --> 00:04:14,400
we want more transparency around data we

96
00:04:12,239 --> 00:04:17,720
want to understand when we feed data

97
00:04:14,400 --> 00:04:19,639
into algorithmics what happens to them

98
00:04:17,720 --> 00:04:21,519
we want to teach our kids about the

99
00:04:19,639 --> 00:04:23,759
privacy of their data that when they

100
00:04:21,519 --> 00:04:27,240
post something on Tik Tok or Instagram

101
00:04:23,759 --> 00:04:29,360
what happens to it how do models work

102
00:04:27,240 --> 00:04:32,240
and we are also seeing the importance of

103
00:04:29,360 --> 00:04:35,039
audit it many of us are interested in

104
00:04:32,240 --> 00:04:37,000
generative AI using Foundation models

105
00:04:35,039 --> 00:04:39,199
many of the foundation models that we

106
00:04:37,000 --> 00:04:42,199
use these days are black boxes there's

107
00:04:39,199 --> 00:04:44,120
no Providence there's no audit Trail and

108
00:04:42,199 --> 00:04:47,479
naturally there is a lot of garbage in

109
00:04:44,120 --> 00:04:49,919
garbage out you have used Reddit it's

110
00:04:47,479 --> 00:04:52,000
great we can continue to go from

111
00:04:49,919 --> 00:04:54,280
billions of parameters to trillion

112
00:04:52,000 --> 00:04:57,080
parameters but that's not going to solve

113
00:04:54,280 --> 00:04:59,000
the problem how do we use AI as a

114
00:04:57,080 --> 00:05:01,280
decision support system rather than a

115
00:04:59,000 --> 00:05:03,440
decision Mak taking system really

116
00:05:01,280 --> 00:05:05,160
putting a person in the middle so we can

117
00:05:03,440 --> 00:05:07,880
co-create with

118
00:05:05,160 --> 00:05:10,280
AI I want to share some examples with

119
00:05:07,880 --> 00:05:13,039
you before I go through some of the

120
00:05:10,280 --> 00:05:15,960
technicalities the goal of our team in

121
00:05:13,039 --> 00:05:19,000
perspective of R AI is allowing people

122
00:05:15,960 --> 00:05:22,639
to wear each other lenses and view the

123
00:05:19,000 --> 00:05:27,039
world through their view we have used it

124
00:05:22,639 --> 00:05:29,720
in space so when you send astronauts to

125
00:05:27,039 --> 00:05:31,759
uh on on the um on the inter

126
00:05:29,720 --> 00:05:33,840
International Space Station they have

127
00:05:31,759 --> 00:05:36,560
different cultural views and how they

128
00:05:33,840 --> 00:05:40,240
behave what if they can really see that

129
00:05:36,560 --> 00:05:43,240
space to each other's point of view um

130
00:05:40,240 --> 00:05:44,600
you may look at a menu at a restaurant

131
00:05:43,240 --> 00:05:47,440
and you want to see what your

132
00:05:44,600 --> 00:05:50,039
nutritional coach would eat when you

133
00:05:47,440 --> 00:05:52,960
look at that menu how do you develop

134
00:05:50,039 --> 00:05:55,880
these decentralized perspective aware

135
00:05:52,960 --> 00:05:57,639
systems our very first project tried to

136
00:05:55,880 --> 00:06:00,440
do that with an interface that you're

137
00:05:57,639 --> 00:06:03,319
very familiar with map-based interfaces

138
00:06:00,440 --> 00:06:05,759
so imagine that instead of you following

139
00:06:03,319 --> 00:06:08,599
cartisian coordinates on a Google map

140
00:06:05,759 --> 00:06:11,440
you can go to a city and say I want to

141
00:06:08,599 --> 00:06:13,639
wear a lens of a celebrity or a friend

142
00:06:11,440 --> 00:06:15,880
and I want to walk the streets of Paris

143
00:06:13,639 --> 00:06:18,199
through their point of view where they

144
00:06:15,880 --> 00:06:20,360
went what they ate what music they

145
00:06:18,199 --> 00:06:22,880
listened to so this is the

146
00:06:20,360 --> 00:06:24,319
decentralization of knowledge while you

147
00:06:22,880 --> 00:06:26,680
really have benefit from the

148
00:06:24,319 --> 00:06:28,840
centralization of computing power

149
00:06:26,680 --> 00:06:30,360
allowing you to generate these types of

150
00:06:28,840 --> 00:06:32,360
experiences

151
00:06:30,360 --> 00:06:35,759
so a lot of our work in our team is to

152
00:06:32,360 --> 00:06:40,280
work on these multimodal AI systems that

153
00:06:35,759 --> 00:06:43,080
you can process images cat files text of

154
00:06:40,280 --> 00:06:46,319
course turn them into vectors and turn

155
00:06:43,080 --> 00:06:48,759
them into graphs so the core uh research

156
00:06:46,319 --> 00:06:51,759
Focus we have is on this notion of

157
00:06:48,759 --> 00:06:54,120
Reason ready AI using graphs we

158
00:06:51,759 --> 00:06:56,160
published papers around that but now we

159
00:06:54,120 --> 00:06:58,039
are demonstrating that these reason

160
00:06:56,160 --> 00:07:00,800
Readiness is very important in the

161
00:06:58,039 --> 00:07:03,080
future of AI In classical computer

162
00:07:00,800 --> 00:07:04,919
science departments people who worked on

163
00:07:03,080 --> 00:07:06,639
neural networks really didn't like the

164
00:07:04,919 --> 00:07:09,000
people who were working on ontologies

165
00:07:06,639 --> 00:07:10,479
and semantics the great thing about the

166
00:07:09,000 --> 00:07:12,680
media lab is that we are very

167
00:07:10,479 --> 00:07:15,240
cross-disciplinary we're bringing all of

168
00:07:12,680 --> 00:07:17,960
that together and simplifying it for

169
00:07:15,240 --> 00:07:20,840
people to use so we introduced these

170
00:07:17,960 --> 00:07:24,120
Concepts called Chronicles Chronicle is

171
00:07:20,840 --> 00:07:26,960
a graph that represents your behavior

172
00:07:24,120 --> 00:07:30,080
and as you go about your daily life it

173
00:07:26,960 --> 00:07:32,440
gets better and better I highlight these

174
00:07:30,080 --> 00:07:35,800
are not data these are causalities and

175
00:07:32,440 --> 00:07:38,520
relationships on behavioral patterns so

176
00:07:35,800 --> 00:07:40,440
everyone will use these ontologies we

177
00:07:38,520 --> 00:07:41,960
generate these graphs about them the

178
00:07:40,440 --> 00:07:44,120
colar codes represent different

179
00:07:41,960 --> 00:07:46,680
attributes of social ontologies for

180
00:07:44,120 --> 00:07:49,199
example how do I behave as a professor

181
00:07:46,680 --> 00:07:52,360
in school how do I behave as a father at

182
00:07:49,199 --> 00:07:54,360
home what do I do when I play tennis all

183
00:07:52,360 --> 00:07:56,479
of those will be recorded in these

184
00:07:54,360 --> 00:07:58,879
graphs using structures that are

185
00:07:56,479 --> 00:08:01,759
queriable and identifiable so that has

186
00:07:58,879 --> 00:08:03,759
been a lot focus in our research group

187
00:08:01,759 --> 00:08:06,400
now let me show you some

188
00:08:03,759 --> 00:08:08,759
examples we created a language with our

189
00:08:06,400 --> 00:08:11,240
team called I generate you have used

190
00:08:08,759 --> 00:08:13,960
Twitter now X I'm assuming it has a

191
00:08:11,240 --> 00:08:16,080
syntax you have handles and hashtags we

192
00:08:13,960 --> 00:08:19,199
now know how to use it so we have

193
00:08:16,080 --> 00:08:22,000
created a syntax called I generate I

194
00:08:19,199 --> 00:08:25,000
generate a fine-tune model from a base

195
00:08:22,000 --> 00:08:27,319
model that I have and I want to map it

196
00:08:25,000 --> 00:08:29,520
or put it as a lens on a different type

197
00:08:27,319 --> 00:08:31,120
of an information to filter in

198
00:08:29,520 --> 00:08:35,320
information accordingly so let me give

199
00:08:31,120 --> 00:08:40,479
you an an example I want to generate an

200
00:08:35,320 --> 00:08:42,760
experience of going to Paris in Winter I

201
00:08:40,479 --> 00:08:45,839
put it in a parenthesis instead of a

202
00:08:42,760 --> 00:08:47,800
hashtag I will get the available

203
00:08:45,839 --> 00:08:49,480
Chronicles that I have access to these

204
00:08:47,800 --> 00:08:51,680
are people that I know people who have

205
00:08:49,480 --> 00:08:55,320
been to Paris through my trusted Social

206
00:08:51,680 --> 00:08:57,839
Network I generated a graph I do scene

207
00:08:55,320 --> 00:08:59,839
generation and when I go to that City

208
00:08:57,839 --> 00:09:01,760
instead of relying on Central Iz

209
00:08:59,839 --> 00:09:04,680
recommendation systems like Google

210
00:09:01,760 --> 00:09:06,279
Places and Trip Advisor I can now walk

211
00:09:04,680 --> 00:09:08,760
the streets of Paris through the lands

212
00:09:06,279 --> 00:09:11,200
of my friend now what is the

213
00:09:08,760 --> 00:09:13,200
technicality behind it we partner with a

214
00:09:11,200 --> 00:09:14,720
very large credit card company that

215
00:09:13,200 --> 00:09:16,519
allow people to share their

216
00:09:14,720 --> 00:09:19,079
transactional information with each

217
00:09:16,519 --> 00:09:21,560
other on the St level and on the

218
00:09:19,079 --> 00:09:24,160
category level really building a

219
00:09:21,560 --> 00:09:26,399
peer-to-peer Learning Network for

220
00:09:24,160 --> 00:09:28,720
tourism and then we applied our

221
00:09:26,399 --> 00:09:31,440
technology to demonstrate what these

222
00:09:28,720 --> 00:09:35,120
navigational tools for cities can look

223
00:09:31,440 --> 00:09:38,040
like we then applied this to the work

224
00:09:35,120 --> 00:09:40,160
environment uh I'm not a big fan of uh

225
00:09:38,040 --> 00:09:41,519
work from home sorry to say that I don't

226
00:09:40,160 --> 00:09:44,240
I know there are lots of people who

227
00:09:41,519 --> 00:09:45,839
disagree with me but sometimes you don't

228
00:09:44,240 --> 00:09:48,560
have a choice so with our team we

229
00:09:45,839 --> 00:09:51,320
decided to create a digital twin of our

230
00:09:48,560 --> 00:09:53,560
startup the video you see on the left is

231
00:09:51,320 --> 00:09:55,360
our office and the video that you see on

232
00:09:53,560 --> 00:09:57,000
the right hand side is the digital twin

233
00:09:55,360 --> 00:09:59,760
version of that that we have done with

234
00:09:57,000 --> 00:10:01,959
gausson splatting and liar so we now

235
00:09:59,760 --> 00:10:04,440
have techniques that you can map a space

236
00:10:01,959 --> 00:10:06,519
very quickly is a tool that we created

237
00:10:04,440 --> 00:10:09,200
in our group called open Dome I don't

238
00:10:06,519 --> 00:10:12,560
want to call it a metaverse it's a dome

239
00:10:09,200 --> 00:10:14,519
of pixels connected to open data and

240
00:10:12,560 --> 00:10:16,160
then we retrieved information about

241
00:10:14,519 --> 00:10:19,200
colleagues who wanted to share that

242
00:10:16,160 --> 00:10:22,920
payroll tax project management sales you

243
00:10:19,200 --> 00:10:25,959
know uh tickets for it and map that into

244
00:10:22,920 --> 00:10:28,480
the office so now when someone comes

245
00:10:25,959 --> 00:10:30,800
into the office whether they use a

246
00:10:28,480 --> 00:10:33,920
headset or they use a web interface

247
00:10:30,800 --> 00:10:37,279
using webgl they can just say I want to

248
00:10:33,920 --> 00:10:40,480
look at the office through the lens of

249
00:10:37,279 --> 00:10:42,720
maybe the CEO today what does the CEO

250
00:10:40,480 --> 00:10:46,560
think about today what are the

251
00:10:42,720 --> 00:10:49,360
priorities so now you can navigate the

252
00:10:46,560 --> 00:10:51,079
space and retrieve information in a

253
00:10:49,360 --> 00:10:53,079
decentralized way through project

254
00:10:51,079 --> 00:10:55,320
management systems file systems tax

255
00:10:53,079 --> 00:10:57,920
systems to really have a different

256
00:10:55,320 --> 00:11:00,079
perspective in the office and now I can

257
00:10:57,920 --> 00:11:02,240
switch my lens from this CEO to let's

258
00:11:00,079 --> 00:11:05,480
say the ESG specialist how are we doing

259
00:11:02,240 --> 00:11:07,399
in terms of energy consumption we just

260
00:11:05,480 --> 00:11:09,639
partnered with a major company that they

261
00:11:07,399 --> 00:11:13,399
are now using this for onboarding their

262
00:11:09,639 --> 00:11:15,480
new Talent their interns their co-ops so

263
00:11:13,399 --> 00:11:17,560
when you start you get a much better

264
00:11:15,480 --> 00:11:19,240
onboarding and understanding of your

265
00:11:17,560 --> 00:11:21,800
colleagues and you're measuring as part

266
00:11:19,240 --> 00:11:24,120
of a research paper how quickly we can

267
00:11:21,800 --> 00:11:27,920
activate that

268
00:11:24,120 --> 00:11:29,880
onboarding um a number of my students

269
00:11:27,920 --> 00:11:31,639
Master students are apply this to

270
00:11:29,880 --> 00:11:34,680
nutrition they are very interested in

271
00:11:31,639 --> 00:11:37,959
the intersection of spatial Computing

272
00:11:34,680 --> 00:11:40,480
and uh gen so this is a case in which

273
00:11:37,959 --> 00:11:42,279
you retrieve an information from online

274
00:11:40,480 --> 00:11:45,440
they are all athletes they are working

275
00:11:42,279 --> 00:11:49,120
with a US Triathlon gold medalist who's

276
00:11:45,440 --> 00:11:51,720
an expert in recovery and as you viewing

277
00:11:49,120 --> 00:11:54,079
that article online you can activate

278
00:11:51,720 --> 00:11:56,800
friends and then ask them what do you

279
00:11:54,079 --> 00:11:59,560
think about this particular subject in

280
00:11:56,800 --> 00:12:02,000
this case recovery and very quick l

281
00:11:59,560 --> 00:12:03,680
through information retrieval and using

282
00:12:02,000 --> 00:12:06,959
some of the protocols that that we

283
00:12:03,680 --> 00:12:09,360
created you gain their perspective about

284
00:12:06,959 --> 00:12:12,880
that subject so instead of AI making the

285
00:12:09,360 --> 00:12:14,800
decision for me I am now Consulting with

286
00:12:12,880 --> 00:12:17,240
people through their available knowledge

287
00:12:14,800 --> 00:12:20,560
graphs to me which is privacy preserved

288
00:12:17,240 --> 00:12:22,480
and I can develop a less biased opinion

289
00:12:20,560 --> 00:12:24,360
and if it's a we actually want an award

290
00:12:22,480 --> 00:12:26,880
from Stanford Stanford democracy

291
00:12:24,360 --> 00:12:29,399
challenge award by applying this to us

292
00:12:26,880 --> 00:12:31,600
politics we actually demonstrated that

293
00:12:29,399 --> 00:12:33,199
the two ends of the spectrum can

294
00:12:31,600 --> 00:12:35,560
understand each other more

295
00:12:33,199 --> 00:12:38,440
effectively now everything that I have

296
00:12:35,560 --> 00:12:40,519
shown you up until now are what my

297
00:12:38,440 --> 00:12:43,560
colleagues say at the media lab screen

298
00:12:40,519 --> 00:12:46,959
ai ai that generates pixel on a

299
00:12:43,560 --> 00:12:49,839
two-dimensional screen this can also go

300
00:12:46,959 --> 00:12:51,920
beyond that screen so I'll give you an

301
00:12:49,839 --> 00:12:53,720
example this was my co project I had a

302
00:12:51,920 --> 00:12:56,079
lot of extra time that's my younger

303
00:12:53,720 --> 00:12:58,440
daughter Sierra which is interacting

304
00:12:56,079 --> 00:13:00,279
with a robot this robot is relatively

305
00:12:58,440 --> 00:13:02,360
old but it's a powerful robot so you

306
00:13:00,279 --> 00:13:05,320
could inject these Chronicles into the

307
00:13:02,360 --> 00:13:07,560
robot allowing the kid to interact with

308
00:13:05,320 --> 00:13:10,000
the robot through some type of a value

309
00:13:07,560 --> 00:13:12,480
system that you incorporate in those

310
00:13:10,000 --> 00:13:15,920
Chronicles and it's multimodal so she

311
00:13:12,480 --> 00:13:17,959
was showing a a a a painting uh talking

312
00:13:15,920 --> 00:13:19,959
about it and using the multimodal

313
00:13:17,959 --> 00:13:22,880
systems that I talked about the

314
00:13:19,959 --> 00:13:24,600
chronicle got updated and a paper that

315
00:13:22,880 --> 00:13:27,079
we wrote about this is that in some

316
00:13:24,600 --> 00:13:29,959
cases the kid is more comfortable

317
00:13:27,079 --> 00:13:32,160
sharing and conversing with the robot

318
00:13:29,959 --> 00:13:33,920
than their parents so we are basically

319
00:13:32,160 --> 00:13:36,519
measuring what are those context

320
00:13:33,920 --> 00:13:38,199
elements in which a robot in this case

321
00:13:36,519 --> 00:13:40,600
will be more

322
00:13:38,199 --> 00:13:42,839
useful one of the big grants that we

323
00:13:40,600 --> 00:13:45,800
recently secured is using this to

324
00:13:42,839 --> 00:13:47,639
demonstrate the future of work the graph

325
00:13:45,800 --> 00:13:50,399
on the right hand side is very familiar

326
00:13:47,639 --> 00:13:52,720
to all of you in an organization project

327
00:13:50,399 --> 00:13:55,160
comes in all right based on budget

328
00:13:52,720 --> 00:13:57,600
availability calendar let's bring people

329
00:13:55,160 --> 00:13:59,959
together and let's develop that uh that

330
00:13:57,600 --> 00:14:03,120
project and deliver it what we are

331
00:13:59,959 --> 00:14:05,320
proving in this model is can actually

332
00:14:03,120 --> 00:14:08,160
our Chronicles work with each other in

333
00:14:05,320 --> 00:14:11,160
an organization to develop the first

334
00:14:08,160 --> 00:14:13,360
part of that task so we can all go to

335
00:14:11,160 --> 00:14:16,079
work but instead of doing kind of basic

336
00:14:13,360 --> 00:14:18,480
incremental work we now incorporate that

337
00:14:16,079 --> 00:14:21,440
knowledge in our Chronicles so this is

338
00:14:18,480 --> 00:14:23,680
where auditability contracts which data

339
00:14:21,440 --> 00:14:26,240
is available to whom that's a lot of our

340
00:14:23,680 --> 00:14:29,680
work at the human dynamics group becomes

341
00:14:26,240 --> 00:14:32,560
very important because AI in this is not

342
00:14:29,680 --> 00:14:34,959
replacing our job is basically allowing

343
00:14:32,560 --> 00:14:37,399
us to go about our daily life and learn

344
00:14:34,959 --> 00:14:39,800
and meet with people but our Chronicle

345
00:14:37,399 --> 00:14:41,639
gets better and better and better and we

346
00:14:39,800 --> 00:14:43,839
are demonstrating that things let's say

347
00:14:41,639 --> 00:14:47,160
in a creative agency or in a project

348
00:14:43,839 --> 00:14:47,160
that requires some templatized

349
00:14:54,120 --> 00:14:58,720
now I want to spend a bit of on that and

350
00:14:57,040 --> 00:15:00,600
I want to talk about our ecosystem

351
00:14:58,720 --> 00:15:01,680
approach at the media lab which is very

352
00:15:00,600 --> 00:15:05,320
very

353
00:15:01,680 --> 00:15:07,560
unique in our group we separate the

354
00:15:05,320 --> 00:15:10,199
channel or the interface from the

355
00:15:07,560 --> 00:15:11,959
infrastructure and the algorithmics the

356
00:15:10,199 --> 00:15:13,959
interface can be a conversational

357
00:15:11,959 --> 00:15:16,959
interface could be a web page could be a

358
00:15:13,959 --> 00:15:20,320
prompt-based interface all the way to

359
00:15:16,959 --> 00:15:22,320
what we call synthetic characters these

360
00:15:20,320 --> 00:15:24,680
synthetic characters are becoming more

361
00:15:22,320 --> 00:15:27,279
and more advanced if you're from Germany

362
00:15:24,680 --> 00:15:31,880
this is a very famous uh reporter from

363
00:15:27,279 --> 00:15:34,040
De he came to our lab we mapped his face

364
00:15:31,880 --> 00:15:36,680
we used all of the things that he has

365
00:15:34,040 --> 00:15:40,319
done in Germany into a Knowledge Graph

366
00:15:36,680 --> 00:15:42,319
FedEd into uh the synthetic character

367
00:15:40,319 --> 00:15:45,000
and part of a documentary that you can

368
00:15:42,319 --> 00:15:46,720
watch it's called My Avatar and me I

369
00:15:45,000 --> 00:15:50,079
could basically converse with the

370
00:15:46,720 --> 00:15:52,800
synthetic character instead of uh the

371
00:15:50,079 --> 00:15:54,720
reporter now there are lots of these

372
00:15:52,800 --> 00:15:57,000
things available many of them to be

373
00:15:54,720 --> 00:15:59,680
honest are gimmicks like why would I

374
00:15:57,000 --> 00:16:02,279
want to use this so one of our key focus

375
00:15:59,680 --> 00:16:04,199
is that what is the business application

376
00:16:02,279 --> 00:16:05,959
of these things could it be a professor

377
00:16:04,199 --> 00:16:08,639
could it be someone that people really

378
00:16:05,959 --> 00:16:10,720
want to talk to but they don't have

379
00:16:08,639 --> 00:16:13,880
access imagine you go on the media lab

380
00:16:10,720 --> 00:16:16,399
website even now you see the pictures of

381
00:16:13,880 --> 00:16:19,600
luminaries doing great work what if

382
00:16:16,399 --> 00:16:22,680
instead of a JPEG pixel and a bio you

383
00:16:19,600 --> 00:16:25,800
can actually talk to that professor and

384
00:16:22,680 --> 00:16:28,120
prompt information with control and

385
00:16:25,800 --> 00:16:31,279
governance that could be the future of

386
00:16:28,120 --> 00:16:33,399
digital channels think about a800 number

387
00:16:31,279 --> 00:16:35,800
oh you know wait 10 minutes and someone

388
00:16:33,399 --> 00:16:37,519
picks up activate a character and talk

389
00:16:35,800 --> 00:16:40,839
to them that character already knows a

390
00:16:37,519 --> 00:16:42,920
lot about you through multimodal uh

391
00:16:40,839 --> 00:16:45,079
interfaces so we are building this in

392
00:16:42,920 --> 00:16:47,000
our lab uh we are working this is

393
00:16:45,079 --> 00:16:50,000
another grant that we got to build this

394
00:16:47,000 --> 00:16:51,839
capture systems in our uh in our lab you

395
00:16:50,000 --> 00:16:54,360
can walk into it so we are building

396
00:16:51,839 --> 00:16:57,120
these Pathways you walk into this within

397
00:16:54,360 --> 00:16:59,560
10 minutes we create your digital

398
00:16:57,120 --> 00:17:00,920
version depending on what you want to do

399
00:16:59,560 --> 00:17:02,839
what your company want to do or

400
00:17:00,920 --> 00:17:05,319
individually you can feed your graphs

401
00:17:02,839 --> 00:17:08,480
into your synthetic characters and then

402
00:17:05,319 --> 00:17:10,919
you can um create graphs so I don't want

403
00:17:08,480 --> 00:17:13,120
to take you through a lot of details but

404
00:17:10,919 --> 00:17:16,160
you can use patents you can use

405
00:17:13,120 --> 00:17:18,240
interviews so um this is an example of

406
00:17:16,160 --> 00:17:21,000
someone who came to our lab gave me

407
00:17:18,240 --> 00:17:23,520
access to some of her family uh albums

408
00:17:21,000 --> 00:17:27,280
and things so that we could create her

409
00:17:23,520 --> 00:17:29,640
synthetic characters and then um you

410
00:17:27,280 --> 00:17:31,840
know any information can be fed into

411
00:17:29,640 --> 00:17:33,799
these systems to create these knowledge

412
00:17:31,840 --> 00:17:36,400
graphs we do embeddings and

413
00:17:33,799 --> 00:17:38,280
vectorization from them and there are

414
00:17:36,400 --> 00:17:40,600
papers on this that are published that I

415
00:17:38,280 --> 00:17:43,640
can share with you but to keep you on

416
00:17:40,600 --> 00:17:44,520
time um I'm going to share some examples

417
00:17:43,640 --> 00:17:47,120
with

418
00:17:44,520 --> 00:17:48,960
you there were some very good questions

419
00:17:47,120 --> 00:17:52,280
that were asked in the previous panel

420
00:17:48,960 --> 00:17:55,120
well where is the bias how do I trust

421
00:17:52,280 --> 00:17:57,039
this right how do I know that what this

422
00:17:55,120 --> 00:17:59,880
character is telling me is actually

423
00:17:57,039 --> 00:18:03,480
accurate so this is actually a platform

424
00:17:59,880 --> 00:18:05,600
that we built that you can now ask a

425
00:18:03,480 --> 00:18:09,200
prompt essentially ask a question from

426
00:18:05,600 --> 00:18:10,799
an individual I hope I have audio if not

427
00:18:09,200 --> 00:18:14,039
we'll see what

428
00:18:10,799 --> 00:18:16,320
happens um you prompt and you can

429
00:18:14,039 --> 00:18:18,120
activate different characters and ask

430
00:18:16,320 --> 00:18:21,080
them the same question to see what they

431
00:18:18,120 --> 00:18:23,799
talk about uh my audio is not connected

432
00:18:21,080 --> 00:18:26,760
but this the person is actually

433
00:18:23,799 --> 00:18:28,600
talking based on the prompt the relevant

434
00:18:26,760 --> 00:18:30,720
nodes of the graphs are being captured

435
00:18:28,600 --> 00:18:32,919
that actually our core IP and the

436
00:18:30,720 --> 00:18:36,280
algorithmics that our team uh created

437
00:18:32,919 --> 00:18:38,480
but look at that reliability dashboard

438
00:18:36,280 --> 00:18:41,480
that is basically giving you the score

439
00:18:38,480 --> 00:18:44,400
that although I'm giving you a response

440
00:18:41,480 --> 00:18:46,760
my confidence level on this answer is

441
00:18:44,400 --> 00:18:49,600
low it's up to you if you want to

442
00:18:46,760 --> 00:18:51,720
believe it or not but that's basically

443
00:18:49,600 --> 00:18:54,200
the these are all AI techniques that

444
00:18:51,720 --> 00:18:55,919
give you confidence lift yield those are

445
00:18:54,200 --> 00:18:58,280
techniques that we can use to see do I

446
00:18:55,919 --> 00:19:01,480
have Providence is it open is it being

447
00:18:58,280 --> 00:19:03,520
validated how many people used it so and

448
00:19:01,480 --> 00:19:06,120
then we are mapping these based on

449
00:19:03,520 --> 00:19:08,000
fine-tune expressions of an individual

450
00:19:06,120 --> 00:19:10,600
so in this particular case you could not

451
00:19:08,000 --> 00:19:12,840
hear the audio on areas of research

452
00:19:10,600 --> 00:19:16,280
interest that I have when that comes up

453
00:19:12,840 --> 00:19:18,159
my face becomes happier or sad so think

454
00:19:16,280 --> 00:19:21,440
about where the future of digital

455
00:19:18,159 --> 00:19:23,880
channels uh can go this is a famous

456
00:19:21,440 --> 00:19:26,640
reporter that I'm working on so she's

457
00:19:23,880 --> 00:19:30,120
very big in female empowerment we

458
00:19:26,640 --> 00:19:33,200
created a whole set of uh uh um social

459
00:19:30,120 --> 00:19:35,159
graphs from her and people can talk to

460
00:19:33,200 --> 00:19:37,240
her about different elements of the

461
00:19:35,159 --> 00:19:39,720
business and instead of going and

462
00:19:37,240 --> 00:19:42,559
reading a lot of blogs you now have a

463
00:19:39,720 --> 00:19:45,120
much better more intuitive interactions

464
00:19:42,559 --> 00:19:46,840
uh with her and the interesting thing

465
00:19:45,120 --> 00:19:49,799
about this which surprised us this is my

466
00:19:46,840 --> 00:19:51,600
PhD student originally from Germany a

467
00:19:49,799 --> 00:19:54,080
fantastic student he actually was the

468
00:19:51,600 --> 00:19:58,760
VFX director of Harry

469
00:19:54,080 --> 00:20:01,440
Potter we grafted all of his papers but

470
00:19:58,760 --> 00:20:04,000
for some reason the AI picked you could

471
00:20:01,440 --> 00:20:06,960
not hear the audio the AI picked his

472
00:20:04,000 --> 00:20:09,280
German accent so the synthetic voice

473
00:20:06,960 --> 00:20:12,400
that is coming out actually has a German

474
00:20:09,280 --> 00:20:14,799
accent we did not incorporate that as a

475
00:20:12,400 --> 00:20:18,360
magic number or a magic attribute the AI

476
00:20:14,799 --> 00:20:21,240
picked it so now you need to create more

477
00:20:18,360 --> 00:20:23,320
dashboards for people not to block

478
00:20:21,240 --> 00:20:25,960
things people say biases are bad in AI

479
00:20:23,320 --> 00:20:28,480
biases are actually good we have a case

480
00:20:25,960 --> 00:20:31,320
with the Mount Sinai Hospital in Toronto

481
00:20:28,480 --> 00:20:34,799
that a doctor can consult three other

482
00:20:31,320 --> 00:20:37,480
doctors about a medical dosier we need

483
00:20:34,799 --> 00:20:39,880
their biases there we just need to tell

484
00:20:37,480 --> 00:20:42,400
them why this individual thinks like

485
00:20:39,880 --> 00:20:44,120
that so those are some of the techniques

486
00:20:42,400 --> 00:20:45,760
that we are building in our tool we have

487
00:20:44,120 --> 00:20:49,760
a there is a documentary you can watch

488
00:20:45,760 --> 00:20:54,000
on HBO uh I did it with deac Chopra and

489
00:20:49,760 --> 00:20:56,159
a famous uh movie director an Shin we

490
00:20:54,000 --> 00:20:59,480
mapped a lot of their data we created an

491
00:20:56,159 --> 00:21:01,880
avatar but then the lady brought her

492
00:20:59,480 --> 00:21:04,720
kids in front of the mom's

493
00:21:01,880 --> 00:21:07,679
Avatar and you know what it didn't work

494
00:21:04,720 --> 00:21:09,520
out because the technology was amazing

495
00:21:07,679 --> 00:21:11,760
you could impress people and say this is

496
00:21:09,520 --> 00:21:14,720
a gimmicky cool things but the kids

497
00:21:11,760 --> 00:21:18,120
hated it the kids really didn't like it

498
00:21:14,720 --> 00:21:21,120
and that was an aha moment for our team

499
00:21:18,120 --> 00:21:24,520
because this was not a technology

500
00:21:21,120 --> 00:21:26,120
problem we needed to study empathy we

501
00:21:24,520 --> 00:21:28,600
need to really turn this into

502
00:21:26,120 --> 00:21:30,919
storytelling machines rather than into

503
00:21:28,600 --> 00:21:33,039
Interventional systems and then we are

504
00:21:30,919 --> 00:21:35,240
doing a paper that we are looking at all

505
00:21:33,039 --> 00:21:38,480
the state-of-the-art models and they

506
00:21:35,240 --> 00:21:40,799
perform terribly in terms of identifying

507
00:21:38,480 --> 00:21:43,080
emotion and empathy and others on these

508
00:21:40,799 --> 00:21:46,200
research papers so that's basically

509
00:21:43,080 --> 00:21:49,080
driving our focus and I'm now working

510
00:21:46,200 --> 00:21:51,360
with others like religion experts uh

511
00:21:49,080 --> 00:21:53,720
Divinity experts to get their knowledge

512
00:21:51,360 --> 00:21:55,919
and applying it into our model fine tune

513
00:21:53,720 --> 00:21:57,919
accordingly so hopefully some of those

514
00:21:55,919 --> 00:21:59,799
things go away I don't have a lot of

515
00:21:57,919 --> 00:22:01,520
time to talk about these but I want to

516
00:21:59,799 --> 00:22:04,520
talk a little bit about the media lab

517
00:22:01,520 --> 00:22:07,080
before um I

518
00:22:04,520 --> 00:22:10,200
wrap the unique thing about the media

519
00:22:07,080 --> 00:22:13,080
lab is that you can be all of these

520
00:22:10,200 --> 00:22:14,799
things in one person like you can really

521
00:22:13,080 --> 00:22:18,520
look at it through the business angle

522
00:22:14,799 --> 00:22:20,279
core research angle scale angle and one

523
00:22:18,520 --> 00:22:22,760
thing that we really believe in is that

524
00:22:20,279 --> 00:22:25,559
we need to run this as an ecology and an

525
00:22:22,760 --> 00:22:28,600
ecosystem we need to bring students from

526
00:22:25,559 --> 00:22:30,559
design and systems and mechanical

527
00:22:28,600 --> 00:22:32,760
engineering and fashion all

528
00:22:30,559 --> 00:22:35,640
collaborating uh with each other so I

529
00:22:32,760 --> 00:22:37,840
will wrap by saying this this is the

530
00:22:35,640 --> 00:22:40,840
Paradigm that many of us follow in in

531
00:22:37,840 --> 00:22:43,720
our group if you are bringing AI into a

532
00:22:40,840 --> 00:22:46,559
company you can follow these five AI is

533
00:22:43,720 --> 00:22:49,120
no longer going to be a business a an IT

534
00:22:46,559 --> 00:22:51,559
strategy it's going to be a horizontal

535
00:22:49,120 --> 00:22:53,520
enabler in your organization that even

536
00:22:51,559 --> 00:22:56,200
the boards of directors should be

537
00:22:53,520 --> 00:22:59,200
involved you really need to double down

538
00:22:56,200 --> 00:23:01,679
on the AI literacy imperative from from

539
00:22:59,200 --> 00:23:04,360
K12 all the way to the boards of

540
00:23:01,679 --> 00:23:06,880
directors and tune so when my colleagues

541
00:23:04,360 --> 00:23:09,279
talked about the expro educational

542
00:23:06,880 --> 00:23:11,559
program that is a great step but you

543
00:23:09,279 --> 00:23:14,080
need a lot of that and tune it and

544
00:23:11,559 --> 00:23:15,039
personalize it for different people the

545
00:23:14,080 --> 00:23:17,720
data

546
00:23:15,039 --> 00:23:19,440
imperative the mindset of the industry

547
00:23:17,720 --> 00:23:21,679
right now is a mindset of a graduate

548
00:23:19,440 --> 00:23:23,440
student when it comes to data Professor

549
00:23:21,679 --> 00:23:25,000
give me the data and I'll do my work

550
00:23:23,440 --> 00:23:26,799
that thing does not exist in the

551
00:23:25,000 --> 00:23:28,919
industry you should not give up your

552
00:23:26,799 --> 00:23:31,720
data you should really double down on

553
00:23:28,919 --> 00:23:33,600
the portability privacy audit mechanics

554
00:23:31,720 --> 00:23:36,480
of moving the data around and the last

555
00:23:33,600 --> 00:23:39,799
thing I say that Patty mentioned design

556
00:23:36,480 --> 00:23:42,039
design design in the old days you could

557
00:23:39,799 --> 00:23:43,960
basically bring focus groups and design

558
00:23:42,039 --> 00:23:46,559
low Fidelity and High Fidelity and

559
00:23:43,960 --> 00:23:48,880
launch a channel you're talking about a

560
00:23:46,559 --> 00:23:50,799
time that you're throwing terabytes of

561
00:23:48,880 --> 00:23:52,840
data and channels especially if you're a

562
00:23:50,799 --> 00:23:55,960
regulated industry or emission critical

563
00:23:52,840 --> 00:23:58,880
industry say oh show something useful

564
00:23:55,960 --> 00:24:02,000
you really need to change your uh design

565
00:23:58,880 --> 00:24:04,000
patterns so with that uh I want to wrap

566
00:24:02,000 --> 00:24:06,159
I know I'm time I would love to stay in

567
00:24:04,000 --> 00:24:08,320
touch with you I know some of you may be

568
00:24:06,159 --> 00:24:11,600
visiting the lab it's a very special

569
00:24:08,320 --> 00:24:14,520
place uh I'm happy to staying in touch

570
00:24:11,600 --> 00:24:17,760
and uh thank you for joining thank you

571
00:24:14,520 --> 00:24:17,760
very much

