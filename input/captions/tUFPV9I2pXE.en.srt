1
00:00:00,160 --> 00:00:03,679
Hi everyone. My name is Michelle Lee and

2
00:00:02,320 --> 00:00:06,240
I'm the director of the Master of

3
00:00:03,679 --> 00:00:09,200
Business Analytics program. I'm joined

4
00:00:06,240 --> 00:00:11,920
today with associate professor Swati

5
00:00:09,200 --> 00:00:14,880
Gupta um who will talk about her

6
00:00:11,920 --> 00:00:16,480
background and her journey here at MIT.

7
00:00:14,880 --> 00:00:18,320
Welcome Swati. Would you like to

8
00:00:16,480 --> 00:00:20,400
introduce yourself? Thank you Michelle.

9
00:00:18,320 --> 00:00:22,160
Uh it's my pleasure to be here and to

10
00:00:20,400 --> 00:00:24,320
talk to all of you. My background is in

11
00:00:22,160 --> 00:00:26,480
computer science and mathematics and I'm

12
00:00:24,320 --> 00:00:28,800
a professor of operations research and

13
00:00:26,480 --> 00:00:31,760
statistics at Sloan School of Management

14
00:00:28,800 --> 00:00:34,239
and I did my undergrad at IIT Delhi in

15
00:00:31,760 --> 00:00:36,399
India. Uh I majored in computer science

16
00:00:34,239 --> 00:00:38,879
but I always had this tension of you

17
00:00:36,399 --> 00:00:42,160
know I I loved math and then I wanted to

18
00:00:38,879 --> 00:00:44,640
do CS. So I was a CS major but I took

19
00:00:42,160 --> 00:00:47,920
all maths courses and then when I came

20
00:00:44,640 --> 00:00:50,719
for a PhD to the US I uh found my place

21
00:00:47,920 --> 00:00:52,879
in operations research which is like the

22
00:00:50,719 --> 00:00:55,199
right combination of theory and

23
00:00:52,879 --> 00:00:57,760
algorithms and actually making an impact

24
00:00:55,199 --> 00:01:00,239
in practice. So a lot of my work these

25
00:00:57,760 --> 00:01:02,480
days uh is on machine learning and

26
00:01:00,239 --> 00:01:05,040
optimization and AI all the cool stuff

27
00:01:02,480 --> 00:01:07,920
but we also look at various applications

28
00:01:05,040 --> 00:01:11,040
where we can have a direct impact on uh

29
00:01:07,920 --> 00:01:12,799
lives of people. we can uh help society.

30
00:01:11,040 --> 00:01:15,439
We care about making ethical and

31
00:01:12,799 --> 00:01:17,040
algorithmically fair decisions and we

32
00:01:15,439 --> 00:01:19,759
think about all of this in the changing

33
00:01:17,040 --> 00:01:21,200
landscape of various laws and policies

34
00:01:19,759 --> 00:01:22,960
and how they interact with the

35
00:01:21,200 --> 00:01:25,360
algorithms that we have in the world

36
00:01:22,960 --> 00:01:27,840
around us today. Very cool. Can you tell

37
00:01:25,360 --> 00:01:32,000
us a little bit about your most uh

38
00:01:27,840 --> 00:01:34,479
recent research and what your research

39
00:01:32,000 --> 00:01:37,200
concluded? Recently I've been very

40
00:01:34,479 --> 00:01:39,520
curious about a lot of topics ranging

41
00:01:37,200 --> 00:01:41,280
from how do we allocate organs

42
00:01:39,520 --> 00:01:42,720
ethically. Some of these new

43
00:01:41,280 --> 00:01:44,479
technologies have to do with

44
00:01:42,720 --> 00:01:46,880
cryopreservation and xenor grafted

45
00:01:44,479 --> 00:01:48,960
organs. So we've been thinking about you

46
00:01:46,880 --> 00:01:50,640
know how do we assess which patients

47
00:01:48,960 --> 00:01:53,360
need organs the most? what is going to

48
00:01:50,640 --> 00:01:55,600
be the benefit of these new technologies

49
00:01:53,360 --> 00:01:58,960
and how can we actually develop that

50
00:01:55,600 --> 00:02:01,759
supply chain uh logistic system around

51
00:01:58,960 --> 00:02:04,000
uh the space to quickly get the organs

52
00:02:01,759 --> 00:02:06,560
before their quality deteriorates. Wow.

53
00:02:04,000 --> 00:02:08,959
I've also been thinking about uh how do

54
00:02:06,560 --> 00:02:11,760
we make admissions and hiring processes

55
00:02:08,959 --> 00:02:13,520
more e easier, right? Like uh throughout

56
00:02:11,760 --> 00:02:17,120
my life, I was trained to look at

57
00:02:13,520 --> 00:02:19,360
numbers and take them at face value. But

58
00:02:17,120 --> 00:02:22,400
uh what I've learned in the last few

59
00:02:19,360 --> 00:02:24,239
years is that numbers only reflect an

60
00:02:22,400 --> 00:02:26,319
approximate quantity that we've been

61
00:02:24,239 --> 00:02:29,040
able to measure and there's a lot of

62
00:02:26,319 --> 00:02:30,879
noise in our measurement of individuals.

63
00:02:29,040 --> 00:02:33,120
So there's not a number that I can put

64
00:02:30,879 --> 00:02:36,080
on everybody and say that this person

65
00:02:33,120 --> 00:02:38,560
ranks above this other person. So some

66
00:02:36,080 --> 00:02:40,800
of my work recently has been also on

67
00:02:38,560 --> 00:02:43,040
understanding how do we design hiring

68
00:02:40,800 --> 00:02:45,840
algorithms that give people the benefit

69
00:02:43,040 --> 00:02:48,319
of the doubt that allow interview

70
00:02:45,840 --> 00:02:51,599
committees to select people that show

71
00:02:48,319 --> 00:02:53,599
the progress perhaps that we don't have

72
00:02:51,599 --> 00:02:55,760
exact measurements on all of them but we

73
00:02:53,599 --> 00:02:57,840
don't penalize them for not having that

74
00:02:55,760 --> 00:02:59,519
those exact measurements. And the same

75
00:02:57,840 --> 00:03:02,159
kind of thinking goes into admissions

76
00:02:59,519 --> 00:03:04,879
processes as well where students who

77
00:03:02,159 --> 00:03:06,800
let's say you know have um they have

78
00:03:04,879 --> 00:03:09,519
better financial resources they can take

79
00:03:06,800 --> 00:03:11,360
exams like the SATs and the GREs again

80
00:03:09,519 --> 00:03:14,000
and they can improve the scores and they

81
00:03:11,360 --> 00:03:15,840
can report the top performance. However,

82
00:03:14,000 --> 00:03:17,680
what about students who don't have the

83
00:03:15,840 --> 00:03:21,120
means to take these exams over and over

84
00:03:17,680 --> 00:03:23,760
again and we have a noisier estimate on

85
00:03:21,120 --> 00:03:25,440
the quality of these students. So my

86
00:03:23,760 --> 00:03:28,879
research has been around, you know, how

87
00:03:25,440 --> 00:03:31,280
do we still account for that noise in a

88
00:03:28,879 --> 00:03:33,360
way that's fair to everybody that gives

89
00:03:31,280 --> 00:03:35,920
people the benefit of the doubt, but

90
00:03:33,360 --> 00:03:38,159
ultimately all of these problems are

91
00:03:35,920 --> 00:03:41,360
about allocating a limited amount of

92
00:03:38,159 --> 00:03:44,480
resources to a diverse population. Wow.

93
00:03:41,360 --> 00:03:46,799
And we see these ideas also go into

94
00:03:44,480 --> 00:03:49,040
generative AI and LLMs and some of the

95
00:03:46,799 --> 00:03:52,239
new uh AI advances that we're seeing

96
00:03:49,040 --> 00:03:54,000
these days. when an LLM trains on uh

97
00:03:52,239 --> 00:03:56,400
certain objectives or it's trying to

98
00:03:54,000 --> 00:03:58,480
find policies or it's trying to find

99
00:03:56,400 --> 00:04:00,560
types of solutions that you know let's

100
00:03:58,480 --> 00:04:02,480
say a call center should call this

101
00:04:00,560 --> 00:04:05,519
cohort of people to have the maximum

102
00:04:02,480 --> 00:04:07,920
impact a human decision maker can then

103
00:04:05,519 --> 00:04:09,680
go back and tell the LLM that no you

104
00:04:07,920 --> 00:04:12,080
know I want to have more diversity in

105
00:04:09,680 --> 00:04:13,920
the number of people I call and perhaps

106
00:04:12,080 --> 00:04:16,880
you know I want to make sure that we

107
00:04:13,920 --> 00:04:18,880
call u let's say expectant mothers who

108
00:04:16,880 --> 00:04:22,040
would really benefit from certain

109
00:04:18,880 --> 00:04:24,560
information during their pregnancy.

110
00:04:22,040 --> 00:04:26,160
But another human decision maker could

111
00:04:24,560 --> 00:04:28,000
come and say, well, you know, I really

112
00:04:26,160 --> 00:04:30,400
care about giving that information to

113
00:04:28,000 --> 00:04:32,639
uneducated people because they would

114
00:04:30,400 --> 00:04:34,560
really value, they would really uh

115
00:04:32,639 --> 00:04:37,759
benefit from such information. And so

116
00:04:34,560 --> 00:04:40,479
again, we are in this space of having

117
00:04:37,759 --> 00:04:42,800
constrained resources. We have a small

118
00:04:40,479 --> 00:04:46,080
number of callers in a call center that

119
00:04:42,800 --> 00:04:48,400
might call a diverse set of uh expectant

120
00:04:46,080 --> 00:04:50,880
mothers. And the question again is who

121
00:04:48,400 --> 00:04:52,560
should we prioritize uh based on the

122
00:04:50,880 --> 00:04:55,199
need and the benefit that they will have

123
00:04:52,560 --> 00:04:57,360
from these limited resources. Wow,

124
00:04:55,199 --> 00:05:00,000
that's amazing. You're doing so much.

125
00:04:57,360 --> 00:05:03,520
It's from call centers to admissions to

126
00:05:00,000 --> 00:05:06,080
organ donations. Um what if a student

127
00:05:03,520 --> 00:05:08,479
out there in the audience would like to

128
00:05:06,080 --> 00:05:11,199
work with you on your research? How do

129
00:05:08,479 --> 00:05:13,360
you go about um finding students to do

130
00:05:11,199 --> 00:05:15,600
research with? My metric for finding

131
00:05:13,360 --> 00:05:18,000
students and I've worked with a lot of

132
00:05:15,600 --> 00:05:20,560
students from very diverse backgrounds

133
00:05:18,000 --> 00:05:22,720
is the first thing I'm looking for is

134
00:05:20,560 --> 00:05:24,240
passion for their research. So if you're

135
00:05:22,720 --> 00:05:25,680
passionate about something and you're

136
00:05:24,240 --> 00:05:28,160
passionate about a problem or you're

137
00:05:25,680 --> 00:05:30,800
passionate about a theoretical question

138
00:05:28,160 --> 00:05:32,880
then that passion basically allows you

139
00:05:30,800 --> 00:05:35,039
and makes that space for you to grow in

140
00:05:32,880 --> 00:05:38,400
that area to think creatively about new

141
00:05:35,039 --> 00:05:40,160
solutions and to really learn and then

142
00:05:38,400 --> 00:05:42,080
after passion I think from a very

143
00:05:40,160 --> 00:05:44,400
practical perspective to immediately

144
00:05:42,080 --> 00:05:47,680
make impact on research I look for

145
00:05:44,400 --> 00:05:49,759
course preparation so I look for uh you

146
00:05:47,680 --> 00:05:52,160
know strong mathematical preparation and

147
00:05:49,759 --> 00:05:54,960
courses because you know in today's work

148
00:05:52,160 --> 00:05:57,680
to make these algorithms scale and be

149
00:05:54,960 --> 00:06:00,080
robust and reliable, we need to develop

150
00:05:57,680 --> 00:06:02,639
mathematical guarantees. So we cannot

151
00:06:00,080 --> 00:06:04,880
have huristics where we don't know how

152
00:06:02,639 --> 00:06:07,199
they're going to perform on unseen data

153
00:06:04,880 --> 00:06:09,919
but we want to be able to have

154
00:06:07,199 --> 00:06:12,479
robustness or reliability guarantees and

155
00:06:09,919 --> 00:06:16,240
for that I look for students who have

156
00:06:12,479 --> 00:06:18,800
some uh friendship with mathematics who

157
00:06:16,240 --> 00:06:21,360
have taken like uh you know some logic

158
00:06:18,800 --> 00:06:23,759
courses or proof courses you know some

159
00:06:21,360 --> 00:06:25,759
people who can even prototype things

160
00:06:23,759 --> 00:06:28,080
very fast so they can test their ideas

161
00:06:25,759 --> 00:06:30,240
they can code up things fast and that's

162
00:06:28,080 --> 00:06:32,880
what the MBAN program also equips them

163
00:06:30,240 --> 00:06:35,199
to do. So I actually work with a lot of

164
00:06:32,880 --> 00:06:37,280
MBAN students and these students have

165
00:06:35,199 --> 00:06:39,039
come from either they have more of a

166
00:06:37,280 --> 00:06:41,600
machine learning experience, they have

167
00:06:39,039 --> 00:06:43,520
some background with data analytics. So

168
00:06:41,600 --> 00:06:45,280
they can bring that into the projects

169
00:06:43,520 --> 00:06:47,759
and they can work with the data, they

170
00:06:45,280 --> 00:06:49,759
can estimate uncertainties and biases,

171
00:06:47,759 --> 00:06:51,680
they can develop algorithms and they can

172
00:06:49,759 --> 00:06:53,840
implement and see the impact.

173
00:06:51,680 --> 00:06:56,000
Some students that I work with are

174
00:06:53,840 --> 00:06:58,240
really come from like this mathematical

175
00:06:56,000 --> 00:07:00,080
perspective where they're able to look

176
00:06:58,240 --> 00:07:03,120
at the mathematical model and prove

177
00:07:00,080 --> 00:07:05,199
guarantees about it. And so I've had uh

178
00:07:03,120 --> 00:07:07,440
a lot of fun working with students

179
00:07:05,199 --> 00:07:10,160
across these trends and see them

180
00:07:07,440 --> 00:07:12,400
contribute to projects. Wonderful. And I

181
00:07:10,160 --> 00:07:14,000
know you're also an adviser for capstone

182
00:07:12,400 --> 00:07:16,000
projects. Can you tell us a little bit

183
00:07:14,000 --> 00:07:17,680
about the capstone projects that you're

184
00:07:16,000 --> 00:07:19,919
working with or have worked with in the

185
00:07:17,680 --> 00:07:21,919
past? So capstone project experience was

186
00:07:19,919 --> 00:07:24,800
a fantastic experience that I was

187
00:07:21,919 --> 00:07:26,720
basically exposed to at MIT and I really

188
00:07:24,800 --> 00:07:29,360
enjoyed working with capstone companies

189
00:07:26,720 --> 00:07:31,199
in the last uh couple of years. So I've

190
00:07:29,360 --> 00:07:34,759
worked with Accenture, I've worked with

191
00:07:31,199 --> 00:07:37,520
Cognira and Comcast and uh

192
00:07:34,759 --> 00:07:40,800
Accenture started with a very open-ended

193
00:07:37,520 --> 00:07:42,800
project which I thought was uh a a big

194
00:07:40,800 --> 00:07:45,120
challenge for the students. So it was

195
00:07:42,800 --> 00:07:47,280
not something that was very precisely

196
00:07:45,120 --> 00:07:49,919
defined. The project was about

197
00:07:47,280 --> 00:07:52,639
quantifying the impact of generative AI

198
00:07:49,919 --> 00:07:55,280
and how aggressive companies need to be

199
00:07:52,639 --> 00:07:57,680
in their investment in generative AI to

200
00:07:55,280 --> 00:08:00,479
have a competitive edge uh over their

201
00:07:57,680 --> 00:08:03,080
competitors. And I thought this was uh

202
00:08:00,479 --> 00:08:05,360
you know JVI being a new uh AI

203
00:08:03,080 --> 00:08:06,879
advancement we were kind of in this in

204
00:08:05,360 --> 00:08:09,199
the space that we don't have any

205
00:08:06,879 --> 00:08:10,800
benchmark how do we kind of uh say that

206
00:08:09,199 --> 00:08:14,240
this uh this is going to be the impact

207
00:08:10,800 --> 00:08:15,520
of this new uh set of tools right uh I

208
00:08:14,240 --> 00:08:18,000
thought this was a very open-ended

209
00:08:15,520 --> 00:08:21,520
project but what we could ultimately

210
00:08:18,000 --> 00:08:24,240
develop from that was assess using a

211
00:08:21,520 --> 00:08:27,120
very involved markup decision process

212
00:08:24,240 --> 00:08:29,120
the students developed a tool where any

213
00:08:27,120 --> 00:08:30,879
company could put in their parameter s

214
00:08:29,120 --> 00:08:32,719
and then see what is the growth of the

215
00:08:30,879 --> 00:08:35,279
company in the next 5 years or 10 years

216
00:08:32,719 --> 00:08:37,919
in a planning horizon and they could

217
00:08:35,279 --> 00:08:40,560
compare with the market and see you know

218
00:08:37,919 --> 00:08:42,640
how aggressive the generative AI uh

219
00:08:40,560 --> 00:08:44,880
investment needs to be. I thought that

220
00:08:42,640 --> 00:08:47,680
was a very cool project. My other

221
00:08:44,880 --> 00:08:51,519
project was with Cognira and Cognira is

222
00:08:47,680 --> 00:08:54,640
a retail uh analytics uh company and uh

223
00:08:51,519 --> 00:08:57,760
they've gone from kind of this space of

224
00:08:54,640 --> 00:08:59,440
um understanding pricing strategies and

225
00:08:57,760 --> 00:09:02,240
they go all the way to supply chain

226
00:08:59,440 --> 00:09:03,959
management and so this project was uh

227
00:09:02,240 --> 00:09:06,480
about

228
00:09:03,959 --> 00:09:08,959
estimating impact of promotions of

229
00:09:06,480 --> 00:09:11,760
different kinds on the sales of

230
00:09:08,959 --> 00:09:14,880
different products. M so if you promote

231
00:09:11,760 --> 00:09:17,600
let's say pasta and pasta sauce together

232
00:09:14,880 --> 00:09:20,399
then the sales of both of these products

233
00:09:17,600 --> 00:09:23,279
will go up. If you promote core products

234
00:09:20,399 --> 00:09:25,839
then maybe Pepsi product sales go down

235
00:09:23,279 --> 00:09:27,440
and so one promotion leads to a loss of

236
00:09:25,839 --> 00:09:30,560
sales in the other product and so these

237
00:09:27,440 --> 00:09:33,519
are cannibalization effects and this is

238
00:09:30,560 --> 00:09:35,360
a very challenging mathematical question

239
00:09:33,519 --> 00:09:37,680
that when you have signals that are

240
00:09:35,360 --> 00:09:39,680
mixed because of promotions in one

241
00:09:37,680 --> 00:09:42,320
quantity you have mixed signals in the

242
00:09:39,680 --> 00:09:45,519
others in terms of their baseline sales.

243
00:09:42,320 --> 00:09:48,000
How do you estimate that impact? And so

244
00:09:45,519 --> 00:09:50,160
our team of students and uh the team

245
00:09:48,000 --> 00:09:52,480
from Kmira, we all put our brains

246
00:09:50,160 --> 00:09:55,000
together and we came up with this uh

247
00:09:52,480 --> 00:09:57,120
very interesting market normalization

248
00:09:55,000 --> 00:10:01,120
baseline above which we could kind of

249
00:09:57,120 --> 00:10:02,880
estimate these impacts uh with a higher

250
00:10:01,120 --> 00:10:04,480
fidelity and there were discussions

251
00:10:02,880 --> 00:10:05,839
about you know converting that into a

252
00:10:04,480 --> 00:10:07,920
pattern or something that the company

253
00:10:05,839 --> 00:10:10,080
could really convert into its bottom

254
00:10:07,920 --> 00:10:12,320
line and I thought that was very cool

255
00:10:10,080 --> 00:10:13,920
because you know students went from

256
00:10:12,320 --> 00:10:15,200
defining the project getting the data

257
00:10:13,920 --> 00:10:17,279
implemented ing something and showing

258
00:10:15,200 --> 00:10:19,440
something that was not done before in a

259
00:10:17,279 --> 00:10:21,839
very short span of time. Yeah. Our

260
00:10:19,440 --> 00:10:24,399
projects last only about six to seven

261
00:10:21,839 --> 00:10:26,320
months, right? So they have to very uh

262
00:10:24,399 --> 00:10:28,800
work really hard, very quickly ramp up

263
00:10:26,320 --> 00:10:31,440
and provide results and so these

264
00:10:28,800 --> 00:10:33,839
students also got um if I'm not mistaken

265
00:10:31,440 --> 00:10:36,240
pre-placement offers from these firms.

266
00:10:33,839 --> 00:10:37,519
Yes, that is the goal. Yeah. Great.

267
00:10:36,240 --> 00:10:39,200
Well, thank you so much for your time

268
00:10:37,519 --> 00:10:41,120
today. My pleasure. We look forward to

269
00:10:39,200 --> 00:10:43,360
seeing what you'll do next. Thank you.

270
00:10:41,120 --> 00:10:45,519
And uh we'll see everyone very soon. Hi

271
00:10:43,360 --> 00:10:47,279
everyone, thank you for watching. If you

272
00:10:45,519 --> 00:10:49,839
like this video, check out our other

273
00:10:47,279 --> 00:10:52,959
videos in the series where I interview

274
00:10:49,839 --> 00:10:54,079
influential professors at MIT Sloan. If

275
00:10:52,959 --> 00:10:56,000
you would like to learn more about the

276
00:10:54,079 --> 00:10:58,320
Master Business Analytics program, we

277
00:10:56,000 --> 00:10:59,760
also have a playlist where you can see

278
00:10:58,320 --> 00:11:02,399
more videos about the admissions

279
00:10:59,760 --> 00:11:05,040
process, student life, and the program

280
00:11:02,399 --> 00:11:05,040
overall.

