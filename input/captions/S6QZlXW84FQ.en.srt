1
00:00:14,719 --> 00:00:20,080
Welcome to the AI and open education

2
00:00:17,760 --> 00:00:22,000
initiative speaker series. My name is

3
00:00:20,080 --> 00:00:24,080
Sher Seagull and I am the collaborations

4
00:00:22,000 --> 00:00:26,320
and engagement manager at MIT Open

5
00:00:24,080 --> 00:00:28,400
Courseware. I'm also one of the planning

6
00:00:26,320 --> 00:00:30,560
committee members for the AI and open

7
00:00:28,400 --> 00:00:33,040
education initiative at MIT open

8
00:00:30,560 --> 00:00:34,559
learning in order to meet the moment of

9
00:00:33,040 --> 00:00:37,040
remarkable growth of artificial

10
00:00:34,559 --> 00:00:39,040
intelligence and to examine its benefits

11
00:00:37,040 --> 00:00:41,760
and challenges to the field of open

12
00:00:39,040 --> 00:00:44,399
education. Last summer, MIT Open

13
00:00:41,760 --> 00:00:46,719
Learning solicited rapid response papers

14
00:00:44,399 --> 00:00:48,800
from stakeholders around the world to

15
00:00:46,719 --> 00:00:51,680
articulate how generative AI might

16
00:00:48,800 --> 00:00:54,559
accelerate or hinder the promise of open

17
00:00:51,680 --> 00:00:57,760
education. An international jury of AI

18
00:00:54,559 --> 00:01:00,079
and open education experts reviewed 147

19
00:00:57,760 --> 00:01:02,239
submissions and published nine

20
00:01:00,079 --> 00:01:05,040
thought-provoking papers by 32

21
00:01:02,239 --> 00:01:06,880
researchers on the open platform PubPub

22
00:01:05,040 --> 00:01:10,320
which you can find via the link in the

23
00:01:06,880 --> 00:01:12,880
chat. The initiative and today's speaker

24
00:01:10,320 --> 00:01:15,439
series webinar is generously funded by

25
00:01:12,880 --> 00:01:17,360
the William and Flora Hulet Foundation.

26
00:01:15,439 --> 00:01:20,000
We want to thank Hulet for their ongoing

27
00:01:17,360 --> 00:01:21,840
support. Today's session is the second

28
00:01:20,000 --> 00:01:24,000
of two webinars in the initiative

29
00:01:21,840 --> 00:01:26,080
speaker series. Our first webinar topic

30
00:01:24,000 --> 00:01:28,960
was professional education and the

31
00:01:26,080 --> 00:01:33,439
judicious use of AI. Today's webinar

32
00:01:28,960 --> 00:01:35,520
topic is AI literacies and evaluation.

33
00:01:33,439 --> 00:01:37,520
An overview of today's structure is as

34
00:01:35,520 --> 00:01:39,759
follows. First, I will briefly introduce

35
00:01:37,520 --> 00:01:42,640
our speakers. Then our respondent will

36
00:01:39,759 --> 00:01:44,560
contextualize this moment for us. Third,

37
00:01:42,640 --> 00:01:46,720
select co-authors of the two spotlighted

38
00:01:44,560 --> 00:01:48,799
papers will provide a highle overview of

39
00:01:46,720 --> 00:01:50,479
each of their papers for less than 10

40
00:01:48,799 --> 00:01:52,320
minutes each. And this will be followed

41
00:01:50,479 --> 00:01:54,479
by a discussion with the authors led by

42
00:01:52,320 --> 00:01:55,920
our respondent. And finally, we will

43
00:01:54,479 --> 00:01:59,159
open the discussion for your comments

44
00:01:55,920 --> 00:01:59,159
and questions.

45
00:02:00,560 --> 00:02:04,880
I'm thrilled to introduce today's

46
00:02:02,159 --> 00:02:06,640
speakers. Nick Baker is the digital

47
00:02:04,880 --> 00:02:08,560
learning specialist and former director

48
00:02:06,640 --> 00:02:10,959
of the office of open learning at the

49
00:02:08,560 --> 00:02:13,920
University of Windsor. and he is our

50
00:02:10,959 --> 00:02:16,400
respondent today. Nick was a founding

51
00:02:13,920 --> 00:02:17,920
board member of eCampus Ontario, a

52
00:02:16,400 --> 00:02:20,000
government-f funed nonprofit

53
00:02:17,920 --> 00:02:21,280
organization that provides support for

54
00:02:20,000 --> 00:02:22,879
digital teaching and learning,

55
00:02:21,280 --> 00:02:24,959
educational technology, micro

56
00:02:22,879 --> 00:02:26,640
credentials, and open education for the

57
00:02:24,959 --> 00:02:29,040
publicly assisted post-secondary

58
00:02:26,640 --> 00:02:30,319
institutions of Ontario, Canada. He

59
00:02:29,040 --> 00:02:32,160
works at the intersection of open

60
00:02:30,319 --> 00:02:34,400
educational practices, AI, and

61
00:02:32,160 --> 00:02:36,959
accessibility. His work also entails

62
00:02:34,400 --> 00:02:38,640
higher ed policy and change management.

63
00:02:36,959 --> 00:02:40,239
Some of his publications are in the chat

64
00:02:38,640 --> 00:02:42,239
for you.

65
00:02:40,239 --> 00:02:45,519
Angela Gundar and one of the co-authors

66
00:02:42,239 --> 00:02:48,640
of today's two spotlighted papers is the

67
00:02:45,519 --> 00:02:50,800
CEO and founder of Opened Culture. She's

68
00:02:48,640 --> 00:02:52,560
previously the the chief academic

69
00:02:50,800 --> 00:02:55,200
officer of the online learning

70
00:02:52,560 --> 00:02:56,879
consortium. She's also online faculty at

71
00:02:55,200 --> 00:02:58,800
the University of Arizona's College of

72
00:02:56,879 --> 00:03:00,640
Information Science and she teaches

73
00:02:58,800 --> 00:03:03,120
courses in new media, instructional

74
00:03:00,640 --> 00:03:05,200
design, game design, and AI. Her

75
00:03:03,120 --> 00:03:06,959
research focuses on open remix

76
00:03:05,200 --> 00:03:08,879
practices, open culture, digital

77
00:03:06,959 --> 00:03:10,720
literacies, narrative digital learning

78
00:03:08,879 --> 00:03:13,280
practices, and emerging technology for

79
00:03:10,720 --> 00:03:15,200
language acquisition.

80
00:03:13,280 --> 00:03:16,959
Josh Herren is the director of

81
00:03:15,200 --> 00:03:18,879
professional learning at the online

82
00:03:16,959 --> 00:03:21,920
learning consortium. He is a researcher

83
00:03:18,879 --> 00:03:23,680
with opened culture and a doctorate in

84
00:03:21,920 --> 00:03:25,280
information design from Clemson

85
00:03:23,680 --> 00:03:27,840
University where he also teaches

86
00:03:25,280 --> 00:03:29,519
interdisciplinary courses. His research

87
00:03:27,840 --> 00:03:31,200
and work is largely in new media,

88
00:03:29,519 --> 00:03:32,640
instructional design, and digital

89
00:03:31,200 --> 00:03:34,879
leadership.

90
00:03:32,640 --> 00:03:37,519
The featured papers for today's webinar

91
00:03:34,879 --> 00:03:40,000
are in the chat now for you. Josh and

92
00:03:37,519 --> 00:03:43,120
Angela's paper is titled AI Literacies

93
00:03:40,000 --> 00:03:45,920
and the Advancement of Opened Cultures.

94
00:03:43,120 --> 00:03:49,200
Our next two co-authors are Hannah Beth

95
00:03:45,920 --> 00:03:51,680
Clark uh from the Oak National Academy

96
00:03:49,200 --> 00:03:53,680
in the UK. She's a teacher and senior

97
00:03:51,680 --> 00:03:56,239
leader in London Secondary Schools for

98
00:03:53,680 --> 00:03:58,319
the last 15 years. She's experienced in

99
00:03:56,239 --> 00:04:00,640
curriculum design and impactful teaching

100
00:03:58,319 --> 00:04:02,879
in the classroom. She leads the quality

101
00:04:00,640 --> 00:04:05,920
and safety work of AI tools at Oak

102
00:04:02,879 --> 00:04:08,400
National Academy. Our second co-author

103
00:04:05,920 --> 00:04:10,640
from Oak National Academy is Margot

104
00:04:08,400 --> 00:04:12,480
Dowand. She recently completed a

105
00:04:10,640 --> 00:04:14,879
master's in pure mathematics and then

106
00:04:12,480 --> 00:04:17,280
pivoted into AI and machine learning.

107
00:04:14,879 --> 00:04:19,919
She's a machine learning engineer at Oak

108
00:04:17,280 --> 00:04:22,960
National Academy focusing on evaluation

109
00:04:19,919 --> 00:04:25,600
and designing agent-based systems. Their

110
00:04:22,960 --> 00:04:27,600
paper is titled auto evaluation a

111
00:04:25,600 --> 00:04:30,720
critical measure in driving improvements

112
00:04:27,600 --> 00:04:33,360
in quality and safety of AI generated

113
00:04:30,720 --> 00:04:35,600
lesson resources. Now I'll pass the

114
00:04:33,360 --> 00:04:39,040
baton to our respondent Nick Baker to

115
00:04:35,600 --> 00:04:42,400
contextualize this moment for us.

116
00:04:39,040 --> 00:04:44,080
>> Thank you. And for those for whom a

117
00:04:42,400 --> 00:04:46,639
visual description is helpful, I'm a

118
00:04:44,080 --> 00:04:48,720
light-skinned male uh middle-aged male

119
00:04:46,639 --> 00:04:50,639
with short brown gray hair and short

120
00:04:48,720 --> 00:04:53,360
gray facial hair and glasses. and I'm

121
00:04:50,639 --> 00:04:55,120
wearing a blue shirt and headphones. I

122
00:04:53,360 --> 00:04:56,560
wanted to start by acknowledging that uh

123
00:04:55,120 --> 00:04:58,160
I have the privilege to live and work on

124
00:04:56,560 --> 00:05:00,720
the traditional lands of the three fires

125
00:04:58,160 --> 00:05:03,440
confederacy of the Ojiua, the Ottawa and

126
00:05:00,720 --> 00:05:06,000
the Potawa Tommy. Thanks so much to Sher

127
00:05:03,440 --> 00:05:07,919
and the team for the invitation and the

128
00:05:06,000 --> 00:05:10,639
incredible work that MIT is doing to

129
00:05:07,919 --> 00:05:13,199
advance open education um and all the

130
00:05:10,639 --> 00:05:16,080
open practitioners involved in the AI

131
00:05:13,199 --> 00:05:18,400
and open ed initiative. We're at a

132
00:05:16,080 --> 00:05:20,240
fascinating time in history where

133
00:05:18,400 --> 00:05:22,080
incredibly powerful technologies are

134
00:05:20,240 --> 00:05:23,840
kind of within the reach of almost

135
00:05:22,080 --> 00:05:26,320
everyone and societies around the globe

136
00:05:23,840 --> 00:05:27,919
are trying to figure out not only the

137
00:05:26,320 --> 00:05:29,919
implications of this new wave of

138
00:05:27,919 --> 00:05:31,840
artificial intelligence, but how best to

139
00:05:29,919 --> 00:05:33,440
steer its power towards good things for

140
00:05:31,840 --> 00:05:35,120
humanity because that's certainly not a

141
00:05:33,440 --> 00:05:36,800
given.

142
00:05:35,120 --> 00:05:38,080
There's a lot of potential for AI to

143
00:05:36,800 --> 00:05:40,080
help solve some of the persistent

144
00:05:38,080 --> 00:05:42,320
challenges that we've had in the open

145
00:05:40,080 --> 00:05:44,000
world for decades. how to keep resources

146
00:05:42,320 --> 00:05:46,080
up to date, how to adapt them to local

147
00:05:44,000 --> 00:05:47,840
context, adding local case studies,

148
00:05:46,080 --> 00:05:50,080
providing enough ancillary resources to

149
00:05:47,840 --> 00:05:52,400
compete with other forms of educational

150
00:05:50,080 --> 00:05:54,800
content and so on.

151
00:05:52,400 --> 00:05:57,120
AI can be harnessed to work on these

152
00:05:54,800 --> 00:05:58,560
gnarly challenges. Uh, but it can also

153
00:05:57,120 --> 00:05:59,840
present quite a few challenges of its

154
00:05:58,560 --> 00:06:01,759
own.

155
00:05:59,840 --> 00:06:03,919
There are many concerns that remain

156
00:06:01,759 --> 00:06:06,080
about the quality of AI outputs, biases

157
00:06:03,919 --> 00:06:07,680
and inaccuracies

158
00:06:06,080 --> 00:06:09,840
um that they might contain the potential

159
00:06:07,680 --> 00:06:12,080
to reinforce inequity in existing power

160
00:06:09,840 --> 00:06:13,120
structures and so on. Projects such as

161
00:06:12,080 --> 00:06:15,199
the ones that you're going to hear about

162
00:06:13,120 --> 00:06:16,720
today are critical to addressing those

163
00:06:15,199 --> 00:06:18,560
concerns.

164
00:06:16,720 --> 00:06:20,240
It's clear that there are now a new set

165
00:06:18,560 --> 00:06:22,479
of literacies and competencies that are

166
00:06:20,240 --> 00:06:25,199
emerging in response to the almost

167
00:06:22,479 --> 00:06:27,440
unimaginable growth of AI, which is a

168
00:06:25,199 --> 00:06:29,600
technology that's causing us to question

169
00:06:27,440 --> 00:06:31,759
our understanding of knowledge, how we

170
00:06:29,600 --> 00:06:34,400
create and share it, and even what it

171
00:06:31,759 --> 00:06:35,440
means to be human and sensient. Um, as

172
00:06:34,400 --> 00:06:37,199
our first speakers are going to point

173
00:06:35,440 --> 00:06:39,680
out, many of the competencies we need to

174
00:06:37,199 --> 00:06:42,000
work with AI are actually pretty closely

175
00:06:39,680 --> 00:06:43,600
aligned with the values and approaches

176
00:06:42,000 --> 00:06:47,120
of openness and open educational

177
00:06:43,600 --> 00:06:48,960
practices. With AI evolving so quickly,

178
00:06:47,120 --> 00:06:53,360
technical skills and knowing how to use

179
00:06:48,960 --> 00:06:56,080
a specific tool is rapidly outdated as

180
00:06:53,360 --> 00:06:57,680
knowledge. What we need is to develop

181
00:06:56,080 --> 00:07:00,080
literacies rather than just technical

182
00:06:57,680 --> 00:07:02,400
skills to handle the complexity of the

183
00:07:00,080 --> 00:07:04,240
transformation of knowledge work um in

184
00:07:02,400 --> 00:07:06,160
this new AIdriven environment that we

185
00:07:04,240 --> 00:07:07,520
find ourselves.

186
00:07:06,160 --> 00:07:08,800
As Angela and Josh are going to note in

187
00:07:07,520 --> 00:07:12,080
their paper, we need to place AI

188
00:07:08,800 --> 00:07:14,400
literacies above um AI tools themselves

189
00:07:12,080 --> 00:07:16,800
as central to fostering cultures of

190
00:07:14,400 --> 00:07:19,039
openness in the education in education

191
00:07:16,800 --> 00:07:20,319
globally. There are many questions that

192
00:07:19,039 --> 00:07:21,840
we need to address at the intersection

193
00:07:20,319 --> 00:07:23,759
of AI and openness. And I'm really

194
00:07:21,840 --> 00:07:25,280
excited for this opportunity to have a

195
00:07:23,759 --> 00:07:28,000
chat today with four of the authors of

196
00:07:25,280 --> 00:07:30,319
two excellent rapid response papers from

197
00:07:28,000 --> 00:07:31,759
the AI and open initiative um who've

198
00:07:30,319 --> 00:07:33,759
been obviously thinking about these

199
00:07:31,759 --> 00:07:36,800
questions really deeply. So I'll pass

200
00:07:33,759 --> 00:07:40,639
the baton back. Fantastic. Thank you so

201
00:07:36,800 --> 00:07:44,160
much. So I will kick us off with our

202
00:07:40,639 --> 00:07:47,440
first set of papers, our um co-authors

203
00:07:44,160 --> 00:07:50,240
Angela Gund and Josh Heron uh for their

204
00:07:47,440 --> 00:07:52,479
highlevel overview. Thank you.

205
00:07:50,240 --> 00:07:54,080
>> Thank you for the warm welcome Shira and

206
00:07:52,479 --> 00:07:56,560
Nick. I have to thank you for that

207
00:07:54,080 --> 00:08:00,319
beautiful and profound grounding and

208
00:07:56,560 --> 00:08:03,280
context for us. I know that in this um

209
00:08:00,319 --> 00:08:04,479
iconoclastic moment where um my

210
00:08:03,280 --> 00:08:06,000
colleagues are also going to present

211
00:08:04,479 --> 00:08:08,720
their paper, you're going to see a lot

212
00:08:06,000 --> 00:08:10,560
of connections and uh findings that link

213
00:08:08,720 --> 00:08:13,120
together and hopefully spark new ideas

214
00:08:10,560 --> 00:08:17,440
for you. Um so I'd love to kick off in

215
00:08:13,120 --> 00:08:19,520
that spirit. Uh and typically when I

216
00:08:17,440 --> 00:08:22,160
talk about research findings, especially

217
00:08:19,520 --> 00:08:24,160
from my team, I like to ground them

218
00:08:22,160 --> 00:08:26,000
around narratives because that helps us

219
00:08:24,160 --> 00:08:28,080
to apply theories and put them into

220
00:08:26,000 --> 00:08:30,000
practice. So today I'm going to talk a

221
00:08:28,080 --> 00:08:33,440
little bit about the night sky to kick

222
00:08:30,000 --> 00:08:36,320
us off first. Growing up um and honestly

223
00:08:33,440 --> 00:08:37,919
even now I've always been fascinated by

224
00:08:36,320 --> 00:08:40,080
looking up at the stars and you may be

225
00:08:37,919 --> 00:08:42,159
the same. And whether it's out here in

226
00:08:40,080 --> 00:08:45,040
the desert where I live in Phoenix,

227
00:08:42,159 --> 00:08:48,080
Arizona where the visibility is crisp

228
00:08:45,040 --> 00:08:50,080
and clear or even just catching glimpses

229
00:08:48,080 --> 00:08:52,399
between city lights. I always find

230
00:08:50,080 --> 00:08:55,120
myself drawn to that vastness and the

231
00:08:52,399 --> 00:08:57,200
way that constellations take shape. And

232
00:08:55,120 --> 00:08:59,440
each star alone that we look at is

233
00:08:57,200 --> 00:09:01,279
impressive, but it's the way that they

234
00:08:59,440 --> 00:09:04,160
connect that creates something

235
00:09:01,279 --> 00:09:06,800
meaningful. Whether it's a constellation

236
00:09:04,160 --> 00:09:08,800
that orients us or even helps us to find

237
00:09:06,800 --> 00:09:12,399
our way. You can go to the next slide,

238
00:09:08,800 --> 00:09:14,320
please. Metaphors from the natural world

239
00:09:12,399 --> 00:09:17,040
always creep into my work, especially

240
00:09:14,320 --> 00:09:18,959
within education. and the phenomena

241
00:09:17,040 --> 00:09:21,839
surrounding the vector of AI is really

242
00:09:18,959 --> 00:09:23,360
no exception and the connections to the

243
00:09:21,839 --> 00:09:26,720
metaphors about stars that I'm going to

244
00:09:23,360 --> 00:09:29,279
share here. Um, folks are grappling with

245
00:09:26,720 --> 00:09:31,920
what AI in education means for them in

246
00:09:29,279 --> 00:09:33,760
their roles and for their students and

247
00:09:31,920 --> 00:09:35,680
in Josh and my research and the research

248
00:09:33,760 --> 00:09:37,760
of our team. We've seen this uh from

249
00:09:35,680 --> 00:09:39,760
educators not only uh through their

250
00:09:37,760 --> 00:09:41,519
perspectives but their practices. And

251
00:09:39,760 --> 00:09:43,200
one of the biggest challenges that my

252
00:09:41,519 --> 00:09:47,360
team and I have seen is this quest to

253
00:09:43,200 --> 00:09:49,920
define AI literacy as a singular thing

254
00:09:47,360 --> 00:09:52,320
to find one perfect framework that will

255
00:09:49,920 --> 00:09:54,880
explain how we should think about AI in

256
00:09:52,320 --> 00:09:56,640
our work as educators and honestly it

257
00:09:54,880 --> 00:09:59,200
feels a little bit like chasing a black

258
00:09:56,640 --> 00:10:01,120
hole. Go to the next slide please. So

259
00:09:59,200 --> 00:10:03,680
when it comes to AI literacy, there's

260
00:10:01,120 --> 00:10:06,480
been a bit of obsession with finding the

261
00:10:03,680 --> 00:10:08,399
one framework to rule them all. And this

262
00:10:06,480 --> 00:10:10,640
framework is imagined as an ultimate

263
00:10:08,399 --> 00:10:12,959
guide, a singular model that can define

264
00:10:10,640 --> 00:10:15,839
what it means to be literate in AI

265
00:10:12,959 --> 00:10:17,360
across all contexts and applications.

266
00:10:15,839 --> 00:10:19,760
But in the same way that a black hole

267
00:10:17,360 --> 00:10:22,000
forms when a massive star dies and

268
00:10:19,760 --> 00:10:24,000
collapses under its own gravity, we've

269
00:10:22,000 --> 00:10:26,880
seen the same collapse happen when we

270
00:10:24,000 --> 00:10:29,519
try to define AI literacy as a single

271
00:10:26,880 --> 00:10:31,600
fixed entity of either literate versus

272
00:10:29,519 --> 00:10:33,680
illiterate. And instead of illuminating

273
00:10:31,600 --> 00:10:36,160
the way, this binary rips through

274
00:10:33,680 --> 00:10:38,480
diverse perspectives. It crushes our

275
00:10:36,160 --> 00:10:40,720
context specific practices and

276
00:10:38,480 --> 00:10:42,880
compresses all that richness that we

277
00:10:40,720 --> 00:10:45,519
know exists within education into a

278
00:10:42,880 --> 00:10:47,680
dense unyielding point. When we focus

279
00:10:45,519 --> 00:10:50,399
too much on creating one

280
00:10:47,680 --> 00:10:53,680
all-encompassing model, we risk losing

281
00:10:50,399 --> 00:10:57,040
the nuance, dynamic, and open nature of

282
00:10:53,680 --> 00:10:58,880
AI literacies, and we risk creating a

283
00:10:57,040 --> 00:11:00,560
closed singularity of thought that

284
00:10:58,880 --> 00:11:03,040
doesn't reflect the varied ways

285
00:11:00,560 --> 00:11:06,560
educators and learners actually use and

286
00:11:03,040 --> 00:11:08,560
think about AI. Next slide. So for the

287
00:11:06,560 --> 00:11:10,720
past two years, my team and I have

288
00:11:08,560 --> 00:11:13,040
conducted research on AI literacies,

289
00:11:10,720 --> 00:11:16,800
most recently in a study commissioned by

290
00:11:13,040 --> 00:11:18,720
UNESCO IIT. And in parallel to that

291
00:11:16,800 --> 00:11:21,120
study, we ended up talking to open

292
00:11:18,720 --> 00:11:23,519
educators around the globe about the

293
00:11:21,120 --> 00:11:25,760
competencies and skills needed to use AI

294
00:11:23,519 --> 00:11:28,640
for education as part of our rapid

295
00:11:25,760 --> 00:11:31,279
response paper. And just as the stars

296
00:11:28,640 --> 00:11:33,760
offer more than one just more than one

297
00:11:31,279 --> 00:11:35,440
bright point in the sky, the findings

298
00:11:33,760 --> 00:11:37,519
from these educators have really

299
00:11:35,440 --> 00:11:39,920
illuminated for us that AI literacies

300
00:11:37,519 --> 00:11:41,680
aren't about finding a single guiding

301
00:11:39,920 --> 00:11:43,279
light. Instead, they're more like a

302
00:11:41,680 --> 00:11:46,079
constellation. They're a pattern of

303
00:11:43,279 --> 00:11:48,399
skills, mindsets, and competencies that

304
00:11:46,079 --> 00:11:50,560
when we connect them together help us

305
00:11:48,399 --> 00:11:53,200
navigate the complex realities of AI

306
00:11:50,560 --> 00:11:55,120
within education. And to map these

307
00:11:53,200 --> 00:11:57,200
constellating sets of literacies, my

308
00:11:55,120 --> 00:11:59,440
research team and I identified eight

309
00:11:57,200 --> 00:12:01,040
dimensions of AI literacies, which is a

310
00:11:59,440 --> 00:12:03,920
combination of both skill sets and

311
00:12:01,040 --> 00:12:06,959
mindsets that reflect the multiaceted

312
00:12:03,920 --> 00:12:08,959
ways that we engage with AI. Um, these

313
00:12:06,959 --> 00:12:10,800
dimensions, if you look at them in just

314
00:12:08,959 --> 00:12:12,480
a second, are going to maybe seem

315
00:12:10,800 --> 00:12:14,560
familiar to some of you, and that's

316
00:12:12,480 --> 00:12:16,480
because they're an open remix of the

317
00:12:14,560 --> 00:12:18,720
foundational work of my colleague and

318
00:12:16,480 --> 00:12:20,639
friend Dr. Doug Belshaw, um, who wrote

319
00:12:18,720 --> 00:12:23,040
about digital literacies in the early

320
00:12:20,639 --> 00:12:25,839
2000s. and they're aimed at creating a

321
00:12:23,040 --> 00:12:28,560
more nuanced and pluralistic vocabulary

322
00:12:25,839 --> 00:12:30,079
for AI literacies in education. So with

323
00:12:28,560 --> 00:12:32,480
that, I'm going to have my colleague and

324
00:12:30,079 --> 00:12:35,120
friend Dr. Josh Herren share a little

325
00:12:32,480 --> 00:12:37,519
bit more about the study, our conceptual

326
00:12:35,120 --> 00:12:40,160
framing of literacies as pluralistic and

327
00:12:37,519 --> 00:12:41,360
socioulturally situated. And he's also

328
00:12:40,160 --> 00:12:43,680
going to share the findings of our

329
00:12:41,360 --> 00:12:45,680
inquiry on what those constellating sets

330
00:12:43,680 --> 00:12:47,760
of AI literacies were that emerged from

331
00:12:45,680 --> 00:12:50,639
our study.

332
00:12:47,760 --> 00:12:53,680
>> Thank you, Angela. Yes, and I will echo

333
00:12:50,639 --> 00:12:56,480
it's an great opportunity to be here and

334
00:12:53,680 --> 00:12:57,839
um share these results with you all and

335
00:12:56,480 --> 00:12:59,920
but also invite you into the

336
00:12:57,839 --> 00:13:02,560
conversation. So, as Angela mentioned,

337
00:12:59,920 --> 00:13:04,639
we held dozens of conversations uh as

338
00:13:02,560 --> 00:13:06,959
part of this research uh team with

339
00:13:04,639 --> 00:13:09,440
educators from around the world about

340
00:13:06,959 --> 00:13:11,279
what they were seeing happening with AI

341
00:13:09,440 --> 00:13:14,480
in their context and how it was being

342
00:13:11,279 --> 00:13:17,440
used or not used as part of these um

343
00:13:14,480 --> 00:13:19,279
parallel uh purposes and goals uh that

344
00:13:17,440 --> 00:13:21,839
they had personally and professionally

345
00:13:19,279 --> 00:13:24,959
and institutionally.

346
00:13:21,839 --> 00:13:28,000
And uh as these studies ran in parallel

347
00:13:24,959 --> 00:13:31,120
uh with one another from UNESCO and um

348
00:13:28,000 --> 00:13:33,519
the work for this paper, we coded the

349
00:13:31,120 --> 00:13:35,200
data and noticed that findings were very

350
00:13:33,519 --> 00:13:36,720
close to what we are seeing in learning

351
00:13:35,200 --> 00:13:38,560
environments more broadly. And that is

352
00:13:36,720 --> 00:13:41,760
that we need an interrelated

353
00:13:38,560 --> 00:13:43,519
competencies, skills and mindsets uh to

354
00:13:41,760 --> 00:13:46,880
build instruction rather than knowledge

355
00:13:43,519 --> 00:13:48,560
of just one tool for one purpose. Um,

356
00:13:46,880 --> 00:13:50,320
and we also knew that these literacies

357
00:13:48,560 --> 00:13:53,440
were about more than tools because

358
00:13:50,320 --> 00:13:56,240
they're about culture. And again, we see

359
00:13:53,440 --> 00:13:59,839
uh so much of this in in the uh digital

360
00:13:56,240 --> 00:14:02,079
learning space. And Angela has uh framed

361
00:13:59,839 --> 00:14:03,839
it as the three C's u coming out of

362
00:14:02,079 --> 00:14:06,000
focusing on people, objects, and events.

363
00:14:03,839 --> 00:14:08,560
It's how we collaborate, how we create,

364
00:14:06,000 --> 00:14:10,959
and how we connect to our community. And

365
00:14:08,560 --> 00:14:13,839
from there uh with the remixing of Doug

366
00:14:10,959 --> 00:14:15,680
Bell Shaw's um dimension uh digital

367
00:14:13,839 --> 00:14:17,519
literacies the dimensions of AI

368
00:14:15,680 --> 00:14:19,760
literacies were born and we'll get to

369
00:14:17,519 --> 00:14:23,120
the next slide and uh share the

370
00:14:19,760 --> 00:14:25,360
definition of what are AI literacies. Uh

371
00:14:23,120 --> 00:14:27,360
and yes, we are intentionally using

372
00:14:25,360 --> 00:14:28,959
plural and there's much to be said about

373
00:14:27,360 --> 00:14:31,360
that and much you can read about why

374
00:14:28,959 --> 00:14:34,160
we're we're doing that uh plural

375
00:14:31,360 --> 00:14:36,240
uhization of the term, but they're

376
00:14:34,160 --> 00:14:37,839
simply put a collection of skills and

377
00:14:36,240 --> 00:14:40,959
knowledge that a person needs to

378
00:14:37,839 --> 00:14:43,279
understand, use, and critically evaluate

379
00:14:40,959 --> 00:14:44,800
artificial intelligence.

380
00:14:43,279 --> 00:14:47,040
They work together. They're

381
00:14:44,800 --> 00:14:48,720
interconnected. And you'll see how that

382
00:14:47,040 --> 00:14:52,000
plays out in in just a moment when we

383
00:14:48,720 --> 00:14:54,800
talk about uh the ways that certain uh

384
00:14:52,000 --> 00:14:57,440
connections uh work out within uh

385
00:14:54,800 --> 00:14:59,600
various regions. They can be expressed

386
00:14:57,440 --> 00:15:02,160
in different ways across diverse

387
00:14:59,600 --> 00:15:04,000
contexts and modalities. They give us a

388
00:15:02,160 --> 00:15:05,839
way to talk about our abilities and how

389
00:15:04,000 --> 00:15:08,320
they can be applied in a rapidly

390
00:15:05,839 --> 00:15:11,920
changing world of education. It's

391
00:15:08,320 --> 00:15:14,639
something that's still going to uh last

392
00:15:11,920 --> 00:15:17,519
next week uh and next month and uh and

393
00:15:14,639 --> 00:15:19,519
has held up for the past year and a half

394
00:15:17,519 --> 00:15:21,519
or so that uh the dimensions of AI

395
00:15:19,519 --> 00:15:23,680
literacies have been used in a variety

396
00:15:21,519 --> 00:15:27,040
of contexts. So they're flexible and

397
00:15:23,680 --> 00:15:29,199
grow with the evolution of AI over time.

398
00:15:27,040 --> 00:15:31,680
And next slide we'll show you the

399
00:15:29,199 --> 00:15:32,880
dimensions.

400
00:15:31,680 --> 00:15:35,519
Uh we're not going to read through all

401
00:15:32,880 --> 00:15:38,320
of these. uh you can uh check those out

402
00:15:35,519 --> 00:15:40,880
at uh the the link there. Uh but each

403
00:15:38,320 --> 00:15:45,360
dimension is essential in its own right

404
00:15:40,880 --> 00:15:48,240
and uh there it's not meant to be a uh

405
00:15:45,360 --> 00:15:50,079
deficit or a gap focus. It's meant to

406
00:15:48,240 --> 00:15:52,720
see where are the strengths at where can

407
00:15:50,079 --> 00:15:55,279
we lean on one another uh and reinforce

408
00:15:52,720 --> 00:15:57,600
one another uh as we build this

409
00:15:55,279 --> 00:15:59,120
constellation and make it useful. uh

410
00:15:57,600 --> 00:16:01,839
they help us map out the broader

411
00:15:59,120 --> 00:16:03,920
landscape of I AI allowing us to move

412
00:16:01,839 --> 00:16:06,720
from one context to another with a sense

413
00:16:03,920 --> 00:16:11,120
of direction and purpose which I know uh

414
00:16:06,720 --> 00:16:12,800
feels hard in the current age of AI and

415
00:16:11,120 --> 00:16:14,320
so as Angel said we're not looking for

416
00:16:12,800 --> 00:16:16,800
one perfect model we're inviting you to

417
00:16:14,320 --> 00:16:18,720
think about AI literacies as a dynamic

418
00:16:16,800 --> 00:16:21,199
constellation this interwoven set of

419
00:16:18,720 --> 00:16:22,560
skills and understandings now on this

420
00:16:21,199 --> 00:16:26,079
next slide you'll see that we're

421
00:16:22,560 --> 00:16:29,360
spotlighting three and um that's because

422
00:16:26,079 --> 00:16:32,800
we saw all uh in in our findings uh

423
00:16:29,360 --> 00:16:35,839
certain literacies uh pop more than

424
00:16:32,800 --> 00:16:37,759
others and so uh constructive AI

425
00:16:35,839 --> 00:16:40,560
literacies and critical AI literacies

426
00:16:37,759 --> 00:16:43,440
were particularly um strong and and

427
00:16:40,560 --> 00:16:45,839
those asked that we use AI tools to

428
00:16:43,440 --> 00:16:48,000
rebuild, remix and generate new content.

429
00:16:45,839 --> 00:16:50,560
So folks were interested in how do we

430
00:16:48,000 --> 00:16:52,959
use these tools? What can they do? Uh

431
00:16:50,560 --> 00:16:56,000
and we saw that connection again to

432
00:16:52,959 --> 00:16:59,120
critical literacies. this idea of are we

433
00:16:56,000 --> 00:17:01,519
using them ethically, equitably, uh are

434
00:16:59,120 --> 00:17:04,720
we uh validating outputs for accuracy?

435
00:17:01,519 --> 00:17:07,360
Are we rushing into uh the usage and and

436
00:17:04,720 --> 00:17:08,559
the trust of of what we're seeing? And

437
00:17:07,360 --> 00:17:10,319
also very concerned about the

438
00:17:08,559 --> 00:17:11,839
environmental impacts. I know that's a

439
00:17:10,319 --> 00:17:14,000
big conversation that we're all having

440
00:17:11,839 --> 00:17:16,480
right now as well. And particularly in

441
00:17:14,000 --> 00:17:19,679
the global south, we saw uh cultural AI

442
00:17:16,480 --> 00:17:21,760
literacies as a an important uh

443
00:17:19,679 --> 00:17:24,240
component and and it's something that we

444
00:17:21,760 --> 00:17:28,160
saw pop uh throughout the study as well.

445
00:17:24,240 --> 00:17:30,720
But this idea, how can we use uh uh AI

446
00:17:28,160 --> 00:17:32,559
to support uh points of engagement from

447
00:17:30,720 --> 00:17:35,679
the tools and the environments

448
00:17:32,559 --> 00:17:37,440
recognizing multilingual support and uh

449
00:17:35,679 --> 00:17:39,039
the contextualized multilingual

450
00:17:37,440 --> 00:17:41,760
collaboration? And then on this slide

451
00:17:39,039 --> 00:17:43,679
you're seeing here, these are uh some of

452
00:17:41,760 --> 00:17:46,960
the ways these interconnected literacies

453
00:17:43,679 --> 00:17:51,200
work together. Uh you'll see that uh

454
00:17:46,960 --> 00:17:54,400
each region has a set of interconnected

455
00:17:51,200 --> 00:17:57,039
uh literacies that uh folks in our

456
00:17:54,400 --> 00:17:59,280
conversations were um curious about,

457
00:17:57,039 --> 00:18:00,960
concerned about, interested in. uh we

458
00:17:59,280 --> 00:18:04,559
won't read through each of these but you

459
00:18:00,960 --> 00:18:07,120
can get a sense from from uh this slide

460
00:18:04,559 --> 00:18:09,039
uh that these uh regional approaches

461
00:18:07,120 --> 00:18:10,880
underline the need to embed cultural

462
00:18:09,039 --> 00:18:13,360
ethical constructive AI literacies into

463
00:18:10,880 --> 00:18:15,440
open ed education in particular and this

464
00:18:13,360 --> 00:18:17,760
integration empowers educators and

465
00:18:15,440 --> 00:18:20,240
learners to navigate AI critically and

466
00:18:17,760 --> 00:18:22,160
responsibly while maintaining the values

467
00:18:20,240 --> 00:18:24,559
of openness

468
00:18:22,160 --> 00:18:26,720
and then in this uh next step slide

469
00:18:24,559 --> 00:18:30,240
we're uh just shining a light on a

470
00:18:26,720 --> 00:18:33,039
pluralistic view uh we're asking you to

471
00:18:30,240 --> 00:18:35,280
join in the conversation uh submit a

472
00:18:33,039 --> 00:18:36,880
case example to help us as we continue

473
00:18:35,280 --> 00:18:38,559
the research on AI literacies by

474
00:18:36,880 --> 00:18:41,600
surfacing these regional perspectives

475
00:18:38,559 --> 00:18:46,080
and practices and then uh you're invited

476
00:18:41,600 --> 00:18:50,080
to u participate in uh the uh events and

477
00:18:46,080 --> 00:18:53,200
courses on uh open culture and the many

478
00:18:50,080 --> 00:18:55,039
resources available to you through uh

479
00:18:53,200 --> 00:18:56,799
this collaboration. And with that, I

480
00:18:55,039 --> 00:18:59,360
believe I'm at time and we want to say

481
00:18:56,799 --> 00:19:02,320
thank you so much.

482
00:18:59,360 --> 00:19:05,360
>> Fantastic. Thank you so much. Up next,

483
00:19:02,320 --> 00:19:09,520
we have Hannah Beth Clark and Margot

484
00:19:05,360 --> 00:19:10,960
Deland to talk about their paper.

485
00:19:09,520 --> 00:19:13,280
>> Great. Again, thank you so much for

486
00:19:10,960 --> 00:19:15,840
having us. Um, at Oak, we've been doing

487
00:19:13,280 --> 00:19:17,600
lots of work on evaluating quality. Um,

488
00:19:15,840 --> 00:19:19,120
and this was a really exciting project

489
00:19:17,600 --> 00:19:21,440
to be able to showcase some of the work

490
00:19:19,120 --> 00:19:23,840
that we've been doing. Um Margot and I

491
00:19:21,440 --> 00:19:26,480
are representing two of the 10 authors

492
00:19:23,840 --> 00:19:28,640
on this paper. Um and we come from a

493
00:19:26,480 --> 00:19:30,960
cross functional team um representing

494
00:19:28,640 --> 00:19:34,320
software engineers u machine learning

495
00:19:30,960 --> 00:19:37,360
engineers um sort of members of our team

496
00:19:34,320 --> 00:19:39,200
working from an education perspective um

497
00:19:37,360 --> 00:19:41,039
and Margo's background is in machine

498
00:19:39,200 --> 00:19:42,400
learning mine's in education and so

499
00:19:41,039 --> 00:19:45,039
hopefully we can provide sort of both

500
00:19:42,400 --> 00:19:47,520
sides of that perspective to you here.

501
00:19:45,039 --> 00:19:49,520
Um so just to give you a bit of sort of

502
00:19:47,520 --> 00:19:51,679
introduction into Oak and and a little

503
00:19:49,520 --> 00:19:53,520
bit more about what who we are as an

504
00:19:51,679 --> 00:19:55,919
organization. Um we were set up during

505
00:19:53,520 --> 00:19:59,280
the pandemic as a response to schools

506
00:19:55,919 --> 00:20:01,440
closing because of COVID. Um and over

507
00:19:59,280 --> 00:20:02,960
the summer holidays produced 10,000 free

508
00:20:01,440 --> 00:20:05,200
resources. Lots of teachers came

509
00:20:02,960 --> 00:20:07,280
together to produce those um that were

510
00:20:05,200 --> 00:20:09,360
free and accessible for pupils to use

511
00:20:07,280 --> 00:20:11,679
and access from home. Um and following

512
00:20:09,360 --> 00:20:13,440
the pandemic, we found that pupils and

513
00:20:11,679 --> 00:20:15,520
teachers were still continuing to access

514
00:20:13,440 --> 00:20:17,440
those resources. And so we received some

515
00:20:15,520 --> 00:20:19,760
government funding to be able to remake

516
00:20:17,440 --> 00:20:21,919
those resources um to a really high

517
00:20:19,760 --> 00:20:24,320
quality and standard. And they're all

518
00:20:21,919 --> 00:20:26,559
available on an open government license.

519
00:20:24,320 --> 00:20:29,039
Um we we hope that by making those

520
00:20:26,559 --> 00:20:31,360
available to teachers and to pupils um

521
00:20:29,039 --> 00:20:33,360
we're able to sort of um improve pupil

522
00:20:31,360 --> 00:20:35,280
outcomes, close the disadvantage gap,

523
00:20:33,360 --> 00:20:37,440
but also help other organizations to

524
00:20:35,280 --> 00:20:40,880
innovate in that space using highquality

525
00:20:37,440 --> 00:20:42,720
underlying curriculum material.

526
00:20:40,880 --> 00:20:44,000
So what do we have on offer at Oak? Um

527
00:20:42,720 --> 00:20:45,679
first of all, we have really high

528
00:20:44,000 --> 00:20:48,000
quality teaching materials and

529
00:20:45,679 --> 00:20:50,159
curricular for teachers. Um we have an

530
00:20:48,000 --> 00:20:51,679
interactive uh platform for pupils.

531
00:20:50,159 --> 00:20:54,000
That's for any pupils that might be

532
00:20:51,679 --> 00:20:55,679
learning from home for any reason or

533
00:20:54,000 --> 00:20:58,559
should we get to a point of school

534
00:20:55,679 --> 00:21:00,320
closures due to extreme weather or um

535
00:20:58,559 --> 00:21:02,320
hopefully not another pandemic, we'd

536
00:21:00,320 --> 00:21:05,200
have a sort of um remote education back

537
00:21:02,320 --> 00:21:07,679
stop. Um and then finally um we've been

538
00:21:05,200 --> 00:21:10,240
experimenting with AI building an AI

539
00:21:07,679 --> 00:21:13,120
lesson assistant um which is called ISA

540
00:21:10,240 --> 00:21:15,840
and the name comes from um AI lesson

541
00:21:13,120 --> 00:21:19,039
assistant but its meaning is um oak tree

542
00:21:15,840 --> 00:21:21,200
in Hebrew and from strong roots in um in

543
00:21:19,039 --> 00:21:23,520
Scottish Gaelic which we felt was really

544
00:21:21,200 --> 00:21:24,720
appropriate for for the use case. Um and

545
00:21:23,520 --> 00:21:29,320
we're going to talk specifically today

546
00:21:24,720 --> 00:21:29,320
about our AI lesson assistant Isa.

547
00:21:30,159 --> 00:21:35,840
Could you just go one back? Sorry.

548
00:21:32,559 --> 00:21:37,679
Thanks. Uh yeah, perfect. So um we were

549
00:21:35,840 --> 00:21:40,240
in a really strong position to innovate

550
00:21:37,679 --> 00:21:41,760
with AI. Um and there were sort of four

551
00:21:40,240 --> 00:21:44,400
things that put us into this position.

552
00:21:41,760 --> 00:21:46,640
The first was we have an incredibly um

553
00:21:44,400 --> 00:21:49,600
rich high quality corpus of content. We

554
00:21:46,640 --> 00:21:51,440
have nearly 13,000 lessons and resources

555
00:21:49,600 --> 00:21:54,159
um that have been created by expert

556
00:21:51,440 --> 00:21:56,080
humans and quality assured um by subject

557
00:21:54,159 --> 00:21:58,480
experts that we were able to use to

558
00:21:56,080 --> 00:22:00,720
innovate in this space. We also had lots

559
00:21:58,480 --> 00:22:02,640
of expertise in curriculum design um

560
00:22:00,720 --> 00:22:04,640
from from the lessons and resources that

561
00:22:02,640 --> 00:22:06,400
we'd been creating. We had as an

562
00:22:04,640 --> 00:22:08,720
organization an agreed definition of

563
00:22:06,400 --> 00:22:10,799
quality um which was really useful for

564
00:22:08,720 --> 00:22:12,720
building our prompt. Um and finally we

565
00:22:10,799 --> 00:22:14,159
had cross functional team made up of all

566
00:22:12,720 --> 00:22:16,000
of those different sort of parts

567
00:22:14,159 --> 00:22:17,840
including machine learning engineers but

568
00:22:16,000 --> 00:22:20,720
also lots of expertise from the sort of

569
00:22:17,840 --> 00:22:22,240
education perspective.

570
00:22:20,720 --> 00:22:24,240
Um I'm going to give you just a very

571
00:22:22,240 --> 00:22:26,240
quick brief glimpse into ISA. um

572
00:22:24,240 --> 00:22:27,600
hopefully on the next slide. Um if

573
00:22:26,240 --> 00:22:30,159
anyone wants to have a go with using

574
00:22:27,600 --> 00:22:32,240
ISA, you're very welcome to. Um but ISA

575
00:22:30,159 --> 00:22:34,960
is designed to take a teacher through

576
00:22:32,240 --> 00:22:36,640
planning a lesson step by step. Um

577
00:22:34,960 --> 00:22:39,200
allowing them to personalize that

578
00:22:36,640 --> 00:22:41,360
content for their specific learners. Um

579
00:22:39,200 --> 00:22:43,919
sort of embodying best practice in terms

580
00:22:41,360 --> 00:22:45,520
of pedagogy and cognitive science um to

581
00:22:43,919 --> 00:22:47,280
support them in producing really high

582
00:22:45,520 --> 00:22:48,960
quality lessons and resources that are

583
00:22:47,280 --> 00:22:51,039
going to be useful for them in in the

584
00:22:48,960 --> 00:22:52,799
classroom. And we've intentionally

585
00:22:51,039 --> 00:22:54,799
designed that so that's a co-planning

586
00:22:52,799 --> 00:22:56,240
process for the teacher. Um it's not

587
00:22:54,799 --> 00:22:58,000
just doing all of the hard work for

588
00:22:56,240 --> 00:23:00,159
them. It's asking for their feedback and

589
00:22:58,000 --> 00:23:01,840
their review as they go through. And

590
00:23:00,159 --> 00:23:03,840
after this lesson's been created,

591
00:23:01,840 --> 00:23:05,440
they're able to download adaptable

592
00:23:03,840 --> 00:23:08,960
resources that they can then use within

593
00:23:05,440 --> 00:23:10,880
their classroom. Next slide. Thanks. So

594
00:23:08,960 --> 00:23:13,919
what makes ISA different from other AI

595
00:23:10,880 --> 00:23:16,000
tools? Um firstly, we've got um you can

596
00:23:13,919 --> 00:23:17,200
just pop them all up, Shar. Thanks. Um

597
00:23:16,000 --> 00:23:19,360
firstly, we've got an incredibly

598
00:23:17,200 --> 00:23:21,760
detailed UK specific prompt. Um, we've

599
00:23:19,360 --> 00:23:24,000
codified best practice in in pedigogy

600
00:23:21,760 --> 00:23:25,760
and cognitive science to ensure that the

601
00:23:24,000 --> 00:23:28,159
outputs of ISA are incredibly high

602
00:23:25,760 --> 00:23:30,159
quality. Um, we use retrieval augmented

603
00:23:28,159 --> 00:23:32,000
generation and content anchoring to

604
00:23:30,159 --> 00:23:34,400
bring in that content from our human

605
00:23:32,000 --> 00:23:35,840
created corpus of content. We've got a

606
00:23:34,400 --> 00:23:38,400
whole range of additional safety

607
00:23:35,840 --> 00:23:40,400
guardrails um to ensure that our tool is

608
00:23:38,400 --> 00:23:42,000
really safe for use in the classroom.

609
00:23:40,400 --> 00:23:43,679
And finally, we're keeping the teacher

610
00:23:42,000 --> 00:23:45,360
in the driving seat and we've ensured

611
00:23:43,679 --> 00:23:46,960
the outputs of our tool are really

612
00:23:45,360 --> 00:23:48,559
useful.

613
00:23:46,960 --> 00:23:50,000
So, we've talked a bit about ISA. We're

614
00:23:48,559 --> 00:23:52,559
now going to move on to talk about auto

615
00:23:50,000 --> 00:23:55,520
evaluation. Um, and we want to start off

616
00:23:52,559 --> 00:23:58,159
with the problem here. So, uh, next

617
00:23:55,520 --> 00:24:01,679
slide. Thanks. So, the problem for us is

618
00:23:58,159 --> 00:24:04,159
that, um, AI generated content is, um,

619
00:24:01,679 --> 00:24:06,159
sort of rapidly, uh, being launched into

620
00:24:04,159 --> 00:24:08,000
the market and it's really important

621
00:24:06,159 --> 00:24:10,000
that this specific content is being

622
00:24:08,000 --> 00:24:11,679
evaluated and we wanted to place

623
00:24:10,000 --> 00:24:14,559
ourselves as sort of leaders in this

624
00:24:11,679 --> 00:24:16,320
space. um making sure that there was

625
00:24:14,559 --> 00:24:17,919
importance placed on the quality of

626
00:24:16,320 --> 00:24:19,120
content that was being produced and how

627
00:24:17,919 --> 00:24:21,919
this could then be used in the

628
00:24:19,120 --> 00:24:24,000
classroom. Going to hand over to Margot.

629
00:24:21,919 --> 00:24:25,679
Thank you Hannah Beth. Yes. So with our

630
00:24:24,000 --> 00:24:27,919
focus Oh, back to the previous slide

631
00:24:25,679 --> 00:24:29,520
please. So with our focus on evaluation,

632
00:24:27,919 --> 00:24:31,440
not only can we highlight areas for

633
00:24:29,520 --> 00:24:33,200
improvement, but also it means that

634
00:24:31,440 --> 00:24:35,279
whenever we make changes to ISA, whether

635
00:24:33,200 --> 00:24:37,120
that be new versions of the prompt, uh

636
00:24:35,279 --> 00:24:39,039
using different model models or

637
00:24:37,120 --> 00:24:40,799
adjusting parameters, it allows us to

638
00:24:39,039 --> 00:24:42,400
take a really datadriven approach and

639
00:24:40,799 --> 00:24:44,960
really evaluate the effects that those

640
00:24:42,400 --> 00:24:46,559
changes have. Next slide, please. Now,

641
00:24:44,960 --> 00:24:48,480
the way that we do this is with auto

642
00:24:46,559 --> 00:24:50,480
evaluation, also known as like LLM as

643
00:24:48,480 --> 00:24:52,640
judge, where we essentially ask an LLM

644
00:24:50,480 --> 00:24:54,480
to evaluate content for us using a

645
00:24:52,640 --> 00:24:56,720
prompt. Um and we've designed our own

646
00:24:54,480 --> 00:24:59,679
auto valuation tool which allows us to

647
00:24:56,720 --> 00:25:01,760
do this at scale. So we have 24 uh

648
00:24:59,679 --> 00:25:03,679
benchmarks 24 of these tests in the form

649
00:25:01,760 --> 00:25:05,679
of prompts uh which are a mixture of

650
00:25:03,679 --> 00:25:07,520
tests based on both Oak's pedagogical

651
00:25:05,679 --> 00:25:09,039
rubric for example increasing quiz

652
00:25:07,520 --> 00:25:10,960
difficulty which looks at if the

653
00:25:09,039 --> 00:25:13,279
questions in a quiz are getting harder

654
00:25:10,960 --> 00:25:15,039
um or like common quality issues for

655
00:25:13,279 --> 00:25:18,240
example Americanisms which we hope to

656
00:25:15,039 --> 00:25:20,799
avoid. Uh and what we ask the LLM for is

657
00:25:18,240 --> 00:25:22,960
a score either one to between one and

658
00:25:20,799 --> 00:25:25,120
five where five is ideal or a boolean

659
00:25:22,960 --> 00:25:27,440
output true false if it's well suited to

660
00:25:25,120 --> 00:25:29,120
the test as well as a justification so

661
00:25:27,440 --> 00:25:30,880
we can really track the LLM's reasoning

662
00:25:29,120 --> 00:25:32,960
and we get the justification to be

663
00:25:30,880 --> 00:25:34,960
output before the score due to the

664
00:25:32,960 --> 00:25:36,480
predictive nature of Genai so that

665
00:25:34,960 --> 00:25:38,559
hopefully the the justification

666
00:25:36,480 --> 00:25:40,880
influences the score and not vice versa.

667
00:25:38,559 --> 00:25:43,200
Next slide please. So yeah, it's all

668
00:25:40,880 --> 00:25:45,520
good and well having all these LLMs, you

669
00:25:43,200 --> 00:25:47,760
know, outputting all these different uh

670
00:25:45,520 --> 00:25:49,520
evaluations for us, but unless they're

671
00:25:47,760 --> 00:25:51,520
uh aligned with expert humans, how can

672
00:25:49,520 --> 00:25:53,039
we really trust them? So our goal for

673
00:25:51,520 --> 00:25:55,360
this paper was to get our auto

674
00:25:53,039 --> 00:25:57,600
evaluations to align with expert humans,

675
00:25:55,360 --> 00:25:59,039
i.e. teachers. And the prompt that we

676
00:25:57,600 --> 00:26:00,559
chose was answers are minimally

677
00:25:59,039 --> 00:26:02,080
different, which just looks at what we

678
00:26:00,559 --> 00:26:03,760
call the distractors of a multiple

679
00:26:02,080 --> 00:26:05,600
choice quiz question. So the incorrect

680
00:26:03,760 --> 00:26:07,600
answers, compares them to the answer,

681
00:26:05,600 --> 00:26:10,159
and basically sees if it's too obvious

682
00:26:07,600 --> 00:26:12,799
or not. Next slide, please. And we chose

683
00:26:10,159 --> 00:26:14,880
this one because we ran around 5,000

684
00:26:12,799 --> 00:26:16,720
lessons through our auto evaluation tool

685
00:26:14,880 --> 00:26:18,640
and picked one of the ones where the LLM

686
00:26:16,720 --> 00:26:20,080
was um judging very harshly. So you can

687
00:26:18,640 --> 00:26:22,240
see in the second row the scores are

688
00:26:20,080 --> 00:26:23,760
quite low. Um and out of the ones that

689
00:26:22,240 --> 00:26:25,120
was judging harshly, this was quite a

690
00:26:23,760 --> 00:26:26,240
digestible task. We didn't have to

691
00:26:25,120 --> 00:26:29,120
present the teacher with too much

692
00:26:26,240 --> 00:26:31,279
information. Next slide please.

693
00:26:29,120 --> 00:26:33,760
Um yeah, so what we did so we ran auto

694
00:26:31,279 --> 00:26:36,400
evaluation a number of quiz questions.

695
00:26:33,760 --> 00:26:38,000
Um, we took these same quiz questions,

696
00:26:36,400 --> 00:26:40,240
gave them to the teachers, making sure

697
00:26:38,000 --> 00:26:43,279
they each saw questions over across a

698
00:26:40,240 --> 00:26:45,760
range of quality. Um, we collected 311

699
00:26:43,279 --> 00:26:48,320
of these. Uh, we then conducted thematic

700
00:26:45,760 --> 00:26:50,559
analysis on the scores and well on the

701
00:26:48,320 --> 00:26:51,919
justifications for particular scores.

702
00:26:50,559 --> 00:26:54,080
Um, as well as pulling out

703
00:26:51,919 --> 00:26:55,679
representative examples, took that

704
00:26:54,080 --> 00:26:57,279
information, threw it back into the

705
00:26:55,679 --> 00:26:59,600
prompt, and then compared the alignment

706
00:26:57,279 --> 00:27:02,159
before and after. And next slide,

707
00:26:59,600 --> 00:27:04,000
please. So, it worked. So we found a

708
00:27:02,159 --> 00:27:06,320
statistically significant improvement in

709
00:27:04,000 --> 00:27:09,039
alignment between our auto evaluation

710
00:27:06,320 --> 00:27:10,799
tool and the human evaluations. Um the

711
00:27:09,039 --> 00:27:13,039
three overarching categories that we got

712
00:27:10,799 --> 00:27:14,880
were what makes a good distractor

713
00:27:13,039 --> 00:27:17,039
essentially was plausibility,

714
00:27:14,880 --> 00:27:18,960
commonality and structural coherence.

715
00:27:17,039 --> 00:27:21,200
And not only were we able to improve the

716
00:27:18,960 --> 00:27:23,039
auto evaluation prompt, but through

717
00:27:21,200 --> 00:27:24,640
being able to define what makes a good

718
00:27:23,039 --> 00:27:26,960
distractor, we could improve the ISA

719
00:27:24,640 --> 00:27:28,880
prompt itself. But perhaps the more

720
00:27:26,960 --> 00:27:30,080
important finding um even though

721
00:27:28,880 --> 00:27:32,400
alignment's great is that the

722
00:27:30,080 --> 00:27:33,679
justifications the LLM gave ended up

723
00:27:32,400 --> 00:27:35,520
aligning more with what the teachers

724
00:27:33,679 --> 00:27:37,840
were thinking. And so it could be the

725
00:27:35,520 --> 00:27:40,080
case that um the LLM is really a more

726
00:27:37,840 --> 00:27:42,400
consistent evaluator. Um so as long as

727
00:27:40,080 --> 00:27:44,480
it's in line with the themes that's uh

728
00:27:42,400 --> 00:27:47,600
that's the goal really. So yeah, next

729
00:27:44,480 --> 00:27:49,919
slide please.

730
00:27:47,600 --> 00:27:52,159
Great. So um we obviously published this

731
00:27:49,919 --> 00:27:53,520
paper a few months ago. Um but we've got

732
00:27:52,159 --> 00:27:55,840
lots of other areas that we're working

733
00:27:53,520 --> 00:27:58,159
on um that sort of build on this

734
00:27:55,840 --> 00:27:59,679
specific part of auto evaluation. The

735
00:27:58,159 --> 00:28:01,360
first is we're looking at our safety

736
00:27:59,679 --> 00:28:02,880
guard rails. As I mentioned earlier, we

737
00:28:01,360 --> 00:28:05,120
have a number of additional safety guard

738
00:28:02,880 --> 00:28:06,960
rails to ensure our tool is safe for use

739
00:28:05,120 --> 00:28:08,880
um and the content that's provided is

740
00:28:06,960 --> 00:28:10,799
appropriate for use in schools. Um and

741
00:28:08,880 --> 00:28:13,039
so we're using our auto evaluation tool

742
00:28:10,799 --> 00:28:16,080
to help us to ensure that that aligns to

743
00:28:13,039 --> 00:28:18,320
human uh human judgments on that. We

744
00:28:16,080 --> 00:28:20,480
also um are hoping to build in live

745
00:28:18,320 --> 00:28:22,960
evaluations so that the auto evaluation

746
00:28:20,480 --> 00:28:24,559
tool is um is reviewing the content

747
00:28:22,960 --> 00:28:27,120
that's being produced as it's being

748
00:28:24,559 --> 00:28:28,799
produced rather than async um and is

749
00:28:27,120 --> 00:28:30,799
able to provide the teacher with

750
00:28:28,799 --> 00:28:32,559
feedback on on areas of the lesson that

751
00:28:30,799 --> 00:28:34,240
they might want to go back and improve

752
00:28:32,559 --> 00:28:36,799
as well as going back through and making

753
00:28:34,240 --> 00:28:38,880
automatic updates. Um and then finally

754
00:28:36,799 --> 00:28:41,679
we're looking at um some agentic

755
00:28:38,880 --> 00:28:43,039
approaches. um first of all agentic

756
00:28:41,679 --> 00:28:46,799
approaches to this sort of auto

757
00:28:43,039 --> 00:28:48,880
evaluation um challenge but also to um

758
00:28:46,799 --> 00:28:50,799
different sections of our of our tool

759
00:28:48,880 --> 00:28:53,600
and how we can build that in to to

760
00:28:50,799 --> 00:28:56,159
improve efficiency and um and quality of

761
00:28:53,600 --> 00:28:58,559
our outputs. Um we're really excited to

762
00:28:56,159 --> 00:29:00,000
answer some questions at the end. Um but

763
00:28:58,559 --> 00:29:01,520
if you have any other further questions

764
00:29:00,000 --> 00:29:03,039
after this then please don't hesitate to

765
00:29:01,520 --> 00:29:04,320
get in touch. Our email addresses are up

766
00:29:03,039 --> 00:29:06,799
on the screen.

767
00:29:04,320 --> 00:29:09,919
>> Wonderful. Thank you so much. that we

768
00:29:06,799 --> 00:29:12,080
will now enter the discussion between

769
00:29:09,919 --> 00:29:14,240
the respondent and the co-authors before

770
00:29:12,080 --> 00:29:16,399
opening up to your questions. Again,

771
00:29:14,240 --> 00:29:18,640
make use of that Q&A function to let

772
00:29:16,399 --> 00:29:21,760
your comments and questions and concerns

773
00:29:18,640 --> 00:29:25,760
and enthusiasm be heard. Um, and we will

774
00:29:21,760 --> 00:29:28,080
circle back to them. So, I will stop

775
00:29:25,760 --> 00:29:30,399
that and now we have Nick in

776
00:29:28,080 --> 00:29:33,440
conversation with Angela, Josh, Hannah

777
00:29:30,399 --> 00:29:36,000
Beth, and Margot.

778
00:29:33,440 --> 00:29:38,480
>> Awesome. Thank you so much uh for that

779
00:29:36,000 --> 00:29:41,840
wonderful work and uh for sharing it.

780
00:29:38,480 --> 00:29:44,240
And I want to start with you know just a

781
00:29:41,840 --> 00:29:45,919
small question for Angela and Josh

782
00:29:44,240 --> 00:29:48,320
because I know you love these little

783
00:29:45,919 --> 00:29:51,840
questions. Um one of the things that you

784
00:29:48,320 --> 00:29:53,600
point to in the paper is um the idea

785
00:29:51,840 --> 00:29:55,520
that that literacies are socially

786
00:29:53,600 --> 00:29:57,840
socially constructed. They're culturally

787
00:29:55,520 --> 00:30:01,520
constructed and that they vary around

788
00:29:57,840 --> 00:30:03,840
the world. you noted that um but we also

789
00:30:01,520 --> 00:30:07,200
know that the concept of literacy has

790
00:30:03,840 --> 00:30:08,880
been used harmfully throughout history

791
00:30:07,200 --> 00:30:11,120
in a number of different kind of

792
00:30:08,880 --> 00:30:15,039
settings and I'm wondering about the

793
00:30:11,120 --> 00:30:16,399
idea of AI literacy if if we don't get

794
00:30:15,039 --> 00:30:19,120
to the point where the stuff that you're

795
00:30:16,399 --> 00:30:21,120
pushing back against in considering it

796
00:30:19,120 --> 00:30:22,559
as a multi- as a pluralistic kind of

797
00:30:21,120 --> 00:30:24,320
approach to literacies if we think of it

798
00:30:22,559 --> 00:30:28,080
in this binary you either have it or you

799
00:30:24,320 --> 00:30:31,679
don't might we see a world that pushes

800
00:30:28,080 --> 00:30:34,960
is that where AI literacy becomes

801
00:30:31,679 --> 00:30:36,559
another kind of tool for preventing

802
00:30:34,960 --> 00:30:39,919
people from access having access to

803
00:30:36,559 --> 00:30:42,720
things and and might um might openness

804
00:30:39,919 --> 00:30:45,200
be an antidote to that?

805
00:30:42,720 --> 00:30:47,679
>> I I love this question. I could probably

806
00:30:45,200 --> 00:30:50,480
fill up a full couple of hours talking

807
00:30:47,679 --> 00:30:55,520
about this both from a challenges and an

808
00:30:50,480 --> 00:30:57,760
opportunities perspective. Um there um

809
00:30:55,520 --> 00:31:01,360
have been many many many conversations

810
00:30:57,760 --> 00:31:04,399
around us validating the outputs of AI

811
00:31:01,360 --> 00:31:06,799
and really looking at whose privileged

812
00:31:04,399 --> 00:31:08,799
uh whose power is on display based on

813
00:31:06,799 --> 00:31:12,159
the outputs but also how are we thinking

814
00:31:08,799 --> 00:31:13,679
about training these models um around uh

815
00:31:12,159 --> 00:31:17,120
what Josh had mentioned before around

816
00:31:13,679 --> 00:31:20,799
the ethical uh pieces the equitable

817
00:31:17,120 --> 00:31:24,399
pieces and the and the impact and um

818
00:31:20,799 --> 00:31:26,880
when we think about AI literacies uh

819
00:31:24,399 --> 00:31:30,640
within this plurality.

820
00:31:26,880 --> 00:31:33,360
What it does is it also allows us to get

821
00:31:30,640 --> 00:31:36,320
back to our values and those values are

822
00:31:33,360 --> 00:31:38,720
defined within communities. Certainly um

823
00:31:36,320 --> 00:31:41,039
the open community has very specific

824
00:31:38,720 --> 00:31:44,720
values that we're that we're using and

825
00:31:41,039 --> 00:31:48,080
that we are um talking about within our

826
00:31:44,720 --> 00:31:51,440
perspectives and our practices both. So

827
00:31:48,080 --> 00:31:54,559
yes, I I think that while we're in this

828
00:31:51,440 --> 00:31:57,840
very uh tense space and in this tense

829
00:31:54,559 --> 00:32:00,559
moment in time, having conversations

830
00:31:57,840 --> 00:32:02,799
around vocabulary, which was our hope

831
00:32:00,559 --> 00:32:04,559
with uh creating a taxonomy and not a

832
00:32:02,799 --> 00:32:08,159
framework is the way that we can start

833
00:32:04,559 --> 00:32:10,720
to chip away um at what is in support of

834
00:32:08,159 --> 00:32:13,120
our values. uh whether it's how tools

835
00:32:10,720 --> 00:32:15,919
are used but also how tools are created

836
00:32:13,120 --> 00:32:19,039
and contextualized and what that tool

837
00:32:15,919 --> 00:32:22,399
usage means from a community or a

838
00:32:19,039 --> 00:32:25,039
cultural aspect. Um we have to start

839
00:32:22,399 --> 00:32:27,919
with that dialogic moment before we can

840
00:32:25,039 --> 00:32:31,519
get to determining okay and now is how

841
00:32:27,919 --> 00:32:33,440
we actually um use these tools uh from

842
00:32:31,519 --> 00:32:37,159
from sort of a more technical or

843
00:32:33,440 --> 00:32:37,159
operational perspective.

844
00:32:38,799 --> 00:32:43,440
Wonderful. Um

845
00:32:41,279 --> 00:32:46,799
I think

846
00:32:43,440 --> 00:32:50,240
uh a question for Hannah Beth and Margo

847
00:32:46,799 --> 00:32:52,960
then uh you were working with a pool of

848
00:32:50,240 --> 00:32:54,960
content that kind of belonged to a

849
00:32:52,960 --> 00:32:59,200
particular community.

850
00:32:54,960 --> 00:33:02,640
um you know and I'm wondering whether

851
00:32:59,200 --> 00:33:05,360
uh that made a difference in how you

852
00:33:02,640 --> 00:33:07,760
trained that model and kind of the the

853
00:33:05,360 --> 00:33:10,240
quality of the output that you that you

854
00:33:07,760 --> 00:33:12,880
saw. Um, also wondering a little bit

855
00:33:10,240 --> 00:33:14,799
about uh one of the things we talk about

856
00:33:12,880 --> 00:33:17,120
in the open community is as as a

857
00:33:14,799 --> 00:33:20,559
challenge is we've openly licensed

858
00:33:17,120 --> 00:33:22,960
material for the good of everyone and

859
00:33:20,559 --> 00:33:26,559
lots of these uh AI companies come along

860
00:33:22,960 --> 00:33:29,120
and hoover it up as a um as a part of

861
00:33:26,559 --> 00:33:31,360
what they do. And in this case, you have

862
00:33:29,120 --> 00:33:33,840
a a a group of content that was already

863
00:33:31,360 --> 00:33:35,600
designed to be used by that community.

864
00:33:33,840 --> 00:33:39,200
wondering how you thought about kind of

865
00:33:35,600 --> 00:33:42,240
consent and and uh using that content

866
00:33:39,200 --> 00:33:44,080
for training the AA tool.

867
00:33:42,240 --> 00:33:45,279
>> Yeah, that's a great question. Um Marggo

868
00:33:44,080 --> 00:33:46,960
might be able to jump in on some of the

869
00:33:45,279 --> 00:33:49,440
sort of more tech technical aspects of

870
00:33:46,960 --> 00:33:50,880
this. Um there was a question in the

871
00:33:49,440 --> 00:33:52,480
chat about whether we're using a sort of

872
00:33:50,880 --> 00:33:55,200
underlying large language model and at

873
00:33:52,480 --> 00:33:56,799
the moment we're using GPT40. Um we've

874
00:33:55,200 --> 00:33:58,640
tried to make the tool as sort of model

875
00:33:56,799 --> 00:34:00,320
agnostic as possible so that we can move

876
00:33:58,640 --> 00:34:02,159
to different models or potentially move

877
00:34:00,320 --> 00:34:03,679
different aspects of to different models

878
00:34:02,159 --> 00:34:06,320
as we go forward.

879
00:34:03,679 --> 00:34:07,840
Um, but we're not at the moment training

880
00:34:06,320 --> 00:34:09,679
a model underneath. We're using

881
00:34:07,840 --> 00:34:12,639
retrieval augmented generation to bring

882
00:34:09,679 --> 00:34:16,079
in content from our corpus um to improve

883
00:34:12,639 --> 00:34:17,520
the accuracy of of the tool. Um, and I

884
00:34:16,079 --> 00:34:20,079
think for us it was that sort of

885
00:34:17,520 --> 00:34:21,599
situating it in our local context was

886
00:34:20,079 --> 00:34:24,879
really important. We wanted to make sure

887
00:34:21,599 --> 00:34:27,359
that it was designed um for UK schools

888
00:34:24,879 --> 00:34:29,919
um specifically for the UK and that

889
00:34:27,359 --> 00:34:32,240
context here and our national curriculum

890
00:34:29,919 --> 00:34:34,320
um but also for for that sort of

891
00:34:32,240 --> 00:34:35,679
specific age range of pupils and and

892
00:34:34,320 --> 00:34:37,280
making sure the content was really

893
00:34:35,679 --> 00:34:39,200
appropriate for them. I think there's

894
00:34:37,280 --> 00:34:41,359
nothing more frustrating as a teacher um

895
00:34:39,200 --> 00:34:42,639
as using an AI tool that then produces

896
00:34:41,359 --> 00:34:44,240
something that's totally irrelevant to

897
00:34:42,639 --> 00:34:47,200
what you you need to teach as part of

898
00:34:44,240 --> 00:34:48,560
your curriculum. Um I think for us we

899
00:34:47,200 --> 00:34:50,639
were in a really fortunate position

900
00:34:48,560 --> 00:34:52,639
where we had this huge corpus of

901
00:34:50,639 --> 00:34:54,480
content. Um and again that's sort of

902
00:34:52,639 --> 00:34:57,119
available for others to make use of

903
00:34:54,480 --> 00:34:59,520
which is really exciting. Um but that

904
00:34:57,119 --> 00:35:01,280
was very specific to our use case and it

905
00:34:59,520 --> 00:35:02,240
enabled us to sort of innovate on top of

906
00:35:01,280 --> 00:35:04,000
that. And I think for other

907
00:35:02,240 --> 00:35:06,720
organizations in other countries and

908
00:35:04,000 --> 00:35:09,040
other regions, um, thinking about how

909
00:35:06,720 --> 00:35:11,040
they're going to, um, sort of source

910
00:35:09,040 --> 00:35:12,480
that high quality underlying content to

911
00:35:11,040 --> 00:35:14,880
use within their tool is really

912
00:35:12,480 --> 00:35:16,400
important, um, for them to be able to

913
00:35:14,880 --> 00:35:18,079
sort of, you know, make sure that that's

914
00:35:16,400 --> 00:35:21,079
aligned to their context and their their

915
00:35:18,079 --> 00:35:21,079
outputs.

916
00:35:23,920 --> 00:35:27,040
>> I one of the things I noticed that was

917
00:35:25,359 --> 00:35:28,880
really interesting in your paper as

918
00:35:27,040 --> 00:35:31,359
well, Hannah Beth and Margo, was that

919
00:35:28,880 --> 00:35:34,160
um, on that first run through

920
00:35:31,359 --> 00:35:37,200
the AI evaluation agent was kind of

921
00:35:34,160 --> 00:35:38,960
harsher or stricter than uh than the

922
00:35:37,200 --> 00:35:42,240
human evaluators.

923
00:35:38,960 --> 00:35:44,079
And I guess a controversial question for

924
00:35:42,240 --> 00:35:47,200
you is it possible that the AI was

925
00:35:44,079 --> 00:35:51,040
actually applying or better at applying

926
00:35:47,200 --> 00:35:53,839
assessment theory in that situation to

927
00:35:51,040 --> 00:35:57,280
um uh to the design of multiple choice

928
00:35:53,839 --> 00:35:58,800
questions in this in this case. Um, and

929
00:35:57,280 --> 00:36:00,480
I wonder what the implications of that

930
00:35:58,800 --> 00:36:02,560
are down the track if people to were to

931
00:36:00,480 --> 00:36:04,720
use these, you know, perhaps less

932
00:36:02,560 --> 00:36:07,280
critically than than with your technical

933
00:36:04,720 --> 00:36:10,320
knowledge or uh, you know, our other

934
00:36:07,280 --> 00:36:12,400
authors as well. How how does that

935
00:36:10,320 --> 00:36:14,720
what's likely to happen if we kind of

936
00:36:12,400 --> 00:36:16,640
unleash these tools on lots of people

937
00:36:14,720 --> 00:36:18,880
um, who may not know what underlies

938
00:36:16,640 --> 00:36:20,640
them?

939
00:36:18,880 --> 00:36:22,079
Just to add a little bit of context for

940
00:36:20,640 --> 00:36:23,920
people who haven't read the paper, the

941
00:36:22,079 --> 00:36:26,079
things that um the LLM was sort of

942
00:36:23,920 --> 00:36:29,359
overly focusing on um before we changed

943
00:36:26,079 --> 00:36:30,960
it was um you know uh focusing on how

944
00:36:29,359 --> 00:36:33,599
deep the understanding it was testing

945
00:36:30,960 --> 00:36:35,119
was for each of the key stages. Um and

946
00:36:33,599 --> 00:36:36,480
whilst that's a good thing to focus on

947
00:36:35,119 --> 00:36:38,560
and it should be sort of keystage

948
00:36:36,480 --> 00:36:40,079
dependent um and we still gave the

949
00:36:38,560 --> 00:36:43,280
teachers the key stage and we still gave

950
00:36:40,079 --> 00:36:44,800
the prompt the key stage now um it

951
00:36:43,280 --> 00:36:46,720
wasn't a thing that the teachers

952
00:36:44,800 --> 00:36:48,320
themselves were overly focusing on. So

953
00:36:46,720 --> 00:36:51,119
even though it might be valid and maybe

954
00:36:48,320 --> 00:36:52,960
the LLM was doing a genuinely good job,

955
00:36:51,119 --> 00:36:54,640
um we decided to have our standard of

956
00:36:52,960 --> 00:36:58,720
quality based on just what the teachers

957
00:36:54,640 --> 00:37:00,400
themselves would tend to focus on. Um so

958
00:36:58,720 --> 00:37:02,640
yeah, even if the LLM was doing a good

959
00:37:00,400 --> 00:37:04,320
job, um and maybe that's something to

960
00:37:02,640 --> 00:37:06,640
look into like what it tended to focus

961
00:37:04,320 --> 00:37:08,079
on and if that was um applying some

962
00:37:06,640 --> 00:37:10,480
sorts of things that we didn't think

963
00:37:08,079 --> 00:37:12,640
about. Um I think we wanted our standard

964
00:37:10,480 --> 00:37:15,040
for quality to be based on wholly what

965
00:37:12,640 --> 00:37:17,359
the teachers were doing.

966
00:37:15,040 --> 00:37:20,000
I think it's also worth saying that um I

967
00:37:17,359 --> 00:37:22,240
think at points the the LLM was applying

968
00:37:20,000 --> 00:37:25,200
those evaluation criteria with more sort

969
00:37:22,240 --> 00:37:28,160
of um consistency maybe than our humans

970
00:37:25,200 --> 00:37:30,720
were um it enabled us to work out some

971
00:37:28,160 --> 00:37:32,240
categories that that even from the

972
00:37:30,720 --> 00:37:34,720
thematic analysis of human

973
00:37:32,240 --> 00:37:36,400
justifications um that I think if you'd

974
00:37:34,720 --> 00:37:37,920
asked a human what made a good multiple

975
00:37:36,400 --> 00:37:40,640
choice distractor they maybe wouldn't

976
00:37:37,920 --> 00:37:42,000
have identified themselves. Um, but we

977
00:37:40,640 --> 00:37:44,240
were able to pull that out either from

978
00:37:42,000 --> 00:37:45,680
the justifications from the LLM or from

979
00:37:44,240 --> 00:37:47,920
the justifications that were given by

980
00:37:45,680 --> 00:37:50,720
the human and then feed those back into

981
00:37:47,920 --> 00:37:52,880
our into our AI sort of prompt and back

982
00:37:50,720 --> 00:37:54,960
in both for the auto evaluation and for

983
00:37:52,880 --> 00:37:57,359
the um prompt that was helping us to

984
00:37:54,960 --> 00:37:59,119
generate those lesson questions. Um, but

985
00:37:57,359 --> 00:38:00,720
I think it's really important as an

986
00:37:59,119 --> 00:38:02,560
organization to have decided what your

987
00:38:00,720 --> 00:38:04,960
sort of measure of quality is because

988
00:38:02,560 --> 00:38:06,480
you can obviously look at multiple

989
00:38:04,960 --> 00:38:07,680
choice questions and what high quality

990
00:38:06,480 --> 00:38:10,240
looks like from lots of different

991
00:38:07,680 --> 00:38:12,000
aspects. Um, and for us, we wanted to

992
00:38:10,240 --> 00:38:13,520
just make sure that that auto evaluation

993
00:38:12,000 --> 00:38:15,440
was really aligned to what our human

994
00:38:13,520 --> 00:38:17,440
evaluators, our expert teachers were

995
00:38:15,440 --> 00:38:19,599
saying. Um, so that we could start to

996
00:38:17,440 --> 00:38:21,359
generalize some of the the findings of

997
00:38:19,599 --> 00:38:23,520
that to make improvements to our tool

998
00:38:21,359 --> 00:38:26,760
and and to sort of evaluate quality on a

999
00:38:23,520 --> 00:38:26,760
bigger scale.

1000
00:38:27,760 --> 00:38:31,680
Angela and Josh,

1001
00:38:29,839 --> 00:38:34,800
to some extent it sounds like, you know,

1002
00:38:31,680 --> 00:38:37,680
when uh this reminds me a little bit of

1003
00:38:34,800 --> 00:38:39,520
your your idea of the the black hole,

1004
00:38:37,680 --> 00:38:43,200
right? That the the potential for some

1005
00:38:39,520 --> 00:38:47,280
AI is to kind of crush the complexity of

1006
00:38:43,200 --> 00:38:48,800
human nature into it and and what uh

1007
00:38:47,280 --> 00:38:51,920
what Hannibbeth and Marggo were looking

1008
00:38:48,800 --> 00:38:53,680
for is ways to maintain that uh

1009
00:38:51,920 --> 00:38:55,200
complexity.

1010
00:38:53,680 --> 00:38:56,880
What kind of things were you kind of

1011
00:38:55,200 --> 00:38:58,079
seeing from the people that you were

1012
00:38:56,880 --> 00:39:00,000
talking to when you're asking about

1013
00:38:58,079 --> 00:39:02,000
competencies that they were looking for?

1014
00:39:00,000 --> 00:39:03,520
Um were they imagining that as a

1015
00:39:02,000 --> 00:39:06,240
challenge? Were they were they trying to

1016
00:39:03,520 --> 00:39:08,880
hold on to that kind of rich human

1017
00:39:06,240 --> 00:39:12,079
complexity?

1018
00:39:08,880 --> 00:39:13,760
Yeah, I think that um one of the most

1019
00:39:12,079 --> 00:39:15,520
common things that we saw in terms of

1020
00:39:13,760 --> 00:39:17,280
the conversations that we had with the

1021
00:39:15,520 --> 00:39:20,560
educators, especially the open

1022
00:39:17,280 --> 00:39:24,880
practitioners, was that they were

1023
00:39:20,560 --> 00:39:27,040
grappling with the complex uh mixed and

1024
00:39:24,880 --> 00:39:28,320
nuanced emotions that existed within the

1025
00:39:27,040 --> 00:39:32,160
communities within which they were

1026
00:39:28,320 --> 00:39:35,359
serving. Um we've seen this uh

1027
00:39:32,160 --> 00:39:37,760
historically show up within um open

1028
00:39:35,359 --> 00:39:40,079
education as well when we talk about

1029
00:39:37,760 --> 00:39:41,680
lensure and you know in the beginning

1030
00:39:40,079 --> 00:39:44,079
some of the early conversations around

1031
00:39:41,680 --> 00:39:46,320
licensing were hey let's make everything

1032
00:39:44,079 --> 00:39:48,079
open be as open as you possibly can be

1033
00:39:46,320 --> 00:39:50,320
but what we found is that in certain

1034
00:39:48,079 --> 00:39:52,880
geographic areas of the world it was a

1035
00:39:50,320 --> 00:39:55,359
privilege to be uh at a certain level of

1036
00:39:52,880 --> 00:39:57,359
openness or even that you might feel

1037
00:39:55,359 --> 00:39:59,440
that you could be open in a particular

1038
00:39:57,359 --> 00:40:01,599
moment in time in a certain way, but

1039
00:39:59,440 --> 00:40:04,160
based on bad actors, based on changes in

1040
00:40:01,599 --> 00:40:06,000
the world, based on politics and the

1041
00:40:04,160 --> 00:40:08,079
climate within which you were living, it

1042
00:40:06,000 --> 00:40:10,720
was no longer safe for you to be open or

1043
00:40:08,079 --> 00:40:13,760
your values or feelings may have changed

1044
00:40:10,720 --> 00:40:16,000
around what openness could be for you.

1045
00:40:13,760 --> 00:40:18,560
We're seeing the same thing with AI.

1046
00:40:16,000 --> 00:40:20,480
We're seeing that um any sort of

1047
00:40:18,560 --> 00:40:23,760
monolithic approaches that we're taking

1048
00:40:20,480 --> 00:40:25,520
where we're trying to um determine how

1049
00:40:23,760 --> 00:40:27,280
everybody collectively feels and we see

1050
00:40:25,520 --> 00:40:29,599
this even with students. You know, all

1051
00:40:27,280 --> 00:40:32,400
students are using AI to cheat. uh you

1052
00:40:29,599 --> 00:40:35,760
know those sweeping judgments that we're

1053
00:40:32,400 --> 00:40:38,720
making don't really reflect not only the

1054
00:40:35,760 --> 00:40:42,960
the complex feelings that folks have

1055
00:40:38,720 --> 00:40:45,280
around AI and its usage um but also how

1056
00:40:42,960 --> 00:40:47,280
those feelings are changing over time

1057
00:40:45,280 --> 00:40:48,960
especially as AI is evolving and

1058
00:40:47,280 --> 00:40:51,119
changing. A tool that you may support

1059
00:40:48,960 --> 00:40:54,480
one day uh may not be one that you

1060
00:40:51,119 --> 00:40:56,480
support later on. So, um, I I love the

1061
00:40:54,480 --> 00:40:59,040
work that Hannah Beth and Margot are

1062
00:40:56,480 --> 00:41:02,640
doing and along with their team because

1063
00:40:59,040 --> 00:41:05,440
they're really bridging that gap between

1064
00:41:02,640 --> 00:41:06,880
the the operational piece and the the

1065
00:41:05,440 --> 00:41:09,359
building of something tangible and

1066
00:41:06,880 --> 00:41:11,839
active, but also creating space for

1067
00:41:09,359 --> 00:41:14,240
folks to talk about how do we feel about

1068
00:41:11,839 --> 00:41:17,920
this usage and how is this usage

1069
00:41:14,240 --> 00:41:20,000
changing how we think about AI?

1070
00:41:17,920 --> 00:41:21,920
And I just to add to to what Angela said

1071
00:41:20,000 --> 00:41:25,200
there is is this idea of the technology

1072
00:41:21,920 --> 00:41:29,760
is not going to be the determiner right

1073
00:41:25,200 --> 00:41:32,880
um that it's it is a discourse-based um

1074
00:41:29,760 --> 00:41:36,640
plurality of experiences and as the map

1075
00:41:32,880 --> 00:41:39,119
showed um folks were interested uh in

1076
00:41:36,640 --> 00:41:41,680
different interconnected literacies um

1077
00:41:39,119 --> 00:41:43,760
depending on the region they were in but

1078
00:41:41,680 --> 00:41:47,280
also I would even say it depends on what

1079
00:41:43,760 --> 00:41:49,599
classroom you're in right um as to how

1080
00:41:47,280 --> 00:41:53,280
open and how vulnerable even the faculty

1081
00:41:49,599 --> 00:41:56,079
member is on um how AI is being used and

1082
00:41:53,280 --> 00:41:59,839
and their openness to it and uh so I I

1083
00:41:56,079 --> 00:42:01,839
think that's uh an interesting way to um

1084
00:41:59,839 --> 00:42:03,839
look at that question of you know what's

1085
00:42:01,839 --> 00:42:07,200
the technology going to do rather than

1086
00:42:03,839 --> 00:42:09,760
it's one piece in a conversation and

1087
00:42:07,200 --> 00:42:12,800
even you know uh this idea of

1088
00:42:09,760 --> 00:42:15,680
prioritizing it as an object over uh

1089
00:42:12,800 --> 00:42:18,000
what we bring to the table uh is uh

1090
00:42:15,680 --> 00:42:19,440
troublesome right but I do love the you

1091
00:42:18,000 --> 00:42:21,200
know what what's happening with Ayah

1092
00:42:19,440 --> 00:42:23,599
here is the perfect example of how

1093
00:42:21,200 --> 00:42:26,560
you're iterating based on something like

1094
00:42:23,599 --> 00:42:28,480
the literacies and competencies. So uh

1095
00:42:26,560 --> 00:42:30,880
love it.

1096
00:42:28,480 --> 00:42:33,200
>> I hate to interrupt but there's so many

1097
00:42:30,880 --> 00:42:35,119
juicy questions coming through the Q&A

1098
00:42:33,200 --> 00:42:36,960
that are relevant to a lot of the things

1099
00:42:35,119 --> 00:42:38,319
that people are talking about. So if

1100
00:42:36,960 --> 00:42:40,319
it's okay with the group I'll go ahead

1101
00:42:38,319 --> 00:42:42,400
and start introducing those into our

1102
00:42:40,319 --> 00:42:45,040
conversation. Uh the first question

1103
00:42:42,400 --> 00:42:47,599
we'll highlight is from Pablo Chiaza and

1104
00:42:45,040 --> 00:42:50,079
the question is in teaching that

1105
00:42:47,599 --> 00:42:52,400
requires hands-on practice such as

1106
00:42:50,079 --> 00:42:54,960
technical skills like carpentry, welding

1107
00:42:52,400 --> 00:42:57,440
and blacksmithing, how do you think AI

1108
00:42:54,960 --> 00:42:59,760
can combine theoretical and practical

1109
00:42:57,440 --> 00:43:03,640
teaching and provide feedback on it? And

1110
00:42:59,760 --> 00:43:03,640
this question is for anybody.

1111
00:43:05,920 --> 00:43:10,800
>> Maybe I'll I'll open the floor with this

1112
00:43:08,000 --> 00:43:14,319
one. Um I think it's a great question. I

1113
00:43:10,800 --> 00:43:16,880
think um at the moment within ISA we've

1114
00:43:14,319 --> 00:43:18,720
focused uh initially on sort of

1115
00:43:16,880 --> 00:43:21,839
classroom based lessons with our design

1116
00:43:18,720 --> 00:43:23,440
of the tool um but but actually we've

1117
00:43:21,839 --> 00:43:24,640
done lots of experimentation with people

1118
00:43:23,440 --> 00:43:26,720
who are working in all sorts of

1119
00:43:24,640 --> 00:43:30,000
different context and it will plan

1120
00:43:26,720 --> 00:43:32,240
really great lessons for um in the UK we

1121
00:43:30,000 --> 00:43:34,400
have alternative provisions where pupils

1122
00:43:32,240 --> 00:43:36,640
um aren't able to attend mainstream

1123
00:43:34,400 --> 00:43:38,960
schools normally because of a behavioral

1124
00:43:36,640 --> 00:43:40,640
reason um and the curriculum won't

1125
00:43:38,960 --> 00:43:42,720
follow a sort of traditional

1126
00:43:40,640 --> 00:43:44,480
um national curriculum subject that we

1127
00:43:42,720 --> 00:43:46,800
have here and they might be doing things

1128
00:43:44,480 --> 00:43:48,640
like ship building or planting gardens

1129
00:43:46,800 --> 00:43:51,359
or all sorts of other types of um

1130
00:43:48,640 --> 00:43:53,599
activity um and Isa will take the

1131
00:43:51,359 --> 00:43:56,240
principles that we've we've sort of um

1132
00:43:53,599 --> 00:43:58,000
codified into it and enable um teachers

1133
00:43:56,240 --> 00:43:59,839
to still produce really high quality

1134
00:43:58,000 --> 00:44:02,640
lessons that that they can deliver in

1135
00:43:59,839 --> 00:44:05,839
those contexts. Um I think as with

1136
00:44:02,640 --> 00:44:08,079
everything uh the more um highquality

1137
00:44:05,839 --> 00:44:10,000
examples that you can give AI tools of

1138
00:44:08,079 --> 00:44:12,000
of what sort of exceptional practice

1139
00:44:10,000 --> 00:44:13,920
looks like in that area the better the

1140
00:44:12,000 --> 00:44:15,680
outputs will be and so I think it's a

1141
00:44:13,920 --> 00:44:17,839
really exciting spa space that will

1142
00:44:15,680 --> 00:44:20,400
hopefully um develop lots as we go

1143
00:44:17,839 --> 00:44:22,000
forward. Um there was also a sort of

1144
00:44:20,400 --> 00:44:24,640
mention of how do we then provide

1145
00:44:22,000 --> 00:44:28,240
feedback. I'm assuming that that's maybe

1146
00:44:24,640 --> 00:44:30,800
on um on outputs of of what people are

1147
00:44:28,240 --> 00:44:32,240
doing. Um there's a really interesting

1148
00:44:30,800 --> 00:44:35,680
uh sort of experiment happening at the

1149
00:44:32,240 --> 00:44:36,880
moment with um uh a government funded

1150
00:44:35,680 --> 00:44:40,079
project where they're looking at

1151
00:44:36,880 --> 00:44:44,000
soldering and soldering of um different

1152
00:44:40,079 --> 00:44:45,839
sort of uh technical devices I think um

1153
00:44:44,000 --> 00:44:47,760
and they're using AI to provide feedback

1154
00:44:45,839 --> 00:44:50,079
to the pupils on their on their

1155
00:44:47,760 --> 00:44:51,440
soldering and um how they've gone about

1156
00:44:50,079 --> 00:44:53,200
implementing that. So I think we'll see

1157
00:44:51,440 --> 00:44:55,119
lots of innovation in that space going

1158
00:44:53,200 --> 00:44:57,440
forward. But I think that really high

1159
00:44:55,119 --> 00:44:59,280
quality um examples that we can give to

1160
00:44:57,440 --> 00:45:00,240
the AI model is is really integral

1161
00:44:59,280 --> 00:45:02,160
there.

1162
00:45:00,240 --> 00:45:04,560
>> Just super briefly, I want to plus one

1163
00:45:02,160 --> 00:45:06,400
that and add that uh Josh and I in

1164
00:45:04,560 --> 00:45:07,599
addition to coming from a teaching

1165
00:45:06,400 --> 00:45:09,200
background, we also come from an

1166
00:45:07,599 --> 00:45:11,280
instructional design background and some

1167
00:45:09,200 --> 00:45:13,680
of the early conversations around AI

1168
00:45:11,280 --> 00:45:15,920
were, you know, how can AI replace

1169
00:45:13,680 --> 00:45:18,560
teachers? How can AI replace or will it

1170
00:45:15,920 --> 00:45:20,000
replace instructional designers? But um

1171
00:45:18,560 --> 00:45:22,400
what we're seeing from the research and

1172
00:45:20,000 --> 00:45:25,040
from practice is that AI actually can be

1173
00:45:22,400 --> 00:45:26,800
used as a tool to enable us to get to

1174
00:45:25,040 --> 00:45:28,319
the higher order thinking, the active

1175
00:45:26,800 --> 00:45:31,440
learning and the experiential learning

1176
00:45:28,319 --> 00:45:34,240
that we've been begging our field to get

1177
00:45:31,440 --> 00:45:36,240
to for for quite some time. So thinking

1178
00:45:34,240 --> 00:45:37,920
through what are the assessments that

1179
00:45:36,240 --> 00:45:40,960
we're creating, what are the intended

1180
00:45:37,920 --> 00:45:44,720
learning outcomes, and how can AI be a

1181
00:45:40,960 --> 00:45:46,960
co-partner, a coach, a mentor, um that

1182
00:45:44,720 --> 00:45:49,680
guides us into that active learning and

1183
00:45:46,960 --> 00:45:52,000
that learning that is um getting us to

1184
00:45:49,680 --> 00:45:54,640
the the deeper funds of knowledge that

1185
00:45:52,000 --> 00:45:56,880
we've we've wanted to hit um is is

1186
00:45:54,640 --> 00:45:58,400
absolutely uh on the table, on the

1187
00:45:56,880 --> 00:46:02,480
horizon, and we're seeing more and more

1188
00:45:58,400 --> 00:46:05,280
educators heading in that direction.

1189
00:46:02,480 --> 00:46:06,880
The um the next question I see Angela

1190
00:46:05,280 --> 00:46:08,720
you wanted to answer live and I think

1191
00:46:06,880 --> 00:46:10,720
it's a fascinating question. I think we

1192
00:46:08,720 --> 00:46:12,720
could open it up to everybody as well

1193
00:46:10,720 --> 00:46:15,599
and that's from an anonymous attendee

1194
00:46:12,720 --> 00:46:18,079
who asks if one intends to develop some

1195
00:46:15,599 --> 00:46:20,319
curriculum to develop AI literacy among

1196
00:46:18,079 --> 00:46:22,400
students and faculty given the

1197
00:46:20,319 --> 00:46:25,040
dimensions it would have to be very

1198
00:46:22,400 --> 00:46:26,640
comprehensive. What do you see as the

1199
00:46:25,040 --> 00:46:28,640
starting point and what could the

1200
00:46:26,640 --> 00:46:31,760
minimal starting point be?

1201
00:46:28,640 --> 00:46:35,040
>> I I adore this question. I know that

1202
00:46:31,760 --> 00:46:37,280
when you're looking out at what needs to

1203
00:46:35,040 --> 00:46:40,160
be done, it can feel like a massive

1204
00:46:37,280 --> 00:46:43,760
challenge and and pretty deflating. Uh

1205
00:46:40,160 --> 00:46:46,000
but one thing that I always guide people

1206
00:46:43,760 --> 00:46:48,319
into doing is starting from a place of

1207
00:46:46,000 --> 00:46:51,359
strengths, looking at current strengths

1208
00:46:48,319 --> 00:46:54,240
and then moving into intended outcomes.

1209
00:46:51,359 --> 00:46:57,200
So when you think about AI literacy's

1210
00:46:54,240 --> 00:47:00,240
development uh not only amongst learners

1211
00:46:57,200 --> 00:47:03,680
but also amongst faculty and the leaders

1212
00:47:00,240 --> 00:47:06,160
the administrators um getting to know

1213
00:47:03,680 --> 00:47:08,640
your stakeholders in which AI literacies

1214
00:47:06,160 --> 00:47:10,960
are appearing naturally in their work

1215
00:47:08,640 --> 00:47:14,160
perhaps have been happening um

1216
00:47:10,960 --> 00:47:16,720
undercover and surfacing them uh is a

1217
00:47:14,160 --> 00:47:19,359
really empowering way to start and then

1218
00:47:16,720 --> 00:47:22,560
from there uh we recommend that you move

1219
00:47:19,359 --> 00:47:24,560
towards which AI literacy ies can be

1220
00:47:22,560 --> 00:47:26,640
incorporated naturally through

1221
00:47:24,560 --> 00:47:30,240
professional development through remix

1222
00:47:26,640 --> 00:47:33,920
practices uh to lead to that uh AI

1223
00:47:30,240 --> 00:47:35,680
literacy's development over time knowing

1224
00:47:33,920 --> 00:47:37,200
that there's never going to be a finish

1225
00:47:35,680 --> 00:47:39,280
line there's never going to be a moment

1226
00:47:37,200 --> 00:47:41,359
where we say oh we're done let's

1227
00:47:39,280 --> 00:47:43,599
high-five uh we're all we're all

1228
00:47:41,359 --> 00:47:45,520
literate um there's no baggage that we

1229
00:47:43,599 --> 00:47:48,079
have to carry in terms of a point of

1230
00:47:45,520 --> 00:47:51,440
perfection or completion uh because our

1231
00:47:48,079 --> 00:47:53,359
AI literacies development is ongoing And

1232
00:47:51,440 --> 00:47:55,760
um we put the link in the chat earlier.

1233
00:47:53,359 --> 00:47:57,440
We should put it in there again, but uh

1234
00:47:55,760 --> 00:48:00,079
as part of the findings of this

1235
00:47:57,440 --> 00:48:02,319
research, we actually put together um a

1236
00:48:00,079 --> 00:48:03,839
free miniourse that's online. Anybody

1237
00:48:02,319 --> 00:48:05,680
can take it. We have educators from all

1238
00:48:03,839 --> 00:48:08,079
over the globe that are taking it that

1239
00:48:05,680 --> 00:48:09,920
really gives you some practical ways to

1240
00:48:08,079 --> 00:48:11,440
get started on just this question, not

1241
00:48:09,920 --> 00:48:12,800
just the theoretical pieces that I

1242
00:48:11,440 --> 00:48:15,359
shared just now.

1243
00:48:12,800 --> 00:48:16,640
>> Fantastic. Marco and Hannah Beth, is

1244
00:48:15,359 --> 00:48:20,200
there anything you want to add for that

1245
00:48:16,640 --> 00:48:20,200
question or Nick?

1246
00:48:20,960 --> 00:48:24,319
just I think it's going to become a

1247
00:48:22,240 --> 00:48:26,319
really important part of the curriculum

1248
00:48:24,319 --> 00:48:27,920
across schools and I think sort of

1249
00:48:26,319 --> 00:48:30,400
starting that really young and making

1250
00:48:27,920 --> 00:48:32,800
sure that that's part of um sort of how

1251
00:48:30,400 --> 00:48:35,119
we're teaching pupils to use sort of

1252
00:48:32,800 --> 00:48:37,040
technology safely and and responsibly is

1253
00:48:35,119 --> 00:48:38,240
is really important and so it's really

1254
00:48:37,040 --> 00:48:40,400
exciting to hear that that work's

1255
00:48:38,240 --> 00:48:43,119
already happening.

1256
00:48:40,400 --> 00:48:46,559
Our next question comes from Diego

1257
00:48:43,119 --> 00:48:49,520
Viauses who asks, "How can we encourage

1258
00:48:46,559 --> 00:48:52,240
the use of AI in the social sciences

1259
00:48:49,520 --> 00:48:55,440
among those who oppose it due to its

1260
00:48:52,240 --> 00:49:00,119
gender and cultural biases?"

1261
00:48:55,440 --> 00:49:00,119
And this is a wild card for everybody.

1262
00:49:00,559 --> 00:49:04,160
>> I'm happy to I keep on jumping in all of

1263
00:49:02,880 --> 00:49:06,319
>> No, kick us off. I think that would be

1264
00:49:04,160 --> 00:49:07,359
great. Do it. Yeah, these are great

1265
00:49:06,319 --> 00:49:10,000
questions.

1266
00:49:07,359 --> 00:49:12,000
um we build the environments that um

1267
00:49:10,000 --> 00:49:14,160
that we live within. We've built these

1268
00:49:12,000 --> 00:49:18,800
structures over time. We've imbued these

1269
00:49:14,160 --> 00:49:22,000
structures with our biases um with our

1270
00:49:18,800 --> 00:49:25,839
um both on and unknown or known and

1271
00:49:22,000 --> 00:49:28,880
unknown discriminatory practices. So the

1272
00:49:25,839 --> 00:49:32,800
work here is not um avoiding and sitting

1273
00:49:28,880 --> 00:49:35,040
back, but it's being proactive, being um

1274
00:49:32,800 --> 00:49:38,880
open about the challenges that exist and

1275
00:49:35,040 --> 00:49:41,119
exist for you specifically. Um, I I

1276
00:49:38,880 --> 00:49:43,680
definitely wear my identity into all of

1277
00:49:41,119 --> 00:49:45,839
the work that I go into as a bipok woman

1278
00:49:43,680 --> 00:49:48,800
from the global north, uh, native

1279
00:49:45,839 --> 00:49:50,800
English speaker, and I I let all of that

1280
00:49:48,800 --> 00:49:53,040
be known in all of the work that I put

1281
00:49:50,800 --> 00:49:55,440
out there so that I can signal to other

1282
00:49:53,040 --> 00:49:58,240
people that they can and should be open

1283
00:49:55,440 --> 00:50:01,599
about their perspective. So, we have a

1284
00:49:58,240 --> 00:50:04,960
call right now um to to do better. And

1285
00:50:01,599 --> 00:50:07,440
again, um, the open community has very

1286
00:50:04,960 --> 00:50:10,240
mature practices for doing this work,

1287
00:50:07,440 --> 00:50:12,720
for creating content that is culturally

1288
00:50:10,240 --> 00:50:15,359
affirming, uh, creating content that

1289
00:50:12,720 --> 00:50:17,599
supports the diverse perspectives and

1290
00:50:15,359 --> 00:50:18,960
lived experiences of all of the

1291
00:50:17,599 --> 00:50:21,680
stakeholders that are involved in

1292
00:50:18,960 --> 00:50:23,920
education. Um, so this is a time for us

1293
00:50:21,680 --> 00:50:25,520
to lean in and the only way that we can

1294
00:50:23,920 --> 00:50:26,800
signal to other people that it's safe to

1295
00:50:25,520 --> 00:50:28,400
lean in is if we're doing that

1296
00:50:26,800 --> 00:50:29,040
ourselves.

1297
00:50:28,400 --> 00:50:31,119
I think

1298
00:50:29,040 --> 00:50:33,359
>> I love the Sorry,

1299
00:50:31,119 --> 00:50:35,280
>> sorry. I say I think that it highlights

1300
00:50:33,359 --> 00:50:36,960
why evaluation is so important in the

1301
00:50:35,280 --> 00:50:39,280
first place. Like cultural bias is

1302
00:50:36,960 --> 00:50:41,040
something that we evaluate for and so

1303
00:50:39,280 --> 00:50:46,359
doing that allows you to be proactive

1304
00:50:41,040 --> 00:50:46,359
and sort of um check as you go.

1305
00:50:46,480 --> 00:50:51,839
I was just going to add to to that piece

1306
00:50:48,960 --> 00:50:55,520
that that both of you spoke so so um

1307
00:50:51,839 --> 00:50:58,559
particularly about is this um need to be

1308
00:50:55,520 --> 00:51:00,240
proactive but also um vulnerable. Uh

1309
00:50:58,559 --> 00:51:03,520
Angela used the word safe, right? It's

1310
00:51:00,240 --> 00:51:04,800
safe uh to to talk about uh what we're

1311
00:51:03,520 --> 00:51:06,960
bringing to the table. I remember when

1312
00:51:04,800 --> 00:51:09,520
we were putting together um the UNESCO

1313
00:51:06,960 --> 00:51:11,359
report in in particular, Angeline had a

1314
00:51:09,520 --> 00:51:13,200
lot of conversation about what we were

1315
00:51:11,359 --> 00:51:15,599
bringing to the conversation and trying

1316
00:51:13,200 --> 00:51:17,839
to represent a global perspective,

1317
00:51:15,599 --> 00:51:20,319
right? And I think that's u part of the

1318
00:51:17,839 --> 00:51:21,839
important pieces behind the scenes is

1319
00:51:20,319 --> 00:51:26,800
that you are proactive, you're thinking

1320
00:51:21,839 --> 00:51:28,960
about that intentionally and that um uh

1321
00:51:26,800 --> 00:51:31,520
from the classroom to the strategy

1322
00:51:28,960 --> 00:51:33,520
level, right? folks are uh open about

1323
00:51:31,520 --> 00:51:36,160
challenges and opportunities, gains and

1324
00:51:33,520 --> 00:51:39,640
losses uh that happens within any uh

1325
00:51:36,160 --> 00:51:39,640
digital transformation.

1326
00:51:41,760 --> 00:51:46,720
>> I'm noticing that there's um there's a

1327
00:51:44,319 --> 00:51:48,559
couple of other authors from the

1328
00:51:46,720 --> 00:51:51,040
initiative that are posting some really

1329
00:51:48,559 --> 00:51:53,200
wonderful questions um in the chat that

1330
00:51:51,040 --> 00:51:54,640
we are answering by text or that are

1331
00:51:53,200 --> 00:51:57,680
being touched on. So, I'm going to go

1332
00:51:54,640 --> 00:51:59,839
ahead and ask our team to put um the

1333
00:51:57,680 --> 00:52:02,480
links to those other author's papers in

1334
00:51:59,839 --> 00:52:05,440
the chat for people to read from Jo and

1335
00:52:02,480 --> 00:52:07,280
Dan Magcguire as well. Um our next

1336
00:52:05,440 --> 00:52:08,800
question, there's actually two questions

1337
00:52:07,280 --> 00:52:11,920
related to this and I'll read the one

1338
00:52:08,800 --> 00:52:13,839
from Roia Mosur. I apologize if I'm

1339
00:52:11,920 --> 00:52:16,960
mispronouncing your name. And the

1340
00:52:13,839 --> 00:52:19,280
question is, as an EFL language teacher,

1341
00:52:16,960 --> 00:52:22,920
I wonder how I could possibly benefit

1342
00:52:19,280 --> 00:52:22,920
from AI.

1343
00:52:25,200 --> 00:52:28,960
Well, I was going to I I've sort of

1344
00:52:27,200 --> 00:52:30,480
answered a very similar one in the in

1345
00:52:28,960 --> 00:52:32,720
the chat. Um Angela might have more to

1346
00:52:30,480 --> 00:52:34,800
add to this, but um we've been talking

1347
00:52:32,720 --> 00:52:37,359
lots about this as being a a great use

1348
00:52:34,800 --> 00:52:39,280
of ISA and um and sort of the ability to

1349
00:52:37,359 --> 00:52:42,319
enable teachers to adapt their content

1350
00:52:39,280 --> 00:52:44,640
for um pupils who are speaking um other

1351
00:52:42,319 --> 00:52:46,720
languages within their their classroom.

1352
00:52:44,640 --> 00:52:48,319
Um at the moment lots of those are

1353
00:52:46,720 --> 00:52:49,680
requiring a teacher to have the sort of

1354
00:52:48,319 --> 00:52:51,440
knowledge and expertise of the

1355
00:52:49,680 --> 00:52:53,760
adaptations that they'd want to make for

1356
00:52:51,440 --> 00:52:55,280
those those specific pupils. Um, but

1357
00:52:53,760 --> 00:52:57,200
we've been talking lots about how we can

1358
00:52:55,280 --> 00:52:59,680
codify best practice in that space, but

1359
00:52:57,200 --> 00:53:01,680
also in the sort of SEND space to make

1360
00:52:59,680 --> 00:53:03,599
those adaptations easier for teachers to

1361
00:53:01,680 --> 00:53:06,640
use, um, and enable us to sort of

1362
00:53:03,599 --> 00:53:09,040
showcase best practice in that area. Um,

1363
00:53:06,640 --> 00:53:10,559
but I feel like that's a a huge sort of

1364
00:53:09,040 --> 00:53:12,880
piece of work that we're we're just

1365
00:53:10,559 --> 00:53:14,559
starting to explore at the at the edges.

1366
00:53:12,880 --> 00:53:16,000
Um, and so it's really exciting that

1367
00:53:14,559 --> 00:53:17,760
that's that's, you know, the

1368
00:53:16,000 --> 00:53:19,119
conversation that you're bringing up.

1369
00:53:17,760 --> 00:53:20,960
Um, and we're really keen to have

1370
00:53:19,119 --> 00:53:22,559
feedback. So if you if you've seen

1371
00:53:20,960 --> 00:53:23,839
things that work really well or if there

1372
00:53:22,559 --> 00:53:25,599
are things that you'd like us to have a

1373
00:53:23,839 --> 00:53:26,720
go with um sort of implementing, please

1374
00:53:25,599 --> 00:53:30,240
get in touch with us. We'd be really

1375
00:53:26,720 --> 00:53:32,720
keen to hear from you.

1376
00:53:30,240 --> 00:53:35,359
So I am going to circle back to one more

1377
00:53:32,720 --> 00:53:36,880
question um that was raised that I'm

1378
00:53:35,359 --> 00:53:38,480
curious how you would say it even though

1379
00:53:36,880 --> 00:53:40,000
we've been talking kind of around it and

1380
00:53:38,480 --> 00:53:42,480
touching on it and it's this question

1381
00:53:40,000 --> 00:53:44,559
from Joel, one of the other co-authors.

1382
00:53:42,480 --> 00:53:46,640
And then we're going to invite the

1383
00:53:44,559 --> 00:53:48,240
initiative's principal investigator uh

1384
00:53:46,640 --> 00:53:49,839
to join the conversation, Christopher

1385
00:53:48,240 --> 00:53:52,319
Capazola, who's also the senior

1386
00:53:49,839 --> 00:53:54,160
associate dean of MIT open learning to

1387
00:53:52,319 --> 00:53:56,559
um to join us for the last five minutes

1388
00:53:54,160 --> 00:53:59,200
in our conversation. Joelle's question

1389
00:53:56,559 --> 00:54:01,760
is, in what ways do you think AI tools

1390
00:53:59,200 --> 00:54:03,440
might redefine the role of teachers in

1391
00:54:01,760 --> 00:54:05,440
education?

1392
00:54:03,440 --> 00:54:09,720
And Nick, feel free to tackle this as

1393
00:54:05,440 --> 00:54:09,720
well since I know you grapple with this.

1394
00:54:15,040 --> 00:54:19,440
I kick us off.

1395
00:54:16,480 --> 00:54:21,200
>> Yeah, recently a teacher. Um I was

1396
00:54:19,440 --> 00:54:23,200
teaching until last summer. So this is a

1397
00:54:21,200 --> 00:54:27,040
very um sort of poignant question for

1398
00:54:23,200 --> 00:54:28,720
me. I think um there's been lots of in

1399
00:54:27,040 --> 00:54:30,640
the UK definitely lots of conversation

1400
00:54:28,720 --> 00:54:32,480
about whether AI is going to replace

1401
00:54:30,640 --> 00:54:36,240
teachers or sort of take over their

1402
00:54:32,480 --> 00:54:37,760
role. Um, I think those of us that sort

1403
00:54:36,240 --> 00:54:41,040
of look at it with that lens are

1404
00:54:37,760 --> 00:54:42,960
probably um, sort of thinking about the

1405
00:54:41,040 --> 00:54:44,240
aspects of what makes great teachers as

1406
00:54:42,960 --> 00:54:46,480
being something quite different to what

1407
00:54:44,240 --> 00:54:48,079
I would consider. Um, I think lots of,

1408
00:54:46,480 --> 00:54:49,920
um, great teaching is about the human

1409
00:54:48,079 --> 00:54:51,760
connection that you make with pupils and

1410
00:54:49,920 --> 00:54:54,880
and that supports pupils in sort of

1411
00:54:51,760 --> 00:54:58,079
progressing and um, sort of achieving

1412
00:54:54,880 --> 00:55:00,800
things during their time at school. Um I

1413
00:54:58,079 --> 00:55:02,240
think AI has the the capacity to be able

1414
00:55:00,800 --> 00:55:04,720
to support teachers with some of the

1415
00:55:02,240 --> 00:55:06,240
things that take them lots of time. Um

1416
00:55:04,720 --> 00:55:08,880
lots of people are leaving the classroom

1417
00:55:06,240 --> 00:55:11,280
in the UK. Workload is really high.

1418
00:55:08,880 --> 00:55:13,599
Teachers are maybe teaching five or six

1419
00:55:11,280 --> 00:55:15,119
lessons a day um across different year

1420
00:55:13,599 --> 00:55:17,040
groups potentially across different

1421
00:55:15,119 --> 00:55:19,119
subjects and planning really high

1422
00:55:17,040 --> 00:55:20,720
quality content for all of the pupils in

1423
00:55:19,119 --> 00:55:23,359
their in their classroom is really

1424
00:55:20,720 --> 00:55:26,240
challenging. Um, and so I think my hope

1425
00:55:23,359 --> 00:55:28,480
is that that the use of AI will help

1426
00:55:26,240 --> 00:55:30,720
teachers to do some of those tasks that

1427
00:55:28,480 --> 00:55:34,160
don't require um, sort of their

1428
00:55:30,720 --> 00:55:36,240
expertise. Um, and that will free up

1429
00:55:34,160 --> 00:55:38,000
their time and capacity to be able to do

1430
00:55:36,240 --> 00:55:39,200
some of those more human aspects of

1431
00:55:38,000 --> 00:55:40,960
teaching that we think are really

1432
00:55:39,200 --> 00:55:43,040
important. I don't think it's great use

1433
00:55:40,960 --> 00:55:44,800
of a teacher's time to be, you know,

1434
00:55:43,040 --> 00:55:46,160
formatting a worksheet or translating

1435
00:55:44,800 --> 00:55:48,240
something from one language into

1436
00:55:46,160 --> 00:55:49,760
another. But I do think thinking really

1437
00:55:48,240 --> 00:55:51,760
carefully about how your pupils are

1438
00:55:49,760 --> 00:55:53,839
going to access a concept or how you're

1439
00:55:51,760 --> 00:55:55,920
going to support a specific pupil who

1440
00:55:53,839 --> 00:55:57,599
maybe have some additional needs is is a

1441
00:55:55,920 --> 00:55:59,119
really integral part of teaching that

1442
00:55:57,599 --> 00:56:00,799
often we just don't have time for

1443
00:55:59,119 --> 00:56:03,280
because we're running from one activity

1444
00:56:00,799 --> 00:56:05,119
to another. Um and so I hope that that

1445
00:56:03,280 --> 00:56:07,520
that's sort of the direction that AI

1446
00:56:05,119 --> 00:56:10,079
heads in.

1447
00:56:07,520 --> 00:56:12,640
>> Thank you. Other thoughts that anyone

1448
00:56:10,079 --> 00:56:15,440
else wants to add?

1449
00:56:12,640 --> 00:56:17,839
ju just that uh I would say that we're

1450
00:56:15,440 --> 00:56:20,240
in a a moment in time where us

1451
00:56:17,839 --> 00:56:23,040
considering this as an opportunity uh

1452
00:56:20,240 --> 00:56:26,160
beyond just a challenge to respond to

1453
00:56:23,040 --> 00:56:28,400
allows us to get to a future of

1454
00:56:26,160 --> 00:56:30,079
education where AI is positioned to

1455
00:56:28,400 --> 00:56:32,400
allow us to do the work that we got into

1456
00:56:30,079 --> 00:56:34,559
education in the in the first place to

1457
00:56:32,400 --> 00:56:37,839
get to that deeper learning uh to

1458
00:56:34,559 --> 00:56:40,000
building those systems of care uh and to

1459
00:56:37,839 --> 00:56:41,680
really innovating in ways that are far

1460
00:56:40,000 --> 00:56:44,319
more inclusive than we've been able to

1461
00:56:41,680 --> 00:56:47,359
be up until this moment. So, I'm excited

1462
00:56:44,319 --> 00:56:49,680
to be on this journey with all of you.

1463
00:56:47,359 --> 00:56:51,920
>> You're here. Thank you so much. So, in

1464
00:56:49,680 --> 00:56:54,720
our final few minutes, I'm bringing in

1465
00:56:51,920 --> 00:56:56,640
uh Christopher Capazola, who is at um

1466
00:56:54,720 --> 00:56:57,920
again our senior associate dean at MIT

1467
00:56:56,640 --> 00:57:00,160
Open Learning, the principal

1468
00:56:57,920 --> 00:57:01,839
investigator of the AI and open

1469
00:57:00,160 --> 00:57:04,880
education speaker series. He's also

1470
00:57:01,839 --> 00:57:06,720
faculty of history at MIT. Uh so, Chris,

1471
00:57:04,880 --> 00:57:08,400
what are you noticing in these last

1472
00:57:06,720 --> 00:57:10,400
couple minutes? And Nick, feel free to

1473
00:57:08,400 --> 00:57:12,720
join us as well.

1474
00:57:10,400 --> 00:57:14,319
Yeah, I would just uh say that in

1475
00:57:12,720 --> 00:57:16,559
response to Shar's last question,

1476
00:57:14,319 --> 00:57:19,040
Hannibbeth and Angela's answers were um

1477
00:57:16,559 --> 00:57:21,760
you know were just uh spoton and really

1478
00:57:19,040 --> 00:57:24,160
sort of um uh addressing that

1479
00:57:21,760 --> 00:57:26,720
intersection between AI and open

1480
00:57:24,160 --> 00:57:29,280
education that motivated us to launch

1481
00:57:26,720 --> 00:57:30,960
this uh this paper series. And I've had

1482
00:57:29,280 --> 00:57:32,799
the privilege to work with the with some

1483
00:57:30,960 --> 00:57:34,640
of the authors and and with our uh with

1484
00:57:32,799 --> 00:57:36,960
our editor Sarah Schwepman and with the

1485
00:57:34,640 --> 00:57:40,079
the whole team here at Open Learning to

1486
00:57:36,960 --> 00:57:41,920
kind of launch this. And this webinar

1487
00:57:40,079 --> 00:57:43,599
really captures what exactly what we

1488
00:57:41,920 --> 00:57:46,559
were trying to do, right? Um the last

1489
00:57:43,599 --> 00:57:49,440
hour has been really both technically

1490
00:57:46,559 --> 00:57:51,200
rich and also culturally thoughtful. Um

1491
00:57:49,440 --> 00:57:54,079
and I was really struck by that, right?

1492
00:57:51,200 --> 00:57:56,640
That I think we're into a phase now u

1493
00:57:54,079 --> 00:57:58,799
you know far beyond where we were in you

1494
00:57:56,640 --> 00:58:00,319
know 2022, right? where where people are

1495
00:57:58,799 --> 00:58:02,480
now talking, you know, really talking

1496
00:58:00,319 --> 00:58:04,559
shop about tools, but also, you know,

1497
00:58:02,480 --> 00:58:06,400
asking and demanding answers to some of

1498
00:58:04,559 --> 00:58:08,559
the hard questions um about how these

1499
00:58:06,400 --> 00:58:10,480
systems will work in the real world that

1500
00:58:08,559 --> 00:58:13,520
we're moving into. Um so, you know, just

1501
00:58:10,480 --> 00:58:15,520
huge thanks um to, you know, to to the

1502
00:58:13,520 --> 00:58:18,160
authors, to the respondents, to about,

1503
00:58:15,520 --> 00:58:20,079
you know, 150 200 people who uh you

1504
00:58:18,160 --> 00:58:22,720
know, tuned in for this. The other part

1505
00:58:20,079 --> 00:58:25,760
is that we really wanted to create a

1506
00:58:22,720 --> 00:58:27,680
global community of practice. Um and so

1507
00:58:25,760 --> 00:58:29,440
I am really excited um that people are

1508
00:58:27,680 --> 00:58:31,359
tuning in you know and I hope that

1509
00:58:29,440 --> 00:58:33,839
people will go to the pub pub platform

1510
00:58:31,359 --> 00:58:35,760
you know use uh use its interactive and

1511
00:58:33,839 --> 00:58:38,400
open features to kind of carry on this

1512
00:58:35,760 --> 00:58:40,720
conversation as we move forward because

1513
00:58:38,400 --> 00:58:42,799
I think many of us are kind of craving

1514
00:58:40,720 --> 00:58:45,920
uh a place to have that conversation at

1515
00:58:42,799 --> 00:58:47,920
the intersection of AI and open um and

1516
00:58:45,920 --> 00:58:49,680
and third you know we called them rapid

1517
00:58:47,920 --> 00:58:51,760
response papers for a reason um first of

1518
00:58:49,680 --> 00:58:54,480
all to thank our authors for for writing

1519
00:58:51,760 --> 00:58:56,640
them um you know rapidly uh but also we

1520
00:58:54,480 --> 00:58:58,799
know u that they will uh very soon be

1521
00:58:56,640 --> 00:59:01,119
out of date. Um so we hope that they

1522
00:58:58,799 --> 00:59:03,839
will be out of date because we are

1523
00:59:01,119 --> 00:59:06,240
advancing um really fast both on the

1524
00:59:03,839 --> 00:59:08,079
technical improvements and on the

1525
00:59:06,240 --> 00:59:10,400
cultural uh and political thought that

1526
00:59:08,079 --> 00:59:12,720
that needs to happen. Um and that's

1527
00:59:10,400 --> 00:59:15,760
exactly what the the open ed community

1528
00:59:12,720 --> 00:59:18,400
uh has tried to do over the last 20 to

1529
00:59:15,760 --> 00:59:20,319
40 years. Um and it's also what what the

1530
00:59:18,400 --> 00:59:22,160
the technical community brings to this

1531
00:59:20,319 --> 00:59:24,559
conversation right now. So a huge thanks

1532
00:59:22,160 --> 00:59:26,640
all around and um and you know the

1533
00:59:24,559 --> 00:59:29,200
reward for for doing a lot of uh great

1534
00:59:26,640 --> 00:59:32,559
work is is to is to do more great work.

1535
00:59:29,200 --> 00:59:34,640
So looking forward to to the to more.

1536
00:59:32,559 --> 00:59:37,119
>> Fantastic. Thank you so much Chris and

1537
00:59:34,640 --> 00:59:38,559
thank you to all of our co-authors and

1538
00:59:37,119 --> 00:59:40,960
everyone that participated in the

1539
00:59:38,559 --> 00:59:44,160
initiative and to our respondent Nick.

1540
00:59:40,960 --> 00:59:46,240
Um we put some ways to stay engaged in

1541
00:59:44,160 --> 00:59:48,319
the chat. We're just so grateful for the

1542
00:59:46,240 --> 00:59:50,640
work that's being done and this um

1543
00:59:48,319 --> 00:59:52,240
public engaged conversation that we are

1544
00:59:50,640 --> 00:59:55,760
having. We look forward to having more

1545
00:59:52,240 --> 00:59:58,559
of them in other forms uh in the future.

1546
00:59:55,760 --> 01:00:00,559
So I will pass the baton to Nick Baker

1547
00:59:58,559 --> 01:00:01,760
to sign us off and to thank everyone

1548
01:00:00,559 --> 01:00:04,640
again.

1549
01:00:01,760 --> 01:00:08,000
>> Thank you so much. Uh this hour went so

1550
01:00:04,640 --> 01:00:10,400
quickly. Um I I want to just say a

1551
01:00:08,000 --> 01:00:13,280
couple of things. one is I I love the

1552
01:00:10,400 --> 01:00:15,920
metaphor of the of literacies as part of

1553
01:00:13,280 --> 01:00:18,880
the night sky as you know that there's

1554
01:00:15,920 --> 01:00:21,920
just such a huge richness of it and I

1555
01:00:18,880 --> 01:00:23,839
think it's important because we tend to

1556
01:00:21,920 --> 01:00:27,040
talk about AI as this monolithic thing

1557
01:00:23,839 --> 01:00:28,880
as well but the reality is that that

1558
01:00:27,040 --> 01:00:30,559
this is a language that we use to

1559
01:00:28,880 --> 01:00:32,160
describe lots and lots of different

1560
01:00:30,559 --> 01:00:34,880
things that are sometimes connected and

1561
01:00:32,160 --> 01:00:37,040
sometimes not and they all require us to

1562
01:00:34,880 --> 01:00:39,839
think about it a little bit differently

1563
01:00:37,040 --> 01:00:42,000
um especially in the way that they're

1564
01:00:39,839 --> 01:00:44,319
that they're applied to our societies.

1565
01:00:42,000 --> 01:00:47,200
And the last thing I want to end with is

1566
01:00:44,319 --> 01:00:50,160
um Andrew Maynard talks about this idea

1567
01:00:47,200 --> 01:00:54,160
of a playground versus a play pen. And I

1568
01:00:50,160 --> 01:00:56,480
think we're I I like this idea of us

1569
01:00:54,160 --> 01:00:58,160
working in a playground as opposed to a

1570
01:00:56,480 --> 01:01:00,799
play pen where someone else has control

1571
01:00:58,160 --> 01:01:02,559
of of what we're allowed to do in there.

1572
01:01:00,799 --> 01:01:04,079
And there are times when we probably

1573
01:01:02,559 --> 01:01:05,920
should be put inside a play pen

1574
01:01:04,079 --> 01:01:08,400
sometimes for the decisions that we

1575
01:01:05,920 --> 01:01:10,960
make. But the playground is this kind of

1576
01:01:08,400 --> 01:01:12,640
rich space where different perspectives

1577
01:01:10,960 --> 01:01:14,240
and values come together where there are

1578
01:01:12,640 --> 01:01:16,400
some scary parts of the playground.

1579
01:01:14,240 --> 01:01:17,920
There is a boundary, you know, roughly

1580
01:01:16,400 --> 01:01:19,599
around the outside and there are some

1581
01:01:17,920 --> 01:01:21,599
teachers there to help us figure out how

1582
01:01:19,599 --> 01:01:24,000
to navigate working with each other in

1583
01:01:21,599 --> 01:01:27,839
that space. And I think that's what um

1584
01:01:24,000 --> 01:01:29,760
AI and openness coming together um kind

1585
01:01:27,839 --> 01:01:32,640
of forms at this point. So, thank you so

1586
01:01:29,760 --> 01:01:35,440
much for for all of your work and for

1587
01:01:32,640 --> 01:01:37,680
continuing this great conversation.

1588
01:01:35,440 --> 01:01:40,079
>> Wonderful. Thank you again, everyone.

1589
01:01:37,680 --> 01:01:44,839
Um, and we'll see you next time,

1590
01:01:40,079 --> 01:01:44,839
whatever shape and form that may take.

