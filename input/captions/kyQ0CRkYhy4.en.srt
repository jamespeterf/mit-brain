1
00:00:00,000 --> 00:00:02,000
All

2
00:00:16,640 --> 00:00:20,960
right. So today's lecture introduction

3
00:00:18,400 --> 00:00:22,480
to neural networks and deep learning. Um

4
00:00:20,960 --> 00:00:24,640
so we'll start with a very quick intro

5
00:00:22,480 --> 00:00:26,000
to these things. Um and then we'll

6
00:00:24,640 --> 00:00:28,160
switch and dive deep into neural

7
00:00:26,000 --> 00:00:30,240
networks. All right. So the field of AI

8
00:00:28,160 --> 00:00:31,439
originated in 1956. Sadly didn't

9
00:00:30,240 --> 00:00:33,440
originate at MIT. It originated at

10
00:00:31,439 --> 00:00:35,200
Dartmouth because all these people got

11
00:00:33,440 --> 00:00:36,960
together at Dartmouth. I guess it's it's

12
00:00:35,200 --> 00:00:39,520
got a nice quad or whatever. They got

13
00:00:36,960 --> 00:00:41,360
together. They defined the field. But

14
00:00:39,520 --> 00:00:44,000
fortunately for us, MIT was very well

15
00:00:41,360 --> 00:00:46,399
represented. So we have Marvin Minsky

16
00:00:44,000 --> 00:00:49,120
who founded the MIT AI lab. John

17
00:00:46,399 --> 00:00:51,440
McCarthy who invented lisp and then

18
00:00:49,120 --> 00:00:53,039
later defected to the west coast and

19
00:00:51,440 --> 00:00:54,480
then Claude Shannon who invented

20
00:00:53,039 --> 00:00:56,480
information theory right who was a

21
00:00:54,480 --> 00:00:57,840
professor at MIT. So MIT was well

22
00:00:56,480 --> 00:00:59,520
represented. These folks, you know,

23
00:00:57,840 --> 00:01:01,600
founded the field and they were so

24
00:00:59,520 --> 00:01:04,320
bright. They thought that AI was going

25
00:01:01,600 --> 00:01:06,640
to be substantially solved quote unquote

26
00:01:04,320 --> 00:01:09,119
by that fall.

27
00:01:06,640 --> 00:01:10,960
Okay. Now, obviously it turned out a bit

28
00:01:09,119 --> 00:01:13,119
differently than what they expected. Uh

29
00:01:10,960 --> 00:01:15,119
so it's been whatever 67 68 years since

30
00:01:13,119 --> 00:01:17,520
its founding. So it's gone through

31
00:01:15,119 --> 00:01:19,520
essentially in my opinion three seminal

32
00:01:17,520 --> 00:01:20,880
breakthroughs. Um starting with the

33
00:01:19,520 --> 00:01:22,080
traditional approach then machine

34
00:01:20,880 --> 00:01:24,320
learning, deep learning and generative

35
00:01:22,080 --> 00:01:25,680
AI. So let's take a very quick look at

36
00:01:24,320 --> 00:01:28,400
each of these breakthroughs and what

37
00:01:25,680 --> 00:01:30,240
motivated them. So let's start with the

38
00:01:28,400 --> 00:01:33,200
traditional approach to AI. And so what

39
00:01:30,240 --> 00:01:35,840
is AI? AI informally is the ability to

40
00:01:33,200 --> 00:01:37,520
imbue computers with the the the ability

41
00:01:35,840 --> 00:01:39,119
to do things that only humans can

42
00:01:37,520 --> 00:01:41,200
typically do. Cognitive tasks, thinking

43
00:01:39,119 --> 00:01:42,799
tasks, and things like that. And so the

44
00:01:41,200 --> 00:01:44,799
most sort of common sensical way to do

45
00:01:42,799 --> 00:01:46,079
that is to say, well, if I want the

46
00:01:44,799 --> 00:01:47,680
computer to do something complicated

47
00:01:46,079 --> 00:01:49,920
like play chess, I'm just going to sit

48
00:01:47,680 --> 00:01:51,680
down with a few chess grand masters,

49
00:01:49,920 --> 00:01:53,600
show them a whole bunch of board moves,

50
00:01:51,680 --> 00:01:55,520
and ask them how they figure out how to

51
00:01:53,600 --> 00:01:56,640
respond, how to play the next move. I'm

52
00:01:55,520 --> 00:01:57,840
going to sort of sit down, talk to all

53
00:01:56,640 --> 00:02:00,000
these people, and then I'm going to

54
00:01:57,840 --> 00:02:01,600
write down a whole bunch of rules. If

55
00:02:00,000 --> 00:02:02,719
this is the board position, move this.

56
00:02:01,600 --> 00:02:04,240
If this is the board position, move

57
00:02:02,719 --> 00:02:06,000
this, and so on and so forth. Or I might

58
00:02:04,240 --> 00:02:07,040
sit down with the cardiologist and tell

59
00:02:06,000 --> 00:02:09,679
them, okay, how do you actually

60
00:02:07,040 --> 00:02:11,440
interpret an ECG? they will give me all

61
00:02:09,679 --> 00:02:12,560
similarly a bunch of if then rules I

62
00:02:11,440 --> 00:02:14,160
will take all these rules I'll put them

63
00:02:12,560 --> 00:02:15,840
into the computer and boom I have a

64
00:02:14,160 --> 00:02:18,319
system that can do what a human can do

65
00:02:15,840 --> 00:02:20,400
right now this approach even though it's

66
00:02:18,319 --> 00:02:23,200
common sensical and kind of makes sense

67
00:02:20,400 --> 00:02:25,280
it had success in only a few areas u and

68
00:02:23,200 --> 00:02:28,720
so the interesting question is why was

69
00:02:25,280 --> 00:02:30,160
it not pervasively successful why was it

70
00:02:28,720 --> 00:02:32,080
not pervasively successful it seems like

71
00:02:30,160 --> 00:02:33,200
a pretty good idea to me right and the

72
00:02:32,080 --> 00:02:35,120
people who came up with these things are

73
00:02:33,200 --> 00:02:36,800
smart people they're not dumb people

74
00:02:35,120 --> 00:02:39,519
they know what they're doing so why did

75
00:02:36,800 --> 00:02:42,800
it not book

76
00:02:39,519 --> 00:02:44,239
>> because because it's time in intensive.

77
00:02:42,800 --> 00:02:46,080
So it means that you have to run through

78
00:02:44,239 --> 00:02:48,400
all the scenarios that can ever exist

79
00:02:46,080 --> 00:02:51,200
and still some new scenarios can come up

80
00:02:48,400 --> 00:02:52,879
that you didn't cater for in

81
00:02:51,200 --> 00:02:54,319
>> Right. So there are two aspects to what

82
00:02:52,879 --> 00:02:56,319
you said which is the first aspect is

83
00:02:54,319 --> 00:02:57,840
it's time intensive that as it turns out

84
00:02:56,319 --> 00:02:58,816
is not a big deal because computers are

85
00:02:57,840 --> 00:02:59,440
getting faster and faster

86
00:02:58,816 --> 00:03:01,280
[clears throat]

87
00:02:59,440 --> 00:03:02,959
right the second thing is actually the

88
00:03:01,280 --> 00:03:05,680
key thing which is that it doesn't

89
00:03:02,959 --> 00:03:08,000
generalize to new situations very well

90
00:03:05,680 --> 00:03:09,280
right the problem is there are an

91
00:03:08,000 --> 00:03:10,400
infinite number of things that you're

92
00:03:09,280 --> 00:03:12,400
going to see when you deploy these

93
00:03:10,400 --> 00:03:13,760
systems in the real world by definition

94
00:03:12,400 --> 00:03:16,159
what you're training it on is a small

95
00:03:13,760 --> 00:03:17,840
sample of rules so these rules are very

96
00:03:16,159 --> 00:03:20,400
brittle but there's actually an even

97
00:03:17,840 --> 00:03:23,920
more interesting reason and that reason

98
00:03:20,400 --> 00:03:25,920
is that we know more than we can tell.

99
00:03:23,920 --> 00:03:27,599
This is called Polani paradox. So the

100
00:03:25,920 --> 00:03:29,840
idea is that if I come to you and say,

101
00:03:27,599 --> 00:03:32,640
"Hey, uh here's a picture. Is it a dog

102
00:03:29,840 --> 00:03:34,000
or a cat?" You will tell me within I

103
00:03:32,640 --> 00:03:34,959
believe they've measured it like 20

104
00:03:34,000 --> 00:03:36,879
milliseconds or something, you know,

105
00:03:34,959 --> 00:03:38,720
it's a dog if it's a dog or a cat. And

106
00:03:36,879 --> 00:03:40,159
then if I ask you to explain to me

107
00:03:38,720 --> 00:03:41,599
exactly how you figure that out, you'll

108
00:03:40,159 --> 00:03:43,760
come up with a bunch of sort of reasons,

109
00:03:41,599 --> 00:03:45,360
right? Alleged reasons. Oh, you know, if

110
00:03:43,760 --> 00:03:47,599
it has whiskers, I think it's a cat or

111
00:03:45,360 --> 00:03:48,799
whatever. But the problem is that you

112
00:03:47,599 --> 00:03:50,000
actually first of all can't really

113
00:03:48,799 --> 00:03:51,680
articulate what's going on in your head.

114
00:03:50,000 --> 00:03:53,840
how you do these things and number two

115
00:03:51,680 --> 00:03:55,440
even if you articulate it often times

116
00:03:53,840 --> 00:03:58,000
your articulation has no correspondence

117
00:03:55,440 --> 00:04:01,200
with how your brain actually does it

118
00:03:58,000 --> 00:04:03,680
so you're incomplete under a liar

119
00:04:01,200 --> 00:04:06,640
so this is polanous paradox so if you

120
00:04:03,680 --> 00:04:07,920
can't even tell me how you do something

121
00:04:06,640 --> 00:04:10,239
how the heck am I supposed to take it

122
00:04:07,920 --> 00:04:11,760
and put into a computer doesn't work and

123
00:04:10,239 --> 00:04:13,599
second is the fact that we can't write

124
00:04:11,760 --> 00:04:16,320
down these rules for all possible

125
00:04:13,599 --> 00:04:18,880
situations edge cases corner cases etc

126
00:04:16,320 --> 00:04:20,639
and the world is full of edge cases so

127
00:04:18,880 --> 00:04:22,960
for these reasons this approach didn't

128
00:04:20,639 --> 00:04:25,040
work and so a different approach was

129
00:04:22,960 --> 00:04:26,639
developed and this approach was well

130
00:04:25,040 --> 00:04:28,240
basically said hey instead of explicitly

131
00:04:26,639 --> 00:04:31,120
telling the computer what to do why

132
00:04:28,240 --> 00:04:33,440
don't we simply give it lots of examples

133
00:04:31,120 --> 00:04:36,639
of inputs and outputs chest positions

134
00:04:33,440 --> 00:04:38,720
next move right ECG diagnosis right

135
00:04:36,639 --> 00:04:40,720
inputs and outputs and then why don't we

136
00:04:38,720 --> 00:04:43,199
just use some statistical techniques to

137
00:04:40,720 --> 00:04:44,960
learn a mapping a function that can go

138
00:04:43,199 --> 00:04:47,040
from the input to the output okay that

139
00:04:44,960 --> 00:04:49,680
was the idea and this idea is machine

140
00:04:47,040 --> 00:04:51,360
learning okay so machineing learning is

141
00:04:49,680 --> 00:04:53,360
basically just a fancy way of saying

142
00:04:51,360 --> 00:04:56,000
learn from input output examples using

143
00:04:53,360 --> 00:04:59,680
statistical techniques.

144
00:04:56,000 --> 00:05:01,040
Good. All right. So, um, now there are

145
00:04:59,680 --> 00:05:02,639
numerous ways to create machine learning

146
00:05:01,040 --> 00:05:03,840
models and if you have ever done linear

147
00:05:02,639 --> 00:05:06,479
regression, congratulations. You've been

148
00:05:03,840 --> 00:05:08,800
doing machine learning.

149
00:05:06,479 --> 00:05:10,000
Okay? And only one of those methods

150
00:05:08,800 --> 00:05:12,240
happens to be something called neural

151
00:05:10,000 --> 00:05:14,160
networks. There are many other methods

152
00:05:12,240 --> 00:05:15,919
and in fact you probably have done these

153
00:05:14,160 --> 00:05:17,199
other methods if you have done the a

154
00:05:15,919 --> 00:05:19,360
course like the analytics edge or

155
00:05:17,199 --> 00:05:21,440
something similar.

156
00:05:19,360 --> 00:05:23,199
Okay. So, machine learning has got

157
00:05:21,440 --> 00:05:25,919
tremendous impact around the world,

158
00:05:23,199 --> 00:05:27,759
right? It's like at this point u it's

159
00:05:25,919 --> 00:05:30,080
widely accepted. It's a very very

160
00:05:27,759 --> 00:05:31,360
successful technology. And in fact,

161
00:05:30,080 --> 00:05:33,840
whenever people are actually talking

162
00:05:31,360 --> 00:05:35,759
about AI, chances are they're actually

163
00:05:33,840 --> 00:05:38,960
talking about machine learning. It's

164
00:05:35,759 --> 00:05:40,960
just that AI sounds cooler.

165
00:05:38,960 --> 00:05:42,400
The only problem is for machine learning

166
00:05:40,960 --> 00:05:44,800
to work really well, the input data has

167
00:05:42,400 --> 00:05:47,520
to be structured. Okay? And what I mean

168
00:05:44,800 --> 00:05:49,840
by that is data that can essentially be

169
00:05:47,520 --> 00:05:52,000
sort of numericalized and stuck into the

170
00:05:49,840 --> 00:05:54,160
columns and rows of a spreadsheet.

171
00:05:52,000 --> 00:05:56,320
Right? So for example here, let's say I

172
00:05:54,160 --> 00:05:59,039
want to put together a data set of you

173
00:05:56,320 --> 00:06:00,880
know uh patients, their symptoms and

174
00:05:59,039 --> 00:06:02,560
their characteristics and then in the

175
00:06:00,880 --> 00:06:04,000
following year after they showed up at

176
00:06:02,560 --> 00:06:06,160
the doctor's office whether they had a

177
00:06:04,000 --> 00:06:08,319
cardiac event or not, I might create a

178
00:06:06,160 --> 00:06:10,000
data set like this with age, smoking

179
00:06:08,319 --> 00:06:12,160
status, yes, no, exercise, blah blah

180
00:06:10,000 --> 00:06:13,840
blah blah blah. Right? And so either

181
00:06:12,160 --> 00:06:15,600
these numbers are numbers, they're

182
00:06:13,840 --> 00:06:17,280
numerical. Or if they're not numerical,

183
00:06:15,600 --> 00:06:19,919
they're categorical,

184
00:06:17,280 --> 00:06:21,520
right? Yes, no. Uh smoking, yes, no,

185
00:06:19,919 --> 00:06:22,960
things like that. Which means that if

186
00:06:21,520 --> 00:06:25,199
you have categorical variables, you can

187
00:06:22,960 --> 00:06:26,720
just numericalize them pretty easily.

188
00:06:25,199 --> 00:06:27,919
You folks have done the some machine

189
00:06:26,720 --> 00:06:29,440
learning before. So you know things like

190
00:06:27,919 --> 00:06:31,039
one hotend coding and stuff like that

191
00:06:29,440 --> 00:06:33,199
can be done to make them all numerical.

192
00:06:31,039 --> 00:06:35,680
So the point is you can just render the

193
00:06:33,199 --> 00:06:37,280
data into the columns and rows of a

194
00:06:35,680 --> 00:06:39,520
spreadsheet pretty easily, right? That's

195
00:06:37,280 --> 00:06:41,120
what I mean by structured data. So when

196
00:06:39,520 --> 00:06:42,960
you but the situation is very different

197
00:06:41,120 --> 00:06:45,440
if you have unstructured data. So if you

198
00:06:42,960 --> 00:06:48,000
have an image of you know a cute puppy

199
00:06:45,440 --> 00:06:51,199
this is my puppy by the way um uh from

200
00:06:48,000 --> 00:06:54,800
many years ago sadly he's no more um but

201
00:06:51,199 --> 00:06:57,680
his name was Google.

202
00:06:54,800 --> 00:06:59,840
So yeah anyway uh uh my DMD alums know

203
00:06:57,680 --> 00:07:02,160
Google well. So this is Google right? If

204
00:06:59,840 --> 00:07:03,759
you want to take Google uh this picture

205
00:07:02,160 --> 00:07:05,520
and figure out how to sort of

206
00:07:03,759 --> 00:07:06,880
numericalize it, the first thing you

207
00:07:05,520 --> 00:07:08,240
want to need to understand is that if

208
00:07:06,880 --> 00:07:11,039
you actually look at how this picture is

209
00:07:08,240 --> 00:07:13,039
represented inside uh digitally in the

210
00:07:11,039 --> 00:07:14,319
computer, basically every picture like

211
00:07:13,039 --> 00:07:17,199
this is represented using three tables

212
00:07:14,319 --> 00:07:18,960
of numbers. Okay? And these and we'll

213
00:07:17,199 --> 00:07:21,120
get to what these numbers mean later on.

214
00:07:18,960 --> 00:07:23,199
But the point I'm making is that each

215
00:07:21,120 --> 00:07:25,280
number basically represents the amount

216
00:07:23,199 --> 00:07:27,759
of light,

217
00:07:25,280 --> 00:07:29,840
right? on a scale of 0 to 255 the amount

218
00:07:27,759 --> 00:07:31,680
of light in that location in that pixel.

219
00:07:29,840 --> 00:07:33,520
That's all the amount of light. So

220
00:07:31,680 --> 00:07:35,759
basically the this table is the amount

221
00:07:33,520 --> 00:07:37,520
of um sorry

222
00:07:35,759 --> 00:07:39,120
this table is amount of red light,

223
00:07:37,520 --> 00:07:41,199
amount of green light, amount of blue

224
00:07:39,120 --> 00:07:42,639
light. Okay. Now you will agree with me

225
00:07:41,199 --> 00:07:45,520
that if you for example look at

226
00:07:42,639 --> 00:07:47,360
something like this and say okay 251 at

227
00:07:45,520 --> 00:07:49,840
this location there is a lot of blue

228
00:07:47,360 --> 00:07:52,160
light because it's 251 out of a possible

229
00:07:49,840 --> 00:07:53,680
255 right maybe a lot of blue light

230
00:07:52,160 --> 00:07:55,759
somewhere here there's a lot of blue

231
00:07:53,680 --> 00:07:59,360
here

232
00:07:55,759 --> 00:08:02,720
whether that area is blue because of a

233
00:07:59,360 --> 00:08:04,800
piece of sky some water or a bunch of

234
00:08:02,720 --> 00:08:08,080
blue paint could be anything it's going

235
00:08:04,800 --> 00:08:09,440
to say 251. So the underlying reality,

236
00:08:08,080 --> 00:08:11,199
the underlying object that's being

237
00:08:09,440 --> 00:08:14,400
described has nothing to do with the

238
00:08:11,199 --> 00:08:16,160
251. Right? So that's the whole problem.

239
00:08:14,400 --> 00:08:18,080
The raw form of the data has no

240
00:08:16,160 --> 00:08:20,080
intrinsic meaning with the underlying

241
00:08:18,080 --> 00:08:21,520
thing. So given that there's no

242
00:08:20,080 --> 00:08:23,440
connection between the number and what

243
00:08:21,520 --> 00:08:27,759
it's describing, how the heck can any

244
00:08:23,440 --> 00:08:30,479
algorithm do anything with it? It can't.

245
00:08:27,759 --> 00:08:32,640
Right? So what you have to do is

246
00:08:30,479 --> 00:08:34,880
something called feature engineering or

247
00:08:32,640 --> 00:08:36,959
feature extraction. Right? where you

248
00:08:34,880 --> 00:08:38,399
have to manually take all these things

249
00:08:36,959 --> 00:08:40,399
and create essentially a spreadsheet

250
00:08:38,399 --> 00:08:42,320
from them. So basically, let's say that

251
00:08:40,399 --> 00:08:44,080
you have a bunch of birds, right? And

252
00:08:42,320 --> 00:08:45,600
you're trying to build a bird classifier

253
00:08:44,080 --> 00:08:47,200
to figure out what kind of bird species

254
00:08:45,600 --> 00:08:49,519
it is. You might actually have to take

255
00:08:47,200 --> 00:08:51,600
this picture and then you have to

256
00:08:49,519 --> 00:08:53,200
measure the beak length, the wingspan,

257
00:08:51,600 --> 00:08:56,560
the primary color, and so on and so

258
00:08:53,200 --> 00:08:59,440
forth. So you're basically structuring

259
00:08:56,560 --> 00:09:02,240
the unstructured data manually, right?

260
00:08:59,440 --> 00:09:06,240
And this process of structuring

261
00:09:02,240 --> 00:09:08,240
unstructured data is basically called

262
00:09:06,240 --> 00:09:10,959
we use the word representation. We take

263
00:09:08,240 --> 00:09:13,440
the raw data and we represent the data

264
00:09:10,959 --> 00:09:15,680
in a different form. And the the reason

265
00:09:13,440 --> 00:09:17,760
why I'm sort of focusing on the use of

266
00:09:15,680 --> 00:09:19,279
the word representation is because it

267
00:09:17,760 --> 00:09:21,040
becomes really really important a bit

268
00:09:19,279 --> 00:09:23,360
later on when we get to deep learning.

269
00:09:21,040 --> 00:09:24,880
Okay. So we have to represent the data

270
00:09:23,360 --> 00:09:27,519
in a different way for it to work.

271
00:09:24,880 --> 00:09:30,320
That's the basic idea. All right. So

272
00:09:27,519 --> 00:09:32,560
what that means is that u historically

273
00:09:30,320 --> 00:09:34,640
researchers would manually develop these

274
00:09:32,560 --> 00:09:36,959
representations and once you develop

275
00:09:34,640 --> 00:09:38,320
them once you representations you could

276
00:09:36,959 --> 00:09:40,560
just use traditional linear regression

277
00:09:38,320 --> 00:09:42,000
or logistic regression get the job done.

278
00:09:40,560 --> 00:09:44,160
So the whole name of the game is the

279
00:09:42,000 --> 00:09:46,240
representations. So in fact people doing

280
00:09:44,160 --> 00:09:48,640
PhDs for example in computer revision

281
00:09:46,240 --> 00:09:50,640
would spend like four years developing

282
00:09:48,640 --> 00:09:52,959
amazing representations for solving one

283
00:09:50,640 --> 00:09:54,959
particular little problem. Right? we

284
00:09:52,959 --> 00:09:56,320
have a bunch of say cat scans and we

285
00:09:54,959 --> 00:09:58,000
need to take the cat scan and figure out

286
00:09:56,320 --> 00:09:59,920
whether a particular kind of stroke

287
00:09:58,000 --> 00:10:01,120
there is evidence for it in the cat scan

288
00:09:59,920 --> 00:10:02,800
right they might actually sit and

289
00:10:01,120 --> 00:10:04,160
develop all kinds of representations and

290
00:10:02,800 --> 00:10:06,080
test it and so on and then they'll

291
00:10:04,160 --> 00:10:07,680
finally declare victory and say yay I'm

292
00:10:06,080 --> 00:10:09,440
done with my PhD here is this amazing

293
00:10:07,680 --> 00:10:11,200
representation and you can build a

294
00:10:09,440 --> 00:10:12,800
classifier with it to predict a

295
00:10:11,200 --> 00:10:16,240
particular kind of stroke with high

296
00:10:12,800 --> 00:10:19,680
accuracy okay so that was that that's

297
00:10:16,240 --> 00:10:21,360
where the world was uh now as you can

298
00:10:19,680 --> 00:10:22,880
imagine developing representations

299
00:10:21,360 --> 00:10:24,880
because it's so manual Well, is this

300
00:10:22,880 --> 00:10:27,279
massive human bottleneck and this

301
00:10:24,880 --> 00:10:29,440
sharply limited limited the reach and

302
00:10:27,279 --> 00:10:31,920
applicability of machine learning as you

303
00:10:29,440 --> 00:10:34,240
would expect

304
00:10:31,920 --> 00:10:35,440
to address this problem a different

305
00:10:34,240 --> 00:10:36,800
approach came about and that's deep

306
00:10:35,440 --> 00:10:39,440
learning. So deep learning sits inside

307
00:10:36,800 --> 00:10:42,560
machine learning. Okay. And deep

308
00:10:39,440 --> 00:10:47,040
learning can handle unstructured input

309
00:10:42,560 --> 00:10:50,320
data without upfront manual processing.

310
00:10:47,040 --> 00:10:51,839
Meaning it will automatically learn the

311
00:10:50,320 --> 00:10:54,160
right representations from the raw

312
00:10:51,839 --> 00:10:55,920
input. Automatically is the key word.

313
00:10:54,160 --> 00:10:57,279
Automatically learn representations.

314
00:10:55,920 --> 00:10:58,320
Which means that you can give it

315
00:10:57,279 --> 00:10:59,360
structured data, you can give it

316
00:10:58,320 --> 00:11:00,560
pictures, you can give it text, you can

317
00:10:59,360 --> 00:11:04,800
give it anything you want. It'll just

318
00:11:00,560 --> 00:11:07,040
learn it. Okay. um it can automatically

319
00:11:04,800 --> 00:11:09,120
extract these representations and since

320
00:11:07,040 --> 00:11:10,880
it's being automatically extracted you

321
00:11:09,120 --> 00:11:12,800
can imagine sort of a pipeline where the

322
00:11:10,880 --> 00:11:14,240
raw data comes in you have a bunch of

323
00:11:12,800 --> 00:11:15,680
stuff in the middle that's learning

324
00:11:14,240 --> 00:11:17,760
these representations automatically

325
00:11:15,680 --> 00:11:19,519
without your help and then boom you just

326
00:11:17,760 --> 00:11:20,880
attach a little linear regression or

327
00:11:19,519 --> 00:11:22,959
logistic regression at the end problem

328
00:11:20,880 --> 00:11:25,120
solved

329
00:11:22,959 --> 00:11:26,800
that in a nutshell is deep learning

330
00:11:25,120 --> 00:11:28,560
input a whole bunch of representations

331
00:11:26,800 --> 00:11:32,000
being learned and then piped into a

332
00:11:28,560 --> 00:11:35,040
linear or logistic regression model okay

333
00:11:32,000 --> 00:11:38,240
you So the amazing thing is this simple

334
00:11:35,040 --> 00:11:40,800
idea, this simple idea is just

335
00:11:38,240 --> 00:11:43,360
incredibly powerful, right? That idea

336
00:11:40,800 --> 00:11:46,880
has led to Chad GPT has led to Alpha Go,

337
00:11:43,360 --> 00:11:49,920
Alpha Fold and so on and so forth. And I

338
00:11:46,880 --> 00:11:51,279
I kid you not, I'm sort of I I've been

339
00:11:49,920 --> 00:11:53,760
doing deep learning for about 10 years

340
00:11:51,279 --> 00:11:56,240
now. And every time I look at it, I

341
00:11:53,760 --> 00:11:58,079
literally get goosebumps every so often

342
00:11:56,240 --> 00:11:59,920
that that something so simple could be

343
00:11:58,079 --> 00:12:02,320
so powerful, right? It's really like

344
00:11:59,920 --> 00:12:03,839
boggles the mind. I'm like I'm just so

345
00:12:02,320 --> 00:12:05,600
lucky to be alive and working during

346
00:12:03,839 --> 00:12:07,760
this period.

347
00:12:05,600 --> 00:12:08,800
Okay. And you know coming from people

348
00:12:07,760 --> 00:12:10,800
who have been in the industry a long

349
00:12:08,800 --> 00:12:12,399
time this sort of breathless exclamation

350
00:12:10,800 --> 00:12:14,720
is not very rare particularly because

351
00:12:12,399 --> 00:12:17,360
I'm not in marketing.

352
00:12:14,720 --> 00:12:19,519
Okay. I actually mean it

353
00:12:17,360 --> 00:12:22,160
with all your apologies to various

354
00:12:19,519 --> 00:12:25,200
marketing folks. So just realized it's

355
00:12:22,160 --> 00:12:26,800
being taped. So uh okay. So so this

356
00:12:25,200 --> 00:12:28,639
demolished the human bottleneck for

357
00:12:26,800 --> 00:12:30,959
using machine learning with unstructured

358
00:12:28,639 --> 00:12:33,120
data. uh and so it comes from the

359
00:12:30,959 --> 00:12:35,600
confluence of three forces uh new

360
00:12:33,120 --> 00:12:37,680
algorithmic ideas a whole a lot of data

361
00:12:35,600 --> 00:12:39,279
and then very importantly the fact that

362
00:12:37,680 --> 00:12:40,880
we have access to parallel computing

363
00:12:39,279 --> 00:12:42,480
hardware in the in the form of these

364
00:12:40,880 --> 00:12:44,880
things called GPUs graphics processing

365
00:12:42,480 --> 00:12:46,800
units u and these three forces came

366
00:12:44,880 --> 00:12:48,160
together and they were applied to an old

367
00:12:46,800 --> 00:12:49,440
idea called neural networks and that's

368
00:12:48,160 --> 00:12:50,399
basically deep learning and I'll go

369
00:12:49,440 --> 00:12:51,920
through it very quickly because

370
00:12:50,399 --> 00:12:53,360
obviously we're going to spend half the

371
00:12:51,920 --> 00:12:56,320
semester looking into this thing in

372
00:12:53,360 --> 00:12:58,639
detail uh so what's the immediate

373
00:12:56,320 --> 00:13:01,600
immediate application of the ability to

374
00:12:58,639 --> 00:13:05,959
automatically handle unstructured data.

375
00:13:01,600 --> 00:13:05,959
What is like the no-brainer application?

376
00:13:10,800 --> 00:13:16,000
It's okay if it's obvious. Tell me

377
00:13:14,240 --> 00:13:18,000
>> sorry.

378
00:13:16,000 --> 00:13:19,519
>> Um, image classification,

379
00:13:18,000 --> 00:13:21,040
>> right? So, image classification. Yes.

380
00:13:19,519 --> 00:13:22,480
So, you can take an image, a good

381
00:13:21,040 --> 00:13:24,399
example of unstructured data, you can do

382
00:13:22,480 --> 00:13:27,200
some classification on it. But more

383
00:13:24,399 --> 00:13:30,240
generally, more generally, what I'm

384
00:13:27,200 --> 00:13:33,279
getting at is that every sensor in the

385
00:13:30,240 --> 00:13:35,120
world can be given the ability to

386
00:13:33,279 --> 00:13:37,120
detect, recognize, and classify what

387
00:13:35,120 --> 00:13:38,639
it's sensing. Every sensor because

388
00:13:37,120 --> 00:13:41,600
remember what is a what does a sensor

389
00:13:38,639 --> 00:13:44,160
do? A sensor is just a receptacle for

390
00:13:41,600 --> 00:13:46,240
unstructured data. A camera is a

391
00:13:44,160 --> 00:13:48,160
receptacle for unstructured video or

392
00:13:46,240 --> 00:13:50,560
unstructured, you know, still images,

393
00:13:48,160 --> 00:13:53,040
microphone, unstructured audio, right?

394
00:13:50,560 --> 00:13:54,959
So every sensor you can you can imagine

395
00:13:53,040 --> 00:13:57,360
taking a sensor and sticking a little

396
00:13:54,959 --> 00:13:59,519
deep learning system behind it and now

397
00:13:57,360 --> 00:14:01,120
suddenly the what comes out of this

398
00:13:59,519 --> 00:14:02,720
center of the deep learning system you

399
00:14:01,120 --> 00:14:04,800
can count you can classify you can

400
00:14:02,720 --> 00:14:07,040
detect you can do all kinds of stuff in

401
00:14:04,800 --> 00:14:10,240
short you can analyze

402
00:14:07,040 --> 00:14:12,160
and you can predict right and this is

403
00:14:10,240 --> 00:14:15,680
the way I'm describing it right now

404
00:14:12,160 --> 00:14:17,760
you'll be like yeah duh obviously

405
00:14:15,680 --> 00:14:20,399
but you know what this obviously thing

406
00:14:17,760 --> 00:14:21,920
is actually not at all obvious in terms

407
00:14:20,399 --> 00:14:24,560
of whether it'll help you find

408
00:14:21,920 --> 00:14:28,320
interesting applications or not. Okay,

409
00:14:24,560 --> 00:14:31,279
so here is something I literally saw

410
00:14:28,320 --> 00:14:32,560
last week. Okay, actually I have another

411
00:14:31,279 --> 00:14:35,360
slide before that but we are coming to

412
00:14:32,560 --> 00:14:37,519
that. So for instance, every time you

413
00:14:35,360 --> 00:14:38,959
use Face ID unlock your phone, this is

414
00:14:37,519 --> 00:14:40,639
the basic principle at work, right? The

415
00:14:38,959 --> 00:14:42,000
the camera and the iPhone is the sensor

416
00:14:40,639 --> 00:14:43,760
and they stuck a deepline system behind

417
00:14:42,000 --> 00:14:46,000
it to do image classification, right?

418
00:14:43,760 --> 00:14:49,040
Rama, non-RAM, right? That's what it's

419
00:14:46,000 --> 00:14:50,880
classifying. Um and so here right you

420
00:14:49,040 --> 00:14:52,880
have a breast cancer this is the breast

421
00:14:50,880 --> 00:14:55,839
cancer detection system from a mamogram.

422
00:14:52,880 --> 00:14:58,160
Uh by the way this picture is a very

423
00:14:55,839 --> 00:15:01,360
interesting picture. So uh there's a

424
00:14:58,160 --> 00:15:03,680
professor in WCS uh Regina Barceli who's

425
00:15:01,360 --> 00:15:06,560
a very well-known expert in this field

426
00:15:03,680 --> 00:15:08,160
and uh she actually has built a breast

427
00:15:06,560 --> 00:15:09,440
cancer detection system which is which

428
00:15:08,160 --> 00:15:12,000
has been deployed at Mass General

429
00:15:09,440 --> 00:15:15,519
Hospital and turns out she's actually a

430
00:15:12,000 --> 00:15:17,920
breast cancer survivor and uh she was

431
00:15:15,519 --> 00:15:21,040
know she's she's she's good now all good

432
00:15:17,920 --> 00:15:23,600
but when um after she built her system I

433
00:15:21,040 --> 00:15:26,639
heard that she actually ran that system

434
00:15:23,600 --> 00:15:30,000
against the mamograms from many years

435
00:15:26,639 --> 00:15:32,720
prior when she went for a mammogram and

436
00:15:30,000 --> 00:15:34,800
was told that everything is fine. She

437
00:15:32,720 --> 00:15:37,680
ran the system on that mamogram and it

438
00:15:34,800 --> 00:15:39,199
came back and said here's a problem. So

439
00:15:37,680 --> 00:15:40,880
a very interesting example where a deep

440
00:15:39,199 --> 00:15:43,519
learning system picked up something that

441
00:15:40,880 --> 00:15:45,760
a radiologist could not right. So these

442
00:15:43,519 --> 00:15:47,839
things are can be quite powerful. Um

443
00:15:45,760 --> 00:15:50,240
obviously any self-driving system has

444
00:15:47,839 --> 00:15:51,519
numerous deep learning algorithms

445
00:15:50,240 --> 00:15:53,440
running under the hood. you know,

446
00:15:51,519 --> 00:15:54,880
pedestrian detection, you know, stop

447
00:15:53,440 --> 00:15:57,440
light detection, zebra crossing

448
00:15:54,880 --> 00:15:58,959
detection, so on and so forth. Um, you

449
00:15:57,440 --> 00:16:01,199
know, it's being very heavily used in

450
00:15:58,959 --> 00:16:02,639
visual inspection manufacturing. Uh, you

451
00:16:01,199 --> 00:16:04,000
have various cameras now in which people

452
00:16:02,639 --> 00:16:05,600
looking at saying, okay, there's a dent

453
00:16:04,000 --> 00:16:07,440
or there's a scratch. They have a little

454
00:16:05,600 --> 00:16:09,040
system which is a dent detector, scratch

455
00:16:07,440 --> 00:16:10,560
detector and so on. That's that's going

456
00:16:09,040 --> 00:16:14,079
on right now. And now I come to the

457
00:16:10,560 --> 00:16:15,360
example I saw last week which is um so

458
00:16:14,079 --> 00:16:17,120
this is an example of you can create

459
00:16:15,360 --> 00:16:19,920
dramatically better products if you

460
00:16:17,120 --> 00:16:21,680
really internalize this idea of okay

461
00:16:19,920 --> 00:16:23,600
it's almost like you're looking the the

462
00:16:21,680 --> 00:16:25,440
world and saying oh there's a sensor can

463
00:16:23,600 --> 00:16:26,560
attach a DL thing behind it that's the

464
00:16:25,440 --> 00:16:28,880
way you should be looking at the world

465
00:16:26,560 --> 00:16:31,120
okay for startup ideas. So, here's an

466
00:16:28,880 --> 00:16:34,480
example. Okay, these apparently are the

467
00:16:31,120 --> 00:16:37,759
world's first smart binoculars.

468
00:16:34,480 --> 00:16:40,959
Okay, this is the binocular

469
00:16:37,759 --> 00:16:43,360
2 weeks ago where you look at the bird.

470
00:16:40,959 --> 00:16:47,360
You look at the bird and now it tells

471
00:16:43,360 --> 00:16:50,320
you what kind of bird it is right there.

472
00:16:47,360 --> 00:16:52,000
It's a simple idea, but imagine, right?

473
00:16:50,320 --> 00:16:53,759
Imagine you are the first out of the

474
00:16:52,000 --> 00:16:54,800
gate with this feature. You'll have a

475
00:16:53,759 --> 00:16:57,759
little bit of an edge till everybody

476
00:16:54,800 --> 00:16:59,120
catches up like 3 months later.

477
00:16:57,759 --> 00:17:01,360
Let's be very clear. There are no

478
00:16:59,120 --> 00:17:03,120
long-term monopoly windows in the world.

479
00:17:01,360 --> 00:17:04,799
There are only short-term windows. So

480
00:17:03,120 --> 00:17:07,679
the the the hunt is always on for a

481
00:17:04,799 --> 00:17:10,640
little monopoly window. So here's an

482
00:17:07,679 --> 00:17:12,559
example of that. Right? So I encourage

483
00:17:10,640 --> 00:17:15,280
you to always think about the world as

484
00:17:12,559 --> 00:17:16,959
you know where are the sensors here and

485
00:17:15,280 --> 00:17:19,199
can I attach something behind the sensor

486
00:17:16,959 --> 00:17:22,280
to do something useful with it. Okay?

487
00:17:19,199 --> 00:17:22,280
All right.

488
00:17:24,880 --> 00:17:28,480
Now let's turn our attention to the

489
00:17:26,400 --> 00:17:30,320
output. You have been talking about in

490
00:17:28,480 --> 00:17:32,640
structured data, unstructured data and

491
00:17:30,320 --> 00:17:34,240
how deep learning has sort of unlocked

492
00:17:32,640 --> 00:17:35,760
the ability to work with unstructured

493
00:17:34,240 --> 00:17:38,080
data. But you've sort of been neglecting

494
00:17:35,760 --> 00:17:40,640
the output side of the equation. So

495
00:17:38,080 --> 00:17:42,880
traditionally uh we could predict single

496
00:17:40,640 --> 00:17:45,679
numbers or a few numbers pretty easily,

497
00:17:42,880 --> 00:17:47,679
right? So you've all done the canonical,

498
00:17:45,679 --> 00:17:49,440
you know, uh should this person be given

499
00:17:47,679 --> 00:17:51,120
a loan application in machine learning,

500
00:17:49,440 --> 00:17:52,559
right? So it just predicts the

501
00:17:51,120 --> 00:17:55,120
probability that a borrower will repay a

502
00:17:52,559 --> 00:17:57,039
loan based on a whole bunch of data or

503
00:17:55,120 --> 00:17:58,799
supply chain you predict the demand for

504
00:17:57,039 --> 00:18:01,039
a product next week or you can predict a

505
00:17:58,799 --> 00:18:02,559
bunch of numbers. So given a given a

506
00:18:01,039 --> 00:18:04,240
picture you can say okay does it which

507
00:18:02,559 --> 00:18:05,840
which one of the 10 kinds of furniture

508
00:18:04,240 --> 00:18:07,679
is it right? You can predict 10 numbers

509
00:18:05,840 --> 00:18:08,880
10 probabilities that add up to one. You

510
00:18:07,679 --> 00:18:10,080
can predict a whole bunch of numbers

511
00:18:08,880 --> 00:18:12,080
that don't have to add up to one such as

512
00:18:10,080 --> 00:18:15,200
the GPS coordinates of a of an Uber

513
00:18:12,080 --> 00:18:16,880
ride. So these are all simple unstructur

514
00:18:15,200 --> 00:18:18,960
sorry simple structured output just a

515
00:18:16,880 --> 00:18:20,960
few numbers right what we could not do

516
00:18:18,960 --> 00:18:23,440
very easily was to actually generate

517
00:18:20,960 --> 00:18:25,440
pictures like this

518
00:18:23,440 --> 00:18:27,760
we could not generate unstructured data

519
00:18:25,440 --> 00:18:30,000
we could only consume unstructured data

520
00:18:27,760 --> 00:18:31,520
right u you couldn't generate text you

521
00:18:30,000 --> 00:18:33,360
can generate pictures and so on audio

522
00:18:31,520 --> 00:18:36,400
and so on and so forth so with

523
00:18:33,360 --> 00:18:38,240
generative AI that problem is gone so

524
00:18:36,400 --> 00:18:40,559
generative AI is the ability to actually

525
00:18:38,240 --> 00:18:42,720
create unstructured data right and

526
00:18:40,559 --> 00:18:44,640
therefore it sits within deep learning

527
00:18:42,720 --> 00:18:47,280
It still runs on deep learning, but it's

528
00:18:44,640 --> 00:18:49,039
just one kind of deep learning.

529
00:18:47,280 --> 00:18:50,160
Okay, there's plenty of stuff going on

530
00:18:49,039 --> 00:18:52,559
in deep learning that's got nothing to

531
00:18:50,160 --> 00:18:53,679
do with generative AI. Nowadays, of

532
00:18:52,559 --> 00:18:55,200
course, you know, if you're a

533
00:18:53,679 --> 00:18:57,600
self-respecting entrepreneur who wants

534
00:18:55,200 --> 00:18:58,720
to ride this craze, you'll probably

535
00:18:57,600 --> 00:19:02,160
declare whatever you're doing as

536
00:18:58,720 --> 00:19:03,520
generative AI, right? Um, and some VCs

537
00:19:02,160 --> 00:19:05,600
may actually be ready to fund you. Who

538
00:19:03,520 --> 00:19:06,640
knows? But the point is there's plenty

539
00:19:05,600 --> 00:19:07,760
of stuff going on in deep learning

540
00:19:06,640 --> 00:19:10,640
that's got nothing to do with genative

541
00:19:07,760 --> 00:19:13,200
AI. Uh, but this is the overall picture.

542
00:19:10,640 --> 00:19:15,840
Now here uh we can produce unstructured

543
00:19:13,200 --> 00:19:17,600
outputs like pictures. You can take this

544
00:19:15,840 --> 00:19:18,880
thing and then you can actually you know

545
00:19:17,600 --> 00:19:20,240
come up with a nice picture description

546
00:19:18,880 --> 00:19:22,400
of it. This actually is a very famous

547
00:19:20,240 --> 00:19:23,600
picture by the way in uh in the world of

548
00:19:22,400 --> 00:19:25,120
computer vision. So we are actually

549
00:19:23,600 --> 00:19:28,720
going to be analyzing this picture a

550
00:19:25,120 --> 00:19:30,320
little later on in the semester. Uh you

551
00:19:28,720 --> 00:19:32,640
can obviously go from a very complicated

552
00:19:30,320 --> 00:19:36,120
caption to an image. Uh you can go from

553
00:19:32,640 --> 00:19:36,120
text to music.

554
00:19:36,240 --> 00:19:40,480
Can people hear it? Okay. Yeah. Yeah.

555
00:19:38,480 --> 00:19:43,200
All right. So and of course we can go

556
00:19:40,480 --> 00:19:45,520
from text to text i.e. chat GPD. Uh and

557
00:19:43,200 --> 00:19:47,120
then uh as of a few months ago things

558
00:19:45,520 --> 00:19:49,520
have gotten even more interesting where

559
00:19:47,120 --> 00:19:51,919
you can actually go you can send text

560
00:19:49,520 --> 00:19:53,840
and an image in and you can get text out

561
00:19:51,919 --> 00:19:56,000
right and in fact as of a few weeks ago

562
00:19:53,840 --> 00:19:58,960
you can send text image text image text

563
00:19:56,000 --> 00:20:00,640
image in an arbitrary sequence into into

564
00:19:58,960 --> 00:20:03,039
the system and it actually come back to

565
00:20:00,640 --> 00:20:04,720
you with text and image. Right? So

566
00:20:03,039 --> 00:20:06,080
things are becoming multimodal and I

567
00:20:04,720 --> 00:20:09,200
just want to share it with you like a

568
00:20:06,080 --> 00:20:12,480
really fun example I saw uh recently. So

569
00:20:09,200 --> 00:20:15,360
this person sends this picture. Can

570
00:20:12,480 --> 00:20:17,520
folks see this? This is very complicated

571
00:20:15,360 --> 00:20:19,840
parking sign apparently in San

572
00:20:17,520 --> 00:20:22,559
Francisco. And they're like, "It's

573
00:20:19,840 --> 00:20:24,080
Wednesday at 4 p.m. Can I park here?

574
00:20:22,559 --> 00:20:25,760
Tell me in one line." Because you really

575
00:20:24,080 --> 00:20:27,520
didn't want GPD4 to be giving you a big

576
00:20:25,760 --> 00:20:30,799
essay about this. Like you literally

577
00:20:27,520 --> 00:20:33,440
want to park. So GPD4 comes back and

578
00:20:30,799 --> 00:20:36,159
says, "Yes, you can park here for up to

579
00:20:33,440 --> 00:20:38,559
1 hour starting at 4 p.m." And folks, I

580
00:20:36,159 --> 00:20:39,919
double check this thing. It's correct.

581
00:20:38,559 --> 00:20:41,200
We all know these things hallucinate,

582
00:20:39,919 --> 00:20:42,240
right? Can you imagine getting a parking

583
00:20:41,200 --> 00:20:43,039
ticket and telling the judge, "I'm

584
00:20:42,240 --> 00:20:45,760
sorry, I didn't realize it was

585
00:20:43,039 --> 00:20:47,600
hallucinating." So, so you have to

586
00:20:45,760 --> 00:20:49,840
double check it. So, yeah. So, things

587
00:20:47,600 --> 00:20:52,159
are getting multimodal very quickly. Uh,

588
00:20:49,840 --> 00:20:54,159
and so the picture here is that within

589
00:20:52,159 --> 00:20:56,080
Genai, we used to have these separate

590
00:20:54,159 --> 00:20:57,600
circles, text to text, text to image,

591
00:20:56,080 --> 00:20:59,360
text to music, text to this, text to

592
00:20:57,600 --> 00:21:01,120
that, so on and so forth. Those are all

593
00:20:59,360 --> 00:21:02,880
beginning to merge now inside Jai

594
00:21:01,120 --> 00:21:04,799
because multimodal models are going to

595
00:21:02,880 --> 00:21:06,400
become the norm this year, right? We

596
00:21:04,799 --> 00:21:08,000
already have really good closed models.

597
00:21:06,400 --> 00:21:10,159
We really have, we actually already have

598
00:21:08,000 --> 00:21:12,480
very good open source multimodal models.

599
00:21:10,159 --> 00:21:16,000
And so my feeling is that by the end of

600
00:21:12,480 --> 00:21:18,240
the year the idea of using a texton

601
00:21:16,000 --> 00:21:20,159
model is going to be like really you do

602
00:21:18,240 --> 00:21:22,000
that still right it's going to become

603
00:21:20,159 --> 00:21:23,840
like a quaint oldfashioned thing I think

604
00:21:22,000 --> 00:21:25,280
multimodal modality is going to become

605
00:21:23,840 --> 00:21:26,799
the norm. So that's where the world is

606
00:21:25,280 --> 00:21:28,960
and this is the landscape. So any

607
00:21:26,799 --> 00:21:30,159
questions on the landscape

608
00:21:28,960 --> 00:21:33,159
before we actually start doing some

609
00:21:30,159 --> 00:21:33,159
math.

610
00:21:35,600 --> 00:21:38,799
>> Okay.

611
00:21:37,840 --> 00:21:40,880
Yeah.

612
00:21:38,799 --> 00:21:42,960
>> Um not related to the landscape but it's

613
00:21:40,880 --> 00:21:45,600
more related to the breast cancer

614
00:21:42,960 --> 00:21:49,840
question. So how did how did she know

615
00:21:45,600 --> 00:21:51,280
that it was actually like there is a

616
00:21:49,840 --> 00:21:52,960
training that you can do and you there's

617
00:21:51,280 --> 00:21:55,440
a testing you can do which is based on

618
00:21:52,960 --> 00:21:57,440
the existing breast cancer samples but

619
00:21:55,440 --> 00:22:00,880
he or she was taking the mamogs from

620
00:21:57,440 --> 00:22:04,080
past 6 weeks past also which is probably

621
00:22:00,880 --> 00:22:05,760
a smaller version which was not there

622
00:22:04,080 --> 00:22:07,520
right like I mean there's an answer

623
00:22:05,760 --> 00:22:09,919
>> you mean the the the evidence of that

624
00:22:07,520 --> 00:22:12,240
being a problem would have been smaller

625
00:22:09,919 --> 00:22:15,120
>> so how do you know your model is great

626
00:22:12,240 --> 00:22:16,400
enough that it can correct it to give

627
00:22:15,120 --> 00:22:18,000
you correct answers. So

628
00:22:16,400 --> 00:22:19,679
>> yeah, so I think the so the question is

629
00:22:18,000 --> 00:22:21,120
that in general how do you train your

630
00:22:19,679 --> 00:22:23,360
model so that it gives you right answers

631
00:22:21,120 --> 00:22:25,200
given that over the passage of time the

632
00:22:23,360 --> 00:22:27,039
amount of evidence in this data could be

633
00:22:25,200 --> 00:22:28,960
very highly variable. So in this

634
00:22:27,039 --> 00:22:31,280
particular case of um you know the

635
00:22:28,960 --> 00:22:32,559
professor I talked about um you

636
00:22:31,280 --> 00:22:35,200
everything at that point was going

637
00:22:32,559 --> 00:22:37,200
through an expert radiologist. So five

638
00:22:35,200 --> 00:22:38,960
years ago this mamogram was seen by a

639
00:22:37,200 --> 00:22:40,640
radiologist and that person concluded

640
00:22:38,960 --> 00:22:42,159
there is no problem. So that was the

641
00:22:40,640 --> 00:22:45,200
training label right the wrong training

642
00:22:42,159 --> 00:22:47,200
label. Uh so typically what happens is

643
00:22:45,200 --> 00:22:48,799
that training labels could be wrong some

644
00:22:47,200 --> 00:22:51,120
small fraction of the time. So you need

645
00:22:48,799 --> 00:22:52,880
to have systems that are robust. So your

646
00:22:51,120 --> 00:22:54,880
data needs to be complete. It needs to

647
00:22:52,880 --> 00:22:57,600
be comprehensive. It needs to be have

648
00:22:54,880 --> 00:22:58,960
correct labels. If these ideas are not

649
00:22:57,600 --> 00:23:00,880
met your systems are not going to be

650
00:22:58,960 --> 00:23:03,120
that good. But as it turns out with

651
00:23:00,880 --> 00:23:04,960
neural networks even with some amount of

652
00:23:03,120 --> 00:23:07,039
noise in the labels they still do a

653
00:23:04,960 --> 00:23:09,679
pretty good job. Right? So that's sort

654
00:23:07,039 --> 00:23:11,600
of the general idea. So there has to be

655
00:23:09,679 --> 00:23:13,120
some verification.

656
00:23:11,600 --> 00:23:16,159
>> The ver the verification comes from the

657
00:23:13,120 --> 00:23:18,960
human. So every remember when we look at

658
00:23:16,159 --> 00:23:21,120
radiology data the the data we working

659
00:23:18,960 --> 00:23:23,440
with is the input is let's say an image

660
00:23:21,120 --> 00:23:25,600
like a radio a mamogram or something and

661
00:23:23,440 --> 00:23:27,760
then a human radiologist or a set of

662
00:23:25,600 --> 00:23:29,520
radiologists have said this has a

663
00:23:27,760 --> 00:23:31,679
problem or does not have a problem. So

664
00:23:29,520 --> 00:23:34,480
that is called the ground truth. So it

665
00:23:31,679 --> 00:23:35,840
is this ground truth image and label

666
00:23:34,480 --> 00:23:39,200
this combination that's being used to

667
00:23:35,840 --> 00:23:39,200
train these models.

668
00:23:39,600 --> 00:23:42,600
Yeah.

669
00:23:43,280 --> 00:23:47,600
>> Embodiment. So, so are we are we going

670
00:23:45,520 --> 00:23:50,000
to cover embodiment? So, the the the

671
00:23:47,600 --> 00:23:53,120
embodiment here refers to the fact that

672
00:23:50,000 --> 00:23:54,559
uh if you have robot robots, right? Uh

673
00:23:53,120 --> 00:23:56,000
they need to actually operate in the

674
00:23:54,559 --> 00:23:57,600
real world. And so, robots are an

675
00:23:56,000 --> 00:23:59,840
example of what's called embodied

676
00:23:57,600 --> 00:24:01,200
intelligence. So, unfortunately, due to

677
00:23:59,840 --> 00:24:03,600
the constraints of time, we're not going

678
00:24:01,200 --> 00:24:04,799
to get into robotics at all. But I will

679
00:24:03,600 --> 00:24:05,919
say that a lot of the deep learning

680
00:24:04,799 --> 00:24:07,520
stuff you're going to talk about, those

681
00:24:05,919 --> 00:24:09,919
are all fundamental building blocks in

682
00:24:07,520 --> 00:24:14,400
modern robotic systems.

683
00:24:09,919 --> 00:24:15,760
All right. So um so in summary, X and Y

684
00:24:14,400 --> 00:24:17,279
can be anything and it can be

685
00:24:15,760 --> 00:24:19,360
multimodal.

686
00:24:17,279 --> 00:24:22,080
Okay, I literally could not have put up

687
00:24:19,360 --> 00:24:24,400
this slide maybe two years ago, right?

688
00:24:22,080 --> 00:24:26,159
So it's very simple in how it looks, but

689
00:24:24,400 --> 00:24:28,159
it's very profound. You can you can

690
00:24:26,159 --> 00:24:29,679
learn a mapping from anything to

691
00:24:28,159 --> 00:24:33,120
anything at this point very easily as

692
00:24:29,679 --> 00:24:35,200
long as you have enough data. Okay. So u

693
00:24:33,120 --> 00:24:37,840
now note that all this excitement that

694
00:24:35,200 --> 00:24:40,799
we see around us is everything stems

695
00:24:37,840 --> 00:24:42,320
from stems from deep learning. Okay.

696
00:24:40,799 --> 00:24:44,480
Everything everything depends on deep

697
00:24:42,320 --> 00:24:45,840
learning. And so if you understand deep

698
00:24:44,480 --> 00:24:48,080
learning a lot of interesting things

699
00:24:45,840 --> 00:24:49,360
become possible. So let's get going. All

700
00:24:48,080 --> 00:24:52,240
right. So we'll start with the very

701
00:24:49,360 --> 00:24:54,960
basics. Uh what's a neural network? Uh

702
00:24:52,240 --> 00:24:56,960
now recall logistic regression from back

703
00:24:54,960 --> 00:24:58,880
in the day. So what is logistic

704
00:24:56,960 --> 00:25:01,360
regression? You send in a bunch of

705
00:24:58,880 --> 00:25:02,960
numbers, a vector of numbers and you get

706
00:25:01,360 --> 00:25:04,400
usually get a probability out, right?

707
00:25:02,960 --> 00:25:05,840
Between zero and one, what is the

708
00:25:04,400 --> 00:25:08,480
probability of something or the other?

709
00:25:05,840 --> 00:25:11,520
Okay. Uh and so this logistic regression

710
00:25:08,480 --> 00:25:13,600
model is also represented in this form

711
00:25:11,520 --> 00:25:15,760
if you'll recall. So basically what we

712
00:25:13,600 --> 00:25:17,840
do is we take all these numbers, we run

713
00:25:15,760 --> 00:25:19,360
it through a linear function, right? We

714
00:25:17,840 --> 00:25:21,039
run it through a linear function, you

715
00:25:19,360 --> 00:25:23,279
get a number and then we take that thing

716
00:25:21,039 --> 00:25:26,000
and run it through 1 / 1 plus e raised

717
00:25:23,279 --> 00:25:27,360
to minus that. And that's guaranteed to

718
00:25:26,000 --> 00:25:28,559
give you a number between zero and one

719
00:25:27,360 --> 00:25:30,240
which can be interpreted as a

720
00:25:28,559 --> 00:25:32,880
probability and that's logistic

721
00:25:30,240 --> 00:25:35,279
regression. Okay. And the canonical you

722
00:25:32,880 --> 00:25:36,880
know uh loan approvals things like that

723
00:25:35,279 --> 00:25:38,640
all fall into this sort of convenient

724
00:25:36,880 --> 00:25:43,480
bucket.

725
00:25:38,640 --> 00:25:43,480
Okay. So this should be super familiar.

726
00:25:44,480 --> 00:25:48,880
All right. Now we're going to actually

727
00:25:46,640 --> 00:25:52,240
look at this you know simple modest

728
00:25:48,880 --> 00:25:53,919
humble little operation

729
00:25:52,240 --> 00:25:55,520
using the lens of a network of

730
00:25:53,919 --> 00:25:57,039
mathematical operations and the reason

731
00:25:55,520 --> 00:25:59,600
why we do it will become clear a bit

732
00:25:57,039 --> 00:26:02,400
later so we'll take this very simple

733
00:25:59,600 --> 00:26:05,520
example where we have uh let's say two

734
00:26:02,400 --> 00:26:08,080
variables GPA and experience right this

735
00:26:05,520 --> 00:26:10,080
is the GPA of some graduates uh number

736
00:26:08,080 --> 00:26:12,159
of years of work experience and then

737
00:26:10,080 --> 00:26:14,880
this is the dependent variable which is

738
00:26:12,159 --> 00:26:16,400
either zero or one and zero if they

739
00:26:14,880 --> 00:26:17,919
don't get called for an interview, one

740
00:26:16,400 --> 00:26:20,240
if they get called for an interview.

741
00:26:17,919 --> 00:26:22,240
Okay, it's a two input variable, one

742
00:26:20,240 --> 00:26:24,159
output variable problem. Okay, and it's

743
00:26:22,240 --> 00:26:25,840
a classification problem because we

744
00:26:24,159 --> 00:26:28,000
classifying people into will they get

745
00:26:25,840 --> 00:26:29,600
called for an interview, yes or no.

746
00:26:28,000 --> 00:26:33,360
Okay,

747
00:26:29,600 --> 00:26:36,080
and so that's a setup for this problem.

748
00:26:33,360 --> 00:26:38,799
And let's say that we actually run it

749
00:26:36,080 --> 00:26:40,720
through any, you know, we actually try

750
00:26:38,799 --> 00:26:42,159
to fit a logic regression model to it.

751
00:26:40,720 --> 00:26:43,760
So if you're familiar with R for

752
00:26:42,159 --> 00:26:46,880
example, you would use something like

753
00:26:43,760 --> 00:26:49,120
GLM to fit this model. Um if you use

754
00:26:46,880 --> 00:26:50,799
something like stats models in Python,

755
00:26:49,120 --> 00:26:52,400
there's a similar function for it.

756
00:26:50,799 --> 00:26:55,200
Scikitlearn, there's another function

757
00:26:52,400 --> 00:26:57,200
for it. You get the idea, right? This

758
00:26:55,200 --> 00:26:58,400
you can use whatever favorite methods

759
00:26:57,200 --> 00:27:00,400
you have for logistic regression

760
00:26:58,400 --> 00:27:02,159
modeling to get this job done. And if

761
00:27:00,400 --> 00:27:04,240
you do that with this little data set,

762
00:27:02,159 --> 00:27:06,640
you're going to get these coefficients,

763
00:27:04,240 --> 00:27:08,400
right? The point 4 is the intercept 02

764
00:27:06,640 --> 00:27:10,000
is the coefficient of a GPA 0.5 for

765
00:27:08,400 --> 00:27:13,679
experience and that is a resulting

766
00:27:10,000 --> 00:27:15,279
sigmoid function. Okay. All right. Cool.

767
00:27:13,679 --> 00:27:17,760
So now let's actually rewrite this

768
00:27:15,279 --> 00:27:19,760
formula as a network in the following

769
00:27:17,760 --> 00:27:21,520
way. So first what we'll do is we'll

770
00:27:19,760 --> 00:27:23,760
take GP and experience and stick it here

771
00:27:21,520 --> 00:27:25,360
on the left side and we'll put little

772
00:27:23,760 --> 00:27:28,720
circles next to them and we'll call them

773
00:27:25,360 --> 00:27:30,640
the input nodes. Okay. And so imagine

774
00:27:28,720 --> 00:27:33,120
that somebody puts writes a GPA into the

775
00:27:30,640 --> 00:27:36,000
circle 3.5 or you know years experience

776
00:27:33,120 --> 00:27:38,159
2.0 zero and then it flows through this

777
00:27:36,000 --> 00:27:40,960
arrow and as it flows through it gets

778
00:27:38,159 --> 00:27:42,880
multiplied by its coefficient 2 the 2 is

779
00:27:40,960 --> 00:27:44,400
coming from here

780
00:27:42,880 --> 00:27:47,200
similarly experience gets multiplied

781
00:27:44,400 --> 00:27:49,200
by.5 it comes in here and this node as

782
00:27:47,200 --> 00:27:51,919
the plus indicates is adding everything

783
00:27:49,200 --> 00:27:54,480
that's coming into it so it's adding2*

784
00:27:51,919 --> 00:27:56,240
GPA.5 time experience plus the intercept

785
00:27:54,480 --> 00:27:58,480
which is a green arrow coming from on

786
00:27:56,240 --> 00:27:59,919
its own it comes through here and what

787
00:27:58,480 --> 00:28:02,799
comes out of this is just a single

788
00:27:59,919 --> 00:28:04,640
number and that number goes into this

789
00:28:02,799 --> 00:28:08,640
little circle circle

790
00:28:04,640 --> 00:28:12,399
and then out pops a probability. Okay,

791
00:28:08,640 --> 00:28:14,960
so I've sort of done this ridiculously

792
00:28:12,399 --> 00:28:17,600
long way long winded way of writing a

793
00:28:14,960 --> 00:28:18,960
simple function. Okay, and the reason we

794
00:28:17,600 --> 00:28:21,120
why I'm doing it will become clear in a

795
00:28:18,960 --> 00:28:23,440
second.

796
00:28:21,120 --> 00:28:26,159
Okay, so this is a little network of

797
00:28:23,440 --> 00:28:28,320
operations for the simple function. And

798
00:28:26,159 --> 00:28:29,919
so for instance, how you would use it is

799
00:28:28,320 --> 00:28:32,000
you to make a prediction, you'll let's

800
00:28:29,919 --> 00:28:33,840
say someone has a 3.8 GPA and 1.2 into

801
00:28:32,000 --> 00:28:36,640
user experience. You just plug it in

802
00:28:33,840 --> 00:28:38,399
here, do the math, you get 76. Same

803
00:28:36,640 --> 00:28:40,960
thing here, comes in here, add them all

804
00:28:38,399 --> 00:28:43,200
up, you get 1.76. You run 1.76 through

805
00:28:40,960 --> 00:28:44,559
the sigmoid, you get 085. And that is

806
00:28:43,200 --> 00:28:45,679
the probability that that particular

807
00:28:44,559 --> 00:28:48,159
individual may get called for an

808
00:28:45,679 --> 00:28:49,360
interview. Okay? At this point, we're

809
00:28:48,159 --> 00:28:51,440
just doing logistic regression. Nothing

810
00:28:49,360 --> 00:28:54,159
more complicated.

811
00:28:51,440 --> 00:28:56,240
Okay. So, um, now if you have many

812
00:28:54,159 --> 00:28:58,559
variables, not two variables like X1

813
00:28:56,240 --> 00:29:00,000
through X K, you can the same sort of

814
00:28:58,559 --> 00:29:01,360
logic applies. Each one has some

815
00:29:00,000 --> 00:29:03,120
coefficient and then there's an

816
00:29:01,360 --> 00:29:04,960
intercept. They all get added up here.

817
00:29:03,120 --> 00:29:07,120
Run through a sigmoid and out pops this

818
00:29:04,960 --> 00:29:09,360
number. Okay. Notice how the data flows

819
00:29:07,120 --> 00:29:15,000
from left to right.

820
00:29:09,360 --> 00:29:15,000
Okay. All right. Any questions on this?

821
00:29:15,120 --> 00:29:20,480
All right. Good. So now terminology. Uh

822
00:29:19,200 --> 00:29:21,919
so you will actually you'll discover

823
00:29:20,480 --> 00:29:24,080
that the world of neural networks and

824
00:29:21,919 --> 00:29:25,600
deep learning has its own terminology.

825
00:29:24,080 --> 00:29:26,960
they have their own ways of referring to

826
00:29:25,600 --> 00:29:28,559
things that we the rest of the world has

827
00:29:26,960 --> 00:29:30,399
been referring using something else for

828
00:29:28,559 --> 00:29:32,159
the longest time right it's kind of

829
00:29:30,399 --> 00:29:35,200
annoying sometimes but it's the way it

830
00:29:32,159 --> 00:29:37,120
is so uh

831
00:29:35,200 --> 00:29:38,640
remember in regression we used to call

832
00:29:37,120 --> 00:29:40,720
those numbers next to each variable as

833
00:29:38,640 --> 00:29:42,559
coefficients and the constant thing as

834
00:29:40,720 --> 00:29:44,320
an intercept well guess what in this

835
00:29:42,559 --> 00:29:46,880
world these multip those coefficients

836
00:29:44,320 --> 00:29:50,000
are actually called weights

837
00:29:46,880 --> 00:29:51,279
and the inter are called biases so in in

838
00:29:50,000 --> 00:29:53,440
the neural network world these are

839
00:29:51,279 --> 00:29:54,720
called weights and biases and sometimes

840
00:29:53,440 --> 00:29:56,799
if If you're a little lazy, you may just

841
00:29:54,720 --> 00:29:59,120
call the whole thing as weights. Okay,

842
00:29:56,799 --> 00:30:00,880
so when you see in the newspaper that

843
00:29:59,120 --> 00:30:03,600
you know, oh my god, this amazing

844
00:30:00,880 --> 00:30:05,360
model's weights have been leaked

845
00:30:03,600 --> 00:30:06,799
on the internet or on Bit Torrent or

846
00:30:05,360 --> 00:30:08,240
something, that's what's going on,

847
00:30:06,799 --> 00:30:10,159
right? All these coefficients have been

848
00:30:08,240 --> 00:30:11,760
leaked because once you know what the

849
00:30:10,159 --> 00:30:12,799
coefficients are and what the

850
00:30:11,760 --> 00:30:15,120
architecture is, you can just

851
00:30:12,799 --> 00:30:16,559
reconstruct the model.

852
00:30:15,120 --> 00:30:19,760
All right, so that's what's going on

853
00:30:16,559 --> 00:30:21,120
here. Now, why did we do this network

854
00:30:19,760 --> 00:30:23,120
business? Why did we write it as a

855
00:30:21,120 --> 00:30:27,559
network?

856
00:30:23,120 --> 00:30:27,559
Yeah. What is advantage? Any guesses?

857
00:30:34,080 --> 00:30:40,320
>> When you have multiple functions,

858
00:30:37,279 --> 00:30:41,919
so it's just easier to see it that way,

859
00:30:40,320 --> 00:30:43,840
>> right? If you have lots of things going

860
00:30:41,919 --> 00:30:45,440
on, it's easier to see it if you

861
00:30:43,840 --> 00:30:49,279
actually write it in graphical form.

862
00:30:45,440 --> 00:30:52,159
Yes, correct. But so is it only like a

863
00:30:49,279 --> 00:30:53,679
usability advantage?

864
00:30:52,159 --> 00:30:56,880
The other thing is you want different

865
00:30:53,679 --> 00:30:59,120
functions for different layers of

866
00:30:56,880 --> 00:31:01,039
>> okay so maybe we want to use different

867
00:30:59,120 --> 00:31:02,640
functions in different layers but I

868
00:31:01,039 --> 00:31:05,679
think there's actually even a larger

869
00:31:02,640 --> 00:31:07,120
sort of a more basic point which is that

870
00:31:05,679 --> 00:31:09,600
then when you the moment you write it

871
00:31:07,120 --> 00:31:12,880
down you suddenly realize that I could

872
00:31:09,600 --> 00:31:14,080
have lots of things in the middle

873
00:31:12,880 --> 00:31:15,600
I don't have to go from the input to

874
00:31:14,080 --> 00:31:17,360
output directly I could do lots of

875
00:31:15,600 --> 00:31:20,880
things in the middle right that's sort

876
00:31:17,360 --> 00:31:22,559
of the key idea so what you do is so

877
00:31:20,880 --> 00:31:24,799
Remember the notion of learning

878
00:31:22,559 --> 00:31:25,919
representations of unstructured data,

879
00:31:24,799 --> 00:31:27,520
right? Where you take the picture and

880
00:31:25,919 --> 00:31:29,520
say beak length and things like that,

881
00:31:27,520 --> 00:31:31,039
right? And remember I said deep learning

882
00:31:29,520 --> 00:31:33,200
actually automatically learns these

883
00:31:31,039 --> 00:31:34,960
things. Where is that automatic learning

884
00:31:33,200 --> 00:31:36,960
coming from?

885
00:31:34,960 --> 00:31:39,120
Well, this is where it's coming from. So

886
00:31:36,960 --> 00:31:40,480
what we do is we take this thing, right?

887
00:31:39,120 --> 00:31:43,760
This just a logistic regression model.

888
00:31:40,480 --> 00:31:46,000
inputs get multip added up as a linear

889
00:31:43,760 --> 00:31:48,960
function or run through a sigmoid and

890
00:31:46,000 --> 00:31:51,679
then we are like hm if we want to learn

891
00:31:48,960 --> 00:31:53,200
representations of the raw input we

892
00:31:51,679 --> 00:31:54,799
better be doing something in the middle

893
00:31:53,200 --> 00:31:57,200
here

894
00:31:54,799 --> 00:31:59,440
because the output is the output that

895
00:31:57,200 --> 00:32:00,880
that's not going to change you know it's

896
00:31:59,440 --> 00:32:04,000
it's either a dog or a cat you don't

897
00:32:00,880 --> 00:32:07,279
have any choice as to what it is okay

898
00:32:04,000 --> 00:32:08,960
the only agency you have at this point

899
00:32:07,279 --> 00:32:11,120
is you can take the raw input and do

900
00:32:08,960 --> 00:32:12,720
things in the middle with

901
00:32:11,120 --> 00:32:14,559
You can do a lot of stuff in the middle

902
00:32:12,720 --> 00:32:18,480
and then run it through something to get

903
00:32:14,559 --> 00:32:21,120
the output. Okay. So in any in in any

904
00:32:18,480 --> 00:32:22,880
mathematical discipline, if someone

905
00:32:21,120 --> 00:32:24,880
comes to you and says here's a bunch of

906
00:32:22,880 --> 00:32:27,519
data, I want you to do something with

907
00:32:24,880 --> 00:32:31,760
it. What should the what is like the the

908
00:32:27,519 --> 00:32:34,559
most basic first thing you should do?

909
00:32:31,760 --> 00:32:36,320
Run it through a linear function.

910
00:32:34,559 --> 00:32:38,000
The most basic thing in math is a linear

911
00:32:36,320 --> 00:32:38,880
function. So given anything, just run it

912
00:32:38,000 --> 00:32:40,000
through a linear function. See what

913
00:32:38,880 --> 00:32:42,880
happens.

914
00:32:40,000 --> 00:32:44,640
So that's exactly what we can do. So the

915
00:32:42,880 --> 00:32:47,279
simplest thing we can do here, we can

916
00:32:44,640 --> 00:32:49,679
insert a bunch of linear functions. So

917
00:32:47,279 --> 00:32:51,360
we do is we take all this input and we

918
00:32:49,679 --> 00:32:53,919
just run it. We we do a linear function

919
00:32:51,360 --> 00:32:56,799
on it. So think of it this as x1 * 2

920
00:32:53,919 --> 00:32:59,039
plus x3 * 4 and all the way to xk * 9

921
00:32:56,799 --> 00:33:01,200
plus some intercept and boom, it goes

922
00:32:59,039 --> 00:33:05,279
out the other end. So this little circle

923
00:33:01,200 --> 00:33:08,240
here with a plus in it is just

924
00:33:05,279 --> 00:33:10,399
thank you. uh that this is just a linear

925
00:33:08,240 --> 00:33:12,080
it's a shortand for a linear function.

926
00:33:10,399 --> 00:33:13,440
So whenever you see a circle with a plus

927
00:33:12,080 --> 00:33:15,519
it's just a short hand for a linear

928
00:33:13,440 --> 00:33:16,559
function. Okay. So you can take this

929
00:33:15,519 --> 00:33:17,919
whole thing and run through a linear

930
00:33:16,559 --> 00:33:20,000
function and when you do it you'll get

931
00:33:17,919 --> 00:33:21,519
some number right there. You'll get some

932
00:33:20,000 --> 00:33:23,360
number. So you have taken these k

933
00:33:21,519 --> 00:33:26,960
numbers and you have sort of compressed

934
00:33:23,360 --> 00:33:28,480
them in some way into one number. Okay.

935
00:33:26,960 --> 00:33:30,799
But you don't have to stop at one

936
00:33:28,480 --> 00:33:32,320
number. You can do more. So we can have

937
00:33:30,799 --> 00:33:35,039
a stack of linear functions in the

938
00:33:32,320 --> 00:33:36,640
middle. Right? There's a linear function

939
00:33:35,039 --> 00:33:39,360
here. Another one here. Another one

940
00:33:36,640 --> 00:33:41,279
here. At this point, the K numbers you

941
00:33:39,360 --> 00:33:43,200
have K could be, for example, a

942
00:33:41,279 --> 00:33:45,360
thousand, right? It's just the size of

943
00:33:43,200 --> 00:33:46,640
your input data. You have taken these K

944
00:33:45,360 --> 00:33:50,960
things and you've compressed them into

945
00:33:46,640 --> 00:33:52,399
three numbers at this point. Okay? So,

946
00:33:50,960 --> 00:33:53,519
okay, maybe three is the right number,

947
00:33:52,399 --> 00:33:55,600
maybe 10 is the right number. We don't

948
00:33:53,519 --> 00:33:58,240
know. And we'll get to how do we know

949
00:33:55,600 --> 00:34:00,000
what the right number is later on. So,

950
00:33:58,240 --> 00:34:02,640
we can stack as many linear functions as

951
00:34:00,000 --> 00:34:04,240
we want. So, we have transformed this K

952
00:34:02,640 --> 00:34:06,799
thing into a threedimensional vector,

953
00:34:04,240 --> 00:34:10,399
right? K numbers become three numbers.

954
00:34:06,799 --> 00:34:12,480
Um and now we can flow this three these

955
00:34:10,399 --> 00:34:14,159
three numbers through some other little

956
00:34:12,480 --> 00:34:16,480
function.

957
00:34:14,159 --> 00:34:18,240
Okay.

958
00:34:16,480 --> 00:34:19,760
And as you will see in a few minutes

959
00:34:18,240 --> 00:34:21,919
that function is called an activation

960
00:34:19,760 --> 00:34:24,159
function and it's chosen to be a

961
00:34:21,919 --> 00:34:26,320
nonlinear function because if you don't

962
00:34:24,159 --> 00:34:28,320
choose it to be a nonlinear function all

963
00:34:26,320 --> 00:34:32,800
the effort we're doing is going to be a

964
00:34:28,320 --> 00:34:34,480
total waste of time. Okay. For now, just

965
00:34:32,800 --> 00:34:38,000
take it on faith that you need to have

966
00:34:34,480 --> 00:34:40,320
nonlinear functions here. But note that

967
00:34:38,000 --> 00:34:41,440
the three numbers here are still three

968
00:34:40,320 --> 00:34:42,480
numbers. They are three different

969
00:34:41,440 --> 00:34:44,879
numbers, but they're still three

970
00:34:42,480 --> 00:34:46,480
numbers. And once we do this, we'll be

971
00:34:44,879 --> 00:34:48,240
like, you know what? This was fun. Let's

972
00:34:46,480 --> 00:34:52,320
do it again.

973
00:34:48,240 --> 00:34:53,679
Okay? So, you can do it again.

974
00:34:52,320 --> 00:34:56,079
And you can keep on doing it. You can

975
00:34:53,679 --> 00:34:58,320
keep it 100 times if you want. And the

976
00:34:56,079 --> 00:35:01,359
key thing is that every time you do it,

977
00:34:58,320 --> 00:35:03,520
you're giving this network some ability,

978
00:35:01,359 --> 00:35:05,920
some capacity to learn something

979
00:35:03,520 --> 00:35:08,160
interesting from the data, to learn an

980
00:35:05,920 --> 00:35:09,760
interesting representation. Now, of

981
00:35:08,160 --> 00:35:10,960
course, you're thinking, well, how do we

982
00:35:09,760 --> 00:35:12,480
know it's interesting? How do we know

983
00:35:10,960 --> 00:35:14,640
it's a useful thing? And we'll come to

984
00:35:12,480 --> 00:35:16,720
all that later on, right? We're just

985
00:35:14,640 --> 00:35:18,079
giving it the capacity, the potential to

986
00:35:16,720 --> 00:35:19,440
learn interesting things with the data.

987
00:35:18,079 --> 00:35:21,599
Whether it actually lives up to its

988
00:35:19,440 --> 00:35:23,839
potential, we don't know yet. Okay?

989
00:35:21,599 --> 00:35:25,440
we'll give it the potential because the

990
00:35:23,839 --> 00:35:27,359
more transformations of the input data

991
00:35:25,440 --> 00:35:29,359
you make, the more opportunity you have

992
00:35:27,359 --> 00:35:30,400
to do interesting things with it. If I

993
00:35:29,359 --> 00:35:31,680
don't even give you the opportunity to

994
00:35:30,400 --> 00:35:33,920
transform it once, you don't have any

995
00:35:31,680 --> 00:35:35,839
opportunity, right? If I give you 10

996
00:35:33,920 --> 00:35:38,240
chances to transform things, you have 10

997
00:35:35,839 --> 00:35:40,320
shots at doing something useful.

998
00:35:38,240 --> 00:35:42,320
So, you can you can do this repeatedly.

999
00:35:40,320 --> 00:35:44,720
And once we are done doing these

1000
00:35:42,320 --> 00:35:46,320
transformations, we just pipe it through

1001
00:35:44,720 --> 00:35:50,560
to our good old logistic regression

1002
00:35:46,320 --> 00:35:54,560
sigmoid here and we are done.

1003
00:35:50,560 --> 00:35:56,400
Okay, so this is the basic idea. And so

1004
00:35:54,560 --> 00:35:58,320
just to contrast it, this was good old

1005
00:35:56,400 --> 00:36:00,240
logistic regression where we take the

1006
00:35:58,320 --> 00:36:02,720
input, we run it through a linear

1007
00:36:00,240 --> 00:36:04,880
function and pop out a number, a

1008
00:36:02,720 --> 00:36:06,800
probability number, but after we do all

1009
00:36:04,880 --> 00:36:08,800
this stuff, the input stays the same,

1010
00:36:06,800 --> 00:36:09,920
the output stays the same, but in the

1011
00:36:08,800 --> 00:36:11,520
middle, we just run it through a whole

1012
00:36:09,920 --> 00:36:13,040
bunch of these functions, you know,

1013
00:36:11,520 --> 00:36:15,920
these layersoop boop, and then we get

1014
00:36:13,040 --> 00:36:19,440
the output. Okay, that's all we have

1015
00:36:15,920 --> 00:36:21,920
done. And this is a neural network.

1016
00:36:19,440 --> 00:36:25,280
A neural network is nothing more than

1017
00:36:21,920 --> 00:36:27,680
repeatedly transformed inputs which are

1018
00:36:25,280 --> 00:36:31,079
finally fed to a linear or logistic

1019
00:36:27,680 --> 00:36:31,079
regression model.

1020
00:36:35,520 --> 00:36:38,400
Any questions?

1021
00:36:37,920 --> 00:36:39,760
>> Two question.

1022
00:36:38,400 --> 00:36:41,839
>> Could you use the thing so that everyone

1023
00:36:39,760 --> 00:36:44,000
can hear? Yeah.

1024
00:36:41,839 --> 00:36:45,440
>> Yeah. Two questions. Firstly, so maybe

1025
00:36:44,000 --> 00:36:48,240
say that there is a challenge of

1026
00:36:45,440 --> 00:36:51,680
explanability. Is it that we don't know

1027
00:36:48,240 --> 00:36:54,880
which arrow it went through? That's one.

1028
00:36:51,680 --> 00:36:56,400
Second, uh who's controlling the number

1029
00:36:54,880 --> 00:36:58,960
of iterations or the number of

1030
00:36:56,400 --> 00:36:59,680
functions? That's up to us. Or how is

1031
00:36:58,960 --> 00:37:01,440
that?

1032
00:36:59,680 --> 00:37:04,160
>> Right. So yeah. So the the first

1033
00:37:01,440 --> 00:37:06,960
question um explainability we actually

1034
00:37:04,160 --> 00:37:09,200
know exactly for any given input input

1035
00:37:06,960 --> 00:37:10,880
uh data data point we know exactly how

1036
00:37:09,200 --> 00:37:13,599
it flows through the network. So there

1037
00:37:10,880 --> 00:37:16,640
is no problem there. The problem is in

1038
00:37:13,599 --> 00:37:19,200
ascribing okay this we we think this

1039
00:37:16,640 --> 00:37:22,000
person is going to be uh repay the loan

1040
00:37:19,200 --> 00:37:24,079
because of this particular attribute we

1041
00:37:22,000 --> 00:37:26,079
don't know that because those attributes

1042
00:37:24,079 --> 00:37:27,920
all get enshed together and goes to this

1043
00:37:26,079 --> 00:37:29,599
complicated thing so we know exactly

1044
00:37:27,920 --> 00:37:32,079
what happens we just can't give credit

1045
00:37:29,599 --> 00:37:34,160
to any one thing very easily I'm again

1046
00:37:32,079 --> 00:37:35,760
I'm just standing on the brink of this

1047
00:37:34,160 --> 00:37:37,680
vast ocean of something called

1048
00:37:35,760 --> 00:37:39,440
explanability and interpretability uh

1049
00:37:37,680 --> 00:37:42,400
which I'll get to a bit later on in the

1050
00:37:39,440 --> 00:37:44,400
semester but that's sort of the quick

1051
00:37:42,400 --> 00:37:48,720
kind of rightish kind of wrong answer.

1052
00:37:44,400 --> 00:37:50,480
Okay. Number two um uh we decide the

1053
00:37:48,720 --> 00:37:51,920
number of layers we decide a whole bunch

1054
00:37:50,480 --> 00:37:53,520
of things and as we'll see in a few

1055
00:37:51,920 --> 00:37:54,800
minutes uh that is something that's

1056
00:37:53,520 --> 00:37:56,560
given to us and something we get to

1057
00:37:54,800 --> 00:37:59,440
design and I'll make it very clear which

1058
00:37:56,560 --> 00:38:02,079
is which

1059
00:37:59,440 --> 00:38:03,200
yeah

1060
00:38:02,079 --> 00:38:06,000
>> did I say your name right?

1061
00:38:03,200 --> 00:38:08,240
>> Yeah. So which functions have to be

1062
00:38:06,000 --> 00:38:09,359
linear and also like why does it have to

1063
00:38:08,240 --> 00:38:12,640
be linear?

1064
00:38:09,359 --> 00:38:15,680
>> Yeah. So these functions uh the f ofx

1065
00:38:12,640 --> 00:38:17,359
here they have to be nonlinear. As to

1066
00:38:15,680 --> 00:38:19,599
why they have to be nonlinear we'll get

1067
00:38:17,359 --> 00:38:23,599
to that in a few minutes.

1068
00:38:19,599 --> 00:38:25,359
Okay so these are called neurons. Okay

1069
00:38:23,599 --> 00:38:27,920
these things where you basically there's

1070
00:38:25,359 --> 00:38:30,800
a linear function followed by a little

1071
00:38:27,920 --> 00:38:32,800
nonlinear function right this is a each

1072
00:38:30,800 --> 00:38:36,480
one of these things is called a neuron.

1073
00:38:32,800 --> 00:38:39,200
Um, by the way, you know, this is

1074
00:38:36,480 --> 00:38:41,760
loosely inspired by the way how, you

1075
00:38:39,200 --> 00:38:45,599
know, neurons work in human in mamalian

1076
00:38:41,760 --> 00:38:49,119
brains. But the connections between

1077
00:38:45,599 --> 00:38:51,359
neuroscience and deep learning are very

1078
00:38:49,119 --> 00:38:54,320
heavily argued. So, I'm going to like

1079
00:38:51,359 --> 00:38:56,560
stay away from it. Okay? Uh, suffice it

1080
00:38:54,320 --> 00:38:58,160
to say, it's I I just think for for

1081
00:38:56,560 --> 00:38:59,920
building practical deep learning systems

1082
00:38:58,160 --> 00:39:02,480
in industry, you're you don't have to

1083
00:38:59,920 --> 00:39:06,320
worry about this. Okay? All right. Let's

1084
00:39:02,480 --> 00:39:09,359
move on terminology. Uh this vertical

1085
00:39:06,320 --> 00:39:11,040
stack of linear functions or neurons,

1086
00:39:09,359 --> 00:39:13,280
right? This vertical stack is called a

1087
00:39:11,040 --> 00:39:16,000
layer, right? This a layer, that's a

1088
00:39:13,280 --> 00:39:17,359
layer. Uh and these little nonlinear

1089
00:39:16,000 --> 00:39:20,800
functions, which we haven't got into

1090
00:39:17,359 --> 00:39:22,320
yet, are called activation functions. Uh

1091
00:39:20,800 --> 00:39:25,440
and we'll get to why they are called

1092
00:39:22,320 --> 00:39:27,200
that in just a second.

1093
00:39:25,440 --> 00:39:31,040
And

1094
00:39:27,200 --> 00:39:33,280
the input is called an input layer. And

1095
00:39:31,040 --> 00:39:34,960
I have the word layer in double quotes

1096
00:39:33,280 --> 00:39:36,960
because like it's not really doing

1097
00:39:34,960 --> 00:39:40,000
anything, right? It's just the input.

1098
00:39:36,960 --> 00:39:41,920
So, but we call it the input layer. And

1099
00:39:40,000 --> 00:39:43,760
what the very final thing that produces

1100
00:39:41,920 --> 00:39:45,680
our output is called the output layer,

1101
00:39:43,760 --> 00:39:48,480
right? Obviously, and everything in the

1102
00:39:45,680 --> 00:39:50,320
middle is called a hidden layer.

1103
00:39:48,480 --> 00:39:52,560
Okay.

1104
00:39:50,320 --> 00:39:55,040
So, the final piece of terminology is

1105
00:39:52,560 --> 00:39:56,240
that when you have a layer like this in

1106
00:39:55,040 --> 00:39:58,320
which say three numbers are coming out

1107
00:39:56,240 --> 00:40:00,960
and there's another another layer,

1108
00:39:58,320 --> 00:40:03,359
right? If every neuron in this layer is

1109
00:40:00,960 --> 00:40:05,520
connected to every neuron in this layer,

1110
00:40:03,359 --> 00:40:08,079
it's called a fully connected or dense

1111
00:40:05,520 --> 00:40:10,240
layer. So for instance here this arrow

1112
00:40:08,079 --> 00:40:11,280
that's whatever the whatever number is

1113
00:40:10,240 --> 00:40:12,880
coming out. Let's say the number three

1114
00:40:11,280 --> 00:40:15,440
is coming out of this thing here that

1115
00:40:12,880 --> 00:40:17,599
number three goes flows on this arrow to

1116
00:40:15,440 --> 00:40:19,680
this thing flows on this arrow to this

1117
00:40:17,599 --> 00:40:21,359
neuron and flows on this third arrow to

1118
00:40:19,680 --> 00:40:23,520
this neuron. That's what I mean. So

1119
00:40:21,359 --> 00:40:25,359
every neuron its output is being sent to

1120
00:40:23,520 --> 00:40:27,599
every neuron in the following layer.

1121
00:40:25,359 --> 00:40:29,440
Okay, that's we call it fully connected

1122
00:40:27,599 --> 00:40:31,520
or dense.

1123
00:40:29,440 --> 00:40:33,520
And then if you look at logistic

1124
00:40:31,520 --> 00:40:35,520
regression, right, this is just logistic

1125
00:40:33,520 --> 00:40:37,440
regression. You can see basically

1126
00:40:35,520 --> 00:40:40,960
logistic regression is a neural network

1127
00:40:37,440 --> 00:40:42,320
with no hidden layers.

1128
00:40:40,960 --> 00:40:43,760
So in some sense, logistic regression is

1129
00:40:42,320 --> 00:40:46,560
like almost the simplest possible

1130
00:40:43,760 --> 00:40:49,359
network you can think of. Like barely a

1131
00:40:46,560 --> 00:40:50,880
neural network, right? It's got no no

1132
00:40:49,359 --> 00:40:53,839
hidden layers. That's what makes it

1133
00:40:50,880 --> 00:40:55,760
logistic regression. And so as you might

1134
00:40:53,839 --> 00:40:57,359
have guessed by now, deep learning is

1135
00:40:55,760 --> 00:40:58,880
just neural networks with lots and lots

1136
00:40:57,359 --> 00:41:00,319
of

1137
00:40:58,880 --> 00:41:04,640
of what?

1138
00:41:00,319 --> 00:41:07,119
>> Yes, layers. So here are a few

1139
00:41:04,640 --> 00:41:08,560
uh and by the way, these are not even

1140
00:41:07,119 --> 00:41:11,359
considered all that, you know,

1141
00:41:08,560 --> 00:41:13,599
impressive these days. Okay. Uh but I

1142
00:41:11,359 --> 00:41:17,040
put them up because this uh this thing

1143
00:41:13,599 --> 00:41:19,920
here is called RestNet. And it's famous

1144
00:41:17,040 --> 00:41:23,200
because the restnet neural network was I

1145
00:41:19,920 --> 00:41:24,480
think the first network to surpass human

1146
00:41:23,200 --> 00:41:26,800
level performance and image

1147
00:41:24,480 --> 00:41:29,119
classification.

1148
00:41:26,800 --> 00:41:31,599
Sort of it's sort of like the Skynet of

1149
00:41:29,119 --> 00:41:33,440
image classification. Okay, it surpassed

1150
00:41:31,599 --> 00:41:34,800
human level performance and I'm putting

1151
00:41:33,440 --> 00:41:36,480
it up here because we'll actually work

1152
00:41:34,800 --> 00:41:38,079
with RestNet on Wednesday next next

1153
00:41:36,480 --> 00:41:39,760
Wednesday and we'll actually take restn

1154
00:41:38,079 --> 00:41:42,000
net we'll fine-tune it and solve a real

1155
00:41:39,760 --> 00:41:44,000
problem in class.

1156
00:41:42,000 --> 00:41:46,160
All right. So it's got lots and lots of

1157
00:41:44,000 --> 00:41:47,359
layers. Uh now let's turn to these

1158
00:41:46,160 --> 00:41:50,079
activation functions. We've been

1159
00:41:47,359 --> 00:41:52,800
ignoring these little guys right so far.

1160
00:41:50,079 --> 00:41:55,040
So the activation function at a node is

1161
00:41:52,800 --> 00:41:57,200
a first of all it's a function that

1162
00:41:55,040 --> 00:41:58,880
receives a single number and outputs a

1163
00:41:57,200 --> 00:42:01,599
single number right it's not very

1164
00:41:58,880 --> 00:42:03,359
complicated right it receives basically

1165
00:42:01,599 --> 00:42:04,960
this this is a linear function which

1166
00:42:03,359 --> 00:42:07,119
receives all these inputs. It could be

1167
00:42:04,960 --> 00:42:08,640
10 inputs, a thousand inputs, runs it

1168
00:42:07,119 --> 00:42:10,319
through a linear function, outputs a

1169
00:42:08,640 --> 00:42:12,720
number, and that single number, a

1170
00:42:10,319 --> 00:42:15,680
scalar, goes in here, and it comes out

1171
00:42:12,720 --> 00:42:17,760
as another single number. Just just just

1172
00:42:15,680 --> 00:42:19,440
remember that. And so these are some of

1173
00:42:17,760 --> 00:42:21,280
the most common activation functions. In

1174
00:42:19,440 --> 00:42:23,040
fact, the sigmoid we saw, which is

1175
00:42:21,280 --> 00:42:24,960
actually be used for the output, is

1176
00:42:23,040 --> 00:42:27,200
actually a kind of activation function

1177
00:42:24,960 --> 00:42:29,200
where a single number comes in and it

1178
00:42:27,200 --> 00:42:30,880
gets mapped into this curve because of

1179
00:42:29,200 --> 00:42:32,800
this thing. So the single number that

1180
00:42:30,880 --> 00:42:35,520
comes in is a and it and it gets

1181
00:42:32,800 --> 00:42:37,599
transformed as 1 / 1 plus e to minus a

1182
00:42:35,520 --> 00:42:39,760
and you get a shape like this and it's

1183
00:42:37,599 --> 00:42:42,560
called the sigmoid activation function

1184
00:42:39,760 --> 00:42:45,920
and and as you can see here for very

1185
00:42:42,560 --> 00:42:47,920
small values for very negative values

1186
00:42:45,920 --> 00:42:50,319
it's going to be pretty close to zero

1187
00:42:47,920 --> 00:42:52,800
meaning it won't get activated

1188
00:42:50,319 --> 00:42:55,920
and for very very large values it's

1189
00:42:52,800 --> 00:42:58,160
going to be pretty close to one. All the

1190
00:42:55,920 --> 00:42:59,520
action happens in the middle. When your

1191
00:42:58,160 --> 00:43:00,880
when your when your values are somewhere

1192
00:42:59,520 --> 00:43:04,319
in this range, there's a dramatic

1193
00:43:00,880 --> 00:43:05,760
increases in what comes out. Okay? So

1194
00:43:04,319 --> 00:43:09,119
that little thing in the middle is a

1195
00:43:05,760 --> 00:43:11,599
sweet spot for these functions. Uh and

1196
00:43:09,119 --> 00:43:13,119
this, you know, I'm also embarrassed to

1197
00:43:11,599 --> 00:43:14,960
call it an activation function because

1198
00:43:13,119 --> 00:43:16,800
it's literally not doing anything. It's

1199
00:43:14,960 --> 00:43:18,720
sort of getting a nice label for free.

1200
00:43:16,800 --> 00:43:19,920
Um right, you basically it says you just

1201
00:43:18,720 --> 00:43:22,079
get a number, you just pass it straight

1202
00:43:19,920 --> 00:43:23,520
along. It's a linear activation

1203
00:43:22,079 --> 00:43:25,599
function, but just for completeness, I

1204
00:43:23,520 --> 00:43:28,480
want to put it here.

1205
00:43:25,599 --> 00:43:31,119
And then we come to the hero of deep

1206
00:43:28,480 --> 00:43:34,720
learning which is the rectified linear

1207
00:43:31,119 --> 00:43:37,200
unit right rectified linear unit it's

1208
00:43:34,720 --> 00:43:38,800
called relu uh and relu is going to

1209
00:43:37,200 --> 00:43:41,200
become part of your vocabulary very very

1210
00:43:38,800 --> 00:43:43,119
quickly uh and so relu is actually a

1211
00:43:41,200 --> 00:43:45,119
very interesting function so you write

1212
00:43:43,119 --> 00:43:48,400
it as maximum of whatever number and

1213
00:43:45,119 --> 00:43:50,240
zero which is another way of saying if

1214
00:43:48,400 --> 00:43:52,480
the number is positive just send it

1215
00:43:50,240 --> 00:43:55,599
along unchanged if the number is

1216
00:43:52,480 --> 00:43:57,839
negative send a zero instead squash it

1217
00:43:55,599 --> 00:43:59,920
to zero. So which means if the number is

1218
00:43:57,839 --> 00:44:03,280
negative nothing happens. If the number

1219
00:43:59,920 --> 00:44:05,200
is positive it wakes up.

1220
00:44:03,280 --> 00:44:07,280
So what happens is that you could have a

1221
00:44:05,200 --> 00:44:09,599
very complicated linear function with

1222
00:44:07,280 --> 00:44:10,640
millions of variables and then it puts a

1223
00:44:09,599 --> 00:44:12,000
single number and that number

1224
00:44:10,640 --> 00:44:13,359
unfortunately happens to be negative.

1225
00:44:12,000 --> 00:44:16,160
The rel is not impressed. It's going to

1226
00:44:13,359 --> 00:44:19,920
sort of zero out. Okay. It's a very

1227
00:44:16,160 --> 00:44:21,359
simple function and many many folks who

1228
00:44:19,920 --> 00:44:24,240
have been in deep learning for a long

1229
00:44:21,359 --> 00:44:27,359
long time believe that the use of the

1230
00:44:24,240 --> 00:44:29,599
relus is one of the key factors that led

1231
00:44:27,359 --> 00:44:31,119
to the amazing success of deep learning

1232
00:44:29,599 --> 00:44:33,440
because it's got some very interesting

1233
00:44:31,119 --> 00:44:35,920
properties uh which we'll get to

1234
00:44:33,440 --> 00:44:40,079
hopefully on Wednesday.

1235
00:44:35,920 --> 00:44:42,160
Okay. So the short hand here is that um

1236
00:44:40,079 --> 00:44:43,760
whenever you see this thing it's just a

1237
00:44:42,160 --> 00:44:44,880
linear activation linear function

1238
00:44:43,760 --> 00:44:47,440
followed by just sending it straight

1239
00:44:44,880 --> 00:44:49,440
out. If I if you do this this if I put a

1240
00:44:47,440 --> 00:44:52,319
relu in here I'm going to denote it like

1241
00:44:49,440 --> 00:44:54,160
that which mimics the graph uh how it

1242
00:44:52,319 --> 00:44:56,000
looks. And if I'm if I put a sigmo it

1243
00:44:54,160 --> 00:44:59,941
I'm just going to use this thing here.

1244
00:44:56,000 --> 00:45:01,440
Okay just a visual shorthand

1245
00:44:59,941 --> 00:45:03,520
[clears throat] there are many other

1246
00:45:01,440 --> 00:45:04,720
functions activation functions by the

1247
00:45:03,520 --> 00:45:07,520
way. There's something called the

1248
00:45:04,720 --> 00:45:09,839
tanhage function, the leaky relu, the

1249
00:45:07,520 --> 00:45:12,079
gillu, the swish. I mean it's like a

1250
00:45:09,839 --> 00:45:13,599
like a menagery of activation functions

1251
00:45:12,079 --> 00:45:15,200
because very often researchers will be

1252
00:45:13,599 --> 00:45:16,560
like well I don't like this activation

1253
00:45:15,200 --> 00:45:17,440
function. Here's a little modified

1254
00:45:16,560 --> 00:45:19,440
version of the function which is going

1255
00:45:17,440 --> 00:45:21,200
to be better for certain things. So you

1256
00:45:19,440 --> 00:45:23,599
know people's research creativity is

1257
00:45:21,200 --> 00:45:26,240
sort of at this point has gone unhinged.

1258
00:45:23,599 --> 00:45:29,200
Um so lots of options but if you just

1259
00:45:26,240 --> 00:45:31,200
stick to the reu for your hidden layers

1260
00:45:29,200 --> 00:45:32,400
you can basically get anything done

1261
00:45:31,200 --> 00:45:33,839
practically right? You don't have to

1262
00:45:32,400 --> 00:45:36,400
worry about anything else. So we'll only

1263
00:45:33,839 --> 00:45:38,640
focus on ReLUS for all the intermediate

1264
00:45:36,400 --> 00:45:40,480
stuff. Uh yeah.

1265
00:45:38,640 --> 00:45:42,160
>> Yeah. How do you gauge which activation

1266
00:45:40,480 --> 00:45:42,880
function is more suited for your use

1267
00:45:42,160 --> 00:45:45,280
case?

1268
00:45:42,880 --> 00:45:48,079
>> Yeah. So the rule of thumb here is that

1269
00:45:45,280 --> 00:45:49,839
for your hidden layers, use ReLUS,

1270
00:45:48,079 --> 00:45:52,240
right? Because empirically we have seen

1271
00:45:49,839 --> 00:45:54,400
that they they do an amazing job. For

1272
00:45:52,240 --> 00:45:56,400
your output layer, your very final

1273
00:45:54,400 --> 00:45:58,079
thing, you actually don't have a choice

1274
00:45:56,400 --> 00:45:59,760
because what you have to use depends on

1275
00:45:58,079 --> 00:46:01,440
what kind of output you have to work

1276
00:45:59,760 --> 00:46:02,880
with. If it's an output which is a

1277
00:46:01,440 --> 00:46:05,839
probability number between 0 and one,

1278
00:46:02,880 --> 00:46:07,760
you have to use a sigmoid. U if it is

1279
00:46:05,839 --> 00:46:09,280
say 10 numbers all of which have to be

1280
00:46:07,760 --> 00:46:11,040
probabilities and they have to add up to

1281
00:46:09,280 --> 00:46:13,040
one you got to use something called the

1282
00:46:11,040 --> 00:46:14,960
softmax which we'll get to on Wednesday.

1283
00:46:13,040 --> 00:46:16,319
So it really depends on the output and

1284
00:46:14,960 --> 00:46:18,800
the nature of the output dictates what

1285
00:46:16,319 --> 00:46:22,079
you use in the output layer.

1286
00:46:18,800 --> 00:46:24,079
Okay. So coming back to this, so if you

1287
00:46:22,079 --> 00:46:27,599
want to design a deep neural neural

1288
00:46:24,079 --> 00:46:30,000
network, uh the input is the input, the

1289
00:46:27,599 --> 00:46:31,200
output is the output. And so you get to

1290
00:46:30,000 --> 00:46:33,200
choose everything else. You get to

1291
00:46:31,200 --> 00:46:35,440
choose the number of hidden layers, the

1292
00:46:33,200 --> 00:46:37,760
number of neurons in each layer, the

1293
00:46:35,440 --> 00:46:39,680
activation functions you're going to use

1294
00:46:37,760 --> 00:46:41,280
and uh for the hidden layers. And then

1295
00:46:39,680 --> 00:46:42,880
you have to make sure that the what you

1296
00:46:41,280 --> 00:46:44,480
choose for the output layer matches the

1297
00:46:42,880 --> 00:46:46,960
kind of output you want to generate.

1298
00:46:44,480 --> 00:46:48,880
Okay. So this is this sort of this is

1299
00:46:46,960 --> 00:46:52,560
all in your hands. You decide what

1300
00:46:48,880 --> 00:46:53,520
happens. But you will there's a lot of

1301
00:46:52,560 --> 00:46:54,800
guidance for how to do these things

1302
00:46:53,520 --> 00:46:57,760
which you'll which we'll cover as we go

1303
00:46:54,800 --> 00:47:00,640
along. Did you have a question

1304
00:46:57,760 --> 00:47:03,520
>> kind of but I guess I'll do it. Um

1305
00:47:00,640 --> 00:47:07,119
is is there also exploration in kind of

1306
00:47:03,520 --> 00:47:09,119
dynamic uh setting of layers so that a

1307
00:47:07,119 --> 00:47:11,760
user isn't determining the amount of

1308
00:47:09,119 --> 00:47:12,720
layers itself

1309
00:47:11,760 --> 00:47:14,400
layers.

1310
00:47:12,720 --> 00:47:17,440
>> Yeah. So there's a whole field called

1311
00:47:14,400 --> 00:47:18,960
neural architecture search NAS where we

1312
00:47:17,440 --> 00:47:21,200
can actually try a whole bunch of

1313
00:47:18,960 --> 00:47:22,720
different architectures uh and then use

1314
00:47:21,200 --> 00:47:24,160
some optimization and in fact

1315
00:47:22,720 --> 00:47:26,640
reinforcement learning which we won't

1316
00:47:24,160 --> 00:47:28,319
get to in this class as a way to figure

1317
00:47:26,640 --> 00:47:32,560
out really good architectures for any

1318
00:47:28,319 --> 00:47:34,560
particular problem. uh but the you the

1319
00:47:32,560 --> 00:47:36,800
question of okay when I'm training a

1320
00:47:34,560 --> 00:47:38,400
model with a particular kind of data the

1321
00:47:36,800 --> 00:47:39,680
first pass through the training data I'm

1322
00:47:38,400 --> 00:47:41,760
going to use two layers the second pass

1323
00:47:39,680 --> 00:47:44,240
I'm going to do seven layers that is not

1324
00:47:41,760 --> 00:47:46,000
done uh and the reason it's not done is

1325
00:47:44,240 --> 00:47:47,440
because of certain other constraints we

1326
00:47:46,000 --> 00:47:49,040
have and how we can do the the

1327
00:47:47,440 --> 00:47:51,200
optimization and the gradient descent

1328
00:47:49,040 --> 00:47:52,880
and stuff like that but what you can do

1329
00:47:51,200 --> 00:47:55,839
and we will you we'll look at this thing

1330
00:47:52,880 --> 00:47:57,599
called dropout for certain layers you

1331
00:47:55,839 --> 00:48:00,079
can actually for each time you run it

1332
00:47:57,599 --> 00:48:01,599
through the network you and decide in

1333
00:48:00,079 --> 00:48:03,280
this layer I'm not going to use all the

1334
00:48:01,599 --> 00:48:05,680
nodes. I'm going to drop out a few of

1335
00:48:03,280 --> 00:48:06,720
the nodes randomly and it's a very

1336
00:48:05,680 --> 00:48:08,160
effective technique to prevent

1337
00:48:06,720 --> 00:48:11,599
overfitting and we'll come to that a

1338
00:48:08,160 --> 00:48:14,079
little later on. Uh yeah,

1339
00:48:11,599 --> 00:48:17,119
one question regarding like uh neural

1340
00:48:14,079 --> 00:48:21,280
networks is about the coefficients to

1341
00:48:17,119 --> 00:48:23,599
decide not or we have user deficient for

1342
00:48:21,280 --> 00:48:25,680
the brain system. No, the whole trick

1343
00:48:23,599 --> 00:48:28,880
here, the whole name of the game is we

1344
00:48:25,680 --> 00:48:30,240
use the data, the training data and

1345
00:48:28,880 --> 00:48:32,480
something called a loss function, which

1346
00:48:30,240 --> 00:48:34,640
I'll get to on Wednesday, along with an

1347
00:48:32,480 --> 00:48:37,040
optimization algorithm. So the the

1348
00:48:34,640 --> 00:48:38,000
network figures out by itself what the

1349
00:48:37,040 --> 00:48:39,839
weights need to be, what the

1350
00:48:38,000 --> 00:48:42,720
coefficients need to be so as to

1351
00:48:39,839 --> 00:48:44,319
minimize prediction error. And that's

1352
00:48:42,720 --> 00:48:46,319
the whole thing. And the magic here is

1353
00:48:44,319 --> 00:48:48,559
that we don't have to do anything. We

1354
00:48:46,319 --> 00:48:50,400
only have to set it up, sit back often

1355
00:48:48,559 --> 00:48:51,760
for many hours and watch it do its

1356
00:48:50,400 --> 00:48:52,880
thing.

1357
00:48:51,760 --> 00:48:54,480
Yep.

1358
00:48:52,880 --> 00:48:56,160
>> Just one last question. Um, you

1359
00:48:54,480 --> 00:48:58,240
mentioned nodes just now when you were

1360
00:48:56,160 --> 00:49:01,119
answering Rolando's question. Can you

1361
00:48:58,240 --> 00:49:02,800
just confirm exactly what a node is? I

1362
00:49:01,119 --> 00:49:04,960
have an idea that it's basically any

1363
00:49:02,800 --> 00:49:05,520
circle, but you've just added a lot more

1364
00:49:04,960 --> 00:49:06,960
detail.

1365
00:49:05,520 --> 00:49:08,160
>> Sure. No, when when I'm referring to a

1366
00:49:06,960 --> 00:49:10,319
node, I'm literally referring to

1367
00:49:08,160 --> 00:49:12,800
something like this, which think of it

1368
00:49:10,319 --> 00:49:15,760
as a linear function followed by a

1369
00:49:12,800 --> 00:49:17,200
nonlinear activation. So it it re a

1370
00:49:15,760 --> 00:49:19,359
bunch of inputs runs it through a linear

1371
00:49:17,200 --> 00:49:21,440
function and pass it through like a relu

1372
00:49:19,359 --> 00:49:24,240
or a sigmoid or something and out pops a

1373
00:49:21,440 --> 00:49:26,079
number. So in general a node will have

1374
00:49:24,240 --> 00:49:29,119
many numbers potentially coming in but

1375
00:49:26,079 --> 00:49:31,599
only one number going out. Now that one

1376
00:49:29,119 --> 00:49:33,680
number may get copied to every node in

1377
00:49:31,599 --> 00:49:34,880
the next layer but what comes out of

1378
00:49:33,680 --> 00:49:36,720
that particular node is just a single

1379
00:49:34,880 --> 00:49:40,079
number.

1380
00:49:36,720 --> 00:49:43,760
All right. So uh so let's use a DNN for

1381
00:49:40,079 --> 00:49:45,520
our interview um example. So in this

1382
00:49:43,760 --> 00:49:47,599
problem we had two inputs right GP and

1383
00:49:45,520 --> 00:49:48,640
experience the output variable has to be

1384
00:49:47,599 --> 00:49:49,680
between 0ero and one because you're

1385
00:49:48,640 --> 00:49:52,000
trying to predict the probability that

1386
00:49:49,680 --> 00:49:54,480
someone gets called for an interview. So

1387
00:49:52,000 --> 00:49:56,800
the output size is fixed the sorry the

1388
00:49:54,480 --> 00:49:59,760
input size is fixed the output is fixed

1389
00:49:56,800 --> 00:50:00,960
u and we so since it's really only the

1390
00:49:59,760 --> 00:50:04,000
very first network we're actually

1391
00:50:00,960 --> 00:50:06,160
playing with uh let's just start simple

1392
00:50:04,000 --> 00:50:08,880
right we'll just have one hidden layer

1393
00:50:06,160 --> 00:50:11,200
and we'll have three neurons right and

1394
00:50:08,880 --> 00:50:13,359
and as I mentioned to Tomaso's question

1395
00:50:11,200 --> 00:50:15,440
from before if you are choosing

1396
00:50:13,359 --> 00:50:16,960
activation functions in the hidden

1397
00:50:15,440 --> 00:50:18,960
layers just go with the relu as a

1398
00:50:16,960 --> 00:50:21,200
default it usually works really well out

1399
00:50:18,960 --> 00:50:22,960
of the box so we'll just use a relu and

1400
00:50:21,200 --> 00:50:25,119
Since the output has to be between 0 and

1401
00:50:22,960 --> 00:50:27,119
one, we don't have a choice. We have to

1402
00:50:25,119 --> 00:50:29,359
use a sigmoid for the output layer.

1403
00:50:27,119 --> 00:50:31,280
Okay, that's it. So we have those those

1404
00:50:29,359 --> 00:50:33,040
are the design choices. And when we do

1405
00:50:31,280 --> 00:50:35,200
that, this is how it's looked like,

1406
00:50:33,040 --> 00:50:36,880
right? We have two inputs x1 and x2 gpn

1407
00:50:35,200 --> 00:50:41,319
experience and then it goes through

1408
00:50:36,880 --> 00:50:41,319
these three uh reloosmo

1409
00:50:42,880 --> 00:50:47,599
and we get a probability y at the end.

1410
00:50:44,960 --> 00:50:50,480
All right, quick question concept check.

1411
00:50:47,599 --> 00:50:51,920
How many weights how many parameters

1412
00:50:50,480 --> 00:50:53,839
both weights and biases does this

1413
00:50:51,920 --> 00:50:56,839
network have? Just take a moment to

1414
00:50:53,839 --> 00:50:56,839
count.

1415
00:51:11,200 --> 00:51:15,079
All right. Any guesses?

1416
00:51:15,599 --> 00:51:18,599
>> Yeah.

1417
00:51:18,640 --> 00:51:23,920
I think you're almost there.

1418
00:51:22,160 --> 00:51:25,520
Um,

1419
00:51:23,920 --> 00:51:29,319
are folks going to be doing a binary

1420
00:51:25,520 --> 00:51:29,319
search on this now? Okay.

1421
00:51:29,359 --> 00:51:31,680
>> Uh, no.

1422
00:51:31,200 --> 00:51:32,240
>> Yes.

1423
00:51:31,680 --> 00:51:36,160
>> 13.

1424
00:51:32,240 --> 00:51:37,599
>> Yes. Very good. So, that's 13. And my

1425
00:51:36,160 --> 00:51:39,119
guess is that the reason you came up

1426
00:51:37,599 --> 00:51:41,119
with 12, and I made the same mistake,

1427
00:51:39,119 --> 00:51:45,280
that's why I know it is you probably

1428
00:51:41,119 --> 00:51:48,319
forgot this green thing here.

1429
00:51:45,280 --> 00:51:50,880
Um so so what folks often forget is the

1430
00:51:48,319 --> 00:51:52,880
bias right we all count the things right

1431
00:51:50,880 --> 00:51:55,440
okay and the easy way to do it is okay

1432
00:51:52,880 --> 00:51:59,440
two things here uh three things here 2 *

1433
00:51:55,440 --> 00:52:00,880
6 3 is six uh 3 * 1 is 3 another nine

1434
00:51:59,440 --> 00:52:02,480
and then you have to add up all the

1435
00:52:00,880 --> 00:52:04,960
intercepts

1436
00:52:02,480 --> 00:52:07,440
right so you get 13 and so when we get

1437
00:52:04,960 --> 00:52:08,960
to very complicated networks the the

1438
00:52:07,440 --> 00:52:11,359
first two or three times you work with

1439
00:52:08,960 --> 00:52:13,040
very complex networks uh and we'll do it

1440
00:52:11,359 --> 00:52:15,119
you know starting very soon just get

1441
00:52:13,040 --> 00:52:16,640
into the habit of hand calculating the

1442
00:52:15,119 --> 00:52:18,319
number of parameters just to make sure

1443
00:52:16,640 --> 00:52:19,839
you understand what's going on. Once you

1444
00:52:18,319 --> 00:52:21,280
get it right a couple of times, you

1445
00:52:19,839 --> 00:52:22,880
don't have to do it anymore. Okay, the

1446
00:52:21,280 --> 00:52:25,520
first couple of times hand calculate to

1447
00:52:22,880 --> 00:52:27,040
make sure you get it. Okay, so yeah, so

1448
00:52:25,520 --> 00:52:30,160
let's say that we have trained this

1449
00:52:27,040 --> 00:52:32,240
network using you know using techniques

1450
00:52:30,160 --> 00:52:33,839
which we'll cover on Wednesday and it is

1451
00:52:32,240 --> 00:52:36,000
it comes back to you after training and

1452
00:52:33,839 --> 00:52:38,480
says okay these are the optimal the best

1453
00:52:36,000 --> 00:52:40,559
values for the weights and the biases

1454
00:52:38,480 --> 00:52:43,920
that I have found. So now your network

1455
00:52:40,559 --> 00:52:46,000
is ready for action is ready to be used

1456
00:52:43,920 --> 00:52:47,359
and so so what you can do is let's say

1457
00:52:46,000 --> 00:52:50,880
that you want to predict with this

1458
00:52:47,359 --> 00:52:53,680
network um you know if you have x1 and

1459
00:52:50,880 --> 00:52:55,599
x2 what comes out of what so what comes

1460
00:52:53,680 --> 00:52:58,800
out of the stop neuron right let's call

1461
00:52:55,599 --> 00:53:00,400
it a1 it's basically this

1462
00:52:58,800 --> 00:53:02,400
okay that's what's coming out of this

1463
00:53:00,400 --> 00:53:06,480
thing for any x1 and x2 this is what's

1464
00:53:02,400 --> 00:53:08,720
coming out similarly for a2 and a3 okay

1465
00:53:06,480 --> 00:53:12,160
and then what comes out at the very end

1466
00:53:08,720 --> 00:53:15,280
is basically a1 * that plus a2 * that

1467
00:53:12,160 --> 00:53:17,040
plus a3 * that plus 0.05 and the whole

1468
00:53:15,280 --> 00:53:20,160
thing gets run through the sigmoid and

1469
00:53:17,040 --> 00:53:21,599
that's what you get. Okay, so this slide

1470
00:53:20,160 --> 00:53:22,960
and the one before just make sure you

1471
00:53:21,599 --> 00:53:24,640
look at it afterwards and to make sure

1472
00:53:22,960 --> 00:53:27,440
you totally understand the mechanics of

1473
00:53:24,640 --> 00:53:28,880
it because this is really important. If

1474
00:53:27,440 --> 00:53:30,640
you don't if you don't fully understand

1475
00:53:28,880 --> 00:53:32,000
like internalize the mechanics when we

1476
00:53:30,640 --> 00:53:33,760
get to things like transformers it's

1477
00:53:32,000 --> 00:53:35,200
going to get hard. Okay. So just make

1478
00:53:33,760 --> 00:53:37,200
sure it's like automatic at this point.

1479
00:53:35,200 --> 00:53:40,960
It should be reflexive.

1480
00:53:37,200 --> 00:53:42,000
Um okay so yeah and so when we when you

1481
00:53:40,960 --> 00:53:42,960
want to predict anything you just run

1482
00:53:42,000 --> 00:53:45,040
some numbers through it you get all

1483
00:53:42,960 --> 00:53:46,720
these things uh and boom you calculate

1484
00:53:45,040 --> 00:53:48,559
it turns out to be 22.6 fixed. That's

1485
00:53:46,720 --> 00:53:50,960
the answer.

1486
00:53:48,559 --> 00:53:52,480
All right. So, uh I just want to say

1487
00:53:50,960 --> 00:53:55,440
that let's say that you built this

1488
00:53:52,480 --> 00:53:57,200
network and now you're like, "Hey, um

1489
00:53:55,440 --> 00:54:00,240
given any x1 and x2, I can come up with

1490
00:53:57,200 --> 00:54:02,000
a y, but I'm feeling a little matty. Can

1491
00:54:00,240 --> 00:54:03,280
you actually write down the function?"

1492
00:54:02,000 --> 00:54:06,920
Yeah, you can write down the function.

1493
00:54:03,280 --> 00:54:06,920
This is what it looks like.

1494
00:54:07,359 --> 00:54:13,200
Super interpretable, right?

1495
00:54:10,480 --> 00:54:16,880
So this goes to the comment that it made

1496
00:54:13,200 --> 00:54:19,359
earlier on where the act of uh depicting

1497
00:54:16,880 --> 00:54:21,760
something using this sort of graphical

1498
00:54:19,359 --> 00:54:24,319
layout makes it so much easier to reason

1499
00:54:21,760 --> 00:54:25,520
with and to think about uh compared to

1500
00:54:24,319 --> 00:54:28,079
trying to figure out what this function

1501
00:54:25,520 --> 00:54:31,680
is doing. Right? The other point I want

1502
00:54:28,079 --> 00:54:33,040
to make is that um this contrast what we

1503
00:54:31,680 --> 00:54:34,319
just saw with the logistic regression

1504
00:54:33,040 --> 00:54:38,400
thing we saw earlier which was this

1505
00:54:34,319 --> 00:54:40,880
little function. And so here

1506
00:54:38,400 --> 00:54:42,960
even this simple network with just three

1507
00:54:40,880 --> 00:54:45,280
hidden layers sorry three nodes in that

1508
00:54:42,960 --> 00:54:47,280
single hidden layer right it's so much

1509
00:54:45,280 --> 00:54:48,960
more complicated than the logistic

1510
00:54:47,280 --> 00:54:51,520
regression model so much more

1511
00:54:48,960 --> 00:54:54,720
complicated right and it is from this

1512
00:54:51,520 --> 00:54:56,800
complexity springs the ability of these

1513
00:54:54,720 --> 00:54:58,240
networks to do basically magical things

1514
00:54:56,800 --> 00:55:00,079
right that's where the complexity comes

1515
00:54:58,240 --> 00:55:02,640
from that's where the magic comes from

1516
00:55:00,079 --> 00:55:04,000
so and here in this case the number of

1517
00:55:02,640 --> 00:55:05,920
variables hasn't even changed it's still

1518
00:55:04,000 --> 00:55:07,920
only two,

1519
00:55:05,920 --> 00:55:10,400
but we can go from the two inputs to the

1520
00:55:07,920 --> 00:55:12,000
one output in very complicated ways as

1521
00:55:10,400 --> 00:55:13,280
long as we know how to train these

1522
00:55:12,000 --> 00:55:15,599
networks the right way. That's sort of

1523
00:55:13,280 --> 00:55:17,680
the m the secret source which we'll

1524
00:55:15,599 --> 00:55:19,520
spend a lot of time on. So yeah, to

1525
00:55:17,680 --> 00:55:21,440
summarize uh this is what we have. It's

1526
00:55:19,520 --> 00:55:22,800
a deep neural network. Uh by the way,

1527
00:55:21,440 --> 00:55:24,559
this kind of network where things just

1528
00:55:22,800 --> 00:55:27,680
flow from left to right is called a feed

1529
00:55:24,559 --> 00:55:29,520
forward uh neural network. uh in

1530
00:55:27,680 --> 00:55:31,119
contrast to some other kinds of networks

1531
00:55:29,520 --> 00:55:34,720
called recurrent networks which we won't

1532
00:55:31,119 --> 00:55:37,119
get to uh in this class because

1533
00:55:34,720 --> 00:55:39,040
transformers have actually proven to be

1534
00:55:37,119 --> 00:55:40,800
much more capable than recurrent

1535
00:55:39,040 --> 00:55:42,960
networks and those have become the norm.

1536
00:55:40,800 --> 00:55:45,119
So we'll just focus on those instead. Um

1537
00:55:42,960 --> 00:55:46,720
and so this arrangement of neurons into

1538
00:55:45,119 --> 00:55:48,319
layers and activation functions and all

1539
00:55:46,720 --> 00:55:50,240
that stuff this called the architecture

1540
00:55:48,319 --> 00:55:52,240
of the neural network and as you will

1541
00:55:50,240 --> 00:55:54,079
see later on the transformer the famous

1542
00:55:52,240 --> 00:55:55,920
transformer network [clears throat] is

1543
00:55:54,079 --> 00:55:58,000
just an example of a particular neural

1544
00:55:55,920 --> 00:55:59,680
network architecture much like

1545
00:55:58,000 --> 00:56:01,680
convolutional neural networks which

1546
00:55:59,680 --> 00:56:03,520
we'll get to next week for computer

1547
00:56:01,680 --> 00:56:06,319
vision or another example of a

1548
00:56:03,520 --> 00:56:07,680
particular network of architecture. So

1549
00:56:06,319 --> 00:56:09,040
uh we will focus on transformers they

1550
00:56:07,680 --> 00:56:10,960
are a particular kind of architecture.

1551
00:56:09,040 --> 00:56:13,040
All right. So in summary, that's what we

1552
00:56:10,960 --> 00:56:14,160
have. Um, you know, you get to choose

1553
00:56:13,040 --> 00:56:15,200
the hidden layers, the neurons,

1554
00:56:14,160 --> 00:56:17,119
activation functions, and stuff like

1555
00:56:15,200 --> 00:56:18,880
that. Uh, then inputs and outputs are

1556
00:56:17,119 --> 00:56:21,119
what you have to work with. And so we

1557
00:56:18,880 --> 00:56:25,520
will actually take this idea and then

1558
00:56:21,119 --> 00:56:28,160
use it to uh to actually solve a problem

1559
00:56:25,520 --> 00:56:29,359
from start to finish on Wednesday. So I

1560
00:56:28,160 --> 00:56:32,284
think I'm done. I give you three minutes

1561
00:56:29,359 --> 00:56:34,304
back of your day. Thank you.

1562
00:56:32,284 --> 00:56:34,304
[applause]

