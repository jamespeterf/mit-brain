1
00:00:03,120 --> 00:00:09,519
Yeah, it's coming up. Um,

2
00:00:06,480 --> 00:00:11,040
so energy systems optimization is is a

3
00:00:09,519 --> 00:00:13,200
hard problem. Uh, we are talking about

4
00:00:11,040 --> 00:00:15,519
heterogeneous physics. Uh, we are

5
00:00:13,200 --> 00:00:17,760
talking about different uh spatial

6
00:00:15,519 --> 00:00:20,320
temporal scales and how do we approach

7
00:00:17,760 --> 00:00:22,240
this typically in practice is through

8
00:00:20,320 --> 00:00:23,840
physics based modeling. And the good

9
00:00:22,240 --> 00:00:26,000
news is that we know the fundamental

10
00:00:23,840 --> 00:00:28,720
physics of most of these engineering

11
00:00:26,000 --> 00:00:32,880
systems and we can build high fidelity

12
00:00:28,720 --> 00:00:34,320
digital twins. But on on uh the downside

13
00:00:32,880 --> 00:00:36,960
is that they are typically

14
00:00:34,320 --> 00:00:39,120
confidentially very expensive uh in a

15
00:00:36,960 --> 00:00:41,520
forward simulation and even more so for

16
00:00:39,120 --> 00:00:43,520
closed loop optimization and control.

17
00:00:41,520 --> 00:00:45,440
And then this creates uh lot of

18
00:00:43,520 --> 00:00:47,520
scalability issues and a lot of uh

19
00:00:45,440 --> 00:00:49,360
software integration issues from

20
00:00:47,520 --> 00:00:51,840
practical engineering perspectives when

21
00:00:49,360 --> 00:00:56,160
it comes to real-time decision makings

22
00:00:51,840 --> 00:00:58,559
for uh sustainable energy systems. So um

23
00:00:56,160 --> 00:01:00,960
one of the specific examples I want to

24
00:00:58,559 --> 00:01:03,440
walk you through is a practical

25
00:01:00,960 --> 00:01:06,640
deployment of high fidelity model pretty

26
00:01:03,440 --> 00:01:09,520
control in real world uh office building

27
00:01:06,640 --> 00:01:12,479
in Belgium. that was done during my time

28
00:01:09,520 --> 00:01:14,159
at Cow Lumen as a postoc with Lea Helen

29
00:01:12,479 --> 00:01:16,159
at the mechanical engine department

30
00:01:14,159 --> 00:01:19,280
where we spent two years in developing

31
00:01:16,159 --> 00:01:23,119
high fidelity uh building models and

32
00:01:19,280 --> 00:01:24,880
model control to uh optimize operations

33
00:01:23,119 --> 00:01:27,439
of heating ventilation air conditioning

34
00:01:24,880 --> 00:01:29,200
systems in buildings. And you may ask

35
00:01:27,439 --> 00:01:32,479
why we would like to do that why it is

36
00:01:29,200 --> 00:01:38,079
relevant. Well, the buildings uh

37
00:01:32,479 --> 00:01:40,079
globally uh use over 40% of the energy

38
00:01:38,079 --> 00:01:42,240
and in the US alone 70% of the

39
00:01:40,079 --> 00:01:44,400
electricity goes to residential and

40
00:01:42,240 --> 00:01:46,479
commercial buildings and with the raise

41
00:01:44,400 --> 00:01:48,799
of like the growth of the data center

42
00:01:46,479 --> 00:01:50,399
this this percentage will grow even

43
00:01:48,799 --> 00:01:52,799
further

44
00:01:50,399 --> 00:01:54,720
and the problem is that the current

45
00:01:52,799 --> 00:01:57,600
controls in buildings are very simple

46
00:01:54,720 --> 00:01:59,200
rules and proportional integral

47
00:01:57,600 --> 00:02:00,560
derivative controllers that are very

48
00:01:59,200 --> 00:02:02,079
inefficient.

49
00:02:00,560 --> 00:02:04,960
um

50
00:02:02,079 --> 00:02:06,960
that creates a lot of u energy waste, a

51
00:02:04,960 --> 00:02:09,119
lot of overheating, over cooling in in

52
00:02:06,960 --> 00:02:12,400
times that it shouldn't be really

53
00:02:09,119 --> 00:02:14,239
happening. So on the upside of these

54
00:02:12,400 --> 00:02:17,920
advanced control technologies in in

55
00:02:14,239 --> 00:02:19,920
academia in in um practical tools, we

56
00:02:17,920 --> 00:02:21,520
have the capability to op highly

57
00:02:19,920 --> 00:02:23,599
optimize these systems. And in this

58
00:02:21,520 --> 00:02:25,680
specific case study that was running

59
00:02:23,599 --> 00:02:29,520
several months in the field, we have

60
00:02:25,680 --> 00:02:32,720
been able to save over 50% uh on the

61
00:02:29,520 --> 00:02:35,280
energy bill and at the same time improve

62
00:02:32,720 --> 00:02:39,040
thermal comfort in the building uh

63
00:02:35,280 --> 00:02:40,959
occupants. But on the downside, this

64
00:02:39,040 --> 00:02:46,000
project took over a year and army of

65
00:02:40,959 --> 00:02:48,800
PhDs to deliver. And the reason was um

66
00:02:46,000 --> 00:02:50,879
in the modeling and simulation layer

67
00:02:48,800 --> 00:02:52,480
where we spent half a year just building

68
00:02:50,879 --> 00:02:56,319
the high fidelity digital twin using

69
00:02:52,480 --> 00:02:58,879
modelica language and then we have this

70
00:02:56,319 --> 00:03:00,560
high fidelity model but is not really

71
00:02:58,879 --> 00:03:02,159
tailored for control. So we had to

72
00:03:00,560 --> 00:03:04,159
develop second model that is control

73
00:03:02,159 --> 00:03:07,680
oriented with linearizing and and

74
00:03:04,159 --> 00:03:12,000
decomposing the system uh into different

75
00:03:07,680 --> 00:03:16,640
pieces uh then design uh optimal control

76
00:03:12,000 --> 00:03:19,840
strategy using Gurobi and and MATLAB uh

77
00:03:16,640 --> 00:03:21,680
optimization libraries. Then use the

78
00:03:19,840 --> 00:03:23,680
machine learning models for forecasting

79
00:03:21,680 --> 00:03:25,599
our disturbances and then stitch it all

80
00:03:23,680 --> 00:03:28,560
together with functional macab unit in

81
00:03:25,599 --> 00:03:31,360
this uh horrendous uh software project

82
00:03:28,560 --> 00:03:33,120
that took one year to build in order to

83
00:03:31,360 --> 00:03:36,000
actually deploy this in real time

84
00:03:33,120 --> 00:03:38,640
decision making uh on the cloud by

85
00:03:36,000 --> 00:03:41,200
hacking the building management system

86
00:03:38,640 --> 00:03:44,159
with the help of uh the control system

87
00:03:41,200 --> 00:03:45,680
vendors. So obvious that was obvious

88
00:03:44,159 --> 00:03:46,959
that we cannot really commercialize this

89
00:03:45,680 --> 00:03:49,920
technology. Even when we talk with

90
00:03:46,959 --> 00:03:52,159
several building automation companies uh

91
00:03:49,920 --> 00:03:53,519
they are saying to us that okay we don't

92
00:03:52,159 --> 00:03:55,840
have the manpower we don't have the

93
00:03:53,519 --> 00:03:58,080
necessary expertise to deploy this u in

94
00:03:55,840 --> 00:04:00,720
the field. So what are the actual

95
00:03:58,080 --> 00:04:02,080
bottlenecks when it comes to uh

96
00:04:00,720 --> 00:04:03,360
deploying these advanced control

97
00:04:02,080 --> 00:04:06,480
technologies that have been developed

98
00:04:03,360 --> 00:04:08,640
and are in practice in aerospace

99
00:04:06,480 --> 00:04:10,879
automotive and process control

100
00:04:08,640 --> 00:04:14,000
industries in the energy systems. Now

101
00:04:10,879 --> 00:04:17,440
the first one is that uh the building

102
00:04:14,000 --> 00:04:19,600
toolkits um are heterogeneous and uh

103
00:04:17,440 --> 00:04:22,240
high fidelity digital twins are not

104
00:04:19,600 --> 00:04:23,759
control oriented while the blackbox

105
00:04:22,240 --> 00:04:24,960
machine learning models they typically

106
00:04:23,759 --> 00:04:26,479
don't have guarantees. So we're

107
00:04:24,960 --> 00:04:29,840
typically restricted to relatively

108
00:04:26,479 --> 00:04:32,000
simple reduce order uh gray box models.

109
00:04:29,840 --> 00:04:34,479
The second challenge is that we are

110
00:04:32,000 --> 00:04:36,240
solving heterogeneous problems. We are

111
00:04:34,479 --> 00:04:37,759
solving real-time optimization for

112
00:04:36,240 --> 00:04:40,000
constraint decision making. We are

113
00:04:37,759 --> 00:04:42,080
solving forward simulation models for

114
00:04:40,000 --> 00:04:43,840
the complicated heat transfer including

115
00:04:42,080 --> 00:04:45,919
all the nonlinearities and we are

116
00:04:43,840 --> 00:04:48,400
typically solving some supervised

117
00:04:45,919 --> 00:04:50,880
machine learning for forecasting and we

118
00:04:48,400 --> 00:04:54,080
need to stitch them these together but

119
00:04:50,880 --> 00:04:56,080
these methods and uh to theories have

120
00:04:54,080 --> 00:04:59,600
been developed in different communities

121
00:04:56,080 --> 00:05:01,520
and all the software toolkit is very

122
00:04:59,600 --> 00:05:03,199
heterogeneous. So we have different

123
00:05:01,520 --> 00:05:06,320
tools for constraint optimization

124
00:05:03,199 --> 00:05:07,680
control, different tools for solving our

125
00:05:06,320 --> 00:05:10,320
differential equations and different

126
00:05:07,680 --> 00:05:13,360
tools for machine learning. And this was

127
00:05:10,320 --> 00:05:16,080
historically like separated silos that

128
00:05:13,360 --> 00:05:19,440
have been quite uh complicated and

129
00:05:16,080 --> 00:05:22,400
laborous uh to integrate in uh end to

130
00:05:19,440 --> 00:05:25,840
end software stack. But what is exciting

131
00:05:22,400 --> 00:05:28,960
opportunity I see in academia is the

132
00:05:25,840 --> 00:05:30,800
confluence of these two domains. machine

133
00:05:28,960 --> 00:05:34,000
learning community, computer science

134
00:05:30,800 --> 00:05:36,960
community bringing wonderful tools that

135
00:05:34,000 --> 00:05:40,479
enable automatic differentiation um at

136
00:05:36,960 --> 00:05:43,680
scale and democratize u these toolkits

137
00:05:40,479 --> 00:05:45,600
through user-friendly APIs and more and

138
00:05:43,680 --> 00:05:48,160
more ideas from constraint optimization

139
00:05:45,600 --> 00:05:50,240
dynamic systems being embedded in these

140
00:05:48,160 --> 00:05:51,919
uh machine learning toolkits and we call

141
00:05:50,240 --> 00:05:54,880
this scientific machine learning or

142
00:05:51,919 --> 00:05:57,680
physics inform machine learning that uh

143
00:05:54,880 --> 00:06:00,560
enables us to do so and brings us two

144
00:05:57,680 --> 00:06:03,600
major benefit benefits. One, we can

145
00:06:00,560 --> 00:06:05,919
learn with way fewer data because we are

146
00:06:03,600 --> 00:06:08,800
having fewer degrees of freedom in the

147
00:06:05,919 --> 00:06:11,759
model and we can enforce certain

148
00:06:08,800 --> 00:06:13,840
properties like safety or physical

149
00:06:11,759 --> 00:06:15,919
quantities like dissipativity or energy

150
00:06:13,840 --> 00:06:17,280
conservation in applications when we

151
00:06:15,919 --> 00:06:20,479
know that we have these. So we don't

152
00:06:17,280 --> 00:06:23,440
need to discover them from data.

153
00:06:20,479 --> 00:06:25,600
And there are a lot of different domains

154
00:06:23,440 --> 00:06:27,919
that are already working in in this

155
00:06:25,600 --> 00:06:30,639
broader field of scientific mer machine

156
00:06:27,919 --> 00:06:34,479
learning that is emergent and growing

157
00:06:30,639 --> 00:06:36,479
very fast. Here are a few papers uh that

158
00:06:34,479 --> 00:06:38,720
I want to highlight that influence my

159
00:06:36,479 --> 00:06:40,319
thinking in this area is it incomplete

160
00:06:38,720 --> 00:06:42,880
and biased literature review. But this

161
00:06:40,319 --> 00:06:45,600
kind of sample uh a few things in

162
00:06:42,880 --> 00:06:49,039
physics informed neural network for PDS

163
00:06:45,600 --> 00:06:51,520
learning to optimize learning models

164
00:06:49,039 --> 00:06:53,600
which is typically nonlinear system

165
00:06:51,520 --> 00:06:56,319
identification area and learning to

166
00:06:53,600 --> 00:06:58,080
control and what I want to argue today

167
00:06:56,319 --> 00:06:59,759
that even though that these different

168
00:06:58,080 --> 00:07:01,680
methods have been developed by operation

169
00:06:59,759 --> 00:07:03,759
research community machine learning

170
00:07:01,680 --> 00:07:06,400
community controls community or applied

171
00:07:03,759 --> 00:07:09,440
map community they are really the

172
00:07:06,400 --> 00:07:11,599
instance of one higher abstraction of

173
00:07:09,440 --> 00:07:13,759
problem. So we can call scientific or

174
00:07:11,599 --> 00:07:16,479
physics informed machine learning. And

175
00:07:13,759 --> 00:07:19,280
when you look at each of these papers uh

176
00:07:16,479 --> 00:07:21,280
they can be abstracted in the four major

177
00:07:19,280 --> 00:07:23,919
components. Then the first one obviously

178
00:07:21,280 --> 00:07:26,160
is is data. We need some form of active

179
00:07:23,919 --> 00:07:29,199
sampling or data acquisition to sample

180
00:07:26,160 --> 00:07:32,400
our u parametric uncertainty scenarios

181
00:07:29,199 --> 00:07:35,199
or or gather uh the time series data for

182
00:07:32,400 --> 00:07:37,440
system identification. We have some

183
00:07:35,199 --> 00:07:39,599
machine learning function approximator,

184
00:07:37,440 --> 00:07:42,160
some surrogate model that approximates

185
00:07:39,599 --> 00:07:44,000
our unknown relationships in the model.

186
00:07:42,160 --> 00:07:46,800
And then the critical component is a

187
00:07:44,000 --> 00:07:48,960
domain prior. That can be either your

188
00:07:46,800 --> 00:07:51,120
known differential equations or it can

189
00:07:48,960 --> 00:07:54,000
be the structure of your state space

190
00:07:51,120 --> 00:07:55,919
model or it could be constraints in your

191
00:07:54,000 --> 00:07:58,400
optimization problem. Then you embed in

192
00:07:55,919 --> 00:08:00,879
this computational graph.

193
00:07:58,400 --> 00:08:03,520
And the last component is physics or

194
00:08:00,879 --> 00:08:05,680
domain informed loss function where

195
00:08:03,520 --> 00:08:08,160
those quantities that you cannot enforce

196
00:08:05,680 --> 00:08:11,360
as a hard structural constraints or

197
00:08:08,160 --> 00:08:13,599
treat as a implicit layers you enforce

198
00:08:11,360 --> 00:08:15,919
as form of soft constraint penalties

199
00:08:13,599 --> 00:08:18,479
through residual learning and there is a

200
00:08:15,919 --> 00:08:22,560
again whole field of collocation based

201
00:08:18,479 --> 00:08:25,680
um methods um in in dynamical systems um

202
00:08:22,560 --> 00:08:27,199
that can cover these type of approaches

203
00:08:25,680 --> 00:08:31,120
and I would also argue it's nicely

204
00:08:27,199 --> 00:08:34,560
linked with stoastic optimization in O.

205
00:08:31,120 --> 00:08:37,599
Okay. So to give you a quick uh walk

206
00:08:34,560 --> 00:08:39,279
through of the major approaches uh in

207
00:08:37,599 --> 00:08:41,039
scientific machine learning, everyone

208
00:08:39,279 --> 00:08:43,519
knows physics inform neural networks or

209
00:08:41,039 --> 00:08:46,399
pins in short. They have been used for

210
00:08:43,519 --> 00:08:50,880
both forward and inverse modeling mostly

211
00:08:46,399 --> 00:08:52,480
for uh scaling uh inference for PDS. Uh

212
00:08:50,880 --> 00:08:54,720
so they come with several advantages.

213
00:08:52,480 --> 00:08:57,600
They are mesh free. So you don't need to

214
00:08:54,720 --> 00:09:01,680
uh construct your finite discretization

215
00:08:57,600 --> 00:09:03,760
beforehand. You can sample with finer uh

216
00:09:01,680 --> 00:09:06,480
samples for offline training and then

217
00:09:03,760 --> 00:09:09,040
query your solutions only at the spatial

218
00:09:06,480 --> 00:09:10,560
temporal points when you need um and at

219
00:09:09,040 --> 00:09:12,640
the inference time you're typically

220
00:09:10,560 --> 00:09:15,200
getting orders of magnitude acceleration

221
00:09:12,640 --> 00:09:17,200
because you are uh approximating the

222
00:09:15,200 --> 00:09:19,600
solution in the learned function

223
00:09:17,200 --> 00:09:22,399
approximator typically neural network.

224
00:09:19,600 --> 00:09:25,839
So how do these components look

225
00:09:22,399 --> 00:09:28,399
specifically? Well, the data set are

226
00:09:25,839 --> 00:09:30,480
collocation points in spatial temporal

227
00:09:28,399 --> 00:09:33,279
coordinates. So here simple example you

228
00:09:30,480 --> 00:09:36,240
have temporal domain spatial domain and

229
00:09:33,279 --> 00:09:40,240
you sample these points uh that you

230
00:09:36,240 --> 00:09:43,360
query your neural network model to

231
00:09:40,240 --> 00:09:45,760
generate candidate solution that aims to

232
00:09:43,360 --> 00:09:47,519
satisfy your partial differential

233
00:09:45,760 --> 00:09:50,880
equation. There's a neck layer. So you

234
00:09:47,519 --> 00:09:53,839
embed this PD as a layer uh in your

235
00:09:50,880 --> 00:09:55,760
computation graph. For that uh to

236
00:09:53,839 --> 00:09:57,200
represent the PD that is dependent now

237
00:09:55,760 --> 00:09:58,720
on the neural network solution, you need

238
00:09:57,200 --> 00:10:02,000
to compute these partial derivatives.

239
00:09:58,720 --> 00:10:04,399
But thanks to uh wonderful and scalable

240
00:10:02,000 --> 00:10:06,560
automatic differentiation tool kits, we

241
00:10:04,399 --> 00:10:09,519
can compute them rather efficiently. And

242
00:10:06,560 --> 00:10:11,680
now you construct your pin loss function

243
00:10:09,519 --> 00:10:15,200
which is nothing else as empirical risk

244
00:10:11,680 --> 00:10:17,279
minimization on this PTE equality

245
00:10:15,200 --> 00:10:19,040
constraint. and potentially some

246
00:10:17,279 --> 00:10:20,800
supervisory loss function that you

247
00:10:19,040 --> 00:10:22,000
typically impose on your initial and

248
00:10:20,800 --> 00:10:24,079
boundary conditions. So this is a

249
00:10:22,000 --> 00:10:28,000
vanilla pin and there is a lot of

250
00:10:24,079 --> 00:10:29,680
research in advanced architectures. Uh

251
00:10:28,000 --> 00:10:31,760
how do we sample these collocation

252
00:10:29,680 --> 00:10:36,240
points? How do we quantify errors? how

253
00:10:31,760 --> 00:10:39,040
do we uh weight the relative um terms in

254
00:10:36,240 --> 00:10:40,240
this multi-term loss function? But here

255
00:10:39,040 --> 00:10:43,680
I just want to like show you this

256
00:10:40,240 --> 00:10:46,640
vanilla approach and show you that

257
00:10:43,680 --> 00:10:48,240
conceptually it's very similar or

258
00:10:46,640 --> 00:10:50,160
basically the same as learning to

259
00:10:48,240 --> 00:10:52,240
optimize in self-supervised setting that

260
00:10:50,160 --> 00:10:55,040
is really

261
00:10:52,240 --> 00:10:57,920
emerging AI and operation research power

262
00:10:55,040 --> 00:11:01,680
systems optimization that um has a huge

263
00:10:57,920 --> 00:11:04,399
potential in u um improving our

264
00:11:01,680 --> 00:11:05,839
real-time decision- making. Of course

265
00:11:04,399 --> 00:11:07,839
when we are talking about power systems

266
00:11:05,839 --> 00:11:10,480
these are safety critical systems. So if

267
00:11:07,839 --> 00:11:14,160
you are having some residual errors in

268
00:11:10,480 --> 00:11:16,079
pins in for PD solutions they don't they

269
00:11:14,160 --> 00:11:18,399
they don't cause like in catastrophic

270
00:11:16,079 --> 00:11:20,399
failures but any errors or potential

271
00:11:18,399 --> 00:11:22,720
constraints violations in power systems

272
00:11:20,399 --> 00:11:24,640
can cause catastrophic failures. So a

273
00:11:22,720 --> 00:11:26,320
lot of research in this domain is in

274
00:11:24,640 --> 00:11:28,800
so-called physibility restoration

275
00:11:26,320 --> 00:11:30,480
layers. So how do we correct uh the

276
00:11:28,800 --> 00:11:34,800
solutions generated by this neural

277
00:11:30,480 --> 00:11:37,120
network to satisfy hard constraints?

278
00:11:34,800 --> 00:11:39,600
And conceptually the method is again

279
00:11:37,120 --> 00:11:41,279
very similar than pins. But here instead

280
00:11:39,600 --> 00:11:43,360
of spatial temporal coordinates we

281
00:11:41,279 --> 00:11:46,959
sample some parametric space that can

282
00:11:43,360 --> 00:11:49,680
represent initial conditions or um some

283
00:11:46,959 --> 00:11:52,399
stoastic uncertainties in your problem.

284
00:11:49,680 --> 00:11:54,800
And your neural network now tries to

285
00:11:52,399 --> 00:11:57,360
replace your numerical solver by mapping

286
00:11:54,800 --> 00:11:59,200
these parametric uh scenarios into the

287
00:11:57,360 --> 00:12:01,600
primal solutions of the optimization

288
00:11:59,200 --> 00:12:04,000
problem that in general form um is

289
00:12:01,600 --> 00:12:05,920
described by some cost function over

290
00:12:04,000 --> 00:12:09,120
your primal decisions parameterized by

291
00:12:05,920 --> 00:12:11,360
the the uncertainties and some general

292
00:12:09,120 --> 00:12:15,760
inequality constraints.

293
00:12:11,360 --> 00:12:18,079
Um so how do we embed uh the domain

294
00:12:15,760 --> 00:12:20,560
awareness in this self-supervised

295
00:12:18,079 --> 00:12:22,160
learning to optimize? Well, if your

296
00:12:20,560 --> 00:12:24,160
objective functions and constraints are

297
00:12:22,160 --> 00:12:26,720
differentiable, ideally smooth, so you

298
00:12:24,160 --> 00:12:29,519
can embed them as a layer and evaluate

299
00:12:26,720 --> 00:12:31,040
their violations in the loss functions

300
00:12:29,519 --> 00:12:33,279
through the same empirical risk

301
00:12:31,040 --> 00:12:34,880
minimization that we use in pins. But if

302
00:12:33,279 --> 00:12:37,600
you want to have further hard constraint

303
00:12:34,880 --> 00:12:39,360
guarantees again a lot of research is in

304
00:12:37,600 --> 00:12:41,519
these feasibility restoration layers

305
00:12:39,360 --> 00:12:44,000
that can be done either through

306
00:12:41,519 --> 00:12:46,000
differentiating through optimization

307
00:12:44,000 --> 00:12:48,320
solvers through implicit function

308
00:12:46,000 --> 00:12:51,680
theorem or differentiating to unrolled

309
00:12:48,320 --> 00:12:53,600
fixed point operators um projections or

310
00:12:51,680 --> 00:12:56,959
Newton type solvers. So a lot of

311
00:12:53,600 --> 00:12:59,279
exciting uh new research done even in

312
00:12:56,959 --> 00:13:02,160
this community here uh in this uh

313
00:12:59,279 --> 00:13:04,959
learning to optimize method.

314
00:13:02,160 --> 00:13:08,000
The next category of uh models that I

315
00:13:04,959 --> 00:13:09,600
would argue is again related and could

316
00:13:08,000 --> 00:13:12,240
be formalized on the high level as the

317
00:13:09,600 --> 00:13:15,360
same class of problems is nonlinear

318
00:13:12,240 --> 00:13:18,800
system identification with constraints.

319
00:13:15,360 --> 00:13:21,040
And here instead of sampling

320
00:13:18,800 --> 00:13:23,440
uh some parametric scenarios or

321
00:13:21,040 --> 00:13:25,040
uncertainties or special temporal

322
00:13:23,440 --> 00:13:26,560
coordinates, we assume some time series

323
00:13:25,040 --> 00:13:29,120
data which we from which we want to

324
00:13:26,560 --> 00:13:31,360
learn. And

325
00:13:29,120 --> 00:13:33,040
uh one methodology that is really

326
00:13:31,360 --> 00:13:35,120
standing out is neural differential

327
00:13:33,040 --> 00:13:37,360
equation that have been being used

328
00:13:35,120 --> 00:13:41,040
extensively in generative AI. But here I

329
00:13:37,360 --> 00:13:42,320
want to uh think about neural

330
00:13:41,040 --> 00:13:44,320
differential equations from the

331
00:13:42,320 --> 00:13:47,600
perspective of engineering applications.

332
00:13:44,320 --> 00:13:49,920
Can we utilize these methodologies in

333
00:13:47,600 --> 00:13:52,959
modeling engineering systems subject to

334
00:13:49,920 --> 00:13:54,480
constraints and again apply them in

335
00:13:52,959 --> 00:13:56,480
safety critical systems like power

336
00:13:54,480 --> 00:13:59,519
systems or or heat transfer energy

337
00:13:56,480 --> 00:14:02,240
systems modeling. Um

338
00:13:59,519 --> 00:14:04,399
so how do we think about constructing

339
00:14:02,240 --> 00:14:06,720
those data sets? Uh there are typically

340
00:14:04,399 --> 00:14:08,480
batches of trajectories which we assume

341
00:14:06,720 --> 00:14:14,079
over finite horizon typically on

342
00:14:08,480 --> 00:14:15,760
discretized u time samples. And now the

343
00:14:14,079 --> 00:14:17,839
uh combination of your function

344
00:14:15,760 --> 00:14:19,600
approximator with the domain prior comes

345
00:14:17,839 --> 00:14:23,120
in different shapes and forms. There's

346
00:14:19,600 --> 00:14:25,120
there is a lot of uh nonlinear system

347
00:14:23,120 --> 00:14:27,120
identification methodologies in the

348
00:14:25,120 --> 00:14:29,600
literature that utilize either neural

349
00:14:27,120 --> 00:14:31,120
networks or polomial basis functions in

350
00:14:29,600 --> 00:14:34,240
differential programming setting in

351
00:14:31,120 --> 00:14:37,680
different shapes and forms. So in the

352
00:14:34,240 --> 00:14:40,160
neur blackbox neural OD setting you

353
00:14:37,680 --> 00:14:41,839
right hand side is represented by neural

354
00:14:40,160 --> 00:14:43,920
network and you differentiate through

355
00:14:41,839 --> 00:14:46,639
the OD solver either by unrolling the

356
00:14:43,920 --> 00:14:48,800
operations of the numerical integrators

357
00:14:46,639 --> 00:14:51,839
are like RK4 or using a joint

358
00:14:48,800 --> 00:14:54,320
sensitivity method. On the other hand,

359
00:14:51,839 --> 00:14:57,199
operator based approaches like deep

360
00:14:54,320 --> 00:14:59,120
coupman operator, they uh embed the

361
00:14:57,199 --> 00:15:02,399
observables into the latent space

362
00:14:59,120 --> 00:15:04,240
through neural network encoding and in

363
00:15:02,399 --> 00:15:05,839
the higher dimensional latent space they

364
00:15:04,240 --> 00:15:08,399
evolve the linear operator and then

365
00:15:05,839 --> 00:15:10,880
project it back uh to the observable

366
00:15:08,399 --> 00:15:13,199
space with this type of approximate

367
00:15:10,880 --> 00:15:15,199
inverse encoder decoder architectures. A

368
00:15:13,199 --> 00:15:17,680
lot of interesting theory and

369
00:15:15,199 --> 00:15:20,480
interesting properties for for control

370
00:15:17,680 --> 00:15:22,160
in this space. um other approaches like

371
00:15:20,480 --> 00:15:24,639
sparse identification of nonlinear

372
00:15:22,160 --> 00:15:26,800
dynamics or CID for short um are also

373
00:15:24,639 --> 00:15:29,600
used for model discovery. So what I'm

374
00:15:26,800 --> 00:15:31,600
arguing here we can train all of these

375
00:15:29,600 --> 00:15:33,600
uh with stoastic gradient descent based

376
00:15:31,600 --> 00:15:37,120
on this empirical corros minimization of

377
00:15:33,600 --> 00:15:39,920
the trajectory matching uh for dynamical

378
00:15:37,120 --> 00:15:41,839
systems as as well. And now when you

379
00:15:39,920 --> 00:15:45,440
have capability for modeling dynamical

380
00:15:41,839 --> 00:15:46,880
system when you have ability to handle

381
00:15:45,440 --> 00:15:48,720
constraints either structurally or

382
00:15:46,880 --> 00:15:51,519
through these implicit layers you put

383
00:15:48,720 --> 00:15:54,480
them together and you can train closed

384
00:15:51,519 --> 00:15:56,720
loop dynamical systems to solve certain

385
00:15:54,480 --> 00:15:58,800
control task and this is the methodology

386
00:15:56,720 --> 00:16:00,079
that we have been developing my

387
00:15:58,800 --> 00:16:01,440
colleagues at the Pacific Northwest

388
00:16:00,079 --> 00:16:05,360
National Lab that we call differential

389
00:16:01,440 --> 00:16:09,680
priority control that uh combines this

390
00:16:05,360 --> 00:16:11,680
idea together and again we are based on

391
00:16:09,680 --> 00:16:13,839
sampling based collocation approaches of

392
00:16:11,680 --> 00:16:16,000
the parametric scenarios in optimal

393
00:16:13,839 --> 00:16:18,639
control problem and we represent a

394
00:16:16,000 --> 00:16:22,480
closed loop system with some function

395
00:16:18,639 --> 00:16:25,360
approximator for the control policy most

396
00:16:22,480 --> 00:16:27,519
commonly deep neural net and system

397
00:16:25,360 --> 00:16:30,560
dynamics representation either with

398
00:16:27,519 --> 00:16:33,440
family or state space models or known

399
00:16:30,560 --> 00:16:35,680
differential equations or learned neural

400
00:16:33,440 --> 00:16:37,360
differential representations.

401
00:16:35,680 --> 00:16:39,440
Of course there needs to be some

402
00:16:37,360 --> 00:16:40,880
assumptions on on the the well

403
00:16:39,440 --> 00:16:43,680
conditioning of the system because now

404
00:16:40,880 --> 00:16:47,040
we are unrolling this system which can

405
00:16:43,680 --> 00:16:49,040
create rather deep computial graphs. So

406
00:16:47,040 --> 00:16:52,000
uh conditioning of this dynamic system

407
00:16:49,040 --> 00:16:54,399
is very very crucial. But from the

408
00:16:52,000 --> 00:16:56,560
higher level methodological perspective

409
00:16:54,399 --> 00:16:58,720
you're solving again empirical

410
00:16:56,560 --> 00:17:00,720
minimization by sampling the scenarios

411
00:16:58,720 --> 00:17:02,880
in your parametric space that can be

412
00:17:00,720 --> 00:17:05,039
initial conditions of this problem or

413
00:17:02,880 --> 00:17:07,679
there can be uh conditioning of your

414
00:17:05,039 --> 00:17:09,360
constraints or objective functions like

415
00:17:07,679 --> 00:17:11,439
different desired reference trajectories

416
00:17:09,360 --> 00:17:13,679
to solve this parametric optimal control

417
00:17:11,439 --> 00:17:16,400
problem approximately

418
00:17:13,679 --> 00:17:17,919
uh by training this deep neural network.

419
00:17:16,400 --> 00:17:19,919
So for those of you who are familiar

420
00:17:17,919 --> 00:17:24,720
with control theoretic approaches like

421
00:17:19,919 --> 00:17:26,480
model property control um it's uh

422
00:17:24,720 --> 00:17:29,280
approximate approach to solve explicit

423
00:17:26,480 --> 00:17:30,880
model pretty control that traditionally

424
00:17:29,280 --> 00:17:32,480
is being solved with multiparametric

425
00:17:30,880 --> 00:17:34,080
programming but unfortunately the

426
00:17:32,480 --> 00:17:36,880
multiparametric programming approaches

427
00:17:34,080 --> 00:17:38,640
don't scale very well because they blow

428
00:17:36,880 --> 00:17:42,640
up in complexity of all possible

429
00:17:38,640 --> 00:17:45,120
combination of active constraints. So um

430
00:17:42,640 --> 00:17:46,799
how you can think uh structurally

431
00:17:45,120 --> 00:17:49,200
between this differential operating

432
00:17:46,799 --> 00:17:54,080
control and traditional implicit MPC. So

433
00:17:49,200 --> 00:17:55,600
it is simple example on uh linear model

434
00:17:54,080 --> 00:17:58,799
printing control when we have linear

435
00:17:55,600 --> 00:18:01,919
state space model and quadratic cost

436
00:17:58,799 --> 00:18:03,760
where we want to match our desired uh

437
00:18:01,919 --> 00:18:06,320
reference trajectories with our state

438
00:18:03,760 --> 00:18:08,799
trajectories and we can conveniently

439
00:18:06,320 --> 00:18:11,120
represent the state system state

440
00:18:08,799 --> 00:18:13,200
evolution over finite horizon uh with

441
00:18:11,120 --> 00:18:15,600
this type of like lower triangle matrix

442
00:18:13,200 --> 00:18:17,600
where a matrix is our my state

443
00:18:15,600 --> 00:18:19,039
transition matrix b is the my input

444
00:18:17,600 --> 00:18:21,120
dynamics matrix

445
00:18:19,039 --> 00:18:22,640
So when you think about it structurally

446
00:18:21,120 --> 00:18:24,559
is nothing else as a recurren neural

447
00:18:22,640 --> 00:18:27,360
network. Yeah. When you unroll it as a

448
00:18:24,559 --> 00:18:30,240
computation graph and if you replace the

449
00:18:27,360 --> 00:18:32,240
decision variable uh to be your control

450
00:18:30,240 --> 00:18:34,480
sequence like it is done in online model

451
00:18:32,240 --> 00:18:38,160
property control with explicit control

452
00:18:34,480 --> 00:18:40,799
policy. Now we have this closed loop

453
00:18:38,160 --> 00:18:42,799
neural dynamical system where the

454
00:18:40,799 --> 00:18:45,200
policies represented by the the neural

455
00:18:42,799 --> 00:18:46,960
part and encodes the explicit control

456
00:18:45,200 --> 00:18:49,520
policy approximately and it can be

457
00:18:46,960 --> 00:18:51,120
trained in a batch over distribution of

458
00:18:49,520 --> 00:18:53,440
desired reference trajectories and

459
00:18:51,120 --> 00:18:57,520
initial conditions.

460
00:18:53,440 --> 00:19:00,000
Okay. So that's a high level overview of

461
00:18:57,520 --> 00:19:02,799
different methodologies in scientific

462
00:19:00,000 --> 00:19:05,760
machine learning that uh my I'm arguing

463
00:19:02,799 --> 00:19:08,080
that are family of this high level of

464
00:19:05,760 --> 00:19:10,160
abstraction that we can call scientific

465
00:19:08,080 --> 00:19:12,320
or physics for machine learning. they

466
00:19:10,160 --> 00:19:14,559
come with these four major uh building

467
00:19:12,320 --> 00:19:18,720
blocks and because of this abstraction

468
00:19:14,559 --> 00:19:20,320
it allows us to build uh user friendly

469
00:19:18,720 --> 00:19:23,520
software tools and that's what I have

470
00:19:20,320 --> 00:19:25,039
been doing for last uh already 5 years

471
00:19:23,520 --> 00:19:26,799
together with my colleagues at Pacific

472
00:19:25,039 --> 00:19:28,720
Northwest National Lab by developing the

473
00:19:26,799 --> 00:19:31,360
library that is called Neurommancer

474
00:19:28,720 --> 00:19:34,320
which is built on top of PyTorch and

475
00:19:31,360 --> 00:19:37,440
really the vision is to enable a

476
00:19:34,320 --> 00:19:40,559
software toolkit that is integrating all

477
00:19:37,440 --> 00:19:42,240
of these advancements and the focuses on

478
00:19:40,559 --> 00:19:45,280
modularity

479
00:19:42,240 --> 00:19:47,200
um simplicity and user experience rather

480
00:19:45,280 --> 00:19:50,480
than on performance.

481
00:19:47,200 --> 00:19:52,240
uh we chose PyTorch as our back end

482
00:19:50,480 --> 00:19:55,600
consciously because of the large

483
00:19:52,240 --> 00:19:58,480
community and uh robust support uh from

484
00:19:55,600 --> 00:20:02,080
the hardware accelerators and from the

485
00:19:58,480 --> 00:20:04,000
front end the API is modeled based on

486
00:20:02,080 --> 00:20:05,919
algebraic modeling languages that we

487
00:20:04,000 --> 00:20:09,600
commonly use in controls and

488
00:20:05,919 --> 00:20:12,640
optimization like jump or cvxpy or yip

489
00:20:09,600 --> 00:20:14,400
that allow you to formulate the highle

490
00:20:12,640 --> 00:20:17,200
constraint optimization problems or

491
00:20:14,400 --> 00:20:19,039
optimal control problems in uh

492
00:20:17,200 --> 00:20:21,840
convenient symbolic abstractions by

493
00:20:19,039 --> 00:20:24,480
defining the variables and on these

494
00:20:21,840 --> 00:20:27,280
variables you symbolically define your

495
00:20:24,480 --> 00:20:29,919
objective expressions or constraints and

496
00:20:27,280 --> 00:20:32,640
then you plug them in together in a

497
00:20:29,919 --> 00:20:34,559
problem and in traditional algebraic

498
00:20:32,640 --> 00:20:36,400
model languages this problem is parsed

499
00:20:34,559 --> 00:20:40,000
into solver specific form and then you

500
00:20:36,400 --> 00:20:42,960
solve it with interior point or or or um

501
00:20:40,000 --> 00:20:44,799
Gurobi or other solver skip baron but

502
00:20:42,960 --> 00:20:46,559
here what we are doing is that we

503
00:20:44,799 --> 00:20:49,440
combine this with these machine learning

504
00:20:46,559 --> 00:20:51,840
components that are being trained

505
00:20:49,440 --> 00:20:54,159
through stoastic gradient descent uh in

506
00:20:51,840 --> 00:20:57,840
PyTorch and through our problem

507
00:20:54,159 --> 00:21:00,880
constructor we generate this custom

508
00:20:57,840 --> 00:21:02,640
computial graph of scientific machine

509
00:21:00,880 --> 00:21:04,960
learning components whether it is for

510
00:21:02,640 --> 00:21:06,480
pins or learning to optimize for

511
00:21:04,960 --> 00:21:09,280
learning to control you have the same

512
00:21:06,480 --> 00:21:11,520
standardized API and really the vision

513
00:21:09,280 --> 00:21:13,440
is to be interoperable so once you train

514
00:21:11,520 --> 00:21:15,520
a pin and you have approximation of the

515
00:21:13,440 --> 00:21:17,520
PD and you want to do PD the constraint

516
00:21:15,520 --> 00:21:20,159
optimization you can embed it in in in

517
00:21:17,520 --> 00:21:22,720
the learning to optimize.

518
00:21:20,159 --> 00:21:25,039
Um okay so that's the software

519
00:21:22,720 --> 00:21:27,120
capability that has been developed by my

520
00:21:25,039 --> 00:21:30,480
wonder with my wonderful colleagues at

521
00:21:27,120 --> 00:21:34,159
PNL uh funded with US department of

522
00:21:30,480 --> 00:21:36,720
energy. So want to acknowledge all of

523
00:21:34,159 --> 00:21:38,559
their hard work and uh commitment to

524
00:21:36,720 --> 00:21:40,080
making this tool open source and

525
00:21:38,559 --> 00:21:42,799
maintainable.

526
00:21:40,080 --> 00:21:44,400
Um next I want to highlight two

527
00:21:42,799 --> 00:21:46,480
applications that we have been working

528
00:21:44,400 --> 00:21:49,360
on and buildings has been something I

529
00:21:46,480 --> 00:21:51,039
was been working for 10 years which has

530
00:21:49,360 --> 00:21:52,400
been already a long time and we are

531
00:21:51,039 --> 00:21:56,080
still not really like solving these

532
00:21:52,400 --> 00:21:58,240
problems in in real life but I hope that

533
00:21:56,080 --> 00:22:00,400
these type of new tools and capabilities

534
00:21:58,240 --> 00:22:02,559
could um mitigate some of those

535
00:22:00,400 --> 00:22:04,080
engineering problems uh in in the

536
00:22:02,559 --> 00:22:06,640
practice because we don't really have

537
00:22:04,080 --> 00:22:10,320
problems with theory uh we have problem

538
00:22:06,640 --> 00:22:13,039
in making uh the theory and wonderful

539
00:22:10,320 --> 00:22:16,240
numerical tools to be easily usable and

540
00:22:13,039 --> 00:22:18,400
scalable in in practice. And that's what

541
00:22:16,240 --> 00:22:21,200
we are trying to demonstrate in our more

542
00:22:18,400 --> 00:22:23,679
applied research where we want to show

543
00:22:21,200 --> 00:22:26,000
that we can now train these uh control

544
00:22:23,679 --> 00:22:29,919
policies through scientific machine

545
00:22:26,000 --> 00:22:32,640
learning with surrogate models um at the

546
00:22:29,919 --> 00:22:34,480
fraction of the time investment and also

547
00:22:32,640 --> 00:22:37,200
expertise investment because if you have

548
00:22:34,480 --> 00:22:39,039
engineer that now is capable in the

549
00:22:37,200 --> 00:22:40,640
scientific machine learning uh

550
00:22:39,039 --> 00:22:42,000
functionality

551
00:22:40,640 --> 00:22:43,360
you don't need to have one expert in

552
00:22:42,000 --> 00:22:45,520
modeling you don't need to have one

553
00:22:43,360 --> 00:22:48,640
expert in optimization control and then

554
00:22:45,520 --> 00:22:50,799
expert in ML and stitch all of their uh

555
00:22:48,640 --> 00:22:52,640
software solutions together in

556
00:22:50,799 --> 00:22:55,600
functional mockup units and or in other

557
00:22:52,640 --> 00:22:57,600
ways. So that's that's the argument here

558
00:22:55,600 --> 00:22:59,280
and in uh the building space

559
00:22:57,600 --> 00:23:02,080
specifically we know a lot of physics

560
00:22:59,280 --> 00:23:04,640
and we can exploit this in these uh

561
00:23:02,080 --> 00:23:06,799
datadriven machine learning uh

562
00:23:04,640 --> 00:23:08,720
surrogates these physics informed ML

563
00:23:06,799 --> 00:23:10,559
models. We know the buildings are

564
00:23:08,720 --> 00:23:12,400
thermodynamically stable systems. They

565
00:23:10,559 --> 00:23:14,960
are dissipative. they want to get to the

566
00:23:12,400 --> 00:23:18,080
equilibria with the the environment. So

567
00:23:14,960 --> 00:23:21,120
we can encode those properties in um our

568
00:23:18,080 --> 00:23:24,000
neural ods or neural taste models. We

569
00:23:21,120 --> 00:23:25,760
also know um that their partial

570
00:23:24,000 --> 00:23:28,960
derivatives need to satisfy certain

571
00:23:25,760 --> 00:23:31,520
properties uh um with with the

572
00:23:28,960 --> 00:23:34,080
environment. So if I'm increasing the

573
00:23:31,520 --> 00:23:36,080
temperature in in this room uh the heat

574
00:23:34,080 --> 00:23:37,440
transfer to through the walls we also

575
00:23:36,080 --> 00:23:39,120
increase temperature in the in the other

576
00:23:37,440 --> 00:23:41,440
room or not other way around. So there

577
00:23:39,120 --> 00:23:43,520
are a lot of uh physics constraints that

578
00:23:41,440 --> 00:23:45,280
we can embed in these models to uh

579
00:23:43,520 --> 00:23:48,480
reduce the degrees of freedom and allow

580
00:23:45,280 --> 00:23:50,400
us to learn from uh smaller and less

581
00:23:48,480 --> 00:23:52,880
excited data because that's also one of

582
00:23:50,400 --> 00:23:55,120
the big problems in industry when a lot

583
00:23:52,880 --> 00:23:58,799
of your data is coming with missing data

584
00:23:55,120 --> 00:24:01,679
points uh low fidelity data and

585
00:23:58,799 --> 00:24:04,159
operating really close to steady states.

586
00:24:01,679 --> 00:24:05,919
So how do we how do we solve this

587
00:24:04,159 --> 00:24:08,559
problem without physics? Well, we need

588
00:24:05,919 --> 00:24:10,960
we need to really utilize a lot of

589
00:24:08,559 --> 00:24:12,559
physics in order to have any like

590
00:24:10,960 --> 00:24:15,120
datadriven modeling here. So, it's more

591
00:24:12,559 --> 00:24:18,159
like data tuning of the physics models

592
00:24:15,120 --> 00:24:22,320
rather than uh large data set machine

593
00:24:18,159 --> 00:24:25,120
learning. And uh one of like very early

594
00:24:22,320 --> 00:24:27,520
prototype uh type of applications which

595
00:24:25,120 --> 00:24:29,279
we are exploring is uh learning to

596
00:24:27,520 --> 00:24:32,159
control learning to optimize for power

597
00:24:29,279 --> 00:24:33,679
systems where we want to reuse all of

598
00:24:32,159 --> 00:24:36,559
the software infrastructure that we

599
00:24:33,679 --> 00:24:38,320
built for the buildings application and

600
00:24:36,559 --> 00:24:40,960
uh applying in a different context. So

601
00:24:38,320 --> 00:24:43,360
here is again small scale example on

602
00:24:40,960 --> 00:24:46,880
night bus system where we know some

603
00:24:43,360 --> 00:24:48,640
physics we know uh the the stability

604
00:24:46,880 --> 00:24:50,400
conditions that we can embed in our

605
00:24:48,640 --> 00:24:52,640
model. In this case, we have chosen this

606
00:24:50,400 --> 00:24:56,400
deep coupman operator that was uh

607
00:24:52,640 --> 00:24:59,440
physics inspired and uh constrained to

608
00:24:56,400 --> 00:25:02,400
to be stable operator um to allow us to

609
00:24:59,440 --> 00:25:04,559
train this uh explicit control policies

610
00:25:02,400 --> 00:25:07,919
offline with this empirical ro

611
00:25:04,559 --> 00:25:10,240
minimization. Um from the computation

612
00:25:07,919 --> 00:25:13,120
perspective what you are getting uh is

613
00:25:10,240 --> 00:25:15,919
that you are basically offloading your

614
00:25:13,120 --> 00:25:19,520
computational effort from the online to

615
00:25:15,919 --> 00:25:21,279
offline. So it takes some time may maybe

616
00:25:19,520 --> 00:25:23,279
you require some GPUs to train these

617
00:25:21,279 --> 00:25:24,880
models offline but at the online

618
00:25:23,279 --> 00:25:26,960
inference we are typically getting

619
00:25:24,880 --> 00:25:29,919
orders of magnitude faster acceleration

620
00:25:26,960 --> 00:25:31,679
and in our papers and others we are

621
00:25:29,919 --> 00:25:34,320
getting up to five orders of magnitude

622
00:25:31,679 --> 00:25:36,480
acceleration compared to uh nonlinear

623
00:25:34,320 --> 00:25:40,799
optimization solvers like uh inferior

624
00:25:36,480 --> 00:25:45,600
point so that's very very exciting

625
00:25:40,799 --> 00:25:48,240
um but a lot of problems remain um I

626
00:25:45,600 --> 00:25:51,120
will talk about two specific problems

627
00:25:48,240 --> 00:25:52,720
today. Um there are many many more

628
00:25:51,120 --> 00:25:54,960
including differentiability, well

629
00:25:52,720 --> 00:25:56,720
conditioning and convergence

630
00:25:54,960 --> 00:25:59,120
um of these methods but I want to

631
00:25:56,720 --> 00:26:02,559
highlight two more of a of the

632
00:25:59,120 --> 00:26:04,480
structural problems that uh uh are

633
00:26:02,559 --> 00:26:07,039
common in applications whether it is

634
00:26:04,480 --> 00:26:09,919
power systems or uh HVAC systems in

635
00:26:07,039 --> 00:26:11,440
buildings. The first we have um

636
00:26:09,919 --> 00:26:13,440
differential algebraic equations

637
00:26:11,440 --> 00:26:15,919
describing the system where the

638
00:26:13,440 --> 00:26:18,000
algebraic equation can represent some

639
00:26:15,919 --> 00:26:20,000
energy or mass conservation property

640
00:26:18,000 --> 00:26:21,760
very common in power systems and and

641
00:26:20,000 --> 00:26:23,840
buildings here like very simple example

642
00:26:21,760 --> 00:26:25,840
when you have the reservoir and then you

643
00:26:23,840 --> 00:26:27,919
have a pump and now the mass is

644
00:26:25,840 --> 00:26:30,559
distributed through this three three-way

645
00:26:27,919 --> 00:26:33,120
valve into two tanks. Well, there needs

646
00:26:30,559 --> 00:26:36,559
to be some mass conservation at this

647
00:26:33,120 --> 00:26:39,120
point and that's represented by this uh

648
00:26:36,559 --> 00:26:42,320
algebraic uh constraints on on your

649
00:26:39,120 --> 00:26:44,960
dynamical system. So how can we train

650
00:26:42,320 --> 00:26:47,520
these models and discover their uh

651
00:26:44,960 --> 00:26:50,559
components from data?

652
00:26:47,520 --> 00:26:53,520
That's uh was the question that we asked

653
00:26:50,559 --> 00:26:56,400
uh with the colleagues at PNL and uh

654
00:26:53,520 --> 00:26:59,039
took inspiration from a lot of recent

655
00:26:56,400 --> 00:27:01,279
advances in neural differential equation

656
00:26:59,039 --> 00:27:03,760
especially structured differential

657
00:27:01,279 --> 00:27:05,360
equations and and learning for what is

658
00:27:03,760 --> 00:27:07,039
called universal differential equations.

659
00:27:05,360 --> 00:27:10,880
I believe this is term coined by Chris

660
00:27:07,039 --> 00:27:13,120
Rock here from MIT um uh who has been

661
00:27:10,880 --> 00:27:16,240
working on these methods uh for a while.

662
00:27:13,120 --> 00:27:18,240
So here uh we

663
00:27:16,240 --> 00:27:20,480
uh asked the question that if we can

664
00:27:18,240 --> 00:27:23,679
represent our dynamical system in a

665
00:27:20,480 --> 00:27:25,840
operator splitting uh scheme which is

666
00:27:23,679 --> 00:27:28,640
typically used for large scale uh

667
00:27:25,840 --> 00:27:31,039
systems uh we can apply these

668
00:27:28,640 --> 00:27:32,559
traditional fractional step or what is

669
00:27:31,039 --> 00:27:35,679
called operator splitting schemes by

670
00:27:32,559 --> 00:27:37,760
first solving the uh one part of the

671
00:27:35,679 --> 00:27:40,559
problem get the update on on the dynamic

672
00:27:37,760 --> 00:27:43,039
system state update and then use it as

673
00:27:40,559 --> 00:27:45,679
initial condition for another initial

674
00:27:43,039 --> 00:27:48,480
value problem for the second operator B.

675
00:27:45,679 --> 00:27:51,279
So that's like standard operators uh

676
00:27:48,480 --> 00:27:53,600
splitting scheme and what we ask like

677
00:27:51,279 --> 00:27:55,600
okay can we think about this uh

678
00:27:53,600 --> 00:27:57,679
differential algebraic equation and

679
00:27:55,600 --> 00:28:00,240
apply the same operators splitting

680
00:27:57,679 --> 00:28:02,640
scheme uh in this context uh the answer

681
00:28:00,240 --> 00:28:04,720
is yes but only for certain type of

682
00:28:02,640 --> 00:28:06,399
systems of course so uh the type of

683
00:28:04,720 --> 00:28:10,080
systems is index one differential

684
00:28:06,399 --> 00:28:12,399
algebraic equations uh where uh you can

685
00:28:10,080 --> 00:28:14,480
reduce these das into odes by

686
00:28:12,399 --> 00:28:16,960
differentiating your uh algebraic

687
00:28:14,480 --> 00:28:19,440
constraints exactly wants and for that

688
00:28:16,960 --> 00:28:21,360
we have this pickar lend theorem that is

689
00:28:19,440 --> 00:28:23,360
telling us what properties do you need

690
00:28:21,360 --> 00:28:27,840
to have in order this problem to have

691
00:28:23,360 --> 00:28:31,120
unique solution. So basically um your uh

692
00:28:27,840 --> 00:28:34,640
differential equation is smooth and uh

693
00:28:31,120 --> 00:28:37,440
the algebraic equation is invertible. So

694
00:28:34,640 --> 00:28:39,440
if this holds then you can apply this

695
00:28:37,440 --> 00:28:43,440
relatively simple explicit scheme this

696
00:28:39,440 --> 00:28:45,919
operator splitting where we uh introduce

697
00:28:43,440 --> 00:28:47,760
algebra solver H that takes your

698
00:28:45,919 --> 00:28:50,399
differential states algebra states and

699
00:28:47,760 --> 00:28:53,600
some potential exogenous inputs to get

700
00:28:50,399 --> 00:28:55,840
you update on the algebra solution.

701
00:28:53,600 --> 00:28:58,159
This can be either Newton solver or it

702
00:28:55,840 --> 00:29:00,399
can be uh trained as a soft constraints

703
00:28:58,159 --> 00:29:03,039
with pins. And then you take this

704
00:29:00,399 --> 00:29:05,679
updated algebra state and plug it in in

705
00:29:03,039 --> 00:29:07,919
your forward OD.

706
00:29:05,679 --> 00:29:10,799
And that's it really like super simple

707
00:29:07,919 --> 00:29:13,760
trivial kind of

708
00:29:10,799 --> 00:29:15,840
solution for uh having this neural OD

709
00:29:13,760 --> 00:29:17,279
with algebra constraints that you can

710
00:29:15,840 --> 00:29:19,279
think from the computational graph

711
00:29:17,279 --> 00:29:23,279
perspective. you get uh some initial

712
00:29:19,279 --> 00:29:25,679
condition control action that first sol

713
00:29:23,279 --> 00:29:28,159
goes through the algebra solver solves

714
00:29:25,679 --> 00:29:30,320
the algebraic update and then this

715
00:29:28,159 --> 00:29:32,480
algebraic update initialize the OD

716
00:29:30,320 --> 00:29:34,880
solver to get you the uh differential

717
00:29:32,480 --> 00:29:36,480
state solution and this can be trained

718
00:29:34,880 --> 00:29:38,240
again with the same empirical risk

719
00:29:36,480 --> 00:29:41,200
minimization on the trajectory matching

720
00:29:38,240 --> 00:29:42,960
for both uh differential on algebraic

721
00:29:41,200 --> 00:29:45,760
states.

722
00:29:42,960 --> 00:29:47,919
um lot of different uh and exciting

723
00:29:45,760 --> 00:29:50,399
extensions are already in the literature

724
00:29:47,919 --> 00:29:52,640
where instead of this explicit scheme

725
00:29:50,399 --> 00:29:54,960
you can have semi-explicit scheme where

726
00:29:52,640 --> 00:29:57,600
you first solve the OD solve and then

727
00:29:54,960 --> 00:29:59,120
you project uh on the algebra or

728
00:29:57,600 --> 00:30:01,120
reformulate the whole problem as a

729
00:29:59,120 --> 00:30:04,480
nonlinear optimization problem and solve

730
00:30:01,120 --> 00:30:07,520
it with IP opt so u different direction

731
00:30:04,480 --> 00:30:09,200
exist the one is worked by Chris Rockos

732
00:30:07,520 --> 00:30:12,799
group and another another one with

733
00:30:09,200 --> 00:30:15,360
Lawrence bigler CMU um so We are seeing

734
00:30:12,799 --> 00:30:17,279
a lot of exciting opportunities in

735
00:30:15,360 --> 00:30:19,279
learning for differential algebra

736
00:30:17,279 --> 00:30:22,080
equations tremendously important for

737
00:30:19,279 --> 00:30:26,320
industrial applications.

738
00:30:22,080 --> 00:30:29,760
Um so toy example uh to illustrate uh

739
00:30:26,320 --> 00:30:32,320
the idea where we have this two tank

740
00:30:29,760 --> 00:30:34,640
system and the three-way manifold and

741
00:30:32,320 --> 00:30:36,399
the pump from the reservoir.

742
00:30:34,640 --> 00:30:39,120
We don't need to represent everything as

743
00:30:36,399 --> 00:30:42,320
a black box. We know a lot of structure

744
00:30:39,120 --> 00:30:45,520
from this problem and uh the underlying

745
00:30:42,320 --> 00:30:48,000
differential equations are described um

746
00:30:45,520 --> 00:30:50,720
by the ratios where this fi equation is

747
00:30:48,000 --> 00:30:52,799
basically height area function that we

748
00:30:50,720 --> 00:30:55,520
may not know. So we want to approximate

749
00:30:52,799 --> 00:30:58,320
these uh and then we have this basically

750
00:30:55,520 --> 00:30:59,919
conservation laws um on the three-way

751
00:30:58,320 --> 00:31:01,279
valve and we know that the tanks are

752
00:30:59,919 --> 00:31:03,919
basically at the same level so their

753
00:31:01,279 --> 00:31:06,240
levels needs to like uh level up uh

754
00:31:03,919 --> 00:31:08,640
because of the gravity. So what we can

755
00:31:06,240 --> 00:31:10,240
do we can uh enforce a lot of those

756
00:31:08,640 --> 00:31:12,399
structural constraints just by knowing

757
00:31:10,240 --> 00:31:14,480
the problem. Uh like this conservation

758
00:31:12,399 --> 00:31:19,440
law can be represented by a simple

759
00:31:14,480 --> 00:31:21,440
convex uh uh combination and uh by doing

760
00:31:19,440 --> 00:31:23,360
so we can enforce some of the

761
00:31:21,440 --> 00:31:25,200
constraints as a hard architectural

762
00:31:23,360 --> 00:31:26,880
constraints rather than uh soft

763
00:31:25,200 --> 00:31:30,399
constraints in the penalties. And this

764
00:31:26,880 --> 00:31:34,399
allows you to not only fit uh the

765
00:31:30,399 --> 00:31:37,039
measurement data but also to respect

766
00:31:34,399 --> 00:31:39,840
those conservation laws which is really

767
00:31:37,039 --> 00:31:42,720
helpful for extrapolation out of

768
00:31:39,840 --> 00:31:45,679
distribution data where you can change

769
00:31:42,720 --> 00:31:48,159
uh different design parameters but uh

770
00:31:45,679 --> 00:31:50,480
your model still respects those physical

771
00:31:48,159 --> 00:31:52,880
constraints.

772
00:31:50,480 --> 00:31:54,640
So yeah, this is uh something that I'm

773
00:31:52,880 --> 00:31:57,039
uh very excited for the universe

774
00:31:54,640 --> 00:32:00,320
modeling uh and really practical

775
00:31:57,039 --> 00:32:02,720
applications in energy systems.

776
00:32:00,320 --> 00:32:05,200
Yeah, acknowledging all my wonderful

777
00:32:02,720 --> 00:32:07,360
collaborators and again founding of US

778
00:32:05,200 --> 00:32:09,279
department of energy that enabled this

779
00:32:07,360 --> 00:32:11,760
research on neuro differential algebraic

780
00:32:09,279 --> 00:32:15,760
equations. And now I want to pee to the

781
00:32:11,760 --> 00:32:18,159
second uh challenge that we have in um

782
00:32:15,760 --> 00:32:21,600
energy systems optimization and it's to

783
00:32:18,159 --> 00:32:23,679
handle mixed integer decisions because

784
00:32:21,600 --> 00:32:27,760
uh all of the scientific machine

785
00:32:23,679 --> 00:32:31,919
learning is mostly uh focused on

786
00:32:27,760 --> 00:32:33,519
uh continuous spaces for natural reasons

787
00:32:31,919 --> 00:32:35,120
because we are using automatic

788
00:32:33,519 --> 00:32:39,039
differentiation as a workhorse to

789
00:32:35,120 --> 00:32:41,519
compute our uh gradients. Um but a lot

790
00:32:39,039 --> 00:32:44,880
of practical problems in power systems

791
00:32:41,519 --> 00:32:46,240
like unit commitment problem or uh data

792
00:32:44,880 --> 00:32:48,240
center cooling chiller plant

793
00:32:46,240 --> 00:32:50,720
optimization HRA systems in buildings

794
00:32:48,240 --> 00:32:53,200
have integer decisions switching between

795
00:32:50,720 --> 00:32:56,240
different modes um or dispatching

796
00:32:53,200 --> 00:32:58,799
different units. So what are the

797
00:32:56,240 --> 00:33:00,559
challenges in current uh landscape of

798
00:32:58,799 --> 00:33:03,360
machine learning based optimization? The

799
00:33:00,559 --> 00:33:05,360
first one is um many of the methods

800
00:33:03,360 --> 00:33:07,360
particularly in the machine learning for

801
00:33:05,360 --> 00:33:09,760
mixed integer are based on super

802
00:33:07,360 --> 00:33:12,080
supervisory signals. So the supervised

803
00:33:09,760 --> 00:33:14,640
learning requires you to generate a lot

804
00:33:12,080 --> 00:33:17,279
of training data by solving a lot of my

805
00:33:14,640 --> 00:33:20,080
problems which are expensive. So uh

806
00:33:17,279 --> 00:33:22,480
that's that's the first problem. So for

807
00:33:20,080 --> 00:33:24,960
this reason we we said okay let's focus

808
00:33:22,480 --> 00:33:28,480
on self-supervised learning approaches.

809
00:33:24,960 --> 00:33:30,399
Um second problem is when you are using

810
00:33:28,480 --> 00:33:32,159
neural networks as a surrogate policies

811
00:33:30,399 --> 00:33:35,840
for solving constraint optimization

812
00:33:32,159 --> 00:33:37,760
problems um they are continuous uh how

813
00:33:35,840 --> 00:33:39,919
can we enforce hard integer constraints

814
00:33:37,760 --> 00:33:42,159
without losing differentiability. So we

815
00:33:39,919 --> 00:33:45,440
have been focusing uh on this question

816
00:33:42,159 --> 00:33:48,240
how can we come up with mixed integer

817
00:33:45,440 --> 00:33:51,039
surrogates uh for these control policies

818
00:33:48,240 --> 00:33:53,279
that retain uh differentiability through

819
00:33:51,039 --> 00:33:55,440
smooth gradients and the the third one

820
00:33:53,279 --> 00:33:58,240
is like how do we guarantee feasibility

821
00:33:55,440 --> 00:34:00,320
once we train these models

822
00:33:58,240 --> 00:34:03,600
important for any safety critical

823
00:34:00,320 --> 00:34:05,760
applications so yeah he'll be focusing

824
00:34:03,600 --> 00:34:07,279
actually on extension of uh gradient

825
00:34:05,760 --> 00:34:10,079
based projection that had been proposed

826
00:34:07,279 --> 00:34:12,000
by Priya in her paper in DC3 three um

827
00:34:10,079 --> 00:34:14,320
have been really great inspiration of

828
00:34:12,000 --> 00:34:16,720
this particular work and I want to walk

829
00:34:14,320 --> 00:34:18,480
you through the pipeline of this

830
00:34:16,720 --> 00:34:22,240
self-surprise learning to optimize for

831
00:34:18,480 --> 00:34:25,359
mixed integer um programming. So we

832
00:34:22,240 --> 00:34:27,119
assume again empirical risk minimization

833
00:34:25,359 --> 00:34:29,760
of some nonlinear objective functions

834
00:34:27,119 --> 00:34:33,280
where x is my primal decision variables

835
00:34:29,760 --> 00:34:34,879
and xi are those uh scenarios of problem

836
00:34:33,280 --> 00:34:37,119
parameters that I'm assuming I'm

837
00:34:34,879 --> 00:34:38,960
sampling from some known distribution

838
00:34:37,119 --> 00:34:41,839
and if I know let's say operation

839
00:34:38,960 --> 00:34:45,760
constraints of my energy system then

840
00:34:41,839 --> 00:34:49,359
this is relatively um cheap uh thing to

841
00:34:45,760 --> 00:34:51,919
do. uh the constraints we assume general

842
00:34:49,359 --> 00:34:54,879
nonlinear constraints smooth and

843
00:34:51,919 --> 00:34:58,400
differentiable and for the policy

844
00:34:54,879 --> 00:35:00,240
representation let's have a mapping um

845
00:34:58,400 --> 00:35:03,920
fi that maps these problem parameters to

846
00:35:00,240 --> 00:35:06,400
primal decisions so this is up to now

847
00:35:03,920 --> 00:35:08,480
standard learning to optimize that you

848
00:35:06,400 --> 00:35:11,280
train on this uh empirical corros

849
00:35:08,480 --> 00:35:14,000
minimization of your cost on and

850
00:35:11,280 --> 00:35:16,000
constraint penalties

851
00:35:14,000 --> 00:35:18,960
how can we embed these mixed integer

852
00:35:16,000 --> 00:35:21,200
constraints And so what we introduce are

853
00:35:18,960 --> 00:35:23,839
two components. One is called integer

854
00:35:21,200 --> 00:35:26,240
correction layer that does the integer

855
00:35:23,839 --> 00:35:27,839
rounding while retaining the phys while

856
00:35:26,240 --> 00:35:29,920
retaining the gradients for end to end

857
00:35:27,839 --> 00:35:32,960
trading and the second one is this

858
00:35:29,920 --> 00:35:34,640
physibility projection that utilizes the

859
00:35:32,960 --> 00:35:37,520
gradients from the integer rounding in

860
00:35:34,640 --> 00:35:39,760
the gradient based updates.

861
00:35:37,520 --> 00:35:42,240
So let's focus on these uh integer

862
00:35:39,760 --> 00:35:45,599
correction layers. Um here the

863
00:35:42,240 --> 00:35:47,200
inspiration comes from primal heristics

864
00:35:45,599 --> 00:35:49,760
in mixed integer programming that is

865
00:35:47,200 --> 00:35:52,720
called relaxation and force neighborhood

866
00:35:49,760 --> 00:35:55,280
search or rens in short. And in rens

867
00:35:52,720 --> 00:35:56,720
what we do we have mixed integer program

868
00:35:55,280 --> 00:35:59,280
and we first solve continuous

869
00:35:56,720 --> 00:36:02,560
relaxation. Then we obtain these uh

870
00:35:59,280 --> 00:36:04,880
continuous primals and we get uh the

871
00:36:02,560 --> 00:36:07,760
nearest integer bounds lower and upper

872
00:36:04,880 --> 00:36:10,480
bound to tighten the subsequent myip

873
00:36:07,760 --> 00:36:13,200
solution. So what we ask here is can we

874
00:36:10,480 --> 00:36:17,520
come up with trainable end to end ren

875
00:36:13,200 --> 00:36:19,839
type layers in learning to optimize and

876
00:36:17,520 --> 00:36:22,240
um short answer is yes and basically

877
00:36:19,839 --> 00:36:24,320
what these differentiable integer

878
00:36:22,240 --> 00:36:28,079
correction layers do they are learning

879
00:36:24,320 --> 00:36:30,720
to round up or down for some fractional

880
00:36:28,079 --> 00:36:32,480
value and they are learning with respect

881
00:36:30,720 --> 00:36:34,000
to this empirical cost minimization of

882
00:36:32,480 --> 00:36:36,160
your optimization problem that includes

883
00:36:34,000 --> 00:36:38,079
the constraint penalties. So what what

884
00:36:36,160 --> 00:36:41,359
happens that you are basically learning

885
00:36:38,079 --> 00:36:44,079
this uh conditional rounding uh

886
00:36:41,359 --> 00:36:46,800
mechanisms for specific uh formulation

887
00:36:44,079 --> 00:36:49,680
of the parametric mix integer program.

888
00:36:46,800 --> 00:36:52,320
So how do we do it technically is okay

889
00:36:49,680 --> 00:36:54,800
we assume that uh we get initial relax

890
00:36:52,320 --> 00:36:57,440
solution that can come from continuous

891
00:36:54,800 --> 00:36:59,280
neural network or other means uh the

892
00:36:57,440 --> 00:37:02,400
parametric instance and the second

893
00:36:59,280 --> 00:37:04,320
neural network this delta that is taking

894
00:37:02,400 --> 00:37:06,240
those instances and gives us these

895
00:37:04,320 --> 00:37:08,720
hidden states and these hidden states

896
00:37:06,240 --> 00:37:11,760
basically acts as a corrections

897
00:37:08,720 --> 00:37:13,359
um for the continuous variables they add

898
00:37:11,760 --> 00:37:14,880
basically as additive term to correct

899
00:37:13,359 --> 00:37:18,560
those continuous variables in the

900
00:37:14,880 --> 00:37:22,000
forward pass and For the the integer

901
00:37:18,560 --> 00:37:24,720
part, we first round down the fractional

902
00:37:22,000 --> 00:37:27,040
value and then treat this uh integer

903
00:37:24,720 --> 00:37:28,720
hidden value through two mechanisms

904
00:37:27,040 --> 00:37:30,640
either what we call rounding

905
00:37:28,720 --> 00:37:31,839
classification or learnable threshold.

906
00:37:30,640 --> 00:37:34,880
They are you know practically

907
00:37:31,839 --> 00:37:37,280
equivalent. They are just trying to um

908
00:37:34,880 --> 00:37:39,040
get us this threshold for these

909
00:37:37,280 --> 00:37:41,040
indicator functions to decide whether we

910
00:37:39,040 --> 00:37:43,280
should round up and down. So one is

911
00:37:41,040 --> 00:37:45,599
implemented through a scaled sigmoid

912
00:37:43,280 --> 00:37:47,520
layer and another one with a gumball

913
00:37:45,599 --> 00:37:49,760
sigmoid that introduces a little bit of

914
00:37:47,520 --> 00:37:51,520
stoasticity. But the good thing about

915
00:37:49,760 --> 00:37:54,000
these functions is that they are smooth

916
00:37:51,520 --> 00:37:56,640
and we know analytically they're lip

917
00:37:54,000 --> 00:37:58,800
shits uh constants which is will be very

918
00:37:56,640 --> 00:38:01,440
important for like theoretical analysis

919
00:37:58,800 --> 00:38:03,920
of these uh integer layers in in

920
00:38:01,440 --> 00:38:06,720
projection mechanisms.

921
00:38:03,920 --> 00:38:09,359
Good. So that's uh integer correction

922
00:38:06,720 --> 00:38:12,480
layer allows us to round forward in the

923
00:38:09,359 --> 00:38:14,720
pass uh with this hard rounding and

924
00:38:12,480 --> 00:38:17,440
retains differentiability uh thanks to

925
00:38:14,720 --> 00:38:19,440
the smooth approximations uh of the

926
00:38:17,440 --> 00:38:21,359
rounding threshold and how we deal with

927
00:38:19,440 --> 00:38:23,119
this indicator function we just ignore

928
00:38:21,359 --> 00:38:24,640
it in backward pass through straight

929
00:38:23,119 --> 00:38:27,200
straight to estimator we basically treat

930
00:38:24,640 --> 00:38:29,599
it as identity so that's kind of a trick

931
00:38:27,200 --> 00:38:32,720
for differentiability has been used in

932
00:38:29,599 --> 00:38:36,640
machine learning for a long time

933
00:38:32,720 --> 00:38:38,720
okay uh some small example to illustrate

934
00:38:36,640 --> 00:38:40,480
uh how these layers would work. Let's

935
00:38:38,720 --> 00:38:42,800
say you have some parametric problem

936
00:38:40,480 --> 00:38:45,280
where a and b is a parameter for

937
00:38:42,800 --> 00:38:48,640
objective function and constraint and it

938
00:38:45,280 --> 00:38:50,800
is feature for your uh first continuous

939
00:38:48,640 --> 00:38:53,680
relaxation of the problem that gives you

940
00:38:50,800 --> 00:38:55,280
this relaxed primals and then you take

941
00:38:53,680 --> 00:38:57,680
those relaxed primers together with

942
00:38:55,280 --> 00:38:59,520
those parametric instances in the second

943
00:38:57,680 --> 00:39:01,839
neural network that gives you these

944
00:38:59,520 --> 00:39:03,760
hidden states. And the hidden state for

945
00:39:01,839 --> 00:39:05,200
the continuous variable you just use for

946
00:39:03,760 --> 00:39:08,000
directly updating the continuous

947
00:39:05,200 --> 00:39:10,000
variable. And the hidden state for the

948
00:39:08,000 --> 00:39:12,160
integer variable you either run through

949
00:39:10,000 --> 00:39:13,920
this gumball sigmoid layer or scaled

950
00:39:12,160 --> 00:39:15,760
sigmoid layer to give you the the

951
00:39:13,920 --> 00:39:18,560
threshold value for rounding up and

952
00:39:15,760 --> 00:39:20,880
down. And you train this end to end with

953
00:39:18,560 --> 00:39:22,320
this uh empirical risk minimization of

954
00:39:20,880 --> 00:39:24,320
your objective and constraint

955
00:39:22,320 --> 00:39:27,119
violations. So that's like kind of

956
00:39:24,320 --> 00:39:28,880
learning this uh conditional rounding

957
00:39:27,119 --> 00:39:31,440
for this class of problem. So it's not

958
00:39:28,880 --> 00:39:33,200
like general rounding for any problem.

959
00:39:31,440 --> 00:39:36,400
You will train the rounding mechanism

960
00:39:33,200 --> 00:39:38,880
for your specific problem to um favor

961
00:39:36,400 --> 00:39:41,680
feasibility but there are no physibility

962
00:39:38,880 --> 00:39:43,359
guarantees as such in this integer

963
00:39:41,680 --> 00:39:44,880
rounding with empirical risk

964
00:39:43,359 --> 00:39:48,160
minimization training. So the question

965
00:39:44,880 --> 00:39:50,720
is how can we enforce this physibility

966
00:39:48,160 --> 00:39:53,440
and here is the inspiration from the DCT

967
00:39:50,720 --> 00:39:55,280
tribe projection that we can also uh

968
00:39:53,440 --> 00:39:57,520
think as

969
00:39:55,280 --> 00:39:59,760
uh alternative to physibility pump

970
00:39:57,520 --> 00:40:01,520
heristics that is used in uh mixed

971
00:39:59,760 --> 00:40:04,000
integer programming that again

972
00:40:01,520 --> 00:40:06,480
alternates between rounding and uh

973
00:40:04,000 --> 00:40:08,960
projection. So again you solve a

974
00:40:06,480 --> 00:40:10,960
continuous problem you round to the

975
00:40:08,960 --> 00:40:12,800
nearest integer if it is invisible you

976
00:40:10,960 --> 00:40:16,480
solve projection problem again on a

977
00:40:12,800 --> 00:40:20,320
continuous space uh and you you youerate

978
00:40:16,480 --> 00:40:22,880
yeah u until convergence. So here we

979
00:40:20,320 --> 00:40:25,920
again ask questions how can you utilize

980
00:40:22,880 --> 00:40:29,359
this integer uh correction layers that

981
00:40:25,920 --> 00:40:33,680
ren type mechanism in the context of uh

982
00:40:29,359 --> 00:40:36,880
gradient base updates and the mechanism

983
00:40:33,680 --> 00:40:38,880
is very straightforward. We um okay

984
00:40:36,880 --> 00:40:41,040
assume that we have this initial relax

985
00:40:38,880 --> 00:40:43,359
solution. We have the parametric

986
00:40:41,040 --> 00:40:47,040
instance and we have this pre-trained

987
00:40:43,359 --> 00:40:48,560
correction layer. So that acts acts

988
00:40:47,040 --> 00:40:50,079
basically as a transformation of this

989
00:40:48,560 --> 00:40:51,920
relax solution to the mix integer

990
00:40:50,079 --> 00:40:54,400
solution and then we use this mix

991
00:40:51,920 --> 00:40:56,640
integer solution to evaluate uh our

992
00:40:54,400 --> 00:40:59,520
constraint violations and it can be done

993
00:40:56,640 --> 00:41:02,480
with L1 L2 penalties you get basically

994
00:40:59,520 --> 00:41:05,040
this scalar energy function for the

995
00:41:02,480 --> 00:41:08,400
constraint violations and if it is zero

996
00:41:05,040 --> 00:41:10,720
okay you terminate if it is non zero you

997
00:41:08,400 --> 00:41:13,359
basically run gradient descent on this

998
00:41:10,720 --> 00:41:16,240
function but on the continuous variable

999
00:41:13,359 --> 00:41:19,040
not on the integer var variable here. So

1000
00:41:16,240 --> 00:41:22,319
basically um you have this composite

1001
00:41:19,040 --> 00:41:23,760
function this uh penalty and the integer

1002
00:41:22,319 --> 00:41:25,200
correction layer that you you're

1003
00:41:23,760 --> 00:41:28,160
differentiating through to update the

1004
00:41:25,200 --> 00:41:31,760
continuous variable. And this this was

1005
00:41:28,160 --> 00:41:34,800
really like important from empirical uh

1006
00:41:31,760 --> 00:41:38,000
perspective uh to get us

1007
00:41:34,800 --> 00:41:39,280
um robust uh performance from this

1008
00:41:38,000 --> 00:41:41,599
algorithm. We are still like trying to

1009
00:41:39,280 --> 00:41:44,560
theoretically understand why it is so

1010
00:41:41,599 --> 00:41:49,280
and um one of the potential explanations

1011
00:41:44,560 --> 00:41:51,760
uh could be um our um um convergence

1012
00:41:49,280 --> 00:41:54,240
analysis that um is based on on some

1013
00:41:51,760 --> 00:41:58,720
assumptions of course. So if you assume

1014
00:41:54,240 --> 00:42:02,160
that our constraints are uh l smooth and

1015
00:41:58,720 --> 00:42:04,800
um they admit the

1016
00:42:02,160 --> 00:42:06,640
lipshits gradients then we can use

1017
00:42:04,800 --> 00:42:09,119
standard descent lema on this but of

1018
00:42:06,640 --> 00:42:12,640
course this will be approximate uh uh

1019
00:42:09,119 --> 00:42:14,720
convergence guarantees for less uh

1020
00:42:12,640 --> 00:42:16,640
stringent structural assumptions we can

1021
00:42:14,720 --> 00:42:18,640
use this cordika lawyer sherich

1022
00:42:16,640 --> 00:42:20,800
inequality as again these are standard

1023
00:42:18,640 --> 00:42:22,720
tools in nonlinear optimization we are

1024
00:42:20,800 --> 00:42:25,359
which we are borrowing here in the

1025
00:42:22,720 --> 00:42:26,960
context of this uh gradient based uh

1026
00:42:25,359 --> 00:42:30,160
optimization.

1027
00:42:26,960 --> 00:42:32,079
Uh so a lot of interesting uh

1028
00:42:30,160 --> 00:42:34,640
theoretical advancement that we can

1029
00:42:32,079 --> 00:42:39,040
still uh say about the approximation

1030
00:42:34,640 --> 00:42:41,280
error of our uh smoothened um uh

1031
00:42:39,040 --> 00:42:43,200
gradients of of the integer correction

1032
00:42:41,280 --> 00:42:44,800
layers with with respect to the hard

1033
00:42:43,200 --> 00:42:48,240
integer rounding and how this

1034
00:42:44,800 --> 00:42:50,400
approximation uh layer um interplays

1035
00:42:48,240 --> 00:42:53,200
with our approximate uh physibility

1036
00:42:50,400 --> 00:42:56,560
guarantees. So we with assumptions on

1037
00:42:53,200 --> 00:43:01,520
smoothness you can also obtain one / k

1038
00:42:56,560 --> 00:43:04,560
complexity um um

1039
00:43:01,520 --> 00:43:06,480
non asytotic convergence guarantees but

1040
00:43:04,560 --> 00:43:09,040
these are still again approximate uh

1041
00:43:06,480 --> 00:43:10,800
convergence guarantees u lot of space

1042
00:43:09,040 --> 00:43:14,319
for improvement in this theoretical

1043
00:43:10,800 --> 00:43:17,040
analysis but thanks to family gradient

1044
00:43:14,319 --> 00:43:18,720
based uh structure and and moodness

1045
00:43:17,040 --> 00:43:20,880
approximations that really like opens

1046
00:43:18,720 --> 00:43:24,400
door for some uh more rigorous

1047
00:43:20,880 --> 00:43:28,000
visibility uh analysis here. So how

1048
00:43:24,400 --> 00:43:30,480
about empirical performance? Um we are

1049
00:43:28,000 --> 00:43:33,839
aiming to solve large scale optimization

1050
00:43:30,480 --> 00:43:36,160
problems where traditional solvers fail

1051
00:43:33,839 --> 00:43:39,760
or too they are too slow to be

1052
00:43:36,160 --> 00:43:41,760
implemented in in in practice and in

1053
00:43:39,760 --> 00:43:44,640
those settings even for like modest size

1054
00:43:41,760 --> 00:43:47,119
problems like 100 decision variables uh

1055
00:43:44,640 --> 00:43:51,200
they're integer and 100 constraints for

1056
00:43:47,119 --> 00:43:52,800
quality cases. um state-of-the-art

1057
00:43:51,200 --> 00:43:54,880
solvers like Gurobi or skip for

1058
00:43:52,800 --> 00:43:56,319
nonlinear optimization over time they

1059
00:43:54,880 --> 00:43:59,119
will find better solution than than

1060
00:43:56,319 --> 00:44:00,960
these approximate algorithms but by

1061
00:43:59,119 --> 00:44:02,880
pre-training these parametric solutions

1062
00:44:00,960 --> 00:44:05,359
offline we are really offloading a lot

1063
00:44:02,880 --> 00:44:07,920
of cost for the online inference because

1064
00:44:05,359 --> 00:44:10,160
now we can in milliseconds obtain a high

1065
00:44:07,920 --> 00:44:12,960
quality solutions with feasibility

1066
00:44:10,160 --> 00:44:14,480
guarantees on inequalities um that can

1067
00:44:12,960 --> 00:44:16,960
make a difference between real-time

1068
00:44:14,480 --> 00:44:18,560
implementation so that's what is what we

1069
00:44:16,960 --> 00:44:20,720
are really arguing here and empirical

1070
00:44:18,560 --> 00:44:22,400
ally we are getting up to five or

1071
00:44:20,720 --> 00:44:24,880
magnitude acceleration which can be

1072
00:44:22,400 --> 00:44:26,079
really massive

1073
00:44:24,880 --> 00:44:26,800
>> yes to ask

1074
00:44:26,079 --> 00:44:30,079
>> please please yeah

1075
00:44:26,800 --> 00:44:33,599
>> yeah question one is like how do you

1076
00:44:30,079 --> 00:44:35,920
choose lambda I mean the term balancing

1077
00:44:33,599 --> 00:44:38,240
>> that's that's a crucial question so uh

1078
00:44:35,920 --> 00:44:42,160
in

1079
00:44:38,240 --> 00:44:45,119
next slide yeah so we did lot of um

1080
00:44:42,160 --> 00:44:48,240
ablation studies empirically to analyze

1081
00:44:45,119 --> 00:44:50,560
the effect of uh the penalty weight And

1082
00:44:48,240 --> 00:44:52,079
of course like if you are putting high

1083
00:44:50,560 --> 00:44:53,839
penalty weight on the constraints you

1084
00:44:52,079 --> 00:44:59,119
are favoring feasibility versus

1085
00:44:53,839 --> 00:45:02,240
optimality. And for instance um these uh

1086
00:44:59,119 --> 00:45:04,480
full full lines represent uh just

1087
00:45:02,240 --> 00:45:07,119
training the policies with integer

1088
00:45:04,480 --> 00:45:08,880
correction layers without uh physibility

1089
00:45:07,119 --> 00:45:11,520
projection. So in order to get

1090
00:45:08,880 --> 00:45:15,359
empirically high degrees of uh

1091
00:45:11,520 --> 00:45:18,720
feasibility where feasibility uh goes up

1092
00:45:15,359 --> 00:45:21,760
on on the y-axis uh we require high

1093
00:45:18,720 --> 00:45:24,240
values of of these lambdas. Yeah. High

1094
00:45:21,760 --> 00:45:26,480
penalties but that kind of significantly

1095
00:45:24,240 --> 00:45:29,440
degrades our our objective values of

1096
00:45:26,480 --> 00:45:31,119
course. Um so after applying these

1097
00:45:29,440 --> 00:45:35,440
physibility projections we can train

1098
00:45:31,119 --> 00:45:37,920
with way smaller penalties and get and

1099
00:45:35,440 --> 00:45:40,240
kind of uh get the the physibility

1100
00:45:37,920 --> 00:45:42,960
projection handle uh the physibility

1101
00:45:40,240 --> 00:45:46,160
part and if we are combining them

1102
00:45:42,960 --> 00:45:47,839
together we are getting comparable and

1103
00:45:46,160 --> 00:45:50,079
sometimes better solutions especially in

1104
00:45:47,839 --> 00:45:52,960
the mix integer nonlinear programming uh

1105
00:45:50,079 --> 00:45:54,720
case than uh state-of-the-art solvers.

1106
00:45:52,960 --> 00:45:56,000
So there is this type of trade-off which

1107
00:45:54,720 --> 00:45:58,800
we are still trying to understand

1108
00:45:56,000 --> 00:46:03,680
theoretically. Um but empirically we are

1109
00:45:58,800 --> 00:46:06,400
having very interesting results on kind

1110
00:46:03,680 --> 00:46:09,119
of large scale uh optimization problems.

1111
00:46:06,400 --> 00:46:11,280
I think the largest one was 20,000 mix

1112
00:46:09,119 --> 00:46:14,560
integer decision variables that we are

1113
00:46:11,280 --> 00:46:17,359
still able to train on on the CPU within

1114
00:46:14,560 --> 00:46:20,319
two hours and we train these policies

1115
00:46:17,359 --> 00:46:23,440
and we still have um subsecond inference

1116
00:46:20,319 --> 00:46:25,520
time. So that's

1117
00:46:23,440 --> 00:46:28,319
uh that's something that I'm uh really

1118
00:46:25,520 --> 00:46:31,200
excited about. Uh lot of opportunities

1119
00:46:28,319 --> 00:46:33,440
about theoretically answering these

1120
00:46:31,200 --> 00:46:35,200
tuning questions also can come from

1121
00:46:33,440 --> 00:46:36,720
applied math community especially the

1122
00:46:35,200 --> 00:46:39,119
residual learning in pins. They're

1123
00:46:36,720 --> 00:46:42,319
they're putting a lot of effort in uh

1124
00:46:39,119 --> 00:46:44,079
adaptive uh weights tuning uh based on

1125
00:46:42,319 --> 00:46:45,599
analysis of your gradients of the loss

1126
00:46:44,079 --> 00:46:47,359
functions and and the loss function

1127
00:46:45,599 --> 00:46:50,400
residuals and adaptive sampling of your

1128
00:46:47,359 --> 00:46:52,400
parametric scenarios in regions that are

1129
00:46:50,400 --> 00:46:54,000
of a high error and we can really

1130
00:46:52,400 --> 00:46:55,280
interpret this also the regions of like

1131
00:46:54,000 --> 00:46:59,280
when the active constraints in

1132
00:46:55,280 --> 00:47:02,640
optimization become um are become active

1133
00:46:59,280 --> 00:47:04,720
basically. So

1134
00:47:02,640 --> 00:47:06,079
the the the this kind of generalized

1135
00:47:04,720 --> 00:47:08,000
view of scientific machine learning that

1136
00:47:06,079 --> 00:47:10,000
brings the ideas from applied mod

1137
00:47:08,000 --> 00:47:14,000
operation research and controls I think

1138
00:47:10,000 --> 00:47:16,560
can create nice opportunities for new

1139
00:47:14,000 --> 00:47:18,960
methods and theoretical development on

1140
00:47:16,560 --> 00:47:20,960
those intersections.

1141
00:47:18,960 --> 00:47:22,640
>> Yeah good. Yeah I think I have another

1142
00:47:20,960 --> 00:47:25,119
question like I think if I understand

1143
00:47:22,640 --> 00:47:27,359
correctly uh I mean the rounding part

1144
00:47:25,119 --> 00:47:29,760
going to deal with the uh integer part.

1145
00:47:27,359 --> 00:47:31,359
Yes. And then the physibility going to

1146
00:47:29,760 --> 00:47:32,960
deal with like continuous

1147
00:47:31,359 --> 00:47:35,200
>> um like

1148
00:47:32,960 --> 00:47:36,640
>> yes uh we are updating the continuous

1149
00:47:35,200 --> 00:47:38,400
variable in the physibility but because

1150
00:47:36,640 --> 00:47:41,440
you have this hard integer rounding in a

1151
00:47:38,400 --> 00:47:43,599
forward pass. So you are actually you

1152
00:47:41,440 --> 00:47:46,240
are actually embedding that rounding in

1153
00:47:43,599 --> 00:47:50,160
the physibility projection layer.

1154
00:47:46,240 --> 00:47:52,960
>> I see. So like mean the fability uh part

1155
00:47:50,160 --> 00:47:56,400
can also change the integer variable.

1156
00:47:52,960 --> 00:47:59,520
>> Exactly. So like um this uh energy

1157
00:47:56,400 --> 00:48:02,640
function like there's a uh the penalty

1158
00:47:59,520 --> 00:48:04,400
on the constraint violations where the

1159
00:48:02,640 --> 00:48:07,200
integer variable is represented by this

1160
00:48:04,400 --> 00:48:08,400
integer correction layer and thanks to

1161
00:48:07,200 --> 00:48:10,079
this smooth approximation of the

1162
00:48:08,400 --> 00:48:12,319
gradients it allows us to compute the

1163
00:48:10,079 --> 00:48:13,920
gradient of uh this function to update

1164
00:48:12,319 --> 00:48:15,920
the continuous variable. But the

1165
00:48:13,920 --> 00:48:19,119
continuous variable is a pass through

1166
00:48:15,920 --> 00:48:20,640
the the rounding layer in a forward

1167
00:48:19,119 --> 00:48:23,760
pass.

1168
00:48:20,640 --> 00:48:25,200
I see I mean it's it's true that all

1169
00:48:23,760 --> 00:48:27,839
together you know

1170
00:48:25,200 --> 00:48:30,559
>> well actually in our experiments what we

1171
00:48:27,839 --> 00:48:33,200
have done we trained the integer

1172
00:48:30,559 --> 00:48:35,200
rounding without projection and then we

1173
00:48:33,200 --> 00:48:38,079
use the projection as a post-processing

1174
00:48:35,200 --> 00:48:40,480
you could do that um but from the

1175
00:48:38,079 --> 00:48:42,559
perspective of the stability of the

1176
00:48:40,480 --> 00:48:44,559
learning and avoiding exploding

1177
00:48:42,559 --> 00:48:46,400
vanishing gradients we kind of separated

1178
00:48:44,559 --> 00:48:48,319
these two so offline training of the

1179
00:48:46,400 --> 00:48:51,920
integer correction and uh

1180
00:48:48,319 --> 00:48:53,599
post-processing of uh projection layers.

1181
00:48:51,920 --> 00:48:58,640
>> Yeah, thank thanks for question really

1182
00:48:53,599 --> 00:49:01,359
good uh and um really relevant for this

1183
00:48:58,640 --> 00:49:04,000
wonderful work that has been done with

1184
00:49:01,359 --> 00:49:06,160
LS Khalil and Bangk

1185
00:49:04,000 --> 00:49:08,720
will be joining your group soon. So yeah

1186
00:49:06,160 --> 00:49:11,200
um really there was one of my uh great

1187
00:49:08,720 --> 00:49:14,480
collaborations I had in last two years.

1188
00:49:11,200 --> 00:49:16,559
Um that brings me to the end of my talk

1189
00:49:14,480 --> 00:49:18,480
where I was arguing for scientific

1190
00:49:16,559 --> 00:49:20,000
machine learning as really potentially

1191
00:49:18,480 --> 00:49:23,119
unifying

1192
00:49:20,000 --> 00:49:25,520
um methodological advancements that can

1193
00:49:23,119 --> 00:49:27,200
bring together communities from applied

1194
00:49:25,520 --> 00:49:30,160
math, operation research, computer

1195
00:49:27,200 --> 00:49:32,800
science and controls to develop this

1196
00:49:30,160 --> 00:49:35,520
type of toolkit that is interoperable

1197
00:49:32,800 --> 00:49:37,440
and can be practical for important

1198
00:49:35,520 --> 00:49:38,880
applications in sustainable energy where

1199
00:49:37,440 --> 00:49:41,280
we are dealing with large decision

1200
00:49:38,880 --> 00:49:43,839
spaces. u mixed integer decision

1201
00:49:41,280 --> 00:49:45,599
variables and hard constraints that are

1202
00:49:43,839 --> 00:49:48,559
necessary for stability and safety of

1203
00:49:45,599 --> 00:49:50,559
these systems. So um a lot of challenges

1204
00:49:48,559 --> 00:49:54,480
remain that I have been just talking

1205
00:49:50,559 --> 00:49:56,480
about two today but uh lot of work on u

1206
00:49:54,480 --> 00:49:58,480
backends on effective automatic

1207
00:49:56,480 --> 00:50:00,400
differentiation dealing with exporting

1208
00:49:58,480 --> 00:50:02,559
vanish ingredients illition problem

1209
00:50:00,400 --> 00:50:05,920
which many of them are ill conditioned

1210
00:50:02,559 --> 00:50:08,880
and require careful tuning and modeling

1211
00:50:05,920 --> 00:50:10,480
um computational cost of guarantees

1212
00:50:08,880 --> 00:50:14,640
there are a lot of interesting methods

1213
00:50:10,480 --> 00:50:17,200
for verifying this machine learning but

1214
00:50:14,640 --> 00:50:18,720
uh yeah they're cost for uncertainty

1215
00:50:17,200 --> 00:50:21,599
quantification and hard constraint

1216
00:50:18,720 --> 00:50:25,920
guarantees is still u largely

1217
00:50:21,599 --> 00:50:28,880
prohibitive for online um inference. Um

1218
00:50:25,920 --> 00:50:31,119
but I want to end on the bright side on

1219
00:50:28,880 --> 00:50:33,760
the opportunities where I was not

1220
00:50:31,119 --> 00:50:36,640
talking about potential of solving PD

1221
00:50:33,760 --> 00:50:39,119
constraint problems or optimal control

1222
00:50:36,640 --> 00:50:40,400
for PDS that I know many people in the

1223
00:50:39,119 --> 00:50:43,119
community are start looking at through

1224
00:50:40,400 --> 00:50:45,760
neural operators uh the University of

1225
00:50:43,119 --> 00:50:48,880
California San Diego specifically and

1226
00:50:45,760 --> 00:50:51,520
other areas and what I'm passionate

1227
00:50:48,880 --> 00:50:53,839
about is not only developing new methods

1228
00:50:51,520 --> 00:50:56,400
and theories but working on software

1229
00:50:53,839 --> 00:50:59,680
tools so we can really transition this

1230
00:50:56,400 --> 00:51:02,000
into practical viable applications that

1231
00:50:59,680 --> 00:51:04,800
um can really

1232
00:51:02,000 --> 00:51:09,119
enable the change the transition in in

1233
00:51:04,800 --> 00:51:12,880
applications that are you know important

1234
00:51:09,119 --> 00:51:15,520
more than ever and uh hopefully we can

1235
00:51:12,880 --> 00:51:17,839
enable that by cutting the cost of

1236
00:51:15,520 --> 00:51:21,440
technologies and making the economical

1237
00:51:17,839 --> 00:51:24,079
case uh for sustainable energy. So here

1238
00:51:21,440 --> 00:51:26,559
I want to yeah thank you very much uh

1239
00:51:24,079 --> 00:51:28,160
for being here and uh thanks for all the

1240
00:51:26,559 --> 00:51:33,590
questions.

1241
00:51:28,160 --> 00:51:33,590
[Applause]

