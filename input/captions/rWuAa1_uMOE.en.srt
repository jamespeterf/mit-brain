1
00:00:01,120 --> 00:00:06,240
So, it's wonderful to be here and um

2
00:00:03,200 --> 00:00:09,040
thanks so much uh to CIRC for organizing

3
00:00:06,240 --> 00:00:12,160
um this day and um for the funding of

4
00:00:09,040 --> 00:00:14,559
this project. Um I'm here representing

5
00:00:12,160 --> 00:00:16,560
um Josh Tennibal and myself in this

6
00:00:14,559 --> 00:00:19,439
collaborative work we um have done

7
00:00:16,560 --> 00:00:22,080
together.

8
00:00:19,439 --> 00:00:24,240
So we'll start off by pointing out an

9
00:00:22,080 --> 00:00:26,800
interesting observation about moral

10
00:00:24,240 --> 00:00:29,840
cognition that one of the most striking

11
00:00:26,800 --> 00:00:32,239
features of the human moral mind is its

12
00:00:29,840 --> 00:00:34,280
flexibility. We can make judgments about

13
00:00:32,239 --> 00:00:37,040
moral cases that we've never seen

14
00:00:34,280 --> 00:00:39,600
before. Is it morally permissible, for

15
00:00:37,040 --> 00:00:42,640
instance, to get a haircut during a

16
00:00:39,600 --> 00:00:44,800
global pandemic? Before a couple of

17
00:00:42,640 --> 00:00:47,360
years ago, none of us had ever thought

18
00:00:44,800 --> 00:00:49,039
about that issue before.

19
00:00:47,360 --> 00:00:50,800
And there doesn't need to be a global

20
00:00:49,039 --> 00:00:52,719
pandemic in order for the moral

21
00:00:50,800 --> 00:00:54,640
landscape to change. There are new

22
00:00:52,719 --> 00:00:57,360
technologies all the time, new

23
00:00:54,640 --> 00:00:59,520
information, and new opportunities. And

24
00:00:57,360 --> 00:01:01,920
we need to make decisions about how it's

25
00:00:59,520 --> 00:01:03,559
morally acceptable to act in light of

26
00:01:01,920 --> 00:01:06,400
these

27
00:01:03,559 --> 00:01:08,720
changes. So what we do in our line of

28
00:01:06,400 --> 00:01:11,920
work is we work at the intersection of

29
00:01:08,720 --> 00:01:13,920
cognitive science and AI safety. think

30
00:01:11,920 --> 00:01:16,240
thinking about the ways that the human

31
00:01:13,920 --> 00:01:18,320
mind works, trying to understand that

32
00:01:16,240 --> 00:01:20,840
with a certain kind of computational

33
00:01:18,320 --> 00:01:24,560
precision in order to see if we can

34
00:01:20,840 --> 00:01:26,720
inform AI safety, how AI systems can

35
00:01:24,560 --> 00:01:29,119
navigate the human, moral, and social

36
00:01:26,720 --> 00:01:31,920
world in a way that's morally acceptable

37
00:01:29,119 --> 00:01:34,000
and safe. And the kind of ideal vision

38
00:01:31,920 --> 00:01:36,159
of this program is that there should be

39
00:01:34,000 --> 00:01:39,119
a dynamic exchange between these two

40
00:01:36,159 --> 00:01:41,439
fields. that as we learn things about

41
00:01:39,119 --> 00:01:43,680
human moral cognition that'll help make

42
00:01:41,439 --> 00:01:45,840
AI systems safer. And then as we attempt

43
00:01:43,680 --> 00:01:47,200
to build those things into AI systems,

44
00:01:45,840 --> 00:01:49,200
we'll discover we actually don't

45
00:01:47,200 --> 00:01:51,119
understand moral cognition as well as we

46
00:01:49,200 --> 00:01:53,040
thought to originally. So th this works

47
00:01:51,119 --> 00:01:56,759
in a kind of virtuous cycle as this

48
00:01:53,040 --> 00:01:56,759
research program progresses.

49
00:01:56,880 --> 00:02:00,000
What I'm going to do today is talk to

50
00:01:58,399 --> 00:02:02,880
you mostly about the cognitive science

51
00:02:00,000 --> 00:02:05,119
work that we did um writing down in

52
00:02:02,880 --> 00:02:07,280
computational terms how you the human

53
00:02:05,119 --> 00:02:10,560
moral mind works and then what I want to

54
00:02:07,280 --> 00:02:12,480
do is sketch also how we think that what

55
00:02:10,560 --> 00:02:14,720
we've learned about moral cognition

56
00:02:12,480 --> 00:02:17,160
could be an important alignment target

57
00:02:14,720 --> 00:02:20,480
for AI safety quite

58
00:02:17,160 --> 00:02:23,599
broadly. Okay. So how does moral

59
00:02:20,480 --> 00:02:26,160
judgment work? One thing seems obvious

60
00:02:23,599 --> 00:02:29,520
just from a simple glance at our moral

61
00:02:26,160 --> 00:02:31,760
world. Rules seem to be important. The

62
00:02:29,520 --> 00:02:34,720
moral world seems to be guided by these

63
00:02:31,760 --> 00:02:36,800
inviable rules. Often we use rules to

64
00:02:34,720 --> 00:02:39,440
communicate moral expectations to one

65
00:02:36,800 --> 00:02:40,959
another. We use them um to teach our

66
00:02:39,440 --> 00:02:44,000
children how to act in social

67
00:02:40,959 --> 00:02:45,599
situations. Um, there are rules about

68
00:02:44,000 --> 00:02:47,360
the way we treat our experimental

69
00:02:45,599 --> 00:02:49,360
subjects, what counts as honesty in

70
00:02:47,360 --> 00:02:52,000
academic work, how we're supposed to

71
00:02:49,360 --> 00:02:54,239
navigate complex um, relationships in

72
00:02:52,000 --> 00:02:56,319
the workplace. Um, they're rules

73
00:02:54,239 --> 00:02:58,480
everywhere and they often seem to be

74
00:02:56,319 --> 00:03:01,319
written in stone and in fact sometimes

75
00:02:58,480 --> 00:03:04,319
they literally are written in

76
00:03:01,319 --> 00:03:07,040
stone. So why do rules play such an

77
00:03:04,319 --> 00:03:09,200
important part of our moral lives and

78
00:03:07,040 --> 00:03:11,519
our theories of morality? Well, there

79
00:03:09,200 --> 00:03:13,200
are many reasons, but one of the most

80
00:03:11,519 --> 00:03:16,000
important reasons is that they're

81
00:03:13,200 --> 00:03:19,000
efficient. Our human minds are limited

82
00:03:16,000 --> 00:03:22,319
in various important ways, and rules are

83
00:03:19,000 --> 00:03:24,440
simple ways to articulate kind of

84
00:03:22,319 --> 00:03:27,519
complex

85
00:03:24,440 --> 00:03:29,920
ideas. So, def despite the fact that

86
00:03:27,519 --> 00:03:32,360
rules seem like they have this rigidity

87
00:03:29,920 --> 00:03:34,480
to them, there are also constant

88
00:03:32,360 --> 00:03:36,319
exceptions. So, for instance, it seems

89
00:03:34,480 --> 00:03:39,360
like there's one very simple rule about

90
00:03:36,319 --> 00:03:41,599
stealing. don't steal. But on the other

91
00:03:39,360 --> 00:03:44,720
hand, we also know that it's okay to

92
00:03:41,599 --> 00:03:46,959
jump in into a cafe and steal a napkin

93
00:03:44,720 --> 00:03:48,400
if you really need one. Yet, we also

94
00:03:46,959 --> 00:03:50,959
know that you can't just take the whole

95
00:03:48,400 --> 00:03:52,799
stack to refill your supply at home. So,

96
00:03:50,959 --> 00:03:54,720
it seems like actually when we dig a

97
00:03:52,799 --> 00:03:57,519
little deeper, we realize that we have

98
00:03:54,720 --> 00:04:00,239
this kind of complex and subtle idea

99
00:03:57,519 --> 00:04:03,599
about what the rule is, when it applies,

100
00:04:00,239 --> 00:04:06,720
and when when it doesn't apply.

101
00:04:03,599 --> 00:04:08,959
Um this is a a lovely example of this

102
00:04:06,720 --> 00:04:11,280
sort of thing. So this is um the kitchen

103
00:04:08,959 --> 00:04:13,200
from building 46 right across the hall

104
00:04:11,280 --> 00:04:15,280
down the street from my old office when

105
00:04:13,200 --> 00:04:17,519
I was postto. And if you zoom in here

106
00:04:15,280 --> 00:04:20,400
you see that someone has written on this

107
00:04:17,519 --> 00:04:22,720
bottle of soap do not steal. So the

108
00:04:20,400 --> 00:04:24,800
question arises don't MIT graduate

109
00:04:22,720 --> 00:04:27,520
students know that it's wrong not to

110
00:04:24,800 --> 00:04:30,080
steal and certainly they do. So what was

111
00:04:27,520 --> 00:04:33,919
this attempting to communicate? It seems

112
00:04:30,080 --> 00:04:36,080
like basic somebody thought that um the

113
00:04:33,919 --> 00:04:39,800
act of taking the do soap did not count

114
00:04:36,080 --> 00:04:43,280
as stealing. So we have a vigilante um

115
00:04:39,800 --> 00:04:45,600
soap um idea corrector who is trying to

116
00:04:43,280 --> 00:04:47,600
tell you that because of the background

117
00:04:45,600 --> 00:04:50,400
conditions here this actually does count

118
00:04:47,600 --> 00:04:53,520
as a case of stealing. So the point here

119
00:04:50,400 --> 00:04:55,680
is that we often have a fine grain um

120
00:04:53,520 --> 00:04:59,000
idea about what rules are, but we can

121
00:04:55,680 --> 00:05:01,360
sometimes go wrong with our use of rule

122
00:04:59,000 --> 00:05:03,199
flexibility. Okay, so there's a lots of

123
00:05:01,360 --> 00:05:04,720
mysteries about rules. Where do they

124
00:05:03,199 --> 00:05:07,120
come from? What do we do when there are

125
00:05:04,720 --> 00:05:08,800
no rules? How do we make new ones? And

126
00:05:07,120 --> 00:05:10,199
how do we know when the rules apply and

127
00:05:08,800 --> 00:05:12,440
they don't

128
00:05:10,199 --> 00:05:15,199
apply? Rules are

129
00:05:12,440 --> 00:05:17,919
flexible. And so we need theories in

130
00:05:15,199 --> 00:05:19,720
order to understand what where and how

131
00:05:17,919 --> 00:05:22,800
this flexibility

132
00:05:19,720 --> 00:05:24,880
arises. So moral psychology has

133
00:05:22,800 --> 00:05:27,120
typically used theories of moral

134
00:05:24,880 --> 00:05:28,800
philosophy in order to get off the

135
00:05:27,120 --> 00:05:30,960
ground in order to make models about how

136
00:05:28,800 --> 00:05:33,199
the human mind works. And traditionally

137
00:05:30,960 --> 00:05:34,880
in moral psychology we've relied on two

138
00:05:33,199 --> 00:05:37,440
main theories of moral philosophy

139
00:05:34,880 --> 00:05:40,479
deontology and consequentialism. Though

140
00:05:37,440 --> 00:05:42,759
in our work, a central driving theory is

141
00:05:40,479 --> 00:05:44,639
this these ideas from

142
00:05:42,759 --> 00:05:47,360
contractualism. And so what we're

143
00:05:44,639 --> 00:05:49,840
attempting to do is show that all of

144
00:05:47,360 --> 00:05:52,479
these theories from moral philosophy can

145
00:05:49,840 --> 00:05:54,880
be combined into a triple theory of

146
00:05:52,479 --> 00:05:57,039
morality which explains not only where

147
00:05:54,880 --> 00:05:58,960
rules come from but how they are

148
00:05:57,039 --> 00:06:01,919
flexible and how they can be applied in

149
00:05:58,960 --> 00:06:05,680
this flexible way.

150
00:06:01,919 --> 00:06:08,160
So what that amounts to is pointing out

151
00:06:05,680 --> 00:06:10,960
that the function of morality is

152
00:06:08,160 --> 00:06:13,520
essentially to help us all get along.

153
00:06:10,960 --> 00:06:15,440
But and that that is the core idea

154
00:06:13,520 --> 00:06:17,919
behind contractualism that we can

155
00:06:15,440 --> 00:06:20,000
understand what morality demands of us

156
00:06:17,919 --> 00:06:22,360
by thinking about what everybody would

157
00:06:20,000 --> 00:06:25,840
agree to. We can do this rational

158
00:06:22,360 --> 00:06:28,240
negotiation for mutual benefit.

159
00:06:25,840 --> 00:06:29,919
But the trouble is that is often

160
00:06:28,240 --> 00:06:32,160
extremely difficult for the moral

161
00:06:29,919 --> 00:06:34,400
situations that we find ourselves

162
00:06:32,160 --> 00:06:36,720
involved in. We lack the time,

163
00:06:34,400 --> 00:06:40,800
information, and processing power in

164
00:06:36,720 --> 00:06:43,440
order to run this full um negotiation

165
00:06:40,800 --> 00:06:45,880
that we would need to figure out how to

166
00:06:43,440 --> 00:06:49,680
reach mutual benefit, the contractualist

167
00:06:45,880 --> 00:06:52,400
ideal. So instead, we do a sort of

168
00:06:49,680 --> 00:06:56,560
resource rational approximation of that

169
00:06:52,400 --> 00:07:00,440
ideal. And what our theory proposes is

170
00:06:56,560 --> 00:07:04,240
that the rest of the moral mechanisms of

171
00:07:00,440 --> 00:07:06,240
cognition, things like using rules, um,

172
00:07:04,240 --> 00:07:08,319
calculating outcomes, the things that

173
00:07:06,240 --> 00:07:10,319
might come to mind when you think about

174
00:07:08,319 --> 00:07:13,039
how we make moral decisions, what we

175
00:07:10,319 --> 00:07:15,759
argue is that all of those mechanisms

176
00:07:13,039 --> 00:07:18,960
are simply approximations of this

177
00:07:15,759 --> 00:07:20,560
contractualist ideal. So what we've been

178
00:07:18,960 --> 00:07:23,280
doing over the past year is actually

179
00:07:20,560 --> 00:07:25,039
developing this theory and then positing

180
00:07:23,280 --> 00:07:27,840
that this theory of how human moral

181
00:07:25,039 --> 00:07:30,960
cognition works should act as a way of

182
00:07:27,840 --> 00:07:30,960
aligning AI

183
00:07:31,000 --> 00:07:35,520
systems. So I'll tell you more about the

184
00:07:33,520 --> 00:07:38,240
specific ways um that we're studying

185
00:07:35,520 --> 00:07:40,639
that in a minute. But what the broad

186
00:07:38,240 --> 00:07:43,599
idea I want you to take from this is to

187
00:07:40,639 --> 00:07:46,160
to understand that humans have these

188
00:07:43,599 --> 00:07:48,800
limitations which is why we use rules

189
00:07:46,160 --> 00:07:51,680
and other mechanisms in order to figure

190
00:07:48,800 --> 00:07:54,240
out how to act in moral situations. AI

191
00:07:51,680 --> 00:07:56,639
systems also have resource limitations.

192
00:07:54,240 --> 00:07:58,879
They also have the imi limitations of

193
00:07:56,639 --> 00:08:01,280
time, information, and processing power.

194
00:07:58,879 --> 00:08:04,879
So if that's the case, we can use human

195
00:08:01,280 --> 00:08:07,919
cognition as an inspiration for how to

196
00:08:04,879 --> 00:08:11,599
have AI systems navigate the human moral

197
00:08:07,919 --> 00:08:13,360
world as well. Okay. So how exactly do

198
00:08:11,599 --> 00:08:15,360
we study this in humans? So I'll tell

199
00:08:13,360 --> 00:08:17,440
you about a a couple of very specific

200
00:08:15,360 --> 00:08:19,520
experiments um that we ran so you get a

201
00:08:17,440 --> 00:08:21,599
sense of how how this sort of um

202
00:08:19,520 --> 00:08:23,199
research goes. So we'll start off with

203
00:08:21,599 --> 00:08:25,120
this question. How do we figure out when

204
00:08:23,199 --> 00:08:27,919
the rules apply and when they don't? Um,

205
00:08:25,120 --> 00:08:29,759
and this is work um with um Josh and my

206
00:08:27,919 --> 00:08:31,840
awesome collaborators also um in the

207
00:08:29,759 --> 00:08:34,240
computational cognitive science lab.

208
00:08:31,840 --> 00:08:35,839
Okay, so we'll start off with what again

209
00:08:34,240 --> 00:08:37,519
seems like a fairly simple rule, the

210
00:08:35,839 --> 00:08:38,880
rule about waiting in line. So it seems

211
00:08:37,519 --> 00:08:40,560
like there's one very simple rule about

212
00:08:38,880 --> 00:08:42,399
waiting in line. Just get in the back of

213
00:08:40,560 --> 00:08:44,000
the line and wait your turn, right? So

214
00:08:42,399 --> 00:08:46,880
the way that we're going to be studying

215
00:08:44,000 --> 00:08:49,200
this um is with in this like game

216
00:08:46,880 --> 00:08:51,200
environment. And what the goal of this

217
00:08:49,200 --> 00:08:53,360
the people in this game is is to collect

218
00:08:51,200 --> 00:08:55,440
water from various water sources and to

219
00:08:53,360 --> 00:08:57,360
deposit it in those buckets as quickly

220
00:08:55,440 --> 00:08:59,360
as they can. So this is what happens

221
00:08:57,360 --> 00:09:02,720
when things go well. People just pick up

222
00:08:59,360 --> 00:09:04,959
water from the buckets and they just

223
00:09:02,720 --> 00:09:07,600
kind of walk in this very orderly

224
00:09:04,959 --> 00:09:10,399
fashion um all the way to the buckets

225
00:09:07,600 --> 00:09:10,399
depositing the

226
00:09:15,240 --> 00:09:21,519
water. Okay. And um sometimes one person

227
00:09:19,360 --> 00:09:23,839
will get out of line and they'll go go

228
00:09:21,519 --> 00:09:25,480
directly towards the water source, get

229
00:09:23,839 --> 00:09:28,720
get the water and deposit it in the

230
00:09:25,480 --> 00:09:30,720
buckets. So it seems like on the face of

231
00:09:28,720 --> 00:09:32,240
things, what went wrong here is that

232
00:09:30,720 --> 00:09:34,000
this person broke the rule, right? There

233
00:09:32,240 --> 00:09:35,440
was a rule about standing in line. This

234
00:09:34,000 --> 00:09:36,440
person got in front of the line without

235
00:09:35,440 --> 00:09:38,800
waiting their

236
00:09:36,440 --> 00:09:41,040
turn. But if that's the case, then we

237
00:09:38,800 --> 00:09:42,880
have trouble explaining this particular

238
00:09:41,040 --> 00:09:44,240
situation. So in this situation,

239
00:09:42,880 --> 00:09:46,000
everybody is again waiting in line and

240
00:09:44,240 --> 00:09:48,320
we see one person get out of line and go

241
00:09:46,000 --> 00:09:51,680
directly towards the water source. But

242
00:09:48,320 --> 00:09:53,920
in this case, while the person in some

243
00:09:51,680 --> 00:09:55,600
sense broke the rule, in another sense,

244
00:09:53,920 --> 00:09:57,680
we feel relieved that they actually

245
00:09:55,600 --> 00:10:00,080
figured that out. After all, it seems

246
00:09:57,680 --> 00:10:02,000
like what they did um benefits

247
00:10:00,080 --> 00:10:03,279
everybody, including themselves, and

248
00:10:02,000 --> 00:10:05,279
it's not clear why those people were

249
00:10:03,279 --> 00:10:08,000
standing in line to begin with. So what

250
00:10:05,279 --> 00:10:09,920
we want is ultimately a theory that can

251
00:10:08,000 --> 00:10:12,640
explain the subtle ways that we

252
00:10:09,920 --> 00:10:17,000
understand when breaking this rule is a

253
00:10:12,640 --> 00:10:20,560
problem and when ultimately it's morally

254
00:10:17,000 --> 00:10:22,880
permissible. So we um propose a

255
00:10:20,560 --> 00:10:25,600
particular model of um how to go about

256
00:10:22,880 --> 00:10:27,600
thinking about this and um it's an

257
00:10:25,600 --> 00:10:29,480
approach that we've been studying for a

258
00:10:27,600 --> 00:10:31,519
number of years now um called

259
00:10:29,480 --> 00:10:34,000
universalization. Ultimately, we think

260
00:10:31,519 --> 00:10:36,800
about this as a sort of um

261
00:10:34,000 --> 00:10:39,360
contractualist heristic approximation to

262
00:10:36,800 --> 00:10:41,200
an agreementbased ideal, though you

263
00:10:39,360 --> 00:10:43,360
might have also heard of this if if

264
00:10:41,200 --> 00:10:46,399
you've read any Kant. So, this is like a

265
00:10:43,360 --> 00:10:48,640
a um a kind of Kantian model. And so,

266
00:10:46,399 --> 00:10:50,320
the the thought is that when a person

267
00:10:48,640 --> 00:10:52,959
looks at this person who's getting out

268
00:10:50,320 --> 00:10:55,440
of line, they think um what would happen

269
00:10:52,959 --> 00:10:57,200
if everybody in this situation felt at

270
00:10:55,440 --> 00:11:00,720
liberty to get out of line? What would

271
00:10:57,200 --> 00:11:02,720
that do in this universalized world? And

272
00:11:00,720 --> 00:11:04,560
what they can essentially figure out is,

273
00:11:02,720 --> 00:11:06,240
well, everybody would kind of be able to

274
00:11:04,560 --> 00:11:08,320
find their way to the water and things

275
00:11:06,240 --> 00:11:10,399
would actually go quite smoothly. This

276
00:11:08,320 --> 00:11:12,320
would actually be a good world to live

277
00:11:10,399 --> 00:11:15,120
in, a world where everybody broke the

278
00:11:12,320 --> 00:11:16,800
rule in a case like this. But if you

279
00:11:15,120 --> 00:11:17,920
contrast that to thinking about what

280
00:11:16,800 --> 00:11:19,680
would happen if everybody felt at

281
00:11:17,920 --> 00:11:21,600
liberty to get out of line in this case,

282
00:11:19,680 --> 00:11:23,600
you realize you get a lot of chaotic

283
00:11:21,600 --> 00:11:26,720
crashing. And that's just simply a bad

284
00:11:23,600 --> 00:11:28,680
thing to do. So what we propose is that

285
00:11:26,720 --> 00:11:31,360
people simulate this sort of

286
00:11:28,680 --> 00:11:33,839
universalization reasoning to figure out

287
00:11:31,360 --> 00:11:36,560
what would happen if somebody broke the

288
00:11:33,839 --> 00:11:38,959
rule in this case. So what we do as an

289
00:11:36,560 --> 00:11:40,720
experiment is we develop a whole series

290
00:11:38,959 --> 00:11:43,040
of different maps that manipulate

291
00:11:40,720 --> 00:11:46,560
various spatiotemporal features of this

292
00:11:43,040 --> 00:11:48,880
sort of world. Um and we ask subjects

293
00:11:46,560 --> 00:11:50,399
how morally permissible was it to do

294
00:11:48,880 --> 00:11:52,800
that thing that that person did which is

295
00:11:50,399 --> 00:11:56,560
always a case of getting out of line.

296
00:11:52,800 --> 00:11:59,440
And what we can then do is model what

297
00:11:56,560 --> 00:12:01,279
exactly would the world be like if

298
00:11:59,440 --> 00:12:03,760
everybody gets out of line. So we have

299
00:12:01,279 --> 00:12:06,560
this kind of agent-based simulation

300
00:12:03,760 --> 00:12:09,120
where each of these people pretends that

301
00:12:06,560 --> 00:12:10,959
no rule exists and just plans directly

302
00:12:09,120 --> 00:12:13,040
towards the water source. And you can

303
00:12:10,959 --> 00:12:15,040
see just as we anticipated everything

304
00:12:13,040 --> 00:12:18,560
goes quite smoothly in this case and

305
00:12:15,040 --> 00:12:20,720
things go badly in in other cases. Um so

306
00:12:18,560 --> 00:12:22,760
then what we can ultimately do is build

307
00:12:20,720 --> 00:12:25,120
a model that includes this

308
00:12:22,760 --> 00:12:27,279
universalizability feature as well as

309
00:12:25,120 --> 00:12:29,440
various other um features that you might

310
00:12:27,279 --> 00:12:32,480
think matter for whether or not um this

311
00:12:29,440 --> 00:12:35,279
breaking this rule is acceptable. Um and

312
00:12:32,480 --> 00:12:38,160
ultimately we find that this is a good

313
00:12:35,279 --> 00:12:40,959
way of understanding um how people are

314
00:12:38,160 --> 00:12:44,079
making judgments in cases like this.

315
00:12:40,959 --> 00:12:46,160
Okay. So here's one way that it seems

316
00:12:44,079 --> 00:12:48,720
like people figure out how to break

317
00:12:46,160 --> 00:12:50,079
rules. they think about what would

318
00:12:48,720 --> 00:12:52,480
happen in this in this quite

319
00:12:50,079 --> 00:12:54,200
sophisticated way if everybody broke the

320
00:12:52,480 --> 00:12:58,720
rule in that

321
00:12:54,200 --> 00:13:01,200
case. Okay. Um so I'll just tell you a

322
00:12:58,720 --> 00:13:05,079
couple of more directions um that we're

323
00:13:01,200 --> 00:13:05,079
taking with this project.

324
00:13:09,600 --> 00:13:17,639
um one is to think about um exactly what

325
00:13:13,760 --> 00:13:22,399
people mean when they communicate simple

326
00:13:17,639 --> 00:13:24,720
rules. So for example um think about the

327
00:13:22,399 --> 00:13:27,519
a park um where you might see a sign

328
00:13:24,720 --> 00:13:29,279
that says don't walk on the grass and if

329
00:13:27,519 --> 00:13:32,560
you're trying to figure out what that

330
00:13:29,279 --> 00:13:34,480
rule means um similar to the case of

331
00:13:32,560 --> 00:13:36,480
don't stand in line what does that rule

332
00:13:34,480 --> 00:13:39,120
mean? You might think it just means

333
00:13:36,480 --> 00:13:42,480
something like all policies that include

334
00:13:39,120 --> 00:13:46,880
steps on the grass are prohibited. Um

335
00:13:42,480 --> 00:13:48,959
but in fact, as as you'll see, um there

336
00:13:46,880 --> 00:13:52,720
are lots of cases where actually it

337
00:13:48,959 --> 00:13:55,440
might be okay to break the rule. Um so

338
00:13:52,720 --> 00:13:57,120
while this path might be one that is

339
00:13:55,440 --> 00:13:58,160
perfectly acceptable, also this path

340
00:13:57,120 --> 00:14:00,320
might be one that's perfectly

341
00:13:58,160 --> 00:14:02,959
acceptable. This being a hospital for

342
00:14:00,320 --> 00:14:05,440
instance. Um, on the other hand, this

343
00:14:02,959 --> 00:14:07,839
might not be um acceptable for somebody

344
00:14:05,440 --> 00:14:11,279
to cut across the grass specifically to

345
00:14:07,839 --> 00:14:13,040
go to um a coffee shop. And this might

346
00:14:11,279 --> 00:14:14,639
not be acceptable either. This person is

347
00:14:13,040 --> 00:14:16,320
headed to the hospital, but it seems

348
00:14:14,639 --> 00:14:17,639
like maybe they actually don't need to

349
00:14:16,320 --> 00:14:21,680
get there that

350
00:14:17,639 --> 00:14:23,760
quickly. Um, what if the person um has a

351
00:14:21,680 --> 00:14:25,440
pogo stick and takes this path to get to

352
00:14:23,760 --> 00:14:28,320
the hospital? Well, they're not walking

353
00:14:25,440 --> 00:14:30,959
on the grass, but it seems obvious that

354
00:14:28,320 --> 00:14:33,440
that shouldn't be permitted either. What

355
00:14:30,959 --> 00:14:35,600
if your dog has run onto the grass and

356
00:14:33,440 --> 00:14:37,680
what you're doing is running over there

357
00:14:35,600 --> 00:14:39,519
to try to get stop them from digging a

358
00:14:37,680 --> 00:14:41,440
hole in the grass? What if you're a

359
00:14:39,519 --> 00:14:44,399
gardener and you take this path across

360
00:14:41,440 --> 00:14:46,320
the grass? So, all of these examples are

361
00:14:44,399 --> 00:14:48,959
simply to get you to realize that when

362
00:14:46,320 --> 00:14:51,120
we communicate a simple rule like don't

363
00:14:48,959 --> 00:14:54,160
walk on the grass, you can actually

364
00:14:51,120 --> 00:14:56,880
recover much more communicative meaning

365
00:14:54,160 --> 00:14:58,480
than is simply articulated. So, one of

366
00:14:56,880 --> 00:15:01,279
the one of the things that we're

367
00:14:58,480 --> 00:15:04,959
positing this rule actually means is

368
00:15:01,279 --> 00:15:08,560
something like, okay, there was somebody

369
00:15:04,959 --> 00:15:10,880
who simulated a bargain of everybody in

370
00:15:08,560 --> 00:15:13,440
the community who who brought their

371
00:15:10,880 --> 00:15:15,920
needs to the table. And what they all

372
00:15:13,440 --> 00:15:18,320
decided together is that sure, each of

373
00:15:15,920 --> 00:15:19,920
us individually would benefit if we

374
00:15:18,320 --> 00:15:21,279
managed to cut across the grass to get

375
00:15:19,920 --> 00:15:23,440
wherever we were going a little bit

376
00:15:21,279 --> 00:15:25,360
quicker. But if that were to happen,

377
00:15:23,440 --> 00:15:27,680
then we would all lose the grass. the

378
00:15:25,360 --> 00:15:30,399
grass would die and that would yield

379
00:15:27,680 --> 00:15:32,399
this collective harm that is more

380
00:15:30,399 --> 00:15:34,480
important than the benefit we would each

381
00:15:32,399 --> 00:15:37,199
individually derive. And so what we're

382
00:15:34,480 --> 00:15:39,680
arguing is all of that can be understood

383
00:15:37,199 --> 00:15:41,760
by simply reading this sign don't walk

384
00:15:39,680 --> 00:15:45,040
across the grass. And then what you can

385
00:15:41,760 --> 00:15:47,680
do is use that simulated model to then

386
00:15:45,040 --> 00:15:50,560
go and predict in these kinds of edge

387
00:15:47,680 --> 00:15:52,240
cases or unusual cases whether or not

388
00:15:50,560 --> 00:15:54,160
it's actually permitted for you to walk

389
00:15:52,240 --> 00:15:56,720
across the grass in these in this case

390
00:15:54,160 --> 00:15:58,639
given your simulation of what everybody

391
00:15:56,720 --> 00:16:00,880
would agree to if they could talk about

392
00:15:58,639 --> 00:16:02,639
it. Um so I'm not going to go go into

393
00:16:00,880 --> 00:16:04,000
the specific like ways we test that and

394
00:16:02,639 --> 00:16:05,839
the models we build and the data we

395
00:16:04,000 --> 00:16:07,199
collect. Um but that's one of the the

396
00:16:05,839 --> 00:16:10,800
kind of things that we're thinking about

397
00:16:07,199 --> 00:16:12,560
right now. Okay. There's another um

398
00:16:10,800 --> 00:16:14,399
couple of really exciting directions

399
00:16:12,560 --> 00:16:18,160
here which I'll just touch on very

400
00:16:14,399 --> 00:16:20,480
briefly. Um you'll notice that all of

401
00:16:18,160 --> 00:16:24,639
the the studies that I've um talked to

402
00:16:20,480 --> 00:16:26,959
talked about up until now um are done

403
00:16:24,639 --> 00:16:31,040
with um in a very kind of western

404
00:16:26,959 --> 00:16:33,680
context. So our subjects are um people

405
00:16:31,040 --> 00:16:36,560
who live in the United States probably

406
00:16:33,680 --> 00:16:39,279
think a lot like I think. Um, but what

407
00:16:36,560 --> 00:16:41,120
we want to do here, especially if we're

408
00:16:39,279 --> 00:16:44,000
trying to use this model of moral

409
00:16:41,120 --> 00:16:47,279
cognition as a guide for AI alignment,

410
00:16:44,000 --> 00:16:49,519
is to ensure that what we're studying is

411
00:16:47,279 --> 00:16:52,000
something that's true of the human mind

412
00:16:49,519 --> 00:16:54,120
and not just true of these particular

413
00:16:52,000 --> 00:16:57,600
participants that we happen to talk to.

414
00:16:54,120 --> 00:17:00,160
Um, so we have an awesome collaboration

415
00:16:57,600 --> 00:17:03,759
going um with Nori Jacobe at Cornell who

416
00:17:00,160 --> 00:17:08,799
has this pipeline where um he can

417
00:17:03,759 --> 00:17:10,799
translate a um a set of stimuli into 80

418
00:17:08,799 --> 00:17:12,319
different languages and run it in 60

419
00:17:10,799 --> 00:17:13,760
different countries. So that's what

420
00:17:12,319 --> 00:17:16,079
we're getting started on now. We're

421
00:17:13,760 --> 00:17:20,160
using um this paradigm of water

422
00:17:16,079 --> 00:17:22,559
gathering. Um, and when you kind of run

423
00:17:20,160 --> 00:17:24,640
a a study like this cross-culturally,

424
00:17:22,559 --> 00:17:26,160
you need to like really understand that

425
00:17:24,640 --> 00:17:28,559
everybody understands everything that's

426
00:17:26,160 --> 00:17:31,440
going on in the scenario. So, there's a

427
00:17:28,559 --> 00:17:34,080
ton of checks to um make sure that that

428
00:17:31,440 --> 00:17:36,559
happens. We translate the instructions

429
00:17:34,080 --> 00:17:39,039
into lots of different languages and we

430
00:17:36,559 --> 00:17:41,280
have various hypotheses about the types

431
00:17:39,039 --> 00:17:44,240
of things that we might see depending on

432
00:17:41,280 --> 00:17:46,559
how people how much people are using the

433
00:17:44,240 --> 00:17:49,039
types of rulebreaking mechanisms that

434
00:17:46,559 --> 00:17:51,280
we've introduced. So we have a set of

435
00:17:49,039 --> 00:17:54,480
pilot data that's really exciting um

436
00:17:51,280 --> 00:17:56,320
just in a handful of languages. But um

437
00:17:54,480 --> 00:17:58,799
one of the main thing that we used the

438
00:17:56,320 --> 00:18:01,039
CIRC funding for was to fund an awesome

439
00:17:58,799 --> 00:18:03,280
student who got this started who has

440
00:18:01,039 --> 00:18:05,919
since moved on to working on AI policy

441
00:18:03,280 --> 00:18:08,320
in Washington. So that's an a awesome

442
00:18:05,919 --> 00:18:10,080
opportunity for him, but we need help

443
00:18:08,320 --> 00:18:13,120
with this project. So we're currently

444
00:18:10,080 --> 00:18:14,960
hiring somebody who wants to um kind of

445
00:18:13,120 --> 00:18:17,360
take up the mantle of this

446
00:18:14,960 --> 00:18:19,520
cross-cultural work. Um so if that might

447
00:18:17,360 --> 00:18:21,520
be you or somebody you know, definitely

448
00:18:19,520 --> 00:18:23,520
um feel free to get in touch. This is an

449
00:18:21,520 --> 00:18:26,559
extremely exciting project that's going

450
00:18:23,520 --> 00:18:29,280
to have, I think, a big impact. Um,

451
00:18:26,559 --> 00:18:33,480
okay. And yeah, with that, I'll take

452
00:18:29,280 --> 00:18:33,480
questions. Thank you.

453
00:18:34,080 --> 00:18:37,640
Questions for Sydney.

454
00:18:39,760 --> 00:18:47,520
The first question that I have is that

455
00:18:42,480 --> 00:18:50,120
lu have a genetic component to it like a

456
00:18:47,520 --> 00:18:52,880
uh there are some rule that can be

457
00:18:50,120 --> 00:18:55,760
inhaled. Uh so that will be a question

458
00:18:52,880 --> 00:18:58,640
they have. Yeah. Does do rules have a

459
00:18:55,760 --> 00:19:01,480
genetic component you ask?

460
00:18:58,640 --> 00:19:06,039
Yeah interesting question.

461
00:19:01,480 --> 00:19:08,440
Um really interesting question. Um,

462
00:19:06,039 --> 00:19:10,960
so, oh gosh.

463
00:19:08,440 --> 00:19:12,799
Um, yeah, I mean, in some ways this is

464
00:19:10,960 --> 00:19:14,880
like one of the most foundational

465
00:19:12,799 --> 00:19:17,799
questions of cognitive science, like

466
00:19:14,880 --> 00:19:21,880
which parts of this sort of moral

467
00:19:17,799 --> 00:19:24,559
cognition are um kind of embedded in our

468
00:19:21,880 --> 00:19:27,440
minds in the architecture of our minds

469
00:19:24,559 --> 00:19:29,120
and which are learned socially. Um, and

470
00:19:27,440 --> 00:19:32,240
that's one of the things that we aim to

471
00:19:29,120 --> 00:19:34,640
test with this cross-cultural study. um

472
00:19:32,240 --> 00:19:37,280
are we going to see broad patterns that

473
00:19:34,640 --> 00:19:39,679
look similar across cultures or do some

474
00:19:37,280 --> 00:19:41,200
cultures just do a very different thing?

475
00:19:39,679 --> 00:19:43,840
So the particular way you phrased your

476
00:19:41,200 --> 00:19:45,840
question was are rules embedded in the

477
00:19:43,840 --> 00:19:48,080
mind? Um and that'll depend a bit on

478
00:19:45,840 --> 00:19:50,880
what you mean by a rule. So the thing

479
00:19:48,080 --> 00:19:53,840
that we hope to find cross-culturally is

480
00:19:50,880 --> 00:19:56,000
this mechanism of interpreting rules. Um

481
00:19:53,840 --> 00:19:58,320
which is something like universalization

482
00:19:56,000 --> 00:20:03,360
or a certain kind of like bargaining

483
00:19:58,320 --> 00:20:05,840
simulation. Um though what I imagine we

484
00:20:03,360 --> 00:20:09,360
might find is that those mechanisms are

485
00:20:05,840 --> 00:20:13,039
used to apply to very different rules

486
00:20:09,360 --> 00:20:15,360
that are designed um in in in different

487
00:20:13,039 --> 00:20:18,160
cultures based on the context that they

488
00:20:15,360 --> 00:20:20,720
emerge from. So I wouldn't be surprised

489
00:20:18,160 --> 00:20:22,880
if we find rule similarity but simply

490
00:20:20,720 --> 00:20:24,880
because environments are similar in lots

491
00:20:22,880 --> 00:20:27,280
of places. But what I hope to find

492
00:20:24,880 --> 00:20:29,600
cross-culturally is the same mechanism

493
00:20:27,280 --> 00:20:30,640
of rule interpretation potentially. I

494
00:20:29,600 --> 00:20:33,760
don't know. This is an empirical

495
00:20:30,640 --> 00:20:36,400
question. Yeah.

496
00:20:33,760 --> 00:20:38,799
Um, hi. Thanks. Um, I wondered if

497
00:20:36,400 --> 00:20:40,799
there's a sort of temporal temporal

498
00:20:38,799 --> 00:20:43,200
component to add to this. So, when do

499
00:20:40,799 --> 00:20:45,679
you jaywalk or something? So it isn't

500
00:20:43,200 --> 00:20:49,280
just your assessment now, but it's like

501
00:20:45,679 --> 00:20:51,120
the rules are set to sort of be absolute

502
00:20:49,280 --> 00:20:53,200
and yet people are making judgments

503
00:20:51,120 --> 00:20:55,360
about under certain circumstances I'll

504
00:20:53,200 --> 00:20:57,840
jaywalk, under other circumstances I

505
00:20:55,360 --> 00:20:59,440
won't. And how you sort of fit that part

506
00:20:57,840 --> 00:21:03,159
in.

507
00:20:59,440 --> 00:21:03,159
Yeah. Um

508
00:21:03,440 --> 00:21:08,240
yeah, exactly.

509
00:21:05,480 --> 00:21:11,280
Um, so figuring I think like one of the

510
00:21:08,240 --> 00:21:14,960
critical questions is to figure out what

511
00:21:11,280 --> 00:21:16,360
counts as a morally relevant feature. So

512
00:21:14,960 --> 00:21:19,520
when you say

513
00:21:16,360 --> 00:21:23,280
um like time might matter, you probably

514
00:21:19,520 --> 00:21:25,520
don't mean um is it like 12:00 or 12:01

515
00:21:23,280 --> 00:21:26,640
or is it Tuesday or Thursday unless for

516
00:21:25,520 --> 00:21:28,240
some reason that picks out something

517
00:21:26,640 --> 00:21:29,760
important. Maybe the traffic laws are

518
00:21:28,240 --> 00:21:32,400
one way on Tuesday and a different law

519
00:21:29,760 --> 00:21:34,640
on Thursday. So what matters is figuring

520
00:21:32,400 --> 00:21:36,480
out what is what is a morally relevant

521
00:21:34,640 --> 00:21:38,159
feature of the context. How do you

522
00:21:36,480 --> 00:21:40,640
represent that feature and how do you

523
00:21:38,159 --> 00:21:43,600
then compute over it in order to get the

524
00:21:40,640 --> 00:21:46,320
right answer. Did did I get your

525
00:21:43,600 --> 00:21:49,320
question? Did I understand?

526
00:21:46,320 --> 00:21:49,320
Yeah.

527
00:21:55,480 --> 00:22:00,039
Uhhuh. Okay. Interesting.

528
00:22:09,200 --> 00:22:16,480
Oh, okay. Okay. Yes. Okay. So, um

529
00:22:13,880 --> 00:22:19,280
yeah. Okay. Yeah. So, that that's like

530
00:22:16,480 --> 00:22:23,760
pretty critical to what we're thinking

531
00:22:19,280 --> 00:22:26,880
about. So, um Yeah. Okay. So when rules

532
00:22:23,760 --> 00:22:28,640
are are codified, they tend to like rule

533
00:22:26,880 --> 00:22:31,760
makers tend to have particular

534
00:22:28,640 --> 00:22:33,600
circumstances in mind. Um so for

535
00:22:31,760 --> 00:22:35,919
instance, like somebody who's putting up

536
00:22:33,600 --> 00:22:38,080
a sign that says no walking on the

537
00:22:35,919 --> 00:22:39,280
grass, they might do that when they know

538
00:22:38,080 --> 00:22:40,320
certain things about the community,

539
00:22:39,280 --> 00:22:42,159
certain things about the health of the

540
00:22:40,320 --> 00:22:43,520
grass and the sustainability of the

541
00:22:42,159 --> 00:22:47,120
grass and what the weather is likely to

542
00:22:43,520 --> 00:22:49,120
be, etc. But um if those things change,

543
00:22:47,120 --> 00:22:51,200
then the applicability of the rule might

544
00:22:49,120 --> 00:22:53,600
change. And as a person comes along,

545
00:22:51,200 --> 00:22:55,520
reads the rule and understands the

546
00:22:53,600 --> 00:22:57,760
context and the circumstances, they

547
00:22:55,520 --> 00:23:00,000
might realize the rule didn't anticipate

548
00:22:57,760 --> 00:23:01,679
my circumstance. And so I need to then

549
00:23:00,000 --> 00:23:04,480
figure out what was the rule trying to

550
00:23:01,679 --> 00:23:06,720
communicate and what should I do in this

551
00:23:04,480 --> 00:23:08,799
case given my update about the context

552
00:23:06,720 --> 00:23:10,720
and the relevant information. Yeah.

553
00:23:08,799 --> 00:23:12,480
Yeah, that's exactly right. Awesome.

554
00:23:10,720 --> 00:23:13,840
Well, I'm afraid with apologies to the

555
00:23:12,480 --> 00:23:15,679
remaining questioners, we have to move

556
00:23:13,840 --> 00:23:17,120
on. That's fascinating stuff. And maybe

557
00:23:15,679 --> 00:23:18,720
you'll get an opportunity to ask Sydney

558
00:23:17,120 --> 00:23:23,400
the questions sort of offline. Sorry

559
00:23:18,720 --> 00:23:23,400
about that. Um, great. Thank you.

