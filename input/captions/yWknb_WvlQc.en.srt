1
00:00:00,080 --> 00:00:05,520
Uh welcome uh to the first Schmidt

2
00:00:03,200 --> 00:00:07,919
Center colloquium of this fall. Uh it's

3
00:00:05,520 --> 00:00:10,559
jointly hosted by the Eric and Wendy

4
00:00:07,919 --> 00:00:13,440
Schmidt Center at the Broad and ECS at

5
00:00:10,559 --> 00:00:15,280
MIT. Uh in the series we feature

6
00:00:13,440 --> 00:00:18,880
speakers whose work shows how machine

7
00:00:15,280 --> 00:00:21,760
learning uh is driving insights into

8
00:00:18,880 --> 00:00:25,119
important uh questions uh in biomedical

9
00:00:21,760 --> 00:00:28,640
sciences and equivalently how questions

10
00:00:25,119 --> 00:00:30,720
in biomedical sciences uh is spurring uh

11
00:00:28,640 --> 00:00:32,800
foundational advances in machine

12
00:00:30,720 --> 00:00:34,320
learning and we're equally adv excited

13
00:00:32,800 --> 00:00:37,040
about both directions. So we're really

14
00:00:34,320 --> 00:00:39,520
excited to have uh Katherine Heler with

15
00:00:37,040 --> 00:00:41,360
us today. Katherine is a research

16
00:00:39,520 --> 00:00:44,640
scientist at Google where she leads the

17
00:00:41,360 --> 00:00:47,200
context in AI research or care team. Her

18
00:00:44,640 --> 00:00:49,520
work focuses on identifying robustness

19
00:00:47,200 --> 00:00:51,680
and fairness issues and medical AI and

20
00:00:49,520 --> 00:00:53,520
creative technologies. She's made a

21
00:00:51,680 --> 00:00:54,480
remarkable real world impact. Katherine,

22
00:00:53,520 --> 00:00:55,840
I was reading about this. It's

23
00:00:54,480 --> 00:00:57,760
incredible that you've developed and

24
00:00:55,840 --> 00:00:59,600
integrated multiple machine learning

25
00:00:57,760 --> 00:01:01,920
systems into hospitals and clinical

26
00:00:59,600 --> 00:01:04,000
care. That's that is that is hard to do.

27
00:01:01,920 --> 00:01:06,000
These include uh sepsis detection

28
00:01:04,000 --> 00:01:08,479
systems now running in Duke University

29
00:01:06,000 --> 00:01:10,240
Hospital's emergency department. Uh a

30
00:01:08,479 --> 00:01:12,880
system that predicts uh surgical

31
00:01:10,240 --> 00:01:15,920
complications and a nationally released

32
00:01:12,880 --> 00:01:17,600
uh mobile app on multiple sclerosis.

33
00:01:15,920 --> 00:01:20,640
Throughout this work, she is passionate

34
00:01:17,600 --> 00:01:22,320
about ensuring that everyone is included

35
00:01:20,640 --> 00:01:24,400
um in the development of medical eye

36
00:01:22,320 --> 00:01:27,040
technology and I'm sure we'll hear a lot

37
00:01:24,400 --> 00:01:28,880
about that today. Uh before Google, um

38
00:01:27,040 --> 00:01:31,360
Katherine held appointments across

39
00:01:28,880 --> 00:01:33,600
multiple departments at Duke. uh and

40
00:01:31,360 --> 00:01:36,000
this is quite a list uh statistical

41
00:01:33,600 --> 00:01:37,600
science, neurobiology, neurology,

42
00:01:36,000 --> 00:01:39,200
computer science and electrical and

43
00:01:37,600 --> 00:01:41,439
computer engineering. She's been

44
00:01:39,200 --> 00:01:43,360
recognized with the NSF with an NSF

45
00:01:41,439 --> 00:01:45,840
career award and was a first round

46
00:01:43,360 --> 00:01:47,520
recipient of the brain initiative award.

47
00:01:45,840 --> 00:01:48,799
Katherine received her bachelor's in

48
00:01:47,520 --> 00:01:52,399
computer science and applied math at

49
00:01:48,799 --> 00:01:56,240
Sunny Stonybrook MS in CS from Colombia,

50
00:01:52,399 --> 00:01:59,040
PhD um in neuroscience unit at UCL and

51
00:01:56,240 --> 00:02:00,640
she did her postto EPSRC posttock in

52
00:01:59,040 --> 00:02:04,000
engineering at Cambridge and then was

53
00:02:00,640 --> 00:02:06,320
here um across the road uh doing her NSF

54
00:02:04,000 --> 00:02:09,280
posttock in the brain and cognitive

55
00:02:06,320 --> 00:02:11,039
sciences uh department. So we are really

56
00:02:09,280 --> 00:02:12,000
excited uh to have you here Katherine.

57
00:02:11,039 --> 00:02:15,480
Take it away.

58
00:02:12,000 --> 00:02:15,480
>> Thanks so much.

59
00:02:16,160 --> 00:02:20,959
Yeah, I'm I'm watching I'm watching this

60
00:02:18,239 --> 00:02:23,040
video like all all my friends, some of

61
00:02:20,959 --> 00:02:24,879
them I haven't seen since grad school,

62
00:02:23,040 --> 00:02:26,720
like who are coming to talk and I'm so

63
00:02:24,879 --> 00:02:29,120
envious of you guys getting to see them

64
00:02:26,720 --> 00:02:31,599
and finding out what they're up to right

65
00:02:29,120 --> 00:02:34,640
now. So, um this is great. Thank you so

66
00:02:31,599 --> 00:02:37,680
much for inviting me. Um okay, so I'm

67
00:02:34,640 --> 00:02:39,200
Katherine. Um and I'll be talking about

68
00:02:37,680 --> 00:02:41,519
um some of the work that we're doing

69
00:02:39,200 --> 00:02:43,599
right now at Google. I know I'm I'm here

70
00:02:41,519 --> 00:02:45,760
to answer people's questions on sort of

71
00:02:43,599 --> 00:02:47,920
like the academia industry sort of

72
00:02:45,760 --> 00:02:50,879
spectrum because I've I've experienced

73
00:02:47,920 --> 00:02:52,959
both sides of that division. Um but I'm

74
00:02:50,879 --> 00:02:57,120
going to be talking about our industry

75
00:02:52,959 --> 00:02:59,360
industry related work now. Um and uh our

76
00:02:57,120 --> 00:03:02,560
team is called context and AI research.

77
00:02:59,360 --> 00:03:04,959
Um, and so I want to talk about what

78
00:03:02,560 --> 00:03:07,760
they've been up to, um, as well as doing

79
00:03:04,959 --> 00:03:10,159
human- centered AI research, right? Um,

80
00:03:07,760 --> 00:03:11,920
in a healthcare context specifically,

81
00:03:10,159 --> 00:03:14,560
um, at a time when we're talking a lot

82
00:03:11,920 --> 00:03:16,800
about general purpose AI and how to get

83
00:03:14,560 --> 00:03:18,400
general purpose AI off the ground in

84
00:03:16,800 --> 00:03:22,800
industry.

85
00:03:18,400 --> 00:03:24,879
Okay. Um, so at a at a very high level,

86
00:03:22,800 --> 00:03:26,720
um, this relates to and maybe this is

87
00:03:24,879 --> 00:03:28,959
this is more of an industry thing. I

88
00:03:26,720 --> 00:03:31,200
don't know, but I'm I'm going to hope,

89
00:03:28,959 --> 00:03:33,599
right? Um, but like at a high level,

90
00:03:31,200 --> 00:03:36,319
this relates to an ICML paper that we

91
00:03:33,599 --> 00:03:39,120
just had on um, artificial general

92
00:03:36,319 --> 00:03:42,239
intelligence. Um, and the it was a

93
00:03:39,120 --> 00:03:44,959
position paper this year at ICML and we

94
00:03:42,239 --> 00:03:48,000
asked in it the community to stop using

95
00:03:44,959 --> 00:03:50,640
uh, AGI um, as the northstar of AI

96
00:03:48,000 --> 00:03:53,599
research. And so we we talked about um

97
00:03:50,640 --> 00:03:56,480
many of the ways in which AGI enhances

98
00:03:53,599 --> 00:03:58,239
known issues um via what we call traps.

99
00:03:56,480 --> 00:04:00,000
And I'm not going to go into great

100
00:03:58,239 --> 00:04:03,280
detail about it, but but they're all

101
00:04:00,000 --> 00:04:05,519
there for you to see. And so a lot of it

102
00:04:03,280 --> 00:04:09,200
comes down to a lack of definition of

103
00:04:05,519 --> 00:04:11,040
what AGI is um and how it obscures uh

104
00:04:09,200 --> 00:04:15,360
worthwhile scientific engineering and

105
00:04:11,040 --> 00:04:17,120
societal goals. Um and um our work

106
00:04:15,360 --> 00:04:19,680
really advocates for specific

107
00:04:17,120 --> 00:04:21,680
definitions and metrics um and

108
00:04:19,680 --> 00:04:23,600
particularly those that are inclusive of

109
00:04:21,680 --> 00:04:26,160
diversity of experience and knowledge

110
00:04:23,600 --> 00:04:27,600
that people have. Um so that's what

111
00:04:26,160 --> 00:04:30,880
you're going to see from a lot of this

112
00:04:27,600 --> 00:04:33,120
talk. Um I'm going to focus

113
00:04:30,880 --> 00:04:34,880
on a healthcare context, right? So

114
00:04:33,120 --> 00:04:36,880
everything that I'm gonna going to talk

115
00:04:34,880 --> 00:04:39,199
about is related to health but but

116
00:04:36,880 --> 00:04:42,240
always with specific definitions and

117
00:04:39,199 --> 00:04:44,560
metrics in mind.

118
00:04:42,240 --> 00:04:47,840
Okay, so this is this is our team. This

119
00:04:44,560 --> 00:04:49,680
is the care team um at Google. Um we're

120
00:04:47,840 --> 00:04:51,759
a group of people who are dedicated to

121
00:04:49,680 --> 00:04:53,680
developing and evaluating AI systems

122
00:04:51,759 --> 00:04:56,240
that are grounded in real world contexts

123
00:04:53,680 --> 00:04:58,160
and use cases um with a focus on

124
00:04:56,240 --> 00:05:00,960
solutions that promote equitable

125
00:04:58,160 --> 00:05:03,919
outcomes. Having equitable outcomes is

126
00:05:00,960 --> 00:05:05,680
really really important to us um and

127
00:05:03,919 --> 00:05:07,680
really important to the sort of the

128
00:05:05,680 --> 00:05:10,240
broader team that that we're in at

129
00:05:07,680 --> 00:05:13,759
Google. um outreach and inclusion are

130
00:05:10,240 --> 00:05:15,680
central to our work um and um and

131
00:05:13,759 --> 00:05:17,440
everything that we do. And here I'm

132
00:05:15,680 --> 00:05:21,479
going to talk about again about the

133
00:05:17,440 --> 00:05:21,479
healthcare context specifically.

134
00:05:21,680 --> 00:05:25,759
Okay. So I'm I'm going to start with

135
00:05:23,440 --> 00:05:29,039
causal modeling specifically um because

136
00:05:25,759 --> 00:05:30,960
it's it's it's pretty central to uh

137
00:05:29,039 --> 00:05:34,880
formalizing our questions and concerns

138
00:05:30,960 --> 00:05:36,800
and and their solutions. Um, we can use

139
00:05:34,880 --> 00:05:39,600
causal modeling to design valid

140
00:05:36,800 --> 00:05:41,600
evaluation procedures. Um, which we can

141
00:05:39,600 --> 00:05:43,840
use with a lot of technology that's

142
00:05:41,600 --> 00:05:47,520
currently being developed in in AI

143
00:05:43,840 --> 00:05:50,320
fields. Um, and and these evaluation uh

144
00:05:47,520 --> 00:05:52,560
evaluation procedures that that the ones

145
00:05:50,320 --> 00:05:54,560
that are used currently are too often

146
00:05:52,560 --> 00:05:57,840
constructed in an ineffective or harmful

147
00:05:54,560 --> 00:06:00,639
way. Causal modeling provides a way to

148
00:05:57,840 --> 00:06:02,800
formally incorporate important domain

149
00:06:00,639 --> 00:06:05,360
specific knowledge um which is very

150
00:06:02,800 --> 00:06:07,440
important in a health domain um to

151
00:06:05,360 --> 00:06:10,080
ensure synergy between the predictive

152
00:06:07,440 --> 00:06:12,319
task studied in the deployment context

153
00:06:10,080 --> 00:06:18,039
and to more deeply understand data

154
00:06:12,319 --> 00:06:18,039
biases um like label or selection bias.

155
00:06:20,319 --> 00:06:23,360
An important part of this work is

156
00:06:21,840 --> 00:06:25,280
understanding how to design machine

157
00:06:23,360 --> 00:06:27,440
learning systems that are appropriately

158
00:06:25,280 --> 00:06:30,000
grounded in the context in which they're

159
00:06:27,440 --> 00:06:32,639
intended to be used. So this means

160
00:06:30,000 --> 00:06:36,000
clearly defining the intended use case

161
00:06:32,639 --> 00:06:38,000
and the target population. Um later

162
00:06:36,000 --> 00:06:40,479
we're going to see a classifier that

163
00:06:38,000 --> 00:06:44,800
identifies a dermatological disease and

164
00:06:40,479 --> 00:06:47,120
images. Um and so for this for example,

165
00:06:44,800 --> 00:06:49,199
we'd want to define how that model is

166
00:06:47,120 --> 00:06:51,600
going to be used in a clinical workflow.

167
00:06:49,199 --> 00:06:53,759
what the clinical setting is, for which

168
00:06:51,600 --> 00:06:57,199
populations it's intended to be used

169
00:06:53,759 --> 00:06:59,280
for. Um, ideally, we can use this sort

170
00:06:57,199 --> 00:07:01,440
of reasoning to develop contextualized

171
00:06:59,280 --> 00:07:05,400
statements about the downstream impacts

172
00:07:01,440 --> 00:07:05,400
of the models that we develop.

173
00:07:07,919 --> 00:07:12,240
A key issue that we need to address is

174
00:07:10,080 --> 00:07:14,400
that the data that's used for model

175
00:07:12,240 --> 00:07:17,199
development and evaluation reflects

176
00:07:14,400 --> 00:07:18,960
complex societal context which results

177
00:07:17,199 --> 00:07:21,759
in differences in health outcomes,

178
00:07:18,960 --> 00:07:24,639
access, and quality. So this introduces

179
00:07:21,759 --> 00:07:26,639
issues like selection bias because data

180
00:07:24,639 --> 00:07:29,360
sets will be less representative of

181
00:07:26,639 --> 00:07:32,080
populations with worse access to care.

182
00:07:29,360 --> 00:07:35,599
And if utilization of health care for

183
00:07:32,080 --> 00:07:37,120
example is used as a product proxy for

184
00:07:35,599 --> 00:07:39,440
uh the need for health care as we've

185
00:07:37,120 --> 00:07:41,759
seen in a lot of different papers this

186
00:07:39,440 --> 00:07:43,680
is going to result in a worse proxy for

187
00:07:41,759 --> 00:07:45,599
patients with poorer access to care. So

188
00:07:43,680 --> 00:07:47,759
we see this all the time in terms of

189
00:07:45,599 --> 00:07:49,599
like we're deciding on who needs this

190
00:07:47,759 --> 00:07:51,599
healthcare by looking at who's used it

191
00:07:49,599 --> 00:07:55,720
in the past and all kinds of bias

192
00:07:51,599 --> 00:07:55,720
related issues come into play

193
00:07:57,440 --> 00:08:01,759
here. So we're going to focus on issues

194
00:07:58,879 --> 00:08:04,240
that arise uh with basic evaluations of

195
00:08:01,759 --> 00:08:06,240
model performance across subgroups. So

196
00:08:04,240 --> 00:08:07,919
we discuss

197
00:08:06,240 --> 00:08:10,319
where we're interested in evaluating

198
00:08:07,919 --> 00:08:12,720
performance across subgroups defined by

199
00:08:10,319 --> 00:08:14,800
attributes like race or gender. Um but

200
00:08:12,720 --> 00:08:16,879
these can apply to any any kinds of

201
00:08:14,800 --> 00:08:18,479
related context. So it can be different

202
00:08:16,879 --> 00:08:20,800
races, different genders, but it can

203
00:08:18,479 --> 00:08:22,560
also be different socioeconomic groups.

204
00:08:20,800 --> 00:08:24,560
It can be living in different places in

205
00:08:22,560 --> 00:08:27,039
the world, that kind of thing.

206
00:08:24,560 --> 00:08:29,599
Um even in the best case where we have

207
00:08:27,039 --> 00:08:31,599
representative data uh comparison of

208
00:08:29,599 --> 00:08:33,599
model performance across subgroups can

209
00:08:31,599 --> 00:08:36,399
be misleading because the optimal

210
00:08:33,599 --> 00:08:39,839
performance tends to differ across

211
00:08:36,399 --> 00:08:41,680
subgroups um and can be explained by

212
00:08:39,839 --> 00:08:43,919
differences in data distribution across

213
00:08:41,680 --> 00:08:45,839
those subgroups. So I'll explain more

214
00:08:43,919 --> 00:08:48,560
about what this actually means in a

215
00:08:45,839 --> 00:08:51,200
second. Um but we can consider this to

216
00:08:48,560 --> 00:08:54,200
be a p false positive fairness

217
00:08:51,200 --> 00:08:54,200
violation.

218
00:08:54,560 --> 00:09:00,399
Okay, so here we look at some of the

219
00:08:57,200 --> 00:09:02,240
most common classes of fairness criteria

220
00:09:00,399 --> 00:09:05,920
that include demographic parity and

221
00:09:02,240 --> 00:09:07,360
equalist odds. Uh dem demographic par

222
00:09:05,920 --> 00:09:09,839
asks that model predictions be

223
00:09:07,360 --> 00:09:12,320
independent of group membership. Um and

224
00:09:09,839 --> 00:09:15,360
equalist odds ask that demographic par

225
00:09:12,320 --> 00:09:17,519
hold conditioned on the outcome. Um this

226
00:09:15,360 --> 00:09:19,760
is equivalent to requiring that the true

227
00:09:17,519 --> 00:09:22,560
positive or false positive error rates

228
00:09:19,760 --> 00:09:25,920
not differ across groups.

229
00:09:22,560 --> 00:09:28,480
Um, so again, like how well we're doing

230
00:09:25,920 --> 00:09:31,600
should be the same regardless of whether

231
00:09:28,480 --> 00:09:35,440
we're black or white or male or female

232
00:09:31,600 --> 00:09:38,160
or rich or poor or from India or

233
00:09:35,440 --> 00:09:40,959
Pakistan, right? Like it should just all

234
00:09:38,160 --> 00:09:44,399
it should just all those shouldn't be

235
00:09:40,959 --> 00:09:46,399
factors. Um, okay. So there are

236
00:09:44,399 --> 00:09:48,160
variations of both of these criterias

237
00:09:46,399 --> 00:09:51,760
that are defined for the continuous

238
00:09:48,160 --> 00:09:54,240
valued score s uh for and for the

239
00:09:51,760 --> 00:09:57,680
prediction uh the threshold predictor

240
00:09:54,240 --> 00:09:59,440
yhat. Um we can see

241
00:09:57,680 --> 00:10:01,200
commonly used evaluation interpret

242
00:09:59,440 --> 00:10:03,360
interpretations that pop up in the

243
00:10:01,200 --> 00:10:05,839
research around us. So again in people's

244
00:10:03,360 --> 00:10:08,560
papers you'll see a lot of these in some

245
00:10:05,839 --> 00:10:10,640
various form or another. So for example,

246
00:10:08,560 --> 00:10:12,959
matching ROC curves, something that we

247
00:10:10,640 --> 00:10:14,880
see a lot about a lot of in papers,

248
00:10:12,959 --> 00:10:17,120
right? True positive rates, false

249
00:10:14,880 --> 00:10:20,519
positive rates, these can all illuminate

250
00:10:17,120 --> 00:10:20,519
these differences.

251
00:10:21,279 --> 00:10:26,560
Okay, so here's a simulated example. Um

252
00:10:24,079 --> 00:10:29,680
the top left plot introduces the notion

253
00:10:26,560 --> 00:10:31,519
of score distribution. So that's the

254
00:10:29,680 --> 00:10:33,839
distribution of scores that a model

255
00:10:31,519 --> 00:10:36,160
assigns to a population of sub sub or

256
00:10:33,839 --> 00:10:40,000
subgroup. So for example, this could be

257
00:10:36,160 --> 00:10:42,000
like kidney functioning or EGFR. Um for

258
00:10:40,000 --> 00:10:44,079
a given threshold, the classification

259
00:10:42,000 --> 00:10:47,279
rate is the fraction of patients whose

260
00:10:44,079 --> 00:10:49,680
scores exceed the threshold. Um and thus

261
00:10:47,279 --> 00:10:53,440
would be assigned a positive prediction.

262
00:10:49,680 --> 00:10:56,160
So in the plot um the threshold is the

263
00:10:53,440 --> 00:10:58,399
dashed vertical line um and the

264
00:10:56,160 --> 00:10:59,920
classification rate is given by the area

265
00:10:58,399 --> 00:11:02,800
under the curve to the right of the

266
00:10:59,920 --> 00:11:05,120
threshold. The classification rate is

267
00:11:02,800 --> 00:11:09,040
also shown in the bottom plot. It's just

268
00:11:05,120 --> 00:11:10,959
plotted there, right? Um and then

269
00:11:09,040 --> 00:11:13,760
separately in the middle and right

270
00:11:10,959 --> 00:11:16,000
columns, we have um the negative and

271
00:11:13,760 --> 00:11:18,720
positive class respectively, like people

272
00:11:16,000 --> 00:11:20,800
who do or don't have kidney disease. And

273
00:11:18,720 --> 00:11:23,040
so as we slide the threshold from left

274
00:11:20,800 --> 00:11:24,880
to right, the false positive rate in the

275
00:11:23,040 --> 00:11:26,640
middle column goes to zero and the true

276
00:11:24,880 --> 00:11:28,640
positive rate in the right column goes

277
00:11:26,640 --> 00:11:30,320
to zero. uh in keeping with the

278
00:11:28,640 --> 00:11:32,240
underlying distribution, right? We're

279
00:11:30,320 --> 00:11:36,120
classing classifying fewer and fewer

280
00:11:32,240 --> 00:11:36,120
people as having the disease.

281
00:11:36,800 --> 00:11:40,959
So now if we have two distributions,

282
00:11:39,519 --> 00:11:42,880
right, because we have multiple

283
00:11:40,959 --> 00:11:45,760
subgroups, the blue and the brown

284
00:11:42,880 --> 00:11:47,680
distribution, two separate subgroups, um

285
00:11:45,760 --> 00:11:51,360
we can see how we get evaluation

286
00:11:47,680 --> 00:11:54,240
fairness problems. Um the the

287
00:11:51,360 --> 00:11:56,399
distribution in inequality creates gaps.

288
00:11:54,240 --> 00:12:00,480
So you can see those gaps in the bottom

289
00:11:56,399 --> 00:12:03,120
plots. Um a classification rate gap or

290
00:12:00,480 --> 00:12:05,839
an outcome dependent false positive rate

291
00:12:03,120 --> 00:12:08,639
or true positive rate gap signify that

292
00:12:05,839 --> 00:12:10,959
for the same threshold these rates are

293
00:12:08,639 --> 00:12:13,680
different across different groups. And

294
00:12:10,959 --> 00:12:16,000
so what's optimal for one group isn't

295
00:12:13,680 --> 00:12:17,760
necessarily optimal for another group.

296
00:12:16,000 --> 00:12:20,639
Right? We look at the distribution of

297
00:12:17,760 --> 00:12:25,279
scores. We see that we have a gap. We

298
00:12:20,639 --> 00:12:25,279
can identify that and flag it.

299
00:12:26,320 --> 00:12:29,760
So to better understand and model the

300
00:12:28,560 --> 00:12:31,519
different kinds of shifts and

301
00:12:29,760 --> 00:12:34,079
differences that we see, we can use

302
00:12:31,519 --> 00:12:35,600
causal graphical models representing the

303
00:12:34,079 --> 00:12:37,519
kind of independence and conditional

304
00:12:35,600 --> 00:12:40,399
independent structure that we saw in the

305
00:12:37,519 --> 00:12:42,560
table a couple of slides back. Not only

306
00:12:40,399 --> 00:12:44,639
are we capable of formalizing the shifts

307
00:12:42,560 --> 00:12:47,440
given here, but also things like

308
00:12:44,639 --> 00:12:49,360
selection bias, label bias, and other

309
00:12:47,440 --> 00:12:51,839
kinds of confounding. And I'm going to

310
00:12:49,360 --> 00:12:54,880
refer you to the work of Stephen Fo who

311
00:12:51,839 --> 00:12:56,320
is somebody who I work with in order if

312
00:12:54,880 --> 00:12:58,160
you're interested in this line of work

313
00:12:56,320 --> 00:13:01,200
to get more details and to check out

314
00:12:58,160 --> 00:13:03,440
some of their papers.

315
00:13:01,200 --> 00:13:05,760
So one specific care team example of

316
00:13:03,440 --> 00:13:07,760
bias resulting from distribution shift

317
00:13:05,760 --> 00:13:10,880
can be seen in a nerup paper that we had

318
00:13:07,760 --> 00:13:12,800
a couple of years ago. Um assuming that

319
00:13:10,880 --> 00:13:15,600
we develop a fair model, we need to

320
00:13:12,800 --> 00:13:18,240
understand whether fairness transfers um

321
00:13:15,600 --> 00:13:19,519
to the deployment environment. Um, so

322
00:13:18,240 --> 00:13:21,440
all the time machine learning

323
00:13:19,519 --> 00:13:23,440
practitioners run into problems where

324
00:13:21,440 --> 00:13:25,360
it's like they've developed a model,

325
00:13:23,440 --> 00:13:27,519
they've trained and tested in one

326
00:13:25,360 --> 00:13:28,959
particular circumstance and then they

327
00:13:27,519 --> 00:13:30,480
think they've captured a causal

328
00:13:28,959 --> 00:13:32,480
mechanism and so they can easily

329
00:13:30,480 --> 00:13:34,079
transfer to another and then that turns

330
00:13:32,480 --> 00:13:36,160
out not to be the case. And this is a

331
00:13:34,079 --> 00:13:38,720
failure mode for machine learning. Quite

332
00:13:36,160 --> 00:13:40,399
commonly we haven't actually captured

333
00:13:38,720 --> 00:13:42,160
the causal structure that we think we're

334
00:13:40,399 --> 00:13:44,480
capturing. We've captured something else

335
00:13:42,160 --> 00:13:47,200
about our particular environment. And so

336
00:13:44,480 --> 00:13:49,519
we want to be able to recognize that.

337
00:13:47,200 --> 00:13:52,079
So if model A is fair in hospital A,

338
00:13:49,519 --> 00:13:53,680
will it be fair in hospital B? Or if it

339
00:13:52,079 --> 00:13:57,040
works for one race, will it work for

340
00:13:53,680 --> 00:13:59,360
another? Um so what you see here is a

341
00:13:57,040 --> 00:14:02,079
study in dermatology predicting a whole

342
00:13:59,360 --> 00:14:03,279
bunch of skin conditions from images. Um

343
00:14:02,079 --> 00:14:05,040
there are two different clinical

344
00:14:03,279 --> 00:14:07,920
environments. one the one that was

345
00:14:05,040 --> 00:14:11,040
trained on was ter

346
00:14:07,920 --> 00:14:15,040
sorry teladermatology in the US and the

347
00:14:11,040 --> 00:14:16,639
target um is clinic uh it's it's a

348
00:14:15,040 --> 00:14:18,959
clinic and it's taken from incl clinic

349
00:14:16,639 --> 00:14:21,120
patients in Australia

350
00:14:18,959 --> 00:14:23,760
and so this involves a shift on

351
00:14:21,120 --> 00:14:25,920
demographics and also images and labels

352
00:14:23,760 --> 00:14:27,680
and you can see the shift in dem

353
00:14:25,920 --> 00:14:30,480
demographics on the lefth hand side

354
00:14:27,680 --> 00:14:33,199
right men and women um ages of the

355
00:14:30,480 --> 00:14:33,199
patients etc

356
00:14:36,639 --> 00:14:43,199
Okay. So on the bottom right this plot

357
00:14:39,920 --> 00:14:45,519
here um we can see the impact of the

358
00:14:43,199 --> 00:14:47,600
shift on fairness right with model that

359
00:14:45,519 --> 00:14:49,519
was fair in training. So that would be

360
00:14:47,600 --> 00:14:51,760
the one with the open dots that's the

361
00:14:49,519 --> 00:14:54,320
source distribution

362
00:14:51,760 --> 00:14:56,480
um leading to large gaps between

363
00:14:54,320 --> 00:14:59,279
different subgroups in deployments in

364
00:14:56,480 --> 00:15:02,480
deployment. Um so in particular age

365
00:14:59,279 --> 00:15:05,600
right so if we look at um I'm gonna walk

366
00:15:02,480 --> 00:15:08,560
over um if we look at 18 to 29 year olds

367
00:15:05,600 --> 00:15:10,560
we're doing about as well in our source

368
00:15:08,560 --> 00:15:12,959
modeling our source distribution as we

369
00:15:10,560 --> 00:15:14,639
were in the original sorry in the target

370
00:15:12,959 --> 00:15:16,639
distribution as we were in the original

371
00:15:14,639 --> 00:15:18,800
source distribution whereas we look at

372
00:15:16,639 --> 00:15:21,440
if we look at people who are 65 and

373
00:15:18,800 --> 00:15:24,079
older we're doing much much worse and so

374
00:15:21,440 --> 00:15:27,360
this leads to different fairness

375
00:15:24,079 --> 00:15:31,000
fairness um fairness outcomes for those

376
00:15:27,360 --> 00:15:31,000
two groups of people.

377
00:15:32,079 --> 00:15:37,360
Okay. And so recently we looked at

378
00:15:33,760 --> 00:15:40,079
evaluating large language model question

379
00:15:37,360 --> 00:15:43,600
answering around um answers to health

380
00:15:40,079 --> 00:15:46,240
questions. Um so this led to this work

381
00:15:43,600 --> 00:15:50,240
led to evaluation done on the MedP 2

382
00:15:46,240 --> 00:15:53,279
model um which you can see. Um the user

383
00:15:50,240 --> 00:15:56,880
in in in MedP 2 the user asked questions

384
00:15:53,279 --> 00:16:00,000
like um can incontinence be cured?

385
00:15:56,880 --> 00:16:02,160
um and the model gives a particular

386
00:16:00,000 --> 00:16:04,000
answer, right? So this is like the kind

387
00:16:02,160 --> 00:16:07,279
of common thing that you see with a

388
00:16:04,000 --> 00:16:10,079
chatbot. Medpom is Google's particular

389
00:16:07,279 --> 00:16:15,480
uh medically related well it was their

390
00:16:10,079 --> 00:16:15,480
old medically related chatbot model.

391
00:16:16,800 --> 00:16:24,079
Um, and so we look at assessment on on a

392
00:16:20,800 --> 00:16:26,560
system like MedP. And um, we actually we

393
00:16:24,079 --> 00:16:30,560
have a nature medicine paper t looking

394
00:16:26,560 --> 00:16:33,120
at assessing med pom

395
00:16:30,560 --> 00:16:36,079
for disparities between different

396
00:16:33,120 --> 00:16:39,519
demographics and bias that might insert

397
00:16:36,079 --> 00:16:41,680
itself within a medal context. Um, and

398
00:16:39,519 --> 00:16:44,639
the nature med medicine paper that we

399
00:16:41,680 --> 00:16:48,240
have um makes a couple of different key

400
00:16:44,639 --> 00:16:52,399
contributions. Um the first contribution

401
00:16:48,240 --> 00:16:54,880
is that um it develops new rubrics for

402
00:16:52,399 --> 00:16:57,360
the assessment of equity related biases

403
00:16:54,880 --> 00:17:00,880
in LLM generated answers to medical

404
00:16:57,360 --> 00:17:02,880
questions. So um this includes three

405
00:17:00,880 --> 00:17:04,880
modes of evaluation independent,

406
00:17:02,880 --> 00:17:06,959
pairwise and counterfactual and I'll

407
00:17:04,880 --> 00:17:09,839
show you causal models for them in a

408
00:17:06,959 --> 00:17:12,559
second. Um but there are also three

409
00:17:09,839 --> 00:17:16,400
kinds of raiders clinical raiders equity

410
00:17:12,559 --> 00:17:18,959
experts and regular consumers.

411
00:17:16,400 --> 00:17:20,799
And then after we develop these rubrics,

412
00:17:18,959 --> 00:17:22,959
um we collect new data sets. We've

413
00:17:20,799 --> 00:17:26,000
collected seven new data sets that are

414
00:17:22,959 --> 00:17:29,760
enriched for equity relevant domains um

415
00:17:26,000 --> 00:17:33,280
and which are documented and released.

416
00:17:29,760 --> 00:17:36,640
Um and lastly, we run an empirical study

417
00:17:33,280 --> 00:17:38,720
with MedPal. This is the largest human

418
00:17:36,640 --> 00:17:40,880
evaluation study to date, or at least it

419
00:17:38,720 --> 00:17:44,640
was when the paper came out within the

420
00:17:40,880 --> 00:17:47,120
last year. Um it includes um assessment

421
00:17:44,640 --> 00:17:49,440
questions and data sets which allow for

422
00:17:47,120 --> 00:17:52,000
identifying previous un previously

423
00:17:49,440 --> 00:17:53,760
unknown limitations and it also exhibits

424
00:17:52,000 --> 00:17:56,480
different modes of evaluation and raider

425
00:17:53,760 --> 00:18:00,039
types which allow for uh probing

426
00:17:56,480 --> 00:18:00,039
dimensions of bias.

427
00:18:00,799 --> 00:18:06,559
Okay. So first up um I'm going to go

428
00:18:04,480 --> 00:18:09,600
through those steps one by one. Right.

429
00:18:06,559 --> 00:18:11,120
So first up um is rubrics. So in order

430
00:18:09,600 --> 00:18:14,640
to understand fairness and equity

431
00:18:11,120 --> 00:18:17,200
challenges in a health context um a

432
00:18:14,640 --> 00:18:20,240
rubric has to be developed to understand

433
00:18:17,200 --> 00:18:24,320
what the definitions of these terms

434
00:18:20,240 --> 00:18:27,360
means. So like what is fair, right? Like

435
00:18:24,320 --> 00:18:29,440
what fairness means? Big big open

436
00:18:27,360 --> 00:18:31,280
question, right? Um it could be a lot of

437
00:18:29,440 --> 00:18:36,400
different things. And so in order to

438
00:18:31,280 --> 00:18:38,080
tackle that, um we want to ask

439
00:18:36,400 --> 00:18:40,720
a whole bunch of different people like

440
00:18:38,080 --> 00:18:42,880
physicians. Um

441
00:18:40,720 --> 00:18:45,679
okay, so we want to know how it's

442
00:18:42,880 --> 00:18:47,840
related to practical medical systems,

443
00:18:45,679 --> 00:18:50,640
what dimensions are relevant to measure.

444
00:18:47,840 --> 00:18:53,679
And so we get a bunch of physicians. Um

445
00:18:50,640 --> 00:18:58,480
and we ask them um and we look at med

446
00:18:53,679 --> 00:19:01,440
promp prompting and uh we try to

447
00:18:58,480 --> 00:19:04,559
understand what

448
00:19:01,440 --> 00:19:07,520
like what what kinds of um what kinds of

449
00:19:04,559 --> 00:19:09,679
bias might might crop up in the answers.

450
00:19:07,520 --> 00:19:11,520
Um and so we do this using the kinds of

451
00:19:09,679 --> 00:19:14,840
causal models that we've seen in the

452
00:19:11,520 --> 00:19:14,840
previous slides.

453
00:19:15,120 --> 00:19:20,160
Um and so we captured six different

454
00:19:17,840 --> 00:19:23,039
dimensions of bias through our our

455
00:19:20,160 --> 00:19:25,679
procedure. Um and and they're shown on

456
00:19:23,039 --> 00:19:27,840
the slide on the right hand side. Um and

457
00:19:25,679 --> 00:19:29,840
so just briefly um the first two

458
00:19:27,840 --> 00:19:32,080
dimensions capture the idea that the

459
00:19:29,840 --> 00:19:33,760
outputs uh should be accurate and

460
00:19:32,080 --> 00:19:36,240
inclusive of the experiences and

461
00:19:33,760 --> 00:19:37,520
perspectives of all groups of people. Uh

462
00:19:36,240 --> 00:19:41,120
the third dimension captures

463
00:19:37,520 --> 00:19:44,160
stereotyping. The fourth dimension

464
00:19:41,120 --> 00:19:45,840
um assesses whether outputs presented

465
00:19:44,160 --> 00:19:48,320
appropriate social and structural

466
00:19:45,840 --> 00:19:51,120
explanations for health disparities. The

467
00:19:48,320 --> 00:19:52,960
fifth dimension deals with the idea that

468
00:19:51,120 --> 00:19:56,000
some of the questions are explicitly

469
00:19:52,960 --> 00:19:58,799
adversarial and potentially contain a

470
00:19:56,000 --> 00:20:00,480
premise that's itself pro problematic um

471
00:19:58,799 --> 00:20:02,880
in terms of presenting content that

472
00:20:00,480 --> 00:20:05,840
contains misinformation,

473
00:20:02,880 --> 00:20:09,360
offensive or stereotyping content etc.

474
00:20:05,840 --> 00:20:11,520
Um and last we uh aim to assess whether

475
00:20:09,360 --> 00:20:14,559
potentially acting on an output could

476
00:20:11,520 --> 00:20:18,600
lead to disproportionate withholding of

477
00:20:14,559 --> 00:20:18,600
opportunities or resources.

478
00:20:19,919 --> 00:20:24,480
And so as I mentioned we introduced

479
00:20:21,760 --> 00:20:26,799
three versions of assessment uh of the

480
00:20:24,480 --> 00:20:29,280
assessment rubic rubric that can be used

481
00:20:26,799 --> 00:20:30,960
for different different types of design.

482
00:20:29,280 --> 00:20:33,440
These are independent, pair-wise and

483
00:20:30,960 --> 00:20:38,240
counterfactual. And I will briefly show

484
00:20:33,440 --> 00:20:41,440
you uh what these rubrics look like. Um

485
00:20:38,240 --> 00:20:44,960
so here is the rubric for uh independent

486
00:20:41,440 --> 00:20:47,600
assessments. Um the main thing to notice

487
00:20:44,960 --> 00:20:49,520
is that the dimensions of bias that we

488
00:20:47,600 --> 00:20:51,520
define show up as options for the

489
00:20:49,520 --> 00:20:54,640
raiders to annotate when flagging an

490
00:20:51,520 --> 00:20:56,799
output as containing bias. Um

491
00:20:54,640 --> 00:20:59,039
independent assessments ask about

492
00:20:56,799 --> 00:21:03,120
whether the answer contains bias. Just

493
00:20:59,039 --> 00:21:03,120
straight up one question, one answer.

494
00:21:03,520 --> 00:21:07,919
pair pair-wise rubric assessments look

495
00:21:05,840 --> 00:21:10,640
at relative bias between alter two

496
00:21:07,919 --> 00:21:13,200
alternatives uh two alternative

497
00:21:10,640 --> 00:21:16,080
scenarios right so it's like you have a

498
00:21:13,200 --> 00:21:17,440
question here's answer A here's answer B

499
00:21:16,080 --> 00:21:20,400
which one do you like better which one

500
00:21:17,440 --> 00:21:23,200
do you think contains less bias and then

501
00:21:20,400 --> 00:21:25,679
counterfactual assessments ask about the

502
00:21:23,200 --> 00:21:27,679
ideal versus the actual answers and so

503
00:21:25,679 --> 00:21:30,080
what I mean by this is like actually the

504
00:21:27,679 --> 00:21:31,600
prompt can change so instead of having a

505
00:21:30,080 --> 00:21:33,679
white person you You can have a black

506
00:21:31,600 --> 00:21:36,159
person inserted into the prompt and then

507
00:21:33,679 --> 00:21:38,880
you can evaluate what kind of bias you

508
00:21:36,159 --> 00:21:42,120
think comes out of of of the answers to

509
00:21:38,880 --> 00:21:42,120
those prompts.

510
00:21:43,840 --> 00:21:48,640
Okay. So using the metrics that we

511
00:21:46,000 --> 00:21:50,880
discussed that we collected from from

512
00:21:48,640 --> 00:21:54,240
phys physicians amongst the axes of bias

513
00:21:50,880 --> 00:21:58,400
that we discussed um we collect the uh

514
00:21:54,240 --> 00:22:01,600
equity med QA data sets. Um so that's a

515
00:21:58,400 --> 00:22:04,559
collection of second seven data sets of

516
00:22:01,600 --> 00:22:07,039
adversarial questions. Um and a few

517
00:22:04,559 --> 00:22:10,320
thousand of them are unique questions.

518
00:22:07,039 --> 00:22:12,960
Um as mentioned the purpose of the data

519
00:22:10,320 --> 00:22:15,919
sets is to probe for potential bias with

520
00:22:12,960 --> 00:22:19,520
um with prompts that plausibly would

521
00:22:15,919 --> 00:22:21,840
lead to problematic outputs. Um so so

522
00:22:19,520 --> 00:22:23,840
again we expect more problematic outputs

523
00:22:21,840 --> 00:22:27,039
than you would normally find using an

524
00:22:23,840 --> 00:22:28,640
LLM. Um each of the individual data sets

525
00:22:27,039 --> 00:22:30,799
was collected for different types of

526
00:22:28,640 --> 00:22:33,120
adversarial questions and are derived

527
00:22:30,799 --> 00:22:34,960
through a variety of methods both manual

528
00:22:33,120 --> 00:22:37,039
and semi-automated.

529
00:22:34,960 --> 00:22:39,120
So for example, one of the data sets is

530
00:22:37,039 --> 00:22:41,360
topically enriched for content related

531
00:22:39,120 --> 00:22:43,280
to health inequities in the US and

532
00:22:41,360 --> 00:22:46,840
another one focuses on tropical and

533
00:22:43,280 --> 00:22:46,840
infectious diseases.

534
00:22:47,679 --> 00:22:52,480
So this this table gives the details of

535
00:22:49,919 --> 00:22:54,320
all of the data sets. Um for additional

536
00:22:52,480 --> 00:22:57,520
details you can read our nature medicine

537
00:22:54,320 --> 00:23:00,000
paper or again talk to Stephen. Um in

538
00:22:57,520 --> 00:23:01,600
particular um we use the tropical and

539
00:23:00,000 --> 00:23:04,080
infectious diseases data set to

540
00:23:01,600 --> 00:23:06,159
instruction chami and I'm going to show

541
00:23:04,080 --> 00:23:09,159
you a recording of that demo in a

542
00:23:06,159 --> 00:23:09,159
second.

543
00:23:10,559 --> 00:23:16,320
The final component of the study is um a

544
00:23:13,840 --> 00:23:18,480
an empirical study where we applied um

545
00:23:16,320 --> 00:23:21,440
our assessment rubrics and data sets in

546
00:23:18,480 --> 00:23:24,320
practice using the outputs of MEDPOM 2

547
00:23:21,440 --> 00:23:26,000
as a proof of concept study. Um an

548
00:23:24,320 --> 00:23:27,600
important part of this was to understand

549
00:23:26,000 --> 00:23:30,240
how groups of raiders with different

550
00:23:27,600 --> 00:23:33,360
expertise and perspectives uh might

551
00:23:30,240 --> 00:23:36,400
evaluate outputs differently. So things

552
00:23:33,360 --> 00:23:40,400
that we do in this evaluation um again

553
00:23:36,400 --> 00:23:42,559
of MedPM 2 is we apply rubrics to the

554
00:23:40,400 --> 00:23:46,240
equity medqa data set and three other

555
00:23:42,559 --> 00:23:48,799
data sets um with uh three raider groups

556
00:23:46,240 --> 00:23:52,080
physicians equity experts and consumers

557
00:23:48,799 --> 00:23:55,679
or patients. We do pair-wise comparisons

558
00:23:52,080 --> 00:23:59,120
between med 2 and medal and med 2 and

559
00:23:55,679 --> 00:24:02,720
physician written answers. We assess

560
00:23:59,120 --> 00:24:04,960
interrator reliability on a subset um

561
00:24:02,720 --> 00:24:07,520
that mix adversarial and non-adversarial

562
00:24:04,960 --> 00:24:10,159
data. We do a pilot study with

563
00:24:07,520 --> 00:24:13,360
counterfactual rubrics and data sets.

564
00:24:10,159 --> 00:24:18,440
And we analyze uh the demographic

565
00:24:13,360 --> 00:24:18,440
effects um with consumer raiders.

566
00:24:18,960 --> 00:24:24,960
Um, and in general, I mean,

567
00:24:23,520 --> 00:24:27,200
I'm just going to I'm just going to

568
00:24:24,960 --> 00:24:29,840
highlight a a few of the key results and

569
00:24:27,200 --> 00:24:33,279
and takeaways from from from the

570
00:24:29,840 --> 00:24:35,360
empirical study. Um, okay. So, first, we

571
00:24:33,279 --> 00:24:37,600
report

572
00:24:35,360 --> 00:24:40,320
a much greater rate of bias in model

573
00:24:37,600 --> 00:24:42,240
outputs than previous work um for the

574
00:24:40,320 --> 00:24:44,640
data sets in common. So, again, there

575
00:24:42,240 --> 00:24:47,039
were some data sets that were already

576
00:24:44,640 --> 00:24:50,000
collected in other papers. We also run

577
00:24:47,039 --> 00:24:53,039
this on them. Um, and we see more data

578
00:24:50,000 --> 00:24:55,919
set resulting more more bi sorry more

579
00:24:53,039 --> 00:24:58,320
bias resulting from using our rubrics

580
00:24:55,919 --> 00:25:01,039
than than those data sets were found to

581
00:24:58,320 --> 00:25:03,120
have in the previously studied papers.

582
00:25:01,039 --> 00:25:05,279
Um,

583
00:25:03,120 --> 00:25:08,480
so this suggests that the rubrics work

584
00:25:05,279 --> 00:25:12,559
in the sense that um they they prime

585
00:25:08,480 --> 00:25:14,320
readers to to identify biases that that

586
00:25:12,559 --> 00:25:16,080
weren't seen with previous versions of

587
00:25:14,320 --> 00:25:19,440
the rubric that were used in these other

588
00:25:16,080 --> 00:25:22,320
papers. Um, we also find a greater rate

589
00:25:19,440 --> 00:25:24,480
of bias um reported with the adversarial

590
00:25:22,320 --> 00:25:26,960
data sets over non-adversarial data

591
00:25:24,480 --> 00:25:30,559
sets. Um and this indicates that the use

592
00:25:26,960 --> 00:25:32,480
of the data sets are um actually

593
00:25:30,559 --> 00:25:35,360
enriched for questions where the model

594
00:25:32,480 --> 00:25:38,960
is more likely to produce bias outputs.

595
00:25:35,360 --> 00:25:40,799
Um we find that the use of multiple data

596
00:25:38,960 --> 00:25:43,200
sets, rubrics and raider groups is

597
00:25:40,799 --> 00:25:46,640
useful for identifying different aspects

598
00:25:43,200 --> 00:25:47,919
of of bias. Um and then lastly, we we

599
00:25:46,640 --> 00:25:50,000
still think that this approach

600
00:25:47,919 --> 00:25:53,320
underestimates bias since it's a post

601
00:25:50,000 --> 00:25:53,320
hawk analysis.

602
00:25:57,520 --> 00:26:02,159
Okay. So, so that that basically

603
00:26:00,400 --> 00:26:04,559
concludes what I have to say about our

604
00:26:02,159 --> 00:26:06,240
health equity toolbox. Um, again, for

605
00:26:04,559 --> 00:26:08,000
more information about the health equity

606
00:26:06,240 --> 00:26:11,039
toolbox, we have a nature medicine

607
00:26:08,000 --> 00:26:13,600
paper. You can look at it. Um, I'm going

608
00:26:11,039 --> 00:26:15,679
to now focus on a particular data set in

609
00:26:13,600 --> 00:26:17,600
the health equity toolbox called the

610
00:26:15,679 --> 00:26:20,960
tropical and infectious diseases data

611
00:26:17,600 --> 00:26:22,960
set. Um and so

612
00:26:20,960 --> 00:26:25,840
here we'd like to understand how large

613
00:26:22,960 --> 00:26:28,400
language models like Gemini perform on

614
00:26:25,840 --> 00:26:31,360
the diseases the tropical more

615
00:26:28,400 --> 00:26:32,960
globalized diseases like malaria um and

616
00:26:31,360 --> 00:26:35,679
what contextual information like

617
00:26:32,960 --> 00:26:40,360
demographic attributes or location

618
00:26:35,679 --> 00:26:40,360
information impact model performance.

619
00:26:42,080 --> 00:26:47,360
tropical diseases are focused on because

620
00:26:44,880 --> 00:26:49,840
they're less likely to be um common

621
00:26:47,360 --> 00:26:51,919
demographically and geographically and

622
00:26:49,840 --> 00:26:56,159
least likely to be captured in commonly

623
00:26:51,919 --> 00:26:58,240
used training data sets. Um so in the

624
00:26:56,159 --> 00:27:00,240
development of a data set that can

625
00:26:58,240 --> 00:27:02,480
capture underrepresented properties

626
00:27:00,240 --> 00:27:06,159
associated with these diseases, we focus

627
00:27:02,480 --> 00:27:08,960
on four different aspects. Um the first

628
00:27:06,159 --> 00:27:11,360
is specific properties of the disease

629
00:27:08,960 --> 00:27:14,480
itself and we find that in authoritative

630
00:27:11,360 --> 00:27:16,799
sources. Um the second is the

631
00:27:14,480 --> 00:27:18,960
symptomology. The third is the location

632
00:27:16,799 --> 00:27:21,039
and the last one is any different risk

633
00:27:18,960 --> 00:27:22,720
factors. These are all contextual pieces

634
00:27:21,039 --> 00:27:24,640
of information that we want to capture

635
00:27:22,720 --> 00:27:28,880
that we think will lead to like a good

636
00:27:24,640 --> 00:27:31,360
diagnosis um in a globalized setting.

637
00:27:28,880 --> 00:27:34,080
risk factors might include lifestyle,

638
00:27:31,360 --> 00:27:37,440
age, sex, things that are associated

639
00:27:34,080 --> 00:27:39,919
with the disease. Um, and so in this

640
00:27:37,440 --> 00:27:42,080
way, either by manually curating or

641
00:27:39,919 --> 00:27:45,880
generating these details, we construct

642
00:27:42,080 --> 00:27:45,880
our trans data set.

643
00:27:46,080 --> 00:27:49,200
Okay, so here's an example for

644
00:27:47,520 --> 00:27:51,600
tuberculosis.

645
00:27:49,200 --> 00:27:53,919
Um, authoritative s sources are searched

646
00:27:51,600 --> 00:27:56,919
and symptomology is added to a seed

647
00:27:53,919 --> 00:27:56,919
prompt.

648
00:27:59,039 --> 00:28:04,000
Then geographical prevalence is assessed

649
00:28:01,919 --> 00:28:05,760
and a location of focus is selected.

650
00:28:04,000 --> 00:28:08,760
This is if we're generating a prompt,

651
00:28:05,760 --> 00:28:08,760
right?

652
00:28:08,960 --> 00:28:13,919
And then lastly, we search for risk

653
00:28:11,200 --> 00:28:16,320
factors related to tuberculosis. So here

654
00:28:13,919 --> 00:28:18,559
we can see differing occurrences based

655
00:28:16,320 --> 00:28:21,120
on age and sex with increasing

656
00:28:18,559 --> 00:28:23,600
prevalence in men in middle age groups,

657
00:28:21,120 --> 00:28:24,960
right? And so that's something that we

658
00:28:23,600 --> 00:28:26,640
want to take into account when we

659
00:28:24,960 --> 00:28:29,640
generate our our prompt for

660
00:28:26,640 --> 00:28:29,640
tuberculosis.

661
00:28:29,679 --> 00:28:36,880
Okay. So here's an example for sleeping

662
00:28:32,559 --> 00:28:39,279
sickness. Um we have um different

663
00:28:36,880 --> 00:28:44,399
attributes being discussed 28-year-old

664
00:28:39,279 --> 00:28:48,240
male. We have general symptoms. Um so

665
00:28:44,399 --> 00:28:50,640
they have fever, severe headaches, etc.

666
00:28:48,240 --> 00:28:52,799
We have specific symptoms like they've

667
00:28:50,640 --> 00:28:54,720
developed a skin rash and show signs of

668
00:28:52,799 --> 00:28:57,200
confusion.

669
00:28:54,720 --> 00:29:00,080
Um we have the location of high

670
00:28:57,200 --> 00:29:01,600
incidence. Um the patient the patient

671
00:29:00,080 --> 00:29:03,279
lives in the particular province they

672
00:29:01,600 --> 00:29:05,600
do. I'm sorry I'm going to totally

673
00:29:03,279 --> 00:29:10,399
butcher the name of that province. Feel

674
00:29:05,600 --> 00:29:12,880
badly. Um and then um their lifestyle

675
00:29:10,399 --> 00:29:15,520
factors that they discuss like they own

676
00:29:12,880 --> 00:29:17,360
domestic animals.

677
00:29:15,520 --> 00:29:19,760
And so this in general is sort of the

678
00:29:17,360 --> 00:29:22,000
schema that's used for developing the

679
00:29:19,760 --> 00:29:25,840
prompts that that we study that we

680
00:29:22,000 --> 00:29:28,880
generated um for the trends data set. Um

681
00:29:25,840 --> 00:29:33,440
and so the person enters their their

682
00:29:28,880 --> 00:29:37,039
demographic information, their um

683
00:29:33,440 --> 00:29:39,679
a young woman from DAR. Um they enter

684
00:29:37,039 --> 00:29:42,080
any risk factors that they have, right?

685
00:29:39,679 --> 00:29:45,279
So this could be that they slept with

686
00:29:42,080 --> 00:29:48,399
mosquito nut without the mosquito net.

687
00:29:45,279 --> 00:29:49,760
Um and then they enter all of the

688
00:29:48,399 --> 00:29:52,559
symptoms that they're currently

689
00:29:49,760 --> 00:29:56,039
experiencing. So fever, fatigue, sore

690
00:29:52,559 --> 00:29:56,039
throat, etc.

691
00:29:56,320 --> 00:29:59,919
So the next part is going to take a

692
00:29:57,600 --> 00:30:01,760
while and I apologize. It actually takes

693
00:29:59,919 --> 00:30:07,840
a long time in real life too. This is

694
00:30:01,760 --> 00:30:11,039
not it's not just this video. Um yeah.

695
00:30:07,840 --> 00:30:13,120
Um okay. So then they try to use that

696
00:30:11,039 --> 00:30:14,720
that inputed information and they're

697
00:30:13,120 --> 00:30:18,080
going to generate a summary and a

698
00:30:14,720 --> 00:30:20,640
diagnosis of what disease um what

699
00:30:18,080 --> 00:30:24,960
disease they're experiencing.

700
00:30:20,640 --> 00:30:27,200
Um and so basically oh here we go. So

701
00:30:24,960 --> 00:30:29,039
here's a summary of the of the case

702
00:30:27,200 --> 00:30:31,200
file, right? So it takes that

703
00:30:29,039 --> 00:30:35,440
information, it generally generates a

704
00:30:31,200 --> 00:30:37,679
prompt and it says um

705
00:30:35,440 --> 00:30:39,360
what disease what tropical disease the

706
00:30:37,679 --> 00:30:41,039
patient might be experiencing. It gives

707
00:30:39,360 --> 00:30:45,600
some confidence measurement around that

708
00:30:41,039 --> 00:30:49,360
and then you can see um like an

709
00:30:45,600 --> 00:30:51,360
an incident an incidence graph in the

710
00:30:49,360 --> 00:30:53,120
world where it talks about what the

711
00:30:51,360 --> 00:30:56,720
incidence of the particular disease

712
00:30:53,120 --> 00:30:59,360
might be. Um, and so I don't show a lot

713
00:30:56,720 --> 00:31:01,360
of head-to-head comparisons with Gemini.

714
00:30:59,360 --> 00:31:04,960
Um, for those again you can look in the

715
00:31:01,360 --> 00:31:07,960
paper. Um, but

716
00:31:04,960 --> 00:31:07,960
uh

717
00:31:08,559 --> 00:31:13,600
we will just take it for granted that

718
00:31:10,399 --> 00:31:15,279
this is substantially improved. Okay. By

719
00:31:13,600 --> 00:31:16,799
I mean sorry we won't take it for

720
00:31:15,279 --> 00:31:18,799
granted. You can look in the paper. It

721
00:31:16,799 --> 00:31:20,799
is substantially improved. I have plots

722
00:31:18,799 --> 00:31:23,120
there.

723
00:31:20,799 --> 00:31:24,720
Um but and you can see you can see you

724
00:31:23,120 --> 00:31:28,640
can see the improvement that it makes.

725
00:31:24,720 --> 00:31:30,559
Okay. So um again like the these large

726
00:31:28,640 --> 00:31:32,799
language models are have historically

727
00:31:30,559 --> 00:31:35,840
been quite bad at at more globalized

728
00:31:32,799 --> 00:31:37,760
diseases and so this is um an attempt

729
00:31:35,840 --> 00:31:40,399
that we made in order to try to try to

730
00:31:37,760 --> 00:31:42,559
improve them. Okay. Um the next one that

731
00:31:40,399 --> 00:31:46,080
I'm going to talk about is the largest

732
00:31:42,559 --> 00:31:48,720
study to date on uh on African

733
00:31:46,080 --> 00:31:52,240
healthcare. Um I think you know this

734
00:31:48,720 --> 00:31:56,720
paper was published a month ago. So I

735
00:31:52,240 --> 00:31:59,440
guess since since a month ago um and it

736
00:31:56,720 --> 00:32:01,360
collects a different kind of data uh

737
00:31:59,440 --> 00:32:03,360
called the Afromed QA data which I'll

738
00:32:01,360 --> 00:32:06,000
get into in a second. Okay. So that

739
00:32:03,360 --> 00:32:08,720
there's an Afromed QA consortium that

740
00:32:06,000 --> 00:32:10,640
people on my team put together. Um it

741
00:32:08,720 --> 00:32:13,919
includes Google research but it also

742
00:32:10,640 --> 00:32:16,159
includes a whole bunch of um a whole

743
00:32:13,919 --> 00:32:19,600
bunch of uh organizations in Africa like

744
00:32:16,159 --> 00:32:22,720
Sasanki Biotech and Masakane. Um it

745
00:32:19,600 --> 00:32:25,600
involves funders from all over.

746
00:32:22,720 --> 00:32:27,360
Um and the form of this data set um is

747
00:32:25,600 --> 00:32:32,000
going to be something like this, right?

748
00:32:27,360 --> 00:32:33,679
Where you have um um like a bunch of um

749
00:32:32,000 --> 00:32:35,919
questions like those found on medical

750
00:32:33,679 --> 00:32:37,679
board exams where a situation is posed

751
00:32:35,919 --> 00:32:40,320
and then there's like a multiple choice

752
00:32:37,679 --> 00:32:43,279
response. So there's there's an example

753
00:32:40,320 --> 00:32:45,919
here, but this is like med QA is like

754
00:32:43,279 --> 00:32:49,600
often like something used as a prompt

755
00:32:45,919 --> 00:32:51,519
for uh medical situations with um with

756
00:32:49,600 --> 00:32:53,679
large language models, and this is no

757
00:32:51,519 --> 00:32:56,480
different, but specific to the African

758
00:32:53,679 --> 00:32:58,640
context. Um so we have a 2-year-old boy

759
00:32:56,480 --> 00:33:01,360
presenting with a history of vomiting.

760
00:32:58,640 --> 00:33:03,760
So the question is the best next step is

761
00:33:01,360 --> 00:33:05,600
what? And then you see multiple choice

762
00:33:03,760 --> 00:33:08,720
answers.

763
00:33:05,600 --> 00:33:11,440
Um, and so the LLM answer and rationale

764
00:33:08,720 --> 00:33:15,039
is is given and then rated by experts

765
00:33:11,440 --> 00:33:17,919
along relevant axes such as quality um

766
00:33:15,039 --> 00:33:20,240
or um how well it reflects an African

767
00:33:17,919 --> 00:33:23,200
context.

768
00:33:20,240 --> 00:33:25,760
Um and you can see that that that rating

769
00:33:23,200 --> 00:33:27,600
sheet there

770
00:33:25,760 --> 00:33:31,039
and that's collected together and that

771
00:33:27,600 --> 00:33:34,159
makes up the aformed QA data set. Um so

772
00:33:31,039 --> 00:33:36,159
both the data sets of the d the trends

773
00:33:34,159 --> 00:33:38,559
data set of prompts and the aphromed QA

774
00:33:36,159 --> 00:33:42,159
data sets are data sets generally that

775
00:33:38,559 --> 00:33:45,440
that in that capture more globalized

776
00:33:42,159 --> 00:33:49,519
um more globalized uh behavior for large

777
00:33:45,440 --> 00:33:51,440
language models um in healthcare. Um,

778
00:33:49,519 --> 00:33:54,000
and you can find out more information on

779
00:33:51,440 --> 00:33:56,720
them on the Google research blog or by

780
00:33:54,000 --> 00:33:59,519
contacting Mercy Asidu who is the person

781
00:33:56,720 --> 00:34:01,919
again on my team um, who has really been

782
00:33:59,519 --> 00:34:05,679
instrumental in this work. Um, I should

783
00:34:01,919 --> 00:34:08,320
say the Afromed QA data was both used

784
00:34:05,679 --> 00:34:12,879
um, in order to benchmark MedeMema which

785
00:34:08,320 --> 00:34:15,599
is Google's current medical LLM. Um, and

786
00:34:12,879 --> 00:34:18,000
so you can find it in the um the data

787
00:34:15,599 --> 00:34:22,240
sheet for that model, but it also won

788
00:34:18,000 --> 00:34:24,560
the um best so social best, let me get

789
00:34:22,240 --> 00:34:30,240
this right, the best social impact paper

790
00:34:24,560 --> 00:34:34,119
award at ACL um and was published

791
00:34:30,240 --> 00:34:34,119
a month ago at ACL.

792
00:34:34,480 --> 00:34:41,599
Okay. So now switching gears a little

793
00:34:37,119 --> 00:34:44,240
bit um I want to talk about um about

794
00:34:41,599 --> 00:34:47,119
pregnancy intelligence which is the work

795
00:34:44,240 --> 00:34:48,639
that we're doing relating to wearable

796
00:34:47,119 --> 00:34:50,320
devices

797
00:34:48,639 --> 00:34:52,800
um

798
00:34:50,320 --> 00:34:56,399
and time series data uh for maternal

799
00:34:52,800 --> 00:34:58,480
health prediction. um Mercy and Bon

800
00:34:56,399 --> 00:35:02,720
Bonaventure Bonaventure Dell he's a

801
00:34:58,480 --> 00:35:04,960
student at MA um worked on this um quite

802
00:35:02,720 --> 00:35:09,280
a bit um and so I'm going to talk about

803
00:35:04,960 --> 00:35:11,359
here okay so uh in this part of the talk

804
00:35:09,280 --> 00:35:14,640
I'm going to focus on wearable data as a

805
00:35:11,359 --> 00:35:17,599
digital biomarker um time series data

806
00:35:14,640 --> 00:35:20,079
set wearable data gives time series data

807
00:35:17,599 --> 00:35:23,839
sets uh which are measured from wearable

808
00:35:20,079 --> 00:35:26,800
devices that you find uh via pixel

809
00:35:23,839 --> 00:35:28,720
uh Pixel Watch or via Fitbit. Um and

810
00:35:26,800 --> 00:35:31,520
they present um very extensive

811
00:35:28,720 --> 00:35:33,680
longitudinal passively collected data

812
00:35:31,520 --> 00:35:36,480
that can potentially enable discovery of

813
00:35:33,680 --> 00:35:38,160
unknown biomarkers. Um and so there's a

814
00:35:36,480 --> 00:35:40,079
growing interest in using this wearable

815
00:35:38,160 --> 00:35:42,880
data for health agents and foundation

816
00:35:40,079 --> 00:35:45,119
models um because they generate

817
00:35:42,880 --> 00:35:47,119
proactive insights and personalized

818
00:35:45,119 --> 00:35:49,839
health responses provided by large

819
00:35:47,119 --> 00:35:52,800
language model based agents for health

820
00:35:49,839 --> 00:35:54,560
and they create time series foundational

821
00:35:52,800 --> 00:35:56,240
models that they allow the creation of

822
00:35:54,560 --> 00:36:00,640
time series foundation models that can

823
00:35:56,240 --> 00:36:05,200
scale up uh multimodal LLM use. Okay. So

824
00:36:00,640 --> 00:36:07,839
um here sorry um so wearable data can be

825
00:36:05,200 --> 00:36:10,720
of great uh use for maternal health.

826
00:36:07,839 --> 00:36:12,800
It's passively collected uh longitudinal

827
00:36:10,720 --> 00:36:14,480
data that can potentially enable better

828
00:36:12,800 --> 00:36:17,280
understanding of pregnancy and

829
00:36:14,480 --> 00:36:20,400
postpartum journeys um and the discovery

830
00:36:17,280 --> 00:36:22,320
of new biomarkers for maternal health um

831
00:36:20,400 --> 00:36:25,960
and give personalized pregnancy and

832
00:36:22,320 --> 00:36:25,960
postpartum insights.

833
00:36:26,560 --> 00:36:30,880
Um Google or formerly Fitbit has one of

834
00:36:28,880 --> 00:36:32,160
the largest data sets of wearable data

835
00:36:30,880 --> 00:36:34,240
collected during pregnancy and

836
00:36:32,160 --> 00:36:38,320
postpartum. This is on about 40,000

837
00:36:34,240 --> 00:36:40,240
women. Um there's infrastructure that

838
00:36:38,320 --> 00:36:43,280
allows for rapid prototyping and

839
00:36:40,240 --> 00:36:45,680
experimental launching of LM verticals.

840
00:36:43,280 --> 00:36:48,079
Pregnancy um presents significant

841
00:36:45,680 --> 00:36:50,640
changes in biometrics um which can

842
00:36:48,079 --> 00:36:52,800
differ from pre pre- pregnancy baselines

843
00:36:50,640 --> 00:36:54,960
and recommendation systems. Um and

844
00:36:52,800 --> 00:36:58,240
agents need this knowledge to provide

845
00:36:54,960 --> 00:37:00,320
accurate insights. Um we're working

846
00:36:58,240 --> 00:37:03,760
towards the development of a maternal

847
00:37:00,320 --> 00:37:06,079
health sub aent um with the ability to

848
00:37:03,760 --> 00:37:09,400
improve accuracy of Gemini activity and

849
00:37:06,079 --> 00:37:09,400
sleep recommendations.

850
00:37:10,320 --> 00:37:15,760
Okay. So in particular in our work we

851
00:37:13,440 --> 00:37:18,480
were interested in predicting postpartum

852
00:37:15,760 --> 00:37:21,119
mood disorders from multiple collected

853
00:37:18,480 --> 00:37:25,359
time series. So the rest of this part is

854
00:37:21,119 --> 00:37:27,920
going to focus on mood disorders. Um

855
00:37:25,359 --> 00:37:30,240
mood disorders are very very common

856
00:37:27,920 --> 00:37:32,160
during and after pregnancy. Um and

857
00:37:30,240 --> 00:37:34,240
they're underdiagnosed and undertreated.

858
00:37:32,160 --> 00:37:36,560
So one in eight women experience

859
00:37:34,240 --> 00:37:38,560
postpartum depression. There's a three

860
00:37:36,560 --> 00:37:42,000
times higher likelihood of suicidal

861
00:37:38,560 --> 00:37:44,800
behavior. Up to 80% experience some form

862
00:37:42,000 --> 00:37:48,760
of baby blues. And 85% are untreated

863
00:37:44,800 --> 00:37:48,760
with low screening rates.

864
00:37:49,200 --> 00:37:54,640
Okay, so we posit that the biometric

865
00:37:52,400 --> 00:37:56,480
data that we have can be used to make

866
00:37:54,640 --> 00:37:59,839
predictions of postpartum mood mood

867
00:37:56,480 --> 00:38:02,880
disorders um and show us what digital

868
00:37:59,839 --> 00:38:05,359
good digital biomark biomarkers might

869
00:38:02,880 --> 00:38:06,960
look like. Um we aim to leverage the

870
00:38:05,359 --> 00:38:09,760
large amount of maternal health data

871
00:38:06,960 --> 00:38:12,320
that we have to improve on studies that

872
00:38:09,760 --> 00:38:14,079
have pointed to this being a promising

873
00:38:12,320 --> 00:38:16,480
direction. And so there are many many

874
00:38:14,079 --> 00:38:18,240
studies that have pointed to this being

875
00:38:16,480 --> 00:38:22,480
a promising direction, but they've

876
00:38:18,240 --> 00:38:26,920
lacked potentially in numbers um or a

877
00:38:22,480 --> 00:38:26,920
focus on particular outcomes.

878
00:38:28,400 --> 00:38:33,839
Um and here's an example of the kinds of

879
00:38:31,119 --> 00:38:36,320
uh time series that we're using to model

880
00:38:33,839 --> 00:38:40,720
mood disorders. So for example, heart

881
00:38:36,320 --> 00:38:43,359
rate and step count. Um

882
00:38:40,720 --> 00:38:45,760
so um as you can see there are very uh

883
00:38:43,359 --> 00:38:47,520
like obvious differences um in these

884
00:38:45,760 --> 00:38:49,359
time series between women with and

885
00:38:47,520 --> 00:38:51,680
without mood disorders. You can sort of

886
00:38:49,359 --> 00:38:53,680
eyeball a lot of these plots whether

887
00:38:51,680 --> 00:38:56,240
it's resting heart rate or step count

888
00:38:53,680 --> 00:38:57,920
and sort of like easily distinguish the

889
00:38:56,240 --> 00:39:01,079
differences in time series between the

890
00:38:57,920 --> 00:39:01,079
two groups.

891
00:39:01,839 --> 00:39:07,359
Um and so here you can see uh some of

892
00:39:04,320 --> 00:39:10,720
the demographics for the particular set

893
00:39:07,359 --> 00:39:14,160
of people who are uh who our data points

894
00:39:10,720 --> 00:39:16,720
in our study. Um so it's almost entirely

895
00:39:14,160 --> 00:39:21,079
white women. Um and almost entirely

896
00:39:16,720 --> 00:39:21,079
between the ages of 30 and 40.

897
00:39:21,280 --> 00:39:27,119
Um and how long their pregnancies lasted

898
00:39:24,400 --> 00:39:29,440
early term versus fullterm. um as well

899
00:39:27,119 --> 00:39:31,839
as high rates of depression, anxiety and

900
00:39:29,440 --> 00:39:34,480
sadness.

901
00:39:31,839 --> 00:39:36,880
Okay,

902
00:39:34,480 --> 00:39:38,960
so this is the data processing and

903
00:39:36,880 --> 00:39:41,839
machine learning pipeline that we used.

904
00:39:38,960 --> 00:39:44,640
Um so first we narrowed down to a co the

905
00:39:41,839 --> 00:39:47,359
cohort uh um to pregnant women who we

906
00:39:44,640 --> 00:39:49,839
have enough data on. um we wanted to

907
00:39:47,359 --> 00:39:52,720
make sure that like we could reliably

908
00:39:49,839 --> 00:39:54,960
sort of try to do prediction um a full

909
00:39:52,720 --> 00:39:56,960
schematic of the inclusion exclusion

910
00:39:54,960 --> 00:39:59,839
criteria given in the paper if you're

911
00:39:56,960 --> 00:40:02,320
interested. Um

912
00:39:59,839 --> 00:40:04,640
and so we then create features that are

913
00:40:02,320 --> 00:40:07,359
relative to the person's baseline rates.

914
00:40:04,640 --> 00:40:10,160
Um and this was very important. So like

915
00:40:07,359 --> 00:40:12,400
what is their pre-reg resting heart rate

916
00:40:10,160 --> 00:40:14,800
versus their pregnancy resting heart

917
00:40:12,400 --> 00:40:17,040
rate. Um and we found this to be really

918
00:40:14,800 --> 00:40:19,359
useful for prediction.

919
00:40:17,040 --> 00:40:22,320
We divide the data set into testing and

920
00:40:19,359 --> 00:40:24,000
training cohorts.

921
00:40:22,320 --> 00:40:26,640
And then because some of the classes are

922
00:40:24,000 --> 00:40:28,560
underrepresented and this is really key

923
00:40:26,640 --> 00:40:30,880
um compared to others, we upsamp

924
00:40:28,560 --> 00:40:33,920
upsample the underrepresented classes

925
00:40:30,880 --> 00:40:39,280
through synthetic data collection. So in

926
00:40:33,920 --> 00:40:41,920
particular, we use a uh custom time GAN

927
00:40:39,280 --> 00:40:44,320
um in order to do this.

928
00:40:41,920 --> 00:40:47,440
So again upsampling

929
00:40:44,320 --> 00:40:49,760
and this goes back to model training. Um

930
00:40:47,440 --> 00:40:53,839
so we use all the data we train the

931
00:40:49,760 --> 00:40:57,960
model and use the results to interpret

932
00:40:53,839 --> 00:40:57,960
features and subgroup analysis.

933
00:41:00,720 --> 00:41:04,400
Okay. So and we were able to

934
00:41:02,640 --> 00:41:07,359
successfully do quite a good job of

935
00:41:04,400 --> 00:41:10,480
predicting mood disorders from from this

936
00:41:07,359 --> 00:41:12,160
methodology from this pipeline. Um and

937
00:41:10,480 --> 00:41:15,599
so you can see some of this in the

938
00:41:12,160 --> 00:41:17,839
bottom row of plots. Um

939
00:41:15,599 --> 00:41:20,480
the model that we ended up using was a

940
00:41:17,839 --> 00:41:22,400
time um GAN that's a generative

941
00:41:20,480 --> 00:41:25,440
adversarial network specifically for

942
00:41:22,400 --> 00:41:28,160
time for generating time series data um

943
00:41:25,440 --> 00:41:31,280
with some modifications most importantly

944
00:41:28,160 --> 00:41:33,599
um a non-stcastic latent space mapping

945
00:41:31,280 --> 00:41:36,480
for um upsampling the less well

946
00:41:33,599 --> 00:41:39,520
reppresented classes.

947
00:41:36,480 --> 00:41:42,960
um for more information about the work

948
00:41:39,520 --> 00:41:45,839
or on this methodology um basically I'll

949
00:41:42,960 --> 00:41:49,400
refer you to Bonaventure um again at Ma

950
00:41:45,839 --> 00:41:49,400
who's the first author.

951
00:41:51,200 --> 00:41:55,839
Okay. So we can also get a sense of the

952
00:41:53,359 --> 00:41:59,359
importance of each feature used in the

953
00:41:55,839 --> 00:42:01,680
prediction of mood disorders. Um so here

954
00:41:59,359 --> 00:42:04,960
we see that deep sleep is important in

955
00:42:01,680 --> 00:42:07,839
anxiety and sadness uh mood disorders

956
00:42:04,960 --> 00:42:11,520
whereas light sleep and and REM sleep is

957
00:42:07,839 --> 00:42:14,160
more important in depression. Um we can

958
00:42:11,520 --> 00:42:15,760
also see on the right hand side that if

959
00:42:14,160 --> 00:42:17,920
we project the data down to two

960
00:42:15,760 --> 00:42:19,280
dimensions mood disorder clusters

961
00:42:17,920 --> 00:42:20,960
separated out and are quite

962
00:42:19,280 --> 00:42:24,640
distinguishable

963
00:42:20,960 --> 00:42:26,400
hopefully again to the eye.

964
00:42:24,640 --> 00:42:28,800
Okay. So, the last thing I want to talk

965
00:42:26,400 --> 00:42:31,520
about um is the expansion of this work

966
00:42:28,800 --> 00:42:34,000
to underrepresented classes because um

967
00:42:31,520 --> 00:42:36,400
the data that we've focused on so far

968
00:42:34,000 --> 00:42:38,640
here in case again you didn't notice it

969
00:42:36,400 --> 00:42:40,800
when I showed you was is that it's it's

970
00:42:38,640 --> 00:42:43,440
almost all on well-off white women. Um

971
00:42:40,800 --> 00:42:45,440
and so we really we really really want

972
00:42:43,440 --> 00:42:47,839
to be able to run this on other groups

973
00:42:45,440 --> 00:42:50,800
of people. Um so we're running a study

974
00:42:47,839 --> 00:42:54,079
in Ghana in partnership that Mercy Mercy

975
00:42:50,800 --> 00:42:55,520
is from Ghana. Um, and so there are some

976
00:42:54,079 --> 00:42:58,960
great connections with the University of

977
00:42:55,520 --> 00:43:01,760
Ghana there. Um,

978
00:42:58,960 --> 00:43:04,000
and we are using u mobile devices, some

979
00:43:01,760 --> 00:43:06,720
of which I've been working on for for

980
00:43:04,000 --> 00:43:08,319
quite a while and wearables uh on women

981
00:43:06,720 --> 00:43:10,160
there so we can eventually make these

982
00:43:08,319 --> 00:43:12,960
kinds of predictions with them. And we

983
00:43:10,160 --> 00:43:15,920
call this study the Akoaba study. Um, we

984
00:43:12,960 --> 00:43:18,240
want to open source our tools um for

985
00:43:15,920 --> 00:43:20,240
other researcher other researchers to be

986
00:43:18,240 --> 00:43:22,319
able to conduct similar similar studies

987
00:43:20,240 --> 00:43:26,720
and improve the representation of

988
00:43:22,319 --> 00:43:28,400
minority women in um in in the work that

989
00:43:26,720 --> 00:43:30,240
they do.

990
00:43:28,400 --> 00:43:32,960
Okay. So there are several important

991
00:43:30,240 --> 00:43:34,640
questions um to be considered as we move

992
00:43:32,960 --> 00:43:37,440
towards making predictions in lower and

993
00:43:34,640 --> 00:43:39,839
middle inome countries. So how might the

994
00:43:37,440 --> 00:43:41,599
metrics change? what are the impacts of

995
00:43:39,839 --> 00:43:43,520
demographics and geoloccation in

996
00:43:41,599 --> 00:43:47,920
general? Is this even feasible for

997
00:43:43,520 --> 00:43:50,000
LMIC's? Um, as well as things that we

998
00:43:47,920 --> 00:43:52,160
can measure that weren't necessarily

999
00:43:50,000 --> 00:43:54,880
available in the previous data set that

1000
00:43:52,160 --> 00:43:57,760
we had access to. So like is there a

1001
00:43:54,880 --> 00:44:03,000
positive behavior change um from the act

1002
00:43:57,760 --> 00:44:03,000
from the activities of our mobile phone?

1003
00:44:03,520 --> 00:44:09,760
Um and so uh the passive signals

1004
00:44:07,200 --> 00:44:11,599
collected are given here in this bubble.

1005
00:44:09,760 --> 00:44:14,800
Um and they include things like sleep,

1006
00:44:11,599 --> 00:44:18,800
activity, heart rate, variability, etc.

1007
00:44:14,800 --> 00:44:21,359
Um again, all off the wearable.

1008
00:44:18,800 --> 00:44:23,680
Um and so there's also data that's

1009
00:44:21,359 --> 00:44:26,160
coming from the mobile the mo the set of

1010
00:44:23,680 --> 00:44:29,200
mobile activities itself like the

1011
00:44:26,160 --> 00:44:32,400
surveys um as well as clinician reported

1012
00:44:29,200 --> 00:44:35,040
outcomes. Um so this would be like

1013
00:44:32,400 --> 00:44:37,119
preeacclampsia or hypertension and then

1014
00:44:35,040 --> 00:44:39,839
also their observation about birth

1015
00:44:37,119 --> 00:44:43,760
timing like was it was the baby born

1016
00:44:39,839 --> 00:44:45,839
pre-term or at term

1017
00:44:43,760 --> 00:44:48,319
and clinician reports also include pre

1018
00:44:45,839 --> 00:44:50,240
and post study analyses um which include

1019
00:44:48,319 --> 00:44:52,640
health history as well as demographic

1020
00:44:50,240 --> 00:44:54,960
information um that might be relevant to

1021
00:44:52,640 --> 00:44:56,720
the setting. It's actually um the

1022
00:44:54,960 --> 00:44:58,720
clinician, one of the clinicians that

1023
00:44:56,720 --> 00:45:00,160
we're working with is like really keen

1024
00:44:58,720 --> 00:45:02,160
to get some of the health history

1025
00:45:00,160 --> 00:45:04,079
information which might represent things

1026
00:45:02,160 --> 00:45:07,319
like induction which are important to

1027
00:45:04,079 --> 00:45:07,319
the setting.

1028
00:45:09,040 --> 00:45:12,880
And then lastly, here are some snapshots

1029
00:45:10,880 --> 00:45:15,119
of the app that we developed that are

1030
00:45:12,880 --> 00:45:17,280
going to allow um the remote collection

1031
00:45:15,119 --> 00:45:20,400
of some of this data. So these are just

1032
00:45:17,280 --> 00:45:26,000
sort of like the opening screens for the

1033
00:45:20,400 --> 00:45:29,200
Akawaba study. um intro and overview and

1034
00:45:26,000 --> 00:45:34,400
these are some screenshots um for like

1035
00:45:29,200 --> 00:45:36,800
the main screens dashboards um etc. So

1036
00:45:34,400 --> 00:45:39,599
um we've started the study in Ghana and

1037
00:45:36,800 --> 00:45:41,599
we've enrolled our first 40 participants

1038
00:45:39,599 --> 00:45:45,119
but we have continuing enroll enrollment

1039
00:45:41,599 --> 00:45:47,760
and we hope to um get more patients

1040
00:45:45,119 --> 00:45:49,839
involved and to have more results and if

1041
00:45:47,760 --> 00:45:51,599
you know anybody in Ghana who might be a

1042
00:45:49,839 --> 00:45:54,960
good participant for this study, please

1043
00:45:51,599 --> 00:45:56,800
please refer them. Um but uh we hope to

1044
00:45:54,960 --> 00:46:00,070
have more results to report back to you

1045
00:45:56,800 --> 00:46:06,139
soon. Okay. Thank you.

1046
00:46:00,070 --> 00:46:06,139
[Applause]

1047
00:46:09,839 --> 00:46:13,040
Thank you, Katherine. That was that was

1048
00:46:11,359 --> 00:46:14,560
fantastic. Uh I'm sure we have a bunch

1049
00:46:13,040 --> 00:46:16,640
of questions. We could I could probably

1050
00:46:14,560 --> 00:46:18,319
get you started on a few. I was I was

1051
00:46:16,640 --> 00:46:20,079
very curious about the about the policy

1052
00:46:18,319 --> 00:46:22,079
aspect of it in terms of actually

1053
00:46:20,079 --> 00:46:24,800
implementing it. You know, for example,

1054
00:46:22,079 --> 00:46:26,960
how does one u establish deployment

1055
00:46:24,800 --> 00:46:30,240
thresholds? uh if you have if you have

1056
00:46:26,960 --> 00:46:34,160
for example a bias of what 13%

1057
00:46:30,240 --> 00:46:37,359
recognized by your evaluation system uh

1058
00:46:34,160 --> 00:46:41,119
are there thoughts around what that who

1059
00:46:37,359 --> 00:46:42,800
would then decide if 12% is too high um

1060
00:46:41,119 --> 00:46:43,599
are those things that your group has

1061
00:46:42,800 --> 00:46:45,920
been thinking about?

1062
00:46:43,599 --> 00:46:49,520
>> Yeah. No, it does. It it makes it makes

1063
00:46:45,920 --> 00:46:51,119
a lot of sense. Um, it's not I think my

1064
00:46:49,520 --> 00:46:53,280
group has been thinking more about how

1065
00:46:51,119 --> 00:46:54,800
to accurately report this as opposed to

1066
00:46:53,280 --> 00:46:56,800
like how to make the decisions about

1067
00:46:54,800 --> 00:46:59,760
what to deploy. That's right. Because

1068
00:46:56,800 --> 00:47:01,680
like policy and and particularly in an

1069
00:46:59,760 --> 00:47:03,920
industry setting is like often

1070
00:47:01,680 --> 00:47:07,200
controlled by other people, right? And

1071
00:47:03,920 --> 00:47:10,319
so there are like execs who make that

1072
00:47:07,200 --> 00:47:12,079
decision and that is not necessarily

1073
00:47:10,319 --> 00:47:14,560
really a decision and there are policy

1074
00:47:12,079 --> 00:47:16,480
people who are specifically employed to

1075
00:47:14,560 --> 00:47:19,040
do that. So there's like in in this

1076
00:47:16,480 --> 00:47:23,440
setting like a real breaking up of

1077
00:47:19,040 --> 00:47:25,599
different job roles, right? Um and so

1078
00:47:23,440 --> 00:47:28,560
our job is more sort of like on the

1079
00:47:25,599 --> 00:47:30,560
science information end and less on the

1080
00:47:28,560 --> 00:47:32,640
like yes, we're going to do this or no,

1081
00:47:30,560 --> 00:47:34,160
we're not going to do this kind of end.

1082
00:47:32,640 --> 00:47:35,760
>> Yeah, that that totally makes sense.

1083
00:47:34,160 --> 00:47:37,920
>> We have our own personal thoughts, but

1084
00:47:35,760 --> 00:47:40,400
>> yes, of course. Uh and in terms of the

1085
00:47:37,920 --> 00:47:43,680
role of uh in terms of the ability for

1086
00:47:40,400 --> 00:47:47,040
you to go back to the uh to the training

1087
00:47:43,680 --> 00:47:49,680
data like are you able to to to uh

1088
00:47:47,040 --> 00:47:52,000
identify the biases uh going back to the

1089
00:47:49,680 --> 00:47:53,680
training data uh and see whether you can

1090
00:47:52,000 --> 00:47:55,200
eliminate that at the training data

1091
00:47:53,680 --> 00:47:56,240
stage or is that is that hard

1092
00:47:55,200 --> 00:47:58,400
fundamentally?

1093
00:47:56,240 --> 00:48:00,240
>> Yeah, I mean that's a good question. I

1094
00:47:58,400 --> 00:48:03,839
mean, I think I think to some degree we

1095
00:48:00,240 --> 00:48:05,599
could do that, but like what

1096
00:48:03,839 --> 00:48:07,520
I mean, I guess it's a good question

1097
00:48:05,599 --> 00:48:09,680
about like if we were to completely

1098
00:48:07,520 --> 00:48:12,640
retrain some of these models, right?

1099
00:48:09,680 --> 00:48:13,839
Like do this analysis, eliminate data. I

1100
00:48:12,640 --> 00:48:16,480
think this is something that like

1101
00:48:13,839 --> 00:48:19,599
pre-training teams are looking into

1102
00:48:16,480 --> 00:48:22,240
quite a bit. Um, but I think it's

1103
00:48:19,599 --> 00:48:26,160
relatively unknown generally in

1104
00:48:22,240 --> 00:48:29,520
generative AI land like what exactly the

1105
00:48:26,160 --> 00:48:32,960
removal or addition of specific data

1106
00:48:29,520 --> 00:48:35,119
points gets you right like there's not a

1107
00:48:32,960 --> 00:48:38,640
not a fixed answer to that but I mean I

1108
00:48:35,119 --> 00:48:41,280
think that is an area of like further

1109
00:48:38,640 --> 00:48:43,440
research or can we do some kind of like

1110
00:48:41,280 --> 00:48:45,040
influ like influence discovery right

1111
00:48:43,440 --> 00:48:47,520
like influence functions can we learn

1112
00:48:45,040 --> 00:48:50,000
the right influence functions to detect

1113
00:48:47,520 --> 00:48:52,400
like like what data point might actually

1114
00:48:50,000 --> 00:48:55,760
be making it behave in a certain manner.

1115
00:48:52,400 --> 00:48:58,480
I think that would be would be really

1116
00:48:55,760 --> 00:49:00,000
really interesting to find out.

1117
00:48:58,480 --> 00:49:01,280
>> And the six dimensions that you

1118
00:49:00,000 --> 00:49:03,440
identified, have there been any thoughts

1119
00:49:01,280 --> 00:49:06,880
on whether you could potentially bake

1120
00:49:03,440 --> 00:49:09,040
that into a loss function uh itself such

1121
00:49:06,880 --> 00:49:09,760
that you can eliminate bias in the

1122
00:49:09,040 --> 00:49:11,680
model?

1123
00:49:09,760 --> 00:49:13,599
>> Yeah. function or is that not

1124
00:49:11,680 --> 00:49:15,440
>> No, I mean that is interesting and and

1125
00:49:13,599 --> 00:49:16,880
we're we're trying to do work where

1126
00:49:15,440 --> 00:49:18,960
we're coming up with evaluation

1127
00:49:16,880 --> 00:49:21,760
procedures and you're right I mean act

1128
00:49:18,960 --> 00:49:24,240
in order to this is not for collecting

1129
00:49:21,760 --> 00:49:25,599
data where you have clinicians like in

1130
00:49:24,240 --> 00:49:27,280
mind but if you're trying to do

1131
00:49:25,599 --> 00:49:30,000
something automated you're right you

1132
00:49:27,280 --> 00:49:32,880
need to be able to come up with a metric

1133
00:49:30,000 --> 00:49:37,119
um and we're in the process of trying to

1134
00:49:32,880 --> 00:49:39,839
work with people it to do it but like I

1135
00:49:37,119 --> 00:49:41,760
I don't know that um it it certainly

1136
00:49:39,839 --> 00:49:42,240
wasn't done for this work if that makes

1137
00:49:41,760 --> 00:49:42,480
sense.

1138
00:49:42,240 --> 00:49:44,720
>> Makes

1139
00:49:42,480 --> 00:49:46,240
>> sense. Great. Well, I'll hand it over to

1140
00:49:44,720 --> 00:49:47,839
the audience uh if you have questions

1141
00:49:46,240 --> 00:49:51,599
going in.

1142
00:49:47,839 --> 00:49:53,520
>> Oh, so I work in the cancer program here

1143
00:49:51,599 --> 00:49:55,280
and something that I'm thinking about

1144
00:49:53,520 --> 00:49:59,520
patient stratification in terms of

1145
00:49:55,280 --> 00:50:01,839
patient populations uh is always

1146
00:49:59,520 --> 00:50:04,240
uh clinical trials,

1147
00:50:01,839 --> 00:50:06,400
right? because there are a lot of people

1148
00:50:04,240 --> 00:50:08,079
who are might be eligible for certain

1149
00:50:06,400 --> 00:50:10,240
kinds of clinical trials that are not

1150
00:50:08,079 --> 00:50:14,559
necessarily taking part in them if they

1151
00:50:10,240 --> 00:50:17,280
might want to. uh and there's, you know,

1152
00:50:14,559 --> 00:50:19,520
there are efforts in expanding this and

1153
00:50:17,280 --> 00:50:22,800
I'm wondering if you have thought about

1154
00:50:19,520 --> 00:50:26,400
ways to use these kinds of uh

1155
00:50:22,800 --> 00:50:29,280
advancements in um understanding where

1156
00:50:26,400 --> 00:50:32,960
these kinds of biases come from to help

1157
00:50:29,280 --> 00:50:35,119
do some kind of matching in that regard.

1158
00:50:32,960 --> 00:50:37,520
>> Yeah. I mean, okay. So are you like

1159
00:50:35,119 --> 00:50:40,240
thinking like when do I know if I've So

1160
00:50:37,520 --> 00:50:42,880
one for example one question that say

1161
00:50:40,240 --> 00:50:44,880
like people at Fitbit or or other places

1162
00:50:42,880 --> 00:50:46,720
have been interested in quite a bit is

1163
00:50:44,880 --> 00:50:48,480
like how do I know when I need to

1164
00:50:46,720 --> 00:50:50,160
collect more data on a particular kind

1165
00:50:48,480 --> 00:50:51,520
of demographic. Is that the kind of

1166
00:50:50,160 --> 00:50:53,520
thing that you're thinking of?

1167
00:50:51,520 --> 00:50:55,520
>> Sort of but also in reverse where like

1168
00:50:53,520 --> 00:50:58,000
if you have someone who is eligible

1169
00:50:55,520 --> 00:50:59,440
maybe for a number of different uh

1170
00:50:58,000 --> 00:50:59,920
clinical trials

1171
00:50:59,440 --> 00:51:01,520
>> Oh,

1172
00:50:59,920 --> 00:51:02,880
>> how might you want to sort where they

1173
00:51:01,520 --> 00:51:05,760
might go?

1174
00:51:02,880 --> 00:51:07,359
>> Yeah. based on availability and what

1175
00:51:05,760 --> 00:51:08,240
kind of demographics they might need and

1176
00:51:07,359 --> 00:51:10,000
that kind of thing.

1177
00:51:08,240 --> 00:51:12,640
>> I think that there are a whole bunch of

1178
00:51:10,000 --> 00:51:14,880
really good questions around I I hadn't

1179
00:51:12,640 --> 00:51:17,040
even thought of that until now, but yes,

1180
00:51:14,880 --> 00:51:18,559
you're right. You have a person actually

1181
00:51:17,040 --> 00:51:20,480
and it's not the other way around

1182
00:51:18,559 --> 00:51:22,400
necessar I mean to some degree they can

1183
00:51:20,480 --> 00:51:23,839
go out and collect collect more data,

1184
00:51:22,400 --> 00:51:25,920
but you're right. Often you're presented

1185
00:51:23,839 --> 00:51:30,040
with a person and you need to know where

1186
00:51:25,920 --> 00:51:30,040
to sort them. Um,

1187
00:51:31,200 --> 00:51:35,839
I think that there are

1188
00:51:34,400 --> 00:51:38,319
I mean I mean the short answer is going

1189
00:51:35,839 --> 00:51:42,079
to be no, right? Like I haven't I

1190
00:51:38,319 --> 00:51:44,079
haven't done that. Um, like the the the

1191
00:51:42,079 --> 00:51:47,119
opposite of that is something that I've

1192
00:51:44,079 --> 00:51:49,599
been trying to do for a long time. Um,

1193
00:51:47,119 --> 00:51:51,280
the way that you've cast it is again

1194
00:51:49,599 --> 00:51:53,839
like something a little bit novel to me

1195
00:51:51,280 --> 00:51:55,839
but also makes sense. Um, I think that

1196
00:51:53,839 --> 00:51:57,920
there's a lot of room here for the

1197
00:51:55,839 --> 00:52:01,040
improvement of clinical trials in

1198
00:51:57,920 --> 00:52:03,359
general, right? Like in general, I mean,

1199
00:52:01,040 --> 00:52:06,640
the best I can say is to make a plug for

1200
00:52:03,359 --> 00:52:08,800
in general, the people who make up like

1201
00:52:06,640 --> 00:52:10,640
all of our clinical trials should be

1202
00:52:08,800 --> 00:52:12,480
being determined through methodology

1203
00:52:10,640 --> 00:52:14,319
like this. There's there's no reason

1204
00:52:12,480 --> 00:52:16,160
that that that that shouldn't be

1205
00:52:14,319 --> 00:52:19,839
happening, right? like you should be

1206
00:52:16,160 --> 00:52:22,000
able to like like do all kinds of like

1207
00:52:19,839 --> 00:52:23,920
assessments where you're like, "Okay, I

1208
00:52:22,000 --> 00:52:25,760
think that this particular like there

1209
00:52:23,920 --> 00:52:27,920
are also sort of like pair-wise things

1210
00:52:25,760 --> 00:52:30,079
where we're running into too many drugs

1211
00:52:27,920 --> 00:52:32,400
needed to be triled head-to-head with

1212
00:52:30,079 --> 00:52:34,640
like you know that then then we can

1213
00:52:32,400 --> 00:52:36,000
actually pay for and staff, right? And

1214
00:52:34,640 --> 00:52:38,720
we should be able to use things like

1215
00:52:36,000 --> 00:52:40,640
this in order to point us to promising

1216
00:52:38,720 --> 00:52:43,680
like clinical trials that we should be

1217
00:52:40,640 --> 00:52:46,160
running and running those." Um, why

1218
00:52:43,680 --> 00:52:48,559
that's not happening, I don't know. I

1219
00:52:46,160 --> 00:52:50,079
think I I I think it's it's a funding

1220
00:52:48,559 --> 00:52:51,440
issue. People who talk to me today are

1221
00:52:50,079 --> 00:52:53,119
not going to be surprised to hear me say

1222
00:52:51,440 --> 00:52:55,359
that. I think there's there's a real

1223
00:52:53,119 --> 00:52:57,599
funding issue out there. But but yes,

1224
00:52:55,359 --> 00:52:59,599
absolutely. You're correct that that

1225
00:52:57,599 --> 00:53:01,119
should be happening and it's not. I mean

1226
00:52:59,599 --> 00:53:06,160
in terms of like the particular

1227
00:53:01,119 --> 00:53:08,079
demographics and bias for for um for

1228
00:53:06,160 --> 00:53:11,359
clinical trials that are already being

1229
00:53:08,079 --> 00:53:13,760
run again I would love help at Google

1230
00:53:11,359 --> 00:53:15,760
like to be able to actually explore that

1231
00:53:13,760 --> 00:53:17,839
because I think it's super important and

1232
00:53:15,760 --> 00:53:19,599
actually really valued in a lot of

1233
00:53:17,839 --> 00:53:21,280
different contexts. Um, I think it's

1234
00:53:19,599 --> 00:53:23,680
actually one of the more important

1235
00:53:21,280 --> 00:53:26,000
questions facing Google right now in my

1236
00:53:23,680 --> 00:53:28,079
opinion because it applies not only to

1237
00:53:26,000 --> 00:53:30,800
like medical clinical trials but even

1238
00:53:28,079 --> 00:53:33,599
things like um

1239
00:53:30,800 --> 00:53:36,000
again not sure which way it goes but

1240
00:53:33,599 --> 00:53:38,720
like things like when do we know when

1241
00:53:36,000 --> 00:53:40,640
we've like collected enough data on a

1242
00:53:38,720 --> 00:53:42,640
particular language to be able to do

1243
00:53:40,640 --> 00:53:44,559
like Google translation well and things

1244
00:53:42,640 --> 00:53:48,160
like that right like even outside of a

1245
00:53:44,559 --> 00:53:51,839
clinical trial setting. Um but but yeah,

1246
00:53:48,160 --> 00:53:55,040
I mean lots of lots of open questions

1247
00:53:51,839 --> 00:53:56,800
there and they're great and I highly

1248
00:53:55,040 --> 00:53:59,800
recommend doing more research in this

1249
00:53:56,800 --> 00:53:59,800
area.

1250
00:54:01,280 --> 00:54:05,839
Um thanks for the really interesting

1251
00:54:02,800 --> 00:54:07,599
talk. Um is there like a fundamental

1252
00:54:05,839 --> 00:54:11,119
difference between say like model

1253
00:54:07,599 --> 00:54:13,280
fairness and model generalizability

1254
00:54:11,119 --> 00:54:16,480
beyond just fairness being more about

1255
00:54:13,280 --> 00:54:18,319
like sensitive attributes? So is do

1256
00:54:16,480 --> 00:54:20,079
would you do you go about those problems

1257
00:54:18,319 --> 00:54:22,880
differently or are they basically like

1258
00:54:20,079 --> 00:54:24,400
very similar? It's just as I was as you

1259
00:54:22,880 --> 00:54:25,440
were going through the talk that's kind

1260
00:54:24,400 --> 00:54:28,000
of what I was just wondering.

1261
00:54:25,440 --> 00:54:31,440
>> Yeah. So I think in a lot of work a lot

1262
00:54:28,000 --> 00:54:33,280
of our work we treat um a lot of model

1263
00:54:31,440 --> 00:54:35,359
model fairness questions as questions

1264
00:54:33,280 --> 00:54:38,160
which can be derived by looking at sort

1265
00:54:35,359 --> 00:54:40,720
of like the the causal structure around

1266
00:54:38,160 --> 00:54:42,160
sensitive attributes. Um, is there

1267
00:54:40,720 --> 00:54:44,240
something particular that you had in

1268
00:54:42,160 --> 00:54:47,359
mind that that can't be linked to an

1269
00:54:44,240 --> 00:54:49,760
attribute that you think should be No.

1270
00:54:47,359 --> 00:54:51,520
Um, so I think in general we try to take

1271
00:54:49,760 --> 00:54:54,079
a lot of these questions around fairness

1272
00:54:51,520 --> 00:54:55,839
and sort of link them to attributes in

1273
00:54:54,079 --> 00:54:57,520
general. Like they don't have again they

1274
00:54:55,839 --> 00:55:00,240
don't have to be attributes of a

1275
00:54:57,520 --> 00:55:02,319
specific race or gender but to say there

1276
00:55:00,240 --> 00:55:03,760
are some attributes of this particular

1277
00:55:02,319 --> 00:55:05,680
class and we're going to look at those

1278
00:55:03,760 --> 00:55:07,680
and then we're going to use things like

1279
00:55:05,680 --> 00:55:09,839
false positive and true positive rate

1280
00:55:07,680 --> 00:55:12,960
gaps in order to identify when there's a

1281
00:55:09,839 --> 00:55:16,079
problem. Um

1282
00:55:12,960 --> 00:55:20,960
I think solutions are coming right like

1283
00:55:16,079 --> 00:55:23,440
but um but we can at least that's a well

1284
00:55:20,960 --> 00:55:26,920
well paved path for thinking about

1285
00:55:23,440 --> 00:55:26,920
identifying issues.

1286
00:55:34,319 --> 00:55:37,599
>> Uh thanks for the talk. This is really

1287
00:55:36,079 --> 00:55:39,359
thoughtprovoking.

1288
00:55:37,599 --> 00:55:43,359
uh how do you think about the

1289
00:55:39,359 --> 00:55:47,640
stochasticity in model outputs as it as

1290
00:55:43,359 --> 00:55:47,640
you're thinking about evaluating them?

1291
00:55:47,839 --> 00:55:53,119
>> Okay, so like sto Oh, so I'm trying to

1292
00:55:50,799 --> 00:55:53,599
think of which model you're potentially

1293
00:55:53,119 --> 00:55:55,119
referring to.

1294
00:55:53,599 --> 00:55:56,960
>> I guess it's maybe more of a holistic

1295
00:55:55,119 --> 00:55:59,119
question just sort of if you're saying

1296
00:55:56,960 --> 00:56:01,359
given X context, what does the model

1297
00:55:59,119 --> 00:56:04,319
output? The fact that it might not give

1298
00:56:01,359 --> 00:56:06,720
you the exact same output even with the

1299
00:56:04,319 --> 00:56:08,000
same input. I could be mistaken on

1300
00:56:06,720 --> 00:56:10,799
whether that's true.

1301
00:56:08,000 --> 00:56:12,880
>> No. Yeah. I mean, yes, it's definitely

1302
00:56:10,799 --> 00:56:16,000
true. Um

1303
00:56:12,880 --> 00:56:18,319
uh so

1304
00:56:16,000 --> 00:56:19,839
I think there's like some amount of

1305
00:56:18,319 --> 00:56:23,359
variability

1306
00:56:19,839 --> 00:56:26,559
inherent to all of this. That's that's

1307
00:56:23,359 --> 00:56:28,880
correct. Um, I think that if you think

1308
00:56:26,559 --> 00:56:31,680
about things sort of like in probability

1309
00:56:28,880 --> 00:56:33,680
distribution land, you're kind of saying

1310
00:56:31,680 --> 00:56:35,760
you think that this is modeled well say

1311
00:56:33,680 --> 00:56:37,599
like by by a particular probability

1312
00:56:35,760 --> 00:56:39,280
distribution. You could represent a

1313
00:56:37,599 --> 00:56:41,599
class with a probability distribution

1314
00:56:39,280 --> 00:56:45,680
and then you expect the variability to

1315
00:56:41,599 --> 00:56:48,400
fall within sort of like the variance

1316
00:56:45,680 --> 00:56:50,559
say like of that that specific form of

1317
00:56:48,400 --> 00:56:53,520
probability distribution. And I think

1318
00:56:50,559 --> 00:56:56,960
it's in that context generally that we

1319
00:56:53,520 --> 00:56:59,280
think about um different classes or

1320
00:56:56,960 --> 00:57:01,119
different variables

1321
00:56:59,280 --> 00:57:03,839
with from which we do this.

1322
00:57:01,119 --> 00:57:05,359
>> Yeah. Okay. I think we'll have to close

1323
00:57:03,839 --> 00:57:07,040
out now. Uh but Katherine's going to be

1324
00:57:05,359 --> 00:57:09,280
around for a bit more. Please feel free

1325
00:57:07,040 --> 00:57:10,480
to come over and have a chat. Katherine,

1326
00:57:09,280 --> 00:57:12,240
thank you once again.

1327
00:57:10,480 --> 00:57:14,880
>> Thanks so much. Thank you all.

1328
00:57:12,240 --> 00:57:16,330
>> Those excellent uh questions of the

1329
00:57:14,880 --> 00:57:21,860
Thank you.

1330
00:57:16,330 --> 00:57:21,860
[Applause]

