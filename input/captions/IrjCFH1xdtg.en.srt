1
00:00:01,199 --> 00:00:05,879
the arrival of artificial intelligence.

2
00:00:03,360 --> 00:00:08,880
Uh Justin Reich is the principal

3
00:00:05,879 --> 00:00:11,200
investigator. Uh he lives here. He's

4
00:00:08,880 --> 00:00:13,320
presenting in California today. I live

5
00:00:11,200 --> 00:00:15,519
in California. I'm presenting here

6
00:00:13,320 --> 00:00:19,119
today. It's not the most efficient way

7
00:00:15,519 --> 00:00:21,840
to do it, but here we are. Um I'm a

8
00:00:19,119 --> 00:00:24,160
journalist by background. So, um Justin

9
00:00:21,840 --> 00:00:26,720
is the social scientist, the researcher,

10
00:00:24,160 --> 00:00:28,400
the educational technologist.

11
00:00:26,720 --> 00:00:32,360
uh but it turns out there's a lot of

12
00:00:28,400 --> 00:00:35,440
overlap between qualitative research and

13
00:00:32,360 --> 00:00:37,520
journalism and I want to start with a

14
00:00:35,440 --> 00:00:40,239
anecdote uh as a journalist I tend to

15
00:00:37,520 --> 00:00:43,040
think in terms of narrative in terms of

16
00:00:40,239 --> 00:00:47,120
anecdote so I want to start with an

17
00:00:43,040 --> 00:00:49,039
anecdote that I think expresses why

18
00:00:47,120 --> 00:00:51,680
we're using the term arrival in the

19
00:00:49,039 --> 00:00:53,360
title uh so this is from Steve by the

20
00:00:51,680 --> 00:00:55,600
way I just I really enjoy playing around

21
00:00:53,360 --> 00:00:57,360
with the older generation of dolly to

22
00:00:55,600 --> 00:00:59,320
make my slides sides. So, uh, forgive

23
00:00:57,360 --> 00:01:02,719
me. I just I I find them kind of

24
00:00:59,320 --> 00:01:03,879
hilarious. Uh, Steve is a technology

25
00:01:02,719 --> 00:01:06,320
director,

26
00:01:03,879 --> 00:01:09,040
meaning he doesn't, you know, he's not

27
00:01:06,320 --> 00:01:10,799
just managing laptops and Chromebooks

28
00:01:09,040 --> 00:01:12,320
and things like that at at the school,

29
00:01:10,799 --> 00:01:14,799
although he probably does that, manages

30
00:01:12,320 --> 00:01:17,600
the servers, but he also part of his job

31
00:01:14,799 --> 00:01:20,000
is to help teachers think through how to

32
00:01:17,600 --> 00:01:22,000
use technology to advance their

33
00:01:20,000 --> 00:01:24,000
educational goals. So he's here in

34
00:01:22,000 --> 00:01:28,320
Massachusetts and this is a story he

35
00:01:24,000 --> 00:01:32,159
told about uh November December of 2022.

36
00:01:28,320 --> 00:01:36,560
So I think it was it was December 8th

37
00:01:32,159 --> 00:01:38,320
and I was home sick with COVID. I got an

38
00:01:36,560 --> 00:01:39,840
email. I'm on a list serve, you know,

39
00:01:38,320 --> 00:01:42,040
with all the tech directors in

40
00:01:39,840 --> 00:01:45,360
Massachusetts and I got an email that

41
00:01:42,040 --> 00:01:47,680
said have AI write your next uh English

42
00:01:45,360 --> 00:01:50,240
paper or something. And then uh the

43
00:01:47,680 --> 00:01:52,880
subcaption was buckle up, here it comes.

44
00:01:50,240 --> 00:01:56,720
And I someone had basically shared a

45
00:01:52,880 --> 00:01:59,119
video of this thing called chat GBT that

46
00:01:56,720 --> 00:02:01,399
was generating an essay about I think it

47
00:01:59,119 --> 00:02:05,320
was about raising in the sun and I was

48
00:02:01,399 --> 00:02:09,280
like what is going on

49
00:02:05,320 --> 00:02:11,280
here? So Steve uh sees that email

50
00:02:09,280 --> 00:02:14,080
realizes this is going to be a big deal.

51
00:02:11,280 --> 00:02:17,360
This is, you know, a week or two after

52
00:02:14,080 --> 00:02:18,800
uh chat GPT or GPT3.5

53
00:02:17,360 --> 00:02:21,040
uh dropped and people were just starting

54
00:02:18,800 --> 00:02:23,280
to figure out its capabilities. And so

55
00:02:21,040 --> 00:02:25,520
he wrote an email, in fact, he got chat

56
00:02:23,280 --> 00:02:28,239
GPT to write an email to all the

57
00:02:25,520 --> 00:02:30,319
teachers in that school district uh to

58
00:02:28,239 --> 00:02:32,080
say, "Hey, there's this new tool. Uh it

59
00:02:30,319 --> 00:02:35,040
may end up being very helpful to us as

60
00:02:32,080 --> 00:02:36,640
educators. It can also do uh your kids

61
00:02:35,040 --> 00:02:38,319
homework assignment for them, so you

62
00:02:36,640 --> 00:02:40,160
might want to know about it." uh they

63
00:02:38,319 --> 00:02:42,319
very quickly put together some

64
00:02:40,160 --> 00:02:44,560
professional development

65
00:02:42,319 --> 00:02:45,920
uh gave some guidelines to the teachers

66
00:02:44,560 --> 00:02:47,760
and they've been sort of working to

67
00:02:45,920 --> 00:02:49,440
catch up ever since then but I like this

68
00:02:47,760 --> 00:02:52,400
idea of buckle up here it comes I think

69
00:02:49,440 --> 00:02:54,080
it gets to the idea of uh arrival

70
00:02:52,400 --> 00:02:55,920
technologies versus adoption

71
00:02:54,080 --> 00:02:58,480
technologies and this is something that

72
00:02:55,920 --> 00:03:00,400
Justin and I came up with as far as we

73
00:02:58,480 --> 00:03:04,080
know we originated this idea at least in

74
00:03:00,400 --> 00:03:06,800
the context of education um we may be

75
00:03:04,080 --> 00:03:10,640
wrong about that so um school districts

76
00:03:06,800 --> 00:03:12,879
throughout history. Um, there's been

77
00:03:10,640 --> 00:03:15,360
various technologies present in schools,

78
00:03:12,879 --> 00:03:17,760
in classrooms, things like chalkboards,

79
00:03:15,360 --> 00:03:21,200
pencils, textbooks, all sorts of other

80
00:03:17,760 --> 00:03:22,959
kinds of technology. And generally, um,

81
00:03:21,200 --> 00:03:25,040
most of those technologies that are

82
00:03:22,959 --> 00:03:27,040
present in learning environments are

83
00:03:25,040 --> 00:03:29,120
adopted by the schools, by the school

84
00:03:27,040 --> 00:03:31,519
district, by the teachers. Uh,

85
00:03:29,120 --> 00:03:33,440
educational technologies can cause harm

86
00:03:31,519 --> 00:03:35,920
to educational systems as well as

87
00:03:33,440 --> 00:03:37,920
benefits. Uh they technologies can cause

88
00:03:35,920 --> 00:03:40,000
distraction, academic misconduct,

89
00:03:37,920 --> 00:03:42,400
reduced instructional quality, cyber

90
00:03:40,000 --> 00:03:44,959
bullying. Uh but with technology

91
00:03:42,400 --> 00:03:47,040
adoption uh schools can introduce new

92
00:03:44,959 --> 00:03:49,120
technologies when they are prepared to

93
00:03:47,040 --> 00:03:51,200
manage the risks. They have policies in

94
00:03:49,120 --> 00:03:53,519
place. They have data privacy agreements

95
00:03:51,200 --> 00:03:56,000
in place. They have updated their

96
00:03:53,519 --> 00:03:57,920
academic integrity policy. um they can

97
00:03:56,000 --> 00:04:00,159
make sometimes the wise, important and

98
00:03:57,920 --> 00:04:01,920
prudent and ethical choice not to adopt

99
00:04:00,159 --> 00:04:03,439
a particular technology if they decide

100
00:04:01,920 --> 00:04:06,319
the school's not ready for it or if they

101
00:04:03,439 --> 00:04:08,879
decide the technologies affordances are

102
00:04:06,319 --> 00:04:11,680
not worth the risks or the challenges.

103
00:04:08,879 --> 00:04:14,080
But in the case of arrival technologies

104
00:04:11,680 --> 00:04:16,079
um they bypass the planning, the

105
00:04:14,080 --> 00:04:20,320
assessment, the policym the professional

106
00:04:16,079 --> 00:04:21,759
learning um and uh they crash the party.

107
00:04:20,320 --> 00:04:24,000
they show up and in the case of

108
00:04:21,759 --> 00:04:26,720
something like chat GPT, the students

109
00:04:24,000 --> 00:04:28,800
have access to it irrespective of any

110
00:04:26,720 --> 00:04:30,720
policy that the school district is going

111
00:04:28,800 --> 00:04:33,040
to put in place. They might try to ban

112
00:04:30,720 --> 00:04:36,320
it at the server. Uh but kids with

113
00:04:33,040 --> 00:04:39,040
laptops um will have access to it at

114
00:04:36,320 --> 00:04:41,120
home. Also, uh students are pretty good

115
00:04:39,040 --> 00:04:43,040
at getting around uh bans. They can

116
00:04:41,120 --> 00:04:44,800
figure out how to use things like VPNs.

117
00:04:43,040 --> 00:04:47,360
Maybe they have a phone, maybe they have

118
00:04:44,800 --> 00:04:49,840
two phones uh where they can access chat

119
00:04:47,360 --> 00:04:52,560
GPT. So it's very very hard to ban the

120
00:04:49,840 --> 00:04:54,400
technology. Uh pocket calculators a kind

121
00:04:52,560 --> 00:04:57,280
of canonical

122
00:04:54,400 --> 00:04:59,600
uh arrival technology cliff notes or

123
00:04:57,280 --> 00:05:03,360
spark notes uh the books that students

124
00:04:59,600 --> 00:05:05,680
use to bypass uh the learning and the

125
00:05:03,360 --> 00:05:08,080
thinking their uh their teachers want

126
00:05:05,680 --> 00:05:10,160
them to engage in. Uh smartphones

127
00:05:08,080 --> 00:05:11,520
arguably are something of an arrival

128
00:05:10,160 --> 00:05:12,960
technology. And actually many many

129
00:05:11,520 --> 00:05:14,880
school districts right now are in the

130
00:05:12,960 --> 00:05:18,560
process of thinking about whether they

131
00:05:14,880 --> 00:05:20,880
need to restrict uh mobile phones. Um so

132
00:05:18,560 --> 00:05:22,880
we wanted to understand how teachers are

133
00:05:20,880 --> 00:05:25,680
experiencing all of this and also

134
00:05:22,880 --> 00:05:27,440
students and actually we started with a

135
00:05:25,680 --> 00:05:30,960
grant from the Spencer Foundation in

136
00:05:27,440 --> 00:05:32,880
Chicago and then uh the CIRC uh grant

137
00:05:30,960 --> 00:05:35,199
really allowed us to expand what we were

138
00:05:32,880 --> 00:05:39,360
doing to diversify our cohort to

139
00:05:35,199 --> 00:05:42,479
diversify geographically. Um so we

140
00:05:39,360 --> 00:05:44,000
talked to I think yeah 85ish uh we're

141
00:05:42,479 --> 00:05:45,919
always doing a couple more interviews. I

142
00:05:44,000 --> 00:05:49,759
think we did a few more this week. uh

143
00:05:45,919 --> 00:05:51,680
teachers about 40 students um we didn't

144
00:05:49,759 --> 00:05:53,360
want to just ask them hey what do you

145
00:05:51,680 --> 00:05:54,639
think about chat GPT is it making your

146
00:05:53,360 --> 00:05:56,080
life great is it making your life

147
00:05:54,639 --> 00:05:57,600
terrible we wanted to understand what

148
00:05:56,080 --> 00:05:59,680
was top of mind for them before

149
00:05:57,600 --> 00:06:00,960
introducing the concept of AI so you

150
00:05:59,680 --> 00:06:03,120
know what's your school like what's your

151
00:06:00,960 --> 00:06:05,199
job like what's going on what are your

152
00:06:03,120 --> 00:06:07,840
top challenges right now sometimes they

153
00:06:05,199 --> 00:06:10,080
would bring up AI chat GPT in this list

154
00:06:07,840 --> 00:06:12,160
sometimes they wouldn't um and then

155
00:06:10,080 --> 00:06:14,400
after we got sort of that baseline top

156
00:06:12,160 --> 00:06:15,680
of mind information we would ask if they

157
00:06:14,400 --> 00:06:18,800
had noticed AI

158
00:06:15,680 --> 00:06:20,720
in their school. How is it being used by

159
00:06:18,800 --> 00:06:22,319
whom are students using it? Are teachers

160
00:06:20,720 --> 00:06:23,759
using it? Are you using it? Are you

161
00:06:22,319 --> 00:06:25,600
using it to support learning and

162
00:06:23,759 --> 00:06:27,360
education? Are students using it in ways

163
00:06:25,600 --> 00:06:30,319
that you approve of? Are students using

164
00:06:27,360 --> 00:06:32,880
it in ways that you disapprove of? And I

165
00:06:30,319 --> 00:06:35,199
would say that probably the most common

166
00:06:32,880 --> 00:06:37,199
story we got is something like this. I

167
00:06:35,199 --> 00:06:38,639
think probably we're still coding the

168
00:06:37,199 --> 00:06:39,919
data, but I would just estimate

169
00:06:38,639 --> 00:06:41,600
something like threequarters of the

170
00:06:39,919 --> 00:06:43,520
teachers we spoke to had a story like

171
00:06:41,600 --> 00:06:45,520
Alec who's an eighth grade social

172
00:06:43,520 --> 00:06:47,199
studies teacher in Chicago. So, you

173
00:06:45,520 --> 00:06:50,720
know, if I'm used to looking at a

174
00:06:47,199 --> 00:06:52,319
student's classwork and uh they sound

175
00:06:50,720 --> 00:06:53,840
very much like an eighth grader, you

176
00:06:52,319 --> 00:06:56,080
know, in the way that they structure

177
00:06:53,840 --> 00:06:59,120
their sentences, the vocabulary that

178
00:06:56,080 --> 00:07:01,039
they pick out, and then I, you know, I

179
00:06:59,120 --> 00:07:04,160
receive an assignment, most likely an

180
00:07:01,039 --> 00:07:05,599
assignment done at home that is, you

181
00:07:04,160 --> 00:07:07,919
know, you can pick it out almost

182
00:07:05,599 --> 00:07:10,639
instantly. Uh, you know, the the

183
00:07:07,919 --> 00:07:12,080
manifold differences between the south

184
00:07:10,639 --> 00:07:13,840
and the north, you know, during the

185
00:07:12,080 --> 00:07:15,840
reconstruction prompted, you know,

186
00:07:13,840 --> 00:07:18,240
whatever. it's it's not it's not eighth

187
00:07:15,840 --> 00:07:20,880
grade language or it's not these

188
00:07:18,240 --> 00:07:22,479
students eighth grade language. And so,

189
00:07:20,880 --> 00:07:24,240
uh, that's something that I would

190
00:07:22,479 --> 00:07:26,479
probably have a conversation with the

191
00:07:24,240 --> 00:07:29,039
student and I would say, "Hey, look, you

192
00:07:26,479 --> 00:07:31,599
know, um, this doesn't sound like your

193
00:07:29,039 --> 00:07:33,680
usual voice. Uh, when I looked in your

194
00:07:31,599 --> 00:07:36,000
edit history, I saw that this paragraph

195
00:07:33,680 --> 00:07:38,240
had just kind of popped up in, you know,

196
00:07:36,000 --> 00:07:41,199
in a split second. You know, can you

197
00:07:38,240 --> 00:07:42,960
explain to me how that happened?" Going

198
00:07:41,199 --> 00:07:44,240
to move to the next one. Uh you missed

199
00:07:42,960 --> 00:07:46,960
unfortunately when the audio dropped

200
00:07:44,240 --> 00:07:49,520
out, you missed the kind of hilarious

201
00:07:46,960 --> 00:07:52,000
uh um the manifold differences between

202
00:07:49,520 --> 00:07:54,479
the north and south uh during the uh

203
00:07:52,000 --> 00:07:56,319
postbell or antibbellum period led

204
00:07:54,479 --> 00:07:57,759
inevitably to the conflict. You know

205
00:07:56,319 --> 00:07:59,840
that there was this very kind of

206
00:07:57,759 --> 00:08:01,039
academic language used used by an eighth

207
00:07:59,840 --> 00:08:02,639
grader who probably doesn't normally

208
00:08:01,039 --> 00:08:04,720
speak that way. Uh we got a lot of

209
00:08:02,639 --> 00:08:06,800
stories like that. I think most teachers

210
00:08:04,720 --> 00:08:08,240
in K12 schools have experienced

211
00:08:06,800 --> 00:08:10,240
something like that. We also got a lot

212
00:08:08,240 --> 00:08:12,479
of stories like that from students uh

213
00:08:10,240 --> 00:08:14,720
some of whom um well they were on

214
00:08:12,479 --> 00:08:15,919
different sides of uh of sometimes

215
00:08:14,720 --> 00:08:17,199
they're talking about what they're

216
00:08:15,919 --> 00:08:18,800
doing, sometimes they're talking about

217
00:08:17,199 --> 00:08:20,160
what their fellow students are doing,

218
00:08:18,800 --> 00:08:21,680
sometimes they're concerned about it,

219
00:08:20,160 --> 00:08:23,599
sometimes they're excited to have a new

220
00:08:21,680 --> 00:08:24,800
tool that um you know allows them to get

221
00:08:23,599 --> 00:08:27,120
their homework done quickly so they can

222
00:08:24,800 --> 00:08:29,280
go hang out with their friends. Um we

223
00:08:27,120 --> 00:08:31,680
also heard some stories like this one.

224
00:08:29,280 --> 00:08:33,680
This is Eric Timmons. Um, and he's

225
00:08:31,680 --> 00:08:36,080
talking about he'll start by talking

226
00:08:33,680 --> 00:08:37,839
about the challenge of generating uh

227
00:08:36,080 --> 00:08:39,680
discussion questions and other kind of

228
00:08:37,839 --> 00:08:41,360
curriculum for his students who are film

229
00:08:39,680 --> 00:08:44,000
students. So, he's often getting them to

230
00:08:41,360 --> 00:08:45,760
watch uh movies and that or read things

231
00:08:44,000 --> 00:08:47,519
that aren't in the textbooks and he

232
00:08:45,760 --> 00:08:50,160
wants them to engage in certain kinds of

233
00:08:47,519 --> 00:08:51,600
discussions. I'll be honest, like as a

234
00:08:50,160 --> 00:08:52,959
teacher, there's like sometimes a

235
00:08:51,600 --> 00:08:55,200
feeling of like, well, you know what,

236
00:08:52,959 --> 00:08:56,880
like I'm not a curriculum designer.

237
00:08:55,200 --> 00:08:59,519
Like, should I be going and doing all of

238
00:08:56,880 --> 00:09:00,399
this work for free, you know? Um, but at

239
00:08:59,519 --> 00:09:01,680
the end of the day, it's not going to

240
00:09:00,399 --> 00:09:05,440
happen. So, I have to do it, right?

241
00:09:01,680 --> 00:09:08,240
Right. So, uh, um, so I, uh, um, being

242
00:09:05,440 --> 00:09:10,560
able to work with the smartest

243
00:09:08,240 --> 00:09:13,120
curriculum designer that's ever existed

244
00:09:10,560 --> 00:09:15,920
and have that at at my disposal, it

245
00:09:13,120 --> 00:09:17,600
means GPT. And being able to take

246
00:09:15,920 --> 00:09:20,320
designer that's ever, Project Zero's

247
00:09:17,600 --> 00:09:24,240
thinking routines and apply it to like

248
00:09:20,320 --> 00:09:26,720
every assignment that I have is

249
00:09:24,240 --> 00:09:28,800
remarkable. I remember I was at a PD and

250
00:09:26,720 --> 00:09:31,120
someone introduced it. I'll stop that

251
00:09:28,800 --> 00:09:32,839
there. Um, one thing I'll say about Eric

252
00:09:31,120 --> 00:09:35,440
Timonss, and I think this is a general

253
00:09:32,839 --> 00:09:37,760
observation. I think that somebody like

254
00:09:35,440 --> 00:09:39,839
him is able to use chat GPT effectively

255
00:09:37,760 --> 00:09:41,120
to design discussion questions for his

256
00:09:39,839 --> 00:09:42,560
students because he already knows how to

257
00:09:41,120 --> 00:09:44,720
do that. He already has that domain

258
00:09:42,560 --> 00:09:46,240
knowledge. Uh, that's just an aside, but

259
00:09:44,720 --> 00:09:48,720
uh, an aside, but that's another pattern

260
00:09:46,240 --> 00:09:50,320
we've noticed. Um, and from the

261
00:09:48,720 --> 00:09:52,000
students, this is probably the most

262
00:09:50,320 --> 00:09:53,920
extreme story, but I find it kind of

263
00:09:52,000 --> 00:09:55,440
amusing. Uh, Woody is an eighth grader

264
00:09:53,920 --> 00:09:57,040
in the Northeast. We talked to him about

265
00:09:55,440 --> 00:09:59,760
a year ago. I think he's in nth grade

266
00:09:57,040 --> 00:10:02,000
now. He did he he described his science

267
00:09:59,760 --> 00:10:04,480
class moving through topics more quickly

268
00:10:02,000 --> 00:10:06,080
than was comfortable to him because he

269
00:10:04,480 --> 00:10:09,040
felt like his teacher was getting an

270
00:10:06,080 --> 00:10:11,120
illusion like a mirage uh in terms of

271
00:10:09,040 --> 00:10:12,560
what the students understood and he

272
00:10:11,120 --> 00:10:15,200
thought this because he sits in the back

273
00:10:12,560 --> 00:10:16,480
and he watches his fellow students type

274
00:10:15,200 --> 00:10:18,240
prompts that the teachers are giving

275
00:10:16,480 --> 00:10:20,040
them in the chat GPT and the teacher

276
00:10:18,240 --> 00:10:22,839
can't see but he can. If you had to

277
00:10:20,040 --> 00:10:26,800
estimate, say in a classroom of 20

278
00:10:22,839 --> 00:10:28,320
students, how many were using chat GPT

279
00:10:26,800 --> 00:10:31,279
to cheat the way that you're describing,

280
00:10:28,320 --> 00:10:34,800
how many would you say? Okay. So, I'd

281
00:10:31,279 --> 00:10:37,040
say that there's 10 people in that class

282
00:10:34,800 --> 00:10:40,240
using it for everything, like cheating

283
00:10:37,040 --> 00:10:42,959
on the whole paper is AI. I'd say

284
00:10:40,240 --> 00:10:45,040
there's another five that probably half

285
00:10:42,959 --> 00:10:47,680
of it's written by AI, but they do

286
00:10:45,040 --> 00:10:49,279
actually read it through and go, gee,

287
00:10:47,680 --> 00:10:51,760
maybe I don't want to include the part

288
00:10:49,279 --> 00:10:54,480
that says as a large language model, but

289
00:10:51,760 --> 00:10:57,440
they like read it through and copy parts

290
00:10:54,480 --> 00:11:00,959
and splice bits and do whatever. Then

291
00:10:57,440 --> 00:11:04,240
I'd say of So you've got five remaining.

292
00:11:00,959 --> 00:11:06,240
I'd say probably four of that five do

293
00:11:04,240 --> 00:11:09,720
the paper legitimately

294
00:11:06,240 --> 00:11:12,320
and then there's another one

295
00:11:09,720 --> 00:11:14,320
that's going and I'll skip the rest of

296
00:11:12,320 --> 00:11:15,839
the of of his answer just what he's

297
00:11:14,320 --> 00:11:18,079
going to say is that there might be

298
00:11:15,839 --> 00:11:19,440
students who are using chat GPT to

299
00:11:18,079 --> 00:11:22,000
supplement their work maybe as a

300
00:11:19,440 --> 00:11:24,320
research tool or something like that. Um

301
00:11:22,000 --> 00:11:26,160
so overall lots of concerns about

302
00:11:24,320 --> 00:11:27,839
cheating or bypassing learning lots of

303
00:11:26,160 --> 00:11:30,399
discussions of enforcement cat-and-

304
00:11:27,839 --> 00:11:32,320
mouse games. There are some examples of

305
00:11:30,399 --> 00:11:34,320
teachers saying I I is helping them

306
00:11:32,320 --> 00:11:36,480
support their work as teachers. I would

307
00:11:34,320 --> 00:11:38,880
say it's probably twothirds the top one

308
00:11:36,480 --> 00:11:40,640
to maybe one-third. Um we're we're

309
00:11:38,880 --> 00:11:42,800
coding the data. That's my sense of it

310
00:11:40,640 --> 00:11:44,880
right now. Uh some examples of AI

311
00:11:42,800 --> 00:11:46,959
helping with administrative tasks. Uh

312
00:11:44,880 --> 00:11:49,360
teachers role model modeling uses of AI

313
00:11:46,959 --> 00:11:51,519
by students to support um student

314
00:11:49,360 --> 00:11:52,959
learning. Uh perplexity use perplexity

315
00:11:51,519 --> 00:11:54,880
as a research tool. It searches the

316
00:11:52,959 --> 00:11:57,040
internet. That's a good example of that.

317
00:11:54,880 --> 00:12:00,560
Um some experimentation with AI powered

318
00:11:57,040 --> 00:12:01,920
edtech. Um and uh most teachers feel

319
00:12:00,560 --> 00:12:03,600
that the support and guidance from the

320
00:12:01,920 --> 00:12:05,760
school and district has been inadequate.

321
00:12:03,600 --> 00:12:09,200
We fielded a study with Rand last fall

322
00:12:05,760 --> 00:12:12,399
with their national teacher panel. They

323
00:12:09,200 --> 00:12:14,959
talked to a thousand or so uh teachers.

324
00:12:12,399 --> 00:12:16,959
25% said they've gotten any guidance or

325
00:12:14,959 --> 00:12:19,200
support with regard to the arrival of

326
00:12:16,959 --> 00:12:21,839
generative AI in this is in public

327
00:12:19,200 --> 00:12:23,279
education in the United States. 5% say

328
00:12:21,839 --> 00:12:25,760
the guidance or support that they've

329
00:12:23,279 --> 00:12:28,680
gotten is adequate or good. Uh that's a

330
00:12:25,760 --> 00:12:31,279
year ago, so it may have gotten better.

331
00:12:28,680 --> 00:12:33,920
um lots of lots of examples of building

332
00:12:31,279 --> 00:12:36,079
the plane as they fly it. Uh ethical

333
00:12:33,920 --> 00:12:37,920
challenges in the age of generative AI.

334
00:12:36,079 --> 00:12:40,000
The two at the top are the ones that I

335
00:12:37,920 --> 00:12:42,079
would say we found evidence of uh

336
00:12:40,000 --> 00:12:45,519
students bypassing learning. The teacher

337
00:12:42,079 --> 00:12:47,200
has designed a activity, homework, you

338
00:12:45,519 --> 00:12:48,720
name it. Something like that for the

339
00:12:47,200 --> 00:12:51,760
students in order to get them to think

340
00:12:48,720 --> 00:12:54,560
in a certain way so they will learn and

341
00:12:51,760 --> 00:12:56,000
they use AI to bypass that thinking. How

342
00:12:54,560 --> 00:12:57,839
big of a problem is this? We're not

343
00:12:56,000 --> 00:12:59,360
really sure. There's there's been some

344
00:12:57,839 --> 00:13:01,360
disputes on it. I can talk about the

345
00:12:59,360 --> 00:13:03,519
Stanford study and what the latest

346
00:13:01,360 --> 00:13:05,120
research shows. We don't know. Seems

347
00:13:03,519 --> 00:13:06,399
like it's a problem. Teachers think it's

348
00:13:05,120 --> 00:13:08,480
a problem. Students think it's a

349
00:13:06,399 --> 00:13:10,160
problem. False accusations, harsh

350
00:13:08,480 --> 00:13:12,200
harsher discipline. I have a slide I'll

351
00:13:10,160 --> 00:13:15,519
share about that.

352
00:13:12,200 --> 00:13:17,920
Um, students who might present as less

353
00:13:15,519 --> 00:13:20,320
academic to their teacher are less

354
00:13:17,920 --> 00:13:24,240
likely to be trusted or more likely to

355
00:13:20,320 --> 00:13:27,440
be accused of using AI to get schoolwork

356
00:13:24,240 --> 00:13:30,240
done. Um and that can have consequences.

357
00:13:27,440 --> 00:13:31,839
Uh another ethical issue, um are the

358
00:13:30,240 --> 00:13:34,240
students who are using AI actually

359
00:13:31,839 --> 00:13:36,079
getting better grades? Um what if if

360
00:13:34,240 --> 00:13:38,480
that's the case, what does that uh teach

361
00:13:36,079 --> 00:13:41,440
our youth? Um sorry I didn't word this

362
00:13:38,480 --> 00:13:43,200
in more um researchy phrase, but we're

363
00:13:41,440 --> 00:13:44,560
already starting to see crappy AI for

364
00:13:43,200 --> 00:13:46,240
poor kids. We're already starting to see

365
00:13:44,560 --> 00:13:48,800
differentiation between the models that

366
00:13:46,240 --> 00:13:50,800
we pay for and the models that we get

367
00:13:48,800 --> 00:13:53,440
for free. So, if AI is in fact going to

368
00:13:50,800 --> 00:13:55,760
be useful for students and uh for

369
00:13:53,440 --> 00:13:57,199
teachers, who's going to get the good

370
00:13:55,760 --> 00:13:59,839
models and who's going to get the free

371
00:13:57,199 --> 00:14:02,000
models? Um AI replacing human

372
00:13:59,839 --> 00:14:05,160
instruction. We're not seeing that yet,

373
00:14:02,000 --> 00:14:07,680
although people are talking about it.

374
00:14:05,160 --> 00:14:10,000
Um I'm going to skip this because I'm

375
00:14:07,680 --> 00:14:11,760
running out of time. Uh black teens are

376
00:14:10,000 --> 00:14:13,600
more than twice as likely as white or

377
00:14:11,760 --> 00:14:14,959
Latino teens to say that teachers

378
00:14:13,600 --> 00:14:16,720
flagged their schoolwork as being

379
00:14:14,959 --> 00:14:20,000
created by generative AI when it was

380
00:14:16,720 --> 00:14:21,959
not. That's a result from a common sense

381
00:14:20,000 --> 00:14:24,320
media survey in

382
00:14:21,959 --> 00:14:25,839
2024. Um, that's something we're

383
00:14:24,320 --> 00:14:28,300
interested in trying to find out more

384
00:14:25,839 --> 00:14:29,959
about. Um,

385
00:14:28,300 --> 00:14:33,040
[Music]

386
00:14:29,959 --> 00:14:34,320
and yeah, I already mentioned this. I

387
00:14:33,040 --> 00:14:36,160
just want to make I I think our

388
00:14:34,320 --> 00:14:37,760
hypothesis is though that the ethical

389
00:14:36,160 --> 00:14:39,440
concerns we talked about will be better

390
00:14:37,760 --> 00:14:41,440
addressed by schools that have made an

391
00:14:39,440 --> 00:14:45,120
effort to respond coherently. To me, the

392
00:14:41,440 --> 00:14:46,639
fact that teachers say only 25 only 25%

393
00:14:45,120 --> 00:14:48,720
of teachers say that their district has

394
00:14:46,639 --> 00:14:51,279
given them any policy guidance when it

395
00:14:48,720 --> 00:14:52,880
comes to AI to me is shockingly low.

396
00:14:51,279 --> 00:14:54,480
Although, the more time I've spent

397
00:14:52,880 --> 00:14:57,199
talking to public school teachers, the

398
00:14:54,480 --> 00:14:58,480
less shocked I am by that statistic. Uh,

399
00:14:57,199 --> 00:15:00,160
we also think that districts with

400
00:14:58,480 --> 00:15:01,839
wealthier students and more resources

401
00:15:00,160 --> 00:15:03,600
are more likely to have put clear,

402
00:15:01,839 --> 00:15:06,480
coherent, and wise, responsible AI

403
00:15:03,600 --> 00:15:08,079
policies in place. That's my sense of

404
00:15:06,480 --> 00:15:10,800
the data, but again, we're still kind of

405
00:15:08,079 --> 00:15:12,560
coding it for that. Um, so we have an

406
00:15:10,800 --> 00:15:14,639
article we're submitting to the British

407
00:15:12,560 --> 00:15:17,120
Journal of Education Technology this

408
00:15:14,639 --> 00:15:18,320
week. Um, to me the signature thing that

409
00:15:17,120 --> 00:15:20,920
we're working on because I'm a

410
00:15:18,320 --> 00:15:23,360
journalist and a podcast producer is a

411
00:15:20,920 --> 00:15:25,040
miniseries scripted narrated podcast

412
00:15:23,360 --> 00:15:27,040
within the Teach Lab podcast called The

413
00:15:25,040 --> 00:15:28,880
Homework Machine. That's coming out in

414
00:15:27,040 --> 00:15:30,880
July that will feature many more voices

415
00:15:28,880 --> 00:15:33,279
like the ones I shared with you, plus

416
00:15:30,880 --> 00:15:37,440
some analysis. We're also maybe going to

417
00:15:33,279 --> 00:15:40,560
make a responsible AI policy handbook

418
00:15:37,440 --> 00:15:42,560
for teachers or schools or districts who

419
00:15:40,560 --> 00:15:45,040
don't have a policy in place and would

420
00:15:42,560 --> 00:15:46,720
like some guidance. Um, and we're

421
00:15:45,040 --> 00:15:48,639
thinking that maybe the next thing is to

422
00:15:46,720 --> 00:15:49,839
look at schools that are adopting AI

423
00:15:48,639 --> 00:15:51,680
powered edtech because there's a lot of

424
00:15:49,839 --> 00:15:53,120
that going on out there. But I will stop

425
00:15:51,680 --> 00:15:56,230
there and I'd love to take some

426
00:15:53,120 --> 00:16:01,120
questions if we have time.

427
00:15:56,230 --> 00:16:02,959
[Applause]

428
00:16:01,120 --> 00:16:06,360
Thank you, Jesse. There is absolutely

429
00:16:02,959 --> 00:16:06,360
time for questions.

430
00:16:10,320 --> 00:16:15,279
So, I I just wondering if you know based

431
00:16:12,800 --> 00:16:17,600
on your talk, it's about those uh um

432
00:16:15,279 --> 00:16:19,519
pre-ol students. Do you think that those

433
00:16:17,600 --> 00:16:22,160
students doing that right now will bring

434
00:16:19,519 --> 00:16:23,600
this into college and into the graduate

435
00:16:22,160 --> 00:16:25,759
school?

436
00:16:23,600 --> 00:16:27,360
My understanding talking to college

437
00:16:25,759 --> 00:16:30,399
professors is there's quite a bit of

438
00:16:27,360 --> 00:16:35,680
bypassing learning that is happening at

439
00:16:30,399 --> 00:16:37,920
the college level. Um Justin thinks so

440
00:16:35,680 --> 00:16:41,399
if we have chat GPT is released you know

441
00:16:37,920 --> 00:16:45,600
GPT 3.5 in December

442
00:16:41,399 --> 00:16:47,759
2022 that we might notice something when

443
00:16:45,600 --> 00:16:50,399
a gener when students who have spent

444
00:16:47,759 --> 00:16:53,120
their entire four years of high school

445
00:16:50,399 --> 00:16:55,279
uh relying on chat GPT to do say writing

446
00:16:53,120 --> 00:16:59,839
assignments and that sort of thing show

447
00:16:55,279 --> 00:17:02,160
up uh in higher ed that we might notice

448
00:16:59,839 --> 00:17:03,759
it it'll be anecdotal because We don't

449
00:17:02,160 --> 00:17:05,760
necessarily test these things, but we

450
00:17:03,759 --> 00:17:07,880
might just notice some real diminished

451
00:17:05,760 --> 00:17:10,400
capacity for reading and

452
00:17:07,880 --> 00:17:11,520
writing. Students have expressed to me

453
00:17:10,400 --> 00:17:13,520
that they're worried about that

454
00:17:11,520 --> 00:17:15,600
possibility. I talked with a student who

455
00:17:13,520 --> 00:17:17,199
said that she's been relying on chat GPT

456
00:17:15,600 --> 00:17:19,199
to do a lot of her essay writing for the

457
00:17:17,199 --> 00:17:21,280
last two or three years and she's

458
00:17:19,199 --> 00:17:22,720
nervous. She she likes writing. Uh she

459
00:17:21,280 --> 00:17:24,319
likes reading and she's nervous that

460
00:17:22,720 --> 00:17:26,480
she's relying on it too much and she's

461
00:17:24,319 --> 00:17:27,600
losing her ability. U but we're not

462
00:17:26,480 --> 00:17:29,200
really paying attention to what's

463
00:17:27,600 --> 00:17:31,280
happening in higher ed. So anything I

464
00:17:29,200 --> 00:17:33,120
know about that is anecdotal. Um but

465
00:17:31,280 --> 00:17:36,240
from what I understand it's something

466
00:17:33,120 --> 00:17:37,840
that professors are aware of. Uh I want

467
00:17:36,240 --> 00:17:39,880
to ask about thank you for this this

468
00:17:37,840 --> 00:17:41,880
wonderful talk. I want to ask about a po

469
00:17:39,880 --> 00:17:45,440
popular

470
00:17:41,880 --> 00:17:48,000
possible positive effect of AI uh from

471
00:17:45,440 --> 00:17:51,520
the students side which is kind of an

472
00:17:48,000 --> 00:17:54,080
always available very patient tutor that

473
00:17:51,520 --> 00:17:56,000
you can ask questions of. Did you hear

474
00:17:54,080 --> 00:17:58,320
any of that coming out of your student

475
00:17:56,000 --> 00:18:00,240
interviews that they were Yes. they felt

476
00:17:58,320 --> 00:18:02,720
more comfortable asking the AI questions

477
00:18:00,240 --> 00:18:04,960
or that they didn't need a tutor or they

478
00:18:02,720 --> 00:18:08,320
stopped their parents didn't hire a

479
00:18:04,960 --> 00:18:10,480
tutor or anything like that? I think

480
00:18:08,320 --> 00:18:12,400
there are stories about that and I think

481
00:18:10,480 --> 00:18:14,559
part of I mentioned this idea of

482
00:18:12,400 --> 00:18:17,120
researching the schools adopting AI

483
00:18:14,559 --> 00:18:19,039
powered edtech. My sense right now, this

484
00:18:17,120 --> 00:18:21,120
is just a hypothesis, but from talking

485
00:18:19,039 --> 00:18:22,640
both to students and from talking to

486
00:18:21,120 --> 00:18:24,240
some teachers who are experimenting with

487
00:18:22,640 --> 00:18:27,559
AI to support that the work they're

488
00:18:24,240 --> 00:18:30,960
doing, that the most

489
00:18:27,559 --> 00:18:33,520
innovative examples of teachers or

490
00:18:30,960 --> 00:18:34,760
students, but students using AI to

491
00:18:33,520 --> 00:18:38,160
support their

492
00:18:34,760 --> 00:18:40,440
learning. Those are not happening with

493
00:18:38,160 --> 00:18:42,720
these companies that are creating

494
00:18:40,440 --> 00:18:44,960
purpose-built purpose-built AI powered

495
00:18:42,720 --> 00:18:47,120
edtech like Conigo, Magic School or

496
00:18:44,960 --> 00:18:48,720
School AI. It's students who have gotten

497
00:18:47,120 --> 00:18:51,520
very comfortable and familiar with chat

498
00:18:48,720 --> 00:18:53,039
GPT playing around with chat GPT and

499
00:18:51,520 --> 00:18:54,559
prompting it to be the kind of tutor

500
00:18:53,039 --> 00:18:55,919
that they want in that particular

501
00:18:54,559 --> 00:18:57,600
moment. So, I've talked with a lot of

502
00:18:55,919 --> 00:19:00,960
students who talk about using it to

503
00:18:57,600 --> 00:19:02,880
prep, study for tests, practice writing

504
00:19:00,960 --> 00:19:05,520
essays, that sort of thing. There's a

505
00:19:02,880 --> 00:19:07,640
great story from a young woman, I think

506
00:19:05,520 --> 00:19:09,760
a 12th grader, who described herself as

507
00:19:07,640 --> 00:19:11,520
dyslexic, who got a writing assignment

508
00:19:09,760 --> 00:19:12,799
that she found very intimidating and

509
00:19:11,520 --> 00:19:14,240
challenging. And she went to her

510
00:19:12,799 --> 00:19:16,160
teacher, who she had a very trusting

511
00:19:14,240 --> 00:19:18,880
relationship with, and say, "Look, can I

512
00:19:16,160 --> 00:19:21,200
use Chachi PT to get some ideas for

513
00:19:18,880 --> 00:19:23,520
this? I'll do the writing, but I'm stuck

514
00:19:21,200 --> 00:19:25,440
and I'm kind of intimidated by getting

515
00:19:23,520 --> 00:19:27,360
started." and they had a discussion and

516
00:19:25,440 --> 00:19:29,200
together they came up with a protocol

517
00:19:27,360 --> 00:19:30,880
where she could use chat GPT to kind of

518
00:19:29,200 --> 00:19:33,360
help her break the ice of the project

519
00:19:30,880 --> 00:19:35,200
that she felt intimidated by but he

520
00:19:33,360 --> 00:19:37,520
still felt like he was getting his

521
00:19:35,200 --> 00:19:38,640
learning goals across uh in terms of how

522
00:19:37,520 --> 00:19:40,640
she did that assignment. I thought that

523
00:19:38,640 --> 00:19:42,160
was a great story. I also thought what

524
00:19:40,640 --> 00:19:45,280
was kind of powerful and important about

525
00:19:42,160 --> 00:19:46,960
that is the social dimension to it. She

526
00:19:45,280 --> 00:19:49,840
she told me she probably wouldn't have

527
00:19:46,960 --> 00:19:50,880
asked that teacher that question if she

528
00:19:49,840 --> 00:19:52,559
didn't already have a pretty good

529
00:19:50,880 --> 00:19:54,720
relationship trusting relationship with

530
00:19:52,559 --> 00:19:57,559
him too. And I think that's vice versa.

531
00:19:54,720 --> 00:20:00,000
So you see this example

532
00:19:57,559 --> 00:20:02,160
of the social relationships between

533
00:20:00,000 --> 00:20:05,280
teachers and students mediating

534
00:20:02,160 --> 00:20:06,880
responsible uses of AI and those two

535
00:20:05,280 --> 00:20:08,160
parties working that out together. And

536
00:20:06,880 --> 00:20:09,840
I've heard of a few other stories like

537
00:20:08,160 --> 00:20:11,440
that too. I I mean I think I think

538
00:20:09,840 --> 00:20:13,760
there's tremendous potential for

539
00:20:11,440 --> 00:20:16,400
positive use of AI in education and many

540
00:20:13,760 --> 00:20:19,440
many teachers think so too. I mean I if

541
00:20:16,400 --> 00:20:21,240
you get anything from the talk it's that

542
00:20:19,440 --> 00:20:24,160
this technology which is quite

543
00:20:21,240 --> 00:20:26,240
disruptive hit very suddenly and schools

544
00:20:24,160 --> 00:20:28,000
do not adapt very very quickly and they

545
00:20:26,240 --> 00:20:30,960
weren't necessarily prepared and that's

546
00:20:28,000 --> 00:20:35,320
where a lot of the harm comes from.

547
00:20:30,960 --> 00:20:35,320
Thank you. One last question.

548
00:20:35,840 --> 00:20:40,880
Hi. Yes. Thank you so much for this

549
00:20:37,440 --> 00:20:42,480
talk. Um, I was curious, um, although

550
00:20:40,880 --> 00:20:46,480
I'm not a big fan of things like

551
00:20:42,480 --> 00:20:49,760
standardized tests, um, I was curious if

552
00:20:46,480 --> 00:20:52,000
there was any impact on like average,

553
00:20:49,760 --> 00:20:54,799
uh, like scores and those kinds of like

554
00:20:52,000 --> 00:20:56,559
state-run tests or things like the SAT

555
00:20:54,799 --> 00:21:00,000
um, since the advent of things like

556
00:20:56,559 --> 00:21:02,960
CHPT. It's a really good question. It's

557
00:21:00,000 --> 00:21:04,799
better question for Justin than for me.

558
00:21:02,960 --> 00:21:06,240
uh I really have focused on talking with

559
00:21:04,799 --> 00:21:08,240
the teachers and he's really the

560
00:21:06,240 --> 00:21:11,280
education researcher. I think you can

561
00:21:08,240 --> 00:21:12,799
find his email uh online and he'll he'll

562
00:21:11,280 --> 00:21:15,280
respond to it if you want to ask him or

563
00:21:12,799 --> 00:21:17,360
I can try to remember to ask him. My

564
00:21:15,280 --> 00:21:19,520
sense of this though is that

565
00:21:17,360 --> 00:21:21,679
standardized testing is really murky

566
00:21:19,520 --> 00:21:24,240
right now because there are other crises

567
00:21:21,679 --> 00:21:27,520
happening within education emerging from

568
00:21:24,240 --> 00:21:30,400
a pandemic that really did have bad

569
00:21:27,520 --> 00:21:32,240
impacts on learning particularly things

570
00:21:30,400 --> 00:21:35,120
like reading and math particularly for

571
00:21:32,240 --> 00:21:36,880
younger students um who if you think

572
00:21:35,120 --> 00:21:38,720
about a three-year-old trying to learn

573
00:21:36,880 --> 00:21:41,120
reading and math over Zoom that's

574
00:21:38,720 --> 00:21:42,880
particularly challenging maybe 11th

575
00:21:41,120 --> 00:21:44,720
grader sorry not three-year-old a third

576
00:21:42,880 --> 00:21:46,640
grader it would be very hard for a

577
00:21:44,720 --> 00:21:47,840
three-year-old. Um, but if you think of

578
00:21:46,640 --> 00:21:49,600
a yeah, like you know, like a

579
00:21:47,840 --> 00:21:53,360
10-year-old or a nine-year-old uh trying

580
00:21:49,600 --> 00:21:54,720
to stay engaged in a Zoom math class uh

581
00:21:53,360 --> 00:21:57,280
a couple years ago, that would have been

582
00:21:54,720 --> 00:22:00,240
very hard. Um, so that's a complicated

583
00:21:57,280 --> 00:22:02,480
factor. Um, I do think we've seen scores

584
00:22:00,240 --> 00:22:04,400
uh drop in the last few years, but it's

585
00:22:02,480 --> 00:22:06,799
hard to know if that's pandemic. We also

586
00:22:04,400 --> 00:22:09,120
are in the middle of a attendance crisis

587
00:22:06,799 --> 00:22:10,480
in schools right now, too. Um, and

588
00:22:09,120 --> 00:22:11,760
schools, our public schools in

589
00:22:10,480 --> 00:22:13,440
particular, are just generally

590
00:22:11,760 --> 00:22:16,240
belleaguered. teacher burnout has never

591
00:22:13,440 --> 00:22:18,799
been higher. So, it's kind of hard to

592
00:22:16,240 --> 00:22:20,240
figure out, well, what's what's AI?

593
00:22:18,799 --> 00:22:22,799
What's one of these other historical

594
00:22:20,240 --> 00:22:27,320
factors?

595
00:22:22,799 --> 00:22:27,320
Thank you very much. This great

