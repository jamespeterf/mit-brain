1
00:00:00,640 --> 00:00:07,720
so I am one of those people that uh Andy

2
00:00:04,160 --> 00:00:10,679
talked about who left her area of

3
00:00:07,720 --> 00:00:15,240
education and expertise namely computer

4
00:00:10,679 --> 00:00:18,160
science and AI because I felt that at um

5
00:00:15,240 --> 00:00:22,920
the place where I was then the AI Lab at

6
00:00:18,160 --> 00:00:25,400
MIT I couldn't quite combine my passion

7
00:00:22,920 --> 00:00:28,400
with that field that I was working in

8
00:00:25,400 --> 00:00:30,519
the field of engineering and Ai and that

9
00:00:28,400 --> 00:00:33,760
passion was people

10
00:00:30,519 --> 00:00:35,840
I was more interested in designing a

11
00:00:33,760 --> 00:00:38,559
future and inventing technologies that

12
00:00:35,840 --> 00:00:41,079
would benefit people that would make

13
00:00:38,559 --> 00:00:44,440
people more intelligent for example

14
00:00:41,079 --> 00:00:47,039
rather than inventing smarter robots and

15
00:00:44,440 --> 00:00:49,879
possibly AI systems that would one day

16
00:00:47,039 --> 00:00:53,399
replace us so that's why I joined the

17
00:00:49,879 --> 00:00:56,239
media lab while initially starting at

18
00:00:53,399 --> 00:01:00,199
the AI lab and that's also the work that

19
00:00:56,239 --> 00:01:03,600
I'll talk about today so I've been

20
00:01:00,199 --> 00:01:07,479
working in AI for a very long time and

21
00:01:03,600 --> 00:01:11,240
um I believe that we are quickly moving

22
00:01:07,479 --> 00:01:15,240
towards an era where AI is always with

23
00:01:11,240 --> 00:01:17,320
us and is mediating our daily experience

24
00:01:15,240 --> 00:01:20,479
whether it's at work whether it's at

25
00:01:17,320 --> 00:01:22,000
home Etc how many of you have seen the

26
00:01:20,479 --> 00:01:26,560
movie

27
00:01:22,000 --> 00:01:28,159
her so about a quarter of the people the

28
00:01:26,560 --> 00:01:30,920
other ones you know what to watch

29
00:01:28,159 --> 00:01:33,640
tonight um I think think it's as

30
00:01:30,920 --> 00:01:36,560
educational as the uh maybe some of the

31
00:01:33,640 --> 00:01:38,960
talks at the conference today in terms

32
00:01:36,560 --> 00:01:42,640
of telling you what future we are

33
00:01:38,960 --> 00:01:45,159
heading towards basically the person uh

34
00:01:42,640 --> 00:01:48,560
the main character in the movie has this

35
00:01:45,159 --> 00:01:51,200
AI system uh it lives in its in his uh

36
00:01:48,560 --> 00:01:53,520
shirt pocket actually it has the camera

37
00:01:51,200 --> 00:01:56,600
always on it's basically a phone with a

38
00:01:53,520 --> 00:01:59,000
camera always on and that and an AI

39
00:01:56,600 --> 00:02:01,119
system that constantly talks to him

40
00:01:59,000 --> 00:02:04,640
about everything going on everything

41
00:02:01,119 --> 00:02:08,599
going on around him Etc uh it knows

42
00:02:04,640 --> 00:02:11,120
about all of his data and personalizes

43
00:02:08,599 --> 00:02:14,640
that whole experience is really Charming

44
00:02:11,120 --> 00:02:18,560
on top of it voiced by Scarlet Johansson

45
00:02:14,640 --> 00:02:22,160
and so on so a wearable AI system always

46
00:02:18,560 --> 00:02:24,000
on aware of the user's context and

47
00:02:22,160 --> 00:02:26,840
States through

48
00:02:24,000 --> 00:02:32,000
sensors personalized aware of all the

49
00:02:26,840 --> 00:02:35,599
users data and proac active anticipating

50
00:02:32,000 --> 00:02:37,879
the needs of that user uh providing

51
00:02:35,599 --> 00:02:41,360
informations helping with decision

52
00:02:37,879 --> 00:02:44,080
making Etc that is the future of

53
00:02:41,360 --> 00:02:47,280
computer human interaction that we are

54
00:02:44,080 --> 00:02:50,519
very rapidly moving towards and of

55
00:02:47,280 --> 00:02:53,599
course that future offers unprecedented

56
00:02:50,519 --> 00:02:57,120
opportunities to benefit people in the

57
00:02:53,599 --> 00:02:59,840
area of Health once our phones are even

58
00:02:57,120 --> 00:03:02,319
more proactive more personalized have

59
00:02:59,840 --> 00:03:04,519
have even more sensors to know what is

60
00:03:02,319 --> 00:03:09,760
going on with our bodies Our mental

61
00:03:04,519 --> 00:03:12,959
health Etc they can basically P um like

62
00:03:09,760 --> 00:03:17,080
monitor uh all sorts of uh possible

63
00:03:12,959 --> 00:03:19,480
issues diagnosis uh can be made early uh

64
00:03:17,080 --> 00:03:22,720
these systems can intervene before

65
00:03:19,480 --> 00:03:24,599
something becomes a problem Etc in the

66
00:03:22,720 --> 00:03:28,200
area of learning there's huge

67
00:03:24,599 --> 00:03:31,640
opportunities one day I believe our kids

68
00:03:28,200 --> 00:03:34,760
will have a personal tool running on the

69
00:03:31,640 --> 00:03:38,120
Next Generation phone that just helps

70
00:03:34,760 --> 00:03:40,720
them learn about the world um all day

71
00:03:38,120 --> 00:03:43,200
long and knows what their learning style

72
00:03:40,720 --> 00:03:47,040
is how to get them excited and engaged

73
00:03:43,200 --> 00:03:49,280
in learning about something and so on in

74
00:03:47,040 --> 00:03:51,720
work of course the work area there's

75
00:03:49,280 --> 00:03:54,599
huge opportunities for amplifying

76
00:03:51,720 --> 00:03:58,040
creativity performance Discovery

77
00:03:54,599 --> 00:04:00,519
decision making and more and the elderly

78
00:03:58,040 --> 00:04:03,360
I think are also a group that really

79
00:04:00,519 --> 00:04:06,879
could benefit from AI assistance because

80
00:04:03,360 --> 00:04:08,760
maybe they almost needed most uh these

81
00:04:06,879 --> 00:04:11,920
days and I'll talk about that a little

82
00:04:08,760 --> 00:04:15,600
bit more later so wonderful

83
00:04:11,920 --> 00:04:19,280
opportunities but also

84
00:04:15,600 --> 00:04:21,440
unprecedented uh challenges for example

85
00:04:19,280 --> 00:04:24,240
we are already seeing again I'll talk

86
00:04:21,440 --> 00:04:27,919
about that more that people are over

87
00:04:24,240 --> 00:04:29,960
relying on AI trusting it too much and

88
00:04:27,919 --> 00:04:33,240
thereby making decisions that are are

89
00:04:29,960 --> 00:04:37,800
not optimal uh so there's a lot of

90
00:04:33,240 --> 00:04:40,199
misplaced uh trust in AI there's a lot

91
00:04:37,800 --> 00:04:43,840
of loss of or there's a potential for

92
00:04:40,199 --> 00:04:47,479
the loss of human agency the more we

93
00:04:43,840 --> 00:04:50,840
rely on AI to help us make decisions and

94
00:04:47,479 --> 00:04:53,600
solve problems the more we risk no

95
00:04:50,840 --> 00:04:56,919
longer being in control of all of these

96
00:04:53,600 --> 00:05:01,039
decisions we make especially when AI

97
00:04:56,919 --> 00:05:05,280
gets better at things than we are um we

98
00:05:01,039 --> 00:05:08,600
risk deskilling whenever you delegate a

99
00:05:05,280 --> 00:05:11,400
task to AI or rely too much on AI to

100
00:05:08,600 --> 00:05:13,639
help you with some decision you risk

101
00:05:11,400 --> 00:05:17,080
losing that skill yourself because

102
00:05:13,639 --> 00:05:20,240
you're no longer exercising it there's a

103
00:05:17,080 --> 00:05:23,360
potential loss of understanding of how

104
00:05:20,240 --> 00:05:27,759
the world Works how uh things should be

105
00:05:23,360 --> 00:05:30,600
done Etc right now most science uh

106
00:05:27,759 --> 00:05:33,280
scientific knowledge is the problem of

107
00:05:30,600 --> 00:05:37,000
people uh basically coming up with all

108
00:05:33,280 --> 00:05:40,280
of this knowledge and so we mostly

109
00:05:37,000 --> 00:05:41,880
understand what um what things are uh in

110
00:05:40,280 --> 00:05:44,880
all these different areas but

111
00:05:41,880 --> 00:05:48,240
increasingly we are moving towards a

112
00:05:44,880 --> 00:05:50,720
world where AI makes the discoveries and

113
00:05:48,240 --> 00:05:52,840
discovers things that we no longer can

114
00:05:50,720 --> 00:05:55,800
understand so it knows or it has

115
00:05:52,840 --> 00:05:59,160
discovered something and we have to like

116
00:05:55,800 --> 00:06:01,160
catch up over um uh and and maybe will

117
00:05:59,160 --> 00:06:04,360
never catch catch up in trying to

118
00:06:01,160 --> 00:06:06,639
understand what it discovered um

119
00:06:04,360 --> 00:06:09,199
misinformation of course if you have an

120
00:06:06,639 --> 00:06:11,400
AI with you all the time that is going

121
00:06:09,199 --> 00:06:15,280
to help you interpret everything that

122
00:06:11,400 --> 00:06:18,880
you come across you risk that um there

123
00:06:15,280 --> 00:06:22,520
is a bias in that AI or that it's um um

124
00:06:18,880 --> 00:06:25,960
a mischievous AI Etc so in our work we

125
00:06:22,520 --> 00:06:28,520
show that there's uh very um big

126
00:06:25,960 --> 00:06:32,599
potential for misinformation also

127
00:06:28,520 --> 00:06:36,199
because of that misplaced trust in AI

128
00:06:32,599 --> 00:06:39,880
manipulation AI is the biggest

129
00:06:36,199 --> 00:06:44,479
persuasion uh technology ever

130
00:06:39,880 --> 00:06:47,520
invented AI is fabulous at persuading us

131
00:06:44,479 --> 00:06:50,240
of all sorts of things it's um and of

132
00:06:47,520 --> 00:06:52,840
course that can be of a lot of uh

133
00:06:50,240 --> 00:06:55,879
misinformation or it may try to sterious

134
00:06:52,840 --> 00:07:00,599
to into certain biased directions and so

135
00:06:55,879 --> 00:07:04,280
on loss of control privacy ER expion

136
00:07:00,599 --> 00:07:07,639
unhealthy attachment to chatbots

137
00:07:04,280 --> 00:07:10,360
increasingly an issue as well and

138
00:07:07,639 --> 00:07:13,360
dehumanization if you no longer interact

139
00:07:10,360 --> 00:07:17,840
with other people as much to that teach

140
00:07:13,360 --> 00:07:19,120
you or that you work with or that um or

141
00:07:17,840 --> 00:07:22,280
even for

142
00:07:19,120 --> 00:07:24,599
socializing of course we risk losing

143
00:07:22,280 --> 00:07:27,960
sort of a lot as a

144
00:07:24,599 --> 00:07:31,520
society um because we no longer sort of

145
00:07:27,960 --> 00:07:32,840
reinforce that human fabric um that uh

146
00:07:31,520 --> 00:07:35,919
is so

147
00:07:32,840 --> 00:07:38,400
important so AI is not just an

148
00:07:35,919 --> 00:07:42,199
engineering challenge I hope that long

149
00:07:38,400 --> 00:07:45,639
list made that clear to you AI is also

150
00:07:42,199 --> 00:07:48,280
very much a human design challenge we

151
00:07:45,639 --> 00:07:51,440
really have to think more carefully

152
00:07:48,280 --> 00:07:54,120
about what this future with AI or this

153
00:07:51,440 --> 00:07:57,120
present with AI because the future in AI

154
00:07:54,120 --> 00:08:00,479
is next week basically what are these

155
00:07:57,120 --> 00:08:04,120
going to do to us to people

156
00:08:00,479 --> 00:08:06,599
people and that's actually what my work

157
00:08:04,120 --> 00:08:09,560
uh at the media lab focuses on it's not

158
00:08:06,599 --> 00:08:11,720
just my work people like Andy and others

159
00:08:09,560 --> 00:08:15,240
uh other colleagues work in this area as

160
00:08:11,720 --> 00:08:19,120
well we are actually starting a program

161
00:08:15,240 --> 00:08:23,479
at the media lab to really reinforce um

162
00:08:19,120 --> 00:08:26,759
this type of research uh the AHA

163
00:08:23,479 --> 00:08:30,199
program um stands for advancing human AI

164
00:08:26,759 --> 00:08:33,279
interaction with the eye upside down but

165
00:08:30,199 --> 00:08:36,560
the goal really is to provide some aha

166
00:08:33,279 --> 00:08:39,880
moments to research and development in

167
00:08:36,560 --> 00:08:42,839
Ai and make people out there think a

168
00:08:39,880 --> 00:08:45,760
little bit more carefully about how we

169
00:08:42,839 --> 00:08:50,200
are designing Ai and whether we are

170
00:08:45,760 --> 00:08:52,480
designing AI to benefit people so in

171
00:08:50,200 --> 00:08:55,399
this aha program we do a number of

172
00:08:52,480 --> 00:08:58,839
different activities we Inspire or we

173
00:08:55,399 --> 00:09:02,760
aim to inspire with positive use cases

174
00:08:58,839 --> 00:09:06,600
of AI that help people thrive um we

175
00:09:02,760 --> 00:09:09,720
invent and we provide Insight uh we do

176
00:09:06,600 --> 00:09:13,959
studies to show some of the possible

177
00:09:09,720 --> 00:09:16,920
negative and positive impact of AI on

178
00:09:13,959 --> 00:09:19,680
people's cognition and intelligence and

179
00:09:16,920 --> 00:09:22,240
more so let me give you some examples

180
00:09:19,680 --> 00:09:24,880
from my own work in uh sort of the

181
00:09:22,240 --> 00:09:28,440
inspiration category and then in the

182
00:09:24,880 --> 00:09:31,640
inside category so we do develop a lot

183
00:09:28,440 --> 00:09:34,000
of novel use cases typically in areas

184
00:09:31,640 --> 00:09:37,640
that Silicon Valley doesn't think all

185
00:09:34,000 --> 00:09:40,040
that much about uh for example we have

186
00:09:37,640 --> 00:09:42,640
been looking a lot at memory

187
00:09:40,040 --> 00:09:44,720
augmentation all of us have to remember

188
00:09:42,640 --> 00:09:47,720
more and more information but of course

189
00:09:44,720 --> 00:09:50,120
as you age uh that also becomes more and

190
00:09:47,720 --> 00:09:55,920
more difficult so we've been building

191
00:09:50,120 --> 00:09:59,000
systems that uh assist you by basically

192
00:09:55,920 --> 00:10:01,760
memorizing a everything you say or if

193
00:09:59,000 --> 00:10:05,000
you have permission even conversations

194
00:10:01,760 --> 00:10:08,360
with other people other data um from

195
00:10:05,000 --> 00:10:11,560
your computer Etc and it's sort of acts

196
00:10:08,360 --> 00:10:15,040
like your own personal chat GPT

197
00:10:11,560 --> 00:10:17,519
basically um it can really uh predict

198
00:10:15,040 --> 00:10:19,279
almost what you're going to say in a

199
00:10:17,519 --> 00:10:22,120
certain moment and if you can think of

200
00:10:19,279 --> 00:10:25,200
the words you just ask it and uh you

201
00:10:22,120 --> 00:10:28,160
click a button and it gives you uh the

202
00:10:25,200 --> 00:10:31,160
information you're looking for so super

203
00:10:28,160 --> 00:10:33,240
quick video to show show this mamor is

204
00:10:31,160 --> 00:10:35,519
an audio-based wearable memory assistant

205
00:10:33,240 --> 00:10:38,360
powered by an AI agent it runs on

206
00:10:35,519 --> 00:10:40,760
unobtrusive commercially available

207
00:10:38,360 --> 00:10:42,760
Hardware hi nice to meet you I'm Robert

208
00:10:40,760 --> 00:10:44,639
I'm an investment maner from Investments

209
00:10:42,760 --> 00:10:46,720
oh and what do you do there so we invest

210
00:10:44,639 --> 00:10:51,959
in B startups that help with rare

211
00:10:46,720 --> 00:10:55,200
diseases oh wait what was his name again

212
00:10:51,959 --> 00:10:57,760
Robert a biotic do you know where I can

213
00:10:55,200 --> 00:10:59,800
look for investment oh yes I actually

214
00:10:57,760 --> 00:11:03,279
just met this guy last week his name is

215
00:10:59,800 --> 00:11:06,240
Dober and he works in fluid fluid

216
00:11:03,279 --> 00:11:09,560
Investments and they specialize in rare

217
00:11:06,240 --> 00:11:12,680
diseas biod startups working in

218
00:11:09,560 --> 00:11:15,279
actually so we have a working prototype

219
00:11:12,680 --> 00:11:17,480
of this system both on uh the hardware

220
00:11:15,279 --> 00:11:20,240
you saw there but it can also work on a

221
00:11:17,480 --> 00:11:22,880
Samsung Galaxy watch actually which it

222
00:11:20,240 --> 00:11:26,600
turns out can record all your

223
00:11:22,880 --> 00:11:29,040
conversations um so we have been working

224
00:11:26,600 --> 00:11:32,440
on this um for a while trying to make

225
00:11:29,040 --> 00:11:35,279
this system as uh nondisruptive as

226
00:11:32,440 --> 00:11:37,680
possible actually if you think about the

227
00:11:35,279 --> 00:11:40,279
smartphone today of course it can give

228
00:11:37,680 --> 00:11:42,200
you a lot of that same information but

229
00:11:40,279 --> 00:11:44,560
if you have to look up the name of a

230
00:11:42,200 --> 00:11:47,680
person that you came across say at the

231
00:11:44,560 --> 00:11:50,600
last last ILP conference it would take

232
00:11:47,680 --> 00:11:53,279
you a long time um to actually go

233
00:11:50,600 --> 00:11:55,760
through all your data Etc on your phone

234
00:11:53,279 --> 00:11:57,800
you have to completely take away your

235
00:11:55,760 --> 00:11:59,920
attention from say the person you're

236
00:11:57,800 --> 00:12:02,920
talking with here at a conference to

237
00:11:59,920 --> 00:12:06,560
look that information up so the input

238
00:12:02,920 --> 00:12:09,360
required to use our phones today for

239
00:12:06,560 --> 00:12:13,079
memory assistance is really very high

240
00:12:09,360 --> 00:12:17,120
for a smartphone and also the output is

241
00:12:13,079 --> 00:12:20,560
um uh effort required is very high um if

242
00:12:17,120 --> 00:12:23,440
a system gives you an entire email or

243
00:12:20,560 --> 00:12:25,760
the entire agenda from the last ILP

244
00:12:23,440 --> 00:12:28,079
conference or something you still have

245
00:12:25,760 --> 00:12:30,000
to like go find a little bit of

246
00:12:28,079 --> 00:12:33,000
information that you forgot that you

247
00:12:30,000 --> 00:12:35,839
were looking for so instead in our

248
00:12:33,000 --> 00:12:39,079
Memorial system we try to make the

249
00:12:35,839 --> 00:12:41,120
system context aware so it can predict

250
00:12:39,079 --> 00:12:43,639
what words you need based on what you

251
00:12:41,120 --> 00:12:46,399
previously said as well as the situation

252
00:12:43,639 --> 00:12:50,120
you're in and we also aim for

253
00:12:46,399 --> 00:12:52,800
conciseness in the output of the agent

254
00:12:50,120 --> 00:12:54,959
so that uh it just gives you the word

255
00:12:52,800 --> 00:12:57,360
that you need and you can be using this

256
00:12:54,959 --> 00:13:00,920
system while you're talking to another

257
00:12:57,360 --> 00:13:04,240
individual without it being uh so

258
00:13:00,920 --> 00:13:08,079
disruptive now of course these memory

259
00:13:04,240 --> 00:13:11,160
augmentation systems are uh potentially

260
00:13:08,079 --> 00:13:14,320
very um uh useful for the elderly

261
00:13:11,160 --> 00:13:16,800
population and in the spring last year

262
00:13:14,320 --> 00:13:19,839
we've been organizing co-design

263
00:13:16,800 --> 00:13:22,000
workshops with older adults um you see

264
00:13:19,839 --> 00:13:23,959
some of them there together with all of

265
00:13:22,000 --> 00:13:27,519
the students in my research group who

266
00:13:23,959 --> 00:13:31,959
were helping facilitate this we had 40

267
00:13:27,519 --> 00:13:35,720
people between the ages of of uh 74 and

268
00:13:31,959 --> 00:13:39,639
94 that were really eager to learn about

269
00:13:35,720 --> 00:13:42,839
generative Ai and to help us um design

270
00:13:39,639 --> 00:13:45,160
and co-design possible use cases of

271
00:13:42,839 --> 00:13:48,760
generative AI that could really help

272
00:13:45,160 --> 00:13:52,680
them in their daily lives um we have a

273
00:13:48,760 --> 00:13:55,519
paper um coming out in the the AI impact

274
00:13:52,680 --> 00:13:59,480
volume that um our president corn Bluth

275
00:13:55,519 --> 00:14:02,199
is uh editing uh on those workshops

276
00:13:59,480 --> 00:14:04,680
one of the um systems that we have been

277
00:14:02,199 --> 00:14:07,959
co-designing with these old older adults

278
00:14:04,680 --> 00:14:09,959
is the meal system it's a little bit

279
00:14:07,959 --> 00:14:13,399
similar to the previous one but it's

280
00:14:09,959 --> 00:14:16,000
more focused on helping an older person

281
00:14:13,399 --> 00:14:19,480
live in their home safely and

282
00:14:16,000 --> 00:14:22,680
independently for longer so what it

283
00:14:19,480 --> 00:14:25,639
consists of is just a camera that you

284
00:14:22,680 --> 00:14:29,279
wear around your neck and a a speech

285
00:14:25,639 --> 00:14:32,600
input output and that camera is constant

286
00:14:29,279 --> 00:14:34,399
ly taking images uh uh pictures

287
00:14:32,600 --> 00:14:36,759
basically of what it is that you're

288
00:14:34,399 --> 00:14:38,759
doing with your hands so if I'm wearing

289
00:14:36,759 --> 00:14:40,800
this it would know she picked up the

290
00:14:38,759 --> 00:14:43,959
water bottle she opens the water bottle

291
00:14:40,800 --> 00:14:47,759
she takes the Sip Etc it takes all these

292
00:14:43,959 --> 00:14:52,040
images translates um uh basically these

293
00:14:47,759 --> 00:14:56,560
images into descriptions using an AI um

294
00:14:52,040 --> 00:14:58,600
Vision model and then puts that in a big

295
00:14:56,560 --> 00:15:01,320
database of all the activities the

296
00:14:58,600 --> 00:15:03,680
person does and that database can be

297
00:15:01,320 --> 00:15:06,240
queried by the person themselves like

298
00:15:03,680 --> 00:15:08,720
they can say where did I put my keys and

299
00:15:06,240 --> 00:15:11,480
the system just has to look where was

300
00:15:08,720 --> 00:15:14,240
were the keys last scene oh she put her

301
00:15:11,480 --> 00:15:16,720
keys down next to the microwave in the

302
00:15:14,240 --> 00:15:19,320
kitchen so we'll see a little demo hi

303
00:15:16,720 --> 00:15:22,399
pal where is my

304
00:15:19,320 --> 00:15:24,759
phone and she hears this spoken in her

305
00:15:22,399 --> 00:15:27,079
ear so your phone was last seen in the

306
00:15:24,759 --> 00:15:30,079
living area near near the gray upholster

307
00:15:27,079 --> 00:15:33,199
chair white charger uh charger table

308
00:15:30,079 --> 00:15:33,199
blah blah blah very

309
00:15:34,560 --> 00:15:42,079
specific so we've been testing this in

310
00:15:37,480 --> 00:15:44,079
people's homes uh 17 of them and um the

311
00:15:42,079 --> 00:15:47,040
system doesn't just help you find

312
00:15:44,079 --> 00:15:49,560
misplaced objects but it also prevents

313
00:15:47,040 --> 00:15:52,279
repeated actions uh these are safety

314
00:15:49,560 --> 00:15:54,120
measures if then rules that either the

315
00:15:52,279 --> 00:15:57,279
older adult themselves or their

316
00:15:54,120 --> 00:15:59,560
caregivers can enter for example you can

317
00:15:57,279 --> 00:16:02,800
enter a rule saying she has to take her

318
00:15:59,560 --> 00:16:05,759
meds before 9:00 a.m. and after 400 p.m.

319
00:16:02,800 --> 00:16:07,720
if she tries to take her medications a

320
00:16:05,759 --> 00:16:11,199
second time the system warns you and

321
00:16:07,720 --> 00:16:13,279
says you took your medication at 8:06 or

322
00:16:11,199 --> 00:16:16,519
it can tell her you haven't taken your

323
00:16:13,279 --> 00:16:20,839
medication it's 9:00 a.m. um the system

324
00:16:16,519 --> 00:16:23,399
also gives you other um safety reminders

325
00:16:20,839 --> 00:16:27,040
uh if you turn the stove on you forget

326
00:16:23,399 --> 00:16:29,040
to turn the stove off um motivated by a

327
00:16:27,040 --> 00:16:32,399
real use case of one of our Cod

328
00:16:29,040 --> 00:16:34,880
designers by the way who had the fire

329
00:16:32,399 --> 00:16:38,319
department at her door after she left

330
00:16:34,880 --> 00:16:40,920
her home with the stove on um uh so it

331
00:16:38,319 --> 00:16:44,800
can U remind you of things like that it

332
00:16:40,920 --> 00:16:47,800
can also reduce caregiver stress and um

333
00:16:44,800 --> 00:16:50,880
uh for example tell the the kids of

334
00:16:47,800 --> 00:16:53,600
these aging uh adults uh that their

335
00:16:50,880 --> 00:16:57,279
mother has had three good meals and went

336
00:16:53,600 --> 00:16:59,480
outside Etc or can even tell the doctor

337
00:16:57,279 --> 00:17:01,480
this person is increasingly forgetting

338
00:16:59,480 --> 00:17:04,600
where they put things not eating healthy

339
00:17:01,480 --> 00:17:08,679
meals Etc so they can monitor but in a

340
00:17:04,600 --> 00:17:10,640
safety preserving way um we've also been

341
00:17:08,679 --> 00:17:13,120
focused a lot on

342
00:17:10,640 --> 00:17:16,160
education a lot of our work is either

343
00:17:13,120 --> 00:17:20,319
younger uh people education or older

344
00:17:16,160 --> 00:17:23,640
adults aging population uh for example

345
00:17:20,319 --> 00:17:26,439
um we know that unfortunately maybe kids

346
00:17:23,640 --> 00:17:29,919
are no longer interested in reading

347
00:17:26,439 --> 00:17:33,880
books um and um we thought well how can

348
00:17:29,919 --> 00:17:36,160
we still let kids learn from say

349
00:17:33,880 --> 00:17:39,520
historical figures like in the figure

350
00:17:36,160 --> 00:17:42,280
there uh Leonardo da Vinci um without

351
00:17:39,520 --> 00:17:45,760
necessarily having to go through um uh

352
00:17:42,280 --> 00:17:49,720
his journals Etc so we trained an AI

353
00:17:45,760 --> 00:17:54,039
system uh to basically um be able to

354
00:17:49,720 --> 00:17:57,320
talk to people about um uh and represent

355
00:17:54,039 --> 00:18:00,360
Leonardo hello I am an AI generated

356
00:17:57,320 --> 00:18:03,559
living memory of Leonardo divin so the

357
00:18:00,360 --> 00:18:06,159
whole system I mean that this is cute

358
00:18:03,559 --> 00:18:10,280
the the VIS visual part of it but it's

359
00:18:06,159 --> 00:18:13,320
actually using rag techniques in AI to

360
00:18:10,280 --> 00:18:16,679
take the primary sources namely

361
00:18:13,320 --> 00:18:20,039
Leonardo's journals to basically as well

362
00:18:16,679 --> 00:18:24,520
as then secondary U sources biographies

363
00:18:20,039 --> 00:18:26,960
about Leonardo to provide um trustworthy

364
00:18:24,520 --> 00:18:29,880
um information and you can just talk to

365
00:18:26,960 --> 00:18:32,159
Leonardo saying okay what did you what

366
00:18:29,880 --> 00:18:34,520
are some things you're famous for and

367
00:18:32,159 --> 00:18:37,400
tell me more about your experiments to

368
00:18:34,520 --> 00:18:40,080
build flying machines and so on we're

369
00:18:37,400 --> 00:18:42,760
actually um soon installing that at a

370
00:18:40,080 --> 00:18:45,720
museum uh the last place where Leonardo

371
00:18:42,760 --> 00:18:49,440
lived uh two hours away from France uh

372
00:18:45,720 --> 00:18:51,600
from uh Paris um where a lot of school

373
00:18:49,440 --> 00:18:55,720
kids visit Etc and they'll be able to

374
00:18:51,600 --> 00:18:59,320
learn about Leonardo that way um another

375
00:18:55,720 --> 00:19:01,559
um system focused on younger people

376
00:18:59,320 --> 00:19:05,280
younger people often have a very hard

377
00:19:01,559 --> 00:19:07,720
time imagining their future and acting

378
00:19:05,280 --> 00:19:12,000
in the interest of this longer term

379
00:19:07,720 --> 00:19:15,640
future like saving money uh studying to

380
00:19:12,000 --> 00:19:19,240
get a better job things like that um so

381
00:19:15,640 --> 00:19:22,039
delayed um uh gratification basically is

382
00:19:19,240 --> 00:19:24,200
is often a human problem and so we

383
00:19:22,039 --> 00:19:26,679
thought can we build an AI that helps

384
00:19:24,200 --> 00:19:28,880
you with that problem with uh thinking

385
00:19:26,679 --> 00:19:31,799
more longterm

386
00:19:28,880 --> 00:19:35,960
um and acting more in your own long-term

387
00:19:31,799 --> 00:19:38,440
interest so we built a system that uh

388
00:19:35,960 --> 00:19:40,919
basically you tell it a little bit about

389
00:19:38,440 --> 00:19:44,039
yourself and your goals and then it

390
00:19:40,919 --> 00:19:46,600
actually creates an older version of you

391
00:19:44,039 --> 00:19:48,960
that you can chat with so you can say I

392
00:19:46,600 --> 00:19:50,760
think I want to become a biology teacher

393
00:19:48,960 --> 00:19:52,440
but maybe the next day you can try

394
00:19:50,760 --> 00:19:53,760
something else and say I think maybe I

395
00:19:52,440 --> 00:19:56,559
want to become a

396
00:19:53,760 --> 00:19:59,039
pediatrician um you tell it about where

397
00:19:56,559 --> 00:20:01,760
you're at in your life what else is

398
00:19:59,039 --> 00:20:04,960
important to you and then we create this

399
00:20:01,760 --> 00:20:08,600
potential future you we tell you this is

400
00:20:04,960 --> 00:20:11,679
not a prediction this is uh uh only one

401
00:20:08,600 --> 00:20:14,039
possible future uh we also give you the

402
00:20:11,679 --> 00:20:16,840
picture to go along with it to increase

403
00:20:14,039 --> 00:20:19,600
the vividness and then you can just talk

404
00:20:16,840 --> 00:20:22,280
to your future self and say well what is

405
00:20:19,600 --> 00:20:24,919
it like to be a biology teacher tell me

406
00:20:22,280 --> 00:20:27,440
about some good and bad experiences what

407
00:20:24,919 --> 00:20:29,520
are the pros and cons what is a typical

408
00:20:27,440 --> 00:20:32,280
day like Etc

409
00:20:29,520 --> 00:20:34,720
we showed together with Hal hfield and

410
00:20:32,280 --> 00:20:38,480
actually the paper is published rather

411
00:20:34,720 --> 00:20:40,760
than under review uh that this increases

412
00:20:38,480 --> 00:20:43,159
uh future self-continuity a

413
00:20:40,760 --> 00:20:46,400
psychological aspect that measures to

414
00:20:43,159 --> 00:20:48,559
what extent people uh take actions in

415
00:20:46,400 --> 00:20:51,120
the uh their long-term interest you can

416
00:20:48,559 --> 00:20:54,840
try this system or your kids can try it

417
00:20:51,120 --> 00:20:57,600
future you. media. mit.edu we have

418
00:20:54,840 --> 00:21:01,320
people from over 90 countries Now using

419
00:20:57,600 --> 00:21:03,679
it they daily frequently coming back to

420
00:21:01,320 --> 00:21:06,919
uh talk to their future self maybe

421
00:21:03,679 --> 00:21:09,400
change their goals Etc we have

422
00:21:06,919 --> 00:21:12,480
prototypes in the area of health and

423
00:21:09,400 --> 00:21:14,720
well-being uh sort of creating systems

424
00:21:12,480 --> 00:21:17,279
with AI that make it easier to make

425
00:21:14,720 --> 00:21:20,440
sense of your own health I won't don't

426
00:21:17,279 --> 00:21:23,559
have time to talk about them uh but this

427
00:21:20,440 --> 00:21:26,520
is one type of activity we do

428
00:21:23,559 --> 00:21:29,120
inspirational uh prototypes and uh

429
00:21:26,520 --> 00:21:32,600
technologies that can help people

430
00:21:29,120 --> 00:21:36,400
with things where they have a real need

431
00:21:32,600 --> 00:21:40,600
um the second activity that we engage in

432
00:21:36,400 --> 00:21:46,120
is uh studies and experiments that shed

433
00:21:40,600 --> 00:21:49,600
light on how AI affects people basically

434
00:21:46,120 --> 00:21:52,840
um so for example you may think well

435
00:21:49,600 --> 00:21:55,159
having a eye at your fingertips improves

436
00:21:52,840 --> 00:21:59,279
people's decision making is that the

437
00:21:55,159 --> 00:22:02,919
case not necessarily we did an experim M

438
00:21:59,279 --> 00:22:05,760
we first uh built a an AI that is honest

439
00:22:02,919 --> 00:22:09,159
and knows the grand truth and it

440
00:22:05,760 --> 00:22:11,799
actually um you a person would read a

441
00:22:09,159 --> 00:22:14,440
statement then they would guess like

442
00:22:11,799 --> 00:22:16,440
whether this statement was true or false

443
00:22:14,440 --> 00:22:19,039
and then we had an AI tell them actually

444
00:22:16,440 --> 00:22:21,679
it's false or true and because of this

445
00:22:19,039 --> 00:22:24,440
reason and we did see that especially if

446
00:22:21,679 --> 00:22:27,039
the AI gives an explanation people do

447
00:22:24,440 --> 00:22:29,840
change their mind and improve their

448
00:22:27,039 --> 00:22:31,520
decision um of the the on the accuracy

449
00:22:29,840 --> 00:22:34,919
of these statements that they came

450
00:22:31,520 --> 00:22:36,919
across not if the AI just said this is

451
00:22:34,919 --> 00:22:39,120
true or false it had to give an

452
00:22:36,919 --> 00:22:43,400
explanation but then we thought well

453
00:22:39,120 --> 00:22:46,679
what if the AI is a malicious Ai and it

454
00:22:43,400 --> 00:22:49,000
actually provides the wrong information

455
00:22:46,679 --> 00:22:52,120
with an explanation that sounds

456
00:22:49,000 --> 00:22:56,679
believable and so this graph here shows

457
00:22:52,120 --> 00:22:59,919
you for false and true headlines of um

458
00:22:56,679 --> 00:23:02,960
newspapers um how person performs in

459
00:22:59,919 --> 00:23:06,520
terms of um assessing accuracy of

460
00:23:02,960 --> 00:23:09,840
newspaper headlines in Gray on their own

461
00:23:06,520 --> 00:23:13,799
in blue with an honest AI but in red

462
00:23:09,840 --> 00:23:17,760
with a um malicious AI so this shows you

463
00:23:13,799 --> 00:23:21,200
that people are very susceptible to

464
00:23:17,760 --> 00:23:24,720
misinformation from Ai and they believe

465
00:23:21,200 --> 00:23:27,600
AI too much basically I believe this is

466
00:23:24,720 --> 00:23:30,279
the case because AI does sound so

467
00:23:27,600 --> 00:23:32,760
believable

468
00:23:30,279 --> 00:23:34,919
I mean it always gives you 10 points for

469
00:23:32,760 --> 00:23:38,720
why something is the way it is and it

470
00:23:34,919 --> 00:23:41,080
seems very uh uh sort of determined Etc

471
00:23:38,720 --> 00:23:45,200
it's a little bit overly confident in

472
00:23:41,080 --> 00:23:48,919
its own um uh answers so we have to be

473
00:23:45,200 --> 00:23:52,039
careful when people uh have an AI at

474
00:23:48,919 --> 00:23:55,400
their fingertips they risk really

475
00:23:52,039 --> 00:23:59,240
reducing their decision-making accuracy

476
00:23:55,400 --> 00:24:01,279
because they become too reliant on a AI

477
00:23:59,240 --> 00:24:05,039
so we've been working on methods that

478
00:24:01,279 --> 00:24:08,240
can prevent this for example um ask the

479
00:24:05,039 --> 00:24:10,679
AI has to First engage the person in

480
00:24:08,240 --> 00:24:12,480
thinking about the headline and that

481
00:24:10,679 --> 00:24:15,760
seems to make a difference and then they

482
00:24:12,480 --> 00:24:18,559
do better so it's sort of people stop

483
00:24:15,760 --> 00:24:21,320
thinking for themselves in many cases

484
00:24:18,559 --> 00:24:25,360
when they have this convenient AI that

485
00:24:21,320 --> 00:24:27,919
seems to know everything anyway um we

486
00:24:25,360 --> 00:24:30,480
also have been looking at things like

487
00:24:27,919 --> 00:24:34,440
how prior beliefs that people have and

488
00:24:30,480 --> 00:24:37,679
their biases uh affect evaluation of AI

489
00:24:34,440 --> 00:24:41,679
and how people work with AI so we gave

490
00:24:37,679 --> 00:24:44,679
three groups uh the same AI system that

491
00:24:41,679 --> 00:24:48,080
would like give mental health advice and

492
00:24:44,679 --> 00:24:50,080
we said you have to um evaluate how good

493
00:24:48,080 --> 00:24:53,760
this system is at giving mental health

494
00:24:50,080 --> 00:24:56,720
advice and one group we told them the AI

495
00:24:53,760 --> 00:24:59,720
is actually caring and the second group

496
00:24:56,720 --> 00:25:02,120
said careful this AI is manipulative and

497
00:24:59,720 --> 00:25:05,320
the third group we just didn't say

498
00:25:02,120 --> 00:25:08,440
anything at all what we learned is that

499
00:25:05,320 --> 00:25:11,600
how people rated the AI after a half

500
00:25:08,440 --> 00:25:14,120
hour conversation with that AI was

501
00:25:11,600 --> 00:25:17,039
significantly different across these

502
00:25:14,120 --> 00:25:20,880
conditions so people who believe that AI

503
00:25:17,039 --> 00:25:23,080
is good and caring they actually would

504
00:25:20,880 --> 00:25:25,640
say the agent is empathetic and

505
00:25:23,080 --> 00:25:27,960
trustworthy and I would recommend it to

506
00:25:25,640 --> 00:25:32,600
someone else and so on people who were

507
00:25:27,960 --> 00:25:35,640
told this agent is um uh manipulative

508
00:25:32,600 --> 00:25:38,000
they actually would uh rate it

509
00:25:35,640 --> 00:25:41,720
significantly lower but they were all

510
00:25:38,000 --> 00:25:44,640
talking to the same system so these are

511
00:25:41,720 --> 00:25:48,480
all factors that I think are not enough

512
00:25:44,640 --> 00:25:51,320
taken into account in AI deployments as

513
00:25:48,480 --> 00:25:54,039
well we should really be sure that

514
00:25:51,320 --> 00:25:56,200
people all start from the same page a

515
00:25:54,039 --> 00:25:59,200
little bit more about their beliefs in

516
00:25:56,200 --> 00:26:02,640
Ai and how useful it is because it makes

517
00:25:59,200 --> 00:26:05,679
significant differences another related

518
00:26:02,640 --> 00:26:09,720
experiment we did was you hear so much

519
00:26:05,679 --> 00:26:12,440
about bias in AI data and how we should

520
00:26:09,720 --> 00:26:15,799
prevent it and I very much uh agree with

521
00:26:12,440 --> 00:26:19,039
that uh but of course what people forget

522
00:26:15,799 --> 00:26:21,880
is that people themselves are biased as

523
00:26:19,039 --> 00:26:25,480
well and these systems these AI systems

524
00:26:21,880 --> 00:26:28,640
work with biased people and so we did

525
00:26:25,480 --> 00:26:32,559
another experiment where we basically

526
00:26:28,640 --> 00:26:35,440
first tested people on their own values

527
00:26:32,559 --> 00:26:38,840
in a certain area and then we actually

528
00:26:35,440 --> 00:26:43,559
gave them an AI that either was neutral

529
00:26:38,840 --> 00:26:46,840
in those in that same area of values or

530
00:26:43,559 --> 00:26:49,880
uh aligned with their biased values or

531
00:26:46,840 --> 00:26:52,840
the opposite of their values uh some of

532
00:26:49,880 --> 00:26:55,720
the areas were for example um

533
00:26:52,840 --> 00:26:58,279
market-based Solutions versus more

534
00:26:55,720 --> 00:27:01,440
Equitable solutions to economic problem

535
00:26:58,279 --> 00:27:04,120
problems this was uh uh one of the areas

536
00:27:01,440 --> 00:27:08,240
uh where we did uh the study in and what

537
00:27:04,120 --> 00:27:09,919
we learned is that the ai's values if

538
00:27:08,240 --> 00:27:12,360
they're aligned with a person if they're

539
00:27:09,919 --> 00:27:15,480
both biased in some way of course the

540
00:27:12,360 --> 00:27:19,520
results of the combined human AI system

541
00:27:15,480 --> 00:27:23,559
is even more biased than the human alone

542
00:27:19,520 --> 00:27:26,159
if the AI had the opposite values of the

543
00:27:23,559 --> 00:27:29,880
person actually that made it more

544
00:27:26,159 --> 00:27:32,440
neutral and that actually down the uh

545
00:27:29,880 --> 00:27:35,320
bias in the result of the system and the

546
00:27:32,440 --> 00:27:37,520
AI working together uh so you can

547
00:27:35,320 --> 00:27:41,559
actually it's interesting you can bias

548
00:27:37,520 --> 00:27:44,760
on purpose an AI system so that it can

549
00:27:41,559 --> 00:27:48,320
compensate for the bias in one of your

550
00:27:44,760 --> 00:27:50,159
employees uh decision making we've also

551
00:27:48,320 --> 00:27:53,480
been looking at the effects of

552
00:27:50,159 --> 00:27:56,600
anthropomorphizing AI this is now a a

553
00:27:53,480 --> 00:28:00,519
four-year-old study but uh back when

554
00:27:56,600 --> 00:28:03,880
Elon Musk still had more uh supporters I

555
00:28:00,519 --> 00:28:06,480
guess um so we thought well what if we

556
00:28:03,880 --> 00:28:08,679
create this deep fake of Elon Musk

557
00:28:06,480 --> 00:28:12,679
everybody knows him and then this

558
00:28:08,679 --> 00:28:14,880
non-existing person and we put um people

559
00:28:12,679 --> 00:28:17,519
in front or or we give people this

560
00:28:14,880 --> 00:28:19,559
virtual system to teach them something

561
00:28:17,519 --> 00:28:21,720
but this could also be to sell them

562
00:28:19,559 --> 00:28:25,760
something or explain something or work

563
00:28:21,720 --> 00:28:28,840
with them and um in our case like I said

564
00:28:25,760 --> 00:28:32,279
we um basically had ver virtual Elon

565
00:28:28,840 --> 00:28:35,840
Musk and this non-existing person same

566
00:28:32,279 --> 00:28:39,480
age gender race Etc they would give the

567
00:28:35,840 --> 00:28:42,480
exact same lecture with the exact same

568
00:28:39,480 --> 00:28:44,840
soundtrack and what we learned about

569
00:28:42,480 --> 00:28:46,000
learning about vaccines this was the

570
00:28:44,840 --> 00:28:49,480
beginning of

571
00:28:46,000 --> 00:28:53,919
Co um so what we learned in our

572
00:28:49,480 --> 00:28:56,519
experiment is that the more people liked

573
00:28:53,919 --> 00:28:58,919
ilon musk and they were told this is not

574
00:28:56,519 --> 00:29:02,039
the real ilon musk the system itself

575
00:28:58,919 --> 00:29:03,640
said I am a virtual Elon Musk here to

576
00:29:02,039 --> 00:29:06,159
tell you about

577
00:29:03,640 --> 00:29:08,720
vaccines the more people liked and

578
00:29:06,159 --> 00:29:12,000
admired Elon Musk the more they thought

579
00:29:08,720 --> 00:29:15,640
this teacher was uh great they wanted to

580
00:29:12,000 --> 00:29:18,200
learn more about vaccines Etc so it had

581
00:29:15,640 --> 00:29:21,279
this huge impact even though it was a

582
00:29:18,200 --> 00:29:24,159
deep fake of Elon Musk and it was the

583
00:29:21,279 --> 00:29:25,760
exact same experience other than that

584
00:29:24,159 --> 00:29:28,559
talking pH

585
00:29:25,760 --> 00:29:31,440
basically uh being different so so these

586
00:29:28,559 --> 00:29:34,320
are all very important issues in

587
00:29:31,440 --> 00:29:38,120
deploying AI that we should be more

588
00:29:34,320 --> 00:29:41,679
careful about um another last study that

589
00:29:38,120 --> 00:29:45,840
we just completed does AI make us more

590
00:29:41,679 --> 00:29:48,600
creative we had three groups one group

591
00:29:45,840 --> 00:29:51,120
uh we asked everybody to write sat

592
00:29:48,600 --> 00:29:54,480
essays actually um they got the same

593
00:29:51,120 --> 00:29:57,399
prompts and one group could use chat GPT

594
00:29:54,480 --> 00:29:59,960
if they wanted one group could use uh

595
00:29:57,399 --> 00:30:02,679
Google search SE and a third group did

596
00:29:59,960 --> 00:30:05,799
not have any tools to write their sat

597
00:30:02,679 --> 00:30:09,480
essay same 30 minutes that you get uh on

598
00:30:05,799 --> 00:30:13,039
the SAT what we noticed we actually

599
00:30:09,480 --> 00:30:15,399
looked at brain activity the group that

600
00:30:13,039 --> 00:30:18,720
wasn't using anything at the bottom here

601
00:30:15,399 --> 00:30:21,640
the bottom row a lot more brain activity

602
00:30:18,720 --> 00:30:23,760
than the group that used Google search

603
00:30:21,640 --> 00:30:27,640
versus the group the group that used

604
00:30:23,760 --> 00:30:32,200
chat GPT very little brain activity in

605
00:30:27,640 --> 00:30:35,080
that group but even worse um we looked

606
00:30:32,200 --> 00:30:37,080
at the topics of the essays that people

607
00:30:35,080 --> 00:30:40,159
came up with we graded them as well we

608
00:30:37,080 --> 00:30:43,519
had teachers gr these essays the essays

609
00:30:40,159 --> 00:30:46,840
actually got comparable ratings from

610
00:30:43,519 --> 00:30:49,480
people but what we noticed was that all

611
00:30:46,840 --> 00:30:51,320
of the people in the chat GPT group they

612
00:30:49,480 --> 00:30:54,480
talked about the same things in their

613
00:30:51,320 --> 00:30:57,480
essays so they were all writing similar

614
00:30:54,480 --> 00:31:00,120
essays while the people that used search

615
00:30:57,480 --> 00:31:02,960
had a a little bit more variety and the

616
00:31:00,120 --> 00:31:04,919
people that came up with these articles

617
00:31:02,960 --> 00:31:07,960
themselves using their own brains they

618
00:31:04,919 --> 00:31:11,320
were much more original this points

619
00:31:07,960 --> 00:31:13,799
again at a possible danger if you deploy

620
00:31:11,320 --> 00:31:18,039
AI in your company and all your people

621
00:31:13,799 --> 00:31:20,960
use that AI to fulfill a certain task or

622
00:31:18,039 --> 00:31:24,039
um basically all of the results will be

623
00:31:20,960 --> 00:31:26,279
more homogeneous but that also means

624
00:31:24,039 --> 00:31:28,440
that you're not improving you're not

625
00:31:26,279 --> 00:31:31,480
like discovering new ways of doing

626
00:31:28,440 --> 00:31:34,399
things Etc and potentially of course

627
00:31:31,480 --> 00:31:36,320
there's the whole deskilling issue that

628
00:31:34,399 --> 00:31:38,039
nobody is actually learning anything

629
00:31:36,320 --> 00:31:41,799
from doing this

630
00:31:38,039 --> 00:31:46,960
work so we need to optimize the behavior

631
00:31:41,799 --> 00:31:50,240
of combined human plus AI systems today

632
00:31:46,960 --> 00:31:53,320
um in AI research and development all

633
00:31:50,240 --> 00:31:56,039
you hear about is training models and

634
00:31:53,320 --> 00:31:58,720
then testing them for certain objectives

635
00:31:56,039 --> 00:32:02,799
and you try to improve accur Y and

636
00:31:58,720 --> 00:32:04,519
efficiency and reduce bias Etc and and

637
00:32:02,799 --> 00:32:07,279
reduce

638
00:32:04,519 --> 00:32:10,679
hallucinations I think it's important

639
00:32:07,279 --> 00:32:13,480
that we actually test AI in the human

640
00:32:10,679 --> 00:32:16,639
context where it's meant to be used and

641
00:32:13,480 --> 00:32:19,519
that we look at different objectives

642
00:32:16,639 --> 00:32:23,360
what is the decision making made by

643
00:32:19,519 --> 00:32:25,519
people plus AI look like also not just

644
00:32:23,360 --> 00:32:29,679
in the short term but in the long term

645
00:32:25,519 --> 00:32:33,559
do we risk that certain skill Etc are uh

646
00:32:29,679 --> 00:32:37,080
being lost and so on so that's the point

647
00:32:33,559 --> 00:32:40,120
that I wanted to make uh with um my uh

648
00:32:37,080 --> 00:32:42,399
presentation here I do believe that AI

649
00:32:40,120 --> 00:32:45,240
is here to stay and I do believe there

650
00:32:42,399 --> 00:32:48,279
are great uh uh there's a great

651
00:32:45,240 --> 00:32:52,559
potential for AI but let's make sure

652
00:32:48,279 --> 00:32:55,919
that we design AI to preserve to augment

653
00:32:52,559 --> 00:32:59,600
people and preserve human agency meaning

654
00:32:55,919 --> 00:33:02,240
and control and that is not happening um

655
00:32:59,600 --> 00:33:04,679
in today's AI developments and

656
00:33:02,240 --> 00:33:09,279
deployments so if you're interested in

657
00:33:04,679 --> 00:33:12,159
this um email me um if you're interested

658
00:33:09,279 --> 00:33:15,399
in getting involved Etc and then I was

659
00:33:12,159 --> 00:33:18,919
also asked so email is Patty media.

660
00:33:15,399 --> 00:33:23,600
mit.edu I was also asked to put in a

661
00:33:18,919 --> 00:33:27,240
plug for this course on AI that MIT

662
00:33:23,600 --> 00:33:30,000
expro um is uh organizing it's a six

663
00:33:27,240 --> 00:33:32,799
week course starting on October 15th

664
00:33:30,000 --> 00:33:35,080
remote course uh where you can learn

665
00:33:32,799 --> 00:33:38,960
from a couple of media lab people as

666
00:33:35,080 --> 00:33:42,480
well as a lot of um seale people about

667
00:33:38,960 --> 00:33:47,120
um AI today and this uh driving

668
00:33:42,480 --> 00:33:47,120
Innovation with generative AI thank

669
00:33:51,320 --> 00:33:57,799
you yeah so I think we have a couple of

670
00:33:54,120 --> 00:33:59,240
minutes for questions also if if there

671
00:33:57,799 --> 00:34:05,080
are

672
00:33:59,240 --> 00:34:05,080
any on oh they're on the screen oh

673
00:34:06,000 --> 00:34:11,280
good yeah yeah so if first question with

674
00:34:10,200 --> 00:34:14,119
lots of

675
00:34:11,280 --> 00:34:16,560
vots how does this affect societal

676
00:34:14,119 --> 00:34:19,919
differentiation do the rich and more

677
00:34:16,560 --> 00:34:23,720
accessible um uh tech oh do the rich and

678
00:34:19,919 --> 00:34:27,240
more accessible get richer um and um who

679
00:34:23,720 --> 00:34:30,119
is making money of all of this Etc so

680
00:34:27,240 --> 00:34:32,839
one we reason why I have always been

681
00:34:30,119 --> 00:34:35,560
very interested in technology and what

682
00:34:32,839 --> 00:34:38,280
it can do for people is that I do

683
00:34:35,560 --> 00:34:41,200
believe it is actually a uh

684
00:34:38,280 --> 00:34:44,280
democratizing or can be a democratizing

685
00:34:41,200 --> 00:34:46,599
force these days a phone doesn't

686
00:34:44,280 --> 00:34:50,040
necessarily need to cost a lot of money

687
00:34:46,599 --> 00:34:53,599
and it is giving access to uh so many

688
00:34:50,040 --> 00:34:57,480
people out there uh to all the wealth of

689
00:34:53,599 --> 00:35:00,880
knowledge uh information Etc uh in the

690
00:34:57,480 --> 00:35:03,320
world world that used to be only

691
00:35:00,880 --> 00:35:07,599
available and accessible to people if

692
00:35:03,320 --> 00:35:10,119
you came or were born in a family uh

693
00:35:07,599 --> 00:35:12,640
with money that valued

694
00:35:10,119 --> 00:35:15,119
education um in this country even that

695
00:35:12,640 --> 00:35:16,760
can afford to often send you to private

696
00:35:15,119 --> 00:35:20,599
school if there are no good public

697
00:35:16,760 --> 00:35:23,680
schools around Etc so I think overall

698
00:35:20,599 --> 00:35:26,800
that um technology is still a

699
00:35:23,680 --> 00:35:29,800
democratizing force that can give uh

700
00:35:26,800 --> 00:35:33,240
make certain opportunities more

701
00:35:29,800 --> 00:35:36,440
accessible uh to a wider range of people

702
00:35:33,240 --> 00:35:39,320
but as the uh question answer uh uh

703
00:35:36,440 --> 00:35:42,800
question asker also asked we have to be

704
00:35:39,320 --> 00:35:46,920
very careful um uh how this is deployed

705
00:35:42,800 --> 00:35:50,200
what the business models are Etc um and

706
00:35:46,920 --> 00:35:52,920
who is ultimately paying uh for all of

707
00:35:50,200 --> 00:35:55,280
this today's phone and a lot of today's

708
00:35:52,920 --> 00:35:59,079
phone services and internet

709
00:35:55,280 --> 00:36:02,240
services uh while they seem free you are

710
00:35:59,079 --> 00:36:04,520
ultimately the one that is paying um you

711
00:36:02,240 --> 00:36:08,319
are paying by giving them your data

712
00:36:04,520 --> 00:36:12,560
making yourself more uh easy to

713
00:36:08,319 --> 00:36:15,200
manipulate um and influence so we have

714
00:36:12,560 --> 00:36:18,319
to think about uh business models for

715
00:36:15,200 --> 00:36:22,520
all of this very much as

716
00:36:18,319 --> 00:36:26,440
well more questions oh yes I'm sorry I

717
00:36:22,520 --> 00:36:26,440
need that memory augmentation

718
00:36:28,839 --> 00:36:33,480
yeah when and where does ethics of these

719
00:36:31,119 --> 00:36:36,359
developments come in is it always

720
00:36:33,480 --> 00:36:40,200
hindsight I think it should not be

721
00:36:36,359 --> 00:36:45,200
hindsight obviously and and um I think

722
00:36:40,200 --> 00:36:48,880
that um by involving uh people like the

723
00:36:45,200 --> 00:36:51,800
Elder um individuals that I uh uh talked

724
00:36:48,880 --> 00:36:55,839
about uh or Target users in the

725
00:36:51,800 --> 00:36:59,400
development of AI it enables us to think

726
00:36:55,839 --> 00:37:02,760
of the ethics um uh sort of and be more

727
00:36:59,400 --> 00:37:05,119
mindful of ethical considerations as we

728
00:37:02,760 --> 00:37:08,680
develop these systems and still can

729
00:37:05,119 --> 00:37:12,240
change them so this is another reason I

730
00:37:08,680 --> 00:37:15,680
think why uh we shouldn't just develop

731
00:37:12,240 --> 00:37:17,680
Ai and then drop it on the world which

732
00:37:15,680 --> 00:37:20,720
is pretty much what Silicon Valley is

733
00:37:17,680 --> 00:37:23,880
doing right now but a reason why we

734
00:37:20,720 --> 00:37:26,119
should do more evaluations to see what

735
00:37:23,880 --> 00:37:29,000
happens what are the possible uh

736
00:37:26,119 --> 00:37:31,440
consequences uh not just for individuals

737
00:37:29,000 --> 00:37:35,640
as a lot of the work that I've shown but

738
00:37:31,440 --> 00:37:38,880
also for communities for society Etc um

739
00:37:35,640 --> 00:37:42,440
it's very important that we um talk

740
00:37:38,880 --> 00:37:44,960
about all of that very early on in um

741
00:37:42,440 --> 00:37:47,839
these uh development

742
00:37:44,960 --> 00:37:48,599
processes yeah we have maybe time for

743
00:37:47,839 --> 00:37:51,880
one

744
00:37:48,599 --> 00:37:54,599
more who decides and how do we decide

745
00:37:51,880 --> 00:37:57,520
what information is misinformation I

746
00:37:54,599 --> 00:37:59,560
don't have a uh necessarily a solution

747
00:37:57,520 --> 00:38:02,440
for that problem it seems that we don't

748
00:37:59,560 --> 00:38:04,160
have a notion of truth anymore

749
00:38:02,440 --> 00:38:06,960
especially in this country but

750
00:38:04,160 --> 00:38:13,520
increasingly we're in other countries as

751
00:38:06,960 --> 00:38:17,119
well um so I think uh that um it's

752
00:38:13,520 --> 00:38:20,920
important that people generally learn to

753
00:38:17,119 --> 00:38:24,640
think more critically and um I think I

754
00:38:20,920 --> 00:38:28,040
do fear that with AI there is a danger

755
00:38:24,640 --> 00:38:31,280
that we actually Outsource critical

756
00:38:28,040 --> 00:38:34,520
thinking to Ai and so there is then

757
00:38:31,280 --> 00:38:38,400
definitely more of a danger for

758
00:38:34,520 --> 00:38:41,280
misinformation to influence us so I

759
00:38:38,400 --> 00:38:44,760
think we have to design interfaces for

760
00:38:41,280 --> 00:38:47,200
working with AI that ultimately improve

761
00:38:44,760 --> 00:38:50,520
people's thinking people's critical

762
00:38:47,200 --> 00:38:53,440
thinking Etc that is not happening today

763
00:38:50,520 --> 00:38:56,520
as I showed with some of our studies if

764
00:38:53,440 --> 00:38:59,079
you um ask are wondering about something

765
00:38:56,520 --> 00:39:03,960
and an AI give gives you this very

766
00:38:59,079 --> 00:39:07,520
elaborate comprehensive 10o onepage

767
00:39:03,960 --> 00:39:10,000
answer you are no longer motivated to

768
00:39:07,520 --> 00:39:12,520
even think about like oh well what do I

769
00:39:10,000 --> 00:39:16,000
think about that issue because it seems

770
00:39:12,520 --> 00:39:18,839
like it's all there and and um so people

771
00:39:16,000 --> 00:39:23,040
become a little bit lazier and I think

772
00:39:18,839 --> 00:39:25,800
we have to rethink interfaces to AI so

773
00:39:23,040 --> 00:39:27,920
that they engage us more in thinking

774
00:39:25,800 --> 00:39:30,480
they think more about what

775
00:39:27,920 --> 00:39:33,800
um I mean uh about or they're developed

776
00:39:30,480 --> 00:39:36,839
more to improve our critical thinking

777
00:39:33,800 --> 00:39:39,720
abilities rather than just providing

778
00:39:36,839 --> 00:39:43,079
answers and that's definitely true in

779
00:39:39,720 --> 00:39:46,000
education of course I wish we had AI

780
00:39:43,079 --> 00:39:49,119
systems for learning that just ask

781
00:39:46,000 --> 00:39:51,560
questions that's the Socratic method no

782
00:39:49,119 --> 00:39:54,040
of teaching you ask really important

783
00:39:51,560 --> 00:39:57,440
interesting questions to make the person

784
00:39:54,040 --> 00:40:00,240
come up with the insights themselves um

785
00:39:57,440 --> 00:40:03,839
so let's think a little bit more broadly

786
00:40:00,240 --> 00:40:08,560
about um interfaces with AI that uh

787
00:40:03,839 --> 00:40:12,000
maybe have some uh of these U desired uh

788
00:40:08,560 --> 00:40:15,960
outcomes thank you what a wonderful talk

789
00:40:12,000 --> 00:40:15,960
thank you so much Ted

