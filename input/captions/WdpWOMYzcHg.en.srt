1
00:00:00,199 --> 00:00:04,520
my name is Sunan I'm glad to present

2
00:00:02,600 --> 00:00:07,000
efficient multimodel large language

3
00:00:04,520 --> 00:00:08,960
model on edge devices so on the right

4
00:00:07,000 --> 00:00:12,000
hand side is a data center with a lot of

5
00:00:08,960 --> 00:00:15,160
GPU uh servers the image is not real

6
00:00:12,000 --> 00:00:17,960
it's generated and we are proud to U be

7
00:00:15,160 --> 00:00:20,720
able to run this uh model on a laptop

8
00:00:17,960 --> 00:00:24,119
the model is called

9
00:00:20,720 --> 00:00:26,679
s so we need efficient AI Computing uh

10
00:00:24,119 --> 00:00:29,320
we see on the left figure uh this is the

11
00:00:26,679 --> 00:00:32,160
supply and demand of computing uh the

12
00:00:29,320 --> 00:00:34,559
green is the GPU memory the red is the

13
00:00:32,160 --> 00:00:37,840
model size the large language model size

14
00:00:34,559 --> 00:00:40,719
is growing much faster compared with the

15
00:00:37,840 --> 00:00:43,200
uh amount of GPU memory we have on the

16
00:00:40,719 --> 00:00:46,600
right hand side is showing the advanced

17
00:00:43,200 --> 00:00:49,719
technology noes design cost from 65

18
00:00:46,600 --> 00:00:51,559
nanometer all the way to 5 nanometer the

19
00:00:49,719 --> 00:00:53,800
the proportion of software is just

20
00:00:51,559 --> 00:00:57,160
getting more and more expensive software

21
00:00:53,800 --> 00:00:59,480
is important the cost is very high so

22
00:00:57,160 --> 00:01:02,879
our research focus on using software

23
00:00:59,480 --> 00:01:05,560
approach to make Hardware more

24
00:01:02,879 --> 00:01:07,799
efficient this is some of my prior work

25
00:01:05,560 --> 00:01:09,920
including deep compression which can

26
00:01:07,799 --> 00:01:12,600
drastically reduced the model size by an

27
00:01:09,920 --> 00:01:14,920
order of magnitude with pruning and

28
00:01:12,600 --> 00:01:17,479
quantization these techniques has become

29
00:01:14,920 --> 00:01:20,600
industry standard and a standard lexicon

30
00:01:17,479 --> 00:01:22,840
in our field widely used um for

31
00:01:20,600 --> 00:01:25,560
efficient air Computing industry these

32
00:01:22,840 --> 00:01:27,560
days uh later I designed this efficient

33
00:01:25,560 --> 00:01:30,159
inference engine which is a hardware

34
00:01:27,560 --> 00:01:32,200
accelerator toward this spars and

35
00:01:30,159 --> 00:01:34,040
compressed neural networks without

36
00:01:32,200 --> 00:01:37,040
having to decompress it we can directly

37
00:01:34,040 --> 00:01:40,320
run infuence on the compressed model so

38
00:01:37,040 --> 00:01:43,399
that is the one of the top five most

39
00:01:40,320 --> 00:01:46,000
cited paper in the 50 years of isca isca

40
00:01:43,399 --> 00:01:50,079
is a flagship conference in our in our

41
00:01:46,000 --> 00:01:52,799
field from 1953 to

42
00:01:50,079 --> 00:01:54,960
2023 and recently model compression

43
00:01:52,799 --> 00:01:56,560
about pruni and sparcity is gaining a

44
00:01:54,960 --> 00:01:58,640
lot of momentum with a lot of

45
00:01:56,560 --> 00:02:01,680
Publications uh since deep compression

46
00:01:58,640 --> 00:02:04,439
was published

47
00:02:01,680 --> 00:02:06,439
so our efficient ml project Bridges the

48
00:02:04,439 --> 00:02:09,280
gap between the supply and demand of

49
00:02:06,439 --> 00:02:11,800
computing from the following aspects uh

50
00:02:09,280 --> 00:02:15,080
we co-design software and Hardware from

51
00:02:11,800 --> 00:02:17,840
Tiny ml which is uh in my early days at

52
00:02:15,080 --> 00:02:21,160
MIT versus large language model which is

53
00:02:17,840 --> 00:02:23,959
my recent work at MIT we focus on both

54
00:02:21,160 --> 00:02:25,920
dce model and also sparse model in in

55
00:02:23,959 --> 00:02:29,080
fact there's a lot of opportunity to

56
00:02:25,920 --> 00:02:32,360
exploit sparsely to accelerate and

57
00:02:29,080 --> 00:02:34,959
compress neuron make it run faster a lot

58
00:02:32,360 --> 00:02:37,720
cheaper um including using both full

59
00:02:34,959 --> 00:02:41,360
precision and quantization uh from not

60
00:02:37,720 --> 00:02:43,879
only 8bit to even 4 bit like nidia's

61
00:02:41,360 --> 00:02:46,920
recent GPU uh the black well GPU has

62
00:02:43,879 --> 00:02:49,120
this amazing feature uh fp4 using only

63
00:02:46,920 --> 00:02:51,800
four bit floating point to represent the

64
00:02:49,120 --> 00:02:53,560
numbers so we are showing very Pro

65
00:02:51,800 --> 00:02:56,440
promising results to go with low

66
00:02:53,560 --> 00:02:58,280
Precision to uh decrease the cost of

67
00:02:56,440 --> 00:03:01,519
inference and decrease the cost of

68
00:02:58,280 --> 00:03:03,120
training using fp8

69
00:03:01,519 --> 00:03:06,280
uh we focus both on inference and

70
00:03:03,120 --> 00:03:09,879
training um as the skating law continue

71
00:03:06,280 --> 00:03:11,920
to um to to scale with not only on the

72
00:03:09,879 --> 00:03:15,360
training side but also inference time

73
00:03:11,920 --> 00:03:17,879
scating is getting super popular um in

74
00:03:15,360 --> 00:03:20,120
uh recent releases from CH

75
00:03:17,879 --> 00:03:22,040
GPT uh we focus not only on

76
00:03:20,120 --> 00:03:25,040
discriminative model but also generative

77
00:03:22,040 --> 00:03:26,879
models Genera images we we have already

78
00:03:25,040 --> 00:03:29,200
done we're also working on generating

79
00:03:26,879 --> 00:03:32,319
videos which is super computationally

80
00:03:29,200 --> 00:03:35,480
heavy and which require a lot of gpus so

81
00:03:32,319 --> 00:03:38,239
if a startup recently raised $10 million

82
00:03:35,480 --> 00:03:40,439
it can purchase only a few hundred gpus

83
00:03:38,239 --> 00:03:43,519
and that's it it's super expensive in

84
00:03:40,439 --> 00:03:45,560
the field so making AI more um more

85
00:03:43,519 --> 00:03:49,519
efficient can reduce the cost and we'll

86
00:03:45,560 --> 00:03:54,000
make democratize AI to diverse field of

87
00:03:49,519 --> 00:03:56,280
lives and improve the um efficiency for

88
00:03:54,000 --> 00:03:58,840
the practitioners previously we have to

89
00:03:56,280 --> 00:04:00,840
take a long time to train a model but

90
00:03:58,840 --> 00:04:03,760
with such effic efficiency techniques we

91
00:04:00,840 --> 00:04:04,640
can greatly reduce the time and improve

92
00:04:03,760 --> 00:04:07,200
the

93
00:04:04,640 --> 00:04:09,360
productivity and finally we focus on not

94
00:04:07,200 --> 00:04:12,120
only a single modality like text but

95
00:04:09,360 --> 00:04:14,280
also multiple modalities including

96
00:04:12,120 --> 00:04:17,000
images videos

97
00:04:14,280 --> 00:04:19,320
Etc to start with I will first talk

98
00:04:17,000 --> 00:04:21,959
about large language model

99
00:04:19,320 --> 00:04:24,840
quantization so quantization can map a

100
00:04:21,959 --> 00:04:27,919
floating Point number into a integer or

101
00:04:24,840 --> 00:04:30,600
even low bit floting Point like ip8 or

102
00:04:27,919 --> 00:04:32,560
ip4 uh the challenge of applying

103
00:04:30,600 --> 00:04:35,560
quantization to large language model is

104
00:04:32,560 --> 00:04:38,520
that there's a lot of outliers making it

105
00:04:35,560 --> 00:04:40,919
difficult to quantize we designed a very

106
00:04:38,520 --> 00:04:43,160
smart idea mathematical eal approach to

107
00:04:40,919 --> 00:04:46,000
migrate the quantization difficulty like

108
00:04:43,160 --> 00:04:48,039
these three channels of outlier we

109
00:04:46,000 --> 00:04:50,240
migrate the quantization difficulty from

110
00:04:48,039 --> 00:04:52,680
activation to the weight so the

111
00:04:50,240 --> 00:04:55,759
activation becomes much smoother to

112
00:04:52,680 --> 00:04:57,680
quantize and the weight is also uh is

113
00:04:55,759 --> 00:05:00,680
harder but still relatively easy to

114
00:04:57,680 --> 00:05:02,680
quantize compared with here as a result

115
00:05:00,680 --> 00:05:06,160
we can reduce the serving cost from

116
00:05:02,680 --> 00:05:11,120
eight gpus to only four gpus by

117
00:05:06,160 --> 00:05:14,520
switching from fp16 to in8 reducing the

118
00:05:11,120 --> 00:05:18,280
memory by half while the latency is even

119
00:05:14,520 --> 00:05:21,520
shorter and the principle is that 100 *

120
00:05:18,280 --> 00:05:24,080
1 equal to 10 by 10 so using this way we

121
00:05:21,520 --> 00:05:26,520
can equalize the quantization difficulty

122
00:05:24,080 --> 00:05:29,720
between activation and

123
00:05:26,520 --> 00:05:32,720
weights not only 8 bit we went one step

124
00:05:29,720 --> 00:05:35,840
further for forbit quantization using

125
00:05:32,720 --> 00:05:39,240
awq activation aware weight quantization

126
00:05:35,840 --> 00:05:41,560
for large language model uh quantization

127
00:05:39,240 --> 00:05:44,440
so we want to deploy large language

128
00:05:41,560 --> 00:05:47,639
models on mobile on edge devices like on

129
00:05:44,440 --> 00:05:50,680
the cars on AI PC like here we can

130
00:05:47,639 --> 00:05:53,720
deploy U this coding model locally on a

131
00:05:50,680 --> 00:05:55,960
laptop to write a c sort an array in a c

132
00:05:53,720 --> 00:05:59,199
program and it's reading pretty writing

133
00:05:55,960 --> 00:06:00,280
the code pretty fast even give you the

134
00:05:59,199 --> 00:06:03,960
uh the Cod

135
00:06:00,280 --> 00:06:06,080
per so the idea is that when quantizing

136
00:06:03,960 --> 00:06:08,240
the uh weight we should not just look at

137
00:06:06,080 --> 00:06:11,000
the weight distribution and this

138
00:06:08,240 --> 00:06:14,039
activation awareness is the primary

139
00:06:11,000 --> 00:06:16,360
focus uh we find by not quantizing only

140
00:06:14,039 --> 00:06:20,039
1% of the channel can greatly improve

141
00:06:16,360 --> 00:06:23,000
the accuracy but increasing um but using

142
00:06:20,039 --> 00:06:26,000
ip16 introduces mixed Precision so we

143
00:06:23,000 --> 00:06:28,240
avoid that by just multiplying scating

144
00:06:26,000 --> 00:06:31,000
this channel by a larger number so

145
00:06:28,240 --> 00:06:33,840
scating by two if actively can increase

146
00:06:31,000 --> 00:06:37,039
the number of bits by one so for example

147
00:06:33,840 --> 00:06:39,360
0.5 and one they cannot be distinguished

148
00:06:37,039 --> 00:06:41,560
they both will be quantized to one but

149
00:06:39,360 --> 00:06:43,840
if you multiply them by two they become

150
00:06:41,560 --> 00:06:46,160
one and two now you can distinguish them

151
00:06:43,840 --> 00:06:48,680
so that's the fundamental principle and

152
00:06:46,160 --> 00:06:50,560
it's mathematically equal so we can have

153
00:06:48,680 --> 00:06:52,319
very good accuracy while maintaining the

154
00:06:50,560 --> 00:06:54,120
accuracy while accelerating the

155
00:06:52,319 --> 00:06:56,720
inference by about three times the

156
00:06:54,120 --> 00:06:58,800
theoretical saving is about four times

157
00:06:56,720 --> 00:07:02,560
that has been adopted by Nvidia chat

158
00:06:58,800 --> 00:07:05,440
with RTX to chat with RTX

159
00:07:02,560 --> 00:07:08,960
laptop we also built this CH tiny chat

160
00:07:05,440 --> 00:07:12,599
computer to realize uh the algorithm so

161
00:07:08,960 --> 00:07:15,919
we 3D printed the compu uh the the tiny

162
00:07:12,599 --> 00:07:18,280
CH computer and it's asking about the

163
00:07:15,919 --> 00:07:21,199
attractions in Boston is giving Museum

164
00:07:18,280 --> 00:07:23,199
of Science Harvard Square Freedom Trail

165
00:07:21,199 --> 00:07:26,240
Etc and in the back you can see it's

166
00:07:23,199 --> 00:07:27,240
powered by a very small Json AR Nano in

167
00:07:26,240 --> 00:07:29,919
the

168
00:07:27,240 --> 00:07:32,160
back so we believe such power and

169
00:07:29,919 --> 00:07:34,000
quantisation techniques can enable

170
00:07:32,160 --> 00:07:36,360
expensive large language model to be

171
00:07:34,000 --> 00:07:39,280
deployed on resource constrained low

172
00:07:36,360 --> 00:07:39,280
power uh

173
00:07:39,440 --> 00:07:44,240
devices uh so these quantization

174
00:07:41,879 --> 00:07:46,720
techniques smooth Quant awq has been

175
00:07:44,240 --> 00:07:49,039
widely adopted by industry it has more

176
00:07:46,720 --> 00:07:51,120
than 17 million downloads on huging

177
00:07:49,039 --> 00:07:54,080
phase so if you're interested in

178
00:07:51,120 --> 00:07:57,520
applying T awq in your products we come

179
00:07:54,080 --> 00:08:00,360
to check out our uh code base is fully

180
00:07:57,520 --> 00:08:03,159
released under permissive license so you

181
00:08:00,360 --> 00:08:05,400
can grab it for free and there are many

182
00:08:03,159 --> 00:08:09,000
companies using these techniques

183
00:08:05,400 --> 00:08:11,479
including nvidia's tens RT which is a

184
00:08:09,000 --> 00:08:14,080
flagship inference library for large

185
00:08:11,479 --> 00:08:17,199
language model Intel neuro compressor

186
00:08:14,080 --> 00:08:20,319
and Q8 Chad we collaborate IBM the

187
00:08:17,199 --> 00:08:22,400
integrated for the grite also Berkeley

188
00:08:20,319 --> 00:08:25,000
integrated tiny chat huging face

189
00:08:22,400 --> 00:08:29,960
Transformer make St standard package and

190
00:08:25,000 --> 00:08:29,960
also several startups adopted awq

191
00:08:30,319 --> 00:08:34,800
now let's switch gear from large

192
00:08:32,440 --> 00:08:36,719
language model to multimodel large

193
00:08:34,800 --> 00:08:39,399
language model we want to handle not

194
00:08:36,719 --> 00:08:41,479
only language but also Vision okay so we

195
00:08:39,399 --> 00:08:43,959
want to deploy them also locally on edge

196
00:08:41,479 --> 00:08:46,040
device like chess and or for example

197
00:08:43,959 --> 00:08:48,600
here we can ask what is unusual about it

198
00:08:46,040 --> 00:08:50,440
scene any safety concerns and the model

199
00:08:48,600 --> 00:08:53,160
can understand the image saying this is

200
00:08:50,440 --> 00:08:56,360
a person hiding from a rope attached to

201
00:08:53,160 --> 00:08:58,040
a wind the turbo turbine engine um

202
00:08:56,360 --> 00:09:00,240
there's risk of falling from a

203
00:08:58,040 --> 00:09:03,880
significant height

204
00:09:00,240 --> 00:09:06,399
and this is also a um in the factory

205
00:09:03,880 --> 00:09:09,680
scenario we train the model and can

206
00:09:06,399 --> 00:09:13,600
answer how many cars are jacked up um

207
00:09:09,680 --> 00:09:16,480
there and also what color is the gloves

208
00:09:13,600 --> 00:09:19,160
such kind of QA in a real time manner

209
00:09:16,480 --> 00:09:20,760
running just locally on a media just

210
00:09:19,160 --> 00:09:24,079
orent a mobile

211
00:09:20,760 --> 00:09:26,440
GPU and what we how we achieve that is

212
00:09:24,079 --> 00:09:29,079
we convert all the modalities into

213
00:09:26,440 --> 00:09:31,839
tokens so everything can be tokenized

214
00:09:29,079 --> 00:09:34,720
now not only language but also images we

215
00:09:31,839 --> 00:09:37,880
convert image into patches and feed it

216
00:09:34,720 --> 00:09:40,079
into a vision encoder plus a projector

217
00:09:37,880 --> 00:09:43,160
such that we have both visual token and

218
00:09:40,079 --> 00:09:47,200
text token we feed such kind of tokens

219
00:09:43,160 --> 00:09:49,079
to L fine-tune the L um and make the

220
00:09:47,200 --> 00:09:52,000
final

221
00:09:49,079 --> 00:09:54,399
prediction and this is a homogeneous uh

222
00:09:52,000 --> 00:09:57,640
approach all the techniques we invented

223
00:09:54,399 --> 00:10:00,040
before like awq quation can immediately

224
00:09:57,640 --> 00:10:02,680
help uh to increase efficiency to make

225
00:10:00,040 --> 00:10:07,200
it Deployable on the edge so here are

226
00:10:02,680 --> 00:10:09,320
some examples so how according to these

227
00:10:07,200 --> 00:10:11,720
two images how much should I pay for all

228
00:10:09,320 --> 00:10:13,640
the beer on the table I'm going to say

229
00:10:11,720 --> 00:10:17,519
the beer on the table is a Magna which

230
00:10:13,640 --> 00:10:19,399
is priced $6 each right here two bottles

231
00:10:17,519 --> 00:10:20,480
on the on the table so the total should

232
00:10:19,399 --> 00:10:22,480
be

233
00:10:20,480 --> 00:10:25,000
$12 what's the implication of

234
00:10:22,480 --> 00:10:28,720
temperature between these two images it

235
00:10:25,000 --> 00:10:30,680
says Arctic eyes decreased so global

236
00:10:28,720 --> 00:10:33,560
warming warmer

237
00:10:30,680 --> 00:10:36,360
temperatures in healthcare it can check

238
00:10:33,560 --> 00:10:38,200
the patient status it's just one model

239
00:10:36,360 --> 00:10:41,000
we can do diverse

240
00:10:38,200 --> 00:10:42,680
tasks and for driving scenario again the

241
00:10:41,000 --> 00:10:44,600
same model that's the beauty of

242
00:10:42,680 --> 00:10:46,320
foundation model you just need to tr one

243
00:10:44,600 --> 00:10:49,079
model and then generalize to different

244
00:10:46,320 --> 00:10:51,560
scenarios well previously we have to tr

245
00:10:49,079 --> 00:10:53,760
separate model for driving inside the

246
00:10:51,560 --> 00:10:55,800
car outside the car so this is inside

247
00:10:53,760 --> 00:10:58,959
the car we can check the driver if the

248
00:10:55,800 --> 00:11:00,920
driver is distracted is is on the phone

249
00:10:58,959 --> 00:11:03,519
how many many people in the car where's

250
00:11:00,920 --> 00:11:05,519
the passenger seating and the same model

251
00:11:03,519 --> 00:11:07,639
can be applied to Smart Manu uh

252
00:11:05,519 --> 00:11:11,680
manufacturing to check how many chip

253
00:11:07,639 --> 00:11:13,800
bags are picked and uh how many how how

254
00:11:11,680 --> 00:11:15,959
long does it take to pick up one chip

255
00:11:13,800 --> 00:11:18,880
bag

256
00:11:15,959 --> 00:11:21,440
Etc we also make this Deployable on a

257
00:11:18,880 --> 00:11:24,079
laptop so we can here ask what is the

258
00:11:21,440 --> 00:11:27,440
name of the painting is running locally

259
00:11:24,079 --> 00:11:29,920
on a MacBook uh this is just running on

260
00:11:27,440 --> 00:11:33,120
the CPU the name of the painting is

261
00:11:29,920 --> 00:11:35,200
Portrait of Mona Lisa with GPU it might

262
00:11:33,120 --> 00:11:36,880
be even faster and who drew this

263
00:11:35,200 --> 00:11:40,680
Leonardo

264
00:11:36,880 --> 00:11:42,160
DaVinci and here we ask it describe uh

265
00:11:40,680 --> 00:11:45,560
the painting in

266
00:11:42,160 --> 00:11:47,639
details so this is without internet um

267
00:11:45,560 --> 00:11:49,600
showing the model is so lightweighted

268
00:11:47,639 --> 00:11:52,160
you can have a local copy of that on

269
00:11:49,600 --> 00:11:54,560
your laptop say if you have proprietary

270
00:11:52,160 --> 00:11:56,880
data for your company for anything uh

271
00:11:54,560 --> 00:11:56,880
this is

272
00:11:57,399 --> 00:12:02,760
safe all right so so now let's switch

273
00:12:00,240 --> 00:12:05,160
gear from language um to image from

274
00:12:02,760 --> 00:12:07,279
image understanding to uh to image

275
00:12:05,160 --> 00:12:09,480
generation which is even more

276
00:12:07,279 --> 00:12:12,399
computationally heavy since we want to

277
00:12:09,480 --> 00:12:14,720
gener generate each pixel okay now just

278
00:12:12,399 --> 00:12:17,079
predict what is the image we want to

279
00:12:14,720 --> 00:12:19,399
generate the image pixel by pixel that's

280
00:12:17,079 --> 00:12:21,000
very computationally heavy especially

281
00:12:19,399 --> 00:12:23,199
you want to generate these high

282
00:12:21,000 --> 00:12:26,279
resolution images like here you see the

283
00:12:23,199 --> 00:12:28,560
eyes you see the makeups you see the uh

284
00:12:26,279 --> 00:12:30,959
the lips everything has a lot of details

285
00:12:28,560 --> 00:12:32,959
like the first right here lot of details

286
00:12:30,959 --> 00:12:35,600
right so high resolution image

287
00:12:32,959 --> 00:12:38,920
generation consumes a lot of uh

288
00:12:35,600 --> 00:12:41,720
Computing right so there are two

289
00:12:38,920 --> 00:12:43,920
approach um conventionally the approach

290
00:12:41,720 --> 00:12:46,560
to generate image is using the diffusion

291
00:12:43,920 --> 00:12:48,519
model so using diffusion model you start

292
00:12:46,560 --> 00:12:50,360
with a noise and then you directly

293
00:12:48,519 --> 00:12:52,639
predict what is the noise and then you

294
00:12:50,360 --> 00:12:54,959
Den Noise by subtracting the noise from

295
00:12:52,639 --> 00:12:57,000
the noise and then the image will get

296
00:12:54,959 --> 00:13:00,160
cleaner and cleaner finally you get the

297
00:12:57,000 --> 00:13:04,160
final output that is it ative process

298
00:13:00,160 --> 00:13:06,519
which is very slow we use a different

299
00:13:04,160 --> 00:13:09,279
approach okay we use we didn't use

300
00:13:06,519 --> 00:13:12,399
diffusion model but use the auto

301
00:13:09,279 --> 00:13:14,639
regressive model which is similar to the

302
00:13:12,399 --> 00:13:16,959
uh large language model when large

303
00:13:14,639 --> 00:13:19,440
language model is predicting the words

304
00:13:16,959 --> 00:13:22,000
Tokens The Language tokens here we

305
00:13:19,440 --> 00:13:24,120
predict the image tokens so what is the

306
00:13:22,000 --> 00:13:27,279
image token you can think of it as a

307
00:13:24,120 --> 00:13:29,399
patch patch by patch we PR predict them

308
00:13:27,279 --> 00:13:32,079
patch by patch

309
00:13:29,399 --> 00:13:33,600
um but in order to achieve this we we

310
00:13:32,079 --> 00:13:37,079
need to

311
00:13:33,600 --> 00:13:40,240
discretize the image patches just like

312
00:13:37,079 --> 00:13:42,560
language our words is discretized we

313
00:13:40,240 --> 00:13:45,920
have like 10,000 words in the English

314
00:13:42,560 --> 00:13:48,120
vocabulary for example that's sta usable

315
00:13:45,920 --> 00:13:51,199
um such that we can use this cross

316
00:13:48,120 --> 00:13:54,560
entropy laws to supervise it but image

317
00:13:51,199 --> 00:13:58,079
is not is continuous right so here we

318
00:13:54,560 --> 00:14:00,560
use discrete token discret tokenizer uh

319
00:13:58,079 --> 00:14:03,519
such that we can pred prct uh token by

320
00:14:00,560 --> 00:14:05,880
token uh and apply all the optimizations

321
00:14:03,519 --> 00:14:09,519
we did before for large language model

322
00:14:05,880 --> 00:14:12,759
now we can apply it to uh image

323
00:14:09,519 --> 00:14:15,680
generation however directly applying the

324
00:14:12,759 --> 00:14:18,279
discrete token uh will hurt the accuracy

325
00:14:15,680 --> 00:14:20,600
you can imagine um everything in the

326
00:14:18,279 --> 00:14:23,279
world can be tokenized the images in the

327
00:14:20,600 --> 00:14:27,000
world can be tokenized into like a

328
00:14:23,279 --> 00:14:29,600
vocabulary uh that's very uh coarse so

329
00:14:27,000 --> 00:14:32,320
we also designed a very small

330
00:14:29,600 --> 00:14:35,440
ridu token predictor using a very

331
00:14:32,320 --> 00:14:38,160
lightweighted model to predict the regu

332
00:14:35,440 --> 00:14:41,120
from the discrete token okay so that is

333
00:14:38,160 --> 00:14:44,560
a very small model to predict uh The

334
00:14:41,120 --> 00:14:47,800
Continuous ridu to help uh fill in those

335
00:14:44,560 --> 00:14:51,079
details such that uh we can even surpass

336
00:14:47,800 --> 00:14:53,519
some of the um diffusion models with

337
00:14:51,079 --> 00:14:57,120
respect to the Quality right here by the

338
00:14:53,519 --> 00:14:59,320
speed is about 7.7 Times Higher faster

339
00:14:57,120 --> 00:15:01,560
compared with diffusion model here are

340
00:14:59,320 --> 00:15:04,360
more examples this is a boy and a girl

341
00:15:01,560 --> 00:15:07,199
for in Love on the left hand side is our

342
00:15:04,360 --> 00:15:09,199
heart model it's Auto regressive model

343
00:15:07,199 --> 00:15:11,279
heart for hybrid Auto regressive

344
00:15:09,199 --> 00:15:14,320
Transformer and on the right hand side

345
00:15:11,279 --> 00:15:16,959
is the diffusion models uh playground

346
00:15:14,320 --> 00:15:20,480
pixel art act stable diffusion is right

347
00:15:16,959 --> 00:15:23,000
here second image full body shot of

348
00:15:20,480 --> 00:15:26,079
French woman photo photography French

349
00:15:23,000 --> 00:15:29,399
streets and this is ours it's a blurred

350
00:15:26,079 --> 00:15:31,600
in the back very good artistic style uh

351
00:15:29,399 --> 00:15:33,399
versus this is playground pixel art

352
00:15:31,600 --> 00:15:36,399
stabil diffusion

353
00:15:33,399 --> 00:15:38,440
XL last one is drawn views of waves

354
00:15:36,399 --> 00:15:42,560
crashing against the rugged cliffes in

355
00:15:38,440 --> 00:15:44,839
the BX sir again it's showing very good

356
00:15:42,560 --> 00:15:47,720
quality and the most amazing thing is

357
00:15:44,839 --> 00:15:50,399
that we make card so small such Auto

358
00:15:47,720 --> 00:15:52,360
regressive model is very friendly to run

359
00:15:50,399 --> 00:15:55,680
on the laptop compared with iterative

360
00:15:52,360 --> 00:15:59,160
diffusion models so it can run a 4090

361
00:15:55,680 --> 00:16:00,399
mobile GPU uh just in seconds we can

362
00:15:59,160 --> 00:16:03,759
also change the

363
00:16:00,399 --> 00:16:07,560
prompt uh to draw something an astronaut

364
00:16:03,759 --> 00:16:11,120
riding a horse on the moon in fungal

365
00:16:07,560 --> 00:16:13,319
style and here is a dog that has been

366
00:16:11,120 --> 00:16:17,000
meditating all the time it takes just

367
00:16:13,319 --> 00:16:20,319
seconds to generate un images it can be

368
00:16:17,000 --> 00:16:24,279
applied to different scenarios and uh

369
00:16:20,319 --> 00:16:27,800
for example for uh advertisement for

370
00:16:24,279 --> 00:16:30,800
Branding for anything that require image

371
00:16:27,800 --> 00:16:30,800
generation

372
00:16:34,839 --> 00:16:41,399
uh feel free to try the demo at heart.

373
00:16:38,160 --> 00:16:44,000
it.edu uh we make it fully open sourced

374
00:16:41,399 --> 00:16:46,480
under permissive license so feel free to

375
00:16:44,000 --> 00:16:50,639
grab it for

376
00:16:46,480 --> 00:16:52,560
free all right so another technique is

377
00:16:50,639 --> 00:16:55,040
the auto encoder which is a very

378
00:16:52,560 --> 00:16:57,399
important component for image generation

379
00:16:55,040 --> 00:17:00,759
using diffusion models so here we go

380
00:16:57,399 --> 00:17:03,199
back to also acceler diffusion models

381
00:17:00,759 --> 00:17:05,079
and we find conventional model have a

382
00:17:03,199 --> 00:17:07,160
very poor quality when the compression

383
00:17:05,079 --> 00:17:09,199
ratio is high and here we not only

384
00:17:07,160 --> 00:17:11,959
compress the model but also we want to

385
00:17:09,199 --> 00:17:14,720
compress the images remember the image

386
00:17:11,959 --> 00:17:17,919
will be converted into tokens right so

387
00:17:14,720 --> 00:17:20,079
in the future uh you plug in electricity

388
00:17:17,919 --> 00:17:22,400
and token will come out right token in

389
00:17:20,079 --> 00:17:24,600
token out but token is a very important

390
00:17:22,400 --> 00:17:26,360
resource the more the tokens the more

391
00:17:24,600 --> 00:17:28,600
the computation and the more money you

392
00:17:26,360 --> 00:17:31,039
have to pay so compressing the token is

393
00:17:28,600 --> 00:17:33,559
super important but compressing the

394
00:17:31,039 --> 00:17:35,840
token will lead to very poor quality so

395
00:17:33,559 --> 00:17:38,080
here is a very important very uh

396
00:17:35,840 --> 00:17:41,039
effective token compressor called Deep

397
00:17:38,080 --> 00:17:45,120
compression Auto encoder that can uh

398
00:17:41,039 --> 00:17:49,640
compress the image by 64 times um in

399
00:17:45,120 --> 00:17:51,679
each side so 4K 64 by 64 is 4K

400
00:17:49,640 --> 00:17:53,640
compression ratio Al together while

401
00:17:51,679 --> 00:17:56,720
conventional work can only compress to

402
00:17:53,640 --> 00:17:59,640
eight times f8 indicate eight times on

403
00:17:56,720 --> 00:18:03,039
each side well we can well maintain the

404
00:17:59,640 --> 00:18:05,679
perplexity of the rfad all the way to 64

405
00:18:03,039 --> 00:18:08,679
or 128

406
00:18:05,679 --> 00:18:10,840
times and this is by applying uh deep

407
00:18:08,679 --> 00:18:14,159
compression Auto encoder to diffusion

408
00:18:10,840 --> 00:18:16,960
model uh we can accelerate it here from

409
00:18:14,159 --> 00:18:20,400
very slow like 17 images per second on

410
00:18:16,960 --> 00:18:25,000
the right hand side um it's 19 times

411
00:18:20,400 --> 00:18:27,000
faster 335 images per second and we

412
00:18:25,000 --> 00:18:29,880
compress the token so you have a much

413
00:18:27,000 --> 00:18:33,679
smaller uh latent space compared with

414
00:18:29,880 --> 00:18:35,840
previously a 64x 64 large lat space you

415
00:18:33,679 --> 00:18:39,159
have to generate a lot of tokens now you

416
00:18:35,840 --> 00:18:41,400
can generate only a few tokens more

417
00:18:39,159 --> 00:18:43,760
lightweighted um compared with the

418
00:18:41,400 --> 00:18:46,679
original uh

419
00:18:43,760 --> 00:18:49,320
tokenizer and we applied uh this

420
00:18:46,679 --> 00:18:52,960
tokenizer to build this saana a high

421
00:18:49,320 --> 00:18:55,600
resolution image generation model so we

422
00:18:52,960 --> 00:18:59,880
very proud to generate 4K resolution

423
00:18:55,600 --> 00:19:02,280
photos um and make it about 100 times

424
00:18:59,880 --> 00:19:05,280
faster compared with the flux flux is a

425
00:19:02,280 --> 00:19:08,200
very popular probably the most popular

426
00:19:05,280 --> 00:19:11,360
model these days for image generation uh

427
00:19:08,200 --> 00:19:13,559
we accelerate by 100 times with a deep

428
00:19:11,360 --> 00:19:14,679
compression Auto encoder using a linear

429
00:19:13,559 --> 00:19:17,320
diffusion

430
00:19:14,679 --> 00:19:19,240
Transformer uh with cural fusion and

431
00:19:17,320 --> 00:19:21,440
flow based DPM

432
00:19:19,240 --> 00:19:24,960
solware so these are the images

433
00:19:21,440 --> 00:19:27,159
generated by S of only 1.6 billion

434
00:19:24,960 --> 00:19:30,000
parameters it'll be open source very

435
00:19:27,159 --> 00:19:34,159
soon so stay tuned

436
00:19:30,000 --> 00:19:34,159
uh these are more images generated by

437
00:19:36,120 --> 00:19:43,200
S all right so given the limit of time I

438
00:19:39,840 --> 00:19:45,200
will uh conclude my presentation today

439
00:19:43,200 --> 00:19:47,600
finally I want to mention all the

440
00:19:45,200 --> 00:19:49,720
techniques we discuss today is

441
00:19:47,600 --> 00:19:50,840
incorporated into my course called

442
00:19:49,720 --> 00:19:53,159
efficient

443
00:19:50,840 --> 00:19:55,240
ml. covering efficient inference

444
00:19:53,159 --> 00:19:57,039
techniques pruning quantization NE

445
00:19:55,240 --> 00:19:59,159
architecture search distillation

446
00:19:57,039 --> 00:20:01,679
efficient training techniques and also

447
00:19:59,159 --> 00:20:03,760
application specific optimizations for

448
00:20:01,679 --> 00:20:06,679
large language models and generative AI

449
00:20:03,760 --> 00:20:08,000
feel free to check it out it's also uh

450
00:20:06,679 --> 00:20:10,559
on

451
00:20:08,000 --> 00:20:13,080
YouTube and finally here are some

452
00:20:10,559 --> 00:20:16,080
library of our lab uh photo gallery of

453
00:20:13,080 --> 00:20:19,679
our lab uh including the mcet on my

454
00:20:16,080 --> 00:20:22,960
microcontroller awq on device LM and

455
00:20:19,679 --> 00:20:26,000
sping chips and many mobile uh

456
00:20:22,960 --> 00:20:31,190
demos thank you

457
00:20:26,000 --> 00:20:31,190
[Applause]

