1
00:00:01,520 --> 00:00:07,120
Good evening everybody. Uh my name is AJ

2
00:00:03,760 --> 00:00:10,320
AJ Tornator. I work at MIT libraries.

3
00:00:07,120 --> 00:00:12,320
Welcome. Welcome to the Nexus at MIT

4
00:00:10,320 --> 00:00:16,560
libraries for the screening of Citizen

5
00:00:12,320 --> 00:00:20,000
4, a 2014 documentary directed by Laura

6
00:00:16,560 --> 00:00:22,640
Press. This screening is supported by

7
00:00:20,000 --> 00:00:25,039
the SHA team's grant that provides the

8
00:00:22,640 --> 00:00:27,199
funding for the license of the film and

9
00:00:25,039 --> 00:00:29,760
also pizza in the back. So please help

10
00:00:27,199 --> 00:00:33,040
yourselves to that one. Uh this

11
00:00:29,760 --> 00:00:35,680
documentary will be introduced by two

12
00:00:33,040 --> 00:00:38,879
MIT scholars and I have the honor of

13
00:00:35,680 --> 00:00:42,719
introducing them to you. On my right

14
00:00:38,879 --> 00:00:44,640
it's Maria Garcia Montes uh who is a PhD

15
00:00:42,719 --> 00:00:48,160
candidate at the history anthropology

16
00:00:44,640 --> 00:00:51,920
and science technology and society the

17
00:00:48,160 --> 00:00:54,480
SDS program at MIT. Her dissertation

18
00:00:51,920 --> 00:00:55,920
focuses on the 20th and 21st century

19
00:00:54,480 --> 00:00:58,879
trajectories of surveillance

20
00:00:55,920 --> 00:01:00,480
technologies in Mexico. She worked as a

21
00:00:58,879 --> 00:01:02,640
digital security trainer with

22
00:01:00,480 --> 00:01:05,680
journalists and activists in Mexico at

23
00:01:02,640 --> 00:01:09,520
the time Snowden blew the whistle on the

24
00:01:05,680 --> 00:01:12,000
NSA's prison program. On my left is

25
00:01:09,520 --> 00:01:13,920
Michelle Spectre who is a post-doal

26
00:01:12,000 --> 00:01:17,280
associate at the social and ethical

27
00:01:13,920 --> 00:01:19,840
responsibilities of computing short uh

28
00:01:17,280 --> 00:01:22,000
circ program uh at the college of

29
00:01:19,840 --> 00:01:24,640
computing where she directs the new circ

30
00:01:22,000 --> 00:01:26,159
learning lab program. Her research

31
00:01:24,640 --> 00:01:28,880
focuses on the global history of

32
00:01:26,159 --> 00:01:31,280
biometric surveillance and its social

33
00:01:28,880 --> 00:01:33,280
and political implications.

34
00:01:31,280 --> 00:01:36,159
At the time that Sneran blew the whistle

35
00:01:33,280 --> 00:01:37,920
on the NSA prison program, Michelle

36
00:01:36,159 --> 00:01:40,320
worked for the US government as a

37
00:01:37,920 --> 00:01:42,880
researcher for a bioeththics advisory

38
00:01:40,320 --> 00:01:45,600
commission. It's a complicated uh

39
00:01:42,880 --> 00:01:47,520
documentary and the director is also

40
00:01:45,600 --> 00:01:50,640
very much part of the director and we

41
00:01:47,520 --> 00:01:53,600
appreciate both Marielle and Michelle to

42
00:01:50,640 --> 00:01:55,280
do a little introduction for us here. So

43
00:01:53,600 --> 00:01:58,280
welcome and thank you both.

44
00:01:55,280 --> 00:01:58,280
>> Okay.

45
00:02:01,520 --> 00:02:08,479
>> Um hi everybody. Uh thank you so much

46
00:02:04,399 --> 00:02:11,360
for coming to see uh this important film

47
00:02:08,479 --> 00:02:13,920
which came out just about a decade and a

48
00:02:11,360 --> 00:02:16,480
a decade ago but remains important till

49
00:02:13,920 --> 00:02:19,840
today. Um, and before we get started

50
00:02:16,480 --> 00:02:22,000
with our introductory remarks and the

51
00:02:19,840 --> 00:02:26,000
start of the film screening, just want

52
00:02:22,000 --> 00:02:29,200
to see by uh maybe we can start with a

53
00:02:26,000 --> 00:02:34,599
show of hands how many people in the

54
00:02:29,200 --> 00:02:34,599
room have heard of Edward Snowden.

55
00:02:37,280 --> 00:02:41,440
That's just about everybody, right?

56
00:02:39,599 --> 00:02:43,760
which is very different from what we had

57
00:02:41,440 --> 00:02:46,959
expected thinking of the MIT audience

58
00:02:43,760 --> 00:02:49,440
that were maybe 5 years old or younger

59
00:02:46,959 --> 00:02:51,280
>> when these you know whistleblowing

60
00:02:49,440 --> 00:02:53,120
happened and we have seen in our

61
00:02:51,280 --> 00:02:55,360
classrooms that for some people Snowden

62
00:02:53,120 --> 00:02:58,160
is someone that they heard of in the

63
00:02:55,360 --> 00:02:59,519
dining room in their home many years ago

64
00:02:58,160 --> 00:03:02,800
but they don't quite know what was the

65
00:02:59,519 --> 00:03:05,680
deal uh so something that was a very big

66
00:03:02,800 --> 00:03:08,319
event in our lives for a lot of MIT

67
00:03:05,680 --> 00:03:09,519
students is not um because they were

68
00:03:08,319 --> 00:03:11,599
very very young when this will all

69
00:03:09,519 --> 00:03:13,760
happened.

70
00:03:11,599 --> 00:03:16,080
>> Yeah, that's right. Yeah. In fact, just

71
00:03:13,760 --> 00:03:18,640
a bit earlier today, we uh were speaking

72
00:03:16,080 --> 00:03:20,720
with some MIT first and second years who

73
00:03:18,640 --> 00:03:23,440
uh had I think only one of them in the

74
00:03:20,720 --> 00:03:25,120
room had heard of Edward Snowden. So, um

75
00:03:23,440 --> 00:03:28,319
we're really pleased to be able to share

76
00:03:25,120 --> 00:03:30,159
the documentary um with the community.

77
00:03:28,319 --> 00:03:34,840
And one more question, who here has

78
00:03:30,159 --> 00:03:34,840
watched Citizen 4 already?

79
00:03:35,040 --> 00:03:39,760
just Eric who wants to rewatch it uh

80
00:03:37,440 --> 00:03:41,440
because of its present day relevance but

81
00:03:39,760 --> 00:03:45,599
okay so this is a new film to a lot of

82
00:03:41,440 --> 00:03:48,959
people uh Citizen 4 is a documentary

83
00:03:45,599 --> 00:03:53,120
about Edward Snowden's whistleblowing in

84
00:03:48,959 --> 00:03:55,840
2013 and so the gist of it is that um he

85
00:03:53,120 --> 00:03:58,640
discovered or he he was the one to blow

86
00:03:55,840 --> 00:04:00,879
the whistle on the secret program of the

87
00:03:58,640 --> 00:04:05,439
US government called well of especially

88
00:04:00,879 --> 00:04:08,000
of the NSA that was called Prism And the

89
00:04:05,439 --> 00:04:10,959
oversimplification of this program was

90
00:04:08,000 --> 00:04:12,720
that the government had direct access to

91
00:04:10,959 --> 00:04:17,040
company servers of big technology

92
00:04:12,720 --> 00:04:20,160
companies. So uh Google, Yahoo, uh any

93
00:04:17,040 --> 00:04:21,680
big provider of your email or any

94
00:04:20,160 --> 00:04:24,080
communications channels that you might

95
00:04:21,680 --> 00:04:27,680
use through your phone or your computer,

96
00:04:24,080 --> 00:04:29,520
the NSA had a backdoor into it and had

97
00:04:27,680 --> 00:04:34,240
privileged access to be able to run

98
00:04:29,520 --> 00:04:37,120
searches on it. So um as as in a way to

99
00:04:34,240 --> 00:04:40,320
enact its mass surveillance program at

100
00:04:37,120 --> 00:04:43,120
the time there was a lot of controversy

101
00:04:40,320 --> 00:04:46,479
around this because you know a lot of

102
00:04:43,120 --> 00:04:49,600
people felt that this was a very uh you

103
00:04:46,479 --> 00:04:52,000
know big breach in the relationship

104
00:04:49,600 --> 00:04:54,800
between the government and the people

105
00:04:52,000 --> 00:04:57,199
where uh and also a breach between the

106
00:04:54,800 --> 00:04:58,880
service providers and the people where

107
00:04:57,199 --> 00:05:00,639
people had a reasonable expectation of

108
00:04:58,880 --> 00:05:02,800
privacy when they were sending an email

109
00:05:00,639 --> 00:05:05,520
when they were sending a photograph. And

110
00:05:02,800 --> 00:05:08,240
then the government in its turn decided

111
00:05:05,520 --> 00:05:10,880
to create these back doors, this access

112
00:05:08,240 --> 00:05:14,160
to data that people presumed to be

113
00:05:10,880 --> 00:05:15,840
private. And so this had like a very big

114
00:05:14,160 --> 00:05:18,800
impact in the public opinion at the

115
00:05:15,840 --> 00:05:21,360
time. And as a result of the of the

116
00:05:18,800 --> 00:05:23,520
Snowden revelations, a lot happened. A

117
00:05:21,360 --> 00:05:26,560
lot of change happened in the tech

118
00:05:23,520 --> 00:05:28,560
world. encryption, end-to-end encryption

119
00:05:26,560 --> 00:05:31,520
became the standard for most private

120
00:05:28,560 --> 00:05:33,120
communications in Google. Gmail got

121
00:05:31,520 --> 00:05:35,600
almost immediately, I think within six

122
00:05:33,120 --> 00:05:37,520
months of the revelations, uh, Google

123
00:05:35,600 --> 00:05:40,320
finally implemented end to-end

124
00:05:37,520 --> 00:05:42,320
encryption on its email services, which

125
00:05:40,320 --> 00:05:44,080
advocates that the Electronic Frontier

126
00:05:42,320 --> 00:05:46,960
Foundation had been demanding for years

127
00:05:44,080 --> 00:05:50,080
and have been ghosted on by a lot of big

128
00:05:46,960 --> 00:05:52,320
tech companies. And so these revelations

129
00:05:50,080 --> 00:05:55,360
did catalyze a big wave of change for

130
00:05:52,320 --> 00:05:58,160
privacy uh measures in the tech

131
00:05:55,360 --> 00:06:00,479
companies but this impact was not felt

132
00:05:58,160 --> 00:06:03,680
equally across the spectrum.

133
00:06:00,479 --> 00:06:06,800
>> Right. Yeah. I definitely recall at the

134
00:06:03,680 --> 00:06:08,639
time uh that there was a lot of

135
00:06:06,800 --> 00:06:11,199
expectation that perhaps there would be

136
00:06:08,639 --> 00:06:12,960
some kind of major policy change in the

137
00:06:11,199 --> 00:06:15,759
US, that this policy change would be

138
00:06:12,960 --> 00:06:17,440
imminent, that we would perhaps um see

139
00:06:15,759 --> 00:06:20,000
the introduction and passage of

140
00:06:17,440 --> 00:06:22,800
legislation that preserves uh rights to

141
00:06:20,000 --> 00:06:25,280
privacy in the face of surveillance and

142
00:06:22,800 --> 00:06:26,800
data collection. Um and in the end, that

143
00:06:25,280 --> 00:06:28,479
didn't really end up happening. We've

144
00:06:26,800 --> 00:06:31,280
seen it happen in other places, right?

145
00:06:28,479 --> 00:06:33,840
We have GDPR in Europe. we don't have an

146
00:06:31,280 --> 00:06:36,000
equivalent policy framework here in the

147
00:06:33,840 --> 00:06:37,759
United States. And this kind of mapped

148
00:06:36,000 --> 00:06:40,000
on to what also ended up happening with

149
00:06:37,759 --> 00:06:42,880
public opinion. I mean, Marielle, you

150
00:06:40,000 --> 00:06:44,560
probably also remember this, that um

151
00:06:42,880 --> 00:06:46,639
there was quite a lot of hope among

152
00:06:44,560 --> 00:06:49,199
civil liberties activists and privacy

153
00:06:46,639 --> 00:06:51,759
activists that Edward Snowden's

154
00:06:49,199 --> 00:06:53,600
whistleblowing uh would sort of mark a

155
00:06:51,759 --> 00:06:55,360
sea change in public opinion that it

156
00:06:53,600 --> 00:06:57,520
would really get members of the general

157
00:06:55,360 --> 00:06:59,599
public to really care about issues of

158
00:06:57,520 --> 00:07:01,680
government surveillance, data

159
00:06:59,599 --> 00:07:03,520
collection, and then also protections

160
00:07:01,680 --> 00:07:05,440
that may or may not be afforded to them

161
00:07:03,520 --> 00:07:08,080
by the private companies that create the

162
00:07:05,440 --> 00:07:10,000
technologies that they use every day.

163
00:07:08,080 --> 00:07:13,039
Um, but this also didn't really happen

164
00:07:10,000 --> 00:07:15,039
as well, right? Um,

165
00:07:13,039 --> 00:07:18,240
uh, civil liberties organizations really

166
00:07:15,039 --> 00:07:20,160
latched on to, uh, Edward Snowden and

167
00:07:18,240 --> 00:07:22,319
the events that followed, but it didn't

168
00:07:20,160 --> 00:07:24,639
really end up making as much of a splash

169
00:07:22,319 --> 00:07:27,599
in public discourse, or at least not for

170
00:07:24,639 --> 00:07:30,639
very long. And I think a refrain that we

171
00:07:27,599 --> 00:07:32,479
often hear uh, today, or at least I do

172
00:07:30,639 --> 00:07:34,479
in conversations with students, with

173
00:07:32,479 --> 00:07:36,639
colleagues, um, and just sort of out and

174
00:07:34,479 --> 00:07:38,319
about is that, you know, privacy is

175
00:07:36,639 --> 00:07:39,759
dead. you know, everyone has my

176
00:07:38,319 --> 00:07:41,280
information anyway. The government has

177
00:07:39,759 --> 00:07:43,280
my information. The private companies

178
00:07:41,280 --> 00:07:44,960
have my information. So, what does it

179
00:07:43,280 --> 00:07:46,800
matter? I mean, there was a hope that,

180
00:07:44,960 --> 00:07:48,240
you know, Edward Snowden's um

181
00:07:46,800 --> 00:07:50,000
revelations about the prison program

182
00:07:48,240 --> 00:07:51,840
would sort of change that. That was an

183
00:07:50,000 --> 00:07:54,479
attitude you were already seeing in the

184
00:07:51,840 --> 00:07:57,520
early 201s,

185
00:07:54,479 --> 00:08:00,400
but from my estimation, it just it just

186
00:07:57,520 --> 00:08:02,479
didn't. And as Michelle was saying, this

187
00:08:00,400 --> 00:08:04,960
documentary was, you know, came out

188
00:08:02,479 --> 00:08:07,919
about a decade ago. Uh and the reason we

189
00:08:04,960 --> 00:08:11,440
proposed it for this series on AI uh

190
00:08:07,919 --> 00:08:13,759
thanks to HA and and and Mark is that

191
00:08:11,440 --> 00:08:15,440
even though this was not the moment when

192
00:08:13,759 --> 00:08:18,080
we were talking necessarily about

193
00:08:15,440 --> 00:08:20,319
surveillance and AI, this was one of

194
00:08:18,080 --> 00:08:23,120
those moments in that trajectory of the

195
00:08:20,319 --> 00:08:25,759
computational analysis of personal data.

196
00:08:23,120 --> 00:08:28,639
And even though as you will see these

197
00:08:25,759 --> 00:08:31,280
documentary or really this not in

198
00:08:28,639 --> 00:08:33,120
revelations have to do perhaps mostly

199
00:08:31,280 --> 00:08:35,360
with one of the problems of surveillance

200
00:08:33,120 --> 00:08:38,320
where does the government get the data.

201
00:08:35,360 --> 00:08:40,640
Um and in the present day we start to

202
00:08:38,320 --> 00:08:43,599
see you know the other problem like how

203
00:08:40,640 --> 00:08:45,279
does AI address the other the other need

204
00:08:43,599 --> 00:08:47,600
which is who will look at this data? How

205
00:08:45,279 --> 00:08:49,440
will this data be analyzed? And this not

206
00:08:47,600 --> 00:08:51,519
in revelations gave us much more of a

207
00:08:49,440 --> 00:08:54,640
poor view into the sources of data that

208
00:08:51,519 --> 00:08:56,480
the government had. Um it is still we're

209
00:08:54,640 --> 00:08:59,279
talking about computational analysis of

210
00:08:56,480 --> 00:09:02,240
data and especially data that arises

211
00:08:59,279 --> 00:09:03,839
from electronic communications. So it is

212
00:09:02,240 --> 00:09:06,800
part of this continuum of AI

213
00:09:03,839 --> 00:09:08,880
surveillance even if the term AI might

214
00:09:06,800 --> 00:09:10,880
not feature as prominently in the film

215
00:09:08,880 --> 00:09:13,600
itself because of the time that it came

216
00:09:10,880 --> 00:09:16,959
out. We also proposed it because of its

217
00:09:13,600 --> 00:09:19,279
link to um you know it's we see it as a

218
00:09:16,959 --> 00:09:21,279
productive film to discuss engineering

219
00:09:19,279 --> 00:09:24,480
ethics and professional ethics for for

220
00:09:21,279 --> 00:09:27,120
engineers. Yes, that's right. I mean one

221
00:09:24,480 --> 00:09:29,519
kind of fascinating aspect of this story

222
00:09:27,120 --> 00:09:31,040
is that Edward Snowden himself wasn't

223
00:09:29,519 --> 00:09:34,399
you know he didn't hold a high level

224
00:09:31,040 --> 00:09:36,959
position. Um he was you know a

225
00:09:34,399 --> 00:09:40,560
contractor essentially working on a

226
00:09:36,959 --> 00:09:43,120
large government project. Um

227
00:09:40,560 --> 00:09:45,279
and so you know his actions raise a lot

228
00:09:43,120 --> 00:09:47,200
of questions I think both for engineers

229
00:09:45,279 --> 00:09:49,360
and also for people doing government

230
00:09:47,200 --> 00:09:52,560
work about what their moral and ethical

231
00:09:49,360 --> 00:09:54,320
responsibilities are and then if there

232
00:09:52,560 --> 00:09:56,640
is something that they're being asked to

233
00:09:54,320 --> 00:09:59,600
do in the context of their work in the

234
00:09:56,640 --> 00:10:01,440
context of their technical work uh that

235
00:09:59,600 --> 00:10:04,080
seems to be wrong that seems to

236
00:10:01,440 --> 00:10:07,120
contravene collectively held values or

237
00:10:04,080 --> 00:10:11,360
even may you know be against the law in

238
00:10:07,120 --> 00:10:13,279
some way. um what ought you to do? And I

239
00:10:11,360 --> 00:10:15,200
certainly remember from my experience

240
00:10:13,279 --> 00:10:19,200
working in government that, you know,

241
00:10:15,200 --> 00:10:21,760
kind of quiet conversations outside the

242
00:10:19,200 --> 00:10:24,560
office, you know, with with peers, with

243
00:10:21,760 --> 00:10:26,480
colleagues, you know, DC is a big, you

244
00:10:24,560 --> 00:10:28,000
know, lots of people who live in DC are

245
00:10:26,480 --> 00:10:29,680
working in government. This was really

246
00:10:28,000 --> 00:10:31,040
the talk of the town. And one of the

247
00:10:29,680 --> 00:10:33,120
things that people were talking about

248
00:10:31,040 --> 00:10:35,200
was sort of whether or not, you know,

249
00:10:33,120 --> 00:10:38,079
going to the press and whistleblowing in

250
00:10:35,200 --> 00:10:40,480
this way was the right thing to do,

251
00:10:38,079 --> 00:10:43,120
whether it was an effective thing to do.

252
00:10:40,480 --> 00:10:44,880
Um, people had a lot of opinions about

253
00:10:43,120 --> 00:10:46,959
it and it really kind of speaks to some

254
00:10:44,880 --> 00:10:48,800
of the everyday dilemmas that might come

255
00:10:46,959 --> 00:10:53,200
up in, you know, engineering and

256
00:10:48,800 --> 00:10:55,040
computer science work um about how how

257
00:10:53,200 --> 00:10:57,200
one handles that. And in the

258
00:10:55,040 --> 00:11:00,640
documentary, I mean, we actually will

259
00:10:57,200 --> 00:11:02,560
get to see um how this process unfolded.

260
00:11:00,640 --> 00:11:05,680
We'll get to learn what Edward Snowden's

261
00:11:02,560 --> 00:11:08,720
kind of decision-making process was um

262
00:11:05,680 --> 00:11:12,480
and what the consequences of uh his

263
00:11:08,720 --> 00:11:15,920
decisions uh in terms of bringing this

264
00:11:12,480 --> 00:11:19,440
information uh about an otherwise fully

265
00:11:15,920 --> 00:11:21,040
veiled um government program to the

266
00:11:19,440 --> 00:11:23,519
public.

267
00:11:21,040 --> 00:11:26,000
And the very high price to pay when your

268
00:11:23,519 --> 00:11:28,640
country is divided and you know part of

269
00:11:26,000 --> 00:11:30,560
your country thinks that you acted out

270
00:11:28,640 --> 00:11:32,399
of love for your people and in the

271
00:11:30,560 --> 00:11:33,839
service of your people and the other

272
00:11:32,399 --> 00:11:36,160
part of your country thinks that you're

273
00:11:33,839 --> 00:11:37,920
a traitor and how that plays out in an

274
00:11:36,160 --> 00:11:40,399
individual's life and also in the

275
00:11:37,920 --> 00:11:44,320
technopolitics of surveillance in the US

276
00:11:40,399 --> 00:11:49,320
and beyond. So with that we are ready to

277
00:11:44,320 --> 00:11:49,320
watch some Citizen 4. Yeah.

