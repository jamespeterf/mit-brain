1
00:00:02,040 --> 00:00:06,040
my name is Hussein ranima I'm a

2
00:00:03,760 --> 00:00:08,800
professor at the human dynamics group at

3
00:00:06,040 --> 00:00:11,160
MIT media lab my interest has always

4
00:00:08,800 --> 00:00:14,679
been in the intersection of AI and

5
00:00:11,160 --> 00:00:17,520
disciplines like design and Humanities

6
00:00:14,679 --> 00:00:19,480
and my key Focus area at the lab is on a

7
00:00:17,520 --> 00:00:22,080
concept called perspective aware

8
00:00:19,480 --> 00:00:24,480
Computing essentially a type of AI that

9
00:00:22,080 --> 00:00:27,240
allows us to see the world through other

10
00:00:24,480 --> 00:00:29,720
people's lenses so instead of relying on

11
00:00:27,240 --> 00:00:32,520
large language models we can basically

12
00:00:29,720 --> 00:00:35,280
have our own personal models train it

13
00:00:32,520 --> 00:00:37,800
ourselves and lended as expertise and

14
00:00:35,280 --> 00:00:40,239
knowhow to others contributing to the

15
00:00:37,800 --> 00:00:42,280
formation of a decentralized collective

16
00:00:40,239 --> 00:00:45,000
intelligence if you look at the

17
00:00:42,280 --> 00:00:46,840
literature of AI in the past 50 or 60

18
00:00:45,000 --> 00:00:48,879
years they were very much based on

19
00:00:46,840 --> 00:00:51,199
learning we looked at a lot of data we

20
00:00:48,879 --> 00:00:53,160
looked at patterns of data and then we

21
00:00:51,199 --> 00:00:55,960
had these Winters and summers in the

22
00:00:53,160 --> 00:00:58,559
evolution of AI and the techniques that

23
00:00:55,960 --> 00:01:00,840
we used if you go back to about 20 30

24
00:00:58,559 --> 00:01:03,399
years ago we had dual based systems uh

25
00:01:00,840 --> 00:01:05,760
we had expert systems as we called them

26
00:01:03,399 --> 00:01:08,560
and they were very much based on rules

27
00:01:05,760 --> 00:01:10,920
think about Excel sheets but at a at a

28
00:01:08,560 --> 00:01:13,200
different level uh as we are thinking

29
00:01:10,920 --> 00:01:15,600
about symbolic systems and hybrid

30
00:01:13,200 --> 00:01:17,880
systems so these systems can all come

31
00:01:15,600 --> 00:01:21,560
and work together rather than be relying

32
00:01:17,880 --> 00:01:23,600
on one our interest is forming a new

33
00:01:21,560 --> 00:01:25,439
type of AI and really find an

34
00:01:23,600 --> 00:01:29,240
intersection between what we used to

35
00:01:25,439 --> 00:01:31,360
call semantic Computing symbolic Ai and

36
00:01:29,240 --> 00:01:33,920
Subs symbolic AI which are techniques

37
00:01:31,360 --> 00:01:37,600
like neural networks and really bring

38
00:01:33,920 --> 00:01:41,000
them all together to work in Tandem and

39
00:01:37,600 --> 00:01:43,600
the context will basically decide what

40
00:01:41,000 --> 00:01:45,759
components of this AI ecology we need to

41
00:01:43,600 --> 00:01:48,280
bring together to bring the best

42
00:01:45,759 --> 00:01:51,040
decision and action based on the context

43
00:01:48,280 --> 00:01:54,479
of the request from the user so our

44
00:01:51,040 --> 00:01:57,399
unique technique is to really combine

45
00:01:54,479 --> 00:01:59,439
symbolic AI to really represent the

46
00:01:57,399 --> 00:02:02,840
world and represent our behaviors and

47
00:01:59,439 --> 00:02:05,079
our human traits and then in a hybrid

48
00:02:02,840 --> 00:02:08,599
model combine it with neural networks

49
00:02:05,079 --> 00:02:12,280
for learning process formation of what

50
00:02:08,599 --> 00:02:14,879
we call AI graphs or graph-based Ai and

51
00:02:12,280 --> 00:02:17,800
then use that to populate data

52
00:02:14,879 --> 00:02:20,959
accordingly to really understand users

53
00:02:17,800 --> 00:02:24,120
and entities on a much more broader and

54
00:02:20,959 --> 00:02:27,040
contextual fashion uh so it has been a

55
00:02:24,120 --> 00:02:29,640
unique but relatively a complex area of

56
00:02:27,040 --> 00:02:31,360
AI in the past few years but what is

57
00:02:29,640 --> 00:02:34,319
different different now is that with the

58
00:02:31,360 --> 00:02:36,440
Advent of large language models the

59
00:02:34,319 --> 00:02:40,200
great compute power that we have now

60
00:02:36,440 --> 00:02:42,680
using gpus and tpus and cloud computing

61
00:02:40,200 --> 00:02:47,080
and the relative

62
00:02:42,680 --> 00:02:49,760
uh cheap storage of data and the ample

63
00:02:47,080 --> 00:02:52,159
of data that we have if you think about

64
00:02:49,760 --> 00:02:54,599
that physics and as have as how you're

65
00:02:52,159 --> 00:02:57,120
bringing them together we can now

66
00:02:54,599 --> 00:02:59,840
demonstrate the impact of these symbolic

67
00:02:57,120 --> 00:03:02,480
AI systems at scale our system bu

68
00:02:59,840 --> 00:03:04,760
believes in these AI ecologies where

69
00:03:02,480 --> 00:03:07,280
different types of techniques and models

70
00:03:04,760 --> 00:03:10,239
and de decentralized data schemes they

71
00:03:07,280 --> 00:03:13,640
all come together to essentially forming

72
00:03:10,239 --> 00:03:16,959
what we call an AI solar system or an AI

73
00:03:13,640 --> 00:03:20,599
ecology now if you compare that with the

74
00:03:16,959 --> 00:03:23,200
large language models uh they are great

75
00:03:20,599 --> 00:03:26,200
and we use them we don't necessarily

76
00:03:23,200 --> 00:03:29,120
compare our technique versus large

77
00:03:26,200 --> 00:03:31,959
language models we just create these

78
00:03:29,120 --> 00:03:34,319
full here and hybrid system so our

79
00:03:31,959 --> 00:03:36,799
models can actually benefit from large

80
00:03:34,319 --> 00:03:38,959
language models so if you think about

81
00:03:36,799 --> 00:03:41,560
using an llm because there there is a

82
00:03:38,959 --> 00:03:44,239
hypothesis out there that well if you

83
00:03:41,560 --> 00:03:46,840
have a lot of parameters and a lot of

84
00:03:44,239 --> 00:03:49,599
data training these models and you have

85
00:03:46,840 --> 00:03:52,760
these amazing compute power it's just a

86
00:03:49,599 --> 00:03:56,159
matter of time that llms will completely

87
00:03:52,760 --> 00:03:58,799
displace agents and symbolic agents we

88
00:03:56,159 --> 00:04:01,040
don't necessarily look at it that way

89
00:03:58,799 --> 00:04:02,000
because the challenge with llms is that

90
00:04:01,040 --> 00:04:05,159
you need

91
00:04:02,000 --> 00:04:07,400
verifiability you need audit you need

92
00:04:05,159 --> 00:04:10,799
Providence of course you can benefit

93
00:04:07,400 --> 00:04:13,319
from an nlm but will you be able to say

94
00:04:10,799 --> 00:04:16,120
where is that data coming from that is

95
00:04:13,319 --> 00:04:18,959
giving me that answer can you actually

96
00:04:16,120 --> 00:04:21,479
verify that answer you can try it and

97
00:04:18,959 --> 00:04:23,639
you can ask a similar question in

98
00:04:21,479 --> 00:04:26,440
different context windows and you may

99
00:04:23,639 --> 00:04:29,080
get very different answers now large

100
00:04:26,440 --> 00:04:31,520
language models are amazing if we can

101
00:04:29,080 --> 00:04:34,840
use comp components of those in form of

102
00:04:31,520 --> 00:04:37,400
vectors and bring them to our tiny

103
00:04:34,840 --> 00:04:39,320
models which are more symbolic and

104
00:04:37,400 --> 00:04:42,919
really bring the best of the two

105
00:04:39,320 --> 00:04:45,440
techniques together to build verifiable

106
00:04:42,919 --> 00:04:48,000
and reason ready models so if you need

107
00:04:45,440 --> 00:04:50,039
to audit these models if you want to

108
00:04:48,000 --> 00:04:52,759
verify these models especially when it

109
00:04:50,039 --> 00:04:54,840
comes to personal data then I think you

110
00:04:52,759 --> 00:04:56,800
will achieve the best of the best of the

111
00:04:54,840 --> 00:05:01,160
Two Worlds when you're building these

112
00:04:56,800 --> 00:05:01,160
next Generation AI systems

