1
00:00:00,000 --> 00:00:04,799
Awesome. Thank you, Manish. Um, yay.

2
00:00:03,520 --> 00:00:06,720
Thanks so much for having me. Thank you

3
00:00:04,799 --> 00:00:09,440
all so much for taking the time to come.

4
00:00:06,720 --> 00:00:10,800
Um, so I am going to give you a talk

5
00:00:09,440 --> 00:00:12,400
that I may or may not have written

6
00:00:10,800 --> 00:00:14,559
yesterday on the train. So, please be

7
00:00:12,400 --> 00:00:16,400
patient with me and kind. Um, we'll see

8
00:00:14,559 --> 00:00:19,119
how much of it I get to and how much of

9
00:00:16,400 --> 00:00:20,240
it I don't. Uh, hold most questions to

10
00:00:19,119 --> 00:00:21,039
the end if you don't mind, but if

11
00:00:20,240 --> 00:00:22,400
there's anything that you don't

12
00:00:21,039 --> 00:00:24,400
understand about what I'm saying, please

13
00:00:22,400 --> 00:00:27,840
let me know during. I'm happy to clarify

14
00:00:24,400 --> 00:00:30,400
things. Um, so yes, I will get right

15
00:00:27,840 --> 00:00:31,920
into it. Um this talk is mostly going to

16
00:00:30,400 --> 00:00:33,440
be about some of the things I've been

17
00:00:31,920 --> 00:00:35,760
thinking about evaluating and thinking

18
00:00:33,440 --> 00:00:38,000
about generative AI harms. Um I come

19
00:00:35,760 --> 00:00:41,120
from as Manish said sort of trained in

20
00:00:38,000 --> 00:00:42,800
algorithmic fairness type um work. Uh

21
00:00:41,120 --> 00:00:45,760
and

22
00:00:42,800 --> 00:00:48,160
um as you guys might know, you know,

23
00:00:45,760 --> 00:00:50,640
when we thought about harm model harms

24
00:00:48,160 --> 00:00:53,760
and fairness in traditional AI systems,

25
00:00:50,640 --> 00:00:55,280
we had sort of a lot more structure in

26
00:00:53,760 --> 00:00:57,920
the way that these models work that

27
00:00:55,280 --> 00:00:59,760
helped us understand both how to maybe

28
00:00:57,920 --> 00:01:02,640
test whether or not something nefarious

29
00:00:59,760 --> 00:01:04,239
was going on, but also um evaluate to

30
00:01:02,640 --> 00:01:06,159
the extent to which that nefarious thing

31
00:01:04,239 --> 00:01:08,159
was going on. So we had you know

32
00:01:06,159 --> 00:01:11,680
structured inputs that we had to get

33
00:01:08,159 --> 00:01:14,479
into some specific um you know uh format

34
00:01:11,680 --> 00:01:16,320
and then we also had an output that we

35
00:01:14,479 --> 00:01:18,000
at least decided upon what it was going

36
00:01:16,320 --> 00:01:19,520
to be right and a lot of that time that

37
00:01:18,000 --> 00:01:22,000
was maybe a binary output. A lot of the

38
00:01:19,520 --> 00:01:24,159
fairness work focused around that and so

39
00:01:22,000 --> 00:01:27,759
we had some ideas about how to kind of

40
00:01:24,159 --> 00:01:30,240
like test for different forms of um you

41
00:01:27,759 --> 00:01:31,920
know model harms. I f focused a lot on

42
00:01:30,240 --> 00:01:33,840
fairness where we were thinking about

43
00:01:31,920 --> 00:01:37,040
disperate treatment or disperate um

44
00:01:33,840 --> 00:01:39,520
influence uh impacts uh along uh

45
00:01:37,040 --> 00:01:43,280
different demographic groups. So we had

46
00:01:39,520 --> 00:01:45,840
um we had definitions like demographic

47
00:01:43,280 --> 00:01:47,680
parity which is you know do you have two

48
00:01:45,840 --> 00:01:50,720
demographic groups for example men and

49
00:01:47,680 --> 00:01:52,880
women uh given opportunities at the same

50
00:01:50,720 --> 00:01:54,960
rate um from a model's decision process

51
00:01:52,880 --> 00:01:56,960
stuff like this right so we had kind of

52
00:01:54,960 --> 00:01:59,200
ways that we could think about the

53
00:01:56,960 --> 00:02:01,280
outputs from our models and uh think

54
00:01:59,200 --> 00:02:03,040
about whether or not they were causing

55
00:02:01,280 --> 00:02:05,520
likely to cause harm or causing harm

56
00:02:03,040 --> 00:02:09,599
right um and then now we have a lot of

57
00:02:05,520 --> 00:02:12,560
generative AI systems uh And uh you put

58
00:02:09,599 --> 00:02:15,840
in kind of whatever you want and you

59
00:02:12,560 --> 00:02:18,480
kind of get out whatever you want. Um

60
00:02:15,840 --> 00:02:22,319
and you don't really have to tell it how

61
00:02:18,480 --> 00:02:24,400
to do what you want it to do. And uh you

62
00:02:22,319 --> 00:02:25,920
can change how it works and what you're

63
00:02:24,400 --> 00:02:28,160
asking it to do at any point while

64
00:02:25,920 --> 00:02:30,000
you're using it. And uh basically

65
00:02:28,160 --> 00:02:32,319
there's like a lot more generality here,

66
00:02:30,000 --> 00:02:33,920
a lot less specificity, right? And so,

67
00:02:32,319 --> 00:02:36,239
um, I think that this has really

68
00:02:33,920 --> 00:02:37,760
impacted the way that, uh, we think

69
00:02:36,239 --> 00:02:40,160
about whether or not a model is causing

70
00:02:37,760 --> 00:02:42,400
harm. And it has really brought about a

71
00:02:40,160 --> 00:02:43,840
paradigm shift and stirred a lot of our

72
00:02:42,400 --> 00:02:45,519
thoughts about how we can think about

73
00:02:43,840 --> 00:02:48,000
whether or not a model is acting in a

74
00:02:45,519 --> 00:02:51,280
harmful way and how to understand um,

75
00:02:48,000 --> 00:02:52,879
how to quantify that harm. So, you know,

76
00:02:51,280 --> 00:02:54,160
what I used to think about a lot and

77
00:02:52,879 --> 00:02:56,640
still think about a lot in traditional

78
00:02:54,160 --> 00:02:58,720
AI systems is how do we define whether

79
00:02:56,640 --> 00:03:00,080
or not a harm is occurring? If I think

80
00:02:58,720 --> 00:03:01,760
about some way that a model might be

81
00:03:00,080 --> 00:03:03,920
hurting someone, how do I measure it?

82
00:03:01,760 --> 00:03:06,640
How do I evaluate that measurement? How

83
00:03:03,920 --> 00:03:09,360
do I mitigate that behavior? And what

84
00:03:06,640 --> 00:03:11,840
legal tools do I have to make sure that

85
00:03:09,360 --> 00:03:15,440
people will mitigate that behavior? Um,

86
00:03:11,840 --> 00:03:19,680
and I think generative AI systems change

87
00:03:15,440 --> 00:03:22,720
the way that we think about a lot of uh

88
00:03:19,680 --> 00:03:25,120
each of these things. So for the fact

89
00:03:22,720 --> 00:03:26,879
that we have such flexible outputs means

90
00:03:25,120 --> 00:03:29,120
that there are new modes of harm that

91
00:03:26,879 --> 00:03:31,120
the model can bring about that maybe are

92
00:03:29,120 --> 00:03:32,879
not fully encapsulated by the ways that

93
00:03:31,120 --> 00:03:35,360
we had thought about model harms before

94
00:03:32,879 --> 00:03:37,440
when we had more structured models. Um

95
00:03:35,360 --> 00:03:40,480
the way because also of those flexible

96
00:03:37,440 --> 00:03:42,400
inputs and outputs evaluation is crazy

97
00:03:40,480 --> 00:03:44,239
as we'll talk about a lot during this

98
00:03:42,400 --> 00:03:46,159
talk as well. There's so much

99
00:03:44,239 --> 00:03:47,599
flexibility that it's hard to understand

100
00:03:46,159 --> 00:03:49,360
whether or not you have a reliable

101
00:03:47,599 --> 00:03:51,599
signal of whether some behavior is

102
00:03:49,360 --> 00:03:53,440
happening or not. um and bias

103
00:03:51,599 --> 00:03:56,000
mitigations for a lot of models where

104
00:03:53,440 --> 00:03:58,560
you don't have access to what's going on

105
00:03:56,000 --> 00:04:00,080
inside for proprietary reasons or just

106
00:03:58,560 --> 00:04:02,400
because it's too expensive to train and

107
00:04:00,080 --> 00:04:03,840
look at yourself. Uh what can you do

108
00:04:02,400 --> 00:04:04,959
besides prompting? I think that's

109
00:04:03,840 --> 00:04:06,959
something that people are trying to

110
00:04:04,959 --> 00:04:08,879
figure out a lot right now as well. Um

111
00:04:06,959 --> 00:04:10,560
and then finally people I think are

112
00:04:08,879 --> 00:04:12,720
really

113
00:04:10,560 --> 00:04:14,239
confused about what laws do and don't

114
00:04:12,720 --> 00:04:15,920
apply to generalized systems and how

115
00:04:14,239 --> 00:04:18,479
they apply. So in this talk I'm going to

116
00:04:15,920 --> 00:04:20,720
mostly talk about these three. I've been

117
00:04:18,479 --> 00:04:22,240
thinking about how to think about model

118
00:04:20,720 --> 00:04:23,520
harms and general AI a lot. And for

119
00:04:22,240 --> 00:04:24,880
anyone who's interested in and wants to

120
00:04:23,520 --> 00:04:26,880
talk to me later, I'm happy to talk

121
00:04:24,880 --> 00:04:28,320
about all four things or different

122
00:04:26,880 --> 00:04:29,840
things about each of these three things.

123
00:04:28,320 --> 00:04:31,120
But generally, this is where my mind's

124
00:04:29,840 --> 00:04:32,479
been at. And I'm going to talk to you

125
00:04:31,120 --> 00:04:35,440
mostly about these three that I've

126
00:04:32,479 --> 00:04:37,280
circled. Um, and in particular, I'm

127
00:04:35,440 --> 00:04:39,759
going to talk to you mostly about a

128
00:04:37,280 --> 00:04:41,440
paper that discusses some technical

129
00:04:39,759 --> 00:04:43,360
pitfalls in generative AI harm

130
00:04:41,440 --> 00:04:45,520
evaluation and how that interacts with

131
00:04:43,360 --> 00:04:47,199
some legal frameworks and how that kind

132
00:04:45,520 --> 00:04:48,720
of makes things uh sometimes more

133
00:04:47,199 --> 00:04:51,199
difficult from a policy and regulation

134
00:04:48,720 --> 00:04:53,040
perspective. Um, so that'll be the

135
00:04:51,199 --> 00:04:56,160
majority of the talk. I'll then talk

136
00:04:53,040 --> 00:04:59,120
very briefly about some uh ongoing work

137
00:04:56,160 --> 00:05:00,720
that I have where I talk about uh some

138
00:04:59,120 --> 00:05:03,120
other legal frameworks that are relevant

139
00:05:00,720 --> 00:05:07,360
to gender AI systems and how they can be

140
00:05:03,120 --> 00:05:10,800
used to um incentivize

141
00:05:07,360 --> 00:05:12,960
companies to prevent harm. And then if I

142
00:05:10,800 --> 00:05:14,639
have time since this is a new talk, uh I

143
00:05:12,960 --> 00:05:16,560
will make a hard left turn and talk

144
00:05:14,639 --> 00:05:18,160
about some other kinds of model harm

145
00:05:16,560 --> 00:05:19,680
that I've been thinking about that have

146
00:05:18,160 --> 00:05:22,560
less to do with the law, at least that

147
00:05:19,680 --> 00:05:24,720
I've thought of so far. Um so hopefully

148
00:05:22,560 --> 00:05:26,320
that sounds good. Um and I'll get

149
00:05:24,720 --> 00:05:29,039
started.

150
00:05:26,320 --> 00:05:32,800
So first I'm going to talk about within

151
00:05:29,039 --> 00:05:35,919
this work um with some collaborators at

152
00:05:32,800 --> 00:05:37,600
Colombia uh some of the technical

153
00:05:35,919 --> 00:05:39,520
pitfalls and legal pensions around

154
00:05:37,600 --> 00:05:41,600
generative AI harm evaluation. And so

155
00:05:39,520 --> 00:05:44,000
first we're going to talk about a little

156
00:05:41,600 --> 00:05:46,000
bit some of the mismatches between the

157
00:05:44,000 --> 00:05:48,880
frameworks that exist and have existed

158
00:05:46,000 --> 00:05:51,840
for preventing decision-making harms uh

159
00:05:48,880 --> 00:05:55,120
in the law and some of the ways that AI

160
00:05:51,840 --> 00:05:56,479
mocks that up a little bit. So, um, one

161
00:05:55,120 --> 00:05:59,199
thing that I think people have been

162
00:05:56,479 --> 00:06:01,840
making a little bit of a halaloo about,

163
00:05:59,199 --> 00:06:04,080
uh, is that the at least the

164
00:06:01,840 --> 00:06:06,400
discrimination law statutes that we use

165
00:06:04,080 --> 00:06:08,319
most for preventing discrimination in

166
00:06:06,400 --> 00:06:11,199
all kinds of decision-m including AI

167
00:06:08,319 --> 00:06:13,919
systems work to prevent discrimination

168
00:06:11,199 --> 00:06:16,880
in allocative decisions. What do I mean

169
00:06:13,919 --> 00:06:19,360
by that? Um I mean you know if I have an

170
00:06:16,880 --> 00:06:21,280
opportunity like a job or I have a loan

171
00:06:19,360 --> 00:06:22,720
the an allocated decision is who is

172
00:06:21,280 --> 00:06:24,479
getting allocated the opportunity who

173
00:06:22,720 --> 00:06:26,880
gets hired who gets the loan right and

174
00:06:24,479 --> 00:06:29,680
if there is a disparity among who is

175
00:06:26,880 --> 00:06:31,440
allocated those decisions that's where

176
00:06:29,680 --> 00:06:33,280
uh traditional anti-discrimination law

177
00:06:31,440 --> 00:06:35,520
in the US at least steps in and says

178
00:06:33,280 --> 00:06:37,520
like hey unless under very specific

179
00:06:35,520 --> 00:06:40,160
circumstances you can't have large

180
00:06:37,520 --> 00:06:41,360
disparities between the uh rate at which

181
00:06:40,160 --> 00:06:43,759
people in different demographic groups

182
00:06:41,360 --> 00:06:45,280
are allocated opportunity. And so um

183
00:06:43,759 --> 00:06:46,639
there are some asterisks there that are

184
00:06:45,280 --> 00:06:48,240
important and deal with a lot of work

185
00:06:46,639 --> 00:06:50,080
that Manish and I have thought about but

186
00:06:48,240 --> 00:06:53,120
uh that's kind of the the basic way

187
00:06:50,080 --> 00:06:54,400
things go. Um and so traditional machine

188
00:06:53,120 --> 00:06:56,960
learning models that had more structured

189
00:06:54,400 --> 00:06:58,800
inputs than outputs fit in well with

190
00:06:56,960 --> 00:07:01,039
this paradigm because if you have a

191
00:06:58,800 --> 00:07:03,520
binary classification result from a

192
00:07:01,039 --> 00:07:05,680
model, you can just say hey this model

193
00:07:03,520 --> 00:07:07,360
would suggest to allocate opportunities

194
00:07:05,680 --> 00:07:09,039
to people at different rates. And so we

195
00:07:07,360 --> 00:07:10,560
can very easily slap on so that some of

196
00:07:09,039 --> 00:07:12,479
those definitions that I showed you guys

197
00:07:10,560 --> 00:07:13,919
earlier and say okay this seems to be

198
00:07:12,479 --> 00:07:16,240
acting in a discriminatory fashion that

199
00:07:13,919 --> 00:07:18,479
works well with the law. If you have

200
00:07:16,240 --> 00:07:20,479
definitive outputs where say you know if

201
00:07:18,479 --> 00:07:22,720
we take a hiring example you have a

202
00:07:20,479 --> 00:07:23,840
resume summary instead of an actual

203
00:07:22,720 --> 00:07:25,280
decision about whether or not someone

204
00:07:23,840 --> 00:07:27,840
gets hired or someone is a good match

205
00:07:25,280 --> 00:07:30,800
for this job then there is at least this

206
00:07:27,840 --> 00:07:32,720
question that comes up that's like uh

207
00:07:30,800 --> 00:07:34,400
can the generative AI model be held

208
00:07:32,720 --> 00:07:36,319
liable for maybe a discriminatory

209
00:07:34,400 --> 00:07:38,160
summary. doesn't seem to really fit in

210
00:07:36,319 --> 00:07:40,560
well with this framework of how

211
00:07:38,160 --> 00:07:42,720
discrimination law has work works in the

212
00:07:40,560 --> 00:07:44,319
US. Does this make sense? This like

213
00:07:42,720 --> 00:07:48,639
distinction between allocative and

214
00:07:44,319 --> 00:07:53,360
generative output. Okay, cool. So, um

215
00:07:48,639 --> 00:07:55,440
TLDDRO no TD will come later. Um so, uh

216
00:07:53,360 --> 00:07:57,280
that's one kind of tension is this this

217
00:07:55,440 --> 00:07:58,960
difference between allocative and

218
00:07:57,280 --> 00:08:00,479
generative kind of outputs from a model

219
00:07:58,960 --> 00:08:01,840
and how that matches up with the

220
00:08:00,479 --> 00:08:03,680
discrimination framework that we have.

221
00:08:01,840 --> 00:08:05,440
Yes. I just want to

222
00:08:03,680 --> 00:08:07,680
>> Yes. is the difference that's passing

223
00:08:05,440 --> 00:08:09,039
through a manager before the decision is

224
00:08:07,680 --> 00:08:11,199
the thing that's making it.

225
00:08:09,039 --> 00:08:13,280
>> Yeah. So like before we could kind of

226
00:08:11,199 --> 00:08:15,280
see directly of a traditional model as

227
00:08:13,280 --> 00:08:17,120
acting in a discriminatory fashion and

228
00:08:15,280 --> 00:08:18,720
that but we could match that up with

229
00:08:17,120 --> 00:08:20,319
this discrimination law frameworks

230
00:08:18,720 --> 00:08:22,000
because we could see directly what the

231
00:08:20,319 --> 00:08:24,879
decisions the model was making and be

232
00:08:22,000 --> 00:08:26,720
like hey this seems to be like leading

233
00:08:24,879 --> 00:08:28,879
to disperate impact right this seems to

234
00:08:26,720 --> 00:08:30,240
be leading to a difference in the way

235
00:08:28,879 --> 00:08:32,000
people of different demographic groups

236
00:08:30,240 --> 00:08:36,320
are treated in a way that is legally

237
00:08:32,000 --> 00:08:37,839
relevant here like as we'll discuss

238
00:08:36,320 --> 00:08:39,279
later you can still check if this

239
00:08:37,839 --> 00:08:42,479
overall process is leading to

240
00:08:39,279 --> 00:08:44,720
disparities, but like

241
00:08:42,479 --> 00:08:46,399
creating kind of legal pressure to make

242
00:08:44,720 --> 00:08:49,120
sure you're specifically debiasing your

243
00:08:46,399 --> 00:08:51,760
gener generative AI model when you can't

244
00:08:49,120 --> 00:08:54,160
really directly test to see if it's

245
00:08:51,760 --> 00:08:57,200
influencing allocation in any kind of

246
00:08:54,160 --> 00:08:58,959
direct way that's um leading to

247
00:08:57,200 --> 00:09:01,040
disparities in terms of like who

248
00:08:58,959 --> 00:09:03,120
actually gets in opportunities makes

249
00:09:01,040 --> 00:09:03,680
this harder to match up to the framework

250
00:09:03,120 --> 00:09:05,680
that we have.

251
00:09:03,680 --> 00:09:06,399
>> So it's not leading the decision and say

252
00:09:05,680 --> 00:09:08,160
okay.

253
00:09:06,399 --> 00:09:08,959
>> Yes. Does that make sense? Okay. Cool.

254
00:09:08,160 --> 00:09:12,399
Cool.

255
00:09:08,959 --> 00:09:17,040
Um, another kind of tension that exists

256
00:09:12,399 --> 00:09:20,560
is that uh there's kind of um a problem

257
00:09:17,040 --> 00:09:22,480
in both directions where the regulations

258
00:09:20,560 --> 00:09:23,839
there are fewer than existed a few

259
00:09:22,480 --> 00:09:27,600
months ago, but the regulations that

260
00:09:23,839 --> 00:09:31,040
still do exist that ask for um AI

261
00:09:27,600 --> 00:09:33,760
evaluation are really vague and

262
00:09:31,040 --> 00:09:35,440
inspecific, which allows for potentially

263
00:09:33,760 --> 00:09:38,560
some gaming. So that kind of just says

264
00:09:35,440 --> 00:09:41,839
like do an evaluation according to some

265
00:09:38,560 --> 00:09:44,959
standardized protocols which as we'll

266
00:09:41,839 --> 00:09:46,800
see later can sometimes lead to um with

267
00:09:44,959 --> 00:09:49,279
a lot of instability can lead to

268
00:09:46,800 --> 00:09:51,680
unrealistic or un um kind of

269
00:09:49,279 --> 00:09:54,720
untrustworthy results. But also at the

270
00:09:51,680 --> 00:09:57,040
same time there's this um problem where

271
00:09:54,720 --> 00:09:59,120
the evaluations that do exist at least

272
00:09:57,040 --> 00:10:02,320
in the academic literature so far are

273
00:09:59,120 --> 00:10:05,440
both unstable and don't really um map to

274
00:10:02,320 --> 00:10:06,800
how models are used in real life. Um, so

275
00:10:05,440 --> 00:10:08,399
this is things like, you know, a lot of

276
00:10:06,800 --> 00:10:09,920
evaluation techniques are based off of

277
00:10:08,399 --> 00:10:11,760
single turn interactions where you just

278
00:10:09,920 --> 00:10:13,760
have one round of conversation with a

279
00:10:11,760 --> 00:10:15,920
model, but if you go later to

280
00:10:13,760 --> 00:10:19,680
multi-turn, this doesn't generalize. And

281
00:10:15,920 --> 00:10:21,279
so both like the specificity in the in

282
00:10:19,680 --> 00:10:24,240
the regulation that exists is not really

283
00:10:21,279 --> 00:10:26,160
enough to make sure or at least to try a

284
00:10:24,240 --> 00:10:27,839
little bit to prevent that the um

285
00:10:26,160 --> 00:10:30,800
valuation that's done won't be gameable.

286
00:10:27,839 --> 00:10:32,240
Also, the evaluation strategies that do

287
00:10:30,800 --> 00:10:34,079
exist don't meet some of the few

288
00:10:32,240 --> 00:10:35,600
requirements that are in the regulation,

289
00:10:34,079 --> 00:10:37,200
which is like please try to make this

290
00:10:35,600 --> 00:10:39,040
similar to your deployment environments.

291
00:10:37,200 --> 00:10:40,560
And I don't think that that's happening.

292
00:10:39,040 --> 00:10:43,519
Um, and then finally, this is a little

293
00:10:40,560 --> 00:10:45,680
bit of a lesser point, but uh the amount

294
00:10:43,519 --> 00:10:49,200
that you can change a gen model as

295
00:10:45,680 --> 00:10:50,720
you're using it in the real world also

296
00:10:49,200 --> 00:10:52,560
um heightens this tension that has

297
00:10:50,720 --> 00:10:54,720
existed in the law for the long a long

298
00:10:52,560 --> 00:10:57,040
time, which is who is responsible for

299
00:10:54,720 --> 00:10:58,720
the discrimination that is caused. It is

300
00:10:57,040 --> 00:11:00,480
the person that is making the tool or is

301
00:10:58,720 --> 00:11:02,800
it the person that is using the tool? So

302
00:11:00,480 --> 00:11:04,399
it is the person is it open AAI that

303
00:11:02,800 --> 00:11:06,320
made the baseline model that's being

304
00:11:04,399 --> 00:11:07,600
used by an employment agency to make

305
00:11:06,320 --> 00:11:10,320
their hiring decision or is it the

306
00:11:07,600 --> 00:11:11,680
employment agency? Um there are lots of

307
00:11:10,320 --> 00:11:13,279
situations where it's mostly the

308
00:11:11,680 --> 00:11:15,040
employment agency and I'm happy to talk

309
00:11:13,279 --> 00:11:16,480
about that um offline about you know

310
00:11:15,040 --> 00:11:20,079
problems around that but there are other

311
00:11:16,480 --> 00:11:21,680
situations where there's a lot of um

312
00:11:20,079 --> 00:11:23,120
this is kind of like an active area of

313
00:11:21,680 --> 00:11:25,120
discussion and the fact that you can

314
00:11:23,120 --> 00:11:26,720
change model so much as you're using it

315
00:11:25,120 --> 00:11:28,240
I think really heightens this question

316
00:11:26,720 --> 00:11:29,839
of like how can you possibly ask a

317
00:11:28,240 --> 00:11:31,680
developer to do enough testing to make

318
00:11:29,839 --> 00:11:33,680
sure that it will work reasonably well

319
00:11:31,680 --> 00:11:35,360
on the ground but also you still need

320
00:11:33,680 --> 00:11:36,720
the developer to do a lot because it's

321
00:11:35,360 --> 00:11:38,000
unclear how to make sure a user

322
00:11:36,720 --> 00:11:42,079
understands how to prevent those kinds

323
00:11:38,000 --> 00:11:44,240
of behaviors. as well. Um, okay. So,

324
00:11:42,079 --> 00:11:46,079
those are kind of some I would say 2.5

325
00:11:44,240 --> 00:11:48,800
tensions that I'm going to talk about

326
00:11:46,079 --> 00:11:51,360
three tensions uh in this first part.

327
00:11:48,800 --> 00:11:53,760
So, the first question coming from that

328
00:11:51,360 --> 00:11:57,120
um tension with discrimination law is

329
00:11:53,760 --> 00:11:59,200
kind of uh are gen systems harder to

330
00:11:57,120 --> 00:12:00,720
regulate due to their non-allocative

331
00:11:59,200 --> 00:12:02,160
outputs like they have these more

332
00:12:00,720 --> 00:12:03,920
flexible outputs that don't clearly

333
00:12:02,160 --> 00:12:05,760
match the discrimination law framework.

334
00:12:03,920 --> 00:12:07,519
Um, and then there's this policy for

335
00:12:05,760 --> 00:12:09,600
deployment relevant evaluations that's

336
00:12:07,519 --> 00:12:11,040
being unmet by current unstable and and

337
00:12:09,600 --> 00:12:12,560
evaluation practices, which I'll talk

338
00:12:11,040 --> 00:12:13,839
about a little bit. And then also

339
00:12:12,560 --> 00:12:18,560
exacerbating this deployer versus

340
00:12:13,839 --> 00:12:20,160
development tension. Um, okay. So,

341
00:12:18,560 --> 00:12:21,920
really quick, no, actually not really

342
00:12:20,160 --> 00:12:24,000
quick, I'll get into kind of talking

343
00:12:21,920 --> 00:12:26,000
about this first tension in some detail.

344
00:12:24,000 --> 00:12:28,160
Um, which I think is really related to

345
00:12:26,000 --> 00:12:31,040
some of the problems with quantifying

346
00:12:28,160 --> 00:12:34,720
harm. So, defining what your harm is in

347
00:12:31,040 --> 00:12:36,399
generative AI systems. Uh and so coming

348
00:12:34,720 --> 00:12:38,399
back to this first sentence that I spoke

349
00:12:36,399 --> 00:12:40,399
about um the traditional discrimination

350
00:12:38,399 --> 00:12:44,480
law apply to the generative outputs that

351
00:12:40,399 --> 00:12:48,279
we get. Uh TLDDR yeah

352
00:12:44,480 --> 00:12:48,279
um I think like

353
00:12:48,880 --> 00:12:54,720
legally if you have a discriminatory

354
00:12:51,440 --> 00:12:57,200
decision-m system you can still check

355
00:12:54,720 --> 00:12:59,600
the end outputs right of whether and who

356
00:12:57,200 --> 00:13:01,440
is getting allocated which decisions.

357
00:12:59,600 --> 00:13:03,200
The thing that becomes more difficult in

358
00:13:01,440 --> 00:13:04,880
terms of regulation is understanding

359
00:13:03,200 --> 00:13:06,880
where that bias is coming from and

360
00:13:04,880 --> 00:13:09,839
preventing it in practice. Right? So

361
00:13:06,880 --> 00:13:11,519
like the law still applies. You still

362
00:13:09,839 --> 00:13:13,519
can't have discrimination in your hiring

363
00:13:11,519 --> 00:13:15,839
practices, in your loan application

364
00:13:13,519 --> 00:13:19,360
practices, in your uh housing

365
00:13:15,839 --> 00:13:21,120
distribution practices, but like you can

366
00:13:19,360 --> 00:13:22,639
see evidence for discrimination at this

367
00:13:21,120 --> 00:13:24,320
stage and then trying to understand

368
00:13:22,639 --> 00:13:25,680
where in this overall decision-making

369
00:13:24,320 --> 00:13:27,120
system the bias is coming from if it's

370
00:13:25,680 --> 00:13:28,560
coming from your generative a model

371
00:13:27,120 --> 00:13:31,279
becomes more difficult. And this is

372
00:13:28,560 --> 00:13:33,519
where the issue of how do we actually

373
00:13:31,279 --> 00:13:35,200
measure harm comes in because when this

374
00:13:33,519 --> 00:13:36,800
becomes harder, it's kind of more

375
00:13:35,200 --> 00:13:38,160
difficult to practically reduce the

376
00:13:36,800 --> 00:13:40,000
discrimination that's going on that

377
00:13:38,160 --> 00:13:42,959
you're still liable for. Does this make

378
00:13:40,000 --> 00:13:44,320
sense? I've seen a bunch of people,

379
00:13:42,959 --> 00:13:46,079
myself included, at times being like,

380
00:13:44,320 --> 00:13:47,839
"Oh my god, maybe it doesn't apply. It

381
00:13:46,079 --> 00:13:49,200
does apply. It does apply. Let's all

382
00:13:47,839 --> 00:13:51,760
remember that it does apply. The law

383
00:13:49,200 --> 00:13:55,440
still applies." Um

384
00:13:51,760 --> 00:13:57,839
so again right when we had uh more

385
00:13:55,440 --> 00:13:59,360
structured output we could be like okay

386
00:13:57,839 --> 00:14:00,560
let's look at the differences in rates

387
00:13:59,360 --> 00:14:03,279
in which people are allocated an

388
00:14:00,560 --> 00:14:04,720
opportunity and now say that we have a

389
00:14:03,279 --> 00:14:06,800
resume summary that's coming out of a

390
00:14:04,720 --> 00:14:09,120
large language model and say that we do

391
00:14:06,800 --> 00:14:14,160
see we we want to understand like how do

392
00:14:09,120 --> 00:14:16,399
I think about evaluating my LLM um

393
00:14:14,160 --> 00:14:18,160
resume summarization model to see if it

394
00:14:16,399 --> 00:14:23,240
might cause harm in a way that's like

395
00:14:18,160 --> 00:14:23,240
legally relevant, Right. Um so

396
00:14:23,440 --> 00:14:28,880
we my my people that I worked with my

397
00:14:26,079 --> 00:14:32,240
lab and I um and our collaborators did

398
00:14:28,880 --> 00:14:35,440
some experiments to think about um how

399
00:14:32,240 --> 00:14:36,560
we would test this. Uh and so just to

400
00:14:35,440 --> 00:14:38,880
kind of run through what the

401
00:14:36,560 --> 00:14:40,880
experimental setup is. Um so this is to

402
00:14:38,880 --> 00:14:42,320
understand whether or not uh this kind

403
00:14:40,880 --> 00:14:43,360
of more flexible generative output was

404
00:14:42,320 --> 00:14:46,160
discriminating against people of

405
00:14:43,360 --> 00:14:48,320
different groups. We created some ré

406
00:14:46,160 --> 00:14:51,920
templates from randomly sampled traits.

407
00:14:48,320 --> 00:14:53,600
We created groups of um like kind of

408
00:14:51,920 --> 00:14:55,360
yeah resumes based off of those

409
00:14:53,600 --> 00:14:58,480
templates which were all the same. We

410
00:14:55,360 --> 00:15:00,399
followed um you know procedures from the

411
00:14:58,480 --> 00:15:04,160
social science literature on adding

412
00:15:00,399 --> 00:15:05,600
names that were tied to stereotypical um

413
00:15:04,160 --> 00:15:07,199
uh cereal names for different

414
00:15:05,600 --> 00:15:09,600
demographic groups to different resume

415
00:15:07,199 --> 00:15:11,600
templates. We then asked generative AI

416
00:15:09,600 --> 00:15:13,760
model to make a summary. I'm sure you

417
00:15:11,600 --> 00:15:16,000
guys have seen stuff like this before.

418
00:15:13,760 --> 00:15:18,240
But then what we did um is that we also

419
00:15:16,000 --> 00:15:21,519
asked a decision maker to make decisions

420
00:15:18,240 --> 00:15:24,480
based off of those ré summaries. So we

421
00:15:21,519 --> 00:15:27,680
have one sort of AI model that's takes

422
00:15:24,480 --> 00:15:29,199
um a resume like we have like maybe five

423
00:15:27,680 --> 00:15:32,000
or six or 10 I don't remember how many

424
00:15:29,199 --> 00:15:33,199
different templates a lot of equivalent

425
00:15:32,000 --> 00:15:34,880
like the exact same resume with

426
00:15:33,199 --> 00:15:36,959
different names from different de

427
00:15:34,880 --> 00:15:38,959
demographic groups. We create a summary

428
00:15:36,959 --> 00:15:40,399
for all of those resumes and then based

429
00:15:38,959 --> 00:15:43,199
off of those summaries, we ask a

430
00:15:40,399 --> 00:15:44,800
different decision maker, a more complex

431
00:15:43,199 --> 00:15:48,320
AI model in this case to make a decision

432
00:15:44,800 --> 00:15:50,000
based off of those summaries. Now,

433
00:15:48,320 --> 00:15:52,880
if you maybe don't think about model

434
00:15:50,000 --> 00:15:54,959
harms all the time like me, uh you would

435
00:15:52,880 --> 00:15:57,360
say, okay, how am I going to think about

436
00:15:54,959 --> 00:16:00,360
trying to understand if these Oh, I'm so

437
00:15:57,360 --> 00:16:00,360
sorry.

438
00:16:01,120 --> 00:16:05,519
The summary does contain the names, but

439
00:16:03,199 --> 00:16:07,440
we did an experiment to make sure that

440
00:16:05,519 --> 00:16:09,040
what the doubt that you have is not

441
00:16:07,440 --> 00:16:10,240
true. So, I will show a slide with that.

442
00:16:09,040 --> 00:16:12,480
I'm pretty sure I can predict your

443
00:16:10,240 --> 00:16:14,720
question. Yeah. Am I right that you're

444
00:16:12,480 --> 00:16:19,120
like, is it coming from the name? Okay.

445
00:16:14,720 --> 00:16:22,480
Okay. Word. Um, okay. Uh, yes. So the

446
00:16:19,120 --> 00:16:24,720
summary contains the name, but we show

447
00:16:22,480 --> 00:16:26,639
based off of an experiment where we

448
00:16:24,720 --> 00:16:28,160
create the summaries without the names

449
00:16:26,639 --> 00:16:30,320
and then add the names on them

450
00:16:28,160 --> 00:16:31,759
afterwards and test the bias of the

451
00:16:30,320 --> 00:16:32,959
decision maker that the bias is not

452
00:16:31,759 --> 00:16:33,920
coming from the decision maker but from

453
00:16:32,959 --> 00:16:37,040
the summary. And I'll show you a graph

454
00:16:33,920 --> 00:16:39,600
of that in a second. Um, okay. So if you

455
00:16:37,040 --> 00:16:41,600
were um you know not obsessed with model

456
00:16:39,600 --> 00:16:43,600
harms like me, maybe you would be like

457
00:16:41,600 --> 00:16:46,480
okay maybe I'm going to test like kind

458
00:16:43,600 --> 00:16:48,240
of decision uh summarization accuracy

459
00:16:46,480 --> 00:16:49,680
across people of different demographic

460
00:16:48,240 --> 00:16:51,360
groups and make sure that the model is

461
00:16:49,680 --> 00:16:53,040
summarizing people like giving at least

462
00:16:51,360 --> 00:16:54,800
the like the same factual information

463
00:16:53,040 --> 00:16:58,000
about all these different people in the

464
00:16:54,800 --> 00:17:00,639
same way. Right? So I I'm going to use a

465
00:16:58,000 --> 00:17:02,160
um a score that's very common in the NLP

466
00:17:00,639 --> 00:17:04,400
literature root score which basically

467
00:17:02,160 --> 00:17:05,839
checks how accurate a summary is. We use

468
00:17:04,400 --> 00:17:08,319
the resumeumés that the summary was

469
00:17:05,839 --> 00:17:12,079
based on as the ground truth. And we see

470
00:17:08,319 --> 00:17:15,199
that um for these different models that

471
00:17:12,079 --> 00:17:18,160
we were testing uh the root score within

472
00:17:15,199 --> 00:17:22,160
each model was not changing too too much

473
00:17:18,160 --> 00:17:24,799
um uh between demographic groups, right?

474
00:17:22,160 --> 00:17:26,559
But the interesting thing is that if you

475
00:17:24,799 --> 00:17:28,160
look at the selection rate difference

476
00:17:26,559 --> 00:17:30,640
and these are on different scales. I'm

477
00:17:28,160 --> 00:17:34,080
so sorry this is 2%, this is 4% and 6%

478
00:17:30,640 --> 00:17:35,360
just so that you all know. um the

479
00:17:34,080 --> 00:17:37,600
selection rate difference in the

480
00:17:35,360 --> 00:17:41,200
downstream decision maker actually did

481
00:17:37,600 --> 00:17:43,039
have a larger difference in the um rate

482
00:17:41,200 --> 00:17:45,360
at which it selected different people

483
00:17:43,039 --> 00:17:46,799
for the eventual interview that it was

484
00:17:45,360 --> 00:17:49,360
being asked to choose between people

485
00:17:46,799 --> 00:17:50,960
for. So if you see in particular based

486
00:17:49,360 --> 00:17:52,240
off of these results maybe you would

487
00:17:50,960 --> 00:17:54,160
have gone with the llama model because

488
00:17:52,240 --> 00:17:57,760
none of them seem to have that big of a

489
00:17:54,160 --> 00:18:01,039
difference in their root score um

490
00:17:57,760 --> 00:18:02,640
root score. uh but if you but these two

491
00:18:01,039 --> 00:18:04,240
models are kind of roughly

492
00:18:02,640 --> 00:18:05,919
interchangeable I would say but from a

493
00:18:04,240 --> 00:18:07,600
selection disparity perspective this is

494
00:18:05,919 --> 00:18:09,039
the maximum selection disparity across

495
00:18:07,600 --> 00:18:11,039
any two demographic groups that we were

496
00:18:09,039 --> 00:18:15,039
testing there actually can be a big

497
00:18:11,039 --> 00:18:17,760
difference right um so what is this

498
00:18:15,039 --> 00:18:20,000
coming from uh right hopefully that was

499
00:18:17,760 --> 00:18:23,520
clear um everyone is people good on this

500
00:18:20,000 --> 00:18:25,280
so far great uh is the unfairness coming

501
00:18:23,520 --> 00:18:26,880
from the hiring manager which I assume

502
00:18:25,280 --> 00:18:28,960
was your question in the back because we

503
00:18:26,880 --> 00:18:31,520
did give people the resume. We did give

504
00:18:28,960 --> 00:18:33,919
the the the resumeé summaries with the

505
00:18:31,520 --> 00:18:37,919
name to the decision maker. Um and the

506
00:18:33,919 --> 00:18:40,640
answer I think is no. Uh because we gave

507
00:18:37,919 --> 00:18:42,640
we created ré summaries without giving

508
00:18:40,640 --> 00:18:44,320
the name and then we attached the names

509
00:18:42,640 --> 00:18:46,320
onto them post talk so that it would be

510
00:18:44,320 --> 00:18:48,640
the same or very similar baseline

511
00:18:46,320 --> 00:18:51,200
summaries and we saw that the hiring

512
00:18:48,640 --> 00:18:53,039
manager did not uh choose people with

513
00:18:51,200 --> 00:18:54,160
very different uh selection rates across

514
00:18:53,039 --> 00:18:56,080
demographic groups. So hopefully that

515
00:18:54,160 --> 00:18:57,919
answers your question. Yeah. Awesome.

516
00:18:56,080 --> 00:19:00,480
Okay. So it was in fact coming from the

517
00:18:57,919 --> 00:19:03,280
summary, right? And so we didn't see it

518
00:19:00,480 --> 00:19:05,440
in the rouge score. Um then the question

519
00:19:03,280 --> 00:19:07,520
kind of here is all right. So by

520
00:19:05,440 --> 00:19:09,200
checking this downstream process, we

521
00:19:07,520 --> 00:19:11,120
know that the harm that we're trying to

522
00:19:09,200 --> 00:19:13,600
prevent this like overall access to

523
00:19:11,120 --> 00:19:15,120
opportunity is this bad thing is

524
00:19:13,600 --> 00:19:17,919
happening. We weren't able to see what

525
00:19:15,120 --> 00:19:19,200
the root score. Um what are what are

526
00:19:17,919 --> 00:19:20,640
some other things that we can do? Right?

527
00:19:19,200 --> 00:19:22,000
So maybe we want to understand how

528
00:19:20,640 --> 00:19:25,679
exactly this discrimination was

529
00:19:22,000 --> 00:19:27,200
occurring. Um, if we look at two résumé

530
00:19:25,679 --> 00:19:29,760
summaries that were based off of the

531
00:19:27,200 --> 00:19:31,440
baseline, the same baseline resume, but

532
00:19:29,760 --> 00:19:33,520
one for a white applicant and one for a

533
00:19:31,440 --> 00:19:37,200
Hispanic applicant, what we basically

534
00:19:33,520 --> 00:19:39,679
see is that there's a lot more kind of

535
00:19:37,200 --> 00:19:42,000
flowery language kind of description

536
00:19:39,679 --> 00:19:44,080
used in the white applicant summary,

537
00:19:42,000 --> 00:19:46,640
which maybe speaks more to her character

538
00:19:44,080 --> 00:19:48,960
for some reason, whereas for the um

539
00:19:46,640 --> 00:19:51,520
Hispanic applicant summary, uh, it's

540
00:19:48,960 --> 00:19:56,160
much more factual, right? And so this

541
00:19:51,520 --> 00:19:58,799
influences the um the sentiment uh and

542
00:19:56,160 --> 00:20:00,559
um LM as a judge things that we did like

543
00:19:58,799 --> 00:20:03,760
rankings, people emotional intelligence

544
00:20:00,559 --> 00:20:05,360
and reliability which might be necessary

545
00:20:03,760 --> 00:20:07,200
qualities for the job. This was we did

546
00:20:05,360 --> 00:20:10,160
all of our um experiments based on a

547
00:20:07,200 --> 00:20:12,799
social work job. Um and so you can kind

548
00:20:10,160 --> 00:20:14,320
of see this is like like you know it's

549
00:20:12,799 --> 00:20:15,679
sneaky, right? This discrimination is

550
00:20:14,320 --> 00:20:17,120
kind of sneaky but you can see how it

551
00:20:15,679 --> 00:20:20,880
would influence a downstream decision

552
00:20:17,120 --> 00:20:24,080
maker. Um, and I think, you know, it's

553
00:20:20,880 --> 00:20:26,080
not a perfect um answer by any means. I

554
00:20:24,080 --> 00:20:28,400
think it's definitely an intermediate

555
00:20:26,080 --> 00:20:31,200
step, but I think one thing that we did

556
00:20:28,400 --> 00:20:34,640
see is that if you um have a suite of

557
00:20:31,200 --> 00:20:36,159
metrics that's more um related to the

558
00:20:34,640 --> 00:20:38,159
downstream task that you're actually

559
00:20:36,159 --> 00:20:40,960
using the large language model for. So

560
00:20:38,159 --> 00:20:42,640
in this case, we had um sentiment,

561
00:20:40,960 --> 00:20:44,320
length because length of a positive

562
00:20:42,640 --> 00:20:46,080
thing, you know, indicates probably more

563
00:20:44,320 --> 00:20:47,600
positive things said about that person.

564
00:20:46,080 --> 00:20:49,360
LM as a judge ranking for emotional

565
00:20:47,600 --> 00:20:50,400
intelligence and reliability. Even

566
00:20:49,360 --> 00:20:52,480
though there hadn't been much of a

567
00:20:50,400 --> 00:20:54,159
difference on RE score between the um

568
00:20:52,480 --> 00:20:55,919
Gemma and the Llama models that seemed

569
00:20:54,159 --> 00:20:57,280
interchangeable, you start to see a

570
00:20:55,919 --> 00:20:58,720
pattern of difference between these two

571
00:20:57,280 --> 00:21:01,600
models where maybe you would have made a

572
00:20:58,720 --> 00:21:05,360
different choice. Right? So I think

573
00:21:01,600 --> 00:21:06,960
TLDDR um it's hard to figure out how

574
00:21:05,360 --> 00:21:09,280
harm is happening from large language

575
00:21:06,960 --> 00:21:11,120
models. But since this paper, something

576
00:21:09,280 --> 00:21:12,799
that I've been more interested in and

577
00:21:11,120 --> 00:21:14,640
have been thinking about a lot more is

578
00:21:12,799 --> 00:21:16,960
kind of how we can do more contextual

579
00:21:14,640 --> 00:21:19,200
evaluations for generative AI systems.

580
00:21:16,960 --> 00:21:21,120
So if I think about where the decision

581
00:21:19,200 --> 00:21:23,520
for this model is going, how can I more

582
00:21:21,120 --> 00:21:25,919
accurately reflect the important parts

583
00:21:23,520 --> 00:21:27,919
of that context in how I'm evaluating

584
00:21:25,919 --> 00:21:30,799
the model? Um, and I guess the other

585
00:21:27,919 --> 00:21:32,880
thing that I'll say is, yeah, it's hard,

586
00:21:30,799 --> 00:21:34,880
but if you're like, I don't know, maybe

587
00:21:32,880 --> 00:21:36,240
building a startup or thinking about how

588
00:21:34,880 --> 00:21:39,200
to make sure that you don't build a

589
00:21:36,240 --> 00:21:41,679
discriminatory AI system, um, make sure

590
00:21:39,200 --> 00:21:43,919
you test the downstream thing that is

591
00:21:41,679 --> 00:21:45,760
the thing that might uh, actually cause

592
00:21:43,919 --> 00:21:47,360
harm in the real world and maybe the

593
00:21:45,760 --> 00:21:49,520
intermediate evaluations are a little

594
00:21:47,360 --> 00:21:51,039
bit less important than the overall

595
00:21:49,520 --> 00:21:52,640
system. Yeah.

596
00:21:51,039 --> 00:21:54,799
>> So, you have this counterfactual

597
00:21:52,640 --> 00:21:56,880
pipeline set up. I'm wondering if this

598
00:21:54,799 --> 00:21:59,280
suggests an end to end thing where you

599
00:21:56,880 --> 00:22:01,440
can say something like from people's

600
00:21:59,280 --> 00:22:03,039
decisions I can back out things that are

601
00:22:01,440 --> 00:22:04,480
associated with their decision making.

602
00:22:03,039 --> 00:22:05,679
So for example, emotional intelligence

603
00:22:04,480 --> 00:22:08,159
is

604
00:22:05,679 --> 00:22:10,080
>> so just from data a bunch of summaries

605
00:22:08,159 --> 00:22:11,600
and a bunch of decisions

606
00:22:10,080 --> 00:22:13,440
>> what would have been important

607
00:22:11,600 --> 00:22:15,280
>> characteristics are associated with

608
00:22:13,440 --> 00:22:17,039
people's decision making and then you

609
00:22:15,280 --> 00:22:19,520
could take that set of metrics and go

610
00:22:17,039 --> 00:22:21,600
back to your factual test and run that

611
00:22:19,520 --> 00:22:24,559
say does the name at the top of the

612
00:22:21,600 --> 00:22:26,159
resume affectual intelligence

613
00:22:24,559 --> 00:22:30,960
five metrics are strongly associated

614
00:22:26,159 --> 00:22:31,679
with decision is that like a viable end

615
00:22:30,960 --> 00:22:33,600
so

616
00:22:31,679 --> 00:22:34,640
>> I think So I would have to maybe think

617
00:22:33,600 --> 00:22:35,600
about a little bit more detail. But I

618
00:22:34,640 --> 00:22:38,080
think that that sounds like the kind of

619
00:22:35,600 --> 00:22:40,000
thing that I would yeah definitely just

620
00:22:38,080 --> 00:22:41,919
prescify a bunch of possible metrics

621
00:22:40,000 --> 00:22:43,200
that could be associated those are the

622
00:22:41,919 --> 00:22:45,200
right ones

623
00:22:43,200 --> 00:22:46,640
>> versus like yeah actually verify. I

624
00:22:45,200 --> 00:22:48,960
think yeah I think that that pipeline

625
00:22:46,640 --> 00:22:52,240
sounds like a good way to go about doing

626
00:22:48,960 --> 00:22:54,799
this definitely. Yeah. Um

627
00:22:52,240 --> 00:22:57,679
okay cool. Any other questions on this

628
00:22:54,799 --> 00:22:57,679
part? Yeah.

629
00:23:07,120 --> 00:23:14,880
um we did several Good point. I'm going

630
00:23:11,440 --> 00:23:16,799
to talk a lot about um evaluation in the

631
00:23:14,880 --> 00:23:19,679
next couple of slides. We did definitely

632
00:23:16,799 --> 00:23:21,440
do that. The appendix of this paper is

633
00:23:19,679 --> 00:23:25,280
very long. I don't have all the figures

634
00:23:21,440 --> 00:23:26,640
up promote, but uh I I think I have the

635
00:23:25,280 --> 00:23:28,159
paper name on one of these slides. I'm

636
00:23:26,640 --> 00:23:30,159
happy to talk to you offline about those

637
00:23:28,159 --> 00:23:32,400
different checks that we did. I do also

638
00:23:30,159 --> 00:23:33,919
know for a fact that we reran the entire

639
00:23:32,400 --> 00:23:35,440
experiment several times and came up

640
00:23:33,919 --> 00:23:36,880
with the same answer because we kept on

641
00:23:35,440 --> 00:23:39,039
forgetting to set speeds and stuff like

642
00:23:36,880 --> 00:23:41,120
this. So, I can also just speak from

643
00:23:39,039 --> 00:23:42,720
experience that we got similar results.

644
00:23:41,120 --> 00:23:45,520
Um,

645
00:23:42,720 --> 00:23:47,039
okay. All right. Um, so that's kind of

646
00:23:45,520 --> 00:23:48,640
like I guess my takeaway from that.

647
00:23:47,039 --> 00:23:50,159
Measuring harms is hard, but I think

648
00:23:48,640 --> 00:23:52,159
contextual evaluations are something

649
00:23:50,159 --> 00:23:54,240
that I'm thinking about moving forward.

650
00:23:52,159 --> 00:23:56,320
Um, okay. Now, we're going to talk about

651
00:23:54,240 --> 00:23:58,320
evaluation instability and related

652
00:23:56,320 --> 00:24:00,000
policy repercussions. So, let's say

653
00:23:58,320 --> 00:24:02,400
you've thought about the kind of harm

654
00:24:00,000 --> 00:24:04,159
you're trying to prevent a lot. You know

655
00:24:02,400 --> 00:24:05,760
what you think is relevant to it and now

656
00:24:04,159 --> 00:24:07,200
you're definitely going to be trying to

657
00:24:05,760 --> 00:24:09,120
prevent it, right? But, so now you have

658
00:24:07,200 --> 00:24:11,120
your measurement, but how are you going

659
00:24:09,120 --> 00:24:14,640
to make sure that that phenomenon is

660
00:24:11,120 --> 00:24:17,679
actually happening or not? Um

661
00:24:14,640 --> 00:24:19,279
so uh then the next kind of case study

662
00:24:17,679 --> 00:24:22,880
that we talk about in this paper has to

663
00:24:19,279 --> 00:24:25,440
do with this method of preventive uh

664
00:24:22,880 --> 00:24:28,480
testing for certain kinds of model harms

665
00:24:25,440 --> 00:24:30,400
which is red teaming. Um I just in case

666
00:24:28,480 --> 00:24:33,200
you guys don't know what red teaming is,

667
00:24:30,400 --> 00:24:34,960
there's basically a um one language

668
00:24:33,200 --> 00:24:37,919
model that is used to come up with kind

669
00:24:34,960 --> 00:24:39,760
of adversarial questions to query a base

670
00:24:37,919 --> 00:24:42,080
model that you're testing the toxicity

671
00:24:39,760 --> 00:24:44,640
of and you basically see the rate at

672
00:24:42,080 --> 00:24:47,039
which you do get a toxic response to

673
00:24:44,640 --> 00:24:50,640
those adversarial questions. So this is

674
00:24:47,039 --> 00:24:52,240
red teaming. Um in the uh end of the

675
00:24:50,640 --> 00:24:54,559
Biden administration where a bunch of

676
00:24:52,240 --> 00:24:57,120
like um executive orders were coming out

677
00:24:54,559 --> 00:24:59,360
about how to prevent harm in AI systems.

678
00:24:57,120 --> 00:25:00,799
Red teaming was mentioned quite a lot.

679
00:24:59,360 --> 00:25:02,240
People were like do red teaming. Red

680
00:25:00,799 --> 00:25:06,240
teaming is amazing. Red teaming all the

681
00:25:02,240 --> 00:25:08,080
time. Um and so I think both it is at

682
00:25:06,240 --> 00:25:10,320
the very least one of the most

683
00:25:08,080 --> 00:25:13,840
well-known bias evaluation techniques

684
00:25:10,320 --> 00:25:19,039
spoken about amongst policy folk. Um and

685
00:25:13,840 --> 00:25:21,840
so uh it was not ideal then uh when we

686
00:25:19,039 --> 00:25:25,039
kind of showed that depending on what

687
00:25:21,840 --> 00:25:27,760
model you chose as your base um your

688
00:25:25,039 --> 00:25:29,760
base model that's querying the uh sorry

689
00:25:27,760 --> 00:25:31,120
which model you choose to query the

690
00:25:29,760 --> 00:25:33,600
model that you are testing for

691
00:25:31,120 --> 00:25:35,440
discrimination. So what model you're

692
00:25:33,600 --> 00:25:37,520
using for your red LM that's coming up

693
00:25:35,440 --> 00:25:39,840
with the potentially toxic questions,

694
00:25:37,520 --> 00:25:45,520
you can get kind of different responses

695
00:25:39,840 --> 00:25:47,039
um from uh that different uh answers

696
00:25:45,520 --> 00:25:48,320
about how discriminatory your model is

697
00:25:47,039 --> 00:25:50,320
and which might be the best model to

698
00:25:48,320 --> 00:25:52,880
pick. So here we were testing for bias

699
00:25:50,320 --> 00:25:55,679
against women. Um so we were asking

700
00:25:52,880 --> 00:25:57,760
potentially uh questions that were

701
00:25:55,679 --> 00:25:59,520
leading to kind of a disrespectful toxic

702
00:25:57,760 --> 00:26:01,279
answer. And the nice thing I guess is

703
00:25:59,520 --> 00:26:03,279
that we did actually see a pattern for a

704
00:26:01,279 --> 00:26:06,240
lot of it. But there was a situation

705
00:26:03,279 --> 00:26:09,440
where um we you could very easily come

706
00:26:06,240 --> 00:26:12,720
up with a reasonable construction of a

707
00:26:09,440 --> 00:26:14,080
um of a uh

708
00:26:12,720 --> 00:26:16,320
measurement of this particular kind of

709
00:26:14,080 --> 00:26:19,200
toxicity harm uh that would show you

710
00:26:16,320 --> 00:26:22,559
that a model that seems to usually not

711
00:26:19,200 --> 00:26:24,159
act super fairly is in fact acting the

712
00:26:22,559 --> 00:26:26,320
best of the three that you're choosing

713
00:26:24,159 --> 00:26:28,640
from. Is that a question

714
00:26:26,320 --> 00:26:31,760
>> in this case rather than just taking the

715
00:26:28,640 --> 00:26:35,760
one answer?

716
00:26:31,760 --> 00:26:37,600
>> That's a good question. I don't know.

717
00:26:35,760 --> 00:26:39,120
I think you could do it either way. I'm

718
00:26:37,600 --> 00:26:41,679
not sure how we did it in this

719
00:26:39,120 --> 00:26:43,520
particular um test. I think that there

720
00:26:41,679 --> 00:26:47,600
are like toxicity classifiers that are

721
00:26:43,520 --> 00:26:49,679
used sometimes to decide that. Um, I

722
00:26:47,600 --> 00:26:51,840
think part of the issue that we were

723
00:26:49,679 --> 00:26:54,559
talking about in this paper is that

724
00:26:51,840 --> 00:26:57,200
while like the general setup of red

725
00:26:54,559 --> 00:26:58,799
teaming is clear, it's like ask toxic

726
00:26:57,200 --> 00:27:00,799
questions, get answers, measure those

727
00:26:58,799 --> 00:27:03,120
responses, how you do each one of those

728
00:27:00,799 --> 00:27:04,640
steps can influence the results. So, I

729
00:27:03,120 --> 00:27:06,480
think that's another area where there is

730
00:27:04,640 --> 00:27:08,480
kind of like a vector of difference of

731
00:27:06,480 --> 00:27:09,600
how you decide to evaluate those model

732
00:27:08,480 --> 00:27:11,279
responses and that could lead to

733
00:27:09,600 --> 00:27:15,840
instability in and of itself as well.

734
00:27:11,279 --> 00:27:21,760
Does that answer your question? Yeah. Um

735
00:27:15,840 --> 00:27:23,279
so, um right, uh it's very easy whether

736
00:27:21,760 --> 00:27:24,799
you were trying to or not to come up

737
00:27:23,279 --> 00:27:27,200
with an answer that would allow you to

738
00:27:24,799 --> 00:27:30,799
deploy a model that is potentially

739
00:27:27,200 --> 00:27:33,120
toxic. Um even if you were either trying

740
00:27:30,799 --> 00:27:35,039
your best to find one that wasn't or

741
00:27:33,120 --> 00:27:37,760
trying to find metrics that would show

742
00:27:35,039 --> 00:27:40,000
you that it is fair when it's not. Um,

743
00:27:37,760 --> 00:27:42,240
and I think you know that this is I'm

744
00:27:40,000 --> 00:27:43,679
sorry this slide is kind of ugly, but I

745
00:27:42,240 --> 00:27:46,240
think that this is based off of the

746
00:27:43,679 --> 00:27:48,400
underlying instability in model

747
00:27:46,240 --> 00:27:51,679
responses to any kind of question, not

748
00:27:48,400 --> 00:27:53,440
just adversarial uh toxic questions. But

749
00:27:51,679 --> 00:27:55,440
I'm sure you guys have seen some papers

750
00:27:53,440 --> 00:27:57,520
on this. This is just a particular

751
00:27:55,440 --> 00:27:59,919
example from one uh piece of work that

752
00:27:57,520 --> 00:28:03,440
I'm working on right now. depending on

753
00:27:59,919 --> 00:28:05,200
exactly how you query your Genai system,

754
00:28:03,440 --> 00:28:07,039
um it will make different decisions

755
00:28:05,200 --> 00:28:10,080
based off of seemingly meaningless

756
00:28:07,039 --> 00:28:12,799
differences in um how you form that

757
00:28:10,080 --> 00:28:16,480
question. Right? So here we had a we

758
00:28:12,799 --> 00:28:18,799
were trying to create an LLM to

759
00:28:16,480 --> 00:28:21,279
give advice about whether or not someone

760
00:28:18,799 --> 00:28:22,720
was likely to have diabetes, I believe.

761
00:28:21,279 --> 00:28:24,000
And so here are all these tabular

762
00:28:22,720 --> 00:28:26,880
features. And depending on what

763
00:28:24,000 --> 00:28:28,480
metaprompt we use to ask the model to

764
00:28:26,880 --> 00:28:29,760
give us the decision of how whether or

765
00:28:28,480 --> 00:28:32,000
not it's likely that someone should go

766
00:28:29,760 --> 00:28:34,399
get checked. Um we can get you know

767
00:28:32,000 --> 00:28:39,200
different answer we can get different

768
00:28:34,399 --> 00:28:41,279
answers um in uh in the models uh

769
00:28:39,200 --> 00:28:44,159
decision-m about whether or not someone

770
00:28:41,279 --> 00:28:47,120
has diabetes for something as seemingly

771
00:28:44,159 --> 00:28:49,279
unrelated as um

772
00:28:47,120 --> 00:28:51,039
exactly how we put the system prompt

773
00:28:49,279 --> 00:28:52,960
together. Right? So I think that that's

774
00:28:51,039 --> 00:28:54,399
where this variability and and red

775
00:28:52,960 --> 00:28:56,399
teaming is and I think a lot of people

776
00:28:54,399 --> 00:29:01,279
have been paying attention to this

777
00:28:56,399 --> 00:29:04,559
instability um instability uh in large

778
00:29:01,279 --> 00:29:06,240
language model outputs based off of

779
00:29:04,559 --> 00:29:08,080
semantic changes in prompt and other

780
00:29:06,240 --> 00:29:09,440
kind of seemingly unrelated things. And

781
00:29:08,080 --> 00:29:10,799
I think the next thing that I'm really

782
00:29:09,440 --> 00:29:12,399
interested in also is why does this

783
00:29:10,799 --> 00:29:13,760
happen and can we find patterns? Can we

784
00:29:12,399 --> 00:29:16,240
find a way of kind of exploring the

785
00:29:13,760 --> 00:29:17,840
space in a meaningful way? Um but in

786
00:29:16,240 --> 00:29:20,559
terms of the policy repercussions of

787
00:29:17,840 --> 00:29:22,640
this instability, one is um this thing

788
00:29:20,559 --> 00:29:25,279
that I call dehacking, which I had a

789
00:29:22,640 --> 00:29:26,960
paper on. Um gosh, I really am so bad at

790
00:29:25,279 --> 00:29:28,480
putting my papers in the bottom corner

791
00:29:26,960 --> 00:29:30,799
of my slides, but I swear there was a

792
00:29:28,480 --> 00:29:34,399
paper on this in fact a few years ago.

793
00:29:30,799 --> 00:29:38,000
uh where you know we notice this even

794
00:29:34,399 --> 00:29:41,360
with traditional AI systems but if you

795
00:29:38,000 --> 00:29:44,559
have an unstable evaluation procedure

796
00:29:41,360 --> 00:29:46,559
and you want to pass some test where a

797
00:29:44,559 --> 00:29:48,640
policy document says make sure you do

798
00:29:46,559 --> 00:29:50,799
some evaluation impact assessment of

799
00:29:48,640 --> 00:29:53,600
your model to make sure that it's not

800
00:29:50,799 --> 00:29:55,760
causing harm to people. If you can just

801
00:29:53,600 --> 00:29:58,480
tweak some parameters to design a test

802
00:29:55,760 --> 00:30:01,200
that says, "Hey, I think that this has

803
00:29:58,480 --> 00:30:04,880
lower has low disparity and is safe to

804
00:30:01,200 --> 00:30:07,600
um deploy. If that evaluation process is

805
00:30:04,880 --> 00:30:09,840
unstable, you can find a way to deploy a

806
00:30:07,600 --> 00:30:12,240
model that might actually be harmful um

807
00:30:09,840 --> 00:30:14,159
even if it doesn't look so in that like

808
00:30:12,240 --> 00:30:16,399
one example that you managed to find."

809
00:30:14,159 --> 00:30:20,000
Right? So, we call this dehacking or uh

810
00:30:16,399 --> 00:30:22,000
discrimination hacking. Um and the fact

811
00:30:20,000 --> 00:30:23,679
that LLM evaluations are even more

812
00:30:22,000 --> 00:30:26,320
unstable than traditional AI systems

813
00:30:23,679 --> 00:30:28,000
means that uh this happens even more

814
00:30:26,320 --> 00:30:31,760
when you have standard AI models. So

815
00:30:28,000 --> 00:30:33,039
that is not great. Um oh I guess that's

816
00:30:31,760 --> 00:30:34,720
the only slide I have about the policy

817
00:30:33,039 --> 00:30:36,960
repercussions about that. But uh oh the

818
00:30:34,720 --> 00:30:40,000
other policy repercussion sorry actually

819
00:30:36,960 --> 00:30:42,480
um is that

820
00:30:40,000 --> 00:30:45,919
so the instability of the model's

821
00:30:42,480 --> 00:30:47,679
decision um the model's output itself is

822
00:30:45,919 --> 00:30:50,080
like kind of one axis of instability

823
00:30:47,679 --> 00:30:52,559
that makes dehacking uh worse. But

824
00:30:50,080 --> 00:30:54,799
another downside of instability that is

825
00:30:52,559 --> 00:30:59,919
exacerbated by how much people can edit

826
00:30:54,799 --> 00:31:03,919
models um can edit models uh as they're

827
00:30:59,919 --> 00:31:07,120
deployed on the ground means that

828
00:31:03,919 --> 00:31:10,080
even if you have a relatively well-made

829
00:31:07,120 --> 00:31:12,240
evaluation set up for your model, um if

830
00:31:10,080 --> 00:31:14,799
people are using that model in

831
00:31:12,240 --> 00:31:16,320
complicated ways, that evaluation might

832
00:31:14,799 --> 00:31:18,000
not generalize to the way that people

833
00:31:16,320 --> 00:31:20,480
are actually using that. And that in

834
00:31:18,000 --> 00:31:22,399
turn can kind of complicate this dynamic

835
00:31:20,480 --> 00:31:24,240
between who is liable for that model's

836
00:31:22,399 --> 00:31:28,159
behavior, the developer versus or the

837
00:31:24,240 --> 00:31:31,600
deployer, right? Um so on the top here I

838
00:31:28,159 --> 00:31:34,080
have some um this is essentially we

839
00:31:31,600 --> 00:31:36,320
repeated the red teaming experiment from

840
00:31:34,080 --> 00:31:38,480
the previous slides, but we did it over

841
00:31:36,320 --> 00:31:41,840
multiple turns of an interaction with an

842
00:31:38,480 --> 00:31:43,840
LLM. So, you know, if we saw that one

843
00:31:41,840 --> 00:31:46,640
model was slightly less discriminatory

844
00:31:43,840 --> 00:31:49,519
than another model um on like a one

845
00:31:46,640 --> 00:31:51,039
round interaction, um this should start

846
00:31:49,519 --> 00:31:53,840
at one, but it starts at zero, I guess,

847
00:31:51,039 --> 00:31:56,559
cuz we're computer scientists. Uh but as

848
00:31:53,840 --> 00:31:58,640
you have more turns of interaction with

849
00:31:56,559 --> 00:32:00,399
the model, which model is the least

850
00:31:58,640 --> 00:32:03,120
discriminatory over interactions with

851
00:32:00,399 --> 00:32:05,440
multiple turns can change, right? And so

852
00:32:03,120 --> 00:32:07,279
people in the world are most often

853
00:32:05,440 --> 00:32:09,360
interacting with large language models

854
00:32:07,279 --> 00:32:10,960
more than just asking a question once.

855
00:32:09,360 --> 00:32:12,880
They might have several follow-ups or

856
00:32:10,960 --> 00:32:15,120
kind of engaged with the model over a

857
00:32:12,880 --> 00:32:16,799
longer time period. And if you're only

858
00:32:15,120 --> 00:32:18,480
kind of like that's that's a reasonable

859
00:32:16,799 --> 00:32:21,679
way to use a model, but that's hard to

860
00:32:18,480 --> 00:32:25,039
develop a test for, right? And so if you

861
00:32:21,679 --> 00:32:26,960
are um you have this added instability

862
00:32:25,039 --> 00:32:28,399
from the way the model is used in

863
00:32:26,960 --> 00:32:30,640
practice being much more unpredictable

864
00:32:28,399 --> 00:32:33,039
but also a lack of predictability leads

865
00:32:30,640 --> 00:32:35,840
to a lack of generalization of your

866
00:32:33,039 --> 00:32:38,080
evaluation outcomes.

867
00:32:35,840 --> 00:32:40,240
And the same problem happens when we

868
00:32:38,080 --> 00:32:42,799
have hyperparameters that are left open

869
00:32:40,240 --> 00:32:45,840
to the user. So this is an example from

870
00:32:42,799 --> 00:32:48,880
an image model. Um I forget which exact

871
00:32:45,840 --> 00:32:50,720
image model this was uh but there's a

872
00:32:48,880 --> 00:32:52,399
guidance scale um which is kind of

873
00:32:50,720 --> 00:32:54,960
similar to temperature for the image

874
00:32:52,399 --> 00:32:57,120
model. Uh temperature being controlling

875
00:32:54,960 --> 00:32:58,720
essentially like how how much

876
00:32:57,120 --> 00:33:00,640
stoasticity you have in a model's

877
00:32:58,720 --> 00:33:04,159
outputs. The guidance scale for an image

878
00:33:00,640 --> 00:33:06,000
model is closer to um how kind of

879
00:33:04,159 --> 00:33:09,440
creative you're letting the model to be

880
00:33:06,000 --> 00:33:12,960
in a way. Uh and so we see if you are

881
00:33:09,440 --> 00:33:15,519
evaluating um how much uh kind of not

882
00:33:12,960 --> 00:33:17,840
safe for work images. Is that me doing

883
00:33:15,519 --> 00:33:21,279
all that jostling?

884
00:33:17,840 --> 00:33:24,480
I hope not. Okay. Um

885
00:33:21,279 --> 00:33:26,399
if you have a uh if you're trying to

886
00:33:24,480 --> 00:33:28,159
evaluate an image model for how many

887
00:33:26,399 --> 00:33:31,519
kind of not safe for work images there

888
00:33:28,159 --> 00:33:34,080
are by demographic groups um of women uh

889
00:33:31,519 --> 00:33:36,480
you know you might at a lower guidance

890
00:33:34,080 --> 00:33:38,720
scale find that you can not perfectly

891
00:33:36,480 --> 00:33:41,760
but somewhat control the disparity among

892
00:33:38,720 --> 00:33:43,760
different racial groups but as you go to

893
00:33:41,760 --> 00:33:45,279
higher guidance scales disparities for

894
00:33:43,760 --> 00:33:48,480
certain groups can really change right

895
00:33:45,279 --> 00:33:51,039
and so these these hyperparameters that

896
00:33:48,480 --> 00:33:53,840
are open to the user can also change the

897
00:33:51,039 --> 00:33:56,720
robustness and the kind of viability in

898
00:33:53,840 --> 00:33:58,240
a sense the the reasonleness of trust in

899
00:33:56,720 --> 00:34:01,440
these evaluation methods as well and

900
00:33:58,240 --> 00:34:03,840
again also um increase the tension

901
00:34:01,440 --> 00:34:06,240
between who's liable for the uh for the

902
00:34:03,840 --> 00:34:12,879
discrimination that's being caused.

903
00:34:06,240 --> 00:34:14,639
Um okay. So um as I kind of said what I

904
00:34:12,879 --> 00:34:17,440
think that this is kind of there are uh

905
00:34:14,639 --> 00:34:19,599
takeaways both for policy people and for

906
00:34:17,440 --> 00:34:21,200
um kind of more computer sciency people.

907
00:34:19,599 --> 00:34:22,879
Um something that I'm interested in

908
00:34:21,200 --> 00:34:24,560
after doing this work is thinking about

909
00:34:22,879 --> 00:34:27,200
ways that we can create more tech

910
00:34:24,560 --> 00:34:30,480
contextualized evaluations. But I guess

911
00:34:27,200 --> 00:34:31,599
contextualized evaluations uh hopefully

912
00:34:30,480 --> 00:34:33,119
and where we don't have to think of it

913
00:34:31,599 --> 00:34:34,879
individually for each context. think it

914
00:34:33,119 --> 00:34:36,480
would be nice to come up with ways of

915
00:34:34,879 --> 00:34:39,119
general ways of creating contextualized

916
00:34:36,480 --> 00:34:41,280
evaluations. Um, and then I guess for

917
00:34:39,119 --> 00:34:43,599
instability issues, what I've been

918
00:34:41,280 --> 00:34:45,440
telling people when like you know

919
00:34:43,599 --> 00:34:46,879
business people come to me asking, well,

920
00:34:45,440 --> 00:34:48,560
what do I do about all of this? I

921
00:34:46,879 --> 00:34:50,639
usually say look for patterns of good

922
00:34:48,560 --> 00:34:52,399
behavior and not just one example. This

923
00:34:50,639 --> 00:34:53,760
isn't new advice, but I think it's even

924
00:34:52,399 --> 00:34:55,760
more important with generative AI

925
00:34:53,760 --> 00:34:57,119
systems. So, perturb your evaluation

926
00:34:55,760 --> 00:34:58,560
setup a bunch of times and make sure

927
00:34:57,119 --> 00:35:01,839
that you see a pattern of good behavior

928
00:34:58,560 --> 00:35:03,280
and not just one go. And then um you

929
00:35:01,839 --> 00:35:05,440
know in order from a research

930
00:35:03,280 --> 00:35:07,760
perspective I think this is kind of a

931
00:35:05,440 --> 00:35:09,920
ripe area. I think that the problem has

932
00:35:07,760 --> 00:35:12,720
been kind of shown but thinking about

933
00:35:09,920 --> 00:35:14,480
how to

934
00:35:12,720 --> 00:35:15,920
understand and navigate why this is

935
00:35:14,480 --> 00:35:18,079
happening in a principled way is

936
00:35:15,920 --> 00:35:22,640
something that I'm really interested in.

937
00:35:18,079 --> 00:35:26,000
Um so lots of work to do. Um okay how am

938
00:35:22,640 --> 00:35:30,079
I doing my time? Great. Um I will now

939
00:35:26,000 --> 00:35:32,320
unless anyone has any questions. Yes.

940
00:35:30,079 --> 00:35:34,800
question

941
00:35:32,320 --> 00:35:38,320
about the students of those

942
00:35:34,800 --> 00:35:40,240
um in some sense like for example we had

943
00:35:38,320 --> 00:35:41,839
TV even

944
00:35:40,240 --> 00:35:44,640
right some redeemers are better than

945
00:35:41,839 --> 00:35:46,720
others like that so is it really

946
00:35:44,640 --> 00:35:49,119
instrument building or is it is you kind

947
00:35:46,720 --> 00:35:51,520
of like is there any hope for example

948
00:35:49,119 --> 00:35:53,200
coming up with if you set your system

949
00:35:51,520 --> 00:35:54,800
problems in this particular way like if

950
00:35:53,200 --> 00:35:56,400
you do certain things you'll end up with

951
00:35:54,800 --> 00:35:59,119
a strong red team or you'll end up with

952
00:35:56,400 --> 00:36:01,040
a strong whatever or do you feel like

953
00:35:59,119 --> 00:36:02,800
there's kind of not really much holding

954
00:36:01,040 --> 00:36:06,680
them back around that they have to do

955
00:36:02,800 --> 00:36:06,680
these sensitivity analys

956
00:36:11,520 --> 00:36:17,119
like I think red teaming in general is

957
00:36:14,720 --> 00:36:20,960
not my favorite evaluation method

958
00:36:17,119 --> 00:36:22,800
potentially um but I think like the to

959
00:36:20,960 --> 00:36:24,320
me it seems like the more basic problem

960
00:36:22,800 --> 00:36:26,000
to figure out and understand how to

961
00:36:24,320 --> 00:36:28,000
navigate is like where the instability

962
00:36:26,000 --> 00:36:30,160
and the baseline just model responses

963
00:36:28,000 --> 00:36:31,359
are coming from and then hopefully you

964
00:36:30,160 --> 00:36:32,960
know there have been some paper that

965
00:36:31,359 --> 00:36:35,119
I've seen recently that show like which

966
00:36:32,960 --> 00:36:36,880
semantic patterns seem to be important

967
00:36:35,119 --> 00:36:39,440
to the model in terms of leading to a

968
00:36:36,880 --> 00:36:43,359
difference in opinion or um or outcome

969
00:36:39,440 --> 00:36:45,119
and so if we can maybe a little bit more

970
00:36:43,359 --> 00:36:46,400
in a more principled manner map the

971
00:36:45,119 --> 00:36:47,839
kinds of changes that seem to be

972
00:36:46,400 --> 00:36:49,359
important to how the model outputs

973
00:36:47,839 --> 00:36:52,400
things that'll give us a clue to at

974
00:36:49,359 --> 00:36:54,240
least at first get a stable answer to

975
00:36:52,400 --> 00:36:55,680
the things that we're asking and then

976
00:36:54,240 --> 00:36:57,280
there's a separate question of making

977
00:36:55,680 --> 00:36:58,800
sure that we're asking the right thing

978
00:36:57,280 --> 00:37:01,200
which is I think more the question of

979
00:36:58,800 --> 00:37:02,480
like the first experiment. So I don't

980
00:37:01,200 --> 00:37:05,200
know if that answers your question, but

981
00:37:02,480 --> 00:37:07,280
I think like first understand like why

982
00:37:05,200 --> 00:37:09,040
we're getting instability and then use

983
00:37:07,280 --> 00:37:10,560
that to make sure that when we're doing

984
00:37:09,040 --> 00:37:13,839
a contextualized evaluation, we're

985
00:37:10,560 --> 00:37:18,760
getting a reasonable response. Yeah,

986
00:37:13,839 --> 00:37:18,760
great question though. Yeah. regard

987
00:37:21,680 --> 00:37:27,560
like the situation is framework

988
00:37:31,520 --> 00:37:36,320
or is that like

989
00:37:33,839 --> 00:37:39,920
>> I will tell you in about two minutes.

990
00:37:36,320 --> 00:37:41,200
Yes. Um any other questions on stuff

991
00:37:39,920 --> 00:37:43,040
before I move on? Maybe real curious

992
00:37:41,200 --> 00:37:44,960
about that. Oh, did I sorry my little

993
00:37:43,040 --> 00:37:47,200
blind? Yes. Go ahead.

994
00:37:44,960 --> 00:37:49,440
It seems to me like part of the problem

995
00:37:47,200 --> 00:37:52,400
is coming up with like a set of

996
00:37:49,440 --> 00:37:55,599
standards and like what is the base

997
00:37:52,400 --> 00:37:58,480
truth for what makes some concept toxic?

998
00:37:55,599 --> 00:38:00,640
So are there any efforts to

999
00:37:58,480 --> 00:38:04,880
>> are there any efforts to sort of catalog

1000
00:38:00,640 --> 00:38:06,960
like what makes something toxic to

1001
00:38:04,880 --> 00:38:09,200
>> That's a really good question. I think

1002
00:38:06,960 --> 00:38:13,520
that uh some companies are trying to

1003
00:38:09,200 --> 00:38:16,079
figure out stuff like that. um uh

1004
00:38:13,520 --> 00:38:17,920
um and I think that there are large

1005
00:38:16,079 --> 00:38:20,000
scale user studies that are being done

1006
00:38:17,920 --> 00:38:22,560
by tech companies to try to understand

1007
00:38:20,000 --> 00:38:25,440
what's the what's the

1008
00:38:22,560 --> 00:38:28,880
line at which some content becomes

1009
00:38:25,440 --> 00:38:32,320
toxic. I think that there are for when

1010
00:38:28,880 --> 00:38:34,079
you are using a um any kind of model for

1011
00:38:32,320 --> 00:38:35,839
a

1012
00:38:34,079 --> 00:38:38,079
important decision-making task for

1013
00:38:35,839 --> 00:38:40,079
example in employment there is like an

1014
00:38:38,079 --> 00:38:42,640
entire area of research called

1015
00:38:40,079 --> 00:38:44,480
industrial psychology IOCs that has

1016
00:38:42,640 --> 00:38:46,000
thought about what is kind of the right

1017
00:38:44,480 --> 00:38:47,280
way to make a decision about whether or

1018
00:38:46,000 --> 00:38:49,440
not someone's a good fit for a

1019
00:38:47,280 --> 00:38:51,040
particular job everyone has different

1020
00:38:49,440 --> 00:38:53,920
thoughts about to what extent

1021
00:38:51,040 --> 00:38:55,839
IOCsychology

1022
00:38:53,920 --> 00:38:57,280
people have thoughts on that I have my

1023
00:38:55,839 --> 00:38:59,440
I'll be happy to tell you offline but

1024
00:38:57,280 --> 00:39:01,440
there is research that is has been done

1025
00:38:59,440 --> 00:39:03,119
to kind of see you know what do we think

1026
00:39:01,440 --> 00:39:04,480
is like a valid way to make this kind of

1027
00:39:03,119 --> 00:39:06,560
a decision what do we think is a

1028
00:39:04,480 --> 00:39:08,400
reasonable thing to rely on all that

1029
00:39:06,560 --> 00:39:09,839
kind of stuff so I think that the answer

1030
00:39:08,400 --> 00:39:11,200
would be there are some situations where

1031
00:39:09,839 --> 00:39:13,119
people have thought about this in a

1032
00:39:11,200 --> 00:39:14,960
careful way and then I think there are

1033
00:39:13,119 --> 00:39:17,760
some places where people are starting to

1034
00:39:14,960 --> 00:39:19,280
but um I think there yeah there's

1035
00:39:17,760 --> 00:39:20,640
interest for stuff like toxicity I know

1036
00:39:19,280 --> 00:39:22,880
that there exists the literature for

1037
00:39:20,640 --> 00:39:24,800
employment I think it's kind of like a

1038
00:39:22,880 --> 00:39:25,920
area by area thing if that makes sense

1039
00:39:24,800 --> 00:39:27,520
>> okay I

1040
00:39:25,920 --> 00:39:30,079
would probably benefit from like some

1041
00:39:27,520 --> 00:39:33,079
sort of standard for like considering

1042
00:39:30,079 --> 00:39:33,079
questions.

1043
00:39:34,560 --> 00:39:37,560
So

1044
00:39:38,160 --> 00:39:44,000
I'm sure we can talk about that offline.

1045
00:39:39,680 --> 00:39:46,960
Yeah. Yeah. Um okay. All right. Uh so I

1046
00:39:44,000 --> 00:39:48,880
got a question about um other unless

1047
00:39:46,960 --> 00:39:50,400
there was another thing here. Other geni

1048
00:39:48,880 --> 00:39:54,640
relevant laws. Do you have a question?

1049
00:39:50,400 --> 00:39:55,920
No. Sorry. Um okay. Uh so we talked

1050
00:39:54,640 --> 00:39:57,920
about discrimination law relevant to

1051
00:39:55,920 --> 00:39:59,680
gender AI um and how it still applies

1052
00:39:57,920 --> 00:40:03,200
but sometimes not a good fit. Are there

1053
00:39:59,680 --> 00:40:04,800
any other legal um frameworks of note

1054
00:40:03,200 --> 00:40:06,000
that can help deal with the harms that

1055
00:40:04,800 --> 00:40:08,560
happen in these systems because

1056
00:40:06,000 --> 00:40:10,000
obviously uh you know not all donated

1057
00:40:08,560 --> 00:40:11,520
systems are being used in housing credit

1058
00:40:10,000 --> 00:40:14,400
and employment which is where the main

1059
00:40:11,520 --> 00:40:21,760
discrimination statutes in the US apply.

1060
00:40:14,400 --> 00:40:26,160
Um and uh yes I think so. Um so there is

1061
00:40:21,760 --> 00:40:29,119
a um there is a consumer protection law

1062
00:40:26,160 --> 00:40:31,920
uh called um mudap um which is unfair,

1063
00:40:29,119 --> 00:40:33,520
deceptive or abusive practices. This

1064
00:40:31,920 --> 00:40:37,119
comes from a couple of different places

1065
00:40:33,520 --> 00:40:39,359
in the law. One is the FTC article 5 um

1066
00:40:37,119 --> 00:40:42,160
and then the other is the DoddFrank Act

1067
00:40:39,359 --> 00:40:45,040
which is the act that came after the U

1068
00:40:42,160 --> 00:40:47,760
2008 financial crisis to make sure that

1069
00:40:45,040 --> 00:40:50,720
banks were not doing crazy things. Um

1070
00:40:47,760 --> 00:40:53,359
and what this statute basically says is

1071
00:40:50,720 --> 00:40:56,400
that it uh wants to prevent unfair,

1072
00:40:53,359 --> 00:40:58,240
deceptive or abusive um practices of

1073
00:40:56,400 --> 00:41:00,560
consumer products. So it's protecting

1074
00:40:58,240 --> 00:41:03,839
consumers from these types of behavior

1075
00:41:00,560 --> 00:41:07,599
in products that they have access to. So

1076
00:41:03,839 --> 00:41:10,319
what do they think of as unfair? Um it's

1077
00:41:07,599 --> 00:41:13,520
an act or practice uh that causes

1078
00:41:10,319 --> 00:41:15,520
substantial injury to consumers. um they

1079
00:41:13,520 --> 00:41:16,960
can't be reasonably able to avoid this

1080
00:41:15,520 --> 00:41:18,400
injury and the injury can't be

1081
00:41:16,960 --> 00:41:20,240
outweighed by counterveailing benefits

1082
00:41:18,400 --> 00:41:21,760
to consumers or competition. This is

1083
00:41:20,240 --> 00:41:23,280
similar to the disparate impact

1084
00:41:21,760 --> 00:41:24,720
framework that I talked about earlier,

1085
00:41:23,280 --> 00:41:26,560
but it also has some important

1086
00:41:24,720 --> 00:41:29,520
differences. I won't go into that too

1087
00:41:26,560 --> 00:41:33,599
much. Uh the deceptive prong. So this

1088
00:41:29,520 --> 00:41:35,839
this I think is great because it has a

1089
00:41:33,599 --> 00:41:38,160
wider coverage than discrimination law,

1090
00:41:35,839 --> 00:41:40,960
traditional discrimination law has. So

1091
00:41:38,160 --> 00:41:44,880
it could um apply to some journal harms

1092
00:41:40,960 --> 00:41:46,240
in a wider range of of products. But I

1093
00:41:44,880 --> 00:41:47,920
think something that is really

1094
00:41:46,240 --> 00:41:50,079
interesting that I'm going to talk about

1095
00:41:47,920 --> 00:41:53,680
for an extra 30 seconds um is this

1096
00:41:50,079 --> 00:41:56,800
deceptive prong which is uh that there

1097
00:41:53,680 --> 00:41:59,040
is a some kind of there's some kind of

1098
00:41:56,800 --> 00:42:01,520
practice that is likely to mislead the

1099
00:41:59,040 --> 00:42:02,800
consumer. Uh the consumer has to it has

1100
00:42:01,520 --> 00:42:05,520
to be kind of like a reasonable

1101
00:42:02,800 --> 00:42:07,359
misconception. Um and you know this this

1102
00:42:05,520 --> 00:42:09,119
materiality thing that the fact the

1103
00:42:07,359 --> 00:42:10,319
representation is material just usually

1104
00:42:09,119 --> 00:42:14,280
means that it needs to be written down

1105
00:42:10,319 --> 00:42:14,280
or visible in some way. Yes.

1106
00:42:14,480 --> 00:42:17,839
Like in the hierarchy setting the

1107
00:42:15,680 --> 00:42:19,760
consumer would be the employer not the

1108
00:42:17,839 --> 00:42:22,480
candidate. So I have to show that it's

1109
00:42:19,760 --> 00:42:27,599
forming or is it could be other?

1110
00:42:22,480 --> 00:42:30,880
>> Um it is usually I the the cases that

1111
00:42:27,599 --> 00:42:34,240
I've seen have largely been for the end

1112
00:42:30,880 --> 00:42:36,000
consumer but I'm not sure that's a good

1113
00:42:34,240 --> 00:42:38,160
question. I don't see why it wouldn't be

1114
00:42:36,000 --> 00:42:41,599
able to think of a company as a

1115
00:42:38,160 --> 00:42:43,680
consumer. But I'm honestly not familiar.

1116
00:42:41,599 --> 00:42:45,520
I don't know. I can't give you a 100%

1117
00:42:43,680 --> 00:42:49,119
answer to that question. But certainly

1118
00:42:45,520 --> 00:42:51,280
end and um end consumers like us, right?

1119
00:42:49,119 --> 00:42:52,880
Um

1120
00:42:51,280 --> 00:42:55,119
uh

1121
00:42:52,880 --> 00:42:57,440
the abusive prong is also cool. I won't

1122
00:42:55,119 --> 00:42:58,880
talk about it as much. Um but yeah, I

1123
00:42:57,440 --> 00:43:00,560
know that I've said this twice already,

1124
00:42:58,880 --> 00:43:02,720
but the thing one of the things that

1125
00:43:00,560 --> 00:43:05,440
makes this cool is that it applies to

1126
00:43:02,720 --> 00:43:07,839
all consumer products, which is much

1127
00:43:05,440 --> 00:43:12,240
less limited than just housing, credit,

1128
00:43:07,839 --> 00:43:15,440
and employment um uh um decision-m

1129
00:43:12,240 --> 00:43:17,440
systems. And those laws also have kind

1130
00:43:15,440 --> 00:43:18,800
of the way that they're written

1131
00:43:17,440 --> 00:43:20,880
sometimes can put really big

1132
00:43:18,800 --> 00:43:22,560
restrictions on who is liable for a

1133
00:43:20,880 --> 00:43:24,000
discrimination that's caused. So like

1134
00:43:22,560 --> 00:43:26,319
for example in the employment setting,

1135
00:43:24,000 --> 00:43:29,119
you can only hold an employer or kind of

1136
00:43:26,319 --> 00:43:31,359
like a staffing agency liable. And so

1137
00:43:29,119 --> 00:43:32,960
this has led to a lot fewer employment

1138
00:43:31,359 --> 00:43:35,599
discrimination cases happening in

1139
00:43:32,960 --> 00:43:38,240
practice even when we see employment

1140
00:43:35,599 --> 00:43:40,000
discrimination related activity from say

1141
00:43:38,240 --> 00:43:41,520
a big tech company because they're just

1142
00:43:40,000 --> 00:43:43,680
not the kind of entity that has

1143
00:43:41,520 --> 00:43:47,040
historically been held liable. Um and so

1144
00:43:43,680 --> 00:43:49,040
this leads to a lot of kind of less you

1145
00:43:47,040 --> 00:43:52,800
know less action than we might hope for.

1146
00:43:49,040 --> 00:43:54,880
Um but this UDAP adoption potentially

1147
00:43:52,800 --> 00:43:56,400
could um have a wider reach. The

1148
00:43:54,880 --> 00:43:57,680
downside of course is that no one has

1149
00:43:56,400 --> 00:43:59,920
really thought to use it for these kinds

1150
00:43:57,680 --> 00:44:01,440
of things before. But the bright side is

1151
00:43:59,920 --> 00:44:05,599
which I'll talk about on like the

1152
00:44:01,440 --> 00:44:08,160
penultimate slide uh is that several a

1153
00:44:05,599 --> 00:44:10,880
couple few states Massachusetts, New

1154
00:44:08,160 --> 00:44:13,280
Jersey and I think one other have

1155
00:44:10,880 --> 00:44:15,599
explicitly said that they are looking to

1156
00:44:13,280 --> 00:44:17,440
use the UDAP statute to prevent AI

1157
00:44:15,599 --> 00:44:18,720
discrimination. So I think if it hasn't

1158
00:44:17,440 --> 00:44:20,720
happened yet, it might happen in the

1159
00:44:18,720 --> 00:44:22,880
next couple of months or years. Um so

1160
00:44:20,720 --> 00:44:25,040
specific specifically why do I think you

1161
00:44:22,880 --> 00:44:26,560
know this unfairness thing I think is a

1162
00:44:25,040 --> 00:44:28,720
wider way that we can attack

1163
00:44:26,560 --> 00:44:30,079
discrimination in AI systems and genai

1164
00:44:28,720 --> 00:44:32,000
systems but what I think it's

1165
00:44:30,079 --> 00:44:34,720
particularly interesting um for

1166
00:44:32,000 --> 00:44:37,920
generative AI systems is I think that UD

1167
00:44:34,720 --> 00:44:41,119
also can be um a tool to prevent harm in

1168
00:44:37,920 --> 00:44:42,640
hallucinations. So this deceptive prong

1169
00:44:41,119 --> 00:44:48,319
um

1170
00:44:42,640 --> 00:44:50,319
if you have say a um if you have say a

1171
00:44:48,319 --> 00:44:53,359
uh

1172
00:44:50,319 --> 00:44:55,280
chatbot that's on some website say you

1173
00:44:53,359 --> 00:44:57,440
have it on some health care website like

1174
00:44:55,280 --> 00:44:59,040
an insurance provider or a hospital and

1175
00:44:57,440 --> 00:45:01,280
you ask some question like does my

1176
00:44:59,040 --> 00:45:03,520
insurance cover this procedure and the

1177
00:45:01,280 --> 00:45:08,000
model says yes it does absolutely I'm a

1178
00:45:03,520 --> 00:45:10,079
chatbot on your um on your uh on your

1179
00:45:08,000 --> 00:45:11,680
insurance providers this website and I'm

1180
00:45:10,079 --> 00:45:14,880
going to say yes, it does. But there is

1181
00:45:11,680 --> 00:45:17,920
this little uh

1182
00:45:14,880 --> 00:45:20,000
message down here that says, "Hey, by

1183
00:45:17,920 --> 00:45:22,400
the way, small print, this is generated

1184
00:45:20,000 --> 00:45:24,000
by AI and might not be accurate medical

1185
00:45:22,400 --> 00:45:27,280
advice or accurate insurance advice,

1186
00:45:24,000 --> 00:45:29,440
right?" Um, and so there's very clear

1187
00:45:27,280 --> 00:45:31,680
legal precedent that from the UDAP

1188
00:45:29,440 --> 00:45:32,720
statute, a what's called a pro-forma

1189
00:45:31,680 --> 00:45:35,280
disclaimer, which is something that's

1190
00:45:32,720 --> 00:45:36,800
like a generalized disclaimer, um, you

1191
00:45:35,280 --> 00:45:38,560
know, that's not changing based off of

1192
00:45:36,800 --> 00:45:43,119
the information that's provided to you,

1193
00:45:38,560 --> 00:45:45,040
is not enough to, um, to, uh, cure

1194
00:45:43,119 --> 00:45:47,440
otherwise messages or practices. So

1195
00:45:45,040 --> 00:45:50,720
basically what this what the case law

1196
00:45:47,440 --> 00:45:52,640
says is that if the overall vibe of the

1197
00:45:50,720 --> 00:45:54,480
information you're getting is that this

1198
00:45:52,640 --> 00:45:56,160
is a trustworthy piece of information

1199
00:45:54,480 --> 00:45:57,920
which generative AI systems are really

1200
00:45:56,160 --> 00:46:00,079
good at doing. They're really good at

1201
00:45:57,920 --> 00:46:03,280
giving you information that seems very

1202
00:46:00,079 --> 00:46:05,520
plausible but might not be right. a like

1203
00:46:03,280 --> 00:46:09,280
state a a small disclaimer that doesn't

1204
00:46:05,520 --> 00:46:11,839
like fully change that impression is

1205
00:46:09,280 --> 00:46:14,800
definitely not enough to get you out of

1206
00:46:11,839 --> 00:46:15,440
this deceptive um deceptive liability

1207
00:46:14,800 --> 00:46:19,920
framework. Yeah,

1208
00:46:15,440 --> 00:46:21,920
>> statistic words then it might be right.

1209
00:46:19,920 --> 00:46:23,359
>> Yeah, that would that would be the case.

1210
00:46:21,920 --> 00:46:24,720
Yeah. Yeah, I think you could still make

1211
00:46:23,359 --> 00:46:27,040
the case. I think it would be weaker.

1212
00:46:24,720 --> 00:46:29,680
Yeah. Um so this is something that I've

1213
00:46:27,040 --> 00:46:31,440
been thinking a bunch about. Um feel

1214
00:46:29,680 --> 00:46:35,040
free to talk to me about this. this work

1215
00:46:31,440 --> 00:46:37,520
is unpublished still. Uh so

1216
00:46:35,040 --> 00:46:40,000
be nice. But yes, I'm really excited

1217
00:46:37,520 --> 00:46:42,319
about this and I hope I think that um

1218
00:46:40,000 --> 00:46:46,400
you know

1219
00:46:42,319 --> 00:46:48,400
this uh this um

1220
00:46:46,400 --> 00:46:50,079
this is some coverage showing that there

1221
00:46:48,400 --> 00:46:51,920
have been state attorneys general that

1222
00:46:50,079 --> 00:46:53,760
have specifically called out that they

1223
00:46:51,920 --> 00:46:55,839
are willing to use UDAP to prevent

1224
00:46:53,760 --> 00:46:57,920
discrimination in AI systems. So I'm

1225
00:46:55,839 --> 00:46:59,359
again excited about where this might go

1226
00:46:57,920 --> 00:47:01,040
in the next couple of months or years.

1227
00:46:59,359 --> 00:47:03,280
Um, and I guess something that I want to

1228
00:47:01,040 --> 00:47:06,400
say a little bit more broadly is I know

1229
00:47:03,280 --> 00:47:08,960
that this can be a little bit of an uh

1230
00:47:06,400 --> 00:47:11,200
trying time uh thinking about preventing

1231
00:47:08,960 --> 00:47:12,800
harms and AI systems, but laws still

1232
00:47:11,200 --> 00:47:15,119
exist and are still very much being

1233
00:47:12,800 --> 00:47:17,839
enforced. Um, here's just a smattering

1234
00:47:15,119 --> 00:47:19,760
of some of the state and local laws that

1235
00:47:17,839 --> 00:47:22,800
are preventing discrimination in the US.

1236
00:47:19,760 --> 00:47:25,599
and again statements about uh state

1237
00:47:22,800 --> 00:47:27,920
authorities being poised and ready to

1238
00:47:25,599 --> 00:47:30,240
use the uh statutes that they do have

1239
00:47:27,920 --> 00:47:32,880
and every state has the UDAP statute uh

1240
00:47:30,240 --> 00:47:34,640
to um use the laws that they have to

1241
00:47:32,880 --> 00:47:37,520
prevent AI discrimination on the ground.

1242
00:47:34,640 --> 00:47:39,520
So not all hope is lost. Don't fret too

1243
00:47:37,520 --> 00:47:42,240
much, you know, or at least try not to.

1244
00:47:39,520 --> 00:47:44,160
Um, and then I guess one last thing that

1245
00:47:42,240 --> 00:47:46,400
I'll say because I think I should

1246
00:47:44,160 --> 00:47:48,240
probably be wrapping up soon, um, is

1247
00:47:46,400 --> 00:47:52,480
that if anyone wants to talk to me about

1248
00:47:48,240 --> 00:47:54,800
it later, um, another just PSA that I

1249
00:47:52,480 --> 00:47:56,720
want to have out, uh, that is also

1250
00:47:54,800 --> 00:47:59,359
relevant to some work that I'm doing

1251
00:47:56,720 --> 00:48:02,800
with, um, Charlotte Burrows and Jenny

1252
00:47:59,359 --> 00:48:04,160
Yang, um, uh, who used to be at the

1253
00:48:02,800 --> 00:48:06,160
Equal Employment Opportunity Commission

1254
00:48:04,160 --> 00:48:09,359
and Pauling Kim, um, who I think we have

1255
00:48:06,160 --> 00:48:12,319
some friends of here as well, uh, is do

1256
00:48:09,359 --> 00:48:14,160
not, you know, overread into the

1257
00:48:12,319 --> 00:48:16,560
executive orders that have come out that

1258
00:48:14,160 --> 00:48:18,319
seem to be pushing against um

1259
00:48:16,560 --> 00:48:20,800
discrimination law that exists right

1260
00:48:18,319 --> 00:48:23,200
now. Uh so in particular, there's a

1261
00:48:20,800 --> 00:48:25,599
pretty concerted effort to push back

1262
00:48:23,200 --> 00:48:28,000
against disperate impact liability. Uh

1263
00:48:25,599 --> 00:48:30,880
and I think two things to remember here.

1264
00:48:28,000 --> 00:48:32,640
Um one is that this is just enforcement.

1265
00:48:30,880 --> 00:48:35,440
So the law still exists at least for

1266
00:48:32,640 --> 00:48:37,760
now. But two, um I can go more into this

1267
00:48:35,440 --> 00:48:40,880
if we want to talk offline, but uh the

1268
00:48:37,760 --> 00:48:44,880
the case that these executive orders

1269
00:48:40,880 --> 00:48:48,640
make uh is basically that um disperate

1270
00:48:44,880 --> 00:48:50,800
impact liability uh requires a company

1271
00:48:48,640 --> 00:48:52,240
to essentially make a lot of changes to

1272
00:48:50,800 --> 00:48:53,839
a decision-m system if there's any

1273
00:48:52,240 --> 00:48:56,160
evidence of discrimination of disperate

1274
00:48:53,839 --> 00:48:57,680
impact of differences in allocation

1275
00:48:56,160 --> 00:48:59,520
rates between different demographic

1276
00:48:57,680 --> 00:49:00,800
groups. And that's just factually not

1277
00:48:59,520 --> 00:49:02,160
the case. written into the law of

1278
00:49:00,800 --> 00:49:04,480
disparate impact is the fact that you

1279
00:49:02,160 --> 00:49:08,079
only need to do so if there is no

1280
00:49:04,480 --> 00:49:09,920
business necessity that um that

1281
00:49:08,079 --> 00:49:11,839
essentially excuses that discrimination

1282
00:49:09,920 --> 00:49:13,520
in that particular instance. So I think

1283
00:49:11,839 --> 00:49:16,079
that's just something to keep in mind.

1284
00:49:13,520 --> 00:49:17,920
One, you know, uh executive orders don't

1285
00:49:16,079 --> 00:49:19,440
change the law if you're not, you know,

1286
00:49:17,920 --> 00:49:20,559
famili it can change enforcement but not

1287
00:49:19,440 --> 00:49:24,240
the law because it's the executive

1288
00:49:20,559 --> 00:49:26,880
branch. and to um you know the claims

1289
00:49:24,240 --> 00:49:29,359
that are being made are based off of a

1290
00:49:26,880 --> 00:49:32,480
very gross and inexcusable misreading of

1291
00:49:29,359 --> 00:49:35,280
the law. And I am working on a piece

1292
00:49:32,480 --> 00:49:38,079
with um Charlotte, Pauline, and Jenny

1293
00:49:35,280 --> 00:49:39,760
about how both, you know, explaining why

1294
00:49:38,079 --> 00:49:43,200
not to read into these um executive

1295
00:49:39,760 --> 00:49:45,040
orders too much, but also uh making the

1296
00:49:43,200 --> 00:49:48,559
case that at least in the case of

1297
00:49:45,040 --> 00:49:52,559
employment, um the laws that exist on

1298
00:49:48,559 --> 00:49:55,040
the books actually do require you to uh

1299
00:49:52,559 --> 00:49:58,000
debias an AI system as you are creating

1300
00:49:55,040 --> 00:49:59,760
it. Um I'm happy to talk to um anyone

1301
00:49:58,000 --> 00:50:02,559
about this offline, but basically

1302
00:49:59,760 --> 00:50:04,640
because employment requires a model to

1303
00:50:02,559 --> 00:50:06,400
be valid and that it's making its

1304
00:50:04,640 --> 00:50:07,760
decisions in a reasonable way. This

1305
00:50:06,400 --> 00:50:10,079
validity requirement actually

1306
00:50:07,760 --> 00:50:11,920
necessitates that you uh debias as

1307
00:50:10,079 --> 00:50:13,440
you're creating a system. So that's

1308
00:50:11,920 --> 00:50:14,800
another strain of work that I'm kind of

1309
00:50:13,440 --> 00:50:18,240
excited about, but also just wanted to

1310
00:50:14,800 --> 00:50:20,880
have this PSA, you know, um about these

1311
00:50:18,240 --> 00:50:22,480
things that are going on. Um so I had a

1312
00:50:20,880 --> 00:50:24,400
time permitting piece, but as I might

1313
00:50:22,480 --> 00:50:26,079
have expected, I won't be getting to

1314
00:50:24,400 --> 00:50:28,240
that. Uh if anyone wants to talk to me

1315
00:50:26,079 --> 00:50:29,839
about generative monoculture,

1316
00:50:28,240 --> 00:50:32,960
multilingual prompting, feel free to

1317
00:50:29,839 --> 00:50:36,240
talk to me offline. Um which is cool and

1318
00:50:32,960 --> 00:50:37,440
fun. Uh but with that, I will end. And

1319
00:50:36,240 --> 00:50:39,200
if you want to talk to me about any of

1320
00:50:37,440 --> 00:50:42,520
these things, uh please don't hesitate

1321
00:50:39,200 --> 00:50:42,520
to reach out.

